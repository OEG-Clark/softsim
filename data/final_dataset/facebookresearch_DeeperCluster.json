{"home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.main": [[50, 167], ["parser.parse_args", "print", "src.utils.fix_random_seeds", "src.model.model_factory.model_factory", "src.model.pretrain.load_pretrained", "model.cuda.cuda", "print", "src.data.VOC2007.VOC2007_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "hasattr", "model.cuda.pred_layer.modules", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "torch.BCEWithLogitsLoss", "print", "src.utils.AverageMeter", "print", "print", "src.data.VOC2007.VOC2007_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "eval_voc_classif.evaluate", "print", "src.data.VOC2007.VOC2007_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "eval_voc_classif.evaluate", "torchvision.Normalize", "model.cuda.body.classifier.modules", "isinstance", "hasattr", "filter", "eval_voc_classif.train", "torchvision.Compose", "isinstance", "m.weight.data.normal_", "m.bias.data.fill_", "model.cuda.body.features.parameters", "model.cuda.parameters", "torchvision.Resize", "torchvision.TenCrop", "torchvision.Lambda", "torchvision.Compose", "torchvision.Compose", "m.weight.data.normal_", "m.bias.data.fill_", "torchvision.RandomHorizontalFlip", "torchvision.RandomResizedCrop", "torchvision.ToTensor", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torchvision.RandomHorizontalFlip", "torchvision.RandomResizedCrop", "torchvision.ToTensor", "torchvision.Compose", "torchvision.ToTensor"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.model_factory", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.load_pretrained", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.evaluate", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.evaluate", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "world_size", "=", "1", "\n", "print", "(", "args", ")", "\n", "\n", "fix_random_seeds", "(", "args", ".", "seed", ")", "\n", "\n", "# create model", "\n", "model", "=", "model_factory", "(", "args", ",", "relu", "=", "True", ",", "num_classes", "=", "20", ")", "\n", "\n", "# load pretrained weights", "\n", "load_pretrained", "(", "model", ",", "args", ")", "\n", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "print", "(", "'model to cuda'", ")", "\n", "\n", "# on which split to train", "\n", "if", "args", ".", "split", "==", "'train'", ":", "\n", "        ", "args", ".", "test", "=", "'val'", "\n", "", "elif", "args", ".", "split", "==", "'trainval'", ":", "\n", "        ", "args", ".", "test", "=", "'test'", "\n", "\n", "# data loader", "\n", "", "normalize", "=", "[", "transforms", ".", "Normalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "]", "\n", "dataset", "=", "VOC2007_dataset", "(", "args", ".", "data_path", ",", "split", "=", "args", ".", "split", ",", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "]", "+", "normalize", "\n", ")", ")", "\n", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "16", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "4", ",", "pin_memory", "=", "True", ")", "\n", "print", "(", "'PASCAL VOC 2007 '", "+", "args", ".", "split", "+", "' dataset loaded'", ")", "\n", "\n", "# re initialize classifier", "\n", "if", "hasattr", "(", "model", ".", "body", ",", "'classifier'", ")", ":", "\n", "        ", "for", "m", "in", "model", ".", "body", ".", "classifier", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.01", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.1", ")", "\n", "", "", "", "for", "m", "in", "model", ".", "pred_layer", ".", "modules", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.01", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.1", ")", "\n", "\n", "# freeze conv layers", "\n", "", "", "if", "args", ".", "fc6_8", ":", "\n", "        ", "if", "hasattr", "(", "model", ".", "body", ",", "'features'", ")", ":", "\n", "            ", "for", "param", "in", "model", ".", "body", ".", "features", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "# set optimizer", "\n", "", "", "", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "\n", "filter", "(", "lambda", "x", ":", "x", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "momentum", "=", "0.9", ",", "\n", "weight_decay", "=", "args", ".", "wd", ",", "\n", ")", "\n", "\n", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "'none'", ")", "\n", "\n", "print", "(", "'Start training'", ")", "\n", "it", "=", "0", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "while", "it", "<", "args", ".", "nit", ":", "\n", "        ", "it", "=", "train", "(", "\n", "loader", ",", "\n", "model", ",", "\n", "optimizer", ",", "\n", "criterion", ",", "\n", "args", ".", "fc6_8", ",", "\n", "losses", ",", "\n", "current_iteration", "=", "it", ",", "\n", "total_iterations", "=", "args", ".", "nit", ",", "\n", "stepsize", "=", "args", ".", "stepsize", ",", "\n", ")", "\n", "\n", "", "print", "(", "'Model Evaluation'", ")", "\n", "if", "args", ".", "eval_random_crops", ":", "\n", "        ", "transform_eval", "=", "[", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "]", "+", "normalize", "\n", "", "else", ":", "\n", "        ", "transform_eval", "=", "[", "\n", "transforms", ".", "Resize", "(", "256", ")", ",", "\n", "transforms", ".", "TenCrop", "(", "224", ")", ",", "\n", "transforms", ".", "Lambda", "(", "lambda", "crops", ":", "torch", ".", "stack", "(", "[", "transforms", ".", "Compose", "(", "normalize", ")", "(", "transforms", ".", "ToTensor", "(", ")", "(", "crop", ")", ")", "for", "crop", "in", "crops", "]", ")", ")", "\n", "]", "\n", "\n", "", "print", "(", "'Train set'", ")", "\n", "train_dataset", "=", "VOC2007_dataset", "(", "\n", "args", ".", "data_path", ",", "\n", "split", "=", "args", ".", "split", ",", "\n", "transform", "=", "transforms", ".", "Compose", "(", "transform_eval", ")", ",", "\n", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "4", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "evaluate", "(", "train_loader", ",", "model", ",", "args", ".", "eval_random_crops", ")", "\n", "\n", "print", "(", "'Test set'", ")", "\n", "test_dataset", "=", "VOC2007_dataset", "(", "args", ".", "data_path", ",", "split", "=", "args", ".", "test", ",", "transform", "=", "transforms", ".", "Compose", "(", "transform_eval", ")", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "test_dataset", ",", "\n", "batch_size", "=", "1", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "4", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "evaluate", "(", "test_loader", ",", "model", ",", "args", ".", "eval_random_crops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.evaluate": [[169, 197], ["model.eval", "range", "range", "print", "enumerate", "numpy.concatenate", "numpy.concatenate", "sklearn.metrics.average_precision_score", "aps.append", "numpy.mean", "input.view.cuda", "len", "input.view.size", "input.view.view", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model", "scr.append", "gts.append", "model.cpu().numpy", "input.view.size", "torch.sum().cpu().numpy", "torch.sum().cpu().numpy", "torch.sum().cpu().numpy", "torch.sum().cpu().numpy", "model.cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["None"], ["", "def", "evaluate", "(", "loader", ",", "model", ",", "eval_random_crops", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "gts", "=", "[", "]", "\n", "scr", "=", "[", "]", "\n", "for", "crop", "in", "range", "(", "9", "*", "eval_random_crops", "+", "1", ")", ":", "\n", "        ", "for", "i", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "# move input to gpu and optionally reshape it", "\n", "            ", "if", "len", "(", "input", ".", "size", "(", ")", ")", "==", "5", ":", "\n", "                ", "bs", ",", "ncrops", ",", "c", ",", "h", ",", "w", "=", "input", ".", "size", "(", ")", "\n", "input", "=", "input", ".", "view", "(", "-", "1", ",", "c", ",", "h", ",", "w", ")", "\n", "", "input", "=", "input", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# forward pass without grad computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "output", "=", "model", "(", "input", ")", "\n", "", "if", "crop", "<", "1", ":", "\n", "                    ", "scr", ".", "append", "(", "torch", ".", "sum", "(", "output", ",", "0", ",", "keepdim", "=", "True", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "gts", ".", "append", "(", "target", ")", "\n", "", "else", ":", "\n", "                    ", "scr", "[", "i", "]", "+=", "output", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "", "", "gts", "=", "np", ".", "concatenate", "(", "gts", ",", "axis", "=", "0", ")", ".", "T", "\n", "scr", "=", "np", ".", "concatenate", "(", "scr", ",", "axis", "=", "0", ")", ".", "T", "\n", "aps", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "20", ")", ":", "\n", "# Subtract eps from score to make AP work for tied scores", "\n", "        ", "ap", "=", "metrics", ".", "average_precision_score", "(", "gts", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", ",", "scr", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", "-", "1e-5", "*", "gts", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", ")", "\n", "aps", ".", "append", "(", "ap", ")", "\n", "", "print", "(", "np", ".", "mean", "(", "aps", ")", ",", "'  '", ",", "' '", ".", "join", "(", "[", "'%0.2f'", "%", "a", "for", "a", "in", "aps", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train": [[199, 259], ["src.utils.AverageMeter", "src.utils.AverageMeter", "src.utils.AverageMeter", "time.time", "hasattr", "enumerate", "model.train", "model.body.features.eval", "model.eval", "src.utils.AverageMeter.update", "input.cuda.cuda", "model", "target.float().cuda.float().cuda", "optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "losses.update", "src.utils.AverageMeter.update", "time.time", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "target.float().cuda.size", "model.parameters", "loss.item", "input.cuda.size", "print", "time.time", "print", "target.float().cuda.float", "criterion().masked_fill_", "time.time", "criterion"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update"], ["", "def", "train", "(", "loader", ",", "model", ",", "optimizer", ",", "criterion", ",", "fc6_8", ",", "losses", ",", "current_iteration", "=", "0", ",", "total_iterations", "=", "None", ",", "stepsize", "=", "None", ",", "verbose", "=", "True", ")", ":", "\n", "# to log", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "# use dropout for the MLP", "\n", "if", "hasattr", "(", "model", ".", "body", ",", "'classifier'", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "# in the batch norms always use global statistics", "\n", "model", ".", "body", ".", "features", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "", "for", "i", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "        ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "\n", "# adjust learning rate", "\n", "if", "current_iteration", "!=", "0", "and", "current_iteration", "%", "stepsize", "==", "0", ":", "\n", "            ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                ", "param_group", "[", "'lr'", "]", "=", "param_group", "[", "'lr'", "]", "*", "0.5", "\n", "print", "(", "'iter {0} learning rate is {1}'", ".", "format", "(", "current_iteration", ",", "param_group", "[", "'lr'", "]", ")", ")", "\n", "\n", "# move input to gpu", "\n", "", "", "input", "=", "input", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# forward pass with or without grad computation", "\n", "output", "=", "model", "(", "input", ")", "\n", "\n", "target", "=", "target", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "mask", "=", "(", "target", "==", "255", ")", "\n", "loss", "=", "torch", ".", "sum", "(", "criterion", "(", "output", ",", "target", ")", ".", "masked_fill_", "(", "mask", ",", "0", ")", ")", "/", "target", ".", "size", "(", "0", ")", "\n", "\n", "# backward", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# clip gradients", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "10", ")", "\n", "# and weights update", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# measure accuracy and record loss", "\n", "losses", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "if", "verbose", "is", "True", "and", "current_iteration", "%", "25", "==", "0", ":", "\n", "            ", "print", "(", "'Iteration[{0}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'", "\n", "'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'", ".", "format", "(", "\n", "current_iteration", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "losses", ")", ")", "\n", "", "current_iteration", "=", "current_iteration", "+", "1", "\n", "if", "total_iterations", "is", "not", "None", "and", "current_iteration", "==", "total_iterations", ":", "\n", "            ", "break", "\n", "", "", "return", "current_iteration", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.main.get_parser": [[29, 103], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Unsupervised feature learning.\"", ")", "\n", "\n", "# handling experiment parameters", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint_freq\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Save the model every this epoch.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\"./exp\"", ",", "\n", "help", "=", "\"Experiment dump path.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Current epoch to run.'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_iter'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'First iter to run in the current epoch.'", ")", "\n", "\n", "# network params", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Start from this instead of random weights.'", ")", "\n", "\n", "# datasets params", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to find training dataset.'", ")", "\n", "parser", ".", "add_argument", "(", "'--size_dataset'", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "\n", "help", "=", "'How many images to use.'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Number of data loading workers.'", ")", "\n", "parser", ".", "add_argument", "(", "'--sobel'", ",", "type", "=", "bool_flag", ",", "default", "=", "0", ",", "\n", "help", "=", "'Apply Sobel filter.'", ")", "\n", "\n", "# optim params", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'Learning rate.'", ")", "\n", "parser", ".", "add_argument", "(", "'--wd'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'Weight decay.'", ")", "\n", "parser", ".", "add_argument", "(", "'--nepochs'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Max number of epochs to run.'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "48", ",", "type", "=", "int", ",", "\n", "help", "=", "'Batch-size per process.'", ")", "\n", "\n", "# Model params", "\n", "parser", ".", "add_argument", "(", "'--reassignment'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "\n", "help", "=", "'Reassign clusters every this epoch(s).'", ")", "\n", "parser", ".", "add_argument", "(", "'--dim_pca'", ",", "type", "=", "int", ",", "default", "=", "4096", ",", "\n", "help", "=", "'Dimension of the pca applied to the descriptors.'", ")", "\n", "parser", ".", "add_argument", "(", "'--k'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Total number of clusters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--super_classes'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "'Total number of super-classes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--rotnet'", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "'Network needs to classify large rotations.'", ")", "\n", "\n", "# k-means params", "\n", "parser", ".", "add_argument", "(", "'--warm_restart'", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use previous centroids as init.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_faiss'", ",", "type", "=", "bool_flag", ",", "default", "=", "True", ",", "\n", "help", "=", "'Use faiss for E steps in k-means.'", ")", "\n", "parser", ".", "add_argument", "(", "'--niter'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of k-means iterations.'", ")", "\n", "\n", "# distributed training params", "\n", "parser", ".", "add_argument", "(", "'--rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'Global process rank.'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--world-size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of distributed processes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-url'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "'Url used to set up distributed training.'", ")", "\n", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug within a SLURM job.\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.main.main": [[105, 243], ["src.utils.init_distributed_mode", "src.utils.check_parameters", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.YFCC100M.YFCC100M_dataset", "src.data.loader.get_data_transformations", "src.utils.fix_random_seeds", "src.model.model_factory.model_factory", "logger.info", "src.model.pretrain.load_pretrained", "apex.parallel.convert_syncbn_model", "src.model.model_factory.to_cuda", "logger.info", "src.model.model_factory.sgd_optimizer", "src.clustering.load_cluster_assignments", "src.model.model_factory.build_prediction_layer", "src.model.model_factory.build_prediction_layer", "src.utils.restart_from_checkpoint", "src.utils.restart_from_checkpoint", "range", "str", "logger.info", "src.utils.fix_random_seeds", "src.trainer.train_network", "logger.info", "training_stats.update", "torch.barrier", "os.path.join", "logger.info", "src.clustering.get_cluster_assignments", "src.model.model_factory.build_prediction_layer", "src.utils.end_of_epoch", "src.model.model_factory.build_prediction_layer"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.init_distributed_mode", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.check_parameters", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.initialize_exp", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.get_data_transformations", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.model_factory", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.load_pretrained", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sgd_optimizer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.load_cluster_assignments", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.build_prediction_layer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.build_prediction_layer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.restart_from_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.restart_from_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.train_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.get_cluster_assignments", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.build_prediction_layer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.end_of_epoch", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.build_prediction_layer"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    This code implements the paper: https://arxiv.org/abs/1905.01278\n    The method consists in alternating between a hierachical clustering of the\n    features and learning the parameters of a convnet by predicting both the\n    angle of the rotation applied to the input data and the cluster assignments\n    in a single hierachical loss.\n    \"\"\"", "\n", "\n", "# initialize communication groups", "\n", "training_groups", ",", "clustering_groups", "=", "init_distributed_mode", "(", "args", ")", "\n", "\n", "# check parameters", "\n", "check_parameters", "(", "args", ")", "\n", "\n", "# initialize the experiment", "\n", "logger", ",", "training_stats", "=", "initialize_exp", "(", "args", ",", "'epoch'", ",", "'iter'", ",", "'prec'", ",", "'loss'", ",", "\n", "'prec_super_class'", ",", "'loss_super_class'", ",", "\n", "'prec_sub_class'", ",", "'loss_sub_class'", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "# load data", "\n", "dataset", "=", "YFCC100M_dataset", "(", "args", ".", "data_path", ",", "size", "=", "args", ".", "size_dataset", ")", "\n", "\n", "# prepare the different data transformations", "\n", "tr_cluster", ",", "tr_train", "=", "get_data_transformations", "(", "args", ".", "rotation", "*", "90", ")", "\n", "\n", "# build model skeleton", "\n", "fix_random_seeds", "(", ")", "\n", "model", "=", "model_factory", "(", "args", ".", "sobel", ")", "\n", "logger", ".", "info", "(", "'model created'", ")", "\n", "\n", "# load pretrained weights", "\n", "load_pretrained", "(", "model", ",", "args", ")", "\n", "\n", "# convert batch-norm layers to nvidia wrapper to enable batch stats reduction", "\n", "model", "=", "apex", ".", "parallel", ".", "convert_syncbn_model", "(", "model", ")", "\n", "\n", "# distributed training wrapper", "\n", "model", "=", "to_cuda", "(", "model", ",", "args", ".", "gpu_to_work_on", ",", "apex", "=", "True", ")", "\n", "logger", ".", "info", "(", "'model to cuda'", ")", "\n", "\n", "# set optimizer", "\n", "optimizer", "=", "sgd_optimizer", "(", "model", ",", "args", ".", "lr", ",", "args", ".", "wd", ")", "\n", "\n", "# load cluster assignments", "\n", "cluster_assignments", "=", "load_cluster_assignments", "(", "args", ",", "dataset", ")", "\n", "\n", "# build prediction layer on the super_class", "\n", "pred_layer", ",", "optimizer_pred_layer", "=", "build_prediction_layer", "(", "\n", "model", ".", "module", ".", "body", ".", "dim_output_space", ",", "\n", "args", ",", "\n", ")", "\n", "\n", "nmb_sub_classes", "=", "args", ".", "k", "//", "args", ".", "nmb_super_clusters", "\n", "sub_class_pred_layer", ",", "optimizer_sub_class_pred_layer", "=", "build_prediction_layer", "(", "\n", "model", ".", "module", ".", "body", ".", "dim_output_space", ",", "\n", "args", ",", "\n", "num_classes", "=", "nmb_sub_classes", ",", "\n", "group", "=", "training_groups", "[", "args", ".", "training_local_world_id", "]", ",", "\n", ")", "\n", "\n", "# variables to fetch in checkpoint", "\n", "to_restore", "=", "{", "'epoch'", ":", "0", ",", "'start_iter'", ":", "0", "}", "\n", "\n", "# re start from checkpoint", "\n", "restart_from_checkpoint", "(", "\n", "args", ",", "\n", "run_variables", "=", "to_restore", ",", "\n", "state_dict", "=", "model", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", "pred_layer_state_dict", "=", "pred_layer", ",", "\n", "optimizer_pred_layer", "=", "optimizer_pred_layer", ",", "\n", ")", "\n", "pred_layer_name", "=", "str", "(", "args", ".", "training_local_world_id", ")", "+", "'-pred_layer.pth.tar'", "\n", "restart_from_checkpoint", "(", "\n", "args", ",", "\n", "ckp_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "pred_layer_name", ")", ",", "\n", "state_dict", "=", "sub_class_pred_layer", ",", "\n", "optimizer", "=", "optimizer_sub_class_pred_layer", ",", "\n", ")", "\n", "args", ".", "epoch", "=", "to_restore", "[", "'epoch'", "]", "\n", "args", ".", "start_iter", "=", "to_restore", "[", "'start_iter'", "]", "\n", "\n", "for", "_", "in", "range", "(", "args", ".", "epoch", ",", "args", ".", "nepochs", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "args", ".", "epoch", ")", "\n", "fix_random_seeds", "(", "args", ".", "epoch", ")", "\n", "\n", "# step 1: Get the final activations for the whole dataset / Cluster them", "\n", "\n", "if", "cluster_assignments", "is", "None", "and", "not", "args", ".", "epoch", "%", "args", ".", "reassignment", ":", "\n", "\n", "            ", "logger", ".", "info", "(", "\"=> Start clustering step\"", ")", "\n", "dataset", ".", "transform", "=", "tr_cluster", "\n", "\n", "cluster_assignments", "=", "get_cluster_assignments", "(", "args", ",", "model", ",", "dataset", ",", "clustering_groups", ")", "\n", "\n", "# reset prediction layers", "\n", "if", "args", ".", "nmb_super_clusters", ">", "1", ":", "\n", "                ", "pred_layer", ",", "optimizer_pred_layer", "=", "build_prediction_layer", "(", "\n", "model", ".", "module", ".", "body", ".", "dim_output_space", ",", "\n", "args", ",", "\n", ")", "\n", "", "sub_class_pred_layer", ",", "optimizer_sub_class_pred_layer", "=", "build_prediction_layer", "(", "\n", "model", ".", "module", ".", "body", ".", "dim_output_space", ",", "\n", "args", ",", "\n", "num_classes", "=", "nmb_sub_classes", ",", "\n", "group", "=", "training_groups", "[", "args", ".", "training_local_world_id", "]", ",", "\n", ")", "\n", "\n", "\n", "# step 2: Train the network with the cluster assignments as labels", "\n", "\n", "# prepare dataset", "\n", "", "dataset", ".", "transform", "=", "tr_train", "\n", "dataset", ".", "sub_classes", "=", "cluster_assignments", "\n", "\n", "# concatenate models and their corresponding optimizers", "\n", "models", "=", "[", "model", ",", "pred_layer", ",", "sub_class_pred_layer", "]", "\n", "optimizers", "=", "[", "optimizer", ",", "optimizer_pred_layer", ",", "optimizer_sub_class_pred_layer", "]", "\n", "\n", "# train the network for one epoch", "\n", "scores", "=", "train_network", "(", "args", ",", "models", ",", "optimizers", ",", "dataset", ")", "\n", "\n", "## save training statistics", "\n", "logger", ".", "info", "(", "scores", ")", "\n", "training_stats", ".", "update", "(", "scores", ")", "\n", "\n", "# reassign clusters at the next epoch", "\n", "if", "not", "args", ".", "epoch", "%", "args", ".", "reassignment", ":", "\n", "            ", "cluster_assignments", "=", "None", "\n", "dataset", ".", "subset_indexes", "=", "None", "\n", "end_of_epoch", "(", "args", ")", "\n", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_pretrain.get_parser": [[30, 87], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Train classification\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\".\"", ",", "\n", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Current epoch to run'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_iter'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'First iter to run in the current epoch'", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint_freq\"", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Save the model periodically \"", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluate\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Evaluate the model only\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "35", ",", "help", "=", "'random seed'", ")", "\n", "\n", "# model params", "\n", "parser", ".", "add_argument", "(", "'--sobel'", ",", "type", "=", "bool_flag", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--sobel2RGB'", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "'Incorporate sobel filter in first conv'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Use this instead of random weights.'", ")", "\n", "\n", "# datasets params", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to find ImageNet dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Number of data loading workers'", ")", "\n", "\n", "# optim params", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--wd'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--nepochs'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Max number of epochs to run'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "128", ",", "type", "=", "int", ")", "\n", "\n", "# distributed training params", "\n", "parser", ".", "add_argument", "(", "'--rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'rank'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--world-size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of distributed processes'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-url'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "'url used to set up distributed training'", ")", "\n", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Load val set of ImageNet\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug within a SLURM job\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_pretrain.main": [[89, 181], ["src.utils.init_distributed_mode", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.loader.load_data", "os.path.join", "src.data.loader.load_data", "src.data.loader.get_data_transformations", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "src.utils.fix_random_seeds", "src.model.model_factory.model_factory", "src.model.pretrain.load_pretrained", "hasattr", "src.model.model_factory.to_cuda", "logger.info", "src.model.model_factory.sgd_optimizer", "src.utils.restart_from_checkpoint", "range", "os.path.join", "os.path.join", "src.model.model_factory.sobel2RGB", "src.model.model_factory.to_cuda.body.classifier.modules", "src.trainer.validate_network", "logger.info", "src.utils.fix_random_seeds", "eval_pretrain.adjust_learning_rate", "eval_pretrain.train_network", "src.trainer.validate_network", "logger.info", "training_stats.update", "isinstance", "m.weight.data.normal_", "m.bias.data.fill_"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.init_distributed_mode", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.initialize_exp", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.load_data", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.load_data", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.get_data_transformations", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.model_factory", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.load_pretrained", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sgd_optimizer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.restart_from_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sobel2RGB", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.validate_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_pretrain.adjust_learning_rate", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.train_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.validate_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "# initialize the multi-GPU / multi-node training", "\n", "    ", "init_distributed_mode", "(", "args", ",", "make_communication_groups", "=", "False", ")", "\n", "\n", "# initialize the experiment", "\n", "logger", ",", "training_stats", "=", "initialize_exp", "(", "args", ",", "'epoch'", ",", "'iter'", ",", "'prec'", ",", "\n", "'loss'", ",", "'prec_val'", ",", "'loss_val'", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "main_data_path", "=", "args", ".", "data_path", "\n", "if", "args", ".", "debug", ":", "\n", "        ", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "main_data_path", ",", "'val'", ")", "\n", "", "else", ":", "\n", "        ", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "main_data_path", ",", "'train'", ")", "\n", "", "train_dataset", "=", "load_data", "(", "args", ")", "\n", "\n", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "main_data_path", ",", "'val'", ")", "\n", "val_dataset", "=", "load_data", "(", "args", ")", "\n", "\n", "# prepare the different data transformations", "\n", "tr_val", ",", "tr_train", "=", "get_data_transformations", "(", ")", "\n", "train_dataset", ".", "transform", "=", "tr_train", "\n", "val_dataset", ".", "transform", "=", "tr_val", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "val_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "\n", "# build model skeleton", "\n", "fix_random_seeds", "(", "args", ".", "seed", ")", "\n", "nmb_classes", "=", "205", "if", "'places'", "in", "args", ".", "data_path", "else", "1000", "\n", "model", "=", "model_factory", "(", "args", ",", "relu", "=", "True", ",", "num_classes", "=", "nmb_classes", ")", "\n", "\n", "# load pretrained weights", "\n", "load_pretrained", "(", "model", ",", "args", ")", "\n", "\n", "# merge sobel layers with first convolution layer", "\n", "if", "args", ".", "sobel2RGB", ":", "\n", "        ", "sobel2RGB", "(", "model", ")", "\n", "\n", "# re initialize classifier", "\n", "", "if", "hasattr", "(", "model", ".", "body", ",", "'classifier'", ")", ":", "\n", "        ", "for", "m", "in", "model", ".", "body", ".", "classifier", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.01", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0.1", ")", "\n", "\n", "# distributed training wrapper", "\n", "", "", "", "model", "=", "to_cuda", "(", "model", ",", "[", "args", ".", "gpu_to_work_on", "]", ",", "apex", "=", "True", ")", "\n", "logger", ".", "info", "(", "'model to cuda'", ")", "\n", "\n", "# set optimizer", "\n", "optimizer", "=", "sgd_optimizer", "(", "model", ",", "args", ".", "lr", ",", "args", ".", "wd", ")", "\n", "\n", "## variables to reload to fetch in checkpoint", "\n", "to_restore", "=", "{", "'epoch'", ":", "0", ",", "'start_iter'", ":", "0", "}", "\n", "\n", "# re start from checkpoint", "\n", "restart_from_checkpoint", "(", "\n", "args", ",", "\n", "run_variables", "=", "to_restore", ",", "\n", "state_dict", "=", "model", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", ")", "\n", "args", ".", "epoch", "=", "to_restore", "[", "'epoch'", "]", "\n", "args", ".", "start_iter", "=", "to_restore", "[", "'start_iter'", "]", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "validate_network", "(", "val_loader", ",", "[", "model", "]", ",", "args", ")", "\n", "return", "\n", "\n", "# Supervised training", "\n", "", "for", "_", "in", "range", "(", "args", ".", "epoch", ",", "args", ".", "nepochs", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "args", ".", "epoch", ")", "\n", "\n", "fix_random_seeds", "(", "args", ".", "seed", "+", "args", ".", "epoch", ")", "\n", "\n", "# train the network for one epoch", "\n", "adjust_learning_rate", "(", "optimizer", ",", "args", ")", "\n", "scores", "=", "train_network", "(", "args", ",", "model", ",", "optimizer", ",", "train_dataset", ")", "\n", "\n", "scores_val", "=", "validate_network", "(", "val_loader", ",", "[", "model", "]", ",", "args", ")", "\n", "\n", "# save training statistics", "\n", "logger", ".", "info", "(", "scores", "+", "scores_val", ")", "\n", "training_stats", ".", "update", "(", "scores", "+", "scores_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_pretrain.adjust_learning_rate": [[183, 187], ["None"], "function", ["None"], ["", "", "def", "adjust_learning_rate", "(", "optimizer", ",", "args", ")", ":", "\n", "    ", "lr", "=", "args", ".", "lr", "*", "(", "0.1", "**", "(", "args", ".", "epoch", "//", "30", ")", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_pretrain.train_network": [[189, 294], ["model.train", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "src.utils.AverageMeter", "src.utils.AverageMeter", "src.utils.AverageMeter", "src.utils.AverageMeter", "time.perf_counter", "torch.CrossEntropyLoss().cuda", "enumerate", "src.utils.AverageMeter.update", "inp.cuda.cuda", "target.cuda.cuda", "model", "nn.CrossEntropyLoss().cuda.", "optimizer.zero_grad", "cel.backward", "optimizer.step", "src.utils.AverageMeter.update", "src.trainer.accuracy", "src.utils.AverageMeter.update", "src.utils.AverageMeter.update", "time.perf_counter", "torch.save", "torch.save", "torch.CrossEntropyLoss", "cel.item", "model.size", "src.trainer.accuracy.item", "model.size", "logger.info", "os.path.join", "shutil.copyfile", "len", "time.perf_counter", "torch.save", "torch.save", "src.slurm.trigger_job_requeue", "time.perf_counter", "model.state_dict", "optimizer.state_dict", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "model.state_dict", "optimizer.state_dict", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.trigger_job_requeue"], ["", "", "def", "train_network", "(", "args", ",", "model", ",", "optimizer", ",", "dataset", ")", ":", "\n", "    ", "\"\"\"\n    Train the models on the dataset.\n    \"\"\"", "\n", "# swith to train mode", "\n", "model", ".", "train", "(", ")", "\n", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "dataset", ")", "\n", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "sampler", "=", "sampler", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "\n", "# running statistics", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "\n", "# training statistics", "\n", "log_top1", "=", "AverageMeter", "(", ")", "\n", "log_loss", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "cel", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "for", "iter_epoch", ",", "(", "inp", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "        ", "data_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "\n", "# start at iter start_iter", "\n", "if", "iter_epoch", "<", "args", ".", "start_iter", ":", "\n", "            ", "continue", "\n", "\n", "# move to gpu", "\n", "", "inp", "=", "inp", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# forward", "\n", "output", "=", "model", "(", "inp", ")", "\n", "\n", "# compute cross entropy loss", "\n", "loss", "=", "cel", "(", "output", ",", "target", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# compute the gradients", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# step", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# log", "\n", "\n", "# signal received, relaunch experiment", "\n", "if", "os", ".", "environ", "[", "'SIGNAL_RECEIVED'", "]", "==", "'True'", ":", "\n", "            ", "if", "not", "args", ".", "rank", ":", "\n", "                ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "iter_epoch", "+", "1", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "trigger_job_requeue", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n", "# update stats", "\n", "", "", "log_loss", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "prec1", "=", "accuracy", "(", "args", ",", "output", ",", "target", ")", "\n", "log_top1", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "batch_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "# verbose", "\n", "if", "iter_epoch", "%", "100", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Epoch[{0}] - Iter: [{1}/{2}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'", "\n", "'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'", "\n", "'Prec {log_top1.val:.3f} ({log_top1.avg:.3f})\\t'", "\n", ".", "format", "(", "args", ".", "epoch", ",", "iter_epoch", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "log_loss", ",", "log_top1", "=", "log_top1", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "", "args", ".", "start_iter", "=", "0", "\n", "args", ".", "epoch", "+=", "1", "\n", "\n", "# dump checkpoint", "\n", "if", "not", "args", ".", "rank", ":", "\n", "        ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "0", ",", "\n", "'state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "if", "not", "(", "args", ".", "epoch", "-", "1", ")", "%", "args", ".", "checkpoint_freq", ":", "\n", "            ", "shutil", ".", "copyfile", "(", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "dump_checkpoints", ",", "\n", "'checkpoint'", "+", "str", "(", "args", ".", "epoch", "-", "1", ")", "+", "'.pth.tar'", ")", ",", "\n", ")", "\n", "\n", "", "", "return", "(", "args", ".", "epoch", "-", "1", ",", "args", ".", "epoch", "*", "len", "(", "loader", ")", ",", "log_top1", ".", "avg", ",", "log_loss", ".", "avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.RegLog.__init__": [[243, 262], ["torch.Module.__init__", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFold.__init__"], ["def", "__init__", "(", "self", ",", "num_labels", ",", "conv", ")", ":", "\n", "        ", "super", "(", "RegLog", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "conv", "<", "3", ":", "\n", "            ", "av", "=", "18", "\n", "s", "=", "9216", "\n", "", "elif", "conv", "<", "5", ":", "\n", "            ", "av", "=", "14", "\n", "s", "=", "8192", "\n", "", "elif", "conv", "<", "8", ":", "\n", "            ", "av", "=", "9", "\n", "s", "=", "9216", "\n", "", "elif", "conv", "<", "11", ":", "\n", "            ", "av", "=", "6", "\n", "s", "=", "8192", "\n", "", "elif", "conv", "<", "14", ":", "\n", "            ", "av", "=", "3", "\n", "s", "=", "8192", "\n", "", "self", ".", "av_pool", "=", "nn", ".", "AvgPool2d", "(", "av", ",", "stride", "=", "av", ",", "padding", "=", "0", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "s", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.RegLog.forward": [[263, 267], ["eval_linear.RegLog.av_pool", "x.view.view.view", "eval_linear.RegLog.linear", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "av_pool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "return", "self", ".", "linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.get_parser": [[31, 90], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Generate a parameters parser.\n    \"\"\"", "\n", "# parse parameters", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Train a linear classifier on conv layer\"", ")", "\n", "\n", "# main parameters", "\n", "parser", ".", "add_argument", "(", "\"--dump_path\"", ",", "type", "=", "str", ",", "default", "=", "\".\"", ",", "\n", "help", "=", "\"Experiment dump path\"", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Current epoch to run'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_iter'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'First iter to run in the current epoch'", ")", "\n", "\n", "# model params", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Use this instead of random weights.'", ")", "\n", "parser", ".", "add_argument", "(", "'--conv'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "choices", "=", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "9", ",", "10", ",", "11", ",", "12", ",", "13", "]", ",", "\n", "help", "=", "'On top of which layer train classifier.'", ")", "\n", "\n", "# datasets params", "\n", "parser", ".", "add_argument", "(", "'--data_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to find supervised dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "'Number of data loading workers'", ")", "\n", "parser", ".", "add_argument", "(", "'--sobel'", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ")", "\n", "\n", "# optim params", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--wd'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'Weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--nepochs'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Max number of epochs to run'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "default", "=", "64", ",", "type", "=", "int", ")", "\n", "\n", "# model selection", "\n", "parser", ".", "add_argument", "(", "'--split'", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "'train'", ",", "choices", "=", "[", "'train'", ",", "'trainval'", "]", ",", "\n", "help", "=", "'for PASCAL dataset, train on train or train+val'", ")", "\n", "parser", ".", "add_argument", "(", "'--kfold'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"\"\"dataset randomly partitioned into kfold equal sized subsamples.\n                        Default None: no cross validation: train on full train set\"\"\"", ")", "\n", "parser", ".", "add_argument", "(", "'--cross_valid'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'between 0 and kfold - 1: index of the round of cross validation'", ")", "\n", "\n", "# distributed training params", "\n", "parser", ".", "add_argument", "(", "'--rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'rank'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "\"Multi-GPU - Local rank\"", ")", "\n", "parser", ".", "add_argument", "(", "'--world-size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of distributed processes'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-url'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "\n", "help", "=", "'url used to set up distributed training'", ")", "\n", "\n", "# debug", "\n", "parser", ".", "add_argument", "(", "\"--debug_slurm\"", ",", "type", "=", "bool_flag", ",", "default", "=", "False", ",", "\n", "help", "=", "\"Debug within a SLURM job\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.main": [[92, 203], ["src.utils.init_distributed_mode", "src.utils.initialize_exp", "src.slurm.init_signal_handler", "src.data.loader.get_data_transformations", "src.utils.fix_random_seeds", "src.model.model_factory.model_factory", "src.model.pretrain.load_pretrained", "eval_linear.RegLog", "src.model.model_factory.to_cuda", "src.model.model_factory.to_cuda", "logger.info", "src.model.model_factory.sgd_optimizer", "src.utils.restart_from_checkpoint", "src.model.model_factory.to_cuda.eval", "src.model.model_factory.to_cuda.train", "range", "os.path.join", "src.data.loader.load_data", "src.data.VOC2007.VOC2007_dataset", "src.data.loader.load_data", "src.data.VOC2007.VOC2007_dataset", "src.data.loader.KFold", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "eval_linear.train_network", "logger.info", "training_stats.update", "os.path.join", "src.data.loader.per_target", "src.trainer.validate_network", "eval_linear.evaluate_pascal"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.init_distributed_mode", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.initialize_exp", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.init_signal_handler", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.get_data_transformations", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.model_factory", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.load_pretrained", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sgd_optimizer", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.restart_from_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.load_data", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.load_data", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.train_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.per_target", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.validate_network", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.evaluate_pascal"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "# initialize the multi-GPU / multi-node training", "\n", "    ", "init_distributed_mode", "(", "args", ",", "make_communication_groups", "=", "False", ")", "\n", "\n", "# initialize the experiment", "\n", "logger", ",", "training_stats", "=", "initialize_exp", "(", "args", ",", "'epoch'", ",", "'iter'", ",", "'prec'", ",", "\n", "'loss'", ",", "'prec_val'", ",", "'loss_val'", ")", "\n", "\n", "# initialize SLURM signal handler for time limit / pre-emption", "\n", "init_signal_handler", "(", ")", "\n", "\n", "if", "not", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "        ", "main_data_path", "=", "args", ".", "data_path", "\n", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "main_data_path", ",", "'train'", ")", "\n", "train_dataset", "=", "load_data", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "train_dataset", "=", "VOC2007_dataset", "(", "args", ".", "data_path", ",", "split", "=", "args", ".", "split", ")", "\n", "\n", "", "args", ".", "test", "=", "'val'", "if", "args", ".", "split", "==", "'train'", "else", "'test'", "\n", "if", "not", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "        ", "if", "args", ".", "cross_valid", "is", "None", ":", "\n", "            ", "args", ".", "data_path", "=", "os", ".", "path", ".", "join", "(", "main_data_path", ",", "'val'", ")", "\n", "", "val_dataset", "=", "load_data", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "val_dataset", "=", "VOC2007_dataset", "(", "args", ".", "data_path", ",", "split", "=", "args", ".", "test", ")", "\n", "\n", "", "if", "args", ".", "cross_valid", "is", "not", "None", ":", "\n", "        ", "kfold", "=", "KFold", "(", "per_target", "(", "train_dataset", ".", "imgs", ")", ",", "args", ".", "cross_valid", ",", "args", ".", "kfold", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "sampler", "=", "kfold", ".", "train", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "pin_memory", "=", "True", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "val_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "sampler", "=", "kfold", ".", "val", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "", "else", ":", "\n", "        ", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "True", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "pin_memory", "=", "True", ")", "\n", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "val_dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "# prepare the different data transformations", "\n", "", "tr_val", ",", "tr_train", "=", "get_data_transformations", "(", ")", "\n", "train_dataset", ".", "transform", "=", "tr_train", "\n", "val_dataset", ".", "transform", "=", "tr_val", "\n", "\n", "# build model skeleton", "\n", "fix_random_seeds", "(", ")", "\n", "model", "=", "model_factory", "(", "args", ")", "\n", "\n", "load_pretrained", "(", "model", ",", "args", ")", "\n", "\n", "# keep only conv layers", "\n", "model", ".", "body", ".", "classifier", "=", "None", "\n", "model", ".", "conv", "=", "args", ".", "conv", "\n", "\n", "if", "'places'", "in", "args", ".", "data_path", ":", "\n", "        ", "nmb_classes", "=", "205", "\n", "", "elif", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "        ", "nmb_classes", "=", "20", "\n", "", "else", ":", "\n", "        ", "nmb_classes", "=", "1000", "\n", "\n", "", "reglog", "=", "RegLog", "(", "nmb_classes", ",", "args", ".", "conv", ")", "\n", "\n", "# distributed training wrapper", "\n", "model", "=", "to_cuda", "(", "model", ",", "[", "args", ".", "gpu_to_work_on", "]", ",", "apex", "=", "True", ")", "\n", "reglog", "=", "to_cuda", "(", "reglog", ",", "[", "args", ".", "gpu_to_work_on", "]", ",", "apex", "=", "True", ")", "\n", "logger", ".", "info", "(", "'model to cuda'", ")", "\n", "\n", "# set optimizer", "\n", "optimizer", "=", "sgd_optimizer", "(", "reglog", ",", "args", ".", "lr", ",", "args", ".", "wd", ")", "\n", "\n", "## variables to reload to fetch in checkpoint", "\n", "to_restore", "=", "{", "'epoch'", ":", "0", ",", "'start_iter'", ":", "0", "}", "\n", "\n", "# re start from checkpoint", "\n", "restart_from_checkpoint", "(", "\n", "args", ",", "\n", "run_variables", "=", "to_restore", ",", "\n", "state_dict", "=", "reglog", ",", "\n", "optimizer", "=", "optimizer", ",", "\n", ")", "\n", "args", ".", "epoch", "=", "to_restore", "[", "'epoch'", "]", "\n", "args", ".", "start_iter", "=", "to_restore", "[", "'start_iter'", "]", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "reglog", ".", "train", "(", ")", "\n", "\n", "# Linear training", "\n", "for", "_", "in", "range", "(", "args", ".", "epoch", ",", "args", ".", "nepochs", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"============ Starting epoch %i ... ============\"", "%", "args", ".", "epoch", ")", "\n", "\n", "# train the network for one epoch", "\n", "scores", "=", "train_network", "(", "args", ",", "model", ",", "reglog", ",", "optimizer", ",", "train_loader", ")", "\n", "\n", "if", "not", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "            ", "scores_val", "=", "validate_network", "(", "val_loader", ",", "[", "model", ",", "reglog", "]", ",", "args", ")", "\n", "", "else", ":", "\n", "            ", "scores_val", "=", "evaluate_pascal", "(", "val_dataset", ",", "[", "model", ",", "reglog", "]", ")", "\n", "\n", "", "scores", "=", "scores", "+", "scores_val", "\n", "\n", "# save training statistics", "\n", "logger", ".", "info", "(", "scores", ")", "\n", "training_stats", ".", "update", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.evaluate_pascal": [[205, 239], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "enumerate", "range", "print", "model.eval", "input.cuda.cuda", "numpy.concatenate", "numpy.concatenate", "sklearn.metrics.average_precision_score", "aps.append", "numpy.mean", "numpy.mean", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "torch.no_grad", "torch.no_grad", "torch.no_grad", "scr.append", "gts.append", "output.cpu().numpy", "torch.sum().cpu().numpy", "torch.sum().cpu().numpy", "torch.sum().cpu().numpy", "output.cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum"], "function", ["None"], ["", "", "def", "evaluate_pascal", "(", "val_dataset", ",", "models", ")", ":", "\n", "\n", "    ", "val_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "val_dataset", ",", "\n", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "val_dataset", ")", ",", "\n", "batch_size", "=", "1", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "", "gts", "=", "[", "]", "\n", "scr", "=", "[", "]", "\n", "for", "i", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "# move input to gpu and optionally reshape it", "\n", "        ", "input", "=", "input", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# forward pass without grad computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output", "=", "models", "[", "0", "]", "(", "input", ")", "\n", "output", "=", "models", "[", "1", "]", "(", "output", ")", "\n", "scr", ".", "append", "(", "torch", ".", "sum", "(", "output", ",", "0", ",", "keepdim", "=", "True", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "gts", ".", "append", "(", "target", ")", "\n", "scr", "[", "i", "]", "+=", "output", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "", "gts", "=", "np", ".", "concatenate", "(", "gts", ",", "axis", "=", "0", ")", ".", "T", "\n", "scr", "=", "np", ".", "concatenate", "(", "scr", ",", "axis", "=", "0", ")", ".", "T", "\n", "aps", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "20", ")", ":", "\n", "# Subtract eps from score to make AP work for tied scores", "\n", "        ", "ap", "=", "metrics", ".", "average_precision_score", "(", "gts", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", ",", "scr", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", "-", "1e-5", "*", "gts", "[", "i", "]", "[", "gts", "[", "i", "]", "<=", "1", "]", ")", "\n", "aps", ".", "append", "(", "ap", ")", "\n", "", "print", "(", "np", ".", "mean", "(", "aps", ")", ",", "'  '", ",", "' '", ".", "join", "(", "[", "'%0.2f'", "%", "a", "for", "a", "in", "aps", "]", ")", ")", "\n", "return", "np", ".", "mean", "(", "aps", ")", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.train_network": [[269, 369], ["src.utils.AverageMeter", "src.utils.AverageMeter", "src.utils.AverageMeter", "src.utils.AverageMeter", "time.perf_counter", "enumerate", "torch.BCEWithLogitsLoss", "torch.CrossEntropyLoss().cuda", "src.utils.AverageMeter.update", "eval_linear.learning_rate_decay", "inp.cuda.cuda", "target.float.cuda", "reglog", "nn.CrossEntropyLoss().cuda.", "optimizer.zero_grad", "criterion.backward", "optimizer.step", "src.utils.AverageMeter.update", "src.utils.AverageMeter.update", "time.perf_counter", "torch.save", "torch.save", "torch.save", "target.float.float", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model", "criterion.item", "model.size", "src.trainer.accuracy", "src.utils.AverageMeter.update", "logger.info", "os.path.join", "len", "torch.CrossEntropyLoss", "time.perf_counter", "torch.sum", "torch.sum", "torch.sum", "target.float.size", "torch.save", "torch.save", "torch.save", "src.slurm.trigger_job_requeue", "src.trainer.accuracy.item", "model.size", "time.perf_counter", "reglog.state_dict", "optimizer.state_dict", "len", "criterion.masked_fill_", "os.path.join", "os.path.join", "len", "reglog.state_dict", "optimizer.state_dict"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.learning_rate_decay", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.trigger_job_requeue"], ["", "", "def", "train_network", "(", "args", ",", "model", ",", "reglog", ",", "optimizer", ",", "loader", ")", ":", "\n", "    ", "\"\"\"\n    Train the models on the dataset.\n    \"\"\"", "\n", "# running statistics", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "\n", "# training statistics", "\n", "log_top1", "=", "AverageMeter", "(", ")", "\n", "log_loss", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "if", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "        ", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "", "for", "iter_epoch", ",", "(", "inp", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "# measure data loading time", "\n", "        ", "data_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "\n", "learning_rate_decay", "(", "optimizer", ",", "len", "(", "loader", ")", "*", "args", ".", "epoch", "+", "iter_epoch", ",", "args", ".", "lr", ")", "\n", "\n", "# start at iter start_iter", "\n", "if", "iter_epoch", "<", "args", ".", "start_iter", ":", "\n", "            ", "continue", "\n", "\n", "# move to gpu", "\n", "", "inp", "=", "inp", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "if", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "            ", "target", "=", "target", ".", "float", "(", ")", "\n", "\n", "# forward", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output", "=", "model", "(", "inp", ")", "\n", "", "output", "=", "reglog", "(", "output", ")", "\n", "\n", "# compute cross entropy loss", "\n", "loss", "=", "criterion", "(", "output", ",", "target", ")", "\n", "\n", "if", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "            ", "mask", "=", "(", "target", "==", "255", ")", "\n", "loss", "=", "torch", ".", "sum", "(", "loss", ".", "masked_fill_", "(", "mask", ",", "0", ")", ")", "/", "target", ".", "size", "(", "0", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# compute the gradients", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# step", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# log", "\n", "\n", "# signal received, relaunch experiment", "\n", "if", "os", ".", "environ", "[", "'SIGNAL_RECEIVED'", "]", "==", "'True'", ":", "\n", "            ", "if", "not", "args", ".", "rank", ":", "\n", "                ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "iter_epoch", "+", "1", ",", "\n", "'state_dict'", ":", "reglog", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "trigger_job_requeue", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n", "# update stats", "\n", "", "", "log_loss", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "if", "not", "'pascal'", "in", "args", ".", "data_path", ":", "\n", "            ", "prec1", "=", "accuracy", "(", "args", ",", "output", ",", "target", ")", "\n", "log_top1", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "batch_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "# verbose", "\n", "if", "iter_epoch", "%", "100", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Epoch[{0}] - Iter: [{1}/{2}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'", "\n", "'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'", "\n", "'Prec {log_top1.val:.3f} ({log_top1.avg:.3f})\\t'", "\n", ".", "format", "(", "args", ".", "epoch", ",", "iter_epoch", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "log_loss", ",", "log_top1", "=", "log_top1", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "", "args", ".", "start_iter", "=", "0", "\n", "args", ".", "epoch", "+=", "1", "\n", "\n", "# dump checkpoint", "\n", "if", "not", "args", ".", "rank", ":", "\n", "        ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "0", ",", "\n", "'state_dict'", ":", "reglog", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n", "", "return", "(", "args", ".", "epoch", "-", "1", ",", "args", ".", "epoch", "*", "len", "(", "loader", ")", ",", "log_top1", ".", "avg", ",", "log_loss", ".", "avg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_linear.learning_rate_decay": [[371, 375], ["numpy.sqrt"], "function", ["None"], ["", "def", "learning_rate_decay", "(", "optimizer", ",", "t", ",", "lr_0", ")", ":", "\n", "    ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "lr", "=", "lr_0", "/", "np", ".", "sqrt", "(", "1", "+", "lr_0", "*", "param_group", "[", "'weight_decay'", "]", "*", "t", ")", "\n", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.trigger_job_requeue": [[17, 30], ["exit", "os.path.isfile", "print", "print", "os.system", "print", "int", "str", "RuntimeError", "os.getpid"], "function", ["None"], ["def", "trigger_job_requeue", "(", "checkpoint_filename", ")", ":", "\n", "    ", "''' Submit a new job to resume from checkpoint.\n        Be careful to use only for main process.\n    '''", "\n", "if", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "==", "0", "and", "str", "(", "os", ".", "getpid", "(", ")", ")", "==", "os", ".", "environ", "[", "'MAIN_PID'", "]", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint_filename", ")", ":", "\n", "        ", "print", "(", "'time is up, back to slurm queue'", ",", "flush", "=", "True", ")", "\n", "command", "=", "'scontrol requeue '", "+", "os", ".", "environ", "[", "'SLURM_JOB_ID'", "]", "\n", "print", "(", "command", ")", "\n", "if", "os", ".", "system", "(", "command", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'requeue failed'", ")", "\n", "", "print", "(", "'New job submitted to the queue'", ",", "flush", "=", "True", ")", "\n", "", "exit", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.SIGTERMHandler": [[32, 35], ["print"], "function", ["None"], ["", "def", "SIGTERMHandler", "(", "a", ",", "b", ")", ":", "\n", "    ", "print", "(", "'received sigterm'", ")", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.signalHandler": [[37, 41], ["print", "time.time"], "function", ["None"], ["", "def", "signalHandler", "(", "a", ",", "b", ")", ":", "\n", "    ", "print", "(", "'Signal received'", ",", "a", ",", "time", ".", "time", "(", ")", ",", "flush", "=", "True", ")", "\n", "os", ".", "environ", "[", "'SIGNAL_RECEIVED'", "]", "=", "'True'", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.init_signal_handler": [[43, 53], ["str", "signal.signal", "signal.signal", "print", "os.getpid"], "function", ["None"], ["", "def", "init_signal_handler", "(", ")", ":", "\n", "    ", "\"\"\"\n    Handle signals sent by SLURM for time limit / pre-emption.\n    \"\"\"", "\n", "os", ".", "environ", "[", "'SIGNAL_RECEIVED'", "]", "=", "'False'", "\n", "os", ".", "environ", "[", "'MAIN_PID'", "]", "=", "str", "(", "os", ".", "getpid", "(", ")", ")", "\n", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGUSR1", ",", "signalHandler", ")", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGTERM", ",", "SIGTERMHandler", ")", "\n", "print", "(", "\"Signal handler installed.\"", ",", "flush", "=", "True", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.Subset_Sampler.__init__": [[202, 204], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "indices", "=", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.Subset_Sampler.__iter__": [[205, 207], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.Subset_Sampler.__len__": [[208, 210], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.get_cluster_assignments": [[25, 195], ["model.eval", "torch.arange().int", "torch.arange().int", "os.path.isfile", "torch.all_gather", "torch.cat().cpu().long", "torch.cat().cpu().long", "all_counts.reshape.reshape", "logger.info", "torch.barrier", "int", "numpy.long", "numpy.long", "torch.zeros", "torch.zeros", "torch.cumsum().long", "torch.cumsum().long", "range", "os.path.join", "torch.barrier", "pickle.load().cuda", "distributed_kmeans.distributed_kmeans", "torch.barrier", "len", "os.path.join", "pickle.load", "logger.info", "utils.get_indices_sparse", "torch.zeros().cuda", "torch.zeros().cuda", "range", "clustering.Subset_Sampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "distributed_kmeans.initialize_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "distributed_kmeans.distributed_kmeans", "utils.get_indices_sparse", "torch.zeros().cuda", "torch.zeros().cuda", "range", "torch.barrier", "torch.zeros().cuda", "torch.zeros().cuda", "all_counts.reshape.sum", "numpy.where", "numpy.long", "numpy.long", "numpy.load", "torch.FloatTensor", "torch.FloatTensor", "pickle.dump", "len", "pickle.dump", "pickle.dump", "torch.arange", "torch.arange", "open", "len", "len", "len", "numpy.save", "len", "pickle.dump", "pickle.dump", "range", "torch.cat().cpu", "torch.cat().cpu", "len", "torch.cumsum", "torch.cumsum", "numpy.long", "max", "min", "open", "open", "pickle.load", "open", "open", "os.path.join", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "open", "open", "open", "torch.zeros", "torch.zeros", "all_counts[].long", "os.path.join", "str", "numpy.random.choice", "open", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "torch.cat", "torch.cat", "len", "numpy.arange", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.distributed_kmeans.distributed_kmeans", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.get_indices_sparse", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.distributed_kmeans.initialize_cache", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.distributed_kmeans.distributed_kmeans", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.get_indices_sparse"], ["def", "get_cluster_assignments", "(", "args", ",", "model", ",", "dataset", ",", "groups", ")", ":", "\n", "    ", "\"\"\"\n    \"\"\"", "\n", "# pseudo-labels are confusing", "\n", "dataset", ".", "sub_classes", "=", "None", "\n", "\n", "# swith to eval mode", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# this process deals only with a subset of the dataset", "\n", "local_nmb_data", "=", "len", "(", "dataset", ")", "//", "args", ".", "world_size", "\n", "indices", "=", "torch", ".", "arange", "(", "args", ".", "rank", "*", "local_nmb_data", ",", "(", "args", ".", "rank", "+", "1", ")", "*", "local_nmb_data", ")", ".", "int", "(", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'super_class_assignments.pkl'", ")", ")", ":", "\n", "\n", "# super-class assignments have already been computed in a previous run", "\n", "\n", "        ", "super_class_assignements", "=", "pickle", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'super_class_assignments.pkl'", ")", ",", "'rb'", ")", ")", "\n", "logger", ".", "info", "(", "'loaded super-class assignments'", ")", "\n", "\n", "# dump cache", "\n", "where_helper", "=", "get_indices_sparse", "(", "super_class_assignements", "[", "indices", "]", ")", "\n", "nmb_data_per_super_cluster", "=", "torch", ".", "zeros", "(", "args", ".", "nmb_super_clusters", ")", ".", "cuda", "(", ")", "\n", "for", "super_class", "in", "range", "(", "len", "(", "where_helper", ")", ")", ":", "\n", "            ", "nmb_data_per_super_cluster", "[", "super_class", "]", "=", "len", "(", "where_helper", "[", "super_class", "]", "[", "0", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "sampler", "=", "Subset_Sampler", "(", "indices", ")", "\n", "\n", "# we need a data loader", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "sampler", "=", "sampler", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "\n", "# initialize cache, pca and centroids", "\n", "cache", ",", "centroids", "=", "initialize_cache", "(", "args", ",", "loader", ",", "model", ")", "\n", "\n", "# empty cuda cache (useful because we're about to use faiss on gpu)", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "## perform clustering into super_clusters", "\n", "super_class_assignements", ",", "centroids_sc", "=", "distributed_kmeans", "(", "\n", "args", ",", "\n", "args", ".", "size_dataset", ",", "\n", "args", ".", "nmb_super_clusters", ",", "\n", "cache", ",", "\n", "args", ".", "rank", ",", "\n", "args", ".", "world_size", ",", "\n", "centroids", ",", "\n", ")", "\n", "\n", "# dump activations in the cache", "\n", "where_helper", "=", "get_indices_sparse", "(", "super_class_assignements", "[", "indices", "]", ")", "\n", "nmb_data_per_super_cluster", "=", "torch", ".", "zeros", "(", "args", ".", "nmb_super_clusters", ")", ".", "cuda", "(", ")", "\n", "for", "super_class", "in", "range", "(", "len", "(", "where_helper", ")", ")", ":", "\n", "            ", "ind_sc", "=", "where_helper", "[", "super_class", "]", "[", "0", "]", "\n", "np", ".", "save", "(", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "dump_path", ",", "\n", "'cache/'", ",", "\n", "'super_class'", "+", "str", "(", "super_class", ")", "+", "'-'", "+", "str", "(", "args", ".", "rank", ")", ",", "\n", ")", ",", "'wb'", ")", ",", "cache", "[", "ind_sc", "]", ")", "\n", "\n", "nmb_data_per_super_cluster", "[", "super_class", "]", "=", "len", "(", "ind_sc", ")", "\n", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n", "# dump super_class assignment and centroids of super_class", "\n", "if", "not", "args", ".", "rank", ":", "\n", "            ", "pickle", ".", "dump", "(", "\n", "super_class_assignements", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'super_class_assignments.pkl'", ")", ",", "'wb'", ")", ",", "\n", ")", "\n", "pickle", ".", "dump", "(", "\n", "centroids_sc", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'super_class_centroids.pkl'", ")", ",", "'wb'", ")", ",", "\n", ")", "\n", "\n", "# size of the different super clusters", "\n", "", "", "all_counts", "=", "[", "torch", ".", "zeros", "(", "args", ".", "nmb_super_clusters", ")", ".", "cuda", "(", ")", "for", "_", "in", "range", "(", "args", ".", "world_size", ")", "]", "\n", "dist", ".", "all_gather", "(", "all_counts", ",", "nmb_data_per_super_cluster", ")", "\n", "all_counts", "=", "torch", ".", "cat", "(", "all_counts", ")", ".", "cpu", "(", ")", ".", "long", "(", ")", "\n", "all_counts", "=", "all_counts", ".", "reshape", "(", "args", ".", "world_size", ",", "args", ".", "nmb_super_clusters", ")", "\n", "logger", ".", "info", "(", "all_counts", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "\n", "# what are the data belonging to this super class", "\n", "dataset", ".", "subset_indexes", "=", "np", ".", "where", "(", "super_class_assignements", "==", "args", ".", "clustering_local_world_id", ")", "[", "0", "]", "\n", "div", "=", "args", ".", "batch_size", "*", "args", ".", "clustering_local_world_size", "\n", "dataset", ".", "subset_indexes", "=", "dataset", ".", "subset_indexes", "[", ":", "len", "(", "dataset", ")", "//", "div", "*", "div", "]", "\n", "\n", "dist", ".", "barrier", "(", ")", "\n", "\n", "# which files this process is going to read", "\n", "local_nmb_data", "=", "int", "(", "len", "(", "dataset", ")", "/", "args", ".", "clustering_local_world_size", ")", "\n", "low", "=", "np", ".", "long", "(", "args", ".", "clustering_local_rank", "*", "local_nmb_data", ")", "\n", "high", "=", "np", ".", "long", "(", "low", "+", "local_nmb_data", ")", "\n", "curr_ind", "=", "0", "\n", "cache", "=", "torch", ".", "zeros", "(", "local_nmb_data", ",", "args", ".", "dim_pca", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "cumsum", "=", "torch", ".", "cumsum", "(", "all_counts", "[", ":", ",", "args", ".", "clustering_local_world_id", "]", ".", "long", "(", ")", ",", "0", ")", ".", "long", "(", ")", "\n", "for", "r", "in", "range", "(", "args", ".", "world_size", ")", ":", "\n", "# data in this bucket r: [cumsum[r - 1] : cumsum[r] - 1]", "\n", "        ", "low_bucket", "=", "np", ".", "long", "(", "cumsum", "[", "r", "-", "1", "]", ")", "if", "r", "else", "0", "\n", "\n", "# this bucket is empty", "\n", "if", "low_bucket", ">", "cumsum", "[", "r", "]", "-", "1", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "cumsum", "[", "r", "]", "-", "1", "<", "low", ":", "\n", "            ", "continue", "\n", "", "if", "low_bucket", ">=", "high", ":", "\n", "            ", "break", "\n", "\n", "# which are the data we are interested in inside this bucket ?", "\n", "", "ind_low", "=", "np", ".", "long", "(", "max", "(", "low", ",", "low_bucket", ")", ")", "\n", "ind_high", "=", "np", ".", "long", "(", "min", "(", "high", ",", "cumsum", "[", "r", "]", ")", ")", "\n", "\n", "cache_r", "=", "np", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'cache/'", ",", "'super_class'", "+", "str", "(", "args", ".", "clustering_local_world_id", ")", "+", "'-'", "+", "str", "(", "r", ")", ")", ",", "'rb'", ")", ")", "\n", "cache", "[", "curr_ind", ":", "curr_ind", "+", "ind_high", "-", "ind_low", "]", "=", "torch", ".", "FloatTensor", "(", "cache_r", "[", "ind_low", "-", "low_bucket", ":", "ind_high", "-", "low_bucket", "]", ")", "\n", "\n", "curr_ind", "+=", "(", "ind_high", "-", "ind_low", ")", "\n", "\n", "# randomly pick some centroids and dump them", "\n", "", "centroids_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'centroids'", "+", "str", "(", "args", ".", "clustering_local_world_id", ")", "+", "'.pkl'", ")", "\n", "if", "not", "args", ".", "clustering_local_rank", ":", "\n", "        ", "centroids", "=", "cache", "[", "np", ".", "random", ".", "choice", "(", "\n", "np", ".", "arange", "(", "cache", ".", "shape", "[", "0", "]", ")", ",", "\n", "replace", "=", "cache", ".", "shape", "[", "0", "]", "<", "args", ".", "k", "//", "args", ".", "nmb_super_clusters", ",", "\n", "size", "=", "args", ".", "k", "//", "args", ".", "nmb_super_clusters", ",", "\n", ")", "]", "\n", "pickle", ".", "dump", "(", "centroids", ",", "open", "(", "centroids_path", ",", "'wb'", ")", ",", "-", "1", ")", "\n", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n", "# read centroids", "\n", "centroids", "=", "pickle", ".", "load", "(", "open", "(", "centroids_path", ",", "'rb'", ")", ")", ".", "cuda", "(", ")", "\n", "\n", "# distributed kmeans into sub-classes", "\n", "cluster_assignments", ",", "centroids", "=", "distributed_kmeans", "(", "\n", "args", ",", "\n", "len", "(", "dataset", ")", ",", "\n", "args", ".", "k", "//", "args", ".", "nmb_super_clusters", ",", "\n", "cache", ",", "\n", "args", ".", "clustering_local_rank", ",", "\n", "args", ".", "clustering_local_world_size", ",", "\n", "centroids", ",", "\n", "world_id", "=", "args", ".", "clustering_local_world_id", ",", "\n", "group", "=", "groups", "[", "args", ".", "clustering_local_world_id", "]", ",", "\n", ")", "\n", "\n", "# free RAM", "\n", "del", "cache", "\n", "\n", "# write cluster assignments and centroids", "\n", "if", "not", "args", ".", "clustering_local_rank", ":", "\n", "        ", "pickle", ".", "dump", "(", "\n", "cluster_assignments", ",", "\n", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'cluster_assignments'", "+", "str", "(", "args", ".", "clustering_local_world_id", ")", "+", "'.pkl'", ")", ",", "'wb'", ")", ",", "\n", ")", "\n", "pickle", ".", "dump", "(", "\n", "centroids", ",", "\n", "open", "(", "centroids_path", ",", "'wb'", ")", ",", "\n", ")", "\n", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n", "return", "cluster_assignments", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.clustering.load_cluster_assignments": [[212, 234], ["os.path.join", "os.path.join", "os.path.isfile", "os.path.isfile", "pickle.load", "logger.info", "pickle.load", "open", "numpy.where", "open", "str", "len"], "function", ["None"], ["", "", "def", "load_cluster_assignments", "(", "args", ",", "dataset", ")", ":", "\n", "    ", "\"\"\"\n    Load cluster assignments if they are present in experiment repository.\n    \"\"\"", "\n", "super_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'super_class_assignments.pkl'", ")", "\n", "sub_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "dump_path", ",", "\n", "'sub_class_assignments'", "+", "str", "(", "args", ".", "clustering_local_world_id", ")", "+", "'.pkl'", ",", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "super_file", ")", "and", "os", ".", "path", ".", "isfile", "(", "sub_file", ")", ":", "\n", "        ", "super_class_assignments", "=", "pickle", ".", "load", "(", "open", "(", "super_file", ",", "'rb'", ")", ")", "\n", "dataset", ".", "subset_indexes", "=", "np", ".", "where", "(", "super_class_assignments", "==", "args", ".", "clustering_local_world_id", ")", "[", "0", "]", "\n", "\n", "div", "=", "args", ".", "batch_size", "*", "args", ".", "clustering_local_world_size", "\n", "clustering_size_dataset", "=", "len", "(", "dataset", ")", "//", "div", "*", "div", "\n", "dataset", ".", "subset_indexes", "=", "dataset", ".", "subset_indexes", "[", ":", "clustering_size_dataset", "]", "\n", "\n", "logger", ".", "info", "(", "'Found cluster assignments in experiment repository'", ")", "\n", "return", "pickle", ".", "load", "(", "open", "(", "sub_file", ",", "\"rb\"", ")", ")", "\n", "\n", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.DistUnifTargSampler.__init__": [[30, 74], ["numpy.random.seed", "numpy.unique", "int", "int", "numpy.zeros", "utils.get_indices_sparse", "enumerate", "epoch_indexes.astype.astype.astype", "numpy.random.shuffle", "len", "int", "numpy.asarray", "int", "numpy.random.choice", "len"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.get_indices_sparse"], ["def", "__init__", "(", "self", ",", "total_size", ",", "pseudo_labels", ",", "num_replicas", ",", "rank", ",", "seed", "=", "31", ")", ":", "\n", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "# world size", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "\n", "# rank of this process", "\n", "self", ".", "rank", "=", "rank", "\n", "\n", "# how many data to be loaded by the corpus of processes", "\n", "self", ".", "total_size", "=", "total_size", "\n", "\n", "# set of labels to consider", "\n", "set_of_pseudo_labels", "=", "np", ".", "unique", "(", "pseudo_labels", ")", "\n", "nmb_pseudo_lab", "=", "int", "(", "len", "(", "set_of_pseudo_labels", ")", ")", "\n", "\n", "# number of images per label", "\n", "per_label", "=", "int", "(", "self", ".", "total_size", "//", "nmb_pseudo_lab", "+", "1", ")", "\n", "\n", "# initialize indexes", "\n", "epoch_indexes", "=", "np", ".", "zeros", "(", "int", "(", "per_label", "*", "nmb_pseudo_lab", ")", ")", "\n", "\n", "# select a number of per_label data for each label", "\n", "indexes", "=", "get_indices_sparse", "(", "np", ".", "asarray", "(", "pseudo_labels", ")", ")", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "set_of_pseudo_labels", ")", ":", "\n", "            ", "k", "=", "int", "(", "k", ")", "\n", "label_indexes", "=", "indexes", "[", "k", "]", "[", "0", "]", "\n", "epoch_indexes", "[", "i", "*", "per_label", ":", "(", "i", "+", "1", ")", "*", "per_label", "]", "=", "np", ".", "random", ".", "choice", "(", "\n", "label_indexes", ",", "\n", "per_label", ",", "\n", "replace", "=", "(", "len", "(", "label_indexes", ")", "<=", "per_label", ")", "\n", ")", "\n", "\n", "# make sure indexes are integers", "\n", "", "epoch_indexes", "=", "epoch_indexes", ".", "astype", "(", "int", ")", "\n", "\n", "# shuffle the indexes", "\n", "np", ".", "random", ".", "shuffle", "(", "epoch_indexes", ")", "\n", "\n", "self", ".", "epoch_indexes", "=", "epoch_indexes", "[", ":", "self", ".", "total_size", "]", "\n", "\n", "# this process only deals with this subset", "\n", "self", ".", "process_ind", "=", "self", ".", "epoch_indexes", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.DistUnifTargSampler.__iter__": [[75, 77], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "process_ind", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.DistUnifTargSampler.__len__": [[78, 80], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "process_ind", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.train_network": [[82, 225], ["trainer.DistUnifTargSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "time.perf_counter", "torch.CrossEntropyLoss().cuda", "torch.nn.ReLU().cuda", "torch.nn.ReLU().cuda", "torch.nn.ReLU().cuda", "enumerate", "trainer.save_checkpoint", "model.train", "utils.AverageMeter.update", "relu.cuda", "target.cuda().long.cuda().long", "torch.nn.ReLU().cuda.", "nn.CrossEntropyLoss().cuda.", "nn.CrossEntropyLoss().cuda.", "loss.backward", "utils.AverageMeter.update", "trainer.accuracy", "utils.AverageMeter.update", "utils.AverageMeter.update", "trainer.accuracy", "utils.AverageMeter.update", "utils.AverageMeter.update", "trainer.accuracy", "utils.AverageMeter.update", "utils.AverageMeter.update", "time.perf_counter", "torch.CrossEntropyLoss", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "optimizer.zero_grad", "optimizer.step", "trainer.save_checkpoint", "trainer.save_checkpoint", "loss.item", "output.size", "accuracy.item", "output.size", "cel.item", "output.size", "accuracy.item", "output.size", "cel.item", "output.size", "accuracy.item", "output.size", "logger.info", "shutil.copyfile", "len", "time.perf_counter", "target.cuda().long.cuda", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "src.slurm.trigger_job_requeue", "time.perf_counter", "os.path.join", "os.path.join", "os.path.join", "len", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.save_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.None.eval_voc_classif.train", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.save_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.save_checkpoint", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.slurm.trigger_job_requeue"], ["", "", "def", "train_network", "(", "args", ",", "models", ",", "optimizers", ",", "dataset", ")", ":", "\n", "    ", "\"\"\"\n    Train the models with cluster assignments as targets\n    \"\"\"", "\n", "# swith to train mode", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "\n", "# uniform sampling over pseudo labels", "\n", "", "sampler", "=", "DistUnifTargSampler", "(", "\n", "args", ".", "epoch_size", ",", "\n", "dataset", ".", "sub_classes", ",", "\n", "args", ".", "training_local_world_size", ",", "\n", "args", ".", "training_local_rank", ",", "\n", "seed", "=", "args", ".", "epoch", "+", "args", ".", "training_local_world_id", ",", "\n", ")", "\n", "\n", "loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "sampler", "=", "sampler", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "pin_memory", "=", "True", ",", "\n", ")", "\n", "\n", "# running statistics", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "\n", "# training statistics", "\n", "log_top1_subclass", "=", "AverageMeter", "(", ")", "\n", "log_loss_subclass", "=", "AverageMeter", "(", ")", "\n", "log_top1_superclass", "=", "AverageMeter", "(", ")", "\n", "log_loss_superclass", "=", "AverageMeter", "(", ")", "\n", "\n", "log_top1", "=", "AverageMeter", "(", ")", "\n", "log_loss", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "cel", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "relu", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "for", "iter_epoch", ",", "(", "inp", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "# start at iter start_iter", "\n", "        ", "if", "iter_epoch", "<", "args", ".", "start_iter", ":", "\n", "            ", "continue", "\n", "\n", "# measure data loading time", "\n", "", "data_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "\n", "# move input to gpu", "\n", "inp", "=", "inp", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "cuda", "(", "non_blocking", "=", "True", ")", ".", "long", "(", ")", "\n", "\n", "# forward on the model", "\n", "inp", "=", "relu", "(", "models", "[", "0", "]", "(", "inp", ")", ")", "\n", "\n", "# forward on sub-class prediction layer", "\n", "output", "=", "models", "[", "-", "1", "]", "(", "inp", ")", "\n", "loss_subclass", "=", "cel", "(", "output", ",", "target", ")", "\n", "\n", "# forward on super-class prediction layer", "\n", "super_class_output", "=", "models", "[", "1", "]", "(", "inp", ")", "\n", "sc_target", "=", "args", ".", "training_local_world_id", "+", "0", "*", "torch", ".", "cuda", ".", "LongTensor", "(", "args", ".", "batch_size", ")", "\n", "loss_superclass", "=", "cel", "(", "super_class_output", ",", "sc_target", ")", "\n", "\n", "loss", "=", "loss_subclass", "+", "loss_superclass", "\n", "\n", "# initialize the optimizers", "\n", "for", "optimizer", "in", "optimizers", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# compute the gradients", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "# step", "\n", "for", "optimizer", "in", "optimizers", ":", "\n", "            ", "optimizer", ".", "step", "(", ")", "\n", "\n", "# log", "\n", "\n", "# signal received, relaunch experiment", "\n", "", "if", "os", ".", "environ", "[", "'SIGNAL_RECEIVED'", "]", "==", "'True'", ":", "\n", "            ", "save_checkpoint", "(", "args", ",", "iter_epoch", "+", "1", ",", "models", ",", "optimizers", ")", "\n", "if", "not", "args", ".", "rank", ":", "\n", "                ", "trigger_job_requeue", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ")", "\n", "\n", "# regular checkpoints", "\n", "", "", "if", "iter_epoch", "and", "iter_epoch", "%", "1000", "==", "0", ":", "\n", "            ", "save_checkpoint", "(", "args", ",", "iter_epoch", "+", "1", ",", "models", ",", "optimizers", ")", "\n", "\n", "# update stats", "\n", "", "log_loss", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "prec1", "=", "accuracy", "(", "args", ",", "output", ",", "target", ",", "sc_output", "=", "super_class_output", ")", "\n", "log_top1", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "log_loss_superclass", ".", "update", "(", "loss_superclass", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "prec1", "=", "accuracy", "(", "args", ",", "super_class_output", ",", "sc_target", ")", "\n", "log_top1_superclass", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "log_loss_subclass", ".", "update", "(", "loss_subclass", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "prec1", "=", "accuracy", "(", "args", ",", "output", ",", "target", ")", "\n", "log_top1_subclass", ".", "update", "(", "prec1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "batch_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "# verbose", "\n", "if", "iter_epoch", "%", "100", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Epoch[{0}] - Iter: [{1}/{2}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'", "\n", "'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'", "\n", "'Prec {log_top1.val:.3f} ({log_top1.avg:.3f})\\t'", "\n", "'Super-class loss: {sc_loss.val:.3f} ({sc_loss.avg:.3f})\\t'", "\n", "'Super-class prec: {sc_prec.val:.3f} ({sc_prec.avg:.3f})\\t'", "\n", "'Intra super-class loss: {los.val:.3f} ({los.avg:.3f})\\t'", "\n", "'Intra super-class prec: {prec.val:.3f} ({prec.avg:.3f})\\t'", "\n", ".", "format", "(", "args", ".", "epoch", ",", "iter_epoch", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "data_time", "=", "data_time", ",", "loss", "=", "log_loss", ",", "log_top1", "=", "log_top1", ",", "\n", "sc_loss", "=", "log_loss_superclass", ",", "sc_prec", "=", "log_top1_superclass", ",", "\n", "los", "=", "log_loss_subclass", ",", "prec", "=", "log_top1_subclass", ")", ")", "\n", "\n", "# end of epoch", "\n", "", "", "args", ".", "start_iter", "=", "0", "\n", "args", ".", "epoch", "+=", "1", "\n", "\n", "# dump checkpoint", "\n", "save_checkpoint", "(", "args", ",", "0", ",", "models", ",", "optimizers", ")", "\n", "if", "not", "args", ".", "rank", ":", "\n", "        ", "if", "not", "(", "args", ".", "epoch", "-", "1", ")", "%", "args", ".", "checkpoint_freq", ":", "\n", "            ", "shutil", ".", "copyfile", "(", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "dump_checkpoints", ",", "\n", "'checkpoint'", "+", "str", "(", "args", ".", "epoch", "-", "1", ")", "+", "'.pth.tar'", ")", ",", "\n", ")", "\n", "\n", "", "", "return", "(", "args", ".", "epoch", "-", "1", ",", "\n", "args", ".", "epoch", "*", "len", "(", "loader", ")", ",", "\n", "log_top1", ".", "avg", ",", "log_loss", ".", "avg", ",", "\n", "log_top1_superclass", ".", "avg", ",", "log_loss_superclass", ".", "avg", ",", "\n", "log_top1_subclass", ".", "avg", ",", "log_loss_subclass", ".", "avg", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.save_checkpoint": [[228, 251], ["os.path.isfile", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "models[].state_dict", "optimizers[].state_dict", "models[].state_dict", "optimizers[].state_dict", "models[].state_dict", "optimizers[].state_dict", "str"], "function", ["None"], ["", "def", "save_checkpoint", "(", "args", ",", "iter_epoch", ",", "models", ",", "optimizers", ",", "path", "=", "''", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isfile", "(", "path", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", "\n", "\n", "# main process saves the training state", "\n", "", "if", "not", "args", ".", "rank", ":", "\n", "        ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "iter_epoch", ",", "\n", "'state_dict'", ":", "models", "[", "0", "]", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizers", "[", "0", "]", ".", "state_dict", "(", ")", ",", "\n", "'pred_layer_state_dict'", ":", "models", "[", "1", "]", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_pred_layer'", ":", "optimizers", "[", "1", "]", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "path", ")", "\n", "\n", "# main local training process saves the last layer", "\n", "", "if", "not", "args", ".", "training_local_rank", ":", "\n", "        ", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "args", ".", "epoch", ",", "\n", "'start_iter'", ":", "iter_epoch", ",", "\n", "'state_dict'", ":", "models", "[", "-", "1", "]", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizers", "[", "-", "1", "]", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "str", "(", "args", ".", "training_local_world_id", ")", "+", "'-pred_layer.pth.tar'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy": [[253, 272], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "correct[].view().float().sum", "correct[].view().float().sum.mul_", "target.view().expand_as", "sc_output.topk", "pred.t.t", "pred.t.eq", "target.view().expand_as", "correct[].view().float", "target.view", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "target.view", "correct[].view"], "function", ["None"], ["", "", "def", "accuracy", "(", "args", ",", "output", ",", "target", ",", "sc_output", "=", "None", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "1", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "\n", "if", "sc_output", "is", "not", "None", ":", "\n", "            ", "_", ",", "pred", "=", "sc_output", ".", "topk", "(", "1", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "target", "=", "args", ".", "training_local_world_id", "+", "0", "*", "torch", ".", "cuda", ".", "LongTensor", "(", "batch_size", ")", "\n", "correct_sc", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "correct", "*=", "correct_sc", "\n", "\n", "", "correct_1", "=", "correct", "[", ":", "1", "]", ".", "view", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "return", "correct_1", ".", "mul_", "(", "100.0", "/", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.validate_network": [[274, 317], ["utils.AverageMeter", "utils.AverageMeter", "utils.AverageMeter", "torch.CrossEntropyLoss().cuda", "model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "time.perf_counter", "enumerate", "utils.AverageMeter.avg.item", "torch.CrossEntropyLoss", "inp.cuda.cuda", "target.cuda.cuda", "nn.CrossEntropyLoss().cuda.", "trainer.accuracy", "utils.AverageMeter.update", "utils.AverageMeter.update", "utils.AverageMeter.update", "time.perf_counter", "model", "criterion.item", "inp.cuda.size", "inp.cuda.size", "logger.info", "time.perf_counter", "len"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.trainer.accuracy", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update"], ["", "", "def", "validate_network", "(", "val_loader", ",", "models", ",", "args", ")", ":", "\n", "    ", "batch_time", "=", "AverageMeter", "(", ")", "\n", "losses", "=", "AverageMeter", "(", ")", "\n", "top1", "=", "AverageMeter", "(", ")", "\n", "\n", "# switch to evaluate mode", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "for", "i", ",", "(", "inp", ",", "target", ")", "in", "enumerate", "(", "val_loader", ")", ":", "\n", "\n", "# move to gpu", "\n", "            ", "inp", "=", "inp", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "# compute output", "\n", "output", "=", "inp", "\n", "for", "model", "in", "models", ":", "\n", "                ", "output", "=", "model", "(", "output", ")", "\n", "", "loss", "=", "criterion", "(", "output", ",", "target", ")", "\n", "\n", "# measure accuracy and record loss", "\n", "acc1", "=", "accuracy", "(", "args", ",", "output", ",", "target", ")", "\n", "losses", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "inp", ".", "size", "(", "0", ")", ")", "\n", "top1", ".", "update", "(", "acc1", "[", "0", "]", ",", "inp", ".", "size", "(", "0", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "perf_counter", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "if", "i", "%", "100", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Test: [{0}/{1}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'", "\n", "'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'", "\n", ".", "format", "(", "i", ",", "len", "(", "val_loader", ")", ",", "batch_time", "=", "batch_time", ",", "\n", "loss", "=", "losses", ",", "top1", "=", "top1", ")", ")", "\n", "\n", "", "", "", "return", "(", "top1", ".", "avg", ".", "item", "(", ")", ",", "losses", ".", "avg", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.LogFormatter.__init__": [[16, 18], ["time.time"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.LogFormatter.format": [[19, 30], ["round", "record.getMessage", "message.replace.replace.replace", "time.strftime", "datetime.timedelta", "len"], "methods", ["None"], ["", "def", "format", "(", "self", ",", "record", ")", ":", "\n", "        ", "elapsed_seconds", "=", "round", "(", "record", ".", "created", "-", "self", ".", "start_time", ")", "\n", "\n", "prefix", "=", "\"%s - %s - %s\"", "%", "(", "\n", "record", ".", "levelname", ",", "\n", "time", ".", "strftime", "(", "'%x %X'", ")", ",", "\n", "timedelta", "(", "seconds", "=", "elapsed_seconds", ")", "\n", ")", "\n", "message", "=", "record", ".", "getMessage", "(", ")", "\n", "message", "=", "message", ".", "replace", "(", "'\\n'", ",", "'\\n'", "+", "' '", "*", "(", "len", "(", "prefix", ")", "+", "3", ")", ")", "\n", "return", "\"%s - %s\"", "%", "(", "prefix", ",", "message", ")", "if", "message", "else", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.PD_Stats.__init__": [[74, 86], ["os.path.isfile", "pandas.read_pickle", "pandas.DataFrame", "list", "list"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "path", ",", "columns", ")", ":", "\n", "        ", "self", ".", "path", "=", "path", "\n", "\n", "# reload path stats", "\n", "if", "os", ".", "path", ".", "isfile", "(", "self", ".", "path", ")", ":", "\n", "            ", "self", ".", "stats", "=", "pd", ".", "read_pickle", "(", "self", ".", "path", ")", "\n", "\n", "# check that columns are the same", "\n", "assert", "list", "(", "self", ".", "stats", ".", "columns", ")", "==", "list", "(", "columns", ")", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "stats", "=", "pd", ".", "DataFrame", "(", "columns", "=", "columns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.PD_Stats.update": [[87, 93], ["logger.PD_Stats.stats.to_pickle", "len"], "methods", ["None"], ["", "", "def", "update", "(", "self", ",", "row", ",", "save", "=", "True", ")", ":", "\n", "        ", "self", ".", "stats", ".", "loc", "[", "len", "(", "self", ".", "stats", ".", "index", ")", "]", "=", "row", "\n", "\n", "# save the statistics", "\n", "if", "save", ":", "\n", "            ", "self", ".", "stats", ".", "to_pickle", "(", "self", ".", "path", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.create_logger": [[32, 68], ["logger.LogFormatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.StreamHandler.setFormatter", "logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.addHandler", "logging.FileHandler", "logging.FileHandler.setLevel", "logging.FileHandler.setFormatter", "logging.getLogger.addHandler", "time.time"], "function", ["None"], ["", "", "def", "create_logger", "(", "filepath", ",", "rank", ")", ":", "\n", "    ", "\"\"\"\n    Create a logger.\n    Use a different log file for each process.\n    \"\"\"", "\n", "# create log formatter", "\n", "log_formatter", "=", "LogFormatter", "(", ")", "\n", "\n", "# create file handler and set level to debug", "\n", "if", "filepath", "is", "not", "None", ":", "\n", "        ", "if", "rank", ">", "0", ":", "\n", "            ", "filepath", "=", "'%s-%i'", "%", "(", "filepath", ",", "rank", ")", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "filepath", ",", "\"a\"", ")", "\n", "file_handler", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "file_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create console handler and set level to info", "\n", "", "console_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console_handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "console_handler", ".", "setFormatter", "(", "log_formatter", ")", "\n", "\n", "# create logger and set level to debug", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "handlers", "=", "[", "]", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "if", "filepath", "is", "not", "None", ":", "\n", "        ", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "", "logger", ".", "addHandler", "(", "console_handler", ")", "\n", "\n", "# reset logger elapsed time", "\n", "def", "reset_time", "(", ")", ":", "\n", "        ", "log_formatter", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "", "logger", ".", "reset_time", "=", "reset_time", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.distributed_kmeans.initialize_cache": [[24, 168], ["int", "logger.info", "utils.AverageMeter", "utils.AverageMeter", "time.time", "min", "len", "len", "logger.warning", "torch.no_grad", "torch.no_grad", "enumerate", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "len", "utils.AverageMeter.update", "input_tensor.type().cuda.type().cuda", "model", "utils.AverageMeter.update", "time.time", "torch.all_gather", "utils.normalize.cpu", "os.path.join", "os.path.join", "torch.barrier", "pickle.load", "pickle.load", "utils.PCA.apply", "utils.normalize", "torch.zeros.size", "torch.cat", "torch.cat", "logger.info", "utils.PCA.apply", "utils.normalize", "utils.normalize.cpu", "logger.info", "time.time", "input_tensor.type().cuda.type", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cat().cpu().numpy", "torch.cat().cpu().numpy", "torch.zeros", "torch.zeros", "utils.PCA", "numpy.mean().astype", "utils.PCA.train_pca", "utils.PCA.apply", "utils.normalize", "pickle.dump", "pickle.dump", "open", "open", "time.time", "utils.normalize.size", "range", "numpy.zeros", "utils.normalize.size", "numpy.dot", "open", "open", "torch.zeros", "torch.zeros", "len", "torch.cat().cpu", "torch.cat().cpu", "numpy.mean", "numpy.random.choice", "torch.cat", "torch.cat", "numpy.arange"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.apply", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.normalize", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.apply", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.normalize", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.train_pca", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.apply", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.normalize"], ["def", "initialize_cache", "(", "args", ",", "loader", ",", "model", ")", ":", "\n", "    ", "\"\"\"\n    Accumulate features to compute pca.\n    Cache the dataset.\n    \"\"\"", "\n", "# we limit the size of the cache per process", "\n", "local_cache_size", "=", "min", "(", "len", "(", "loader", ")", ",", "3150000", "//", "args", ".", "batch_size", ")", "*", "args", ".", "batch_size", "\n", "\n", "# total batch_size", "\n", "batch_size", "=", "args", ".", "batch_size", "*", "args", ".", "world_size", "\n", "\n", "# how many batches do we need to approximate the covariance matrix", "\n", "N", "=", "model", ".", "module", ".", "body", ".", "dim_output_space", "\n", "nmb_batches_for_pca", "=", "int", "(", "N", "*", "(", "N", "-", "1", ")", "/", "2", "/", "args", ".", "batch_size", "/", "args", ".", "world_size", ")", "\n", "logger", ".", "info", "(", "\"Require {} images ({} iterations) for pca\"", ".", "format", "(", "\n", "nmb_batches_for_pca", "*", "args", ".", "batch_size", "*", "args", ".", "world_size", ",", "nmb_batches_for_pca", ")", ")", "\n", "if", "nmb_batches_for_pca", ">", "len", "(", "loader", ")", ":", "\n", "        ", "nmb_batches_for_pca", "=", "len", "(", "loader", ")", "\n", "logger", ".", "warning", "(", "\"Compute the PCA on {} images (entire dataset)\"", ".", "format", "(", "args", ".", "size_dataset", ")", ")", "\n", "\n", "# statistics", "\n", "", "batch_time", "=", "AverageMeter", "(", ")", "\n", "data_time", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "(", "input_tensor", ",", "_", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "\n", "# time spent to load data", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "\n", "# move to gpu", "\n", "input_tensor", "=", "input_tensor", ".", "type", "(", "torch", ".", "FloatTensor", ")", ".", "cuda", "(", ")", "\n", "\n", "# forward", "\n", "feat", "=", "model", "(", "input_tensor", ")", "\n", "\n", "# before the pca has been computed", "\n", "if", "i", "<", "nmb_batches_for_pca", ":", "\n", "\n", "# gather the features computed by all processes", "\n", "                ", "all_feat", "=", "[", "torch", ".", "cuda", ".", "FloatTensor", "(", "feat", ".", "size", "(", ")", ")", "for", "src", "in", "range", "(", "args", ".", "world_size", ")", "]", "\n", "dist", ".", "all_gather", "(", "all_feat", ",", "feat", ")", "\n", "\n", "# only main process computes the PCA", "\n", "if", "not", "args", ".", "rank", ":", "\n", "                    ", "all_feat", "=", "torch", ".", "cat", "(", "all_feat", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# initialize storage arrays", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "if", "not", "args", ".", "rank", ":", "\n", "                        ", "for_pca", "=", "np", ".", "zeros", "(", "\n", "(", "nmb_batches_for_pca", "*", "batch_size", ",", "all_feat", ".", "shape", "[", "1", "]", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ",", "\n", ")", "\n", "", "for_cache", "=", "torch", ".", "zeros", "(", "\n", "nmb_batches_for_pca", "*", "args", ".", "batch_size", ",", "\n", "feat", ".", "size", "(", "1", ")", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", ")", "\n", "\n", "# fill in arrays", "\n", "", "if", "not", "args", ".", "rank", ":", "\n", "                    ", "for_pca", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", "=", "all_feat", "\n", "\n", "", "for_cache", "[", "i", "*", "args", ".", "batch_size", ":", "(", "i", "+", "1", ")", "*", "args", ".", "batch_size", "]", "=", "feat", ".", "cpu", "(", ")", "\n", "\n", "# train the pca", "\n", "", "if", "i", "==", "nmb_batches_for_pca", "-", "1", ":", "\n", "                ", "pca_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'pca.pkl'", ")", "\n", "centroids_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'centroids.pkl'", ")", "\n", "\n", "# compute the PCA", "\n", "if", "not", "args", ".", "rank", ":", "\n", "# init PCA object", "\n", "                    ", "pca", "=", "PCA", "(", "dim", "=", "args", ".", "dim_pca", ",", "whit", "=", "0.5", ")", "\n", "\n", "# center data", "\n", "mean", "=", "np", ".", "mean", "(", "for_pca", ",", "axis", "=", "0", ")", ".", "astype", "(", "'float32'", ")", "\n", "for_pca", "-=", "mean", "\n", "\n", "# compute covariance", "\n", "cov", "=", "np", ".", "dot", "(", "for_pca", ".", "T", ",", "for_pca", ")", "/", "for_pca", ".", "shape", "[", "0", "]", "\n", "\n", "# calculate the pca", "\n", "pca", ".", "train_pca", "(", "cov", ")", "\n", "\n", "# randomly pick some centroids", "\n", "centroids", "=", "pca", ".", "apply", "(", "for_pca", "[", "np", ".", "random", ".", "choice", "(", "\n", "np", ".", "arange", "(", "for_pca", ".", "shape", "[", "0", "]", ")", ",", "\n", "replace", "=", "False", ",", "\n", "size", "=", "args", ".", "nmb_super_clusters", ",", "\n", ")", "]", ")", "\n", "centroids", "=", "normalize", "(", "centroids", ")", "\n", "\n", "pca", ".", "mean", "=", "mean", "\n", "\n", "# free memory", "\n", "del", "for_pca", "\n", "\n", "# write PCA to disk", "\n", "pickle", ".", "dump", "(", "pca", ",", "open", "(", "pca_path", ",", "'wb'", ")", ")", "\n", "pickle", ".", "dump", "(", "centroids", ",", "open", "(", "centroids_path", ",", "'wb'", ")", ")", "\n", "\n", "# processes wait that main process compute and write PCA and centroids", "\n", "", "dist", ".", "barrier", "(", ")", "\n", "\n", "# processes read PCA and centroids from disk", "\n", "pca", "=", "pickle", ".", "load", "(", "open", "(", "pca_path", ",", "\"rb\"", ")", ")", "\n", "centroids", "=", "pickle", ".", "load", "(", "open", "(", "centroids_path", ",", "\"rb\"", ")", ")", "\n", "\n", "# apply the pca to the cached features", "\n", "for_cache", "=", "pca", ".", "apply", "(", "for_cache", ")", "\n", "for_cache", "=", "normalize", "(", "for_cache", ")", "\n", "\n", "# extend the cache", "\n", "current_cache_size", "=", "for_cache", ".", "size", "(", "0", ")", "\n", "for_cache", "=", "torch", ".", "cat", "(", "(", "for_cache", ",", "torch", ".", "zeros", "(", "\n", "local_cache_size", "-", "current_cache_size", ",", "\n", "args", ".", "dim_pca", ",", "\n", ")", ")", ")", "\n", "logger", ".", "info", "(", "'{0} imgs cached => cache is {1:.2f} % full'", "\n", ".", "format", "(", "current_cache_size", ",", "100", "*", "current_cache_size", "/", "local_cache_size", ")", ")", "\n", "\n", "# keep accumulating data", "\n", "", "if", "i", ">", "nmb_batches_for_pca", "-", "1", ":", "\n", "                ", "feat", "=", "pca", ".", "apply", "(", "feat", ")", "\n", "feat", "=", "normalize", "(", "feat", ")", "\n", "for_cache", "[", "i", "*", "args", ".", "batch_size", ":", "(", "i", "+", "1", ")", "*", "args", ".", "batch_size", "]", "=", "feat", ".", "cpu", "(", ")", "\n", "\n", "\n", "# verbose", "\n", "", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "if", "i", "%", "200", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'{0} / {1}\\t'", "\n", "'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", "'Data Time: {data_time.val:.3f} ({data_time.avg:.3f})\\t'", "\n", ".", "format", "(", "i", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ",", "data_time", "=", "data_time", ")", ")", "\n", "\n", "# move centroids to GPU", "\n", "", "", "centroids", "=", "torch", ".", "cuda", ".", "FloatTensor", "(", "centroids", ")", "\n", "\n", "return", "for_cache", ",", "centroids", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.distributed_kmeans.distributed_kmeans": [[170, 319], ["time.time", "range", "os.path.join", "pickle.dump", "numpy.zeros", "range", "torch.barrier", "os.remove", "numpy.ones", "faiss.StandardGpuResources", "faiss.GpuIndexFlatConfig", "faiss.GpuIndexFlatL2", "time.time", "utils.AverageMeter", "utils.AverageMeter", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "range", "torch.zeros().cuda.nonzero", "logger.info", "open", "torch.barrier", "torch.barrier", "pickle.load", "centroids.cpu", "faiss.GpuIndexFlatL2.reset", "faiss.GpuIndexFlatL2.add", "utils.AverageMeter.update", "numpy.unique", "utils.AverageMeter.update", "time.time", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "numpy.random.seed", "range", "k.cpu", "logger.info", "open", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "centroids.cpu().numpy().astype", "centroids.norm", "faiss.GpuIndexFlatL2.search", "I.cpu().numpy.squeeze", "l2dist.min", "I.cpu().numpy.cpu().numpy", "D.cpu().numpy.cpu().numpy", "D.cpu().numpy.mean", "len", "[].sum", "logger.info", "str", "torch.arange().int", "torch.arange().int", "os.path.join", "feat.numpy().astype", "numpy.where", "time.time", "local_counts[].unsqueeze", "numpy.random.randint", "time.time", "centroids.cpu().numpy", "I.cpu().numpy.cpu", "D.cpu().numpy.cpu", "len", "str", "torch.arange", "torch.arange", "feat.numpy", "torch.mm", "torch.mm", "feat.cuda", "numpy.where", "int", "str", "centroids.cpu", "feat.cuda", "centroids.transpose", "m.item", "str"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.reset", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update"], ["", "", "def", "distributed_kmeans", "(", "args", ",", "n_all", ",", "nk", ",", "cache", ",", "rank", ",", "world_size", ",", "centroids", ",", "world_id", "=", "0", ",", "group", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Distributed mini-batch k-means.\n    \"\"\"", "\n", "# local assignments", "\n", "assignments", "=", "-", "1", "*", "np", ".", "ones", "(", "n_all", "//", "world_size", ")", "\n", "\n", "# prepare faiss index", "\n", "if", "args", ".", "use_faiss", ":", "\n", "        ", "res", "=", "faiss", ".", "StandardGpuResources", "(", ")", "\n", "cfg", "=", "faiss", ".", "GpuIndexFlatConfig", "(", ")", "\n", "cfg", ".", "device", "=", "args", ".", "gpu_to_work_on", "\n", "index", "=", "faiss", ".", "GpuIndexFlatL2", "(", "res", ",", "args", ".", "dim_pca", ",", "cfg", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "for", "p", "in", "range", "(", "args", ".", "niter", "+", "1", ")", ":", "\n", "        ", "start_pass", "=", "time", ".", "time", "(", ")", "\n", "\n", "# running statistics", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "log_loss", "=", "AverageMeter", "(", ")", "\n", "\n", "# initialize arrays for update", "\n", "local_counts", "=", "torch", ".", "zeros", "(", "nk", ")", ".", "cuda", "(", ")", "\n", "local_feats", "=", "torch", ".", "zeros", "(", "nk", ",", "args", ".", "dim_pca", ")", ".", "cuda", "(", ")", "\n", "\n", "# prepare E step", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "if", "args", ".", "use_faiss", ":", "\n", "            ", "index", ".", "reset", "(", ")", "\n", "index", ".", "add", "(", "centroids", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "astype", "(", "'float32'", ")", ")", "\n", "", "else", ":", "\n", "            ", "centroids_L2_norm", "=", "centroids", ".", "norm", "(", "dim", "=", "1", ")", "**", "2", "\n", "\n", "", "nmb_batches", "=", "n_all", "//", "world_size", "//", "args", ".", "batch_size", "\n", "for", "it", "in", "range", "(", "nmb_batches", ")", ":", "\n", "\n", "# fetch mini-batch", "\n", "            ", "feat", "=", "cache", "[", "it", "*", "args", ".", "batch_size", ":", "(", "it", "+", "1", ")", "*", "args", ".", "batch_size", "]", "\n", "\n", "# E-step", "\n", "if", "args", ".", "use_faiss", ":", "\n", "                ", "D", ",", "I", "=", "index", ".", "search", "(", "feat", ".", "numpy", "(", ")", ".", "astype", "(", "'float32'", ")", ",", "1", ")", "\n", "I", "=", "I", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# find current cluster assignments", "\n", "                ", "l2dist", "=", "1", "-", "2", "*", "torch", ".", "mm", "(", "feat", ".", "cuda", "(", "non_blocking", "=", "True", ")", ",", "centroids", ".", "transpose", "(", "0", ",", "1", ")", ")", "+", "centroids_L2_norm", "\n", "D", ",", "I", "=", "l2dist", ".", "min", "(", "dim", "=", "1", ")", "\n", "I", "=", "I", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "D", "=", "D", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# update assignment array", "\n", "", "assignments", "[", "it", "*", "args", ".", "batch_size", ":", "(", "it", "+", "1", ")", "*", "args", ".", "batch_size", "]", "=", "I", "\n", "\n", "# log", "\n", "log_loss", ".", "update", "(", "D", ".", "mean", "(", ")", ")", "\n", "\n", "for", "k", "in", "np", ".", "unique", "(", "I", ")", ":", "\n", "                ", "idx_k", "=", "np", ".", "where", "(", "I", "==", "k", ")", "[", "0", "]", "\n", "# number of elmt in cluster k for this batch", "\n", "local_counts", "[", "k", "]", "+=", "len", "(", "idx_k", ")", "\n", "\n", "# sum of elmt belonging to this cluster", "\n", "local_feats", "[", "k", ",", ":", "]", "+=", "feat", ".", "cuda", "(", "non_blocking", "=", "True", ")", "[", "idx_k", "]", ".", "sum", "(", "dim", "=", "0", ")", "\n", "\n", "", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "it", "and", "it", "%", "1000", "==", "0", ":", "\n", "                ", "logger", ".", "info", "(", "'Pass[{0}] - Iter: [{1}/{2}]\\t'", "\n", "'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'", "\n", ".", "format", "(", "p", ",", "it", ",", "nmb_batches", ",", "batch_time", "=", "batch_time", ")", ")", "\n", "\n", "# all reduce operation", "\n", "# processes share what it is needed for M-step", "\n", "", "", "if", "group", "is", "not", "None", ":", "\n", "            ", "dist", ".", "all_reduce", "(", "local_counts", ",", "group", "=", "group", ")", "\n", "dist", ".", "all_reduce", "(", "local_feats", ",", "group", "=", "group", ")", "\n", "", "else", ":", "\n", "            ", "dist", ".", "all_reduce", "(", "local_counts", ")", "\n", "dist", ".", "all_reduce", "(", "local_feats", ")", "\n", "\n", "# M-step", "\n", "\n", "# update centroids (for the last pass we only want the assignments)", "\n", "", "mask", "=", "local_counts", ".", "nonzero", "(", ")", "\n", "if", "p", "<", "args", ".", "niter", ":", "\n", "            ", "centroids", "[", "mask", "]", "=", "1.", "/", "local_counts", "[", "mask", "]", ".", "unsqueeze", "(", "1", ")", "*", "local_feats", "[", "mask", "]", "\n", "\n", "# deal with empty clusters", "\n", "", "for", "k", "in", "(", "local_counts", "==", "0", ")", ".", "nonzero", "(", ")", ":", "\n", "\n", "# choose a random cluster from the set of non empty clusters", "\n", "            ", "np", ".", "random", ".", "seed", "(", "world_id", ")", "\n", "m", "=", "mask", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "mask", ")", ")", "]", "\n", "\n", "# replace empty centroid by a non empty one with a perturbation", "\n", "centroids", "[", "k", "]", "=", "centroids", "[", "m", "]", "\n", "for", "j", "in", "range", "(", "args", ".", "dim_pca", ")", ":", "\n", "                ", "sign", "=", "(", "j", "%", "2", ")", "*", "2", "-", "1", ";", "\n", "centroids", "[", "k", ",", "j", "]", "+=", "sign", "*", "1e-7", ";", "\n", "centroids", "[", "m", ",", "j", "]", "-=", "sign", "*", "1e-7", ";", "\n", "\n", "# update the counts", "\n", "", "local_counts", "[", "k", "]", "=", "local_counts", "[", "m", "]", "//", "2", ";", "\n", "local_counts", "[", "m", "]", "-=", "local_counts", "[", "k", "]", ";", "\n", "\n", "# update the assignments", "\n", "assignments", "[", "np", ".", "where", "(", "assignments", "==", "m", ".", "item", "(", ")", ")", "[", "0", "]", "[", ":", "int", "(", "local_counts", "[", "m", "]", ")", "]", "]", "=", "k", ".", "cpu", "(", ")", "\n", "logger", ".", "info", "(", "'cluster {} empty => split cluster {}'", ".", "format", "(", "k", ",", "m", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "' # Pass[{0}]\\tTime {1:.3f}\\tLoss {2:.4f}'", "\n", ".", "format", "(", "p", ",", "time", ".", "time", "(", ")", "-", "start_pass", ",", "log_loss", ".", "avg", ")", ")", "\n", "\n", "# now each process needs to share its own set of pseudo_labels", "\n", "\n", "# where to write / read the pseudo_labels", "\n", "", "dump_labels", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "dump_path", ",", "\n", "'pseudo_labels'", "+", "str", "(", "world_id", ")", "+", "'-'", "+", "str", "(", "rank", ")", "+", "'.pkl'", ",", "\n", ")", "\n", "\n", "# log the cluster assignment", "\n", "pickle", ".", "dump", "(", "\n", "assignments", ",", "\n", "open", "(", "dump_labels", ",", "'wb'", ")", ",", "\n", "-", "1", ",", "\n", ")", "\n", "\n", "# process wait for all processes to finish writing", "\n", "if", "group", "is", "not", "None", ":", "\n", "        ", "dist", ".", "barrier", "(", "group", "=", "group", ")", "\n", "", "else", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "pseudo_labels", "=", "np", ".", "zeros", "(", "n_all", ")", "\n", "\n", "# process read and reconstitute the pseudo_labels", "\n", "local_nmb_data", "=", "n_all", "//", "world_size", "\n", "for", "r", "in", "range", "(", "world_size", ")", ":", "\n", "        ", "pseudo_labels", "[", "torch", ".", "arange", "(", "r", "*", "local_nmb_data", ",", "(", "r", "+", "1", ")", "*", "local_nmb_data", ")", ".", "int", "(", ")", "]", "=", "pickle", ".", "load", "(", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'pseudo_labels'", "+", "str", "(", "world_id", ")", "+", "'-'", "+", "str", "(", "r", ")", "+", "'.pkl'", ")", ",", "\"rb\"", ")", ")", "\n", "\n", "# clean", "\n", "", "del", "assignments", "\n", "dist", ".", "barrier", "(", ")", "\n", "os", ".", "remove", "(", "dump_labels", ")", "\n", "\n", "return", "pseudo_labels", ",", "centroids", ".", "cpu", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.__init__": [[267, 271], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dim", "=", "256", ",", "whit", "=", "0.5", ")", ":", "\n", "        ", "self", ".", "dim", "=", "dim", "\n", "self", ".", "whit", "=", "whit", "\n", "self", ".", "mean", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.train_pca": [[272, 297], ["numpy.linalg.eigh", "numpy.diag.sum", "logger.warning", "numpy.diag", "numpy.dot", "numpy.diag.max", "numpy.argsort", "numpy.diag.sum"], "methods", ["None"], ["", "def", "train_pca", "(", "self", ",", "cov", ")", ":", "\n", "        ", "\"\"\" \n        Takes a covariance matrix (np.ndarray) as input.\n        \"\"\"", "\n", "d", ",", "v", "=", "np", ".", "linalg", ".", "eigh", "(", "cov", ")", "\n", "eps", "=", "d", ".", "max", "(", ")", "*", "1e-5", "\n", "n_0", "=", "(", "d", "<", "eps", ")", ".", "sum", "(", ")", "\n", "if", "n_0", ">", "0", ":", "\n", "            ", "d", "[", "d", "<", "eps", "]", "=", "eps", "\n", "\n", "# total energy", "\n", "", "totenergy", "=", "d", ".", "sum", "(", ")", "\n", "\n", "# sort eigenvectors with eigenvalues order", "\n", "idx", "=", "np", ".", "argsort", "(", "d", ")", "[", ":", ":", "-", "1", "]", "[", ":", "self", ".", "dim", "]", "\n", "d", "=", "d", "[", "idx", "]", "\n", "v", "=", "v", "[", ":", ",", "idx", "]", "\n", "\n", "logger", ".", "warning", "(", "\"keeping %.2f %% of the energy\"", "%", "(", "d", ".", "sum", "(", ")", "/", "totenergy", "*", "100.0", ")", ")", "\n", "\n", "# for the whitening", "\n", "d", "=", "np", ".", "diag", "(", "1.", "/", "d", "**", "self", ".", "whit", ")", "\n", "\n", "# principal components", "\n", "self", ".", "dvt", "=", "np", ".", "dot", "(", "d", ",", "v", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.PCA.apply": [[298, 315], ["isinstance", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.mm().transpose", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "numpy.dot", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "x.transpose", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "x.transpose"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "x", ")", ":", "\n", "# input is from numpy", "\n", "        ", "if", "isinstance", "(", "x", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "if", "self", ".", "mean", "is", "not", "None", ":", "\n", "                ", "x", "-=", "self", ".", "mean", "\n", "", "return", "np", ".", "dot", "(", "self", ".", "dvt", ",", "x", ".", "T", ")", ".", "T", "\n", "\n", "# input is from torch and is on GPU", "\n", "", "if", "x", ".", "is_cuda", ":", "\n", "            ", "if", "self", ".", "mean", "is", "not", "None", ":", "\n", "                ", "x", "-=", "torch", ".", "cuda", ".", "FloatTensor", "(", "self", ".", "mean", ")", "\n", "", "return", "torch", ".", "mm", "(", "torch", ".", "cuda", ".", "FloatTensor", "(", "self", ".", "dvt", ")", ",", "x", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# input if from torch, on CPU", "\n", "", "if", "self", ".", "mean", "is", "not", "None", ":", "\n", "            ", "x", "-=", "torch", ".", "FloatTensor", "(", "self", ".", "mean", ")", "\n", "", "return", "torch", ".", "mm", "(", "torch", ".", "FloatTensor", "(", "self", ".", "dvt", ")", ",", "x", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.__init__": [[319, 321], ["utils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.reset": [[322, 327], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.AverageMeter.update": [[328, 333], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.bool_flag": [[30, 40], ["s.lower", "s.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["def", "bool_flag", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse boolean arguments from the command line.\n    \"\"\"", "\n", "if", "s", ".", "lower", "(", ")", "in", "FALSY_STRINGS", ":", "\n", "        ", "return", "False", "\n", "", "elif", "s", ".", "lower", "(", ")", "in", "TRUTHY_STRINGS", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "\"invalid value for a boolean flag\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.init_distributed_mode": [[42, 121], ["torch.init_process_group", "torch.cuda.set_device", "torch.cuda.set_device", "range", "range", "int", "int", "int", "torch.cuda.device_count", "torch.cuda.device_count", "training_groups.append", "clustering_groups.append", "torch.new_group", "torch.new_group", "range", "range"], "function", ["None"], ["", "", "def", "init_distributed_mode", "(", "args", ",", "make_communication_groups", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Handle single and multi-GPU / multi-node / SLURM jobs.\n    Initialize the following variables:\n        - global rank\n\n        - clustering_local_rank\n        - clustering_local_world_size\n        - clustering_local_world_id\n\n        - training_local_rank\n        - training_local_world_size\n        - training_local_world_id\n\n        - rotation\n    \"\"\"", "\n", "\n", "args", ".", "is_slurm_job", "=", "'SLURM_JOB_ID'", "in", "os", ".", "environ", "and", "not", "args", ".", "debug_slurm", "\n", "\n", "if", "args", ".", "is_slurm_job", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "", "else", ":", "\n", "# jobs started with torch.distributed.launch", "\n", "# read environment variables", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "'RANK'", "]", ")", "\n", "args", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "\n", "# prepare distributed", "\n", "", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "args", ".", "dist_url", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "rank", "=", "args", ".", "rank", ")", "\n", "\n", "# set cuda device", "\n", "args", ".", "gpu_to_work_on", "=", "args", ".", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu_to_work_on", ")", "\n", "\n", "if", "not", "make_communication_groups", ":", "\n", "        ", "return", "None", ",", "None", "\n", "\n", "# each super_class has the same number of processes", "\n", "", "assert", "args", ".", "world_size", "%", "args", ".", "super_classes", "==", "0", "\n", "\n", "# each super-class forms a training communication group", "\n", "args", ".", "training_local_world_size", "=", "args", ".", "world_size", "//", "args", ".", "super_classes", "\n", "args", ".", "training_local_rank", "=", "args", ".", "rank", "%", "args", ".", "training_local_world_size", "\n", "args", ".", "training_local_world_id", "=", "args", ".", "rank", "//", "args", ".", "training_local_world_size", "\n", "\n", "# prepare training groups", "\n", "training_groups", "=", "[", "]", "\n", "for", "group_id", "in", "range", "(", "args", ".", "super_classes", ")", ":", "\n", "        ", "ranks", "=", "[", "args", ".", "training_local_world_size", "*", "group_id", "+", "i", "for", "i", "in", "range", "(", "args", ".", "training_local_world_size", ")", "]", "\n", "training_groups", ".", "append", "(", "dist", ".", "new_group", "(", "ranks", "=", "ranks", ")", ")", "\n", "\n", "# compute number of super-clusters", "\n", "", "if", "args", ".", "rotnet", ":", "\n", "        ", "assert", "args", ".", "super_classes", "%", "4", "==", "0", "\n", "args", ".", "nmb_super_clusters", "=", "args", ".", "super_classes", "//", "4", "\n", "", "else", ":", "\n", "        ", "args", ".", "nmb_super_clusters", "=", "args", ".", "super_classes", "\n", "\n", "# prepare clustering communication groups", "\n", "", "args", ".", "clustering_local_world_size", "=", "args", ".", "training_local_world_size", "*", "(", "args", ".", "super_classes", "//", "args", ".", "nmb_super_clusters", ")", "\n", "args", ".", "clustering_local_rank", "=", "args", ".", "rank", "%", "args", ".", "clustering_local_world_size", "\n", "args", ".", "clustering_local_world_id", "=", "args", ".", "rank", "//", "args", ".", "clustering_local_world_size", "\n", "\n", "clustering_groups", "=", "[", "]", "\n", "for", "group_id", "in", "range", "(", "args", ".", "nmb_super_clusters", ")", ":", "\n", "        ", "ranks", "=", "[", "args", ".", "clustering_local_world_size", "*", "group_id", "+", "i", "for", "i", "in", "range", "(", "args", ".", "clustering_local_world_size", ")", "]", "\n", "clustering_groups", ".", "append", "(", "dist", ".", "new_group", "(", "ranks", "=", "ranks", ")", ")", "\n", "\n", "# this process deals only with a certain rotation", "\n", "", "if", "args", ".", "rotnet", ":", "\n", "        ", "args", ".", "rotation", "=", "args", ".", "clustering_local_rank", "//", "args", ".", "training_local_world_size", "\n", "", "else", ":", "\n", "        ", "args", ".", "rotation", "=", "0", "\n", "\n", "", "return", "training_groups", ",", "clustering_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.check_parameters": [[123, 144], ["min"], "function", ["None"], ["", "def", "check_parameters", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Check if corpus of arguments is consistent.\n    \"\"\"", "\n", "args", ".", "size_dataset", "=", "min", "(", "args", ".", "size_dataset", ",", "95920149", ")", "\n", "\n", "# make dataset size divisible by both the batch-size and the world-size", "\n", "div", "=", "args", ".", "batch_size", "*", "args", ".", "world_size", "\n", "args", ".", "size_dataset", "=", "args", ".", "size_dataset", "//", "div", "*", "div", "\n", "\n", "args", ".", "epoch_size", "=", "args", ".", "size_dataset", "//", "args", ".", "nmb_super_clusters", "//", "4", "\n", "args", ".", "epoch_size", "=", "args", ".", "epoch_size", "//", "div", "*", "div", "\n", "\n", "assert", "args", ".", "super_classes", "\n", "\n", "# number of super classes must be divisible by the number of rotation categories", "\n", "if", "args", ".", "rotnet", ":", "\n", "        ", "assert", "args", ".", "super_classes", "%", "4", "==", "0", "\n", "\n", "# feature dimension", "\n", "", "assert", "args", ".", "dim_pca", "<=", "4096", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.initialize_exp": [[146, 181], ["pickle.dump", "os.path.join", "logger.PD_Stats", "logger.create_logger", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "logger.create_logger.info", "open", "os.mkdir", "os.mkdir", "os.path.join", "os.path.join", "os.path.join", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "str", "sorted", "str", "dict().items", "dict", "vars"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.create_logger"], ["", "def", "initialize_exp", "(", "params", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"\n    Initialize the experience:\n    - dump parameters\n    - create checkpoint and cache repos\n    - create a logger\n    - create a panda object to log the training statistics\n    \"\"\"", "\n", "# dump parameters", "\n", "pickle", ".", "dump", "(", "params", ",", "open", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'params.pkl'", ")", ",", "'wb'", ")", ")", "\n", "\n", "# create repo to store checkpoints", "\n", "params", ".", "dump_checkpoints", "=", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'checkpoints'", ")", "\n", "if", "not", "params", ".", "rank", "and", "not", "os", ".", "path", ".", "isdir", "(", "params", ".", "dump_checkpoints", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "params", ".", "dump_checkpoints", ")", "\n", "\n", "# create repo to cache activations between the two stages of the hierarchical k-means", "\n", "", "if", "not", "params", ".", "rank", "and", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'cache'", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'cache'", ")", ")", "\n", "\n", "# create a panda object to log loss and acc", "\n", "", "training_stats", "=", "PD_Stats", "(", "\n", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'stats'", "+", "str", "(", "params", ".", "rank", ")", "+", "'.pkl'", ")", ",", "\n", "args", ",", "\n", ")", "\n", "\n", "# create a logger", "\n", "logger", "=", "create_logger", "(", "os", ".", "path", ".", "join", "(", "params", ".", "dump_path", ",", "'train.log'", ")", ",", "rank", "=", "params", ".", "rank", ")", "\n", "logger", ".", "info", "(", "\"============ Initialized logger ============\"", ")", "\n", "logger", ".", "info", "(", "\"\\n\"", ".", "join", "(", "\"%s: %s\"", "%", "(", "k", ",", "str", "(", "v", ")", ")", "\n", "for", "k", ",", "v", "in", "sorted", "(", "dict", "(", "vars", "(", "params", ")", ")", ".", "items", "(", ")", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"The experiment will be stored in %s\\n\"", "%", "params", ".", "dump_path", ")", "\n", "logger", ".", "info", "(", "\"\"", ")", "\n", "\n", "return", "logger", ",", "training_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.end_of_epoch": [[183, 214], ["os.path.join", "os.path.join", "utils.end_of_epoch.src_dst"], "function", ["None"], ["", "def", "end_of_epoch", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Remove cluster assignment from experiment repository\n    \"\"\"", "\n", "\n", "def", "src_dst", "(", "what", ",", "cl", "=", "False", ")", ":", "\n", "        ", "src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "dump_path", ",", "\n", "what", "+", "cl", "*", "str", "(", "args", ".", "clustering_local_world_id", ")", "+", "'.pkl'", ",", "\n", ")", "\n", "dst", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "dump_checkpoints", ",", "\n", "what", "+", "'{}-epoch{}.pkl'", ".", "format", "(", "cl", "*", "args", ".", "clustering_local_world_id", ",", "args", ".", "epoch", "-", "1", ")", ",", "\n", ")", "\n", "return", "src", ",", "dst", "\n", "\n", "# main processes only are working here", "\n", "", "if", "not", "args", ".", "clustering_local_rank", ":", "\n", "        ", "for", "what", "in", "[", "'cluster_assignments'", ",", "'centroids'", "]", ":", "\n", "            ", "src", ",", "dst", "=", "src_dst", "(", "what", ",", "cl", "=", "True", ")", "\n", "if", "not", "(", "args", ".", "epoch", "-", "1", ")", "%", "args", ".", "checkpoint_freq", ":", "\n", "                ", "shutil", ".", "copy", "(", "src", ",", "dst", ")", "\n", "", "if", "not", "'centroids'", "in", "src", ":", "\n", "                ", "os", ".", "remove", "(", "src", ")", "\n", "\n", "", "", "", "if", "not", "args", ".", "rank", ":", "\n", "        ", "for", "what", "in", "[", "'super_class_assignments'", ",", "'super_class_centroids'", "]", ":", "\n", "            ", "src", ",", "dst", "=", "src_dst", "(", "what", ")", "\n", "if", "not", "(", "args", ".", "epoch", "-", "1", ")", "%", "args", ".", "checkpoint_freq", ":", "\n", "                ", "shutil", ".", "copy", "(", "src", ",", "dst", ")", "\n", "", "os", ".", "remove", "(", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.restart_from_checkpoint": [[216, 252], ["logger.info", "torch.load", "torch.load", "kwargs.items", "os.path.join", "os.path.isfile", "str", "value.load_state_dict", "logger.info", "logger.warning"], "function", ["None"], ["", "", "", "def", "restart_from_checkpoint", "(", "args", ",", "ckp_path", "=", "None", ",", "run_variables", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Re-start from checkpoint present in experiment repo\n    \"\"\"", "\n", "if", "ckp_path", "is", "None", ":", "\n", "        ", "ckp_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "dump_path", ",", "'checkpoint.pth.tar'", ")", "\n", "\n", "# look for a checkpoint in exp repository", "\n", "", "if", "not", "os", ".", "path", ".", "isfile", "(", "ckp_path", ")", ":", "\n", "        ", "return", "\n", "\n", "", "logger", ".", "info", "(", "'Found checkpoint in experiment repository'", ")", "\n", "\n", "# open checkpoint file", "\n", "map_location", "=", "None", "\n", "if", "args", ".", "world_size", ">", "1", ":", "\n", "        ", "map_location", "=", "\"cuda:\"", "+", "str", "(", "args", ".", "gpu_to_work_on", ")", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "ckp_path", ",", "map_location", "=", "map_location", ")", "\n", "\n", "# key is what to look for in the checkpoint file", "\n", "# value is the object to load", "\n", "# example: {'state_dict': model}", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", "in", "checkpoint", "and", "value", "is", "not", "None", ":", "\n", "            ", "value", ".", "load_state_dict", "(", "checkpoint", "[", "key", "]", ")", "\n", "logger", ".", "info", "(", "\"=> loaded {} from checkpoint '{}'\"", "\n", ".", "format", "(", "key", ",", "ckp_path", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "\"=> failed to load {} from checkpoint '{}'\"", "\n", ".", "format", "(", "key", ",", "ckp_path", ")", ")", "\n", "\n", "# re load variable important for the run", "\n", "", "", "if", "run_variables", "is", "not", "None", ":", "\n", "        ", "for", "var_name", "in", "run_variables", ":", "\n", "            ", "if", "var_name", "in", "checkpoint", ":", "\n", "                ", "run_variables", "[", "var_name", "]", "=", "checkpoint", "[", "var_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.fix_random_seeds": [[254, 261], ["torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed"], "function", ["None"], ["", "", "", "", "def", "fix_random_seeds", "(", "seed", "=", "1993", ")", ":", "\n", "    ", "\"\"\"\n    Fix random seeds.\n    \"\"\"", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.normalize": [[335, 346], ["isinstance", "data.norm", "numpy.linalg.norm"], "function", ["None"], ["", "", "def", "normalize", "(", "data", ")", ":", "\n", "# data in numpy array", "\n", "    ", "if", "isinstance", "(", "data", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "row_sums", "=", "np", ".", "linalg", ".", "norm", "(", "data", ",", "axis", "=", "1", ")", "\n", "data", "=", "data", "/", "row_sums", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "return", "data", "\n", "\n", "# data is a tensor", "\n", "", "row_sums", "=", "data", ".", "norm", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "data", "=", "data", "/", "row_sums", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.compute_M": [[348, 352], ["numpy.arange", "scipy.sparse.csr_matrix", "data.ravel", "data.max"], "function", ["None"], ["", "def", "compute_M", "(", "data", ")", ":", "\n", "    ", "cols", "=", "np", ".", "arange", "(", "data", ".", "size", ")", "\n", "return", "csr_matrix", "(", "(", "cols", ",", "(", "data", ".", "ravel", "(", ")", ",", "cols", ")", ")", ",", "\n", "shape", "=", "(", "data", ".", "max", "(", ")", "+", "1", ",", "data", ".", "size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.get_indices_sparse": [[353, 356], ["utils.compute_M", "numpy.unravel_index"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.utils.compute_M"], ["", "def", "get_indices_sparse", "(", "data", ")", ":", "\n", "    ", "M", "=", "compute_M", "(", "data", ")", "\n", "return", "[", "np", ".", "unravel_index", "(", "row", ".", "data", ",", "data", ".", "shape", ")", "for", "row", "in", "M", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.vgg16.VGG16.__init__": [[22, 42], ["torch.Module.__init__", "vgg16.make_layers", "torch.Sequential", "torch.Sequential", "torch.Sequential", "vgg16.VGG16.modules", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "classifier.append", "isinstance", "torch.ReLU", "torch.ReLU", "torch.ReLU", "m.weight.data.normal_", "m.bias.data.zero_", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFold.__init__", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.vgg16.make_layers"], ["def", "__init__", "(", "self", ",", "dim_in", ",", "relu", "=", "True", ",", "dropout", "=", "0.5", ",", "batch_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "VGG16", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "features", "=", "make_layers", "(", "cfg", "[", "'D'", "]", ",", "dim_in", ",", "batch_norm", "=", "batch_norm", ")", "\n", "self", ".", "dim_output_space", "=", "4096", "\n", "classifier", "=", "[", "\n", "nn", ".", "Linear", "(", "512", "*", "7", "*", "7", ",", "4096", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "4096", ",", "4096", ")", ",", "\n", "]", "\n", "if", "relu", ":", "\n", "            ", "classifier", ".", "append", "(", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "*", "classifier", ")", "\n", "\n", "# Initialize weights", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "n", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.", "/", "n", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.vgg16.VGG16.forward": [[43, 49], ["vgg16.VGG16.features", "vgg16.VGG16.view", "vgg16.VGG16.classifier", "vgg16.VGG16.size"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "if", "self", ".", "classifier", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.vgg16.make_layers": [[51, 64], ["torch.Sequential", "torch.Conv2d", "torch.MaxPool2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU"], "function", ["None"], ["", "", "def", "make_layers", "(", "cfg", ",", "in_channels", ",", "batch_norm", "=", "True", ")", ":", "\n", "    ", "layers", "=", "[", "]", "\n", "for", "v", "in", "cfg", ":", "\n", "        ", "if", "v", "==", "'M'", ":", "\n", "            ", "layers", "+=", "[", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "2", ",", "stride", "=", "2", ")", "]", "\n", "", "else", ":", "\n", "            ", "conv2d", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "v", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "if", "batch_norm", ":", "\n", "                ", "layers", "+=", "[", "conv2d", ",", "nn", ".", "BatchNorm2d", "(", "v", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "", "else", ":", "\n", "                ", "layers", "+=", "[", "conv2d", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "]", "\n", "", "in_channels", "=", "v", "\n", "", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.Net.__init__": [[39, 55], ["torch.Module.__init__", "model_factory.create_sobel_layer"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFold.__init__", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.create_sobel_layer"], ["    ", "def", "__init__", "(", "self", ",", "padding", ",", "sobel", ",", "body", ",", "pred_layer", ")", ":", "\n", "        ", "super", "(", "Net", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# padding", "\n", "self", ".", "padding", "=", "padding", "\n", "\n", "# sobel filter", "\n", "self", ".", "sobel", "=", "create_sobel_layer", "(", ")", "if", "sobel", "else", "None", "\n", "\n", "# main architecture", "\n", "self", ".", "body", "=", "body", "\n", "\n", "# prediction layer", "\n", "self", ".", "pred_layer", "=", "pred_layer", "\n", "\n", "self", ".", "conv", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.Net.forward": [[56, 76], ["model_factory.Net.body", "model_factory.Net.padding", "model_factory.Net.sobel", "model_factory.Net.body.features.modules", "model_factory.Net.pred_layer", "isinstance", "isinstance", "m"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "padding", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "padding", "(", "x", ")", "\n", "", "if", "self", ".", "sobel", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "sobel", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "conv", "is", "not", "None", ":", "\n", "            ", "count", "=", "1", "\n", "for", "m", "in", "self", ".", "body", ".", "features", ".", "modules", "(", ")", ":", "\n", "                ", "if", "not", "isinstance", "(", "m", ",", "nn", ".", "Sequential", ")", ":", "\n", "                    ", "x", "=", "m", "(", "x", ")", "\n", "", "if", "isinstance", "(", "m", ",", "nn", ".", "ReLU", ")", ":", "\n", "                    ", "if", "count", "==", "self", ".", "conv", ":", "\n", "                        ", "return", "x", "\n", "", "count", "=", "count", "+", "1", "\n", "\n", "", "", "", "x", "=", "self", ".", "body", "(", "x", ")", "\n", "if", "self", ".", "pred_layer", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "pred_layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.create_sobel_layer": [[20, 36], ["torch.Conv2d", "nn.Conv2d.weight.data.fill_", "nn.Conv2d.bias.data.zero_", "torch.Conv2d", "nn.Conv2d.weight.data[].copy_", "nn.Conv2d.weight.data[].copy_", "nn.Conv2d.bias.data.zero_", "torch.Sequential", "nn.Sequential.parameters", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["def", "create_sobel_layer", "(", ")", ":", "\n", "    ", "grayscale", "=", "nn", ".", "Conv2d", "(", "3", ",", "1", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "grayscale", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", "/", "3.0", ")", "\n", "grayscale", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "sobel_filter", "=", "nn", ".", "Conv2d", "(", "1", ",", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "sobel_filter", ".", "weight", ".", "data", "[", "0", ",", "0", "]", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "[", "[", "1", ",", "0", ",", "-", "1", "]", ",", "[", "2", ",", "0", ",", "-", "2", "]", ",", "[", "1", ",", "0", ",", "-", "1", "]", "]", ")", "\n", ")", "\n", "sobel_filter", ".", "weight", ".", "data", "[", "1", ",", "0", "]", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "[", "[", "1", ",", "2", ",", "1", "]", ",", "[", "0", ",", "0", ",", "0", "]", ",", "[", "-", "1", ",", "-", "2", ",", "-", "1", "]", "]", ")", "\n", ")", "\n", "sobel_filter", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "sobel", "=", "nn", ".", "Sequential", "(", "grayscale", ",", "sobel_filter", ")", "\n", "for", "p", "in", "sobel", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "", "return", "sobel", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.model_factory": [[78, 92], ["torch.ConstantPad2d", "vgg16.VGG16", "model_factory.Net", "torch.ConstantPad2d", "torch.Linear"], "function", ["None"], ["", "", "def", "model_factory", "(", "sobel", ",", "relu", "=", "False", ",", "num_classes", "=", "0", ",", "batch_norm", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Create a network.\n    \"\"\"", "\n", "dim_in", "=", "2", "if", "sobel", "else", "3", "\n", "\n", "padding", "=", "nn", ".", "ConstantPad2d", "(", "1", ",", "0.0", ")", "\n", "if", "sobel", ":", "\n", "        ", "padding", "=", "nn", ".", "ConstantPad2d", "(", "2", ",", "0.0", ")", "\n", "", "body", "=", "VGG16", "(", "dim_in", ",", "relu", "=", "relu", ",", "batch_norm", "=", "batch_norm", ")", "\n", "\n", "pred_layer", "=", "nn", ".", "Linear", "(", "body", ".", "dim_output_space", ",", "num_classes", ")", "if", "num_classes", "else", "None", "\n", "\n", "return", "Net", "(", "padding", ",", "sobel", ",", "body", ",", "pred_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.build_prediction_layer": [[94, 112], ["torch.Linear", "model_factory.to_cuda", "model_factory.sgd_optimizer"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sgd_optimizer"], ["", "def", "build_prediction_layer", "(", "dim_in", ",", "args", ",", "group", "=", "None", ",", "num_classes", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Create prediction layer on gpu and its associated optimizer.\n    \"\"\"", "\n", "\n", "if", "not", "num_classes", ":", "\n", "        ", "num_classes", "=", "args", ".", "super_classes", "\n", "\n", "# last fully connected layer", "\n", "", "pred_layer", "=", "nn", ".", "Linear", "(", "dim_in", ",", "num_classes", ")", "\n", "\n", "# move prediction layer to gpu", "\n", "pred_layer", "=", "to_cuda", "(", "pred_layer", ",", "args", ".", "gpu_to_work_on", ",", "group", "=", "group", ")", "\n", "\n", "# set optimizer for the prediction layer", "\n", "optimizer_pred_layer", "=", "sgd_optimizer", "(", "pred_layer", ",", "args", ".", "lr", ",", "args", ".", "wd", ")", "\n", "\n", "return", "pred_layer", ",", "optimizer_pred_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.to_cuda": [[114, 126], ["nn.parallel.DistributedDataParallel.cuda", "DDP", "torch.parallel.DistributedDataParallel"], "function", ["None"], ["", "def", "to_cuda", "(", "net", ",", "gpu_id", ",", "apex", "=", "False", ",", "group", "=", "None", ")", ":", "\n", "    ", "net", "=", "net", ".", "cuda", "(", ")", "\n", "if", "apex", ":", "\n", "        ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "net", "=", "DDP", "(", "net", ",", "delay_allreduce", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "net", "=", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "net", ",", "\n", "device_ids", "=", "[", "gpu_id", "]", ",", "\n", "process_group", "=", "group", ",", "\n", ")", "\n", "", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sgd_optimizer": [[128, 134], ["torch.optim.SGD", "torch.optim.SGD", "torch.optim.SGD", "filter", "module.parameters"], "function", ["None"], ["", "def", "sgd_optimizer", "(", "module", ",", "lr", ",", "wd", ")", ":", "\n", "    ", "return", "torch", ".", "optim", ".", "SGD", "(", "\n", "filter", "(", "lambda", "x", ":", "x", ".", "requires_grad", ",", "module", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "momentum", "=", "0.9", ",", "\n", "weight_decay", "=", "wd", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.model_factory.sobel2RGB": [[137, 175], ["list", "torch.Conv2d", "range", "nn.Conv2d.bias.data.copy_", "torch.Sequential", "net.body.features.children", "range", "nn.Conv2d.weight.data[].copy_", "computeweight().expand().transpose", "computeweight().expand", "model_factory.sobel2RGB.computeweight"], "function", ["None"], ["", "def", "sobel2RGB", "(", "net", ")", ":", "\n", "    ", "if", "net", ".", "sobel", "is", "None", ":", "\n", "        ", "return", "\n", "\n", "", "def", "computeweight", "(", "conv", ",", "alist", ",", "blist", ")", ":", "\n", "        ", "sob", "=", "net", ".", "sobel", ".", "_modules", "[", "'1'", "]", ".", "weight", "\n", "res", "=", "0", "\n", "for", "atup", "in", "alist", ":", "\n", "            ", "for", "btup", "in", "blist", ":", "\n", "                ", "x", "=", "conv", "[", ":", ",", "0", ",", "atup", "[", "0", "]", ",", "btup", "[", "0", "]", "]", "*", "sob", "[", "0", ",", ":", ",", "atup", "[", "1", "]", ",", "btup", "[", "1", "]", "]", "\n", "y", "=", "conv", "[", ":", ",", "1", ",", "atup", "[", "0", "]", ",", "btup", "[", "0", "]", "]", "*", "sob", "[", "1", ",", ":", ",", "atup", "[", "1", "]", ",", "btup", "[", "1", "]", "]", "\n", "res", "=", "res", "+", "x", "+", "y", "\n", "", "", "return", "res", "\n", "\n", "", "def", "aux", "(", "a", ")", ":", "\n", "        ", "if", "a", "==", "0", ":", "\n", "            ", "return", "[", "(", "0", ",", "0", ")", "]", "\n", "", "elif", "a", "==", "1", ":", "\n", "            ", "return", "[", "(", "1", ",", "0", ")", ",", "(", "0", ",", "1", ")", "]", "\n", "", "elif", "a", "==", "2", ":", "\n", "            ", "return", "[", "(", "2", ",", "0", ")", ",", "(", "1", ",", "1", ")", ",", "(", "0", ",", "2", ")", "]", "\n", "", "elif", "a", "==", "3", ":", "\n", "            ", "return", "[", "(", "2", ",", "1", ")", ",", "(", "1", ",", "2", ")", "]", "\n", "", "elif", "a", "==", "4", ":", "\n", "            ", "return", "[", "(", "2", ",", "2", ")", "]", "\n", "\n", "", "", "features", "=", "list", "(", "net", ".", "body", ".", "features", ".", "children", "(", ")", ")", "\n", "conv_old", "=", "features", "[", "0", "]", "\n", "conv_final", "=", "nn", ".", "Conv2d", "(", "3", ",", "64", ",", "kernel_size", "=", "5", ",", "padding", "=", "1", ",", "bias", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "conv_old", ".", "kernel_size", "[", "0", "]", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "conv_old", ".", "kernel_size", "[", "0", "]", ")", ":", "\n", "            ", "neweight", "=", "1", "/", "3", "*", "computeweight", "(", "conv_old", ".", "weight", ",", "aux", "(", "i", ")", ",", "aux", "(", "j", ")", ")", ".", "expand", "(", "3", ",", "64", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "conv_final", ".", "weight", ".", "data", "[", ":", ",", ":", ",", "i", ",", "j", "]", ".", "copy_", "(", "neweight", ")", "\n", "", "", "conv_final", ".", "bias", ".", "data", ".", "copy_", "(", "conv_old", ".", "bias", ".", "data", ")", "\n", "features", "[", "0", "]", "=", "conv_final", "\n", "net", ".", "body", ".", "features", "=", "nn", ".", "Sequential", "(", "*", "features", ")", "\n", "net", ".", "sobel", "=", "None", "\n", "return", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.load_pretrained": [[22, 56], ["torch.load", "torch.load", "model.body.load_state_dict", "logger.info", "os.path.isfile", "logger.info", "pretrain.rename_key", "str", "checkpoint[].items"], "function", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.rename_key"], ["def", "load_pretrained", "(", "model", ",", "args", ")", ":", "\n", "    ", "\"\"\"\n    Load weights\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "args", ".", "pretrained", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'pretrained weights not found'", ")", "\n", "return", "\n", "\n", "# open checkpoint file", "\n", "", "map_location", "=", "None", "\n", "if", "args", ".", "world_size", ">", "1", ":", "\n", "        ", "map_location", "=", "\"cuda:\"", "+", "str", "(", "args", ".", "gpu_to_work_on", ")", "\n", "", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "pretrained", ",", "map_location", "=", "map_location", ")", "\n", "\n", "# clean keys from 'module'", "\n", "checkpoint", "[", "'state_dict'", "]", "=", "{", "rename_key", "(", "key", ")", ":", "val", "\n", "for", "key", ",", "val", "\n", "in", "checkpoint", "[", "'state_dict'", "]", ".", "items", "(", ")", "}", "\n", "\n", "# remove sobel keys", "\n", "if", "'sobel.0.weight'", "in", "checkpoint", "[", "'state_dict'", "]", ":", "\n", "        ", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'sobel.0.weight'", "]", "\n", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'sobel.0.bias'", "]", "\n", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'sobel.1.weight'", "]", "\n", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'sobel.1.bias'", "]", "\n", "\n", "# remove pred_layer keys", "\n", "", "if", "'pred_layer.weight'", "in", "checkpoint", "[", "'state_dict'", "]", ":", "\n", "        ", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'pred_layer.weight'", "]", "\n", "del", "checkpoint", "[", "'state_dict'", "]", "[", "'pred_layer.bias'", "]", "\n", "\n", "# load weights", "\n", "", "model", ".", "body", ".", "load_state_dict", "(", "checkpoint", "[", "'state_dict'", "]", ")", "\n", "logger", ".", "info", "(", "\"=> loaded pretrained weights from '{}'\"", ".", "format", "(", "args", ".", "pretrained", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.model.pretrain.rename_key": [[58, 67], ["key.startswith", "key.startswith", "key.split"], "function", ["None"], ["", "def", "rename_key", "(", "key", ")", ":", "\n", "    ", "\"Remove module from key\"", "\n", "if", "not", "'module'", "in", "key", ":", "\n", "        ", "return", "key", "\n", "", "if", "key", ".", "startswith", "(", "'module.body.'", ")", ":", "\n", "        ", "return", "key", "[", "12", ":", "]", "\n", "", "if", "key", ".", "startswith", "(", "'module.'", ")", ":", "\n", "        ", "return", "key", "[", "7", ":", "]", "\n", "", "return", "''", ".", "join", "(", "key", ".", "split", "(", "'.module'", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.VOC2007.VOC2007_dataset.__init__": [[20, 41], ["os.path.join", "glob.glob", "len", "collections.defaultdict", "enumerate", "numpy.random.shuffle", "os.path.join", "len", "sorted", "open", "l.strip().split", "int", "collections.defaultdict.keys", "numpy.ones", "l.strip", "os.path.join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "voc_dir", ",", "split", "=", "'train'", ",", "transform", "=", "None", ")", ":", "\n", "# Find the image sets", "\n", "        ", "image_set_dir", "=", "os", ".", "path", ".", "join", "(", "voc_dir", ",", "'ImageSets'", ",", "'Main'", ")", "\n", "image_sets", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "image_set_dir", ",", "'*_'", "+", "split", "+", "'.txt'", ")", ")", "\n", "assert", "len", "(", "image_sets", ")", "==", "20", "\n", "# Read the labels", "\n", "self", ".", "n_labels", "=", "len", "(", "image_sets", ")", "\n", "images", "=", "defaultdict", "(", "lambda", ":", "-", "np", ".", "ones", "(", "self", ".", "n_labels", ",", "dtype", "=", "np", ".", "uint8", ")", ")", "\n", "for", "k", ",", "s", "in", "enumerate", "(", "sorted", "(", "image_sets", ")", ")", ":", "\n", "            ", "for", "l", "in", "open", "(", "s", ",", "'r'", ")", ":", "\n", "                ", "name", ",", "lbl", "=", "l", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "lbl", "=", "int", "(", "lbl", ")", "\n", "# Switch the ignore label and 0 label (in VOC -1: not present, 0: ignore)", "\n", "if", "lbl", "<", "0", ":", "\n", "                    ", "lbl", "=", "0", "\n", "", "elif", "lbl", "==", "0", ":", "\n", "                    ", "lbl", "=", "255", "\n", "", "images", "[", "os", ".", "path", ".", "join", "(", "voc_dir", ",", "'JPEGImages'", ",", "name", "+", "'.jpg'", ")", "]", "[", "k", "]", "=", "lbl", "\n", "", "", "self", ".", "images", "=", "[", "(", "k", ",", "images", "[", "k", "]", ")", "for", "k", "in", "images", ".", "keys", "(", ")", "]", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "images", ")", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.VOC2007.VOC2007_dataset.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "images", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.VOC2007.VOC2007_dataset.__getitem__": [[45, 51], ["PIL.Image.open", "VOC2007.VOC2007_dataset.convert", "VOC2007.VOC2007_dataset.transform"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "self", ".", "images", "[", "i", "]", "[", "0", "]", ")", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "return", "img", ",", "self", ".", "images", "[", "i", "]", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.YFCC100M.YFCC100M_dataset.__init__": [[31, 45], ["numpy.load", "numpy.arange", "os.path.join", "os.path.dirname", "min", "os.path.realpath", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "root", ",", "size", ",", "flickr_unique_ids", "=", "True", ",", "transform", "=", "None", ")", ":", "\n", "        ", "self", ".", "root", "=", "root", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "sub_classes", "=", "None", "\n", "\n", "# remove data with uniform color and data we didn't manage to download", "\n", "if", "flickr_unique_ids", ":", "\n", "            ", "self", ".", "indexes", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "realpath", "(", "__file__", ")", ")", ",", "'flickr_unique_ids.npy'", ")", ")", "\n", "self", ".", "indexes", "=", "self", ".", "indexes", "[", ":", "min", "(", "size", ",", "len", "(", "self", ".", "indexes", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "indexes", "=", "np", ".", "arange", "(", "size", ")", "\n", "\n", "# for subsets", "\n", "", "self", ".", "subset_indexes", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.YFCC100M.YFCC100M_dataset.__getitem__": [[46, 72], ["format", "YFCC100M.loader", "os.path.join", "YFCC100M.YFCC100M_dataset.transform"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.src.logger.LogFormatter.format", "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.YFCC100M.loader"], ["", "def", "__getitem__", "(", "self", ",", "ind", ")", ":", "\n", "        ", "index", "=", "ind", "\n", "if", "self", ".", "subset_indexes", "is", "not", "None", ":", "\n", "            ", "index", "=", "self", ".", "subset_indexes", "[", "ind", "]", "\n", "", "index", "=", "self", ".", "indexes", "[", "index", "]", "\n", "\n", "index", "=", "format", "(", "index", ",", "\"0>8d\"", ")", "\n", "repo", "=", "index", "[", ":", "2", "]", "\n", "z", "=", "index", "[", "2", ":", "5", "]", "\n", "file_img", "=", "index", "[", "5", ":", "]", "+", "'.jpg'", "\n", "\n", "path_zip", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root", ",", "repo", ",", "z", ")", "+", "'.zip'", "\n", "\n", "# load the image", "\n", "img", "=", "loader", "(", "path_zip", ",", "file_img", ")", "\n", "\n", "# apply transformation", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "\n", "# id of cluster", "\n", "", "sub_class", "=", "-", "100", "\n", "if", "self", ".", "sub_classes", "is", "not", "None", ":", "\n", "            ", "sub_class", "=", "self", ".", "sub_classes", "[", "ind", "]", "\n", "\n", "", "return", "img", ",", "sub_class", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.YFCC100M.YFCC100M_dataset.__len__": [[73, 77], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "subset_indexes", "is", "not", "None", ":", "\n", "            ", "return", "len", "(", "self", ".", "subset_indexes", ")", "\n", "", "return", "len", "(", "self", ".", "indexes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.YFCC100M.loader": [[18, 25], ["Image.open.convert", "zipfile.ZipFile", "PIL.Image.open", "myzip.open"], "function", ["None"], ["def", "loader", "(", "path_zip", ",", "file_img", ")", ":", "\n", "    ", "\"\"\"\n    Load imagefile from zip.\n    \"\"\"", "\n", "with", "zipfile", ".", "ZipFile", "(", "path_zip", ",", "'r'", ")", "as", "myzip", ":", "\n", "        ", "img", "=", "Image", ".", "open", "(", "myzip", ".", "open", "(", "file_img", ")", ")", "\n", "", "return", "img", ".", "convert", "(", "'RGB'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.Rotate.__init__": [[64, 66], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "rot", ")", ":", "\n", "        ", "self", ".", "rot", "=", "rot", "\n", "", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.Rotate.__call__": [[66, 68], ["loader.rotate_img"], "methods", ["home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.rotate_img"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "return", "rotate_img", "(", "img", ",", "self", ".", "rot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFoldSampler.__init__": [[84, 91], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "im_per_target", ",", "shuffle", ")", ":", "\n", "        ", "self", ".", "im_per_target", "=", "im_per_target", "\n", "N", "=", "0", "\n", "for", "tar", "in", "im_per_target", ":", "\n", "            ", "N", "=", "N", "+", "len", "(", "im_per_target", "[", "tar", "]", ")", "\n", "", "self", ".", "N", "=", "N", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFoldSampler.__iter__": [[92, 101], ["numpy.zeros().astype", "iter", "numpy.random.shuffle", "numpy.zeros", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "indices", "=", "np", ".", "zeros", "(", "self", ".", "N", ")", ".", "astype", "(", "int", ")", "\n", "c", "=", "0", "\n", "for", "tar", "in", "self", ".", "im_per_target", ":", "\n", "            ", "indices", "[", "c", ":", "c", "+", "len", "(", "self", ".", "im_per_target", "[", "tar", "]", ")", "]", "=", "self", ".", "im_per_target", "[", "tar", "]", "\n", "c", "=", "c", "+", "len", "(", "self", ".", "im_per_target", "[", "tar", "]", ")", "\n", "", "if", "self", ".", "shuffle", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "indices", ")", "\n", "", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFoldSampler.__len__": [[102, 104], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "N", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.KFold.__init__": [[116, 134], ["range", "loader.KFoldSampler", "loader.KFoldSampler", "int", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "im_per_target", ",", "i", ",", "K", ")", ":", "\n", "        ", "assert", "(", "i", "<", "K", ")", "\n", "per_target", "=", "{", "}", "\n", "for", "tar", "in", "im_per_target", ":", "\n", "            ", "per_target", "[", "tar", "]", "=", "int", "(", "len", "(", "im_per_target", "[", "tar", "]", ")", "//", "K", ")", "\n", "", "im_per_target_train", "=", "{", "}", "\n", "im_per_target_val", "=", "{", "}", "\n", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "            ", "for", "L", "in", "im_per_target", ":", "\n", "                ", "if", "k", "==", "i", ":", "\n", "                    ", "im_per_target_val", "[", "L", "]", "=", "im_per_target", "[", "L", "]", "[", "k", "*", "per_target", "[", "L", "]", ":", "(", "k", "+", "1", ")", "*", "per_target", "[", "L", "]", "]", "\n", "", "else", ":", "\n", "                    ", "if", "not", "L", "in", "im_per_target_train", ":", "\n", "                        ", "im_per_target_train", "[", "L", "]", "=", "[", "]", "\n", "", "im_per_target_train", "[", "L", "]", "=", "im_per_target_train", "[", "L", "]", "+", "im_per_target", "[", "L", "]", "[", "k", "*", "per_target", "[", "L", "]", ":", "(", "k", "+", "1", ")", "*", "per_target", "[", "L", "]", "]", "\n", "\n", "", "", "", "self", ".", "val", "=", "KFoldSampler", "(", "im_per_target_val", ",", "False", ")", "\n", "self", ".", "train", "=", "KFoldSampler", "(", "im_per_target_train", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.load_data": [[25, 32], ["torchvision.ImageFolder", "YFCC100M.YFCC100M_dataset"], "function", ["None"], ["def", "load_data", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    Load dataset.\n    \"\"\"", "\n", "if", "'yfcc100m'", "in", "args", ".", "data_path", ":", "\n", "        ", "return", "YFCC100M_dataset", "(", "args", ".", "data_path", ",", "size", "=", "args", ".", "size_dataset", ")", "\n", "", "return", "datasets", ".", "ImageFolder", "(", "args", ".", "data_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.get_data_transformations": [[34, 61], ["torchvision.Normalize", "torchvision.Compose", "torchvision.Compose", "torchvision.ToTensor", "torchvision.Resize", "torchvision.CenterCrop", "loader.Rotate", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "loader.Rotate", "numpy.asarray", "numpy.asarray"], "function", ["None"], ["", "def", "get_data_transformations", "(", "rotation", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n     Return data transformations for clustering and for training\n    \"\"\"", "\n", "tr_normalize", "=", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ",", "\n", ")", "\n", "final_process", "=", "[", "transforms", ".", "ToTensor", "(", ")", ",", "tr_normalize", "]", "\n", "\n", "# for clustering stage", "\n", "tr_central_crop", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "256", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "lambda", "x", ":", "np", ".", "asarray", "(", "x", ")", ",", "\n", "Rotate", "(", "0", ")", "\n", "]", "+", "final_process", ")", "\n", "\n", "# for training stage", "\n", "tr_dataug", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "lambda", "x", ":", "np", ".", "asarray", "(", "x", ")", ",", "\n", "Rotate", "(", "rotation", ")", "\n", "]", "+", "final_process", ")", "\n", "\n", "return", "tr_central_crop", ",", "tr_dataug", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.rotate_img": [[70, 81], ["numpy.flipud().copy", "numpy.fliplr().copy", "numpy.flipud", "numpy.transpose().copy", "numpy.transpose", "numpy.fliplr", "numpy.flipud", "numpy.transpose", "numpy.flipud"], "function", ["None"], ["", "", "def", "rotate_img", "(", "img", ",", "rot", ")", ":", "\n", "    ", "if", "rot", "==", "0", ":", "# 0 degrees rotation", "\n", "        ", "return", "img", "\n", "", "elif", "rot", "==", "90", ":", "# 90 degrees rotation", "\n", "        ", "return", "np", ".", "flipud", "(", "np", ".", "transpose", "(", "img", ",", "(", "1", ",", "0", ",", "2", ")", ")", ")", ".", "copy", "(", ")", "\n", "", "elif", "rot", "==", "180", ":", "# 90 degrees rotation", "\n", "        ", "return", "np", ".", "fliplr", "(", "np", ".", "flipud", "(", "img", ")", ")", ".", "copy", "(", ")", "\n", "", "elif", "rot", "==", "270", ":", "# 270 degrees rotation / or -90", "\n", "        ", "return", "np", ".", "transpose", "(", "np", ".", "flipud", "(", "img", ")", ",", "(", "1", ",", "0", ",", "2", ")", ")", ".", "copy", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.facebookresearch_DeeperCluster.data.loader.per_target": [[136, 150], ["range", "len", "res[].append"], "function", ["None"], ["", "", "def", "per_target", "(", "imgs", ")", ":", "\n", "    ", "\"\"\"Arrange samples per target.\n        Args:\n            imgs (list): List of (_, target) tuples.\n        Returns:\n            dict: key (target), value (list of data with this target)\n    \"\"\"", "\n", "res", "=", "{", "}", "\n", "for", "index", "in", "range", "(", "len", "(", "imgs", ")", ")", ":", "\n", "        ", "_", ",", "target", "=", "imgs", "[", "index", "]", "\n", "if", "target", "not", "in", "res", ":", "\n", "            ", "res", "[", "target", "]", "=", "[", "]", "\n", "", "res", "[", "target", "]", ".", "append", "(", "index", ")", "\n", "", "return", "res", "\n", "", ""]]}