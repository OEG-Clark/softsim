{"home.repos.pwc.inspect_result.dennlinger_TopicalChange.None.convert_triplets_to_jsonl.get_args": [[7, 18], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"input_file\"", ",", "default", "=", "\"./train_reduction.tsv\"", ",", "\n", "help", "=", "\"File to convert into .jsonl; has to be in triplet format already\"", ")", "\n", "parser", ".", "add_argument", "(", "\"output_file\"", ",", "default", "=", "\"./wiki727k.jsonl\"", ",", "\n", "help", "=", "\"Name of the output file. Will create directory automatically\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_length\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "\"Will make sure that all entries in anchor/positive/negative have this as \"", "\n", "\"their minimum character count.\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.boilerpipe.extract": [[38, 57], ["StringReader", "BoilerpipeSAXInput().getTextDocument", "extractor.process", "HTMLHighlighter.newExtractingInstance", "HTMLHighlighter.newExtractingInstance.process", "jpype.isThreadAttachedToJVM", "jpype.attachThreadToJVM", "kwargs.get", "jpype.JClass", "jpype.JClass", "BoilerpipeSAXInput", "InputSource"], "function", ["None"], ["def", "extract", "(", "html", ",", "extractor", "=", "\"ArticleExtractor\"", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "lock", ":", "\n", "        ", "if", "not", "jpype", ".", "isThreadAttachedToJVM", "(", ")", ":", "\n", "            ", "jpype", ".", "attachThreadToJVM", "(", ")", "\n", "\n", "", "if", "extractor", "==", "\"KeepEverythingWithMinKWordsExtractor\"", ":", "\n", "            ", "kMin", "=", "kwargs", ".", "get", "(", "\"kMin\"", ",", "1", ")", "# set default to 1", "\n", "extractor", "=", "jpype", ".", "JClass", "(", "\"de.l3s.boilerpipe.extractors.\"", "+", "extractor", ")", "(", "kMin", ")", "\n", "", "else", ":", "\n", "            ", "extractor", "=", "jpype", ".", "JClass", "(", "\n", "\"de.l3s.boilerpipe.extractors.\"", "+", "extractor", "\n", ")", ".", "INSTANCE", "\n", "\n", "", "reader", "=", "StringReader", "(", "html", ")", "\n", "source", "=", "BoilerpipeSAXInput", "(", "InputSource", "(", "reader", ")", ")", ".", "getTextDocument", "(", ")", "\n", "extractor", ".", "process", "(", "source", ")", "\n", "\n", "highlighter", "=", "HTMLHighlighter", ".", "newExtractingInstance", "(", ")", "\n", "return", "highlighter", ".", "process", "(", "source", ",", "html", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.01_crawler.crawl": [[26, 149], ["print", "urllib.request.Request", "bs4.BeautifulSoup.find_all", "print", "print", "urllib.request.urlopen", "fp.info().get_content_charset", "fp.read", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "isinstance", "bs4.BeautifulSoup", "link.get_text", "max", "01_crawler.WebsiteNotFound", "urllib.parse.urljoin", "01_crawler.WebsiteNotFound", "urllib.request.Request", "html.decode.decode", "01_crawler.WebsiteNotFound", "01_crawler.WebsiteNotFound", "urllib.request.urlopen", "fp.info().get_content_charset", "fp.read", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "01_crawler.WebsiteRetry", "fp.info", "html.decode.decode", "Levenshtein.ratio", "html.decode.decode", "link.get_text.lower", "sp.lower", "fp.info", "html.decode.decode"], "function", ["None"], ["", "def", "crawl", "(", "website", ",", "subpage", "=", "None", ",", "headless", "=", "True", ",", "lang", "=", "\"en\"", ")", ":", "\n", "    ", "\"\"\"\n    Download the content of a given website using Urllib.\n    Optionally go to a subpage by following a link with a given text.\n    \"\"\"", "\n", "\n", "# FIXME: Language en?", "\n", "headers", "=", "{", "\n", "\"Accept-Language\"", ":", "\"en-US,en;q=0.9\"", ",", "\n", "\"User-Agent\"", ":", "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\"", ",", "\n", "\"Accept\"", ":", "\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"", ",", "\n", "\"Connection\"", ":", "\"keep-alive\"", ",", "\n", "}", "\n", "\n", "if", "lang", "==", "\"de\"", ":", "\n", "        ", "headers", "[", "\"Accept-Language\"", "]", "=", "\"de-DE,de;q=0.9\"", "\n", "\n", "", "print", "(", "\"Opening website %s\"", "%", "(", "website", ",", ")", ")", "\n", "\n", "try", ":", "\n", "        ", "req", "=", "urllib", ".", "request", ".", "Request", "(", "website", ",", "headers", "=", "headers", ")", "\n", "with", "urllib", ".", "request", ".", "urlopen", "(", "req", ",", "timeout", "=", "60", "*", "3", ")", "as", "fp", ":", "\n", "            ", "charset", "=", "fp", ".", "info", "(", ")", ".", "get_content_charset", "(", ")", "\n", "if", "charset", "is", "None", ":", "\n", "                ", "charset", "=", "\"utf8\"", "\n", "", "html", "=", "fp", ".", "read", "(", ")", "\n", "try", ":", "\n", "                ", "html", "=", "html", ".", "decode", "(", "charset", ",", "errors", "=", "\"ignore\"", ")", "\n", "", "except", "LookupError", ":", "\n", "                ", "html", "=", "html", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "\n", "", "", "", "except", "urllib", ".", "error", ".", "URLError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ssl", ".", "CertificateError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "http", ".", "client", ".", "IncompleteRead", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "http", ".", "client", ".", "HTTPException", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ConnectionError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "UnicodeEncodeError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "TimeoutError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "OSError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "UnicodeError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "raise", "WebsiteRetry", "(", ")", "\n", "\n", "# If subpage is specified, go to the corresponding subpage", "\n", "# by matching the link texts.", "\n", "", "if", "subpage", "is", "not", "None", ":", "\n", "        ", "if", "not", "isinstance", "(", "subpage", ",", "list", ")", ":", "\n", "            ", "subpage", "=", "[", "subpage", "]", "\n", "\n", "", "best_ratio", "=", "-", "np", ".", "inf", "\n", "next_link", "=", "None", "\n", "\n", "try", ":", "\n", "            ", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "            ", "raise", "WebsiteNotFound", "(", ")", "# cannot parse html", "\n", "\n", "", "for", "link", "in", "soup", ".", "find_all", "(", "\"a\"", ")", ":", "\n", "            ", "text", "=", "link", ".", "get_text", "(", ")", "\n", "\n", "# FIXME: Is the Levenshtein distance appropriate here?", "\n", "ratio", "=", "max", "(", "[", "Levenshtein", ".", "ratio", "(", "text", ".", "lower", "(", ")", ",", "sp", ".", "lower", "(", ")", ")", "for", "sp", "in", "subpage", "]", ")", "\n", "if", "ratio", ">", "best_ratio", ":", "\n", "                ", "best_ratio", "=", "ratio", "\n", "next_link", "=", "link", "\n", "\n", "", "", "if", "not", "next_link", "or", "best_ratio", "<", "0.75", ":", "\n", "            ", "raise", "WebsiteNotFound", "(", ")", "\n", "\n", "", "headers", "[", "\"Referer\"", "]", "=", "website", "\n", "print", "(", "next_link", ")", "\n", "\n", "try", ":", "\n", "            ", "website", "=", "urllib", ".", "parse", ".", "urljoin", "(", "website", ",", "next_link", "[", "\"href\"", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "raise", "WebsiteNotFound", "(", ")", "\n", "\n", "", "if", "website", "==", "headers", "[", "\"Referer\"", "]", ":", "\n", "            ", "raise", "WebsiteNotFound", "(", ")", "# we ended up on the same page", "\n", "\n", "", "print", "(", "\"Opening website %s\"", "%", "(", "website", ",", ")", ")", "\n", "\n", "try", ":", "\n", "            ", "req", "=", "urllib", ".", "request", ".", "Request", "(", "website", ",", "headers", "=", "headers", ")", "\n", "with", "urllib", ".", "request", ".", "urlopen", "(", "req", ",", "timeout", "=", "60", "*", "3", ")", "as", "fp", ":", "\n", "                ", "charset", "=", "fp", ".", "info", "(", ")", ".", "get_content_charset", "(", ")", "\n", "if", "charset", "is", "None", ":", "\n", "                    ", "charset", "=", "\"utf8\"", "\n", "", "html", "=", "fp", ".", "read", "(", ")", "\n", "try", ":", "\n", "                    ", "html", "=", "html", ".", "decode", "(", "charset", ",", "errors", "=", "\"ignore\"", ")", "\n", "", "except", "LookupError", ":", "\n", "                    ", "html", "=", "html", ".", "decode", "(", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "\n", "", "", "", "except", "urllib", ".", "error", ".", "URLError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ssl", ".", "CertificateError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "http", ".", "client", ".", "IncompleteRead", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "http", ".", "client", ".", "HTTPException", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ConnectionError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "UnicodeEncodeError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "TimeoutError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "OSError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "UnicodeError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "raise", "WebsiteRetry", "(", ")", "\n", "\n", "", "", "return", "html", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.parent_tags": [[41, 48], ["tags.append"], "function", ["None"], ["", "def", "parent_tags", "(", "tag", ")", ":", "\n", "    ", "\"\"\" Return a list of all parent tags of a given node. \"\"\"", "\n", "tags", "=", "[", "]", "\n", "while", "tag", ":", "\n", "        ", "tags", ".", "append", "(", "tag", ".", "name", ")", "\n", "tag", "=", "tag", ".", "parent", "\n", "", "return", "tags", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_pres": [[50, 65], ["soup.find", "reversed", "soup.find.extract", "content.split", "soup.new_tag", "soup.new_tag.append", "soup.find.insert_after", "str", "bs4.NavigableString"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "convert_pres", "(", "soup", ")", ":", "\n", "    ", "\"\"\" Convert <pre>...</pre> to several <p> tags (one for each line). \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "\"pre\"", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "content", "=", "\"\"", ".", "join", "(", "[", "str", "(", "c", ")", "for", "c", "in", "block", ".", "contents", "]", ")", "\n", "\n", "for", "line", "in", "reversed", "(", "content", ".", "split", "(", "\"\\n\"", ")", ")", ":", "\n", "            ", "new_tag", "=", "soup", ".", "new_tag", "(", "\"p\"", ")", "\n", "new_tag", ".", "append", "(", "NavigableString", "(", "line", ")", ")", "\n", "block", ".", "insert_after", "(", "new_tag", ")", "\n", "\n", "", "block", ".", "extract", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_spans": [[67, 102], ["soup.find", "re.match", "re.match", "reversed", "soup.find.extract", "soup.find.get", "soup.find.get", "soup.find.get", "child.extract.extract", "soup.find.insert_after", "02_extractor.parent_tags", "02_extractor.parent_tags", "02_extractor.parent_tags"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.parent_tags", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.parent_tags", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.parent_tags"], ["", "", "def", "convert_spans", "(", "soup", ")", ":", "\n", "    ", "\"\"\" Convert <span>...</span> tags to <b>...</b> or <u>...</u> tags. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "\"span\"", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "if", "re", ".", "match", "(", "\n", "\"^(.*;\\\\s*)?font-weight\\\\s*:\\\\s*bold(;.*)?$\"", ",", "block", ".", "get", "(", "\"style\"", ",", "\"\"", ")", "\n", ")", ":", "\n", "            ", "if", "\"b\"", "not", "in", "parent_tags", "(", "block", ")", ":", "\n", "                ", "block", ".", "name", "=", "\"b\"", "\n", "continue", "\n", "# Otherwise delete it without replacement.", "\n", "\n", "", "", "if", "re", ".", "match", "(", "\n", "\"^(.*;\\\\s*)?text-decoration\\\\s*:\\\\s*underline(;.*)?$\"", ",", "\n", "block", ".", "get", "(", "\"style\"", ",", "\"\"", ")", ",", "\n", ")", ":", "\n", "            ", "if", "\"u\"", "not", "in", "parent_tags", "(", "block", ")", ":", "\n", "                ", "block", ".", "name", "=", "\"u\"", "\n", "continue", "\n", "# Otherwise delete it without replacement.", "\n", "\n", "", "", "if", "\"underline\"", "in", "block", ".", "get", "(", "\"class\"", ",", "[", "]", ")", ":", "\n", "            ", "if", "\"u\"", "not", "in", "parent_tags", "(", "block", ")", ":", "\n", "                ", "block", ".", "name", "=", "\"u\"", "\n", "continue", "\n", "# Otherwise delete it without replacement.", "\n", "\n", "", "", "for", "child", "in", "reversed", "(", "block", ".", "contents", ")", ":", "\n", "            ", "child", "=", "child", ".", "extract", "(", ")", "\n", "block", ".", "insert_after", "(", "child", ")", "\n", "\n", "", "block", ".", "extract", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.delete_divs": [[104, 116], ["soup.find", "reversed", "soup.find.extract", "child.extract.extract", "soup.find.insert_after"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "", "def", "delete_divs", "(", "soup", ")", ":", "\n", "    ", "\"\"\" Delete all <div>...</div> tags. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "\"div\"", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "for", "child", "in", "reversed", "(", "block", ".", "contents", ")", ":", "\n", "            ", "child", "=", "child", ".", "extract", "(", ")", "\n", "block", ".", "insert_after", "(", "child", ")", "\n", "\n", "", "block", ".", "extract", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_string": [[118, 139], ["isinstance", "len", "str().strip", "str"], "function", ["None"], ["", "", "def", "is_string", "(", "tag", ")", ":", "\n", "    ", "\"\"\" Checks if a node is a string without proper tag. \"\"\"", "\n", "for", "child", "in", "tag", ".", "contents", ":", "\n", "        ", "if", "isinstance", "(", "child", ",", "NavigableString", ")", ":", "\n", "            ", "if", "len", "(", "str", "(", "child", ")", ".", "strip", "(", ")", ")", ">", "0", ":", "\n", "                ", "break", "\n", "", "", "if", "child", ".", "name", "in", "[", "\"strong\"", ",", "\"b\"", ",", "\"u\"", "]", ":", "\n", "            ", "break", "\n", "", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n", "", "while", "True", ":", "\n", "        ", "if", "tag", ".", "name", "in", "[", "\"style\"", ",", "\"script\"", ",", "\"title\"", "]", ":", "\n", "            ", "return", "False", "\n", "", "if", "tag", ".", "name", "in", "[", "\"p\"", ",", "\"h1\"", ",", "\"h2\"", ",", "\"h3\"", ",", "\"h4\"", ",", "\"h5\"", "]", ":", "\n", "            ", "return", "False", "\n", "", "tag", "=", "tag", ".", "parent", "\n", "if", "tag", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_strings": [[141, 155], ["soup.find", "soup.new_tag", "list", "soup.find.append", "child.extract.extract", "soup.new_tag.append"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "convert_strings", "(", "soup", ")", ":", "\n", "    ", "\"\"\" Put <p>...</p> tags around all strings without proper tags. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "is_string", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "new_tag", "=", "soup", ".", "new_tag", "(", "\"p\"", ")", "\n", "\n", "for", "child", "in", "list", "(", "block", ".", "contents", ")", ":", "\n", "            ", "child", "=", "child", ".", "extract", "(", ")", "\n", "new_tag", ".", "append", "(", "child", ")", "\n", "\n", "", "block", ".", "append", "(", "new_tag", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_consecutive": [[157, 173], ["isinstance", "len", "str().strip", "str"], "function", ["None"], ["", "", "def", "is_consecutive", "(", "tag", ")", ":", "\n", "    ", "\"\"\" Check if several nodes of same type follow each other. \"\"\"", "\n", "if", "tag", ".", "name", "not", "in", "[", "\"b\"", ",", "\"strong\"", ",", "\"u\"", "]", ":", "\n", "        ", "return", "False", "\n", "\n", "", "next_tag", "=", "tag", ".", "next_sibling", "\n", "while", "True", ":", "\n", "        ", "if", "not", "next_tag", ":", "\n", "            ", "return", "False", "\n", "", "if", "not", "isinstance", "(", "next_tag", ",", "NavigableString", ")", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "str", "(", "next_tag", ")", ".", "strip", "(", ")", ")", ">", "0", ":", "\n", "            ", "return", "False", "\n", "", "next_tag", "=", "next_tag", ".", "next_sibling", "\n", "\n", "", "return", "next_tag", ".", "name", "==", "tag", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.merge_consecutive": [[175, 196], ["soup.find", "list", "next_block.extract", "next_block.extract", "soup.find.append", "child.extract.extract", "soup.find.append", "isinstance"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "merge_consecutive", "(", "soup", ")", ":", "\n", "    ", "\"\"\" Merge consecutive nodes if they have the same type. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "is_consecutive", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "while", "True", ":", "\n", "            ", "next_block", "=", "block", ".", "next_sibling", "\n", "assert", "next_block", "is", "not", "None", "\n", "if", "not", "isinstance", "(", "next_block", ",", "NavigableString", ")", ":", "\n", "                ", "break", "\n", "", "child", "=", "next_block", ".", "extract", "(", ")", "\n", "block", ".", "append", "(", "child", ")", "\n", "\n", "", "assert", "next_block", ".", "name", "==", "block", ".", "name", "\n", "for", "child", "in", "list", "(", "next_block", ".", "contents", ")", ":", "\n", "            ", "child", "=", "child", ".", "extract", "(", ")", "\n", "block", ".", "append", "(", "child", ")", "\n", "\n", "", "next_block", ".", "extract", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_nested": [[198, 202], ["tag.find"], "function", ["None"], ["", "", "def", "is_nested", "(", "tag", ",", "tag_list", ")", ":", "\n", "    ", "if", "tag", ".", "name", "in", "tag_list", ":", "\n", "        ", "return", "tag", ".", "find", "(", "tag_list", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested": [[204, 244], ["soup.find", "soup.find.find", "copy.copy", "copy.copy.clear", "split_parent.index", "split.extract.extract", "copy.copy", "copy.copy.clear", "list", "split.extract.append", "split_parent.insert_after", "02_extractor.is_nested", "element.extract.extract", "copy.copy.append", "len", "split_parent.insert_after", "split_parent.insert_after", "element.extract.extract", "copy.copy.append", "len", "split_parent.extract", "len", "split_parent.extract"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "split_on_nested", "(", "soup", ",", "tag_list", ")", ":", "\n", "    ", "\"\"\" Split nested tags (e.g., nested <p> tags) into separate ones. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "lambda", "tag", ":", "is_nested", "(", "tag", ",", "tag_list", ")", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "split", "=", "block", ".", "find", "(", "tag_list", ")", "\n", "assert", "split", "is", "not", "None", "\n", "\n", "while", "True", ":", "\n", "            ", "split_parent", "=", "split", ".", "parent", "\n", "new_tag", "=", "copy", ".", "copy", "(", "split_parent", ")", "\n", "new_tag", ".", "clear", "(", ")", "\n", "\n", "index", "=", "split_parent", ".", "index", "(", "split", ")", "\n", "for", "element", "in", "split_parent", ".", "contents", "[", "index", "+", "1", ":", "]", ":", "\n", "                ", "element", "=", "element", ".", "extract", "(", ")", "\n", "new_tag", ".", "append", "(", "element", ")", "\n", "\n", "", "split", "=", "split", ".", "extract", "(", ")", "\n", "if", "len", "(", "new_tag", ".", "contents", ")", ">", "0", ":", "\n", "                ", "split_parent", ".", "insert_after", "(", "new_tag", ")", "\n", "", "if", "split_parent", "==", "block", ":", "\n", "                ", "split_parent", ".", "insert_after", "(", "split", ")", "\n", "if", "len", "(", "split_parent", ".", "contents", ")", "==", "0", ":", "\n", "                    ", "split_parent", ".", "extract", "(", ")", "\n", "", "break", "\n", "\n", "", "new_tag", "=", "copy", ".", "copy", "(", "split_parent", ")", "\n", "new_tag", ".", "clear", "(", ")", "\n", "\n", "for", "element", "in", "list", "(", "split", ".", "contents", ")", ":", "\n", "                ", "element", "=", "element", ".", "extract", "(", ")", "\n", "new_tag", ".", "append", "(", "element", ")", "\n", "\n", "", "split", ".", "append", "(", "new_tag", ")", "\n", "split_parent", ".", "insert_after", "(", "split", ")", "\n", "if", "len", "(", "split_parent", ".", "contents", ")", "==", "0", ":", "\n", "                ", "split_parent", ".", "extract", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_splittable_block": [[246, 250], ["tag.find"], "function", ["None"], ["", "", "", "", "def", "is_splittable_block", "(", "tag", ",", "block_tag", ",", "split_tag", ")", ":", "\n", "    ", "if", "tag", ".", "name", "==", "block_tag", ":", "\n", "        ", "return", "tag", ".", "find", "(", "split_tag", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag": [[252, 280], ["soup.find", "soup.find.find", "copy.copy", "copy.copy.clear", "split_parent.index", "split.extract.extract", "split_parent.insert_after", "split_parent.insert_after", "02_extractor.is_splittable_block", "element.extract.extract", "copy.copy.append", "re.sub().strip", "re.sub", "split.extract.get_text"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_splittable_block", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "split_on_tag", "(", "soup", ",", "block_tag", ",", "split_tag", ",", "attr", "=", "None", ")", ":", "\n", "    ", "\"\"\" Split HTML tree blocks based on split tag. \"\"\"", "\n", "while", "True", ":", "\n", "        ", "block", "=", "soup", ".", "find", "(", "lambda", "tag", ":", "is_splittable_block", "(", "tag", ",", "block_tag", ",", "split_tag", ")", ")", "\n", "if", "block", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "split", "=", "block", ".", "find", "(", "split_tag", ")", "\n", "assert", "split", "is", "not", "None", "\n", "\n", "while", "True", ":", "\n", "            ", "split_parent", "=", "split", ".", "parent", "\n", "new_tag", "=", "copy", ".", "copy", "(", "split_parent", ")", "\n", "new_tag", ".", "clear", "(", ")", "\n", "\n", "index", "=", "split_parent", ".", "index", "(", "split", ")", "\n", "for", "element", "in", "split_parent", ".", "contents", "[", "index", "+", "1", ":", "]", ":", "\n", "                ", "element", "=", "element", ".", "extract", "(", ")", "\n", "new_tag", ".", "append", "(", "element", ")", "\n", "\n", "", "split", "=", "split", ".", "extract", "(", ")", "\n", "split_parent", ".", "insert_after", "(", "new_tag", ")", "\n", "if", "split_parent", "==", "block", ":", "\n", "                ", "if", "attr", "is", "not", "None", ":", "\n", "                    ", "new_tag", "[", "attr", "]", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "split", ".", "get_text", "(", ")", ")", ".", "strip", "(", ")", "\n", "", "break", "\n", "\n", "", "split_parent", ".", "insert_after", "(", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index": [[282, 302], ["text[].strip.replace", "text[].strip.replace", "text[].strip.strip", "re_header.match", "re_header.match.group().strip", "text[].strip", "text[].strip", "len", "len", "re_header.match.group"], "function", ["None"], ["", "", "", "def", "extract_index", "(", "text", ")", ":", "\n", "    ", "\"\"\" Clean up text and extract nesting level. \"\"\"", "\n", "count", "=", "0", "\n", "text", "=", "text", ".", "replace", "(", "\"\\u200b\"", ",", "\"\"", ")", "\n", "text", "=", "text", ".", "replace", "(", "\"\\u00ad\"", ",", "\"-\"", ")", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "while", "True", ":", "\n", "        ", "if", "len", "(", "text", ")", ">", "0", "and", "text", "[", "0", "]", "in", "[", "\"\u2022\"", ",", "\"-\"", ",", "\"\u2013\"", "]", ":", "\n", "            ", "text", "=", "text", "[", "1", ":", "]", ".", "strip", "(", ")", "\n", "continue", "\n", "\n", "", "m", "=", "re_header", ".", "match", "(", "text", ")", "\n", "if", "m", "is", "None", ":", "\n", "            ", "break", "\n", "", "text", "=", "m", ".", "group", "(", "1", ")", ".", "strip", "(", ")", "\n", "count", "+=", "1", "\n", "\n", "", "if", "len", "(", "text", ")", ">", "0", "and", "text", "[", "0", "]", "in", "[", "\".\"", ",", "\",\"", "]", ":", "\n", "        ", "text", "=", "text", "[", "1", ":", "]", ".", "strip", "(", ")", "\n", "", "return", "text", ",", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_bold": [[304, 306], ["None"], "function", ["None"], ["", "def", "is_bold", "(", "tag", ")", ":", "\n", "    ", "return", "tag", ".", "name", "in", "[", "\"strong\"", ",", "\"b\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_underline": [[308, 310], ["None"], "function", ["None"], ["", "def", "is_underline", "(", "tag", ")", ":", "\n", "    ", "return", "tag", ".", "name", "in", "[", "\"u\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_enum": [[312, 334], ["re.sub", "02_extractor.extract_index", "tag.get_text", "len", "02_extractor.is_bold", "02_extractor.is_bold", "02_extractor.is_underline", "02_extractor.is_underline"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_bold", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_bold", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_underline", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_underline"], ["", "def", "split_tag_enum", "(", "tag", ",", "func", ",", "count", ")", ":", "\n", "    ", "\"\"\"\n    Helper function for split_on_tag to split based on bold / unterline tags\n    containing an enumeration.\n    \"\"\"", "\n", "\n", "if", "not", "func", "(", "tag", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "tag", ".", "parent", ".", "name", "!=", "\"p\"", ":", "\n", "        ", "return", "False", "\n", "", "if", "tag", ".", "previous_sibling", ":", "\n", "        ", "return", "False", "\n", "\n", "", "text", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "tag", ".", "get_text", "(", ")", ")", "\n", "_", ",", "index", "=", "extract_index", "(", "text", ")", "\n", "\n", "if", "count", "!=", "index", ":", "\n", "        ", "return", "False", "\n", "", "if", "len", "(", "text", ")", ">", "200", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_text_enum": [[336, 364], ["re.sub", "02_extractor.extract_index", "re.sub", "02_extractor.extract_index", "tag.get_text", "len", "tag.next_sibling.get_text"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index"], ["", "def", "split_text_enum", "(", "tag", ",", "count", ")", ":", "\n", "    ", "\"\"\"\n    Helper function for split_on_tag to split based on text containing an\n    enumeration.\n    \"\"\"", "\n", "\n", "if", "tag", ".", "name", "!=", "\"p\"", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "tag", ".", "next_sibling", ":", "\n", "        ", "return", "False", "\n", "", "if", "tag", ".", "next_sibling", ".", "name", "!=", "\"p\"", ":", "\n", "        ", "return", "False", "\n", "\n", "", "text", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "tag", ".", "get_text", "(", ")", ")", "\n", "text", ",", "index", "=", "extract_index", "(", "text", ")", "\n", "\n", "if", "count", "!=", "index", ":", "\n", "        ", "return", "False", "\n", "", "if", "len", "(", "text", ")", ">", "100", ":", "\n", "        ", "return", "False", "\n", "\n", "", "text", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "tag", ".", "next_sibling", ".", "get_text", "(", ")", ")", "\n", "_", ",", "index", "=", "extract_index", "(", "text", ")", "\n", "\n", "if", "index", "!=", "0", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_list": [[366, 395], ["02_extractor.extract_index", "isinstance", "str", "len", "02_extractor.is_bold", "02_extractor.is_underline"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_bold", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.is_underline"], ["", "def", "split_tag_list", "(", "tag", ",", "func", ",", "count", ")", ":", "\n", "    ", "\"\"\"\n    Helper function for split_on_tag to split based on an enumeration with\n    <li> tags.\n    \"\"\"", "\n", "\n", "if", "not", "func", "(", "tag", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "tag", ".", "parent", ".", "name", "!=", "\"p\"", ":", "\n", "        ", "return", "False", "\n", "", "if", "tag", ".", "previous_sibling", ":", "\n", "        ", "prev", "=", "tag", ".", "previous_sibling", "\n", "if", "not", "isinstance", "(", "prev", ",", "NavigableString", ")", ":", "\n", "            ", "return", "False", "\n", "", "text", ",", "_", "=", "extract_index", "(", "str", "(", "prev", ")", ")", "\n", "if", "len", "(", "text", ")", ">", "0", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "index", "=", "0", "\n", "current", "=", "tag", ".", "parent", "\n", "while", "current", ":", "\n", "        ", "if", "current", ".", "name", "==", "\"li\"", ":", "\n", "            ", "index", "+=", "1", "\n", "", "current", "=", "current", ".", "parent", "\n", "\n", "", "if", "count", "!=", "index", ":", "\n", "        ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_content": [[397, 567], ["sys.setrecursionlimit", "02_extractor.convert_pres", "02_extractor.convert_spans", "02_extractor.merge_consecutive", "02_extractor.split_on_tag", "str", "boilerpipe.extract", "bs4.BeautifulSoup", "02_extractor.convert_strings", "02_extractor.split_on_nested", "02_extractor.delete_divs", "langid.langid.LanguageIdentifier.from_modelstring", "collections.defaultdict", "bs4.BeautifulSoup.find_all", "range", "range", "range", "range", "range", "bs4.BeautifulSoup.find_all", "collections.defaultdict", "collections.defaultdict.items", "bs4.BeautifulSoup", "len", "re.sub().strip", "len", "sorted", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "02_extractor.split_on_tag", "len", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "02_extractor.split_on_tag", "len", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "02_extractor.split_on_tag", "copy.copy", "section.find_all", "results.append", "LanguageIdentifier.from_modelstring.classify", "len", "collections.defaultdict.items", "len", "len", "len", "len", "len", "len", "len", "len", "02_extractor.extract_index", "results.append", "re.sub().strip", "02_extractor.extract_index", "re.sub", "len", "02_extractor.split_tag_enum", "len", "02_extractor.split_tag_list", "len", "02_extractor.split_tag_enum", "len", "02_extractor.split_tag_enum", "len", "02_extractor.split_tag_list", "len", "02_extractor.split_tag_enum", "len", "02_extractor.split_text_enum", "len", "len", "section_text.append", "len", "results.append", "paragraph.get_text", "len", "len", "re.sub", "paragraph.get_text", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_pres", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_spans", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.merge_consecutive", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_strings", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.delete_divs", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.extract_index", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_enum", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_list", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_enum", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_enum", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_list", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_tag_enum", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_text_enum"], ["", "def", "extract_content", "(", "html", ",", "headless", "=", "True", ",", "language", "=", "\"en\"", ")", ":", "\n", "    ", "import", "boilerpipe", "\n", "\n", "# Workaround for BeautifulSoup causing a RecursionError.", "\n", "sys", ".", "setrecursionlimit", "(", "10000", ")", "\n", "\n", "try", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "        ", "return", "[", "]", "# Bug in BS4?", "\n", "\n", "", "convert_pres", "(", "soup", ")", "\n", "convert_spans", "(", "soup", ")", "\n", "merge_consecutive", "(", "soup", ")", "\n", "split_on_tag", "(", "soup", ",", "\"p\"", ",", "\"br\"", ")", "\n", "html", "=", "str", "(", "soup", ")", "\n", "\n", "html", "=", "boilerpipe", ".", "extract", "(", "html", ")", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "assert", "len", "(", "soup", ".", "contents", ")", "==", "1", "\n", "assert", "soup", ".", "contents", "[", "0", "]", ".", "name", "==", "\"html\"", "\n", "convert_strings", "(", "soup", ")", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", ",", "\"h1\"", ",", "\"h2\"", ",", "\"h3\"", ",", "\"h4\"", ",", "\"h5\"", "]", ")", "\n", "delete_divs", "(", "soup", ")", "\n", "\n", "identifier", "=", "LanguageIdentifier", ".", "from_modelstring", "(", "model", ",", "norm_probs", "=", "True", ")", "\n", "scores", "=", "collections", ".", "defaultdict", "(", "float", ")", "\n", "for", "paragraph", "in", "soup", ".", "find_all", "(", "\"p\"", ")", ":", "\n", "        ", "text", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "paragraph", ".", "get_text", "(", ")", ")", ".", "strip", "(", ")", "\n", "try", ":", "\n", "            ", "lang", ",", "conf", "=", "identifier", ".", "classify", "(", "text", ")", "\n", "", "except", "UnicodeEncodeError", ":", "\n", "            ", "continue", "\n", "", "scores", "[", "lang", "]", "+=", "len", "(", "text", ")", "*", "conf", "\n", "\n", "", "if", "len", "(", "scores", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "", "lang", ",", "_", "=", "sorted", "(", "scores", ".", "items", "(", ")", ",", "key", "=", "lambda", "item", ":", "item", "[", "1", "]", ",", "reverse", "=", "True", ")", "[", "0", "]", "\n", "if", "lang", "!=", "language", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "", "for", "tag", "in", "[", "\"h1\"", ",", "\"h2\"", ",", "\"h3\"", ",", "\"h4\"", ",", "\"h5\"", "]", ":", "\n", "        ", "attr", "=", "\"section-%s\"", "%", "(", "tag", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "temp", ",", "\"html\"", ",", "tag", ",", "attr", "=", "attr", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "for", "count", "in", "range", "(", "1", ",", "6", ")", ":", "\n", "        ", "attr", "=", "\"section-b%d\"", "%", "(", "count", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\"html\"", ",", "lambda", "tag", ":", "split_tag_enum", "(", "tag", ",", "is_bold", ",", "count", ")", ",", "attr", "=", "attr", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "for", "count", "in", "range", "(", "1", ",", "6", ")", ":", "\n", "        ", "attr", "=", "\"section-li-b%d\"", "%", "(", "count", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\"html\"", ",", "lambda", "tag", ":", "split_tag_list", "(", "tag", ",", "is_bold", ",", "count", ")", ",", "attr", "=", "attr", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "if", "len", "(", "soup", ".", "contents", ")", "==", "1", ":", "\n", "        ", "attr", "=", "\"section-b\"", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\"html\"", ",", "lambda", "tag", ":", "split_tag_enum", "(", "tag", ",", "is_bold", ",", "0", ")", ",", "attr", "=", "attr", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "for", "count", "in", "range", "(", "1", ",", "6", ")", ":", "\n", "        ", "attr", "=", "\"section-u%d\"", "%", "(", "count", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\n", "\"html\"", ",", "\n", "lambda", "tag", ":", "split_tag_enum", "(", "tag", ",", "is_underline", ",", "count", ")", ",", "\n", "attr", "=", "attr", ",", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "for", "count", "in", "range", "(", "1", ",", "6", ")", ":", "\n", "        ", "attr", "=", "\"section-li-u%d\"", "%", "(", "count", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\n", "\"html\"", ",", "\n", "lambda", "tag", ":", "split_tag_list", "(", "tag", ",", "is_underline", ",", "count", ")", ",", "\n", "attr", "=", "attr", ",", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "if", "len", "(", "soup", ".", "contents", ")", "==", "1", ":", "\n", "        ", "attr", "=", "\"section-u\"", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "\n", "temp", ",", "\"html\"", ",", "lambda", "tag", ":", "split_tag_enum", "(", "tag", ",", "is_underline", ",", "0", ")", ",", "attr", "=", "attr", "\n", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "for", "count", "in", "range", "(", "1", ",", "6", ")", ":", "\n", "        ", "attr", "=", "\"section-t%d\"", "%", "(", "count", ",", ")", "\n", "temp", "=", "copy", ".", "copy", "(", "soup", ")", "\n", "split_on_tag", "(", "temp", ",", "\"html\"", ",", "lambda", "tag", ":", "split_text_enum", "(", "tag", ",", "count", ")", ",", "attr", "=", "attr", ")", "\n", "if", "len", "(", "temp", ".", "contents", ")", ">=", "len", "(", "soup", ".", "contents", ")", "+", "5", ":", "\n", "            ", "soup", "=", "temp", "\n", "\n", "", "", "results", "=", "[", "]", "\n", "prev_section_text", "=", "None", "\n", "\n", "for", "section", "in", "soup", ".", "find_all", "(", "\"html\"", ")", ":", "\n", "        ", "section_text", "=", "[", "]", "\n", "for", "attr", "in", "[", "'h1'", ",", "'h2'", ",", "'h3'", ",", "'h4'", ",", "'h5'", ",", "\n", "'b1'", ",", "'b2'", ",", "'b3'", ",", "'b4'", ",", "'b5'", ",", "\n", "'li-b1'", ",", "'li-b2'", ",", "'li-b3'", ",", "'li-b4'", ",", "'li-b5'", ",", "\n", "'b'", ",", "\n", "'u1'", ",", "'u2'", ",", "'u3'", ",", "'u4'", ",", "'u5'", ",", "\n", "'li-u1'", ",", "'li-u2'", ",", "'li-u3'", ",", "'li-u4'", ",", "'li-u5'", ",", "\n", "'u'", ",", "\n", "'t1'", ",", "'t2'", ",", "'t3'", ",", "'t4'", ",", "'t5'", "]", ":", "\n", "            ", "try", ":", "\n", "                ", "text", "=", "section", "[", "\"section-%s\"", "%", "(", "attr", ",", ")", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "continue", "\n", "\n", "", "text", ",", "_", "=", "extract_index", "(", "text", ")", "\n", "if", "len", "(", "text", ")", ">", "0", ":", "\n", "                ", "section_text", ".", "append", "(", "text", ")", "\n", "\n", "# If a section doesn't contain any text, and the following section isn't a subsection", "\n", "# of the previous one, then print the innermost section as regular text.", "\n", "", "", "if", "prev_section_text", "and", "(", "\n", "len", "(", "section_text", ")", "<", "len", "(", "prev_section_text", ")", "\n", "or", "section_text", "[", ":", "len", "(", "prev_section_text", ")", "]", "!=", "prev_section_text", "\n", ")", ":", "\n", "            ", "results", ".", "append", "(", "\n", "{", "\"text\"", ":", "prev_section_text", "[", "-", "1", "]", ",", "\"section\"", ":", "prev_section_text", "[", ":", "-", "1", "]", "}", "\n", ")", "\n", "\n", "", "prev_section_text", "=", "copy", ".", "copy", "(", "section_text", ")", "\n", "\n", "for", "paragraph", "in", "section", ".", "find_all", "(", "\"p\"", ")", ":", "\n", "            ", "text", "=", "re", ".", "sub", "(", "\"\\\\s+\"", ",", "\" \"", ",", "paragraph", ".", "get_text", "(", ")", ")", ".", "strip", "(", ")", "\n", "text", ",", "_", "=", "extract_index", "(", "text", ")", "\n", "if", "len", "(", "text", ")", ">", "0", ":", "\n", "                ", "results", ".", "append", "(", "{", "\"text\"", ":", "text", ",", "\"section\"", ":", "section_text", "}", ")", "\n", "prev_section_text", "=", "None", "\n", "\n", "", "", "", "if", "prev_section_text", ":", "\n", "        ", "results", ".", "append", "(", "\n", "{", "\"text\"", ":", "prev_section_text", "[", "-", "1", "]", ",", "\"section\"", ":", "prev_section_text", "[", ":", "-", "1", "]", "}", "\n", ")", "\n", "\n", "", "counter", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "for", "result", "in", "results", ":", "\n", "        ", "counter", "[", "result", "[", "\"text\"", "]", "]", "+=", "1", "\n", "\n", "", "for", "text", ",", "count", "in", "counter", ".", "items", "(", ")", ":", "\n", "        ", "if", "count", ">", "2", ":", "\n", "            ", "results", "=", "[", "result", "for", "result", "in", "results", "if", "result", "[", "\"text\"", "]", "!=", "text", "]", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.tests": [[569, 638], ["bs4.BeautifulSoup", "02_extractor.convert_pres", "bs4.BeautifulSoup", "02_extractor.convert_spans", "bs4.BeautifulSoup", "02_extractor.convert_strings", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.convert_strings", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.split_on_tag", "bs4.BeautifulSoup", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.split_on_nested", "bs4.BeautifulSoup", "02_extractor.delete_divs", "bs4.BeautifulSoup", "02_extractor.merge_consecutive", "bs4.BeautifulSoup", "02_extractor.merge_consecutive", "bs4.BeautifulSoup", "02_extractor.merge_consecutive", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_pres", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_spans", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_strings", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.convert_strings", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_tag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.split_on_nested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.delete_divs", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.merge_consecutive", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.merge_consecutive", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.dataset.02_extractor.merge_consecutive"], ["", "def", "tests", "(", ")", ":", "\n", "    ", "html", "=", "\"<p>This</p><pre>is\\na\\nsample</pre><p>text</p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "convert_pres", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>This</p><p>is</p><p>a</p><p>sample</p><p>text</p>\"", "\n", "\n", "html", "=", "'<span class=\"underline\">Underline</span><span><span>More text</span></span>'", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "convert_spans", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "'<u class=\"underline\">Underline</u>More text'", "\n", "\n", "html", "=", "\"<body><p>Hello</p>World</body>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "convert_strings", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<body><p><p>Hello</p>World</p></body>\"", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<body><p>Hello</p><p>World</p></body>\"", "\n", "\n", "html", "=", "\"<body>Hello<p>World</p></body>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "convert_strings", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<body><p>Hello<p>World</p></p></body>\"", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<body><p>Hello</p><p>World</p></body>\"", "\n", "\n", "html", "=", "\"<p>Hello<br />World</p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "split_on_tag", "(", "soup", ",", "\"p\"", ",", "\"br\"", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>Hello</p><p>World</p>\"", "\n", "\n", "html", "=", "\"<p>Hello<p>World</p>Test</p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>Hello</p><p>World</p><p>Test</p>\"", "\n", "\n", "html", "=", "\"<p>Hello<h1>World</h1>Test</p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", ",", "\"h1\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>Hello</p><h1>World</h1><p>Test</p>\"", "\n", "\n", "html", "=", "\"<p>Hello<b><u><h1>World</h1></u></b>Test</p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", ",", "\"h1\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>Hello</p><h1><b><u>World</u></b></h1><p>Test</p>\"", "\n", "\n", "html", "=", "\"<p><h1>Hello World</h1></p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "split_on_nested", "(", "soup", ",", "[", "\"p\"", ",", "\"h1\"", "]", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<h1>Hello World</h1>\"", "\n", "\n", "html", "=", "\"<p><div>Hello World</div></p>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "delete_divs", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<p>Hello World</p>\"", "\n", "\n", "html", "=", "\"<b>Hello</b><b>World</b>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "merge_consecutive", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<b>HelloWorld</b>\"", "\n", "\n", "html", "=", "\"<b>Hello</b> <b>World</b>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "merge_consecutive", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<b>Hello World</b>\"", "\n", "\n", "html", "=", "\"<b>Hello</b>&nbsp;<b>World</b>\"", "\n", "soup", "=", "BeautifulSoup", "(", "html", ",", "\"html.parser\"", ")", "\n", "merge_consecutive", "(", "soup", ")", "\n", "assert", "str", "(", "soup", ")", "==", "\"<b>Hello\\xa0World</b>\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.01_clean_sections.extract_section": [[16, 41], ["utils.clean_title", "new_json[].append", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.clean_title"], ["def", "extract_section", "(", "paragraph", ",", "temp_title", ",", "text", ",", "level", ",", "new_json", ")", ":", "\n", "    ", "\"\"\"\n     Formulates the current section from paragraphs into a big text chunk\n    :param paragraph: Current paragraph in the terms of service\n    :param temp_title: The previous section's title. Equals None if the first section.\n    :param text: Similar to temp_title, the previous section's text.\n                 Will be appended if same section.\n    :param level: Whether it is the first or second level heading.\n    :param new_json: the new json file that we are creating with new headings\n    :return:\n    \"\"\"", "\n", "title", "=", "clean_title", "(", "paragraph", "[", "\"section\"", "]", "[", "level", "]", ",", "grouped_keys", ")", "\n", "if", "title", ":", "\n", "        ", "if", "temp_title", "is", "None", ":", "\n", "            ", "temp_title", "=", "title", "\n", "\n", "", "if", "temp_title", "==", "title", ":", "\n", "            ", "text", "=", "text", "+", "paragraph", "[", "\"text\"", "]", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "            ", "new_json", "[", "\"level\"", "+", "str", "(", "level", "+", "1", ")", "+", "\"_headings\"", "]", ".", "append", "(", "{", "\"section\"", ":", "temp_title", ",", "\n", "\"text\"", ":", "text", "[", ":", "-", "2", "]", "}", ")", "\n", "temp_title", "=", "title", "\n", "text", "=", "paragraph", "[", "\"text\"", "]", "+", "\"\\n\"", "\n", "\n", "", "", "return", "text", ",", "temp_title", ",", "new_json", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.01_clean_sections.extract_section_paragraphs": [[43, 58], ["utils.clean_title", "utils.clean_text", "new_json[].append", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.clean_title", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.clean_text"], ["", "def", "extract_section_paragraphs", "(", "paragraph", ",", "new_json", ",", "level", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Formulates the current section in to a cleaner representation with rectified labels.\n    :param paragraph: Current paragraph in the terms of service\n    :param new_json: The document where the new structure is stored for later output.\n    :param level: Whether it is the first or second level heading.\n    :return:\n    \"\"\"", "\n", "title", "=", "clean_title", "(", "paragraph", "[", "\"section\"", "]", "[", "level", "]", ",", "grouped_keys", ")", "\n", "text", "=", "clean_text", "(", "paragraph", "[", "\"text\"", "]", ")", "\n", "if", "title", "and", "text", ":", "\n", "        ", "new_json", "[", "\"level\"", "+", "str", "(", "level", "+", "1", ")", "+", "\"_headings\"", "]", ".", "append", "(", "{", "\"section\"", ":", "title", ",", "\n", "\"text\"", ":", "text", "}", ")", "\n", "\n", "", "return", "new_json", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.convert_to_masses": [[15, 27], ["masses.append", "tuple", "masses.append"], "function", ["None"], ["def", "convert_to_masses", "(", "label", ")", ":", "\n", "    ", "curr_len", "=", "1", "\n", "masses", "=", "[", "]", "\n", "for", "el", "in", "label", ":", "\n", "# If next section starts, increase id", "\n", "        ", "if", "el", "==", "0", ":", "\n", "            ", "masses", ".", "append", "(", "curr_len", ")", "\n", "curr_len", "=", "1", "\n", "", "else", ":", "\n", "            ", "curr_len", "+=", "1", "\n", "", "", "masses", ".", "append", "(", "curr_len", ")", "\n", "return", "tuple", "(", "masses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.count_mistakes": [[29, 39], ["sum", "numpy.abs"], "function", ["None"], ["", "def", "count_mistakes", "(", "label", ",", "preds", ")", ":", "\n", "    ", "\"\"\"\n\n    :param labels: Ground truth label for each section\n    :param preds: Predicted labels.\n    :return: Number of differing predictions (\"mistakes\") across document\n    \"\"\"", "\n", "# Count the differing labels", "\n", "num_mistakes", "=", "sum", "(", "np", ".", "abs", "(", "label", "-", "preds", ")", ")", "\n", "return", "num_mistakes", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.calculate_pk": [[41, 49], ["zip", "numpy.array", "print", "np.array.append", "numpy.mean", "segeval.pk", "numpy.mean"], "function", ["None"], ["", "def", "calculate_pk", "(", "preds", ",", "labels", ",", "name", "=", "\"\"", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "pred", ",", "label", "in", "zip", "(", "preds", ",", "labels", ")", ":", "\n", "        ", "res", ".", "append", "(", "segeval", ".", "pk", "(", "pred", ",", "label", ")", ")", "\n", "\n", "", "res", "=", "np", ".", "array", "(", "res", ")", "\n", "print", "(", "f\"P_k error rate of {name} is : {np.mean(res) * 100:.2f}%\"", ")", "\n", "return", "np", ".", "mean", "(", "res", ")", "*", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.load_sentence_transformers_result": [[51, 61], ["open", "f.readlines", "results.append", "numpy.array", "eval"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "load_sentence_transformers_result", "(", "fn", ")", ":", "\n", "    ", "with", "open", "(", "fn", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "# output format sucks", "\n", "        ", "results", ".", "append", "(", "np", ".", "array", "(", "eval", "(", "\"[\"", "+", "line", "+", "\"]\"", ")", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.load_transformer_pickle": [[63, 67], ["open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "load_transformer_pickle", "(", "fp", ")", ":", "\n", "    ", "with", "open", "(", "fp", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "preds", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.get_labels": [[69, 76], ["05_eval_ensemble.load_transformer_pickle"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.load_transformer_pickle"], ["", "def", "get_labels", "(", "fp", "=", "\"../resources/transformer_results/labels_bert_og_consec_1.pkl\"", ")", ":", "\n", "    ", "\"\"\"\n    Convenience wrapper around loading since results are kept separate.\n    :param fp:\n    :return: Ground truth labels of algo\n    \"\"\"", "\n", "return", "load_transformer_pickle", "(", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.get_random_baseline": [[78, 91], ["len", "numpy.random.choice", "all_rands.append", "len", "sum", "len"], "function", ["None"], ["", "def", "get_random_baseline", "(", "labels", ")", ":", "\n", "# Create random sampling baseline, knowing how many sections are in article", "\n", "    ", "all_rands", "=", "[", "]", "\n", "for", "sample", "in", "labels", ":", "\n", "        ", "curr_paragraphs", "=", "len", "(", "sample", ")", "\n", "curr_sections", "=", "len", "(", "sample", ")", "-", "sum", "(", "sample", ")", "\n", "rands", "=", "np", ".", "random", ".", "choice", "(", "[", "0", ",", "1", "]", ",", "\n", "len", "(", "sample", ")", ",", "\n", "replace", "=", "True", ",", "\n", "p", "=", "[", "curr_sections", "/", "curr_paragraphs", ",", "\n", "1", "-", "(", "curr_sections", "/", "curr_paragraphs", ")", "]", ")", "\n", "all_rands", ".", "append", "(", "rands", ")", "\n", "", "return", "all_rands", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.eval_model_type": [[93, 129], ["enumerate", "numpy.array", "enumerate", "numpy.array", "05_eval_ensemble.calculate_pk", "print", "zip", "numpy.stack", "numpy.round", "majority_preds.append", "05_eval_ensemble.count_mistakes", "np.array.append", "05_eval_ensemble.convert_to_masses", "np.array.append", "05_eval_ensemble.convert_to_masses", "05_eval_ensemble.convert_to_masses", "05_eval_ensemble.calculate_pk", "numpy.average", "numpy.mean", "numpy.std", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.calculate_pk", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.count_mistakes", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.convert_to_masses", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.convert_to_masses", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.convert_to_masses", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.05_eval_ensemble.calculate_pk"], ["", "def", "eval_model_type", "(", "same_type_preds", ",", "labels", ",", "name", "=", "\"all\"", ")", ":", "\n", "    ", "mistakes", "=", "[", "]", "\n", "majority_preds", "=", "[", "]", "\n", "for", "i", ",", "current_preds", "in", "enumerate", "(", "zip", "(", "*", "same_type_preds", ")", ")", ":", "\n", "# for j, pred in enumerate(current_preds):", "\n", "#     print(names[j], len(pred))", "\n", "# print(f\"Length of labels: {len(labels[i])}\")", "\n", "# Simulate ensemble for now", "\n", "        ", "pred", "=", "np", ".", "stack", "(", "current_preds", ")", "\n", "\n", "# Majority vote. NumPy by default rounds 0.5 to 0.", "\n", "# Offset by small amount to avoid. Note this should never happen,", "\n", "# unless we have an even number of models in the ensemble.", "\n", "majority_pred", "=", "np", ".", "round", "(", "np", ".", "average", "(", "pred", ",", "axis", "=", "0", ")", "+", "0.001", ")", "\n", "majority_preds", ".", "append", "(", "majority_pred", ")", "\n", "\n", "num_mistakes", "=", "count_mistakes", "(", "labels", "[", "i", "]", ",", "majority_pred", ")", "\n", "# print(num_mistakes, len(labels[i]))", "\n", "mistakes", ".", "append", "(", "num_mistakes", ")", "\n", "# convert so binary functions work", "\n", "", "mistakes", "=", "np", ".", "array", "(", "mistakes", ")", "\n", "label_masses", "=", "[", "convert_to_masses", "(", "label", ")", "for", "label", "in", "labels", "]", "\n", "\n", "# Convert all other models, too", "\n", "p_ks", "=", "[", "]", "\n", "for", "i", ",", "preds", "in", "enumerate", "(", "same_type_preds", ")", ":", "\n", "        ", "curr_model_masses", "=", "[", "convert_to_masses", "(", "pred", ")", "for", "pred", "in", "preds", "]", "\n", "p_ks", ".", "append", "(", "calculate_pk", "(", "curr_model_masses", ",", "label_masses", ",", "name", "+", "\"_\"", "+", "str", "(", "i", ")", ")", ")", "\n", "", "p_ks", "=", "np", ".", "array", "(", "p_ks", ")", "\n", "\n", "ensemble_masses", "=", "[", "convert_to_masses", "(", "pred", ")", "for", "pred", "in", "majority_preds", "]", "\n", "calculate_pk", "(", "ensemble_masses", ",", "label_masses", ",", "\"ensemble-\"", "+", "name", ")", "\n", "print", "(", "f\"P_k avg error rate of {name} is : {np.mean(p_ks):.2f}% +/- \"", "\n", "f\"{np.std(p_ks):.2f}\"", ")", "\n", "\n", "return", "mistakes", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.__init__": [[20, 35], ["02_generate_training_pairs.Doc.set_tokenizer", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.set_tokenizer"], ["    ", "def", "__init__", "(", "self", ",", "storage_method", ",", "force_shorten", ",", "data_dir", ",", "tokenizer_path", ")", ":", "\n", "        ", "self", ".", "all_lens", "=", "{", "}", "\n", "self", ".", "num_labels", "=", "None", "\n", "self", ".", "final_data", "=", "[", "]", "\n", "self", ".", "final_triplet_data", "=", "[", "]", "\n", "self", ".", "tokenizer_path", "=", "tokenizer_path", "\n", "\n", "self", ".", "storage_method", "=", "storage_method", "\n", "self", ".", "force_shorten", "=", "force_shorten", "\n", "self", ".", "tokenizer", "=", "None", "\n", "self", ".", "set_tokenizer", "(", ")", "\n", "\n", "# Create data directory", "\n", "os", ".", "makedirs", "(", "data_dir", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.set_tokenizer": [[36, 49], ["transformers.BertTokenizer.from_pretrained", "transformers.RobertaTokenizer.from_pretrained", "tokenizers.SentencePieceBPETokenizer", "ValueError", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "set_tokenizer", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "storage_method", "==", "\"raw\"", ":", "\n", "            ", "pass", "# Essentially keep it None. Important for exceptions", "\n", "", "elif", "self", ".", "storage_method", "==", "\"bert\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "", "elif", "self", ".", "storage_method", "==", "\"roberta\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "\"roberta-base\"", ")", "\n", "", "elif", "self", ".", "storage_method", "==", "\"token\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "SentencePieceBPETokenizer", "(", "os", ".", "path", ".", "join", "(", "self", ".", "tokenizer_path", ",", "\"/vocab.json\"", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "tokenizer_path", ",", "\"merges.txt\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown storage method encountered!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.transform_text_accordingly": [[50, 67], ["02_generate_training_pairs.Doc.tokenizer.encode", "02_generate_training_pairs.Doc.tokenizer.encode", "len", "len"], "methods", ["None"], ["", "", "def", "transform_text_accordingly", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "storage_method", "==", "\"raw\"", ":", "\n", "            ", "return", "text", "\n", "", "elif", "self", ".", "storage_method", "==", "\"bert\"", "or", "self", ".", "storage_method", "==", "\"roberta\"", ":", "\n", "            ", "encoded_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", "\n", "# Shorten if necessary", "\n", "if", "self", ".", "force_shorten", "and", "len", "(", "encoded_text", ")", ">", "512", ":", "\n", "# Still need the last token", "\n", "                ", "return", "encoded_text", "[", ":", "511", "]", "+", "[", "encoded_text", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "                ", "return", "encoded_text", "\n", "", "", "else", ":", "# the case for our own embedding", "\n", "            ", "encoded_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", ".", "ids", "\n", "if", "self", ".", "force_shorten", "and", "len", "(", "encoded_text", ")", ">", "512", ":", "\n", "                ", "return", "encoded_text", "[", ":", "512", "]", "# Own encoding has no special symbols", "\n", "", "else", ":", "\n", "                ", "return", "encoded_text", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc._add_samples_triplet_loss": [[68, 72], ["02_generate_training_pairs.Doc.final_triplet_data.append"], "methods", ["None"], ["", "", "", "def", "_add_samples_triplet_loss", "(", "self", ",", "section_pos", ",", "section_neg", ",", "section", ")", ":", "\n", "        ", "self", ".", "final_triplet_data", ".", "append", "(", "{", "\"section_center\"", ":", "section", ",", "\n", "\"section_pos\"", ":", "section_pos", ",", "\n", "\"section_neg\"", ":", "section_neg", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_positive_samples": [[73, 76], ["02_generate_training_pairs.Doc._add_samples"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments._add_samples"], ["", "def", "generate_positive_samples", "(", "self", ",", "label", ",", "section", ",", "doc", ")", ":", "\n", "        ", "second_section", "=", "self", ".", "_add_samples", "(", "label", ",", "section", ",", "doc", ",", "1", ")", "\n", "return", "second_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_negative_samples": [[77, 84], ["02_generate_training_pairs.Doc._add_samples", "random.choice", "list", "doc.keys"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments._add_samples"], ["", "def", "generate_negative_samples", "(", "self", ",", "label", ",", "section", ",", "doc", ")", ":", "\n", "# choose a random label as negative", "\n", "        ", "rand_neg_label", "=", "label", "\n", "while", "rand_neg_label", "==", "label", ":", "\n", "            ", "rand_neg_label", "=", "random", ".", "choice", "(", "list", "(", "doc", ".", "keys", "(", ")", ")", ")", "\n", "", "second_section", "=", "self", ".", "_add_samples", "(", "rand_neg_label", ",", "section", ",", "doc", ",", "0", ")", "\n", "return", "second_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents.__init__": [[87, 111], ["02_generate_training_pairs.Doc.__init__", "zip", "02_generate_training_pairs.Documents.compute_lens", "list", "02_generate_training_pairs.Documents.add_to_section", "02_generate_training_pairs.Documents.all_docs.keys"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.__init__", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents.compute_lens", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "labels", ",", "storage_method", "=", "\"raw\"", ",", "force_shorten", "=", "True", ",", "\n", "data_dir", "=", "\"./data\"", ",", "tokenizer_path", "=", "\"./\"", ")", ":", "\n", "        ", "\"\"\"\n        Essentially bucket-sort all inputs according to their labels.\n        Then we can simply \"draw\" from the correct bucket\n        :param inputs: str, Input texts\n        :param labels: str, Corresponding labels\n        :param storage_method: Either \"raw\" (remove \\n and \\t),\n                               \"bert\" (BERT tokenization),\n                               \"roberta\" (RoBERTa tokenization), or\n                               \"token\" (own tokenizer)\n        :param force_shorten: Force a shortening for BERT-based models to fit.\n        :param data_dir: where to store the output\n        :param tokenizer_path: where to find the sentence piece tokenizer\n\n        \"\"\"", "\n", "Doc", ".", "__init__", "(", "self", ",", "storage_method", ",", "force_shorten", ",", "data_dir", ",", "tokenizer_path", ")", "\n", "self", ".", "all_docs", "=", "{", "}", "\n", "\n", "for", "text", ",", "label", "in", "zip", "(", "inputs", ",", "labels", ")", ":", "\n", "            ", "self", ".", "add_to_section", "(", "text", ",", "label", ")", "\n", "", "self", ".", "compute_lens", "(", ")", "\n", "\n", "self", ".", "all_labels", "=", "list", "(", "self", ".", "all_docs", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents._add_samples": [[112, 117], ["random.randint", "02_generate_training_pairs.Documents.final_data.append"], "methods", ["None"], ["", "def", "_add_samples", "(", "self", ",", "label", ",", "section", ",", "value", ")", ":", "\n", "        ", "rand_pos", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "all_lens", "[", "label", "]", "-", "1", ")", "\n", "second_section", "=", "self", ".", "all_docs", "[", "label", "]", "[", "rand_pos", "]", "\n", "self", ".", "final_data", ".", "append", "(", "{", "\"section_1\"", ":", "section", ",", "\"section_2\"", ":", "second_section", ",", "\"label\"", ":", "value", "}", ")", "\n", "return", "second_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents.add_to_section": [[118, 125], ["02_generate_training_pairs.Documents.transform_text_accordingly", "02_generate_training_pairs.Documents.all_docs[].append"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.transform_text_accordingly"], ["", "def", "add_to_section", "(", "self", ",", "section_text", ",", "section_label", ")", ":", "\n", "        ", "processed_text", "=", "self", ".", "transform_text_accordingly", "(", "section_text", ")", "\n", "\n", "if", "section_label", "in", "self", ".", "all_docs", ":", "\n", "            ", "self", ".", "all_docs", "[", "section_label", "]", ".", "append", "(", "processed_text", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "all_docs", "[", "section_label", "]", "=", "[", "processed_text", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents.compute_lens": [[126, 131], ["02_generate_training_pairs.Documents.all_docs.items", "len", "len"], "methods", ["None"], ["", "", "def", "compute_lens", "(", "self", ")", ":", "\n", "# keep all the lens for random generation later on", "\n", "        ", "for", "label", ",", "text", "in", "self", ".", "all_docs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "all_lens", "[", "label", "]", "=", "len", "(", "self", ".", "all_docs", "[", "label", "]", ")", "\n", "", "self", ".", "num_labels", "=", "len", "(", "self", ".", "all_docs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Documents.generate_data": [[132, 154], ["print", "tqdm.tqdm.tqdm", "print", "print", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "02_generate_training_pairs.Documents.all_docs.items", "os.path.join", "os.path.join", "range", "02_generate_training_pairs.Documents.generate_positive_samples", "02_generate_training_pairs.Documents.generate_negative_samples", "02_generate_training_pairs.Documents._add_samples_triplet_loss", "len"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_positive_samples", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_negative_samples", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler._add_samples_triplet_loss"], ["", "def", "generate_data", "(", "self", ",", "sample_number", ",", "file_name", ")", ":", "\n", "        ", "\"\"\"\n        :param sample_number: Number of positive/negative samples, respectively.\n        :param file_name: Storage file name (train, test, dev)\n        :return: None\n        \"\"\"", "\n", "print", "(", "f\"Generating {file_name} data...\"", ")", "\n", "for", "label", ",", "sections", "in", "tqdm", "(", "self", ".", "all_docs", ".", "items", "(", ")", ")", ":", "\n", "            ", "for", "section", "in", "sections", ":", "\n", "                ", "for", "i", "in", "range", "(", "sample_number", ")", ":", "\n", "                    ", "section_pos", "=", "self", ".", "generate_positive_samples", "(", "label", ",", "section", ")", "\n", "\n", "section_neg", "=", "self", ".", "generate_negative_samples", "(", "label", ",", "section", ")", "\n", "\n", "self", ".", "_add_samples_triplet_loss", "(", "section", ",", "section_pos", ",", "section_neg", ")", "\n", "\n", "", "", "", "print", "(", "f\"{len(self.final_data)} sample generated.\"", ")", "\n", "print", "(", "\"writing data to tsv file...\"", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "final_data", ",", "columns", "=", "[", "\"section_1\"", ",", "\"section_2\"", ",", "\"label\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "final_triplet_data", ",", "columns", "=", "[", "\"section_center\"", ",", "\"section_pos\"", ",", "\"section_neg\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'_triplet.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments.__init__": [[157, 187], ["02_generate_training_pairs.Doc.__init__", "tqdm.tqdm.tqdm", "02_generate_training_pairs.ConsecutiveDocuments.all_docs.append", "open", "json.load", "02_generate_training_pairs.ConsecutiveDocuments.add_to_section", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.__init__", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section"], ["    ", "def", "__init__", "(", "self", ",", "files", ",", "folder", ",", "storage_method", "=", "\"raw\"", ",", "force_shorten", "=", "True", ",", "\n", "data_dir", "=", "\"./data_og_consecutive\"", ",", "tokenizer_path", "=", "\"./\"", ")", ":", "\n", "        ", "\"\"\"\n        Make a consecutive document structure and draw samples where the positive\n        examples come from the paragraphs of the same section in text and\n        negative examples from different sections\n        :param files: files to process\n        :param storage_method: Either \"raw\" (remove \\n and \\t),\n                               \"bert\" (BERT tokenization),\n                               \"roberta\" (RoBERTa tokenization), or\n                               \"token\" (own tokenizer)\n        :param force_shorten: BERT-based models have length limitation.\n        :param data_dir: where to store the output\n        :param tokenizer_path: where to find the sentence piece tokenizer\n\n        \"\"\"", "\n", "Doc", ".", "__init__", "(", "self", ",", "storage_method", ",", "force_shorten", ",", "data_dir", ",", "tokenizer_path", ")", "\n", "self", ".", "all_docs", "=", "[", "]", "\n", "\n", "for", "f", "in", "tqdm", "(", "files", ")", ":", "\n", "            ", "doc", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "                ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "", "for", "section", "in", "tos", ":", "\n", "# Transform dict into X/y sample", "\n", "                ", "text", "=", "section", "[", "\"Text\"", "]", "\n", "label", "=", "section", "[", "\"Section\"", "]", "\n", "doc", "=", "self", ".", "add_to_section", "(", "text", ",", "label", ",", "doc", ")", "\n", "\n", "", "self", ".", "all_docs", ".", "append", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments.add_to_section": [[188, 196], ["02_generate_training_pairs.ConsecutiveDocuments.transform_text_accordingly", "doc[].append"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.transform_text_accordingly"], ["", "", "def", "add_to_section", "(", "self", ",", "section_text", ",", "section_label", ",", "doc", ")", ":", "\n", "        ", "processed_text", "=", "self", ".", "transform_text_accordingly", "(", "section_text", ")", "\n", "\n", "if", "section_label", "in", "doc", ":", "\n", "            ", "doc", "[", "section_label", "]", ".", "append", "(", "processed_text", ")", "\n", "", "else", ":", "\n", "            ", "doc", "[", "section_label", "]", "=", "[", "processed_text", "]", "\n", "", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments._add_samples": [[197, 204], ["random.randint", "02_generate_training_pairs.ConsecutiveDocuments.final_data.append", "len"], "methods", ["None"], ["", "def", "_add_samples", "(", "self", ",", "label", ",", "section", ",", "doc", ",", "value", ")", ":", "\n", "        ", "rand_pos", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "doc", "[", "label", "]", ")", "-", "1", ")", "\n", "second_section", "=", "doc", "[", "label", "]", "[", "rand_pos", "]", "\n", "self", ".", "final_data", ".", "append", "(", "{", "\"section_1\"", ":", "section", ",", "\n", "\"section_2\"", ":", "second_section", ",", "\n", "\"label\"", ":", "value", "}", ")", "\n", "return", "second_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.ConsecutiveDocuments.generate_data": [[205, 241], ["print", "tqdm.tqdm.tqdm", "print", "print", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "doc.items", "os.path.join", "os.path.join", "len", "len", "len", "range", "02_generate_training_pairs.ConsecutiveDocuments.generate_positive_samples", "02_generate_training_pairs.ConsecutiveDocuments.generate_negative_samples", "02_generate_training_pairs.ConsecutiveDocuments._add_samples_triplet_loss"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_positive_samples", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.Doc.generate_negative_samples", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler._add_samples_triplet_loss"], ["", "def", "generate_data", "(", "self", ",", "sample_number", ",", "file_name", ")", ":", "\n", "        ", "\"\"\"\n        :param sample_number: Number of positive/negative samples, respectively.\n        :param file_name: Storage file name (train, test, dev)\n        :return: None\n        \"\"\"", "\n", "docs_skipped", "=", "0", "\n", "sections_skipped", "=", "0", "\n", "print", "(", "f\"Generating {file_name} data...\"", ")", "\n", "for", "doc", "in", "tqdm", "(", "self", ".", "all_docs", ")", ":", "\n", "            ", "for", "label", ",", "sections", "in", "doc", ".", "items", "(", ")", ":", "\n", "                ", "if", "len", "(", "doc", ")", "<", "2", ":", "\n", "                    ", "docs_skipped", "+=", "1", "\n", "continue", "\n", "", "for", "section", "in", "sections", ":", "\n", "                    ", "if", "len", "(", "sections", ")", ">=", "sample_number", ":", "\n", "                        ", "for", "i", "in", "range", "(", "sample_number", ")", ":", "\n", "                            ", "section_pos", "=", "self", ".", "generate_positive_samples", "(", "label", ",", "section", ",", "doc", ")", "\n", "\n", "section_neg", "=", "self", ".", "generate_negative_samples", "(", "label", ",", "section", ",", "doc", ")", "\n", "\n", "self", ".", "_add_samples_triplet_loss", "(", "section", ",", "section_pos", ",", "section_neg", ")", "\n", "", "", "else", ":", "\n", "                        ", "sections_skipped", "+=", "1", "\n", "continue", "\n", "\n", "", "", "", "", "print", "(", "f\"{len(self.final_data)} sample generated. \"", "\n", "f\"{docs_skipped} docs skipped, {sections_skipped} sections skipped! \"", ")", "\n", "print", "(", "\"writing data to tsv file...\"", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "final_data", ",", "columns", "=", "[", "\"section_1\"", ",", "\"section_2\"", ",", "\"label\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'_consecutive.tsv'", ")", ",", "\n", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "final_triplet_data", ",", "\n", "columns", "=", "[", "\"section_center\"", ",", "\"section_pos\"", ",", "\"section_neg\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'_consecutive_triplet.tsv'", ")", ",", "\n", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.02_generate_training_pairs.load_files": [[243, 259], ["tqdm.tqdm", "open", "json.load", "inputs.append", "labels.append", "os.path.join"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "", "def", "load_files", "(", "files", ",", "folder", ")", ":", "\n", "    ", "inputs", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "f", "in", "tqdm", "(", "files", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "            ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "\n", "", "for", "section", "in", "tos", ":", "\n", "# Transform dict into X/y sample", "\n", "            ", "text", "=", "section", "[", "\"Text\"", "]", "\n", "label", "=", "section", "[", "\"Section\"", "]", "\n", "\n", "inputs", ".", "append", "(", "text", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "\n", "", "", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.flip_dict": [[195, 202], ["d.items"], "function", ["None"], ["def", "flip_dict", "(", "d", ")", ":", "\n", "    ", "flipped_d", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "d", ".", "items", "(", ")", ":", "\n", "        ", "for", "alternative", "in", "val", ":", "\n", "            ", "flipped_d", "[", "alternative", "]", "=", "key", "\n", "\n", "", "", "return", "flipped_d", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.clean_title": [[204, 212], ["re.compile", "title.lower().strip", "re.sub", "title.lower"], "function", ["None"], ["", "def", "clean_title", "(", "title", ",", "grouped_keys", ")", ":", "\n", "    ", "section_pattern", "=", "re", ".", "compile", "(", "r\"section [0-9]{1,2} [-\u2013] \"", ")", "\n", "clean_title", "=", "title", ".", "lower", "(", ")", ".", "strip", "(", "\":.,;'\\\"!?0123456789\"", ")", "\n", "clean_title", "=", "re", ".", "sub", "(", "section_pattern", ",", "\"\"", ",", "clean_title", ")", "\n", "if", "clean_title", "in", "grouped_keys", ":", "\n", "        ", "return", "grouped_keys", "[", "clean_title", "]", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.get_section_pattern": [[214, 217], ["functools.lru_cache", "re.compile"], "function", ["None"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "1", ")", "\n", "def", "get_section_pattern", "(", ")", ":", "\n", "    ", "return", "re", ".", "compile", "(", "r\"section [0-9]{1,2} [-\u2013] \"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.clean_text": [[219, 229], ["utils.get_text_pattern", "re.sub", "text.strip.strip", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.get_text_pattern"], ["", "def", "clean_text", "(", "text", ",", "min_length", "=", "20", ")", ":", "\n", "    ", "text_pattern", "=", "get_text_pattern", "(", ")", "\n", "text", "=", "re", ".", "sub", "(", "text_pattern", ",", "\" \"", ",", "text", ")", "\n", "# Order is important to make sure that empty strings would be ignored", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "\n", "if", "len", "(", "text", ")", ">=", "min_length", ":", "\n", "        ", "return", "text", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.utils.get_text_pattern": [[231, 235], ["functools.lru_cache", "re.compile"], "function", ["None"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "1", ")", "\n", "def", "get_text_pattern", "(", ")", ":", "\n", "# Replacing NULL byte, utf-8 space, and anything that could break output formatting.", "\n", "    ", "return", "re", ".", "compile", "(", "r\"[\\x00\\xa0\\t\\n]\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.__init__": [[20, 61], ["SameDocumentSampler.SameDocumentSampler.set_tokenizer", "os.makedirs", "tqdm.tqdm.tqdm", "SameDocumentSampler.SameDocumentSampler.docs.append", "open", "json.load", "SameDocumentSampler.SameDocumentSampler.add_to_section", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.set_tokenizer", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section"], ["    ", "def", "__init__", "(", "self", ",", "files", ",", "folder", ",", "storage_method", "=", "\"raw\"", ",", "force_shorten", "=", "True", ",", "\n", "data_dir", "=", "\"./data_og_consecutive\"", ",", "tokenizer_path", "=", "\"./\"", ")", ":", "\n", "        ", "\"\"\"\n        Make a consecutive document structure and draw samples where the positive\n        examples come from the paragraphs of the same section in text and\n        negative examples from different sections\n        :param files: files to process\n        :param storage_method: Either \"raw\" (remove \\n and \\t),\n                               \"bert\" (BERT tokenization),\n                               \"roberta\" (RoBERTa tokenization), or\n                               \"token\" (own tokenizer)\n        :param force_shorten: BERT-based models have length limitation.\n        :param data_dir: where to store the output\n        :param tokenizer_path: where to find the sentence piece tokenizer\n\n        \"\"\"", "\n", "self", ".", "training_pairs", "=", "[", "]", "\n", "self", ".", "training_triplets", "=", "[", "]", "\n", "self", ".", "tokenizer_path", "=", "tokenizer_path", "\n", "\n", "self", ".", "storage_method", "=", "storage_method", "\n", "self", ".", "force_shorten", "=", "force_shorten", "\n", "self", ".", "tokenizer", "=", "None", "\n", "self", ".", "set_tokenizer", "(", ")", "\n", "\n", "# Create data directory", "\n", "os", ".", "makedirs", "(", "data_dir", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "docs", "=", "[", "]", "\n", "\n", "for", "f", "in", "tqdm", "(", "files", ")", ":", "\n", "            ", "doc", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "                ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "", "for", "section", "in", "tos", ":", "\n", "# Transform dict into X/y sample", "\n", "                ", "text", "=", "section", "[", "\"Text\"", "]", "\n", "label", "=", "section", "[", "\"Section\"", "]", "\n", "doc", "=", "self", ".", "add_to_section", "(", "text", ",", "label", ",", "doc", ")", "\n", "\n", "", "self", ".", "docs", ".", "append", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.set_tokenizer": [[62, 74], ["transformers.BertTokenizer.from_pretrained", "transformers.RobertaTokenizer.from_pretrained", "tokenizers.SentencePieceBPETokenizer", "ValueError", "os.path.join", "os.path.join"], "methods", ["None"], ["", "", "def", "set_tokenizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "storage_method", "==", "\"raw\"", ":", "\n", "            ", "pass", "# Essentially keep it None. Important for exceptions", "\n", "", "elif", "self", ".", "storage_method", "==", "\"bert\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "", "elif", "self", ".", "storage_method", "==", "\"roberta\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "\"roberta-base\"", ")", "\n", "", "elif", "self", ".", "storage_method", "==", "\"token\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "SentencePieceBPETokenizer", "(", "os", ".", "path", ".", "join", "(", "self", ".", "tokenizer_path", ",", "\"/vocab.json\"", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "tokenizer_path", ",", "\"merges.txt\"", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown storage method encountered!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.add_to_section": [[75, 83], ["SameDocumentSampler.SameDocumentSampler.transform_text_accordingly", "doc[].append"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.transform_text_accordingly"], ["", "", "def", "add_to_section", "(", "self", ",", "section_text", ",", "section_heading", ",", "doc", ")", ":", "\n", "        ", "processed_text", "=", "self", ".", "transform_text_accordingly", "(", "section_text", ")", "\n", "\n", "if", "section_heading", "in", "doc", ":", "\n", "            ", "doc", "[", "section_heading", "]", ".", "append", "(", "processed_text", ")", "\n", "", "else", ":", "\n", "            ", "doc", "[", "section_heading", "]", "=", "[", "processed_text", "]", "\n", "", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.transform_text_accordingly": [[84, 102], ["SameDocumentSampler.SameDocumentSampler.tokenizer.encode", "SameDocumentSampler.SameDocumentSampler.tokenizer.encode", "len", "len"], "methods", ["None"], ["", "def", "transform_text_accordingly", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "storage_method", "==", "\"raw\"", ":", "\n", "            ", "return", "text", "\n", "", "elif", "self", ".", "storage_method", "==", "\"bert\"", "or", "self", ".", "storage_method", "==", "\"roberta\"", ":", "\n", "            ", "encoded_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", "\n", "# Shorten if necessary", "\n", "if", "self", ".", "force_shorten", "and", "len", "(", "encoded_text", ")", ">", "512", ":", "\n", "# Still need the last token", "\n", "                ", "return", "encoded_text", "[", ":", "511", "]", "+", "[", "encoded_text", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "                ", "return", "encoded_text", "\n", "", "", "else", ":", "# the case for custom tokenizer", "\n", "# TODO: Currently doesn't support longer input formats due to the hard-coded cutoff", "\n", "            ", "encoded_text", "=", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", ".", "ids", "\n", "if", "self", ".", "force_shorten", "and", "len", "(", "encoded_text", ")", ">", "512", ":", "\n", "                ", "return", "encoded_text", "[", ":", "512", "]", "# Own encoding has no special symbols", "\n", "", "else", ":", "\n", "                ", "return", "encoded_text", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.generate_data": [[103, 149], ["print", "tqdm.tqdm.tqdm", "print", "print", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "doc.items", "os.path.join", "os.path.join", "len", "list", "list.remove", "numpy.random.choice", "len", "min", "SameDocumentSampler.SameDocumentSampler.training_pairs.append", "SameDocumentSampler.SameDocumentSampler.generate_negative_sample", "SameDocumentSampler.SameDocumentSampler._add_samples_triplet_loss", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.generate_negative_sample", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler._add_samples_triplet_loss"], ["", "", "", "def", "generate_data", "(", "self", ",", "sample_number", ",", "file_name", ")", ":", "\n", "        ", "\"\"\"\n        :param sample_number: Number of positive/negative samples, respectively.\n            A document needs to have at least two sections, each with num_samples samples.\n        :param file_name: Storage file name (train, test, dev)\n        :return: None\n        \"\"\"", "\n", "docs_skipped", "=", "0", "\n", "print", "(", "f\"Generating {file_name} data...\"", ")", "\n", "for", "doc", "in", "tqdm", "(", "self", ".", "docs", ")", ":", "\n", "            ", "for", "heading", ",", "sections", "in", "doc", ".", "items", "(", ")", ":", "\n", "                ", "if", "len", "(", "doc", ")", "<", "2", ":", "\n", "                    ", "docs_skipped", "+=", "1", "\n", "continue", "\n", "", "for", "section", "in", "sections", ":", "\n", "# Ensure we only sample other paragraphs in the same section", "\n", "                    ", "other_sections", "=", "list", "(", "sections", ")", "\n", "other_sections", ".", "remove", "(", "section", ")", "\n", "\n", "# Limits the number of samples to either sample_number or the number of available other paragraphs", "\n", "# in the same section. This avoids \"over-sampling\", especially since we repeat this process for", "\n", "# each paragraph in the section", "\n", "# TODO: Evaluate effect of the normalization with // 2", "\n", "positive_sections", "=", "np", ".", "random", ".", "choice", "(", "other_sections", ",", "\n", "min", "(", "sample_number", ",", "len", "(", "other_sections", ")", "//", "2", ")", ",", "\n", "replace", "=", "False", ")", "\n", "\n", "for", "section_pos", "in", "positive_sections", ":", "\n", "                        ", "self", ".", "training_pairs", ".", "append", "(", "{", "\"section_1\"", ":", "section", ",", "\n", "\"section_2\"", ":", "section_pos", ",", "\n", "\"label\"", ":", "1", "}", ")", "\n", "# Generate a matching pair of a negative sample to keep the training set balanced.", "\n", "section_neg", "=", "self", ".", "generate_negative_sample", "(", "heading", ",", "section", ",", "doc", ")", "\n", "\n", "self", ".", "_add_samples_triplet_loss", "(", "section", ",", "section_pos", ",", "section_neg", ")", "\n", "\n", "", "", "", "", "print", "(", "f\"{len(self.training_pairs)} sample generated.\\n\"", "\n", "f\"{docs_skipped} docs skipped entirely ({docs_skipped/len(self.docs):.2f}% of total number of docs)\"", ")", "\n", "print", "(", "\"writing data to tsv file...\"", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "training_pairs", ",", "columns", "=", "[", "\"section_1\"", ",", "\"section_2\"", ",", "\"label\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'.tsv'", ")", ",", "\n", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "df_data", "=", "pd", ".", "DataFrame", "(", "self", ".", "training_triplets", ",", "\n", "columns", "=", "[", "\"section_center\"", ",", "\"section_pos\"", ",", "\"section_neg\"", "]", ")", "\n", "df_data", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "file_name", "+", "'_triplet.tsv'", ")", ",", "\n", "sep", "=", "'\\t'", ",", "index", "=", "False", ",", "header", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler.generate_negative_sample": [[150, 161], ["random.randint", "SameDocumentSampler.SameDocumentSampler.training_pairs.append", "random.choice", "list", "len", "doc.keys"], "methods", ["None"], ["", "def", "generate_negative_sample", "(", "self", ",", "heading", ",", "section", ",", "doc", ")", ":", "\n", "# choose a random other section in the document", "\n", "        ", "rand_other_heading", "=", "heading", "\n", "while", "rand_other_heading", "==", "heading", ":", "\n", "            ", "rand_other_heading", "=", "random", ".", "choice", "(", "list", "(", "doc", ".", "keys", "(", ")", ")", ")", "\n", "", "rand_pos", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "doc", "[", "rand_other_heading", "]", ")", "-", "1", ")", "\n", "negative_section", "=", "doc", "[", "rand_other_heading", "]", "[", "rand_pos", "]", "\n", "self", ".", "training_pairs", ".", "append", "(", "{", "\"section_1\"", ":", "section", ",", "\n", "\"section_2\"", ":", "negative_section", ",", "\n", "\"label\"", ":", "0", "}", ")", "\n", "return", "negative_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.SameDocumentSampler._add_samples_triplet_loss": [[162, 166], ["SameDocumentSampler.SameDocumentSampler.training_triplets.append"], "methods", ["None"], ["", "def", "_add_samples_triplet_loss", "(", "self", ",", "section_pos", ",", "section_neg", ",", "section", ")", ":", "\n", "        ", "self", ".", "training_triplets", ".", "append", "(", "{", "\"section_center\"", ":", "section", ",", "\n", "\"section_pos\"", ":", "section_pos", ",", "\n", "\"section_neg\"", ":", "section_neg", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.our_model.SameDocumentSampler.load_files": [[168, 184], ["tqdm.tqdm", "open", "json.load", "paragraphs.append", "section_headings.append", "os.path.join"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "", "def", "load_files", "(", "files", ",", "folder", ")", ":", "\n", "    ", "paragraphs", "=", "[", "]", "\n", "section_headings", "=", "[", "]", "\n", "for", "f", "in", "tqdm", "(", "files", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "            ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "\n", "", "for", "section", "in", "tos", ":", "\n", "# Transform dict into X/y sample", "\n", "            ", "text", "=", "section", "[", "\"Text\"", "]", "\n", "title", "=", "section", "[", "\"Section\"", "]", "\n", "\n", "paragraphs", ".", "append", "(", "text", ")", "\n", "section_headings", ".", "append", "(", "title", ")", "\n", "\n", "", "", "return", "paragraphs", ",", "section_headings", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.LastUpdatedOrderedDict.__setitem__": [[42, 46], ["collections.OrderedDict.__setitem__"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.MagicWords.__setitem__"], ["def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "if", "key", "in", "self", ":", "\n", "            ", "del", "self", "[", "key", "]", "\n", "", "OrderedDict", ".", "__setitem__", "(", "self", ",", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.TextSegDataGenerator.__init__": [[49, 75], ["tqdm.tqdm.tqdm", "sorted", "04_test_textseg.LastUpdatedOrderedDict", "04_test_textseg.TextSegDataGenerator.all_docs.append", "04_test_textseg.TextSegDataGenerator.docs_names.append", "open", "json.load", "len", "section[].lower", "04_test_textseg.TextSegDataGenerator.add_to_section", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section"], ["    ", "def", "__init__", "(", "self", ",", "files", ",", "input_folder", ")", ":", "\n", "        ", "\"\"\"\n        Generates input data in the format of the textseg model, but per document basis\n        :param files: files to process\n        :param input_folder: the folder containing all the input files\n        \"\"\"", "\n", "self", ".", "all_docs", "=", "[", "]", "\n", "self", ".", "docs_names", "=", "[", "]", "\n", "\n", "# Create data directory", "\n", "\n", "for", "f", "in", "tqdm", "(", "sorted", "(", "files", ")", ")", ":", "\n", "            ", "doc", "=", "LastUpdatedOrderedDict", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "                ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "# Now equal to the test set?", "\n", "", "if", "len", "(", "tos", "[", "\"level1_headings\"", "]", ")", "<", "2", ":", "\n", "                ", "continue", "\n", "", "for", "section", "in", "tos", "[", "\"level1_headings\"", "]", ":", "\n", "# Transform dict into X/y sample", "\n", "                ", "text", "=", "section", "[", "\"text\"", "]", ".", "lower", "(", ")", "\n", "label", "=", "section", "[", "\"section\"", "]", "\n", "doc", "=", "self", ".", "add_to_section", "(", "text", ",", "label", ",", "doc", ")", "\n", "\n", "", "self", ".", "all_docs", ".", "append", "(", "doc", ")", "\n", "self", ".", "docs_names", ".", "append", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.TextSegDataGenerator.add_to_section": [[76, 84], ["None"], "methods", ["None"], ["", "", "def", "add_to_section", "(", "self", ",", "section_text", ",", "section_label", ",", "doc", ")", ":", "\n", "        ", "processed_text", "=", "section_text", "\n", "\n", "if", "section_label", "in", "doc", ":", "\n", "            ", "doc", "[", "section_label", "]", "+=", "processed_text", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "            ", "doc", "[", "section_label", "]", "=", "processed_text", "+", "\"\\n\"", "\n", "", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.TextSegDataGenerator.count_str_occurrences": [[85, 88], ["len", "string.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "count_str_occurrences", "(", "string", ",", "findStr", ")", ":", "\n", "        ", "return", "len", "(", "string", ".", "split", "(", "findStr", ")", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.TextSegDataGenerator.process_section": [[89, 112], ["textseg.text_manipulation.split_sentences", "textseg.text_manipulation.extract_sentence_words", "len", "04_test_textseg.TextSegDataGenerator.count_str_occurrences", "04_test_textseg.TextSegDataGenerator.count_str_occurrences", "section_sentences.append", "textseg.wiki_utils.get_formula_token", "textseg.wiki_utils.get_codesnipet_token"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_formula_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_codesnipet_token"], ["", "def", "process_section", "(", "self", ",", "section", ",", "id", ")", ":", "\n", "        ", "global", "num_sentences_for_avg", "\n", "global", "sum_sentences_for_avg", "\n", "num_sentences_for_avg", "=", "0", "\n", "sum_sentences_for_avg", "=", "0", "\n", "sentences", "=", "text_manipulation", ".", "split_sentences", "(", "section", ",", "id", ")", "\n", "section_sentences", "=", "[", "]", "\n", "num_sentences", "=", "0", "\n", "num_formulas", "=", "0", "\n", "num_codes", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "sentence_words", "=", "text_manipulation", ".", "extract_sentence_words", "(", "sentence", ")", "\n", "\n", "sum_sentences_for_avg", "+=", "len", "(", "sentence_words", ")", "\n", "num_sentences_for_avg", "+=", "1", "\n", "\n", "num_formulas", "+=", "self", ".", "count_str_occurrences", "(", "sentence", ",", "\n", "wiki_utils", ".", "get_formula_token", "(", ")", ")", "\n", "num_codes", "+=", "self", ".", "count_str_occurrences", "(", "sentence", ",", "\n", "wiki_utils", ".", "get_codesnipet_token", "(", ")", ")", "\n", "num_sentences", "+=", "1", "\n", "section_sentences", ".", "append", "(", "sentence", ")", "\n", "", "return", "section_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.TextSegDataGenerator.generate_data": [[113, 143], ["zip", "doc.items", "04_test_textseg.TextSegDataGenerator.process_section", "textseg.text_manipulation.extract_sentence_words", "targets.append", "len", "data.append", "section_sentences_out.append", "print", "textseg.utils.maybe_cuda", "len", "torch.tensor().reshape", "len", "torch.tensor", "textseg.text_manipulation.word_model"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_section", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.word_model"], ["", "def", "generate_data", "(", "self", ",", "word2vec", ")", ":", "\n", "        ", "\"\"\"\n        :param word2vec: the word2vec model for words\n        :return: [data]: lis of all the transformed sentences ,\n                 [targets]: list of number of sentences in each section ,\n                 section_sentences_out: list of sentences in that section,\n                 name: name of the file\n        \"\"\"", "\n", "\n", "for", "doc", ",", "name", "in", "zip", "(", "self", ".", "all_docs", ",", "self", ".", "docs_names", ")", ":", "\n", "            ", "targets", "=", "[", "]", "\n", "data", "=", "[", "]", "\n", "section_sentences_out", "=", "[", "]", "\n", "for", "(", "label", ",", "sections", ")", "in", "doc", ".", "items", "(", ")", ":", "\n", "                ", "section_sentences", "=", "self", ".", "process_section", "(", "sections", ",", "label", ")", "\n", "\n", "for", "sentence", "in", "section_sentences", ":", "\n", "                    ", "sentence_words", "=", "text_manipulation", ".", "extract_sentence_words", "(", "sentence", ",", "remove_special_tokens", "=", "False", ")", "\n", "if", "1", "<=", "len", "(", "sentence_words", ")", ":", "\n", "                        ", "data", ".", "append", "(", "maybe_cuda", "(", "\n", "torch", ".", "tensor", "(", "[", "text_manipulation", ".", "word_model", "(", "word", ",", "word2vec", ")", "\n", "for", "word", "in", "sentence_words", "]", ")", ".", "reshape", "(", "len", "(", "sentence_words", ")", ",", "300", ")", ")", ")", "\n", "section_sentences_out", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Sentence in wikipedia file is empty!'", ")", "\n", "", "", "if", "data", ":", "\n", "                    ", "targets", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "\n", "", "", "yield", "[", "data", "]", ",", "[", "targets", "]", ",", "section_sentences_out", ",", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.softmax": [[24, 29], ["numpy.max", "numpy.exp", "numpy.sum"], "function", ["None"], ["def", "softmax", "(", "x", ")", ":", "\n", "    ", "max_each_row", "=", "np", ".", "max", "(", "x", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "exps", "=", "np", ".", "exp", "(", "x", "-", "max_each_row", ")", "\n", "sums", "=", "np", ".", "sum", "(", "exps", ",", "axis", "=", "1", ",", "keepdims", "=", "True", ")", "\n", "return", "exps", "/", "sums", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.import_model": [[31, 34], ["__import__", "__import__.create"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.create"], ["", "def", "import_model", "(", "model_name", ")", ":", "\n", "    ", "module", "=", "__import__", "(", "'models.'", "+", "model_name", ",", "fromlist", "=", "[", "'models'", "]", ")", "\n", "return", "module", ".", "create", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.test": [[145, 176], ["model.eval", "tqdm.tqdm", "pbar.update", "model", "04_test_textseg.softmax", "len", "model.data.cpu().numpy", "open", "os.path.join", "fp.write", "model.data.cpu", "name.replace", "len", "fp.write", "len", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_test_textseg.softmax", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "", "", "def", "test", "(", "model", ",", "threshold", ",", "test_data", ",", "files", ",", "output_folder", ")", ":", "\n", "    ", "\"\"\"\n    generates segemenation based on the pretraind model\n    :param model: textseg model\n    :param threshold: the threshold for which the output is considered  as section break\n    :param test_data: the generator object\n    :param files: the list of all files\n    :param output_folder: where to store the files\n    :return:\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "with", "tqdm", "(", "desc", "=", "'Testing'", ",", "total", "=", "len", "(", "files", ")", ")", "as", "pbar", ":", "\n", "        ", "for", "data", ",", "target", ",", "section_sentences_out", ",", "name", "in", "test_data", ":", "\n", "            ", "pbar", ".", "update", "(", ")", "\n", "output", "=", "model", "(", "data", ")", "\n", "output_prob", "=", "softmax", "(", "output", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "output", "=", "output_prob", "[", ":", ",", "1", "]", ">", "threshold", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_folder", ",", "name", ".", "replace", "(", "\"json\"", ",", "\"txt\"", ")", ")", ",", "'wb'", ")", "as", "fp", ":", "\n", "                ", "counter", "=", "0", "\n", "for", "sen", "in", "section_sentences_out", ":", "\n", "                    ", "if", "(", "len", "(", "section_sentences_out", ")", "-", "1", ")", "==", "len", "(", "output", ")", ":", "\n", "                        ", "if", "counter", "<", "(", "len", "(", "section_sentences_out", ")", "-", "1", ")", ":", "\n", "                            ", "pred", "=", "output", "[", "counter", "]", "\n", "", "else", ":", "\n", "                            ", "pred", "=", "True", "\n", "\n", "", "", "fp", ".", "write", "(", "(", "sen", "+", "\"\\n\"", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "if", "pred", ":", "\n", "                        ", "fp", ".", "write", "(", "(", "\"==========\"", "+", "\"\\n\"", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "", "counter", "=", "counter", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.__init__": [[19, 52], ["os.makedirs", "tqdm.tqdm.tqdm", "sorted", "02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.all_docs.append", "open", "json.load", "len", "02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section"], ["    ", "def", "__init__", "(", "self", ",", "files", ",", "input_folder", ",", "data_dir", "=", "\"./data_og_textseg\"", ")", ":", "\n", "        ", "\"\"\"\n        Essentially bucket-sort all inputs according to their labels.\n        Then we can simply \"draw\" from the correct bucket\n        :param files: files to process\n        :param data_dir: where to store the output\n        :param input_folder: the folder where the files are located\n\n        \"\"\"", "\n", "# Each document is now a dictionary of sections and", "\n", "# list of paragraphs in the section, all_docs contains all the documents", "\n", "self", ".", "all_docs", "=", "[", "]", "\n", "self", ".", "all_lens", "=", "{", "}", "\n", "self", ".", "num_labels", "=", "None", "\n", "self", ".", "final_data", "=", "[", "]", "\n", "\n", "# Create data directory", "\n", "os", ".", "makedirs", "(", "data_dir", ",", "exist_ok", "=", "True", ")", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "\n", "for", "f", "in", "tqdm", "(", "sorted", "(", "files", ")", ")", ":", "\n", "            ", "doc", "=", "{", "}", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "input_folder", ",", "f", ")", ")", "as", "fp", ":", "\n", "                ", "tos", "=", "json", ".", "load", "(", "fp", ")", "\n", "# Now equal to the test set", "\n", "", "if", "len", "(", "tos", "[", "\"level1_headings\"", "]", ")", "<", "2", ":", "\n", "                ", "continue", "\n", "", "for", "section", "in", "tos", "[", "\"level1_headings\"", "]", ":", "\n", "# Transform dict into X/y sample", "\n", "                ", "text", "=", "section", "[", "\"text\"", "]", "\n", "label", "=", "section", "[", "\"section\"", "]", "\n", "doc", "=", "self", ".", "add_to_section", "(", "text", ",", "label", ",", "doc", ")", "\n", "", "self", ".", "all_docs", ".", "append", "(", "doc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.add_to_section": [[53, 62], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_to_section", "(", "section_text", ",", "section_label", ",", "doc", ")", ":", "\n", "        ", "processed_text", "=", "section_text", "\n", "\n", "if", "section_label", "in", "doc", ":", "\n", "            ", "doc", "[", "section_label", "]", "+=", "processed_text", "\n", "", "else", ":", "\n", "            ", "doc", "[", "section_label", "]", "=", "processed_text", "\n", "", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.count_str_occurrences": [[63, 66], ["len", "string.split"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "count_str_occurrences", "(", "string", ",", "findStr", ")", ":", "\n", "        ", "return", "len", "(", "string", ".", "split", "(", "findStr", ")", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.process_section": [[67, 115], ["baselines.textseg.text_manipulation.split_sentences", "baselines.textseg.text_manipulation.extract_sentence_words", "len", "02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.count_str_occurrences", "02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.count_str_occurrences", "section_sentences.append", "len", "len", "baselines.textseg.wiki_utils.get_formula_token", "baselines.textseg.wiki_utils.get_codesnipet_token", "re.findall"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_formula_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_codesnipet_token"], ["", "def", "process_section", "(", "self", ",", "section", ",", "idx", ")", ":", "\n", "        ", "global", "num_sentences_for_avg", "\n", "global", "sum_sentences_for_avg", "\n", "num_sentences_for_avg", "=", "0", "\n", "sum_sentences_for_avg", "=", "0", "\n", "sentences", "=", "text_manipulation", ".", "split_sentences", "(", "section", ",", "idx", ")", "\n", "section_sentences", "=", "[", "]", "\n", "num_sentences", "=", "0", "\n", "num_formulas", "=", "0", "\n", "num_codes", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "\n", "            ", "sentence_words", "=", "text_manipulation", ".", "extract_sentence_words", "(", "sentence", ")", "\n", "if", "len", "(", "sentence_words", ")", "<", "wiki_thresholds", ".", "min_words_in_sentence", ":", "\n", "# ignore this sentence", "\n", "                ", "continue", "\n", "", "sum_sentences_for_avg", "+=", "len", "(", "sentence_words", ")", "\n", "num_sentences_for_avg", "+=", "1", "\n", "\n", "num_formulas", "+=", "self", ".", "count_str_occurrences", "(", "sentence", ",", "\n", "wiki_utils", ".", "get_formula_token", "(", ")", ")", "\n", "num_codes", "+=", "self", ".", "count_str_occurrences", "(", "sentence", ",", "\n", "wiki_utils", ".", "get_codesnipet_token", "(", ")", ")", "\n", "num_sentences", "+=", "1", "\n", "section_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "", "valid_section", "=", "True", "\n", "error_message", "=", "None", "\n", "if", "num_sentences", "<", "wiki_thresholds", ".", "min_sentence_in_section", ":", "\n", "            ", "valid_section", "=", "False", "\n", "error_message", "=", "\"Sentence count in section is too low!\"", "\n", "\n", "", "section_text", "=", "''", ".", "join", "(", "section_sentences", ")", "\n", "if", "len", "(", "re", ".", "findall", "(", "'[a-zA-Z]'", ",", "section_text", ")", ")", "<", "wiki_thresholds", ".", "min_section_char_count", ":", "\n", "            ", "valid_section", "=", "False", "\n", "error_message", "=", "\"Char count in section is too low!\"", "\n", "\n", "", "if", "num_formulas", ">=", "wiki_thresholds", ".", "max_section_formulas_count", ":", "\n", "            ", "valid_section", "=", "False", "\n", "error_message", "=", "f\"Number of formulas in section is too high: {num_formulas}\"", "\n", "\n", "", "if", "num_codes", ">=", "wiki_thresholds", ".", "max_section_code_snipet_count", ":", "\n", "            ", "valid_section", "=", "False", "\n", "error_message", "=", "f\"Number of code snippets in section is too high: \"", "f\"{num_codes}\"", "\n", "\n", "", "return", "valid_section", ",", "section_sentences", ",", "error_message", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.generate_data": [[116, 140], ["os.makedirs", "print", "enumerate", "sorted", "tqdm.tqdm.tqdm", "os.path.join", "tqdm.tqdm.tqdm", "os.path.join", "sorted", "open", "enumerate", "open", "fp.read", "len", "os.remove", "os.path.join", "doc.items", "02_generate_training_data_textseg.ConsecutiveDocumentsTextSeg.process_section", "os.path.join", "os.path.join", "f.write", "f.write", "fp.read.strip().split", "str", "fp.read.strip", "len", "str"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_section", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "def", "generate_data", "(", "self", ",", "foldername", ")", ":", "\n", "        ", "\"\"\"\n        :param foldername: Storage folder name (train, test, dev)\n        :return: None\n        \"\"\"", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "foldername", ")", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "f\"Generating {foldername} data...\"", ")", "\n", "start_id", "=", "0", "\n", "for", "doc_id", ",", "doc", "in", "enumerate", "(", "tqdm", "(", "self", ".", "all_docs", ")", ",", "start", "=", "start_id", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "*", "[", "self", ".", "data_dir", ",", "foldername", ",", "str", "(", "doc_id", ")", "]", ")", ",", "\"wb\"", ")", "as", "f", ":", "\n", "                ", "for", "index", ",", "(", "label", ",", "sections", ")", "in", "enumerate", "(", "doc", ".", "items", "(", ")", ")", ":", "\n", "                    ", "is_valid_section", ",", "section_sentences", ",", "message", "=", "self", ".", "process_section", "(", "sections", ",", "label", ")", "\n", "if", "is_valid_section", ":", "\n", "                        ", "f", ".", "write", "(", "(", "\"========,\"", "+", "str", "(", "index", ")", "+", "\",\"", "+", "label", "+", "\".\"", "+", "\"\\n\"", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "f", ".", "write", "(", "(", "\"\\n\"", ".", "join", "(", "section_sentences", ")", "+", "\"\\n\"", ")", ".", "encode", "(", "'utf-8'", ")", ",", ")", "\n", "\n", "# after preprocessing some files will be empty, go back and remove them", "\n", "", "", "", "", "files", "=", "sorted", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "foldername", ")", ")", "\n", "for", "f", "in", "tqdm", "(", "sorted", "(", "files", ")", ")", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "*", "[", "self", ".", "data_dir", ",", "foldername", ",", "f", "]", ")", ")", "as", "fp", ":", "\n", "                ", "raw_content", "=", "fp", ".", "read", "(", ")", "\n", "sections", "=", "[", "s", "for", "s", "in", "raw_content", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "if", "len", "(", "s", ")", ">", "0", "and", "s", "!=", "\"\\n\"", "]", "\n", "", "if", "len", "(", "sections", ")", "<", "1", ":", "\n", "                ", "os", ".", "remove", "(", "os", ".", "path", ".", "join", "(", "*", "[", "self", ".", "data_dir", ",", "foldername", ",", "f", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_backtranslate_seg_results.get_labels": [[14, 34], ["open", "json.load", "len", "labels.append", "labels.append"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["def", "get_labels", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "len", "(", "data", "[", "\"level1_headings\"", "]", ")", "<", "2", ":", "\n", "        ", "return", "None", "\n", "\n", "", "labels", "=", "[", "]", "\n", "\n", "last_header", "=", "data", "[", "\"level1_headings\"", "]", "[", "0", "]", "[", "\"section\"", "]", "\n", "\n", "for", "para", "in", "data", "[", "\"level1_headings\"", "]", "[", "1", ":", "]", ":", "\n", "\n", "        ", "if", "para", "[", "\"section\"", "]", "==", "last_header", ":", "\n", "            ", "labels", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "labels", ".", "append", "(", "0", ")", "\n", "", "last_header", "=", "para", "[", "\"section\"", "]", "\n", "\n", "", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_backtranslate_seg_results.get_random_baseline": [[36, 46], ["len", "numpy.random.choice", "len", "sum", "len"], "function", ["None"], ["", "def", "get_random_baseline", "(", "labels", ")", ":", "\n", "# Create random sampling baseline, knowing how many sections are in article", "\n", "    ", "curr_paragraphs", "=", "len", "(", "labels", ")", "\n", "curr_sections", "=", "len", "(", "labels", ")", "-", "sum", "(", "labels", ")", "\n", "rands", "=", "np", ".", "random", ".", "choice", "(", "[", "0", ",", "1", "]", ",", "\n", "len", "(", "labels", ")", ",", "\n", "replace", "=", "True", ",", "\n", "p", "=", "[", "curr_sections", "/", "curr_paragraphs", ",", "\n", "1", "-", "(", "curr_sections", "/", "curr_paragraphs", ")", "]", ")", "\n", "return", "rands", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_backtranslate_seg_results.get_textseg_result": [[48, 92], ["enumerate", "open", "f.readlines", "open", "json.load", "para[].lower", "textseg_preds[].strip().lower", "binary_preds.append", "binary_preds.append", "len", "para_text.replace.replace", "ValueError", "textseg_preds[].strip", "para_text.replace.replace", "len", "len", "len", "set", "len", "[].strip"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "get_textseg_result", "(", "filename", ",", "reference", ")", ":", "\n", "    ", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "        ", "textseg_preds", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "with", "open", "(", "reference", ")", "as", "f", ":", "\n", "        ", "ref_paras", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "ref_paras", "=", "ref_paras", "[", "\"level1_headings\"", "]", "\n", "\n", "prev_pred", "=", "None", "\n", "binary_preds", "=", "[", "]", "\n", "# Last paragraph is always the same", "\n", "line_idx", "=", "0", "\n", "for", "i", ",", "para", "in", "enumerate", "(", "ref_paras", ")", ":", "\n", "        ", "para_text", "=", "para", "[", "\"text\"", "]", ".", "lower", "(", ")", "\n", "recorded_break", "=", "False", "\n", "# iterate until the next segment is no longer in the paragraph.", "\n", "while", "True", ":", "\n", "            ", "if", "line_idx", "==", "len", "(", "textseg_preds", ")", ":", "\n", "                ", "if", "i", "==", "len", "(", "ref_paras", ")", "-", "1", ":", "\n", "                    ", "break", "# We just had the last paragraph", "\n", "", "else", ":", "\n", "# It's okay if the last line is just a single character.", "\n", "                    ", "if", "len", "(", "ref_paras", "[", "-", "1", "]", "[", "\"text\"", "]", ")", "==", "1", "or", "len", "(", "set", "(", "ref_paras", "[", "-", "1", "]", "[", "\"text\"", "]", ".", "strip", "(", ")", ")", ")", "==", "1", ":", "\n", "                        ", "break", "\n", "", "raise", "ValueError", "(", "f\" {i} {len(ref_paras)} Something went wrong with indexing in file {filename}\"", ")", "\n", "", "", "textseg_para", "=", "textseg_preds", "[", "line_idx", "]", ".", "strip", "(", "\"\\n\"", ")", ".", "lower", "(", ")", "\n", "if", "textseg_para", "in", "para_text", ":", "\n", "                ", "para_text", "=", "para_text", ".", "replace", "(", "textseg_para", ",", "\"\"", ",", "1", ")", "\n", "line_idx", "+=", "1", "\n", "", "elif", "textseg_para", "==", "\"==========\"", ":", "\n", "                ", "para_text", "=", "para_text", ".", "replace", "(", "textseg_para", ",", "\"\"", ",", "1", ")", "\n", "line_idx", "+=", "1", "\n", "recorded_break", "=", "True", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "recorded_break", ":", "\n", "            ", "binary_preds", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "binary_preds", ".", "append", "(", "1", ")", "\n", "\n", "", "", "return", "binary_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.baselines.04_backtranslate_seg_results.get_graphseg_result": [[94, 136], ["enumerate", "open", "f.readlines", "open", "json.load", "graphseg_preds[].strip", "binary_preds.append", "binary_preds.append", "len", "para_text.replace.replace", "ValueError", "para_text.replace.replace", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "get_graphseg_result", "(", "filename", ",", "reference", ")", ":", "\n", "    ", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "        ", "graphseg_preds", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "with", "open", "(", "reference", ")", "as", "f", ":", "\n", "        ", "ref_paras", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "ref_paras", "=", "ref_paras", "[", "\"level1_headings\"", "]", "\n", "\n", "prev_pred", "=", "None", "\n", "binary_preds", "=", "[", "]", "\n", "# Last paragraph is always the same", "\n", "line_idx", "=", "0", "\n", "for", "i", ",", "para", "in", "enumerate", "(", "ref_paras", ")", ":", "\n", "        ", "para_text", "=", "para", "[", "\"text\"", "]", "\n", "recorded_break", "=", "False", "\n", "# iterate until the next segment is no longer in the paragraph.", "\n", "while", "True", ":", "\n", "            ", "if", "line_idx", "==", "len", "(", "graphseg_preds", ")", ":", "\n", "                ", "if", "i", "==", "len", "(", "ref_paras", ")", "-", "1", ":", "\n", "                    ", "break", "# We just had the last paragraph", "\n", "", "else", ":", "\n", "                    ", "if", "len", "(", "ref_paras", "[", "-", "1", "]", "[", "\"text\"", "]", ")", "==", "1", ":", "\n", "                        ", "break", "\n", "", "raise", "ValueError", "(", "f\" {i} {len(ref_paras)} Something went wrong with indexing\"", ")", "\n", "", "", "graphseg_para", "=", "graphseg_preds", "[", "line_idx", "]", ".", "strip", "(", "\"\\n\"", ")", "\n", "if", "graphseg_para", "in", "para_text", ":", "\n", "                ", "para_text", "=", "para_text", ".", "replace", "(", "graphseg_para", ",", "\"\"", ",", "1", ")", "\n", "line_idx", "+=", "1", "\n", "", "elif", "graphseg_para", "==", "\"==========\"", ":", "\n", "                ", "para_text", "=", "para_text", ".", "replace", "(", "graphseg_para", ",", "\"\"", ",", "1", ")", "\n", "line_idx", "+=", "1", "\n", "recorded_break", "=", "True", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "recorded_break", ":", "\n", "            ", "binary_preds", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "binary_preds", ".", "append", "(", "1", ")", "\n", "\n", "", "", "return", "binary_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.SentenceEncodingRNN.__init__": [[30, 41], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden", ",", "num_layers", ")", ":", "\n", "        ", "super", "(", "SentenceEncodingRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "hidden", "=", "hidden", "\n", "self", ".", "input_size", "=", "input_size", "\n", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "self", ".", "input_size", ",", "\n", "hidden_size", "=", "self", ".", "hidden", ",", "\n", "num_layers", "=", "self", ".", "num_layers", ",", "\n", "dropout", "=", "0", ",", "\n", "bidirectional", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.SentenceEncodingRNN.forward": [[42, 53], ["max_sentence_embedding.zero_state", "max_sentence_embedding.SentenceEncodingRNN.lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "maybe_cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "padded_output.size"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.zero_state", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "x", ".", "batch_sizes", "[", "0", "]", "\n", "s", "=", "zero_state", "(", "self", ",", "batch_size", ")", "\n", "packed_output", ",", "_", "=", "self", ".", "lstm", "(", "x", ",", "s", ")", "\n", "padded_output", ",", "lengths", "=", "pad_packed_sequence", "(", "packed_output", ")", "# (max sentence len, batch, 256) ", "\n", "\n", "maxes", "=", "Variable", "(", "maybe_cuda", "(", "torch", ".", "zeros", "(", "batch_size", ",", "padded_output", ".", "size", "(", "2", ")", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "maxes", "[", "i", ",", ":", "]", "=", "torch", ".", "max", "(", "padded_output", "[", ":", "lengths", "[", "i", "]", ",", "i", ",", ":", "]", ",", "0", ")", "[", "0", "]", "\n", "\n", "", "return", "maxes", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.__init__": [[56, 75], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.__init__"], ["    ", "def", "__init__", "(", "self", ",", "sentence_encoder", ",", "hidden", "=", "128", ",", "num_layers", "=", "2", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "sentence_encoder", "=", "sentence_encoder", "\n", "\n", "self", ".", "sentence_lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "sentence_encoder", ".", "hidden", "*", "2", ",", "\n", "hidden_size", "=", "hidden", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "batch_first", "=", "True", ",", "\n", "dropout", "=", "0", ",", "\n", "bidirectional", "=", "True", ")", "\n", "\n", "# We have two labels", "\n", "self", ".", "h2s", "=", "nn", ".", "Linear", "(", "hidden", "*", "2", ",", "2", ")", "\n", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "hidden", "=", "hidden", "\n", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad": [[77, 83], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.pad", "torch.pad", "torch.pad", "torch.pad.size", "torch.pad.view", "s.size", "maybe_cuda", "s.unsqueeze().unsqueeze", "s.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda"], ["", "def", "pad", "(", "self", ",", "s", ",", "max_length", ")", ":", "\n", "        ", "s_length", "=", "s", ".", "size", "(", ")", "[", "0", "]", "\n", "v", "=", "Variable", "(", "maybe_cuda", "(", "s", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ")", ")", "\n", "padded", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "0", ",", "0", ",", "max_length", "-", "s_length", ")", ")", "# (1, 1, max_length, 300)", "\n", "shape", "=", "padded", ".", "size", "(", ")", "\n", "return", "padded", ".", "view", "(", "shape", "[", "2", "]", ",", "1", ",", "shape", "[", "3", "]", ")", "# (max_length, 1, 300)", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad_document": [[85, 91], ["d.unsqueeze().unsqueeze", "torch.pad", "torch.pad", "torch.pad", "torch.pad.size", "torch.pad.view", "d.size", "d.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad"], ["", "def", "pad_document", "(", "self", ",", "d", ",", "max_document_length", ")", ":", "\n", "        ", "d_length", "=", "d", ".", "size", "(", ")", "[", "0", "]", "\n", "v", "=", "d", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "padded", "=", "F", ".", "pad", "(", "v", ",", "(", "0", ",", "0", ",", "0", ",", "max_document_length", "-", "d_length", ")", ")", "# (1, 1, max_length, 300)", "\n", "shape", "=", "padded", ".", "size", "(", ")", "\n", "return", "padded", ".", "view", "(", "shape", "[", "2", "]", ",", "1", ",", "shape", "[", "3", "]", ")", "# (max_length, 1, 300)", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.forward": [[92, 143], ["len", "max", "logger.debug", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "max_sentence_embedding.Model.sentence_encoder", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "max_sentence_embedding.Model.index_select", "numpy.max", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "max_sentence_embedding.Model.sentence_lstm", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "max_sentence_embedding.Model.h2s", "all_batch_sentences.extend", "sentences_per_doc.append", "numpy.argsort", "sum", "max_sentence_embedding.Model.pad", "maybe_cuda", "encoded_documents.append", "numpy.argsort", "sorted", "max_sentence_embedding.Model.pad_document", "max_sentence_embedding.zero_state", "doc_outputs.append", "len", "s.size", "s.size", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "doc.size", "unsort", "unsort"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.Model.pad_document", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.zero_state", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.unsort", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.unsort"], ["", "def", "forward", "(", "self", ",", "batch", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "\n", "sentences_per_doc", "=", "[", "]", "\n", "all_batch_sentences", "=", "[", "]", "\n", "for", "document", "in", "batch", ":", "\n", "            ", "all_batch_sentences", ".", "extend", "(", "document", ")", "\n", "sentences_per_doc", ".", "append", "(", "len", "(", "document", ")", ")", "\n", "", "lengths", "=", "[", "s", ".", "size", "(", ")", "[", "0", "]", "for", "s", "in", "all_batch_sentences", "]", "\n", "sort_order", "=", "np", ".", "argsort", "(", "lengths", ")", "[", ":", ":", "-", "1", "]", "\n", "sorted_sentences", "=", "[", "all_batch_sentences", "[", "i", "]", "for", "i", "in", "sort_order", "]", "\n", "sorted_lengths", "=", "[", "s", ".", "size", "(", ")", "[", "0", "]", "for", "s", "in", "sorted_sentences", "]", "\n", "\n", "max_length", "=", "max", "(", "lengths", ")", "\n", "logger", ".", "debug", "(", "'Num sentences: %s, max sentence length: %s'", ",", "\n", "sum", "(", "sentences_per_doc", ")", ",", "max_length", ")", "\n", "\n", "padded_sentences", "=", "[", "self", ".", "pad", "(", "s", ",", "max_length", ")", "for", "s", "in", "sorted_sentences", "]", "\n", "big_tensor", "=", "torch", ".", "cat", "(", "padded_sentences", ",", "1", ")", "# (max_length, batch size, 300)", "\n", "packed_tensor", "=", "pack_padded_sequence", "(", "big_tensor", ",", "sorted_lengths", ")", "\n", "encoded_sentences", "=", "self", ".", "sentence_encoder", "(", "packed_tensor", ")", "\n", "unsort_order", "=", "Variable", "(", "maybe_cuda", "(", "torch", ".", "LongTensor", "(", "unsort", "(", "sort_order", ")", ")", ")", ")", "\n", "unsorted_encodings", "=", "encoded_sentences", ".", "index_select", "(", "0", ",", "unsort_order", ")", "\n", "\n", "index", "=", "0", "\n", "encoded_documents", "=", "[", "]", "\n", "for", "sentences_count", "in", "sentences_per_doc", ":", "\n", "            ", "end_index", "=", "index", "+", "sentences_count", "\n", "encoded_documents", ".", "append", "(", "unsorted_encodings", "[", "index", ":", "end_index", ",", ":", "]", ")", "\n", "index", "=", "end_index", "\n", "\n", "", "doc_sizes", "=", "[", "doc", ".", "size", "(", ")", "[", "0", "]", "for", "doc", "in", "encoded_documents", "]", "\n", "max_doc_size", "=", "np", ".", "max", "(", "doc_sizes", ")", "\n", "ordered_document_idx", "=", "np", ".", "argsort", "(", "doc_sizes", ")", "[", ":", ":", "-", "1", "]", "\n", "ordered_doc_sizes", "=", "sorted", "(", "doc_sizes", ")", "[", ":", ":", "-", "1", "]", "\n", "ordered_documents", "=", "[", "encoded_documents", "[", "idx", "]", "for", "idx", "in", "ordered_document_idx", "]", "\n", "padded_docs", "=", "[", "self", ".", "pad_document", "(", "d", ",", "max_doc_size", ")", "for", "d", "in", "ordered_documents", "]", "\n", "docs_tensor", "=", "torch", ".", "cat", "(", "padded_docs", ",", "1", ")", "\n", "packed_docs", "=", "pack_padded_sequence", "(", "docs_tensor", ",", "ordered_doc_sizes", ")", "\n", "sentence_lstm_output", ",", "_", "=", "self", ".", "sentence_lstm", "(", "packed_docs", ",", "zero_state", "(", "self", ",", "batch_size", "=", "batch_size", ")", ")", "\n", "padded_x", ",", "_", "=", "pad_packed_sequence", "(", "sentence_lstm_output", ")", "# (max sentence len, batch, 256)", "\n", "\n", "doc_outputs", "=", "[", "]", "\n", "for", "i", ",", "doc_len", "in", "enumerate", "(", "ordered_doc_sizes", ")", ":", "\n", "            ", "doc_outputs", ".", "append", "(", "padded_x", "[", "0", ":", "doc_len", "-", "1", ",", "i", ",", ":", "]", ")", "# -1 to remove last prediction", "\n", "\n", "", "unsorted_doc_outputs", "=", "[", "doc_outputs", "[", "i", "]", "for", "i", "in", "unsort", "(", "ordered_document_idx", ")", "]", "\n", "sentence_outputs", "=", "torch", ".", "cat", "(", "unsorted_doc_outputs", ",", "0", ")", "\n", "\n", "x", "=", "self", ".", "h2s", "(", "sentence_outputs", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.zero_state": [[23, 27], ["torch.autograd.Variable", "torch.autograd.Variable", "maybe_cuda", "maybe_cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda"], ["def", "zero_state", "(", "module", ",", "batch_size", ")", ":", "\n", "# * 2 is for the two directions", "\n", "    ", "return", "Variable", "(", "maybe_cuda", "(", "torch", ".", "zeros", "(", "module", ".", "num_layers", "*", "2", ",", "batch_size", ",", "module", ".", "hidden", ")", ")", ")", ",", "Variable", "(", "maybe_cuda", "(", "torch", ".", "zeros", "(", "module", ".", "num_layers", "*", "2", ",", "batch_size", ",", "module", ".", "hidden", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.models.max_sentence_embedding.create": [[145, 150], ["max_sentence_embedding.SentenceEncodingRNN", "max_sentence_embedding.Model"], "function", ["None"], ["", "", "def", "create", "(", ")", ":", "\n", "    ", "sentence_encoder", "=", "SentenceEncodingRNN", "(", "input_size", "=", "300", ",", "\n", "hidden", "=", "256", ",", "\n", "num_layers", "=", "2", ")", "\n", "return", "Model", "(", "sentence_encoder", ",", "hidden", "=", "256", ",", "num_layers", "=", "2", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.parse": [[292, 307], ["wiki_extractor.Template", "wiki_extractor.findMatchingBraces", "Template.append", "Template.append", "Template.append", "wiki_extractor.TemplateText", "wiki_extractor.TemplateText", "wiki_extractor.TemplateArg"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findMatchingBraces"], ["@", "classmethod", "\n", "def", "parse", "(", "cls", ",", "body", ")", ":", "\n", "        ", "tpl", "=", "Template", "(", ")", "\n", "# we must handle nesting, s.a.", "\n", "# {{{1|{{PAGENAME}}}", "\n", "# {{{italics|{{{italic|}}}", "\n", "# {{#if:{{{{{#if:{{{nominee|}}}|nominee|candidate}}|}}}|", "\n", "#", "\n", "start", "=", "0", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "body", ",", "3", ")", ":", "\n", "            ", "tpl", ".", "append", "(", "TemplateText", "(", "body", "[", "start", ":", "s", "]", ")", ")", "\n", "tpl", ".", "append", "(", "TemplateArg", "(", "body", "[", "s", "+", "3", ":", "e", "-", "3", "]", ")", ")", "\n", "start", "=", "e", "\n", "", "tpl", ".", "append", "(", "TemplateText", "(", "body", "[", "start", ":", "]", ")", ")", "# leftover", "\n", "return", "tpl", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.subst": [[308, 330], ["tpl.subst"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.subst"], ["", "def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", "=", "0", ")", ":", "\n", "# We perform parameter substitutions recursively.", "\n", "# We also limit the maximum number of iterations to avoid too long or", "\n", "# even endless loops (in case of malformed input).", "\n", "\n", "# :see: http://meta.wikimedia.org/wiki/Help:Expansion#Distinction_between_variables.2C_parser_functions.2C_and_templates", "\n", "#", "\n", "# Parameter values are assigned to parameters in two (?) passes.", "\n", "# Therefore a parameter name in a template can depend on the value of", "\n", "# another parameter of the same template, regardless of the order in", "\n", "# which they are specified in the template call, for example, using", "\n", "# Template:ppp containing \"{{{{{{p}}}}}}\", {{ppp|p=q|q=r}} and even", "\n", "# {{ppp|q=r|p=q}} gives r, but using Template:tvvv containing", "\n", "# \"{{{{{{{{{p}}}}}}}}}\", {{tvvv|p=q|q=r|r=s}} gives s.", "\n", "\n", "# logging.debug('subst tpl (%d, %d) %s', len(extractor.frame), depth, self)", "\n", "\n", "        ", "if", "depth", ">", "extractor", ".", "maxParameterRecursionLevels", ":", "\n", "            ", "extractor", ".", "recursion_exceeded_3_errs", "+=", "1", "\n", "return", "''", "\n", "\n", "", "return", "''", ".", "join", "(", "[", "tpl", ".", "subst", "(", "params", ",", "extractor", ",", "depth", ")", "for", "tpl", "in", "self", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.__str__": [[331, 333], ["unicode"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "''", ".", "join", "(", "[", "unicode", "(", "x", ")", "for", "x", "in", "self", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateText.subst": [[338, 340], ["None"], "methods", ["None"], ["def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.__init__": [[348, 367], ["wiki_extractor.splitParts", "wiki_extractor.Template.parse", "len", "wiki_extractor.Template.parse"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.splitParts", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.parse", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.parse"], ["def", "__init__", "(", "self", ",", "parameter", ")", ":", "\n", "        ", "\"\"\"\n        :param parameter: the parts of a tplarg.\n        \"\"\"", "\n", "# the parameter name itself might contain templates, e.g.:", "\n", "#   appointe{{#if:{{{appointer14|}}}|r|d}}14|", "\n", "#   4|{{{{{subst|}}}CURRENTYEAR}}", "\n", "\n", "# any parts in a tplarg after the first (the parameter default) are", "\n", "# ignored, and an equals sign in the first part is treated as plain text.", "\n", "# logging.debug('TemplateArg %s', parameter)", "\n", "\n", "parts", "=", "splitParts", "(", "parameter", ")", "\n", "self", ".", "name", "=", "Template", ".", "parse", "(", "parts", "[", "0", "]", ")", "\n", "if", "len", "(", "parts", ")", ">", "1", ":", "\n", "# This parameter has a default value", "\n", "            ", "self", ".", "default", "=", "Template", ".", "parse", "(", "parts", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "default", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.__str__": [[368, 373], ["None"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "default", ":", "\n", "            ", "return", "'{{{%s|%s}}}'", "%", "(", "self", ".", "name", ",", "self", ".", "default", ")", "\n", "", "else", ":", "\n", "            ", "return", "'{{{%s}}}'", "%", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.subst": [[374, 392], ["wiki_extractor.TemplateArg.name.subst", "extractor.expandTemplates", "wiki_extractor.TemplateArg.default.subst", "extractor.expandTemplates"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.subst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.TemplateArg.subst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates"], ["", "", "def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", ")", ":", "\n", "        ", "\"\"\"\n        Substitute value for this argument from dict :param params:\n        Use :param extractor: to evaluate expressions for name and default.\n        Limit substitution to the maximun :param depth:.\n        \"\"\"", "\n", "# the parameter name itself might contain templates, e.g.:", "\n", "# appointe{{#if:{{{appointer14|}}}|r|d}}14|", "\n", "paramName", "=", "self", ".", "name", ".", "subst", "(", "params", ",", "extractor", ",", "depth", "+", "1", ")", "\n", "paramName", "=", "extractor", ".", "expandTemplates", "(", "paramName", ")", "\n", "res", "=", "''", "\n", "if", "paramName", "in", "params", ":", "\n", "            ", "res", "=", "params", "[", "paramName", "]", "# use parameter value specified in template invocation", "\n", "", "elif", "self", ".", "default", ":", "# use the default value", "\n", "            ", "defaultValue", "=", "self", ".", "default", ".", "subst", "(", "params", ",", "extractor", ",", "depth", "+", "1", ")", "\n", "res", "=", "extractor", ".", "expandTemplates", "(", "defaultValue", ")", "\n", "# logging.debug('subst arg %d %s -> %s' % (depth, paramName, res))", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.__init__": [[423, 438], ["wiki_extractor.MagicWords"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "id", ",", "title", ",", "lines", ")", ":", "\n", "        ", "\"\"\"\n        :param id: id of page.\n        :param title: tutle of page.\n        :param lines: a list of lines.\n        \"\"\"", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "text", "=", "''", ".", "join", "(", "lines", ")", "\n", "self", ".", "magicWords", "=", "MagicWords", "(", ")", "\n", "self", ".", "frame", "=", "[", "]", "\n", "self", ".", "recursion_exceeded_1_errs", "=", "0", "# template recursion within expandTemplates()", "\n", "self", ".", "recursion_exceeded_2_errs", "=", "0", "# template recursion within expandTemplate()", "\n", "self", ".", "recursion_exceeded_3_errs", "=", "0", "# parameter recursion", "\n", "self", ".", "template_title_errs", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract": [[439, 471], ["logging.debug", "wiki_extractor.get_url", "header.encode.encode.encode", "time.strftime", "time.strftime", "time.strftime", "time.strftime", "time.strftime", "wiki_extractor.Extractor.clean", "out.write", "wiki_extractor.compact", "out.write", "any", "out.write", "out.write", "logging.warn", "line.encode"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.get_url", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.clean", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.compact", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "def", "extract", "(", "self", ",", "out", ")", ":", "\n", "        ", "\"\"\"\n        :param out: a memory file.\n        \"\"\"", "\n", "logging", ".", "debug", "(", "\"%s\\t%s\"", ",", "self", ".", "id", ",", "self", ".", "title", ")", "\n", "\n", "url", "=", "get_url", "(", "self", ".", "id", ")", "\n", "header", "=", "'<doc id=\"%s\" url=\"%s\" title=\"%s\">\\n'", "%", "(", "self", ".", "id", ",", "url", ",", "self", ".", "title", ")", "\n", "# Separate header from text with a newline.", "\n", "header", "+=", "self", ".", "title", "+", "'\\n\\n'", "\n", "header", "=", "header", ".", "encode", "(", "'utf-8'", ")", "\n", "self", ".", "magicWords", "[", "'pagename'", "]", "=", "self", ".", "title", "\n", "self", ".", "magicWords", "[", "'fullpagename'", "]", "=", "self", ".", "title", "\n", "self", ".", "magicWords", "[", "'currentyear'", "]", "=", "time", ".", "strftime", "(", "'%Y'", ")", "\n", "self", ".", "magicWords", "[", "'currentmonth'", "]", "=", "time", ".", "strftime", "(", "'%m'", ")", "\n", "self", ".", "magicWords", "[", "'currentday'", "]", "=", "time", ".", "strftime", "(", "'%d'", ")", "\n", "self", ".", "magicWords", "[", "'currenthour'", "]", "=", "time", ".", "strftime", "(", "'%H'", ")", "\n", "self", ".", "magicWords", "[", "'currenttime'", "]", "=", "time", ".", "strftime", "(", "'%H:%M:%S'", ")", "\n", "text", "=", "self", ".", "clean", "(", ")", "\n", "footer", "=", "\"\\n</doc>\\n\"", "\n", "out", ".", "write", "(", "header", ")", "\n", "for", "line", "in", "compact", "(", "text", ")", ":", "\n", "            ", "out", ".", "write", "(", "line", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "out", ".", "write", "(", "'\\n'", ")", "\n", "", "out", ".", "write", "(", "footer", ")", "\n", "errs", "=", "(", "self", ".", "template_title_errs", ",", "\n", "self", ".", "recursion_exceeded_1_errs", ",", "\n", "self", ".", "recursion_exceeded_2_errs", ",", "\n", "self", ".", "recursion_exceeded_3_errs", ")", "\n", "if", "any", "(", "errs", ")", ":", "\n", "            ", "logging", ".", "warn", "(", "\"Template errors in article '%s' (%s): title(%d) recursion(%d, %d, %d)\"", ",", "\n", "self", ".", "title", ",", "self", ".", "id", ",", "*", "errs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.clean": [[472, 575], ["wiki_extractor.dropNested", "wiki_extractor.replaceExternalLinks", "wiki_extractor.replaceInternalLinks", "magicWordsRE.sub", "syntaxhighlight.finditer", "text.replace.replace.replace().replace", "comment.finditer", "wiki_extractor.dropSpans", "text.replace.replace.replace().replace", "text.replace.replace.replace", "spaces.sub", "dots.sub", "re.sub", "re.sub", "re.sub", "text.replace.replace.replace().replace", "wiki_extractor.Extractor.expandTemplates", "wiki_extractor.dropNested", "m.end", "wiki_extractor.unescape", "bold_italic.sub", "bold.sub", "italic.sub", "bold_italic.sub", "bold.sub", "italic_quote.sub", "italic.sub", "quote_quote.sub", "spans.append", "pattern.finditer", "left.finditer", "right.finditer", "wiki_extractor.dropNested", "wiki_extractor.unescape", "pattern.finditer", "cgi.escape", "wiki_extractor.unescape", "m.group", "text.replace.replace.replace", "spans.append", "spans.append", "spans.append", "text.replace.replace.replace", "text.replace.replace.replace", "text.replace.replace.replace", "m.start", "m.end", "match.group", "m.start", "m.end", "m.start", "m.end", "m.start", "m.end", "m.start"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropNested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.replaceExternalLinks", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.replaceInternalLinks", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropSpans", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropNested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.unescape", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropNested", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.unescape", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.unescape"], ["", "", "def", "clean", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Transforms wiki markup. If the command line flag --escapedoc is set then the text is also escaped\n        @see https://www.mediawiki.org/wiki/Help:Formatting\n        \"\"\"", "\n", "text", "=", "self", ".", "text", "\n", "self", ".", "text", "=", "''", "# save memory", "\n", "if", "Extractor", ".", "expand_templates", ":", "\n", "# expand templates", "\n", "# See: http://www.mediawiki.org/wiki/Help:Templates", "\n", "            ", "text", "=", "self", ".", "expandTemplates", "(", "text", ")", "\n", "", "else", ":", "\n", "# Drop transclusions (template, parser functions)", "\n", "            ", "text", "=", "dropNested", "(", "text", ",", "r'{{'", ",", "r'}}'", ")", "\n", "\n", "# Drop tables", "\n", "", "text", "=", "dropNested", "(", "text", ",", "r'{\\|'", ",", "r'\\|}'", ")", "\n", "\n", "# replace external links", "\n", "text", "=", "replaceExternalLinks", "(", "text", ")", "\n", "\n", "# replace internal links", "\n", "text", "=", "replaceInternalLinks", "(", "text", ")", "\n", "\n", "# drop MagicWords behavioral switches", "\n", "text", "=", "magicWordsRE", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "# ############### Process HTML ###############", "\n", "\n", "# turn into HTML, except for the content of <syntaxhighlight>", "\n", "res", "=", "''", "\n", "cur", "=", "0", "\n", "for", "m", "in", "syntaxhighlight", ".", "finditer", "(", "text", ")", ":", "\n", "            ", "res", "+=", "unescape", "(", "text", "[", "cur", ":", "m", ".", "start", "(", ")", "]", ")", "+", "m", ".", "group", "(", "1", ")", "\n", "cur", "=", "m", ".", "end", "(", ")", "\n", "", "text", "=", "res", "+", "unescape", "(", "text", "[", "cur", ":", "]", ")", "\n", "\n", "# Handle bold/italic/quote", "\n", "if", "self", ".", "toHTML", ":", "\n", "            ", "text", "=", "bold_italic", ".", "sub", "(", "r'<b>\\1</b>'", ",", "text", ")", "\n", "text", "=", "bold", ".", "sub", "(", "r'<b>\\1</b>'", ",", "text", ")", "\n", "text", "=", "italic", ".", "sub", "(", "r'<i>\\1</i>'", ",", "text", ")", "\n", "", "else", ":", "\n", "            ", "text", "=", "bold_italic", ".", "sub", "(", "r'\\1'", ",", "text", ")", "\n", "text", "=", "bold", ".", "sub", "(", "r'\\1'", ",", "text", ")", "\n", "text", "=", "italic_quote", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "text", "=", "italic", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "text", "=", "quote_quote", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "# residuals of unbalanced quotes", "\n", "", "text", "=", "text", ".", "replace", "(", "\"'''\"", ",", "''", ")", ".", "replace", "(", "\"''\"", ",", "'\"'", ")", "\n", "\n", "# Collect spans", "\n", "\n", "spans", "=", "[", "]", "\n", "# Drop HTML comments", "\n", "for", "m", "in", "comment", ".", "finditer", "(", "text", ")", ":", "\n", "            ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Drop self-closing tags", "\n", "", "for", "pattern", "in", "selfClosing_tag_patterns", ":", "\n", "            ", "for", "m", "in", "pattern", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Drop ignored tags", "\n", "", "", "for", "left", ",", "right", "in", "ignored_tag_patterns", ":", "\n", "            ", "for", "m", "in", "left", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "", "for", "m", "in", "right", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Bulk remove all spans", "\n", "", "", "text", "=", "dropSpans", "(", "spans", ",", "text", ")", "\n", "\n", "# Drop discarded elements", "\n", "for", "tag", "in", "discardElements", ":", "\n", "            ", "text", "=", "dropNested", "(", "text", ",", "r'<\\s*%s\\b[^>/]*>'", "%", "tag", ",", "r'<\\s*/\\s*%s>'", "%", "tag", ")", "\n", "\n", "", "if", "not", "self", ".", "toHTML", ":", "\n", "# Turn into text what is left (&amp;nbsp;) and <syntaxhighlight>", "\n", "            ", "text", "=", "unescape", "(", "text", ")", "\n", "\n", "# Expand placeholders", "\n", "", "for", "pattern", ",", "placeholder", "in", "placeholder_tag_patterns", ":", "\n", "            ", "index", "=", "1", "\n", "for", "match", "in", "pattern", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "text", "=", "text", ".", "replace", "(", "match", ".", "group", "(", ")", ",", "'%s'", "%", "(", "placeholder", ")", ")", "\n", "index", "+=", "1", "\n", "\n", "", "", "text", "=", "text", ".", "replace", "(", "'<<'", ",", "u'\u00c2\u00ab'", ")", ".", "replace", "(", "'>>'", ",", "u'\u00c2\u00bb'", ")", "\n", "\n", "#############################################", "\n", "\n", "# Cleanup text", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "spaces", ".", "sub", "(", "' '", ",", "text", ")", "\n", "text", "=", "dots", ".", "sub", "(", "'...'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "u' (,:\\.\\)\\]\u00c2\u00bb)'", ",", "r'\\1'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "u'(\\[\\(\u00c2\u00ab) '", ",", "r'\\1'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'\\n\\W+?\\n'", ",", "'\\n'", ",", "text", ",", "flags", "=", "re", ".", "U", ")", "# lines with only punctuations", "\n", "text", "=", "text", ".", "replace", "(", "',,'", ",", "','", ")", ".", "replace", "(", "',.'", ",", "'.'", ")", "\n", "if", "escape_doc", ":", "\n", "            ", "text", "=", "cgi", ".", "escape", "(", "text", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates": [[586, 624], ["wiki_extractor.findMatchingBraces", "len", "wiki_extractor.Extractor.expandTemplate"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findMatchingBraces", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplate"], ["def", "expandTemplates", "(", "self", ",", "wikitext", ")", ":", "\n", "        ", "\"\"\"\n        :param wikitext: the text to be expanded.\n\n        Templates are frequently nested. Occasionally, parsing mistakes may\n        cause template insertion to enter an infinite loop, for instance when\n        trying to instantiate Template:Country\n\n        {{country_{{{1}}}|{{{2}}}|{{{2}}}|size={{{size|}}}|name={{{name|}}}}}\n\n        which is repeatedly trying to insert template 'country_', which is\n        again resolved to Template:Country. The straightforward solution of\n        keeping track of templates that were already inserted for the current\n        article would not work, because the same template may legally be used\n        more than once, with different parameters in different parts of the\n        article.  Therefore, we limit the number of iterations of nested\n        template inclusion.\n\n        \"\"\"", "\n", "# Test template expansion at:", "\n", "# https://en.wikipedia.org/wiki/Special:ExpandTemplates", "\n", "\n", "res", "=", "''", "\n", "if", "len", "(", "self", ".", "frame", ")", ">=", "self", ".", "maxTemplateRecursionLevels", ":", "\n", "            ", "self", ".", "recursion_exceeded_1_errs", "+=", "1", "\n", "return", "res", "\n", "\n", "# logging.debug('<expandTemplates ' + str(len(self.frame)))", "\n", "\n", "", "cur", "=", "0", "\n", "# look for matching {{...}}", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "wikitext", ",", "2", ")", ":", "\n", "            ", "res", "+=", "wikitext", "[", "cur", ":", "s", "]", "+", "self", ".", "expandTemplate", "(", "wikitext", "[", "s", "+", "2", ":", "e", "-", "2", "]", ")", "\n", "cur", "=", "e", "\n", "# leftover", "\n", "", "res", "+=", "wikitext", "[", "cur", ":", "]", "\n", "# logging.debug('   expandTemplates> %d %s', len(self.frame), res)", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.templateParams": [[625, 692], ["logging.debug", "logging.debug", "re.match", "re.match.group().strip", "re.match.group", "templateParams.values", "parameterValue.strip.strip.strip", "param.strip.strip.strip", "re.match.group", "str"], "methods", ["None"], ["", "def", "templateParams", "(", "self", ",", "parameters", ")", ":", "\n", "        ", "\"\"\"\n        Build a dictionary with positional or name key to expanded parameters.\n        :param parameters: the parts[1:] of a template, i.e. all except the title.\n        \"\"\"", "\n", "templateParams", "=", "{", "}", "\n", "\n", "if", "not", "parameters", ":", "\n", "            ", "return", "templateParams", "\n", "", "logging", ".", "debug", "(", "'<templateParams: %s'", ",", "'|'", ".", "join", "(", "parameters", ")", ")", "\n", "\n", "# Parameters can be either named or unnamed. In the latter case, their", "\n", "# name is defined by their ordinal position (1, 2, 3, ...).", "\n", "\n", "unnamedParameterCounter", "=", "0", "\n", "\n", "# It's legal for unnamed parameters to be skipped, in which case they", "\n", "# will get default values (if available) during actual instantiation.", "\n", "# That is {{template_name|a||c}} means parameter 1 gets", "\n", "# the value 'a', parameter 2 value is not defined, and parameter 3 gets", "\n", "# the value 'c'.  This case is correctly handled by function 'split',", "\n", "# and does not require any special handling.", "\n", "for", "param", "in", "parameters", ":", "\n", "# Spaces before or after a parameter value are normally ignored,", "\n", "# UNLESS the parameter contains a link (to prevent possible gluing", "\n", "# the link to the following text after template substitution)", "\n", "\n", "# Parameter values may contain \"=\" symbols, hence the parameter", "\n", "# name extends up to the first such symbol.", "\n", "\n", "# It is legal for a parameter to be specified several times, in", "\n", "# which case the last assignment takes precedence. Example:", "\n", "# \"{{t|a|b|c|2=B}}\" is equivalent to \"{{t|a|B|c}}\".", "\n", "# Therefore, we don't check if the parameter has been assigned a", "\n", "# value before, because anyway the last assignment should override", "\n", "# any previous ones.", "\n", "# FIXME: Don't use DOTALL here since parameters may be tags with", "\n", "# attributes, e.g. <div class=\"templatequotecite\">", "\n", "# Parameters may span several lines, like:", "\n", "# {{Reflist|colwidth=30em|refs=", "\n", "# &lt;ref name=&quot;Goode&quot;&gt;Title&lt;/ref&gt;", "\n", "\n", "# The '=' might occurr within an HTML attribute:", "\n", "#   \"&lt;ref name=value\"", "\n", "# but we stop at first.", "\n", "            ", "m", "=", "re", ".", "match", "(", "' *([^= ]*?) *=(.*)'", ",", "param", ",", "re", ".", "DOTALL", ")", "\n", "if", "m", ":", "\n", "# This is a named parameter.  This case also handles parameter", "\n", "# assignments like \"2=xxx\", where the number of an unnamed", "\n", "# parameter (\"2\") is specified explicitly - this is handled", "\n", "# transparently.", "\n", "\n", "                ", "parameterName", "=", "m", ".", "group", "(", "1", ")", ".", "strip", "(", ")", "\n", "parameterValue", "=", "m", ".", "group", "(", "2", ")", "\n", "\n", "if", "']]'", "not", "in", "parameterValue", ":", "# if the value does not contain a link, trim whitespace", "\n", "                    ", "parameterValue", "=", "parameterValue", ".", "strip", "(", ")", "\n", "", "templateParams", "[", "parameterName", "]", "=", "parameterValue", "\n", "", "else", ":", "\n", "# this is an unnamed parameter", "\n", "                ", "unnamedParameterCounter", "+=", "1", "\n", "\n", "if", "']]'", "not", "in", "param", ":", "# if the value does not contain a link, trim whitespace", "\n", "                    ", "param", "=", "param", ".", "strip", "(", ")", "\n", "", "templateParams", "[", "str", "(", "unnamedParameterCounter", ")", "]", "=", "param", "\n", "", "", "logging", ".", "debug", "(", "'   templateParams> %s'", ",", "'|'", ".", "join", "(", "templateParams", ".", "values", "(", ")", ")", ")", "\n", "return", "templateParams", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplate": [[693, 847], ["logging.debug", "wiki_extractor.splitParts", "logging.debug", "wiki_extractor.Extractor.expandTemplates", "re.match", "re.sub.find", "wiki_extractor.fullyQualifiedTemplateTitle", "redirects.get", "logging.debug", "wiki_extractor.Extractor.templateParams", "wiki_extractor.Extractor.frame.append", "wiki_extractor.Template.parse", "logging.debug", "wiki_extractor.Extractor.expandTemplates", "wiki_extractor.Extractor.frame.pop", "logging.debug", "len", "len", "parts[].strip", "parts[].strip", "re.sub", "re.sub.lower", "title[].strip", "wiki_extractor.callParserFunction", "wiki_extractor.Extractor.expandTemplates", "len", "len", "wiki_extractor.Template.parse", "wiki_extractor.Extractor.expandTemplates", "re.sub.lower"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.splitParts", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.fullyQualifiedTemplateTitle", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.templateParams", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.parse", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.callParserFunction", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Template.parse", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.expandTemplates"], ["", "def", "expandTemplate", "(", "self", ",", "body", ")", ":", "\n", "        ", "\"\"\"Expands template invocation.\n        :param body: the parts of a template.\n\n        :see http://meta.wikimedia.org/wiki/Help:Expansion for an explanation\n        of the process.\n\n        See in particular: Expansion of names and values\n        http://meta.wikimedia.org/wiki/Help:Expansion#Expansion_of_names_and_values\n\n        For most parser functions all names and values are expanded,\n        regardless of what is relevant for the result. The branching functions\n        (#if, #ifeq, #iferror, #ifexist, #ifexpr, #switch) are exceptions.\n\n        All names in a template call are expanded, and the titles of the\n        tplargs in the template body, after which it is determined which\n        values must be expanded, and for which tplargs in the template body\n        the first part (default).\n\n        In the case of a tplarg, any parts beyond the first are never\n        expanded.  The possible name and the value of the first part is\n        expanded if the title does not match a name in the template call.\n\n        :see code for braceSubstitution at\n        https://doc.wikimedia.org/mediawiki-core/master/php/html/Parser_8php_source.html#3397:\n\n        \"\"\"", "\n", "\n", "# template        = \"{{\" parts \"}}\"", "\n", "\n", "# Templates and tplargs are decomposed in the same way, with pipes as", "\n", "# separator, even though eventually any parts in a tplarg after the first", "\n", "# (the parameter default) are ignored, and an equals sign in the first", "\n", "# part is treated as plain text.", "\n", "# Pipes inside inner templates and tplargs, or inside double rectangular", "\n", "# brackets within the template or tplargs are not taken into account in", "\n", "# this decomposition.", "\n", "# The first part is called title, the other parts are simply called parts.", "\n", "\n", "# If a part has one or more equals signs in it, the first equals sign", "\n", "# determines the division into name = value. Equals signs inside inner", "\n", "# templates and tplargs, or inside double rectangular brackets within the", "\n", "# part are not taken into account in this decomposition. Parts without", "\n", "# equals sign are indexed 1, 2, .., given as attribute in the <name> tag.", "\n", "\n", "if", "len", "(", "self", ".", "frame", ")", ">=", "self", ".", "maxTemplateRecursionLevels", ":", "\n", "            ", "self", ".", "recursion_exceeded_2_errs", "+=", "1", "\n", "# logging.debug('   INVOCATION> %d %s', len(self.frame), body)", "\n", "return", "''", "\n", "\n", "", "logging", ".", "debug", "(", "'INVOCATION %d %s'", ",", "len", "(", "self", ".", "frame", ")", ",", "body", ")", "\n", "\n", "parts", "=", "splitParts", "(", "body", ")", "\n", "# title is the portion before the first |", "\n", "logging", ".", "debug", "(", "'TITLE %s'", ",", "parts", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "title", "=", "self", ".", "expandTemplates", "(", "parts", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "\n", "# SUBST", "\n", "# Apply the template tag to parameters without", "\n", "# substituting into them, e.g.", "\n", "# {{subst:t|a{{{p|q}}}b}} gives the wikitext start-a{{{p|q}}}b-end", "\n", "# @see https://www.mediawiki.org/wiki/Manual:Substitution#Partial_substitution", "\n", "subst", "=", "False", "\n", "if", "re", ".", "match", "(", "substWords", ",", "title", ",", "re", ".", "IGNORECASE", ")", ":", "\n", "            ", "title", "=", "re", ".", "sub", "(", "substWords", ",", "''", ",", "title", ",", "1", ",", "re", ".", "IGNORECASE", ")", "\n", "subst", "=", "True", "\n", "\n", "", "if", "title", ".", "lower", "(", ")", "in", "self", ".", "magicWords", ".", "values", ":", "\n", "            ", "return", "self", ".", "magicWords", "[", "title", ".", "lower", "(", ")", "]", "\n", "\n", "# Parser functions", "\n", "# The first argument is everything after the first colon.", "\n", "# It has been evaluated above.", "\n", "", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "1", ":", "\n", "            ", "funct", "=", "title", "[", ":", "colon", "]", "\n", "parts", "[", "0", "]", "=", "title", "[", "colon", "+", "1", ":", "]", ".", "strip", "(", ")", "# side-effect (parts[0] not used later)", "\n", "# arguments after first are not evaluated", "\n", "ret", "=", "callParserFunction", "(", "funct", ",", "parts", ",", "self", ".", "frame", ")", "\n", "return", "self", ".", "expandTemplates", "(", "ret", ")", "\n", "\n", "", "title", "=", "fullyQualifiedTemplateTitle", "(", "title", ")", "\n", "if", "not", "title", ":", "\n", "            ", "self", ".", "template_title_errs", "+=", "1", "\n", "return", "''", "\n", "\n", "", "redirected", "=", "redirects", ".", "get", "(", "title", ")", "\n", "if", "redirected", ":", "\n", "            ", "title", "=", "redirected", "\n", "\n", "# get the template", "\n", "", "if", "title", "in", "templateCache", ":", "\n", "            ", "template", "=", "templateCache", "[", "title", "]", "\n", "", "elif", "title", "in", "templates", ":", "\n", "            ", "template", "=", "Template", ".", "parse", "(", "templates", "[", "title", "]", ")", "\n", "# add it to cache", "\n", "templateCache", "[", "title", "]", "=", "template", "\n", "del", "templates", "[", "title", "]", "\n", "", "else", ":", "\n", "# The page being included could not be identified", "\n", "            ", "return", "''", "\n", "\n", "", "logging", ".", "debug", "(", "'TEMPLATE %s: %s'", ",", "title", ",", "template", ")", "\n", "\n", "# tplarg          = \"{{{\" parts \"}}}\"", "\n", "# parts           = [ title *( \"|\" part ) ]", "\n", "# part            = ( part-name \"=\" part-value ) / ( part-value )", "\n", "# part-name       = wikitext-L3", "\n", "# part-value      = wikitext-L3", "\n", "# wikitext-L3     = literal / template / tplarg / link / comment /", "\n", "#                   line-eating-comment / unclosed-comment /", "\n", "#           \t    xmlish-element / *wikitext-L3", "\n", "\n", "# A tplarg may contain other parameters as well as templates, e.g.:", "\n", "#   {{{text|{{{quote|{{{1|{{error|Error: No text given}}}}}}}}}}}", "\n", "# hence no simple RE like this would work:", "\n", "#   '{{{((?:(?!{{{).)*?)}}}'", "\n", "# We must use full CF parsing.", "\n", "\n", "# the parameter name itself might be computed, e.g.:", "\n", "#   {{{appointe{{#if:{{{appointer14|}}}|r|d}}14|}}}", "\n", "\n", "# Because of the multiple uses of double-brace and triple-brace", "\n", "# syntax, expressions can sometimes be ambiguous.", "\n", "# Precedence rules specifed here:", "\n", "# http://www.mediawiki.org/wiki/Preprocessor_ABNF#Ideal_precedence", "\n", "# resolve ambiguities like this:", "\n", "#   {{{{ }}}} -> { {{{ }}} }", "\n", "#   {{{{{ }}}}} -> {{ {{{ }}} }}", "\n", "#", "\n", "# :see: https://en.wikipedia.org/wiki/Help:Template#Handling_parameters", "\n", "\n", "params", "=", "parts", "[", "1", ":", "]", "\n", "\n", "if", "not", "subst", ":", "\n", "# Evaluate parameters, since they may contain templates, including", "\n", "# the symbol \"=\".", "\n", "# {{#ifexpr: {{{1}}} = 1 }}", "\n", "            ", "params", "=", "[", "self", ".", "expandTemplates", "(", "p", ")", "for", "p", "in", "params", "]", "\n", "\n", "# build a dict of name-values for the parameter values", "\n", "", "params", "=", "self", ".", "templateParams", "(", "params", ")", "\n", "\n", "# Perform parameter substitution", "\n", "# extend frame before subst, since there may be recursion in default", "\n", "# parameter value, e.g. {{OTRS|celebrative|date=April 2015}} in article", "\n", "# 21637542 in enwiki.", "\n", "self", ".", "frame", ".", "append", "(", "(", "title", ",", "params", ")", ")", "\n", "instantiated", "=", "template", ".", "subst", "(", "params", ",", "self", ")", "\n", "logging", ".", "debug", "(", "'instantiated %d %s'", ",", "len", "(", "self", ".", "frame", ")", ",", "instantiated", ")", "\n", "value", "=", "self", ".", "expandTemplates", "(", "instantiated", ")", "\n", "self", ".", "frame", ".", "pop", "(", ")", "\n", "logging", ".", "debug", "(", "'   INVOCATION> %s %d %s'", ",", "title", ",", "len", "(", "self", ".", "frame", ")", ",", "value", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.MagicWords.__init__": [[1225, 1227], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "values", "=", "{", "'!'", ":", "'|'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.MagicWords.__getitem__": [[1228, 1230], ["wiki_extractor.MagicWords.values.get"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "self", ".", "values", ".", "get", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.MagicWords.__setitem__": [[1231, 1233], ["None"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "        ", "self", ".", "values", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__init__": [[1336, 1338], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "function", ")", ":", "\n", "        ", "self", ".", "function", "=", "function", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__ror__": [[1339, 1341], ["wiki_extractor.Infix", "wiki_extractor.Infix.function"], "methods", ["None"], ["", "def", "__ror__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "Infix", "(", "lambda", "x", ",", "self", "=", "self", ",", "other", "=", "other", ":", "self", ".", "function", "(", "other", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__or__": [[1342, 1344], ["wiki_extractor.Infix.function"], "methods", ["None"], ["", "def", "__or__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "other", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__rlshift__": [[1345, 1347], ["wiki_extractor.Infix", "wiki_extractor.Infix.function"], "methods", ["None"], ["", "def", "__rlshift__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "Infix", "(", "lambda", "x", ",", "self", "=", "self", ",", "other", "=", "other", ":", "self", ".", "function", "(", "other", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__rshift__": [[1348, 1350], ["wiki_extractor.Infix.function"], "methods", ["None"], ["", "def", "__rshift__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "other", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Infix.__call__": [[1351, 1353], ["wiki_extractor.Infix.function"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "value1", ",", "value2", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "value1", ",", "value2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile.__init__": [[2299, 2303], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "path_name", ")", ":", "\n", "        ", "self", ".", "path_name", "=", "path_name", "\n", "self", ".", "dir_index", "=", "-", "1", "\n", "self", ".", "file_index", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile.next": [[2304, 2312], ["wiki_extractor.NextFile._dirname", "wiki_extractor.NextFile._filepath", "os.path.isdir", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile._dirname", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile._filepath"], ["", "def", "next", "(", "self", ")", ":", "\n", "        ", "self", ".", "file_index", "=", "(", "self", ".", "file_index", "+", "1", ")", "%", "NextFile", ".", "filesPerDir", "\n", "if", "self", ".", "file_index", "==", "0", ":", "\n", "            ", "self", ".", "dir_index", "+=", "1", "\n", "", "dirname", "=", "self", ".", "_dirname", "(", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "dirname", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "dirname", ")", "\n", "", "return", "self", ".", "_filepath", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile._dirname": [[2313, 2317], ["os.path.join", "ord", "ord"], "methods", ["None"], ["", "def", "_dirname", "(", "self", ")", ":", "\n", "        ", "char1", "=", "self", ".", "dir_index", "%", "26", "\n", "char2", "=", "self", ".", "dir_index", "/", "26", "%", "26", "\n", "return", "os", ".", "path", ".", "join", "(", "self", ".", "path_name", ",", "'%c%c'", "%", "(", "ord", "(", "'A'", ")", "+", "char2", ",", "ord", "(", "'A'", ")", "+", "char1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile._filepath": [[2318, 2320], ["wiki_extractor.NextFile._dirname"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile._dirname"], ["", "def", "_filepath", "(", "self", ")", ":", "\n", "        ", "return", "'%s/wiki_%02d'", "%", "(", "self", ".", "_dirname", "(", ")", ",", "self", ".", "file_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.__init__": [[2327, 2338], ["wiki_extractor.OutputSplitter.open", "wiki_extractor.OutputSplitter.nextFile.next"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile.next"], ["def", "__init__", "(", "self", ",", "nextFile", ",", "max_file_size", "=", "0", ",", "compress", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        :param nextFile: a NextFile object from which to obtain filenames\n            to use.\n        :param max_file_size: the maximum size of each file.\n        :para compress: whether to write data with bzip compression.\n        \"\"\"", "\n", "self", ".", "nextFile", "=", "nextFile", "\n", "self", ".", "compress", "=", "compress", "\n", "self", ".", "max_file_size", "=", "max_file_size", "\n", "self", ".", "file", "=", "self", ".", "open", "(", "self", ".", "nextFile", ".", "next", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.reserve": [[2339, 2343], ["wiki_extractor.OutputSplitter.close", "wiki_extractor.OutputSplitter.open", "wiki_extractor.OutputSplitter.file.tell", "wiki_extractor.OutputSplitter.nextFile.next"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile.next"], ["", "def", "reserve", "(", "self", ",", "size", ")", ":", "\n", "        ", "if", "self", ".", "file", ".", "tell", "(", ")", "+", "size", ">", "self", ".", "max_file_size", ":", "\n", "            ", "self", ".", "close", "(", ")", "\n", "self", ".", "file", "=", "self", ".", "open", "(", "self", ".", "nextFile", ".", "next", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write": [[2344, 2347], ["wiki_extractor.OutputSplitter.reserve", "wiki_extractor.OutputSplitter.file.write", "len"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.reserve", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "", "def", "write", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "reserve", "(", "len", "(", "data", ")", ")", "\n", "self", ".", "file", ".", "write", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close": [[2348, 2350], ["wiki_extractor.OutputSplitter.file.close"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open": [[2351, 2356], ["bz2.BZ2File", "wiki_extractor.OutputSplitter.open"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "open", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "self", ".", "compress", ":", "\n", "            ", "return", "bz2", ".", "BZ2File", "(", "filename", "+", "'.bz2'", ",", "'w'", ")", "\n", "", "else", ":", "\n", "            ", "return", "open", "(", "filename", ",", "'w'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.get_url": [[118, 120], ["None"], "function", ["None"], ["def", "get_url", "(", "uid", ")", ":", "\n", "    ", "return", "\"%s?curid=%s\"", "%", "(", "urlbase", ",", "uid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.normalizeTitle": [[172, 208], ["ucfirst.strip", "re.sub", "re.match", "re.match.group", "re.match.group", "re.match.group", "wiki_extractor.normalizeNamespace", "wiki_extractor.ucfirst", "wiki_extractor.ucfirst", "wiki_extractor.ucfirst", "wiki_extractor.ucfirst"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.normalizeNamespace", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst"], ["def", "normalizeTitle", "(", "title", ")", ":", "\n", "    ", "\"\"\"Normalize title\"\"\"", "\n", "# remove leading/trailing whitespace and underscores", "\n", "title", "=", "title", ".", "strip", "(", "' _'", ")", "\n", "# replace sequences of whitespace and underscore chars with a single space", "\n", "title", "=", "re", ".", "sub", "(", "r'[\\s_]+'", ",", "' '", ",", "title", ")", "\n", "\n", "m", "=", "re", ".", "match", "(", "r'([^:]*):(\\s*)(\\S(?:.*))'", ",", "title", ")", "\n", "if", "m", ":", "\n", "        ", "prefix", "=", "m", ".", "group", "(", "1", ")", "\n", "if", "m", ".", "group", "(", "2", ")", ":", "\n", "            ", "optionalWhitespace", "=", "' '", "\n", "", "else", ":", "\n", "            ", "optionalWhitespace", "=", "''", "\n", "", "rest", "=", "m", ".", "group", "(", "3", ")", "\n", "\n", "ns", "=", "normalizeNamespace", "(", "prefix", ")", "\n", "if", "ns", "in", "knownNamespaces", ":", "\n", "# If the prefix designates a known namespace, then it might be", "\n", "# followed by optional whitespace that should be removed to get", "\n", "# the canonical page name", "\n", "# (e.g., \"Category:  Births\" should become \"Category:Births\").", "\n", "            ", "title", "=", "ns", "+", "\":\"", "+", "ucfirst", "(", "rest", ")", "\n", "", "else", ":", "\n", "# No namespace, just capitalize first letter.", "\n", "# If the part before the colon is not a known namespace, then we", "\n", "# must not remove the space after the colon (if any), e.g.,", "\n", "# \"3001: The_Final_Odyssey\" != \"3001:The_Final_Odyssey\".", "\n", "# However, to get the canonical page name we must contract multiple", "\n", "# spaces into one, because", "\n", "# \"3001:   The_Final_Odyssey\" != \"3001: The_Final_Odyssey\".", "\n", "            ", "title", "=", "ucfirst", "(", "prefix", ")", "+", "\":\"", "+", "optionalWhitespace", "+", "ucfirst", "(", "rest", ")", "\n", "", "", "else", ":", "\n", "# no namespace, just capitalize first letter", "\n", "        ", "title", "=", "ucfirst", "(", "title", ")", "\n", "", "return", "title", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.unescape": [[210, 233], ["re.sub", "m.group", "m.group", "unichr", "unichr", "unichr", "int", "int"], "function", ["None"], ["", "def", "unescape", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Removes HTML or XML character references and entities from a text string.\n\n    :param text The HTML (or XML) source text.\n    :return The plain text, as a Unicode string, if necessary.\n    \"\"\"", "\n", "\n", "def", "fixup", "(", "m", ")", ":", "\n", "        ", "text", "=", "m", ".", "group", "(", "0", ")", "\n", "code", "=", "m", ".", "group", "(", "1", ")", "\n", "try", ":", "\n", "            ", "if", "text", "[", "1", "]", "==", "\"#\"", ":", "# character reference", "\n", "                ", "if", "text", "[", "2", "]", "==", "\"x\"", ":", "\n", "                    ", "return", "unichr", "(", "int", "(", "code", "[", "1", ":", "]", ",", "16", ")", ")", "\n", "", "else", ":", "\n", "                    ", "return", "unichr", "(", "int", "(", "code", ")", ")", "\n", "", "", "else", ":", "# named entity", "\n", "                ", "return", "unichr", "(", "name2codepoint", "[", "code", "]", ")", "\n", "", "", "except", ":", "\n", "            ", "return", "text", "# leave as is", "\n", "\n", "", "", "return", "re", ".", "sub", "(", "\"&#?(\\w+);\"", ",", "fixup", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ignoreTag": [[243, 247], ["re.compile", "re.compile", "ignored_tag_patterns.append"], "function", ["None"], ["def", "ignoreTag", "(", "tag", ")", ":", "\n", "    ", "left", "=", "re", ".", "compile", "(", "r'<%s\\b.*?>'", "%", "tag", ",", "re", ".", "IGNORECASE", "|", "re", ".", "DOTALL", ")", "# both <ref> and <reference>", "\n", "right", "=", "re", ".", "compile", "(", "r'</\\s*%s>'", "%", "tag", ",", "re", ".", "IGNORECASE", ")", "\n", "ignored_tag_patterns", ".", "append", "(", "(", "left", ",", "right", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.splitParts": [[853, 923], ["wiki_extractor.findMatchingBraces", "paramsList[].split", "paramsList[].split", "len", "parameters.extend", "len", "parameters.extend"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findMatchingBraces"], ["", "", "def", "splitParts", "(", "paramsList", ")", ":", "\n", "    ", "\"\"\"\n    :param paramsList: the parts of a template or tplarg.\n\n    Split template parameters at the separator \"|\".\n    separator \"=\".\n\n    Template parameters often contain URLs, internal links, text or even\n    template expressions, since we evaluate templates outside in.\n    This is required for cases like:\n      {{#if: {{{1}}} | {{lc:{{{1}}} | \"parameter missing\"}}\n    Parameters are separated by \"|\" symbols. However, we\n    cannot simply split the string on \"|\" symbols, since these\n    also appear inside templates and internal links, e.g.\n\n     {{if:|\n      |{{#if:the president|\n           |{{#if:|\n               [[Category:Hatnote templates|A{{PAGENAME}}]]\n            }}\n       }}\n     }}\n\n    We split parts at the \"|\" symbols that are not inside any pair\n    {{{...}}}, {{...}}, [[...]], {|...|}.\n    \"\"\"", "\n", "\n", "# Must consider '[' as normal in expansion of Template:EMedicine2:", "\n", "# #ifeq: ped|article|[http://emedicine.medscape.com/article/180-overview|[http://www.emedicine.com/ped/topic180.htm#{{#if: |section~}}", "\n", "# as part of:", "\n", "# {{#ifeq: ped|article|[http://emedicine.medscape.com/article/180-overview|[http://www.emedicine.com/ped/topic180.htm#{{#if: |section~}}}} ped/180{{#if: |~}}]", "\n", "\n", "# should handle both tpl arg like:", "\n", "#    4|{{{{{subst|}}}CURRENTYEAR}}", "\n", "# and tpl parameters like:", "\n", "#    ||[[Category:People|{{#if:A|A|{{PAGENAME}}}}]]", "\n", "\n", "sep", "=", "'|'", "\n", "parameters", "=", "[", "]", "\n", "cur", "=", "0", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "paramsList", ")", ":", "\n", "        ", "par", "=", "paramsList", "[", "cur", ":", "s", "]", ".", "split", "(", "sep", ")", "\n", "if", "par", ":", "\n", "            ", "if", "parameters", ":", "\n", "# portion before | belongs to previous parameter", "\n", "                ", "parameters", "[", "-", "1", "]", "+=", "par", "[", "0", "]", "\n", "if", "len", "(", "par", ")", ">", "1", ":", "\n", "# rest are new parameters", "\n", "                    ", "parameters", ".", "extend", "(", "par", "[", "1", ":", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "parameters", "=", "par", "\n", "", "", "elif", "not", "parameters", ":", "\n", "            ", "parameters", "=", "[", "''", "]", "# create first param", "\n", "# add span to last previous parameter", "\n", "", "parameters", "[", "-", "1", "]", "+=", "paramsList", "[", "s", ":", "e", "]", "\n", "cur", "=", "e", "\n", "# leftover", "\n", "", "par", "=", "paramsList", "[", "cur", ":", "]", ".", "split", "(", "sep", ")", "\n", "if", "par", ":", "\n", "        ", "if", "parameters", ":", "\n", "# portion before | belongs to previous parameter", "\n", "            ", "parameters", "[", "-", "1", "]", "+=", "par", "[", "0", "]", "\n", "if", "len", "(", "par", ")", ">", "1", ":", "\n", "# rest are new parameters", "\n", "                ", "parameters", ".", "extend", "(", "par", "[", "1", ":", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "parameters", "=", "par", "\n", "\n", "# logging.debug('splitParts %s %s\\nparams: %s', sep, paramsList, str(parameters))", "\n", "", "", "return", "parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findMatchingBraces": [[925, 1032], ["re.compile", "re.compile", "re.compile", "re.compile", "re.compile.search", "reOpen.search.end", "reOpen.search.end", "reOpen.search.start", "re.compile.search", "reNext.search.end", "reOpen.search.group", "reNext.search.group", "reNext.search.end", "reNext.search.start", "stack.append", "stack.pop", "stack.append", "stack.append", "reOpen.search.start", "len", "stack.pop", "stack.append", "reOpen.search.start", "reOpen.search.start"], "function", ["None"], ["", "def", "findMatchingBraces", "(", "text", ",", "ldelim", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param ldelim: number of braces to match. 0 means match [[]], {{}} and {{{}}}.\n    \"\"\"", "\n", "# Parsing is done with respect to pairs of double braces {{..}} delimiting", "\n", "# a template, and pairs of triple braces {{{..}}} delimiting a tplarg.", "\n", "# If double opening braces are followed by triple closing braces or", "\n", "# conversely, this is taken as delimiting a template, with one left-over", "\n", "# brace outside it, taken as plain text. For any pattern of braces this", "\n", "# defines a set of templates and tplargs such that any two are either", "\n", "# separate or nested (not overlapping).", "\n", "\n", "# Unmatched double rectangular closing brackets can be in a template or", "\n", "# tplarg, but unmatched double rectangular opening brackets cannot.", "\n", "# Unmatched double or triple closing braces inside a pair of", "\n", "# double rectangular brackets are treated as plain text.", "\n", "# Other formulation: in ambiguity between template or tplarg on one hand,", "\n", "# and a link on the other hand, the structure with the rightmost opening", "\n", "# takes precedence, even if this is the opening of a link without any", "\n", "# closing, so not producing an actual link.", "\n", "\n", "# In the case of more than three opening braces the last three are assumed", "\n", "# to belong to a tplarg, unless there is no matching triple of closing", "\n", "# braces, in which case the last two opening braces are are assumed to", "\n", "# belong to a template.", "\n", "\n", "# We must skip individual { like in:", "\n", "#   {{#ifeq: {{padleft:|1|}} | { | | &nbsp;}}", "\n", "# We must resolve ambiguities like this:", "\n", "#   {{{{ }}}} -> { {{{ }}} }", "\n", "#   {{{{{ }}}}} -> {{ {{{ }}} }}", "\n", "#   {{#if:{{{{{#if:{{{nominee|}}}|nominee|candidate}}|}}}|...}}", "\n", "\n", "# Handle:", "\n", "#   {{{{{|safesubst:}}}#Invoke:String|replace|{{{1|{{{{{|safesubst:}}}PAGENAME}}}}}|%s+%([^%(]-%)$||plain=false}}", "\n", "# as well as expressions with stray }:", "\n", "#   {{{link|{{ucfirst:{{{1}}}}}} interchange}}}", "\n", "\n", "if", "ldelim", ":", "# 2-3", "\n", "        ", "reOpen", "=", "re", ".", "compile", "(", "'[{]{%d,}'", "%", "ldelim", ")", "# at least ldelim", "\n", "reNext", "=", "re", ".", "compile", "(", "'[{]{2,}|}{2,}'", ")", "# at least 2", "\n", "", "else", ":", "\n", "        ", "reOpen", "=", "re", ".", "compile", "(", "'{{2,}|\\[{2,}'", ")", "\n", "reNext", "=", "re", ".", "compile", "(", "'{{2,}|}{2,}|\\[{2,}|]{2,}'", ")", "# at least 2", "\n", "\n", "", "cur", "=", "0", "\n", "while", "True", ":", "\n", "        ", "m1", "=", "reOpen", ".", "search", "(", "text", ",", "cur", ")", "\n", "if", "not", "m1", ":", "\n", "            ", "return", "\n", "", "lmatch", "=", "m1", ".", "end", "(", ")", "-", "m1", ".", "start", "(", ")", "\n", "if", "m1", ".", "group", "(", ")", "[", "0", "]", "==", "'{'", ":", "\n", "            ", "stack", "=", "[", "lmatch", "]", "# stack of opening braces lengths", "\n", "", "else", ":", "\n", "            ", "stack", "=", "[", "-", "lmatch", "]", "# negative means [", "\n", "", "end", "=", "m1", ".", "end", "(", ")", "\n", "while", "True", ":", "\n", "            ", "m2", "=", "reNext", ".", "search", "(", "text", ",", "end", ")", "\n", "if", "not", "m2", ":", "\n", "                ", "return", "# unbalanced", "\n", "", "end", "=", "m2", ".", "end", "(", ")", "\n", "brac", "=", "m2", ".", "group", "(", ")", "[", "0", "]", "\n", "lmatch", "=", "m2", ".", "end", "(", ")", "-", "m2", ".", "start", "(", ")", "\n", "\n", "if", "brac", "==", "'{'", ":", "\n", "                ", "stack", ".", "append", "(", "lmatch", ")", "\n", "", "elif", "brac", "==", "'}'", ":", "\n", "                ", "while", "stack", ":", "\n", "                    ", "openCount", "=", "stack", ".", "pop", "(", ")", "# opening span", "\n", "if", "openCount", "==", "0", ":", "# illegal unmatched [[", "\n", "                        ", "continue", "\n", "", "if", "lmatch", ">=", "openCount", ":", "\n", "                        ", "lmatch", "-=", "openCount", "\n", "if", "lmatch", "<=", "1", ":", "# either close or stray }", "\n", "                            ", "break", "\n", "", "", "else", ":", "\n", "# put back unmatched", "\n", "                        ", "stack", ".", "append", "(", "openCount", "-", "lmatch", ")", "\n", "break", "\n", "", "", "if", "not", "stack", ":", "\n", "                    ", "yield", "m1", ".", "start", "(", ")", ",", "end", "-", "lmatch", "\n", "cur", "=", "end", "\n", "break", "\n", "", "elif", "len", "(", "stack", ")", "==", "1", "and", "0", "<", "stack", "[", "0", "]", "<", "ldelim", ":", "\n", "# ambiguous {{{{{ }}} }}", "\n", "                    ", "yield", "m1", ".", "start", "(", ")", "+", "stack", "[", "0", "]", ",", "end", "\n", "cur", "=", "end", "\n", "break", "\n", "", "", "elif", "brac", "==", "'['", ":", "# [[", "\n", "                ", "stack", ".", "append", "(", "-", "lmatch", ")", "\n", "", "else", ":", "# ]]", "\n", "                ", "while", "stack", "and", "stack", "[", "-", "1", "]", "<", "0", ":", "# matching [[", "\n", "                    ", "openCount", "=", "-", "stack", ".", "pop", "(", ")", "\n", "if", "lmatch", ">=", "openCount", ":", "\n", "                        ", "lmatch", "-=", "openCount", "\n", "if", "lmatch", "<=", "1", ":", "# either close or stray ]", "\n", "                            ", "break", "\n", "", "", "else", ":", "\n", "# put back unmatched (negative)", "\n", "                        ", "stack", ".", "append", "(", "lmatch", "-", "openCount", ")", "\n", "break", "\n", "", "", "if", "not", "stack", ":", "\n", "                    ", "yield", "m1", ".", "start", "(", ")", ",", "end", "-", "lmatch", "\n", "cur", "=", "end", "\n", "break", "\n", "# unmatched ]] are discarded", "\n", "", "cur", "=", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findBalanced": [[1034, 1074], ["re.compile", "re.compile", "nextPat.search", "nextPat.search.group", "nextPat.search.end", "re.escape", "itertools.izip", "nextPat.search.start", "stack.append", "stack.pop", "nextPat.search.end", "nextPat.search.end"], "function", ["None"], ["", "", "", "", "def", "findBalanced", "(", "text", ",", "openDelim", "=", "[", "'[['", "]", ",", "closeDelim", "=", "[", "']]'", "]", ")", ":", "\n", "    ", "\"\"\"\n    Assuming that text contains a properly balanced expression using\n    :param openDelim: as opening delimiters and\n    :param closeDelim: as closing delimiters.\n    :return: an iterator producing pairs (start, end) of start and end\n    positions in text containing a balanced expression.\n    \"\"\"", "\n", "openPat", "=", "'|'", ".", "join", "(", "[", "re", ".", "escape", "(", "x", ")", "for", "x", "in", "openDelim", "]", ")", "\n", "# pattern for delimiters expected after each opening delimiter", "\n", "afterPat", "=", "{", "o", ":", "re", ".", "compile", "(", "openPat", "+", "'|'", "+", "c", ",", "re", ".", "DOTALL", ")", "for", "o", ",", "c", "in", "izip", "(", "openDelim", ",", "closeDelim", ")", "}", "\n", "stack", "=", "[", "]", "\n", "start", "=", "0", "\n", "cur", "=", "0", "\n", "# end = len(text)", "\n", "startSet", "=", "False", "\n", "startPat", "=", "re", ".", "compile", "(", "openPat", ")", "\n", "nextPat", "=", "startPat", "\n", "while", "True", ":", "\n", "        ", "next", "=", "nextPat", ".", "search", "(", "text", ",", "cur", ")", "\n", "if", "not", "next", ":", "\n", "            ", "return", "\n", "", "if", "not", "startSet", ":", "\n", "            ", "start", "=", "next", ".", "start", "(", ")", "\n", "startSet", "=", "True", "\n", "", "delim", "=", "next", ".", "group", "(", "0", ")", "\n", "if", "delim", "in", "openDelim", ":", "\n", "            ", "stack", ".", "append", "(", "delim", ")", "\n", "nextPat", "=", "afterPat", "[", "delim", "]", "\n", "", "else", ":", "\n", "            ", "opening", "=", "stack", ".", "pop", "(", ")", "\n", "# assert opening == openDelim[closeDelim.index(next.group(0))]", "\n", "if", "stack", ":", "\n", "                ", "nextPat", "=", "afterPat", "[", "stack", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "                ", "yield", "start", ",", "next", ".", "end", "(", ")", "\n", "nextPat", "=", "startPat", "\n", "start", "=", "next", ".", "end", "(", ")", "\n", "startSet", "=", "False", "\n", "", "", "cur", "=", "next", ".", "end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.if_empty": [[1082, 1123], ["None"], "function", ["None"], ["", "", "def", "if_empty", "(", "*", "rest", ")", ":", "\n", "    ", "\"\"\"\n    This implements If_empty from English Wikipedia module:\n\n       <title>Module:If empty</title>\n       <ns>828</ns>\n       <text>local p = {}\n\n    function p.main(frame)\n            local args = require('Module:Arguments').getArgs(frame, {wrappers = 'Template:If empty', removeBlanks = false})\n\n            -- For backwards compatibility reasons, the first 8 parameters can be unset instead of being blank,\n            -- even though there's really no legitimate use case for this. At some point, this will be removed.\n            local lowestNil = math.huge\n            for i = 8,1,-1 do\n                    if args[i] == nil then\n                            args[i] = ''\n                            lowestNil = i\n                    end\n            end\n\n            for k,v in ipairs(args) do\n                    if v ~= '' then\n                            if lowestNil &lt; k then\n                                    -- If any uses of this template depend on the behavior above, add them to a tracking category.\n                                    -- This is a rather fragile, convoluted, hacky way to do it, but it ensures that this module's output won't be modified\n                                    -- by it.\n                                    frame:extensionTag('ref', '[[Category:Instances of Template:If_empty missing arguments]]', {group = 'TrackingCategory'})\n                                    frame:extensionTag('references', '', {group = 'TrackingCategory'})\n                            end\n                            return v\n                    end\n            end\n    end\n\n    return p   </text>\n    \"\"\"", "\n", "for", "arg", "in", "rest", ":", "\n", "        ", "if", "arg", ":", "\n", "            ", "return", "arg", "\n", "", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst": [[1263, 1274], ["len", "string.upper", "string[].upper"], "function", ["None"], ["def", "ucfirst", "(", "string", ")", ":", "\n", "    ", "\"\"\":return: a string with just its first character uppercase\n    We can't use title() since it coverts all words.\n    \"\"\"", "\n", "if", "string", ":", "\n", "        ", "if", "len", "(", "string", ")", ">", "1", ":", "\n", "            ", "return", "string", "[", "0", "]", ".", "upper", "(", ")", "+", "string", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "string", ".", "upper", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.lcfirst": [[1276, 1285], ["len", "string.lower", "string[].lower"], "function", ["None"], ["", "", "def", "lcfirst", "(", "string", ")", ":", "\n", "    ", "\"\"\":return: a string with its first character lowercase\"\"\"", "\n", "if", "string", ":", "\n", "        ", "if", "len", "(", "string", ")", ">", "1", ":", "\n", "            ", "return", "string", "[", "0", "]", ".", "lower", "(", ")", "+", "string", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "string", ".", "lower", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.fullyQualifiedTemplateTitle": [[1287, 1318], ["templateTitle.startswith", "wiki_extractor.ucfirst", "re.match", "wiki_extractor.normalizeNamespace", "wiki_extractor.ucfirst", "re.match.group", "wiki_extractor.ucfirst", "re.match.group"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.normalizeNamespace", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst"], ["", "", "def", "fullyQualifiedTemplateTitle", "(", "templateTitle", ")", ":", "\n", "    ", "\"\"\"\n    Determine the namespace of the page being included through the template\n    mechanism\n    \"\"\"", "\n", "if", "templateTitle", ".", "startswith", "(", "':'", ")", ":", "\n", "# Leading colon by itself implies main namespace, so strip this colon", "\n", "        ", "return", "ucfirst", "(", "templateTitle", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "        ", "m", "=", "re", ".", "match", "(", "'([^:]*)(:.*)'", ",", "templateTitle", ")", "\n", "if", "m", ":", "\n", "# colon found but not in the first position - check if it", "\n", "# designates a known namespace", "\n", "            ", "prefix", "=", "normalizeNamespace", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "if", "prefix", "in", "knownNamespaces", ":", "\n", "                ", "return", "prefix", "+", "ucfirst", "(", "m", ".", "group", "(", "2", ")", ")", "\n", "# The title of the page being included is NOT in the main namespace and", "\n", "# lacks any other explicit designation of the namespace - therefore, it", "\n", "# is resolved to the Template namespace (that's the default for the", "\n", "# template inclusion mechanism).", "\n", "\n", "# This is a defense against pages whose title only contains UTF-8 chars", "\n", "# that are reduced to an empty string. Right now I can think of one such", "\n", "# case - <C2><A0> which represents the non-breaking space.", "\n", "# In this particular case, this page is a redirect to [[Non-nreaking", "\n", "# space]], but having in the system a redirect page with an empty title", "\n", "# causes numerous problems, so we'll live happier without it.", "\n", "", "", "", "if", "templateTitle", ":", "\n", "        ", "return", "templatePrefix", "+", "ucfirst", "(", "templateTitle", ")", "\n", "", "else", ":", "\n", "        ", "return", "''", "# caller may log as error", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.normalizeNamespace": [[1320, 1322], ["wiki_extractor.ucfirst"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ucfirst"], ["", "", "def", "normalizeNamespace", "(", "ns", ")", ":", "\n", "    ", "return", "ucfirst", "(", "ns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_expr": [[1358, 1367], ["re.sub", "re.sub", "re.sub", "re.sub", "unicode", "eval"], "function", ["None"], ["def", "sharp_expr", "(", "expr", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "expr", "=", "re", ".", "sub", "(", "'='", ",", "'=='", ",", "expr", ")", "\n", "expr", "=", "re", ".", "sub", "(", "'mod'", ",", "'%'", ",", "expr", ")", "\n", "expr", "=", "re", ".", "sub", "(", "'\\bdiv\\b'", ",", "'/'", ",", "expr", ")", "\n", "expr", "=", "re", ".", "sub", "(", "'\\bround\\b'", ",", "'|ROUND|'", ",", "expr", ")", "\n", "return", "unicode", "(", "eval", "(", "expr", ")", ")", "\n", "", "except", ":", "\n", "        ", "return", "'<span class=\"error\"></span>'", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_if": [[1369, 1381], ["testValue.strip", "valueIfTrue.strip.strip", "valueIfFalse.strip"], "function", ["None"], ["", "", "def", "sharp_if", "(", "testValue", ",", "valueIfTrue", ",", "valueIfFalse", "=", "None", ",", "*", "args", ")", ":", "\n", "# In theory, we should evaluate the first argument here,", "\n", "# but it was evaluated while evaluating part[0] in expandTemplate().", "\n", "    ", "if", "testValue", ".", "strip", "(", ")", ":", "\n", "# The {{#if:}} function is an if-then-else construct.", "\n", "# The applied condition is: \"The condition string is non-empty\".", "\n", "        ", "valueIfTrue", "=", "valueIfTrue", ".", "strip", "(", ")", "\n", "if", "valueIfTrue", ":", "\n", "            ", "return", "valueIfTrue", "\n", "", "", "elif", "valueIfFalse", ":", "\n", "        ", "return", "valueIfFalse", ".", "strip", "(", ")", "\n", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_ifeq": [[1383, 1399], ["rvalue.strip.strip", "lvalue.strip", "valueIfTrue.strip", "valueIfFalse.strip"], "function", ["None"], ["", "def", "sharp_ifeq", "(", "lvalue", ",", "rvalue", ",", "valueIfTrue", ",", "valueIfFalse", "=", "None", ",", "*", "args", ")", ":", "\n", "    ", "rvalue", "=", "rvalue", ".", "strip", "(", ")", "\n", "if", "rvalue", ":", "\n", "# lvalue is always defined", "\n", "        ", "if", "lvalue", ".", "strip", "(", ")", "==", "rvalue", ":", "\n", "# The {{#ifeq:}} function is an if-then-else construct. The", "\n", "# applied condition is \"is rvalue equal to lvalue\". Note that this", "\n", "# does only string comparison while MediaWiki implementation also", "\n", "# supports numerical comparissons.", "\n", "\n", "            ", "if", "valueIfTrue", ":", "\n", "                ", "return", "valueIfTrue", ".", "strip", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "valueIfFalse", ":", "\n", "                ", "return", "valueIfFalse", ".", "strip", "(", ")", "\n", "", "", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_iferror": [[1401, 1408], ["re.match", "test.strip", "Else.strip"], "function", ["None"], ["", "def", "sharp_iferror", "(", "test", ",", "then", "=", "''", ",", "Else", "=", "None", ",", "*", "args", ")", ":", "\n", "    ", "if", "re", ".", "match", "(", "'<(?:strong|span|p|div)\\s(?:[^\\s>]*\\s+)*?class=\"(?:[^\"\\s>]*\\s+)*?error(?:\\s[^\">]*)?\"'", ",", "test", ")", ":", "\n", "        ", "return", "then", "\n", "", "elif", "Else", "is", "None", ":", "\n", "        ", "return", "test", ".", "strip", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "Else", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_switch": [[1410, 1452], ["primary.strip.strip", "param.split", "pair[].strip", "len", "pair[].strip", "v.strip", "pair[].strip.split"], "function", ["None"], ["", "", "def", "sharp_switch", "(", "primary", ",", "*", "params", ")", ":", "\n", "# FIXME: we don't support numeric expressions in primary", "\n", "\n", "# {{#switch: comparison string", "\n", "#  | case1 = result1", "\n", "#  | case2", "\n", "#  | case4 = result2", "\n", "#  | 1 | case5 = result3", "\n", "#  | #default = result4", "\n", "# }}", "\n", "\n", "    ", "primary", "=", "primary", ".", "strip", "(", ")", "\n", "found", "=", "False", "# for fall through cases", "\n", "default", "=", "None", "\n", "rvalue", "=", "None", "\n", "lvalue", "=", "''", "\n", "for", "param", "in", "params", ":", "\n", "# handle cases like:", "\n", "#  #default = [http://www.perseus.tufts.edu/hopper/text?doc=Perseus...]", "\n", "        ", "pair", "=", "param", ".", "split", "(", "'='", ",", "1", ")", "\n", "lvalue", "=", "pair", "[", "0", "]", ".", "strip", "(", ")", "\n", "rvalue", "=", "None", "\n", "if", "len", "(", "pair", ")", ">", "1", ":", "\n", "# got \"=\"", "\n", "            ", "rvalue", "=", "pair", "[", "1", "]", ".", "strip", "(", ")", "\n", "# check for any of multiple values pipe separated", "\n", "if", "found", "or", "primary", "in", "[", "v", ".", "strip", "(", ")", "for", "v", "in", "lvalue", ".", "split", "(", "'|'", ")", "]", ":", "\n", "# Found a match, return now", "\n", "                ", "return", "rvalue", "\n", "", "elif", "lvalue", "==", "'#default'", ":", "\n", "                ", "default", "=", "rvalue", "\n", "", "rvalue", "=", "None", "# avoid defaulting to last case", "\n", "", "elif", "lvalue", "==", "primary", ":", "\n", "# If the value matches, set a flag and continue", "\n", "            ", "found", "=", "True", "\n", "# Default case", "\n", "# Check if the last item had no = sign, thus specifying the default case", "\n", "", "", "if", "rvalue", "is", "not", "None", ":", "\n", "        ", "return", "lvalue", "\n", "", "elif", "default", "is", "not", "None", ":", "\n", "        ", "return", "default", "\n", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_invoke": [[1455, 1474], ["modules.get", "modules.get.get", "wiki_extractor.fullyQualifiedTemplateTitle", "next", "logging.warn", "functions.get.", "functions.get.", "params.get", "str", "range", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.fullyQualifiedTemplateTitle", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.NextFile.next"], ["", "def", "sharp_invoke", "(", "module", ",", "function", ",", "frame", ")", ":", "\n", "    ", "functions", "=", "modules", ".", "get", "(", "module", ")", "\n", "if", "functions", ":", "\n", "        ", "funct", "=", "functions", ".", "get", "(", "function", ")", "\n", "if", "funct", ":", "\n", "# find parameters in frame whose title is the one of the original", "\n", "# template invocation", "\n", "            ", "templateTitle", "=", "fullyQualifiedTemplateTitle", "(", "module", ")", "\n", "if", "not", "templateTitle", ":", "\n", "                ", "logging", ".", "warn", "(", "\"Template with empty title\"", ")", "\n", "", "pair", "=", "next", "(", "(", "x", "for", "x", "in", "frame", "if", "x", "[", "0", "]", "==", "templateTitle", ")", ",", "None", ")", "\n", "if", "pair", ":", "\n", "                ", "params", "=", "pair", "[", "1", "]", "\n", "# extract positional args", "\n", "params", "=", "[", "params", ".", "get", "(", "str", "(", "i", "+", "1", ")", ")", "for", "i", "in", "range", "(", "len", "(", "params", ")", ")", "]", "\n", "return", "funct", "(", "*", "params", ")", "\n", "", "else", ":", "\n", "                ", "return", "funct", "(", ")", "\n", "", "", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.callParserFunction": [[1519, 1543], ["wiki_extractor.sharp_invoke", "args[].strip", "args[].strip"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.sharp_invoke"], ["def", "callParserFunction", "(", "functionName", ",", "args", ",", "frame", ")", ":", "\n", "    ", "\"\"\"\n    Parser functions have similar syntax as templates, except that\n    the first argument is everything after the first colon.\n    :return: the result of the invocation, None in case of failure.\n\n    http://meta.wikimedia.org/wiki/Help:ParserFunctions\n    \"\"\"", "\n", "\n", "try", ":", "\n", "        ", "if", "functionName", "==", "'#invoke'", ":", "\n", "# special handling of frame", "\n", "            ", "arg0", ",", "arg1", "=", "args", "[", "0", "]", ".", "strip", "(", ")", ",", "args", "[", "1", "]", ".", "strip", "(", ")", "\n", "ret", "=", "sharp_invoke", "(", "arg0", ",", "arg1", ",", "frame", ")", "\n", "# logging.debug('#invoke> %s %s %s', arg0, arg1, ret)", "\n", "return", "ret", "\n", "", "if", "functionName", "in", "parserFunctions", ":", "\n", "            ", "ret", "=", "parserFunctions", "[", "functionName", "]", "(", "*", "args", ")", "\n", "# logging.debug('parserFunction> %s %s', functionName, ret)", "\n", "return", "ret", "\n", "", "", "except", ":", "\n", "        ", "return", "\"\"", "# FIXME: fix errors", "\n", "\n", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.define_template": [[1571, 1619], ["re.match", "wiki_extractor.unescape", "comment.sub", "reNoinclude.sub", "re.sub", "re.sub", "re.finditer", "re.match.group", "re.match.group", "reIncludeonly.sub", "logging.warn"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.unescape"], ["def", "define_template", "(", "title", ",", "page", ")", ":", "\n", "    ", "\"\"\"\n    Adds a template defined in the :param page:.\n    @see https://en.wikipedia.org/wiki/Help:Template#Noinclude.2C_includeonly.2C_and_onlyinclude\n    \"\"\"", "\n", "global", "templates", "\n", "global", "redirects", "\n", "\n", "# title = normalizeTitle(title)", "\n", "\n", "# check for redirects", "\n", "m", "=", "re", ".", "match", "(", "'#REDIRECT.*?\\[\\[([^\\]]*)]]'", ",", "page", "[", "0", "]", ",", "re", ".", "IGNORECASE", ")", "\n", "if", "m", ":", "\n", "        ", "redirects", "[", "title", "]", "=", "m", ".", "group", "(", "1", ")", "# normalizeTitle(m.group(1))", "\n", "return", "\n", "\n", "", "text", "=", "unescape", "(", "''", ".", "join", "(", "page", ")", ")", "\n", "\n", "# We're storing template text for future inclusion, therefore,", "\n", "# remove all <noinclude> text and keep all <includeonly> text", "\n", "# (but eliminate <includeonly> tags per se).", "\n", "# However, if <onlyinclude> ... </onlyinclude> parts are present,", "\n", "# then only keep them and discard the rest of the template body.", "\n", "# This is because using <onlyinclude> on a text fragment is", "\n", "# equivalent to enclosing it in <includeonly> tags **AND**", "\n", "# enclosing all the rest of the template body in <noinclude> tags.", "\n", "\n", "# remove comments", "\n", "text", "=", "comment", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "# eliminate <noinclude> fragments", "\n", "text", "=", "reNoinclude", ".", "sub", "(", "''", ",", "text", ")", "\n", "# eliminate unterminated <noinclude> elements", "\n", "text", "=", "re", ".", "sub", "(", "r'<noinclude\\s*>.*$'", ",", "''", ",", "text", ",", "flags", "=", "re", ".", "DOTALL", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'<noinclude/>'", ",", "''", ",", "text", ")", "\n", "\n", "onlyincludeAccumulator", "=", "''", "\n", "for", "m", "in", "re", ".", "finditer", "(", "'<onlyinclude>(.*?)</onlyinclude>'", ",", "text", ",", "re", ".", "DOTALL", ")", ":", "\n", "        ", "onlyincludeAccumulator", "+=", "m", ".", "group", "(", "1", ")", "\n", "", "if", "onlyincludeAccumulator", ":", "\n", "        ", "text", "=", "onlyincludeAccumulator", "\n", "", "else", ":", "\n", "        ", "text", "=", "reIncludeonly", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "", "if", "text", ":", "\n", "        ", "if", "title", "in", "templates", ":", "\n", "            ", "logging", ".", "warn", "(", "'Redefining: %s'", ",", "title", ")", "\n", "", "templates", "[", "title", "]", "=", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropNested": [[1623, 1674], ["re.compile", "re.compile", "re.compile.search", "re.compile.search", "wiki_extractor.dropSpans", "openRE.search.end", "re.compile.search", "openRE.search.end", "spans.append", "closeRE.search.end", "openRE.search.start", "re.compile.search", "closeRE.search.end", "re.compile.search", "spans.append", "re.compile.search", "closeRE.search.end", "openRE.search.start", "closeRE.search.end", "closeRE.search.end", "openRE.search.end", "openRE.search.start", "closeRE.search.end", "openRE.search.start"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropSpans"], ["", "", "def", "dropNested", "(", "text", ",", "openDelim", ",", "closeDelim", ")", ":", "\n", "    ", "\"\"\"\n    A matching function for nested expressions, e.g. namespaces and tables.\n    \"\"\"", "\n", "openRE", "=", "re", ".", "compile", "(", "openDelim", ",", "re", ".", "IGNORECASE", ")", "\n", "closeRE", "=", "re", ".", "compile", "(", "closeDelim", ",", "re", ".", "IGNORECASE", ")", "\n", "# partition text in separate blocks { } { }", "\n", "spans", "=", "[", "]", "# pairs (s, e) for each partition", "\n", "nest", "=", "0", "# nesting level", "\n", "start", "=", "openRE", ".", "search", "(", "text", ",", "0", ")", "\n", "if", "not", "start", ":", "\n", "        ", "return", "text", "\n", "", "end", "=", "closeRE", ".", "search", "(", "text", ",", "start", ".", "end", "(", ")", ")", "\n", "next", "=", "start", "\n", "while", "end", ":", "\n", "        ", "next", "=", "openRE", ".", "search", "(", "text", ",", "next", ".", "end", "(", ")", ")", "\n", "if", "not", "next", ":", "# termination", "\n", "            ", "while", "nest", ":", "# close all pending", "\n", "                ", "nest", "-=", "1", "\n", "end0", "=", "closeRE", ".", "search", "(", "text", ",", "end", ".", "end", "(", ")", ")", "\n", "if", "end0", ":", "\n", "                    ", "end", "=", "end0", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "spans", ".", "append", "(", "(", "start", ".", "start", "(", ")", ",", "end", ".", "end", "(", ")", ")", ")", "\n", "break", "\n", "", "while", "end", ".", "end", "(", ")", "<", "next", ".", "start", "(", ")", ":", "\n", "# { } {", "\n", "            ", "if", "nest", ":", "\n", "                ", "nest", "-=", "1", "\n", "# try closing more", "\n", "last", "=", "end", ".", "end", "(", ")", "\n", "end", "=", "closeRE", ".", "search", "(", "text", ",", "end", ".", "end", "(", ")", ")", "\n", "if", "not", "end", ":", "# unbalanced", "\n", "                    ", "if", "spans", ":", "\n", "                        ", "span", "=", "(", "spans", "[", "0", "]", "[", "0", "]", ",", "last", ")", "\n", "", "else", ":", "\n", "                        ", "span", "=", "(", "start", ".", "start", "(", ")", ",", "last", ")", "\n", "", "spans", "=", "[", "span", "]", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "spans", ".", "append", "(", "(", "start", ".", "start", "(", ")", ",", "end", ".", "end", "(", ")", ")", ")", "\n", "# advance start, find next close", "\n", "start", "=", "next", "\n", "end", "=", "closeRE", ".", "search", "(", "text", ",", "next", ".", "end", "(", ")", ")", "\n", "break", "# { }", "\n", "", "", "if", "next", "!=", "start", ":", "\n", "# { { }", "\n", "            ", "nest", "+=", "1", "\n", "# collect text outside partitions", "\n", "", "", "return", "dropSpans", "(", "spans", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.dropSpans": [[1676, 1690], ["spans.sort"], "function", ["None"], ["", "def", "dropSpans", "(", "spans", ",", "text", ")", ":", "\n", "    ", "\"\"\"\n    Drop from text the blocks identified in :param spans:, possibly nested.\n    \"\"\"", "\n", "spans", ".", "sort", "(", ")", "\n", "res", "=", "''", "\n", "offset", "=", "0", "\n", "for", "s", ",", "e", "in", "spans", ":", "\n", "        ", "if", "offset", "<=", "s", ":", "# handle nesting", "\n", "            ", "if", "offset", "<", "s", ":", "\n", "                ", "res", "+=", "text", "[", "offset", ":", "s", "]", "\n", "", "offset", "=", "e", "\n", "", "", "res", "+=", "text", "[", "offset", ":", "]", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.replaceInternalLinks": [[1699, 1739], ["wiki_extractor.findBalanced", "tailRE.match", "inner.find", "tailRE.match.group", "tailRE.match.end", "inner[].rstrip", "wiki_extractor.findBalanced", "inner[].strip", "inner.rfind", "wiki_extractor.makeInternalLink"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findBalanced", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.findBalanced", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeInternalLink"], ["", "def", "replaceInternalLinks", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Replaces internal links of the form:\n    [[title |...|label]]trail\n\n    with title concatenated with trail, when present, e.g. 's' for plural.\n\n    See https://www.mediawiki.org/wiki/Help:Links#Internal_links\n    \"\"\"", "\n", "# call this after removal of external links, so we need not worry about", "\n", "# triple closing ]]].", "\n", "cur", "=", "0", "\n", "res", "=", "''", "\n", "for", "s", ",", "e", "in", "findBalanced", "(", "text", ")", ":", "\n", "        ", "m", "=", "tailRE", ".", "match", "(", "text", ",", "e", ")", "\n", "if", "m", ":", "\n", "            ", "trail", "=", "m", ".", "group", "(", "0", ")", "\n", "end", "=", "m", ".", "end", "(", ")", "\n", "", "else", ":", "\n", "            ", "trail", "=", "''", "\n", "end", "=", "e", "\n", "", "inner", "=", "text", "[", "s", "+", "2", ":", "e", "-", "2", "]", "\n", "# find first |", "\n", "pipe", "=", "inner", ".", "find", "(", "'|'", ")", "\n", "if", "pipe", "<", "0", ":", "\n", "            ", "title", "=", "inner", "\n", "label", "=", "title", "\n", "", "else", ":", "\n", "            ", "title", "=", "inner", "[", ":", "pipe", "]", ".", "rstrip", "(", ")", "\n", "# find last |", "\n", "curp", "=", "pipe", "+", "1", "\n", "for", "s1", ",", "e1", "in", "findBalanced", "(", "inner", ")", ":", "\n", "                ", "last", "=", "inner", ".", "rfind", "(", "'|'", ",", "curp", ",", "s1", ")", "\n", "if", "last", ">=", "0", ":", "\n", "                    ", "pipe", "=", "last", "# advance", "\n", "", "curp", "=", "e1", "\n", "", "label", "=", "inner", "[", "pipe", "+", "1", ":", "]", ".", "strip", "(", ")", "\n", "", "res", "+=", "text", "[", "cur", ":", "s", "]", "+", "makeInternalLink", "(", "title", ",", "label", ")", "+", "trail", "\n", "cur", "=", "end", "\n", "", "return", "res", "+", "text", "[", "cur", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeInternalLink": [[2006, 2019], ["title.find", "title.find", "urllib.quote", "title.encode"], "function", ["None"], ["", "def", "makeInternalLink", "(", "title", ",", "label", ")", ":", "\n", "    ", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "0", "and", "title", "[", ":", "colon", "]", "not", "in", "acceptedNamespaces", ":", "\n", "        ", "return", "''", "\n", "", "if", "colon", "==", "0", ":", "\n", "# drop also :File:", "\n", "        ", "colon2", "=", "title", ".", "find", "(", "':'", ",", "colon", "+", "1", ")", "\n", "if", "colon2", ">", "1", "and", "title", "[", "colon", "+", "1", ":", "colon2", "]", "not", "in", "acceptedNamespaces", ":", "\n", "            ", "return", "''", "\n", "", "", "if", "Extractor", ".", "keepLinks", ":", "\n", "        ", "return", "'<a href=\"%s\">%s</a>'", "%", "(", "urllib", ".", "quote", "(", "title", ".", "encode", "(", "'utf-8'", ")", ")", ",", "label", ")", "\n", "", "else", ":", "\n", "        ", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.replaceExternalLinks": [[2054, 2089], ["ExtLinkBracketedRegex.finditer", "EXT_IMAGE_REGEX.match.end", "EXT_IMAGE_REGEX.match.group", "EXT_IMAGE_REGEX.match.group", "EXT_IMAGE_REGEX.match", "wiki_extractor.makeExternalLink", "wiki_extractor.makeExternalImage", "EXT_IMAGE_REGEX.match.start"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeExternalLink", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeExternalImage"], ["def", "replaceExternalLinks", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    https://www.mediawiki.org/wiki/Help:Links#External_links\n    [URL anchor text]\n    \"\"\"", "\n", "s", "=", "''", "\n", "cur", "=", "0", "\n", "for", "m", "in", "ExtLinkBracketedRegex", ".", "finditer", "(", "text", ")", ":", "\n", "        ", "s", "+=", "text", "[", "cur", ":", "m", ".", "start", "(", ")", "]", "\n", "cur", "=", "m", ".", "end", "(", ")", "\n", "\n", "url", "=", "m", ".", "group", "(", "1", ")", "\n", "label", "=", "m", ".", "group", "(", "3", ")", "\n", "\n", "# # The characters '<' and '>' (which were escaped by", "\n", "# # removeHTMLtags()) should not be included in", "\n", "# # URLs, per RFC 2396.", "\n", "# m2 = re.search('&(lt|gt);', url)", "\n", "# if m2:", "\n", "#     link = url[m2.end():] + ' ' + link", "\n", "#     url = url[0:m2.end()]", "\n", "\n", "# If the link text is an image URL, replace it with an <img> tag", "\n", "# This happened by accident in the original parser, but some people used it extensively", "\n", "m", "=", "EXT_IMAGE_REGEX", ".", "match", "(", "label", ")", "\n", "if", "m", ":", "\n", "            ", "label", "=", "makeExternalImage", "(", "label", ")", "\n", "\n", "# Use the encoded URL", "\n", "# This means that users can paste URLs directly into the text", "\n", "# Funny characters like \u00c3\u00b6 aren't valid in URLs anyway", "\n", "# This was changed in August 2004", "\n", "", "s", "+=", "makeExternalLink", "(", "url", ",", "label", ")", "# + trail", "\n", "\n", "", "return", "s", "+", "text", "[", "cur", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeExternalLink": [[2091, 2097], ["urllib.quote", "url.encode"], "function", ["None"], ["", "def", "makeExternalLink", "(", "url", ",", "anchor", ")", ":", "\n", "    ", "\"\"\"Function applied to wikiLinks\"\"\"", "\n", "if", "Extractor", ".", "keepLinks", ":", "\n", "        ", "return", "'<a href=\"%s\">%s</a>'", "%", "(", "urllib", ".", "quote", "(", "url", ".", "encode", "(", "'utf-8'", ")", ")", ",", "anchor", ")", "\n", "", "else", ":", "\n", "        ", "return", "anchor", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.fix_list_item": [[2098, 2108], ["list_item.strip.strip", "list_item.strip.endswith", "list_item.strip.endswith"], "function", ["None"], ["", "", "def", "fix_list_item", "(", "list_item", ")", ":", "\n", "    ", "list_item", "=", "list_item", ".", "strip", "(", ")", "\n", "if", "not", "list_item", ".", "endswith", "(", "'.'", ")", ":", "\n", "        ", "if", "list_item", ".", "endswith", "(", "(", "';'", ",", "','", ")", ")", ":", "\n", "            ", "list_item", "=", "list_item", "[", ":", "-", "1", "]", "+", "'.'", "\n", "", "else", ":", "\n", "            ", "list_item", "=", "list_item", "+", "'.'", "\n", "#list_item = list_item.replace(len(list_item) - 1, '.')", "\n", "\n", "", "", "return", "list_item", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.makeExternalImage": [[2112, 2117], ["None"], "function", ["None"], ["", "def", "makeExternalImage", "(", "url", ",", "alt", "=", "''", ")", ":", "\n", "    ", "if", "Extractor", ".", "keepLinks", ":", "\n", "        ", "return", "'<img src=\"%s\" alt=\"%s\">'", "%", "(", "url", ",", "alt", ")", "\n", "", "else", ":", "\n", "        ", "return", "alt", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.get_seciton_header_name": [[2136, 2147], ["int", "int", "wiki_utils.get_segment_seperator", "sum", "sum", "text.replace", "print", "c.isdigit", "c.isalpha", "text.replace", "text.replace"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_seperator"], ["def", "get_seciton_header_name", "(", "height", ",", "text", ")", ":", "\n", "    ", "numbers", "=", "int", "(", "sum", "(", "c", ".", "isdigit", "(", ")", "for", "c", "in", "text", ".", "replace", "(", "'.'", ",", "''", ")", ")", ")", "\n", "words", "=", "int", "(", "sum", "(", "c", ".", "isalpha", "(", ")", "for", "c", "in", "text", ".", "replace", "(", "'.'", ",", "''", ")", ")", ")", "\n", "\n", "clean_text", "=", "text", ".", "replace", "(", "'.'", ",", "''", ")", "+", "'.'", "\n", "header", "=", "wiki_utils", ".", "get_segment_seperator", "(", "height", ",", "clean_text", ")", "\n", "\n", "if", "words", "+", "numbers", "==", "0", ":", "\n", "        ", "print", "(", "'error in section. The header: '", "+", "header", ")", "\n", "\n", "", "return", "header", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.compact": [[2150, 2280], ["text.split", "section.match", "section.match.group", "len", "headers.keys", "line[].strip.startswith", "section.match.group", "page.append", "page.append", "itertools.izip_longest", "line[].strip", "len", "page.append", "headers.items", "headers.items.sort", "headers.clear", "page.append", "reversed", "page.append", "page.append", "page.append", "page.append", "len", "page.append", "page.append", "wiki_extractor.get_seciton_header_name", "wiki_utils.get_list_token", "line[].strip.strip", "headers.clear", "page.append", "headers.items", "headers.items.sort", "page.append", "page.append", "wiki_extractor.get_seciton_header_name"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.get_seciton_header_name", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.get_seciton_header_name"], ["", "def", "compact", "(", "text", ")", ":", "\n", "    ", "\"\"\"Deal with headers, lists, empty sections, residuals of tables.\n    :param text: convert to HTML.\n    \"\"\"", "\n", "\n", "page", "=", "[", "]", "# list of paragraph", "\n", "headers", "=", "{", "}", "# Headers for unfilled sections", "\n", "emptySection", "=", "False", "# empty sections are discarded", "\n", "listLevel", "=", "[", "]", "# nesting of lists", "\n", "\n", "current_section_name", "=", "\"\"", "\n", "\n", "for", "line", "in", "text", ".", "split", "(", "'\\n'", ")", ":", "\n", "\n", "        ", "if", "not", "line", ":", "\n", "            ", "continue", "\n", "# Handle section titles", "\n", "", "m", "=", "section", ".", "match", "(", "line", ")", "\n", "if", "m", ":", "\n", "            ", "title", "=", "m", ".", "group", "(", "2", ")", "\n", "lev", "=", "len", "(", "m", ".", "group", "(", "1", ")", ")", "# header level", "\n", "if", "lev", "==", "2", ":", "\n", "                ", "current_section_name", "=", "title", "\n", "", "if", "Extractor", ".", "toHTML", ":", "\n", "                ", "page", ".", "append", "(", "\"<h%d>%s</h%d>\"", "%", "(", "lev", ",", "title", ",", "lev", ")", ")", "\n", "", "if", "title", "and", "title", "[", "-", "1", "]", "not", "in", "'!?'", ":", "\n", "                ", "title", "+=", "'.'", "# terminate sentence.", "\n", "", "headers", "[", "lev", "]", "=", "title", "\n", "# drop previous headers", "\n", "for", "i", "in", "headers", ".", "keys", "(", ")", ":", "\n", "                ", "if", "i", ">", "lev", ":", "\n", "                    ", "del", "headers", "[", "i", "]", "\n", "", "", "emptySection", "=", "True", "\n", "listLevel", "=", "[", "]", "\n", "continue", "\n", "# Handle page title", "\n", "", "elif", "line", ".", "startswith", "(", "'++'", ")", ":", "\n", "            ", "title", "=", "line", "[", "2", ":", "-", "2", "]", "\n", "if", "title", ":", "\n", "                ", "if", "title", "[", "-", "1", "]", "not", "in", "'!?'", ":", "\n", "                    ", "title", "+=", "'.'", "\n", "", "page", ".", "append", "(", "title", ")", "\n", "# handle indents", "\n", "", "", "elif", "line", "[", "0", "]", "==", "':'", ":", "\n", "# page.append(line.lstrip(':*#;'))", "\n", "            ", "continue", "\n", "# handle lists", "\n", "", "elif", "line", "[", "0", "]", "in", "'*#;:'", ":", "\n", "            ", "i", "=", "0", "\n", "# c: current level char", "\n", "# n: next level char", "\n", "for", "c", ",", "n", "in", "izip_longest", "(", "listLevel", ",", "line", ",", "fillvalue", "=", "''", ")", ":", "\n", "                ", "if", "not", "n", "or", "n", "not", "in", "'*#;:'", ":", "# shorter or different", "\n", "                    ", "if", "c", ":", "\n", "                        ", "if", "Extractor", ".", "toHTML", ":", "\n", "                            ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "listLevel", "=", "listLevel", "[", ":", "-", "1", "]", "\n", "continue", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "# n != ''", "\n", "", "", "if", "c", "!=", "n", "and", "(", "not", "c", "or", "(", "c", "not", "in", "';:'", "and", "n", "not", "in", "';:'", ")", ")", ":", "\n", "                    ", "if", "c", ":", "\n", "# close level", "\n", "                        ", "if", "Extractor", ".", "toHTML", ":", "\n", "                            ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "listLevel", "=", "listLevel", "[", ":", "-", "1", "]", "\n", "", "listLevel", "+=", "n", "\n", "if", "Extractor", ".", "toHTML", ":", "\n", "                        ", "page", ".", "append", "(", "listOpen", "[", "n", "]", ")", "\n", "", "", "i", "+=", "1", "\n", "", "n", "=", "line", "[", "i", "-", "1", "]", "# last list char", "\n", "line", "=", "line", "[", "i", ":", "]", ".", "strip", "(", ")", "\n", "if", "line", ":", "# FIXME: n is '\"'", "\n", "                ", "if", "Extractor", ".", "keepLists", ":", "\n", "# keep lists only if they are not in sections to ommit, such as: references, see more etc.", "\n", "#if Extractor.keepLists and not(omitted_sections.__contains__(current_section_name)):", "\n", "# emit open sections", "\n", "                    ", "items", "=", "headers", ".", "items", "(", ")", "\n", "items", ".", "sort", "(", ")", "\n", "for", "i", ",", "v", "in", "items", ":", "\n", "#page.append(v)", "\n", "                        ", "page", ".", "append", "(", "get_seciton_header_name", "(", "i", ",", "v", ")", ")", "\n", "", "headers", ".", "clear", "(", ")", "\n", "# FIXME: use item count for #-lines", "\n", "bullet", "=", "'1. '", "if", "n", "==", "'#'", "else", "'- '", "\n", "#uncomment to keep list bullets", "\n", "# page.append('{0:{1}s}'.format(bullet, len(listLevel)) + line)", "\n", "page", ".", "append", "(", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\". \"", ")", "\n", "\n", "#fixed_list_item = fix_list_item(line)", "\n", "#page.append(fixed_list_item)", "\n", "#if '.' not in fixed_list_item:", "\n", "#    hhh = 789", "\n", "", "elif", "Extractor", ".", "toHTML", ":", "\n", "                    ", "page", ".", "append", "(", "listItem", "[", "n", "]", "%", "line", ")", "\n", "", "", "", "elif", "len", "(", "listLevel", ")", ":", "\n", "            ", "page", ".", "append", "(", "line", ")", "\n", "if", "Extractor", ".", "toHTML", ":", "\n", "                ", "for", "c", "in", "reversed", "(", "listLevel", ")", ":", "\n", "                    ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "", "listLevel", "=", "[", "]", "\n", "\n", "# Drop residuals of lists", "\n", "", "elif", "line", "[", "0", "]", "in", "'{|'", "or", "line", "[", "-", "1", "]", "==", "'}'", ":", "\n", "            ", "continue", "\n", "# Drop irrelevant lines", "\n", "", "elif", "(", "line", "[", "0", "]", "==", "'('", "and", "line", "[", "-", "1", "]", "==", "')'", ")", "or", "line", ".", "strip", "(", "'.-'", ")", "==", "''", ":", "\n", "            ", "continue", "\n", "", "elif", "len", "(", "headers", ")", ":", "\n", "            ", "if", "Extractor", ".", "keepSections", ":", "\n", "                ", "items", "=", "headers", ".", "items", "(", ")", "\n", "items", ".", "sort", "(", ")", "\n", "for", "i", ",", "v", "in", "items", ":", "\n", "# i is the level of the segment. 2 means is top level.", "\n", "#if (i==2):", "\n", "#page.append(wiki_utils.top_segment_seperator)", "\n", "#page.append(wiki_utils.segment_seperator)", "\n", "                    ", "page", ".", "append", "(", "get_seciton_header_name", "(", "i", ",", "v", ")", ")", "\n", "#current_section_name = v", "\n", "\n", "", "", "headers", ".", "clear", "(", ")", "\n", "page", ".", "append", "(", "line", ")", "# first line", "\n", "emptySection", "=", "False", "\n", "", "elif", "not", "emptySection", ":", "\n", "# Drop preformatted", "\n", "            ", "if", "line", "[", "0", "]", "!=", "' '", ":", "# dangerous", "\n", "                ", "page", ".", "append", "(", "line", ")", "\n", "\n", "", "", "", "return", "page", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.handle_unicode": [[2282, 2286], ["int", "unichr"], "function", ["None"], ["", "def", "handle_unicode", "(", "entity", ")", ":", "\n", "    ", "numeric_code", "=", "int", "(", "entity", "[", "2", ":", "-", "1", "]", ")", "\n", "if", "numeric_code", ">=", "0x10000", ":", "return", "''", "\n", "return", "unichr", "(", "numeric_code", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.load_templates": [[2365, 2409], ["enumerate", "codecs.open", "wiki_extractor.pages_from", "codecs.open.close", "logging.info", "wiki_extractor.define_template", "logging.info", "len", "title.find", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.pages_from", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.define_template", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["def", "load_templates", "(", "file", ",", "output_file", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Load templates from :param file:.\n    :param output_file: file where to save templates and modules.\n    \"\"\"", "\n", "global", "templateNamespace", ",", "templatePrefix", "\n", "templatePrefix", "=", "templateNamespace", "+", "':'", "\n", "global", "moduleNamespace", ",", "modulePrefix", "\n", "modulePrefix", "=", "moduleNamespace", "+", "':'", "\n", "if", "output_file", ":", "\n", "        ", "output", "=", "codecs", ".", "open", "(", "output_file", ",", "'wb'", ",", "'utf-8'", ")", "\n", "", "for", "page_count", ",", "page_data", "in", "enumerate", "(", "pages_from", "(", "file", ")", ")", ":", "\n", "        ", "id", ",", "title", ",", "ns", ",", "page", "=", "page_data", "\n", "if", "not", "output_file", "and", "(", "not", "templateNamespace", "or", "\n", "not", "moduleNamespace", ")", ":", "# do not know it yet", "\n", "# reconstruct templateNamespace and moduleNamespace from the first title", "\n", "            ", "if", "ns", "in", "templateKeys", ":", "\n", "                ", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "1", ":", "\n", "                    ", "if", "ns", "==", "'10'", ":", "\n", "                        ", "templateNamespace", "=", "title", "[", ":", "colon", "]", "\n", "templatePrefix", "=", "title", "[", ":", "colon", "+", "1", "]", "\n", "", "elif", "ns", "==", "'828'", ":", "\n", "                        ", "moduleNamespace", "=", "title", "[", ":", "colon", "]", "\n", "modulePrefix", "=", "title", "[", ":", "colon", "+", "1", "]", "\n", "", "", "", "", "if", "ns", "in", "templateKeys", ":", "\n", "            ", "text", "=", "''", ".", "join", "(", "page", ")", "\n", "define_template", "(", "title", ",", "text", ")", "\n", "# save templates and modules to file", "\n", "if", "output_file", ":", "\n", "                ", "output", ".", "write", "(", "'<page>\\n'", ")", "\n", "output", ".", "write", "(", "'   <title>%s</title>\\n'", "%", "title", ")", "\n", "output", ".", "write", "(", "'   <ns>%s</ns>\\n'", "%", "ns", ")", "\n", "output", ".", "write", "(", "'   <id>%s</id>\\n'", "%", "id", ")", "\n", "output", ".", "write", "(", "'   <text>'", ")", "\n", "for", "line", "in", "page", ":", "\n", "                    ", "output", ".", "write", "(", "line", ")", "\n", "", "output", ".", "write", "(", "'   </text>\\n'", ")", "\n", "output", ".", "write", "(", "'</page>\\n'", ")", "\n", "", "", "if", "page_count", "and", "page_count", "%", "100000", "==", "0", ":", "\n", "            ", "logging", ".", "info", "(", "\"Preprocessed %d pages\"", ",", "page_count", ")", "\n", "", "", "if", "output_file", ":", "\n", "        ", "output", ".", "close", "(", ")", "\n", "logging", ".", "info", "(", "\"Saved %d templates to '%s'\"", ",", "len", "(", "templates", ")", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.pages_from": [[2411, 2464], ["line.decode.decode", "tagRE.search", "tagRE.search.group", "page.append", "tagRE.search.group", "tagRE.search.group", "tagRE.search.group", "page.append", "tagRE.search.group", "tagRE.search.start", "tagRE.search.end", "page.append", "page.append", "tagRE.search.group"], "function", ["None"], ["", "", "def", "pages_from", "(", "input", ")", ":", "\n", "    ", "\"\"\"\n    Scans input extracting pages.\n    :return: (id, title, namespace, page), page is a list of lines.\n    \"\"\"", "\n", "# we collect individual lines, since str.join() is significantly faster", "\n", "# than concatenation", "\n", "page", "=", "[", "]", "\n", "id", "=", "None", "\n", "ns", "=", "'0'", "\n", "last_id", "=", "None", "\n", "inText", "=", "False", "\n", "redirect", "=", "False", "\n", "for", "line", "in", "input", ":", "\n", "        ", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "\n", "if", "'<'", "not", "in", "line", ":", "# faster than doing re.search()", "\n", "            ", "if", "inText", ":", "\n", "                ", "page", ".", "append", "(", "line", ")", "\n", "", "continue", "\n", "", "m", "=", "tagRE", ".", "search", "(", "line", ")", "\n", "if", "not", "m", ":", "\n", "            ", "continue", "\n", "", "tag", "=", "m", ".", "group", "(", "2", ")", "\n", "if", "tag", "==", "'page'", ":", "\n", "            ", "page", "=", "[", "]", "\n", "redirect", "=", "False", "\n", "", "elif", "tag", "==", "'id'", "and", "not", "id", ":", "# skip nested <id>", "\n", "            ", "id", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'title'", ":", "\n", "            ", "title", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'ns'", ":", "\n", "            ", "ns", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'redirect'", ":", "\n", "            ", "redirect", "=", "True", "\n", "", "elif", "tag", "==", "'text'", ":", "\n", "            ", "inText", "=", "True", "\n", "line", "=", "line", "[", "m", ".", "start", "(", "3", ")", ":", "m", ".", "end", "(", "3", ")", "]", "\n", "page", ".", "append", "(", "line", ")", "\n", "if", "m", ".", "lastindex", "==", "4", ":", "# open-close", "\n", "                ", "inText", "=", "False", "\n", "", "", "elif", "tag", "==", "'/text'", ":", "\n", "            ", "if", "m", ".", "group", "(", "1", ")", ":", "\n", "                ", "page", ".", "append", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "", "inText", "=", "False", "\n", "", "elif", "inText", ":", "\n", "            ", "page", ".", "append", "(", "line", ")", "\n", "", "elif", "tag", "==", "'/page'", ":", "\n", "            ", "if", "id", "!=", "last_id", "and", "not", "redirect", ":", "\n", "                ", "yield", "(", "id", ",", "title", ",", "ns", ",", "page", ")", "\n", "last_id", "=", "id", "\n", "ns", "=", "'0'", "\n", "", "id", "=", "None", "\n", "page", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.process_dump": [[2466, 2611], ["print", "logging.info", "timeit.default_timer", "multiprocessing.Queue", "max", "multiprocessing.Value", "multiprocessing.Process", "multiprocessing.Process.start", "multiprocessing.Queue", "logging.info", "xrange", "wiki_extractor.pages_from", "fileinput.FileInput.close", "multiprocessing.Queue.put", "multiprocessing.Process.join", "logging.info", "fileinput.FileInput", "line.decode.decode", "tagRE.search", "tagRE.search.group", "timeit.default_timer", "logging.info", "multiprocessing.Process", "multiprocessing.Process.start", "workers.append", "multiprocessing.Queue.put", "w.join", "timeit.default_timer", "str", "tagRE.search.group", "os.path.exists", "timeit.default_timer", "len", "multiprocessing.Queue.put", "knownNamespaces.add", "re.search", "logging.info", "fileinput.FileInput", "wiki_extractor.load_templates", "fileinput.FileInput.close", "logging.info", "wiki_extractor.load_templates", "fileinput.FileInput.close", "fileinput.FileInput", "logging.info", "int", "int", "m.group.rfind", "tagRE.search.group", "tagRE.search.group", "re.search", "ValueError", "time.sleep", "tagRE.search.group"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.pages_from", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.add", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.load_templates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.load_templates", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close"], ["", "", "", "def", "process_dump", "(", "input_file", ",", "template_file", ",", "out_file", ",", "file_size", ",", "file_compress", ",", "\n", "process_count", ",", "max_articles_count", "=", "10000000", ")", ":", "\n", "    ", "\"\"\"\n    :param input_file: name of the wikipedia dump file; '-' to read from stdin\n    :param template_file: optional file with template definitions.\n    :param out_file: directory where to store extracted data, or '-' for stdout\n    :param file_size: max size of each extracted file, or None for no max (one file)\n    :param file_compress: whether to compress files with bzip.\n    :param process_count: number of extraction processes to spawn.\n    \"\"\"", "\n", "global", "urlbase", "\n", "global", "knownNamespaces", "\n", "global", "templateNamespace", ",", "templatePrefix", "\n", "global", "moduleNamespace", ",", "modulePrefix", "\n", "\n", "print", "(", "'max articles to extract: '", "+", "str", "(", "max_articles_count", ")", ")", "\n", "\n", "if", "input_file", "==", "'-'", ":", "\n", "        ", "input", "=", "sys", ".", "stdin", "\n", "", "else", ":", "\n", "        ", "input", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "\n", "# collect siteinfo", "\n", "", "for", "line", "in", "input", ":", "\n", "        ", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "\n", "m", "=", "tagRE", ".", "search", "(", "line", ")", "\n", "if", "not", "m", ":", "\n", "            ", "continue", "\n", "", "tag", "=", "m", ".", "group", "(", "2", ")", "\n", "if", "tag", "==", "'base'", ":", "\n", "# discover urlbase from the xml dump file", "\n", "# /mediawiki/siteinfo/base", "\n", "            ", "base", "=", "m", ".", "group", "(", "3", ")", "\n", "urlbase", "=", "base", "[", ":", "base", ".", "rfind", "(", "\"/\"", ")", "]", "\n", "", "elif", "tag", "==", "'namespace'", ":", "\n", "            ", "knownNamespaces", ".", "add", "(", "m", ".", "group", "(", "3", ")", ")", "\n", "if", "re", ".", "search", "(", "'key=\"10\"'", ",", "line", ")", ":", "\n", "                ", "templateNamespace", "=", "m", ".", "group", "(", "3", ")", "\n", "templatePrefix", "=", "templateNamespace", "+", "':'", "\n", "", "elif", "re", ".", "search", "(", "'key=\"828\"'", ",", "line", ")", ":", "\n", "                ", "moduleNamespace", "=", "m", ".", "group", "(", "3", ")", "\n", "modulePrefix", "=", "moduleNamespace", "+", "':'", "\n", "", "", "elif", "tag", "==", "'/siteinfo'", ":", "\n", "            ", "break", "\n", "\n", "", "", "if", "Extractor", ".", "expand_templates", ":", "\n", "# preprocess", "\n", "        ", "template_load_start", "=", "default_timer", "(", ")", "\n", "if", "template_file", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "template_file", ")", ":", "\n", "                ", "logging", ".", "info", "(", "\"Preprocessing '%s' to collect template definitions: this may take some time.\"", ",", "template_file", ")", "\n", "file", "=", "fileinput", ".", "FileInput", "(", "template_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "load_templates", "(", "file", ")", "\n", "file", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                ", "if", "input_file", "==", "'-'", ":", "\n", "# can't scan then reset stdin; must error w/ suggestion to specify template_file", "\n", "                    ", "raise", "ValueError", "(", "\"to use templates with stdin dump, must supply explicit template-file\"", ")", "\n", "", "logging", ".", "info", "(", "\"Preprocessing '%s' to collect template definitions: this may take some time.\"", ",", "input_file", ")", "\n", "load_templates", "(", "input", ",", "template_file", ")", "\n", "input", ".", "close", "(", ")", "\n", "input", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "", "", "template_load_elapsed", "=", "default_timer", "(", ")", "-", "template_load_start", "\n", "logging", ".", "info", "(", "\"Loaded %d templates in %.1fs\"", ",", "len", "(", "templates", ")", ",", "template_load_elapsed", ")", "\n", "\n", "# process pages", "\n", "", "logging", ".", "info", "(", "\"Starting page extraction from %s.\"", ",", "input_file", ")", "\n", "extract_start", "=", "default_timer", "(", ")", "\n", "\n", "# Parallel Map/Reduce:", "\n", "# - pages to be processed are dispatched to workers", "\n", "# - a reduce process collects the results, sort them and print them.", "\n", "\n", "maxsize", "=", "10", "*", "process_count", "\n", "# output queue", "\n", "output_queue", "=", "Queue", "(", "maxsize", "=", "maxsize", ")", "\n", "\n", "if", "out_file", "==", "'-'", ":", "\n", "        ", "out_file", "=", "None", "\n", "\n", "", "worker_count", "=", "max", "(", "1", ",", "process_count", ")", "\n", "\n", "# load balancing", "\n", "max_spool_length", "=", "10000", "\n", "spool_length", "=", "Value", "(", "'i'", ",", "0", ",", "lock", "=", "False", ")", "\n", "\n", "# reduce job that sorts and prints output", "\n", "reduce", "=", "Process", "(", "target", "=", "reduce_process", ",", "\n", "args", "=", "(", "output_queue", ",", "spool_length", ",", "\n", "out_file", ",", "file_size", ",", "file_compress", ")", ")", "\n", "reduce", ".", "start", "(", ")", "\n", "\n", "# initialize jobs queue", "\n", "jobs_queue", "=", "Queue", "(", "maxsize", "=", "maxsize", ")", "\n", "\n", "# start worker processes", "\n", "logging", ".", "info", "(", "\"Using %d extract processes.\"", ",", "worker_count", ")", "\n", "workers", "=", "[", "]", "\n", "for", "i", "in", "xrange", "(", "worker_count", ")", ":", "\n", "        ", "extractor", "=", "Process", "(", "target", "=", "extract_process", ",", "\n", "args", "=", "(", "i", ",", "jobs_queue", ",", "output_queue", ")", ")", "\n", "extractor", ".", "daemon", "=", "True", "# only live while parent process lives", "\n", "extractor", ".", "start", "(", ")", "\n", "workers", ".", "append", "(", "extractor", ")", "\n", "\n", "# Mapper process", "\n", "", "page_num", "=", "0", "\n", "\n", "for", "page_data", "in", "pages_from", "(", "input", ")", ":", "\n", "        ", "id", ",", "title", ",", "ns", ",", "page", "=", "page_data", "\n", "if", "ns", "not", "in", "templateKeys", ":", "\n", "# slow down", "\n", "            ", "delay", "=", "0", "\n", "if", "spool_length", ".", "value", ">", "max_spool_length", ":", "\n", "# reduce to 10%", "\n", "                ", "while", "spool_length", ".", "value", ">", "max_spool_length", "/", "10", ":", "\n", "                    ", "time", ".", "sleep", "(", "10", ")", "\n", "delay", "+=", "10", "\n", "", "", "if", "delay", ":", "\n", "                ", "logging", ".", "info", "(", "'Delay %ds'", ",", "delay", ")", "\n", "", "job", "=", "(", "id", ",", "title", ",", "page", ",", "page_num", ")", "\n", "jobs_queue", ".", "put", "(", "job", ")", "# goes to any available extract_process", "\n", "page_num", "+=", "1", "\n", "if", "(", "int", "(", "page_num", ")", ">", "int", "(", "max_articles_count", ")", ")", ":", "\n", "                ", "break", "\n", "", "", "page", "=", "None", "# free memory", "\n", "\n", "", "input", ".", "close", "(", ")", "\n", "\n", "# signal termination", "\n", "for", "_", "in", "workers", ":", "\n", "        ", "jobs_queue", ".", "put", "(", "None", ")", "\n", "# wait for workers to terminate", "\n", "", "for", "w", "in", "workers", ":", "\n", "        ", "w", ".", "join", "(", ")", "\n", "\n", "# signal end of work to reduce process", "\n", "", "output_queue", ".", "put", "(", "None", ")", "\n", "# wait for it to finish", "\n", "reduce", ".", "join", "(", ")", "\n", "\n", "extract_duration", "=", "default_timer", "(", ")", "-", "extract_start", "\n", "extract_rate", "=", "page_num", "/", "extract_duration", "\n", "logging", ".", "info", "(", "\"Finished %d-process extraction of %d articles in %.1fs (%.1f art/s)\"", ",", "\n", "process_count", ",", "page_num", ",", "extract_duration", ",", "extract_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.extract_process": [[2617, 2642], ["cStringIO.StringIO", "cStringIO.StringIO.close", "jobs_queue.get", "output_queue.put", "cStringIO.StringIO.truncate", "logging.debug", "wiki_extractor.Extractor", "wiki_extractor.Extractor.extract", "cStringIO.StringIO.getvalue", "logging.error"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract"], ["", "def", "extract_process", "(", "i", ",", "jobs_queue", ",", "output_queue", ")", ":", "\n", "    ", "\"\"\"Pull tuples of raw page content, do CPU/regex-heavy fixup, push finished text\n    :param i: process id.\n    :param jobs_queue: where to get jobs.\n    :param output_queue: where to queue extracted text for output.\n    \"\"\"", "\n", "out", "=", "StringIO", "(", ")", "# memory buffer", "\n", "while", "True", ":", "\n", "        ", "job", "=", "jobs_queue", ".", "get", "(", ")", "# job is (id, title, page, page_num)", "\n", "if", "job", ":", "\n", "            ", "id", ",", "title", ",", "page", ",", "page_num", "=", "job", "\n", "try", ":", "\n", "                ", "e", "=", "Extractor", "(", "*", "job", "[", ":", "3", "]", ")", "# (id, title, page)", "\n", "page", "=", "None", "# free memory", "\n", "e", ".", "extract", "(", "out", ")", "\n", "text", "=", "out", ".", "getvalue", "(", ")", "\n", "", "except", ":", "\n", "                ", "text", "=", "''", "\n", "logging", ".", "error", "(", "'Processing page: %s %s'", ",", "id", ",", "title", ")", "\n", "", "output_queue", ".", "put", "(", "(", "page_num", ",", "text", ")", ")", "\n", "out", ".", "truncate", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "logging", ".", "debug", "(", "'Quit extractor'", ")", "\n", "break", "\n", "", "", "out", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.reduce_process": [[2645, 2695], ["timeit.default_timer", "wiki_extractor.NextFile", "wiki_extractor.OutputSplitter", "wiki_extractor.OutputSplitter.close", "logging.warn", "wiki_extractor.OutputSplitter.write", "len", "output_queue.get", "len", "spool.pop", "logging.info", "timeit.default_timer", "len", "logging.debug", "len", "timeit.default_timer"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["def", "reduce_process", "(", "output_queue", ",", "spool_length", ",", "\n", "out_file", "=", "None", ",", "file_size", "=", "0", ",", "file_compress", "=", "True", ")", ":", "\n", "    ", "\"\"\"Pull finished article text, write series of files (or stdout)\n    :param output_queue: text to be output.\n    :param spool_length: spool length.\n    :param out_file: filename where to print.\n    :param file_size: max file size.\n    :param file_compress: whether to compress output.\n    \"\"\"", "\n", "\n", "if", "out_file", ":", "\n", "        ", "nextFile", "=", "NextFile", "(", "out_file", ")", "\n", "output", "=", "OutputSplitter", "(", "nextFile", ",", "file_size", ",", "file_compress", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "sys", ".", "stdout", "\n", "if", "file_compress", ":", "\n", "            ", "logging", ".", "warn", "(", "\"writing to stdout, so no output compression (use an external tool)\"", ")", "\n", "\n", "", "", "interval_start", "=", "default_timer", "(", ")", "\n", "# FIXME: use a heap", "\n", "spool", "=", "{", "}", "# collected pages", "\n", "next_page", "=", "0", "# sequence numbering of page", "\n", "while", "True", ":", "\n", "        ", "if", "next_page", "in", "spool", ":", "\n", "            ", "output", ".", "write", "(", "spool", ".", "pop", "(", "next_page", ")", ")", "\n", "next_page", "+=", "1", "\n", "# tell mapper our load:", "\n", "spool_length", ".", "value", "=", "len", "(", "spool", ")", "\n", "# progress report", "\n", "if", "next_page", "%", "report_period", "==", "0", ":", "\n", "                ", "interval_rate", "=", "report_period", "/", "(", "default_timer", "(", ")", "-", "interval_start", ")", "\n", "logging", ".", "info", "(", "\"Extracted %d articles (%.1f art/s)\"", ",", "\n", "next_page", ",", "interval_rate", ")", "\n", "interval_start", "=", "default_timer", "(", ")", "\n", "", "", "else", ":", "\n", "# mapper puts None to signal finish", "\n", "            ", "pair", "=", "output_queue", ".", "get", "(", ")", "\n", "if", "not", "pair", ":", "\n", "                ", "break", "\n", "", "page_num", ",", "text", "=", "pair", "\n", "spool", "[", "page_num", "]", "=", "text", "\n", "# tell mapper our load:", "\n", "spool_length", ".", "value", "=", "len", "(", "spool", ")", "\n", "# FIXME: if an extractor dies, process stalls; the other processes", "\n", "# continue to produce pairs, filling up memory.", "\n", "if", "len", "(", "spool", ")", ">", "200", ":", "\n", "                ", "logging", ".", "debug", "(", "'Collected %d, waiting: %d, %d'", ",", "len", "(", "spool", ")", ",", "\n", "next_page", ",", "next_page", "==", "page_num", ")", "\n", "", "", "", "if", "output", "!=", "sys", ".", "stdout", ":", "\n", "        ", "output", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.main": [[2703, 2819], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logging.getLogger", "wiki_extractor.process_dump", "multiprocessing.cpu_count", "set", "logging.getLogger.setLevel", "logging.getLogger.setLevel", "wiki_extractor.ignoreTag", "fileinput.FileInput", "wiki_extractor.pages_from", "fileinput.FileInput.close", "os.path.basename", "int", "ValueError", "logging.error", "parser.parse_args.namespaces.split", "os.path.exists", "wiki_extractor.Extractor.extract", "os.path.isdir", "os.makedirs", "parser.parse_args.bytes[].lower", "logging.error", "open", "wiki_extractor.load_templates", "wiki_extractor.Extractor"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.process_dump", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.ignoreTag", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.pages_from", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.Extractor.extract", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.load_templates"], ["def", "main", "(", ")", ":", "\n", "    ", "global", "urlbase", ",", "acceptedNamespaces", "\n", "global", "templateCache", ",", "escape_doc", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "prog", "=", "os", ".", "path", ".", "basename", "(", "sys", ".", "argv", "[", "0", "]", ")", ",", "\n", "formatter_class", "=", "argparse", ".", "RawDescriptionHelpFormatter", ",", "\n", "description", "=", "__doc__", ")", "\n", "parser", ".", "add_argument", "(", "\"input\"", ",", "\n", "help", "=", "\"XML wiki dump file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--article_count\"", ",", "help", "=", "\"Max number of wikipedia articles to extract\"", ",", "default", "=", "1000000", ")", "\n", "groupO", "=", "parser", ".", "add_argument_group", "(", "'Output'", ")", "\n", "groupO", ".", "add_argument", "(", "\"-o\"", ",", "\"--output\"", ",", "default", "=", "\"text\"", ",", "\n", "help", "=", "\"directory for extracted files (or '-' for dumping to stdout)\"", ")", "\n", "groupO", ".", "add_argument", "(", "\"-b\"", ",", "\"--bytes\"", ",", "default", "=", "\"1M\"", ",", "\n", "help", "=", "\"maximum bytes per output file (default %(default)s)\"", ",", "\n", "metavar", "=", "\"n[KMG]\"", ")", "\n", "groupO", ".", "add_argument", "(", "\"-c\"", ",", "\"--compress\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"compress output files using bzip\"", ")", "\n", "\n", "groupP", "=", "parser", ".", "add_argument_group", "(", "'Processing'", ")", "\n", "groupP", ".", "add_argument", "(", "\"--html\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"produce HTML output, subsumes --links\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-l\"", ",", "\"--links\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve links\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-s\"", ",", "\"--sections\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve sections\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--lists\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve lists\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-ns\"", ",", "\"--namespaces\"", ",", "default", "=", "\"\"", ",", "metavar", "=", "\"ns1,ns2\"", ",", "\n", "help", "=", "\"accepted namespaces\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--templates\"", ",", "\n", "help", "=", "\"use or create file containing templates\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--no-templates\"", ",", "action", "=", "\"store_false\"", ",", "\n", "help", "=", "\"Do not expand templates\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--escapedoc\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"use to escape the contents of the output <doc>...</doc>\"", ")", "\n", "default_process_count", "=", "cpu_count", "(", ")", "-", "1", "\n", "parser", ".", "add_argument", "(", "\"--processes\"", ",", "type", "=", "int", ",", "default", "=", "default_process_count", ",", "\n", "help", "=", "\"Number of processes to use (default %(default)s)\"", ")", "\n", "\n", "groupS", "=", "parser", ".", "add_argument_group", "(", "'Special'", ")", "\n", "groupS", ".", "add_argument", "(", "\"-q\"", ",", "\"--quiet\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"suppress reporting progress info\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"--debug\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"print debug info\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"-a\"", ",", "\"--article\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"analyze a file containing a single article (debug option)\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"-v\"", ",", "\"--version\"", ",", "action", "=", "\"version\"", ",", "\n", "version", "=", "'%(prog)s '", "+", "version", ",", "\n", "help", "=", "\"print program version\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "Extractor", ".", "keepLinks", "=", "args", ".", "links", "\n", "Extractor", ".", "keepSections", "=", "args", ".", "sections", "\n", "Extractor", ".", "keepLists", "=", "args", ".", "lists", "\n", "Extractor", ".", "toHTML", "=", "args", ".", "html", "\n", "if", "args", ".", "html", ":", "\n", "        ", "Extractor", ".", "keepLinks", "=", "True", "\n", "\n", "", "Extractor", ".", "expand_templates", "=", "args", ".", "no_templates", "\n", "escape_doc", "=", "args", ".", "escapedoc", "\n", "\n", "try", ":", "\n", "        ", "power", "=", "'kmg'", ".", "find", "(", "args", ".", "bytes", "[", "-", "1", "]", ".", "lower", "(", ")", ")", "+", "1", "\n", "file_size", "=", "int", "(", "args", ".", "bytes", "[", ":", "-", "1", "]", ")", "*", "1024", "**", "power", "\n", "if", "file_size", "<", "minFileSize", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "", "", "except", "ValueError", ":", "\n", "        ", "logging", ".", "error", "(", "'Insufficient or invalid size: %s'", ",", "args", ".", "bytes", ")", "\n", "return", "\n", "\n", "", "if", "args", ".", "namespaces", ":", "\n", "        ", "acceptedNamespaces", "=", "set", "(", "args", ".", "namespaces", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "FORMAT", "=", "'%(levelname)s: %(message)s'", "\n", "logging", ".", "basicConfig", "(", "format", "=", "FORMAT", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "        ", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "", "if", "args", ".", "debug", ":", "\n", "        ", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "", "input_file", "=", "args", ".", "input", "\n", "\n", "if", "not", "Extractor", ".", "keepLinks", ":", "\n", "        ", "ignoreTag", "(", "'a'", ")", "\n", "\n", "# sharing cache of parser templates is too slow:", "\n", "# manager = Manager()", "\n", "# templateCache = manager.dict()", "\n", "\n", "", "if", "args", ".", "article", ":", "\n", "        ", "if", "args", ".", "templates", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "templates", ")", ":", "\n", "                ", "with", "open", "(", "args", ".", "templates", ")", "as", "file", ":", "\n", "                    ", "load_templates", "(", "file", ")", "\n", "\n", "", "", "", "file", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "for", "page_data", "in", "pages_from", "(", "file", ")", ":", "\n", "            ", "id", ",", "title", ",", "ns", ",", "page", "=", "page_data", "\n", "Extractor", "(", "id", ",", "title", ",", "page", ")", ".", "extract", "(", "sys", ".", "stdout", ")", "\n", "", "file", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "output_path", "=", "args", ".", "output", "\n", "if", "output_path", "!=", "'-'", "and", "not", "os", ".", "path", ".", "isdir", "(", "output_path", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_path", ")", "\n", "", "except", ":", "\n", "            ", "logging", ".", "error", "(", "'Could not create: %s'", ",", "output_path", ")", "\n", "return", "\n", "\n", "", "", "process_dump", "(", "input_file", ",", "args", ".", "templates", ",", "output_path", ",", "file_size", ",", "\n", "args", ".", "compress", ",", "args", ".", "processes", ",", "args", ".", "article_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_seperator": [[4, 6], ["str"], "function", ["None"], ["def", "get_segment_seperator", "(", "level", ",", "name", ")", ":", "\n", "    ", "return", "segment_seperator", "+", "\",\"", "+", "str", "(", "level", ")", "+", "\",\"", "+", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt": [[7, 11], ["str", "str"], "function", ["None"], ["", "def", "get_seperator_foramt", "(", "levels", "=", "None", ")", ":", "\n", "    ", "level_format", "=", "'\\d'", "if", "levels", "==", "None", "else", "'['", "+", "str", "(", "levels", "[", "0", "]", ")", "+", "'-'", "+", "str", "(", "levels", "[", "1", "]", ")", "+", "']'", "\n", "seperator_fromat", "=", "segment_seperator", "+", "','", "+", "level_format", "+", "\",.*?\\.\"", "\n", "return", "seperator_fromat", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.is_seperator_line": [[12, 14], ["line.startswith"], "function", ["None"], ["", "def", "is_seperator_line", "(", "line", ")", ":", "\n", "    ", "return", "line", ".", "startswith", "(", "segment_seperator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_level": [[15, 17], ["int", "seperator_line.split"], "function", ["None"], ["", "def", "get_segment_level", "(", "seperator_line", ")", ":", "\n", "    ", "return", "int", "(", "seperator_line", ".", "split", "(", "','", ")", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_name": [[18, 20], ["seperator_line.split"], "function", ["None"], ["", "def", "get_segment_name", "(", "seperator_line", ")", ":", "\n", "    ", "return", "seperator_line", ".", "split", "(", "','", ")", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token": [[21, 23], ["None"], "function", ["None"], ["", "def", "get_list_token", "(", ")", ":", "\n", "    ", "return", "\"***LIST***\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_formula_token": [[24, 26], ["None"], "function", ["None"], ["", "def", "get_formula_token", "(", ")", ":", "\n", "    ", "return", "\"***formula***\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_codesnipet_token": [[27, 29], ["None"], "function", ["None"], ["", "def", "get_codesnipet_token", "(", ")", ":", "\n", "    ", "return", "\"***codice***\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_special_tokens": [[30, 36], ["special_tokens.append", "special_tokens.append", "special_tokens.append", "wiki_utils.get_list_token", "wiki_utils.get_formula_token", "wiki_utils.get_codesnipet_token"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_formula_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_codesnipet_token"], ["", "def", "get_special_tokens", "(", ")", ":", "\n", "    ", "special_tokens", "=", "[", "]", "\n", "special_tokens", ".", "append", "(", "get_list_token", "(", ")", ")", "\n", "special_tokens", ".", "append", "(", "get_formula_token", "(", ")", ")", "\n", "special_tokens", ".", "append", "(", "get_codesnipet_token", "(", ")", ")", "\n", "return", "special_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.WikipediaDataSet.__init__": [[104, 124], ["list", "len", "RuntimeError", "pathlib2.Path().glob", "wiki_loader.get_files", "pathlib2.Path", "wiki_loader.get_cache_path", "get_cache_path.read_text().splitlines", "get_cache_path.exists", "wiki_loader.cache_wiki_filenames", "pathlib2.Path", "get_cache_path.read_text"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_files", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_cache_path", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.cache_wiki_filenames"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "word2vec", ",", "train", "=", "True", ",", "manifesto", "=", "False", ",", "folder", "=", "False", ",", "high_granularity", "=", "False", ")", ":", "\n", "\n", "        ", "if", "(", "manifesto", ")", ":", "\n", "            ", "self", ".", "textfiles", "=", "list", "(", "Path", "(", "root", ")", ".", "glob", "(", "'*'", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "(", "folder", ")", ":", "\n", "                ", "self", ".", "textfiles", "=", "get_files", "(", "root", ")", "\n", "", "else", ":", "\n", "                ", "root_path", "=", "Path", "(", "root", ")", "\n", "cache_path", "=", "get_cache_path", "(", "root_path", ")", "\n", "if", "not", "cache_path", ".", "exists", "(", ")", ":", "\n", "                    ", "cache_wiki_filenames", "(", "root_path", ")", "\n", "", "self", ".", "textfiles", "=", "cache_path", ".", "read_text", "(", ")", ".", "splitlines", "(", ")", "\n", "\n", "", "", "if", "len", "(", "self", ".", "textfiles", ")", "==", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Found 0 images in subfolders of: {}'", ".", "format", "(", "root", ")", ")", "\n", "", "self", ".", "train", "=", "train", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "word2vec", "=", "word2vec", "\n", "self", ".", "high_granularity", "=", "high_granularity", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.WikipediaDataSet.__getitem__": [[125, 130], ["wiki_loader.read_wiki_file", "pathlib2.Path"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.read_wiki_file"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", "=", "self", ".", "textfiles", "[", "index", "]", "\n", "\n", "return", "read_wiki_file", "(", "Path", "(", "path", ")", ",", "self", ".", "word2vec", ",", "ignore_list", "=", "True", ",", "remove_special_tokens", "=", "True", ",", "\n", "high_granularity", "=", "self", ".", "high_granularity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.WikipediaDataSet.__len__": [[131, 133], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "textfiles", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_files": [[13, 17], ["pathlib2.Path().glob", "str", "pathlib2.Path", "p.is_file"], "function", ["None"], ["def", "get_files", "(", "path", ")", ":", "\n", "    ", "all_objects", "=", "Path", "(", "path", ")", ".", "glob", "(", "'**/*'", ")", "\n", "files", "=", "[", "str", "(", "p", ")", "for", "p", "in", "all_objects", "if", "p", ".", "is_file", "(", ")", "]", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_cache_path": [[19, 22], ["None"], "function", ["None"], ["", "def", "get_cache_path", "(", "wiki_folder", ")", ":", "\n", "    ", "cache_file_path", "=", "wiki_folder", "/", "'paths_cache'", "\n", "return", "cache_file_path", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.cache_wiki_filenames": [[24, 31], ["pathlib2.Path().glob", "wiki_loader.get_cache_path", "get_cache_path.open", "pathlib2.Path", "f.write", "unicode"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_cache_path", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "def", "cache_wiki_filenames", "(", "wiki_folder", ")", ":", "\n", "    ", "files", "=", "Path", "(", "wiki_folder", ")", ".", "glob", "(", "'*/*/*/*'", ")", "\n", "cache_file_path", "=", "get_cache_path", "(", "wiki_folder", ")", "\n", "\n", "with", "cache_file_path", ".", "open", "(", "'w+'", ")", "as", "f", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "f", ".", "write", "(", "unicode", "(", "file", ")", "+", "u'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.clean_section": [[33, 36], ["section.strip"], "function", ["None"], ["", "", "", "def", "clean_section", "(", "section", ")", ":", "\n", "    ", "cleaned_section", "=", "section", ".", "strip", "(", "'\\n'", ")", "\n", "return", "cleaned_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_scections_from_text": [[38, 55], ["re.split", "wiki_utils.get_seperator_foramt", "wiki_utils.get_seperator_foramt", "wiki_utils.get_seperator_foramt", "re.sub", "re.sub.strip().split", "len", "re.sub.strip", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt"], ["", "def", "get_scections_from_text", "(", "txt", ",", "high_granularity", "=", "True", ")", ":", "\n", "    ", "sections_to_keep_pattern", "=", "wiki_utils", ".", "get_seperator_foramt", "(", ")", "if", "high_granularity", "else", "wiki_utils", ".", "get_seperator_foramt", "(", "\n", "(", "1", ",", "2", ")", ")", "\n", "if", "not", "high_granularity", ":", "\n", "# if low granularity required we should flatten segments within segemnt level 2", "\n", "        ", "pattern_to_ommit", "=", "wiki_utils", ".", "get_seperator_foramt", "(", "(", "3", ",", "999", ")", ")", "\n", "txt", "=", "re", ".", "sub", "(", "pattern_to_ommit", ",", "\"\"", ",", "txt", ")", "\n", "\n", "#delete empty lines after re.sub()", "\n", "sentences", "=", "[", "s", "for", "s", "in", "txt", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "if", "len", "(", "s", ")", ">", "0", "and", "s", "!=", "\"\\n\"", "]", "\n", "txt", "=", "'\\n'", ".", "join", "(", "sentences", ")", ".", "strip", "(", "'\\n'", ")", "\n", "\n", "\n", "", "all_sections", "=", "re", ".", "split", "(", "sections_to_keep_pattern", ",", "txt", ")", "\n", "non_empty_sections", "=", "[", "s", "for", "s", "in", "all_sections", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "\n", "return", "non_empty_sections", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.get_sections": [[57, 67], ["open", "open.read", "open.close", "file.read.decode().strip", "str", "wiki_loader.clean_section", "file.read.decode", "wiki_loader.get_scections_from_text"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.clean_section", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_scections_from_text"], ["", "def", "get_sections", "(", "path", ",", "high_granularity", "=", "True", ")", ":", "\n", "    ", "file", "=", "open", "(", "str", "(", "path", ")", ",", "\"r\"", ")", "\n", "raw_content", "=", "file", ".", "read", "(", ")", "\n", "file", ".", "close", "(", ")", "\n", "\n", "clean_txt", "=", "raw_content", ".", "decode", "(", "'utf-8'", ")", ".", "strip", "(", ")", "\n", "\n", "sections", "=", "[", "clean_section", "(", "s", ")", "for", "s", "in", "get_scections_from_text", "(", "clean_txt", ",", "high_granularity", ")", "]", "\n", "\n", "return", "sections", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_loader.read_wiki_file": [[69, 101], ["wiki_loader.get_sections", "section.split", "len", "targets.append", "len", "re.sub.encode", "baselines.textseg.extract_sentence_words", "wiki_utils.get_list_token", "len", "data.append", "logger.info", "re.sub", "data.append", "data.append", "len", "baselines.textseg.word_model"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_sections", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.word_model"], ["", "def", "read_wiki_file", "(", "path", ",", "word2vec", ",", "remove_preface_segment", "=", "True", ",", "ignore_list", "=", "False", ",", "remove_special_tokens", "=", "False", ",", "\n", "return_as_sentences", "=", "False", ",", "high_granularity", "=", "True", ",", "only_letters", "=", "False", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "targets", "=", "[", "]", "\n", "all_sections", "=", "get_sections", "(", "path", ",", "high_granularity", ")", "\n", "required_sections", "=", "all_sections", "[", "1", ":", "]", "if", "remove_preface_segment", "and", "len", "(", "all_sections", ")", ">", "0", "else", "all_sections", "\n", "required_non_empty_sections", "=", "[", "section", "for", "section", "in", "required_sections", "if", "len", "(", "section", ")", ">", "0", "and", "section", "!=", "\"\\n\"", "]", "\n", "\n", "for", "section", "in", "required_non_empty_sections", ":", "\n", "        ", "sentences", "=", "section", ".", "split", "(", "'\\n'", ")", "\n", "if", "sentences", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "is_list_sentence", "=", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", "==", "sentence", ".", "encode", "(", "'utf-8'", ")", "\n", "if", "ignore_list", "and", "is_list_sentence", ":", "\n", "                    ", "continue", "\n", "", "if", "not", "return_as_sentences", ":", "\n", "                    ", "sentence_words", "=", "extract_sentence_words", "(", "sentence", ",", "remove_special_tokens", "=", "remove_special_tokens", ")", "\n", "if", "1", "<=", "len", "(", "sentence_words", ")", ":", "\n", "                        ", "data", ".", "append", "(", "[", "word_model", "(", "word", ",", "word2vec", ")", "for", "word", "in", "sentence_words", "]", ")", "\n", "", "else", ":", "\n", "#raise ValueError('Sentence in wikipedia file is empty')", "\n", "                        ", "logger", ".", "info", "(", "'Sentence in wikipedia file is empty'", ")", "\n", "", "", "else", ":", "# for the annotation. keep sentence as is.", "\n", "                    ", "if", "(", "only_letters", ")", ":", "\n", "                        ", "sentence", "=", "re", ".", "sub", "(", "'[^a-zA-Z0-9 ]+'", ",", "''", ",", "sentence", ")", "\n", "data", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                        ", "data", ".", "append", "(", "sentence", ")", "\n", "", "", "", "if", "data", ":", "\n", "                ", "targets", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "\n", "", "", "", "return", "data", ",", "targets", ",", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.get_punkt": [[15, 28], ["nltk.data.load", "nltk.download", "nltk.data.load"], "function", ["None"], ["def", "get_punkt", "(", ")", ":", "\n", "    ", "global", "sentence_tokenizer", "\n", "if", "sentence_tokenizer", ":", "\n", "        ", "return", "sentence_tokenizer", "\n", "\n", "", "try", ":", "\n", "        ", "tokenizer", "=", "nltk", ".", "data", ".", "load", "(", "'tokenizers/punkt/english.pickle'", ")", "\n", "", "except", ":", "\n", "        ", "nltk", ".", "download", "(", "'punkt'", ")", "\n", "tokenizer", "=", "nltk", ".", "data", ".", "load", "(", "'tokenizers/punkt/english.pickle'", ")", "\n", "\n", "", "sentence_tokenizer", "=", "tokenizer", "\n", "return", "sentence_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.get_words_tokenizer": [[29, 37], ["nltk.tokenize.RegexpTokenizer"], "function", ["None"], ["", "def", "get_words_tokenizer", "(", ")", ":", "\n", "    ", "global", "words_tokenizer", "\n", "\n", "if", "words_tokenizer", ":", "\n", "        ", "return", "words_tokenizer", "\n", "\n", "", "words_tokenizer", "=", "RegexpTokenizer", "(", "r'\\w+'", ")", "\n", "return", "words_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentence_with_list": [[40, 51], ["sentence.endswith", "splited_sentence.append", "wiki_utils.get_list_token", "sentence.split", "wiki_utils.get_list_token", "len", "wiki_utils.get_list_token"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token"], ["", "def", "split_sentence_with_list", "(", "sentence", ")", ":", "\n", "\n", "    ", "list_pattern", "=", "\"\\n\"", "+", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", "\n", "if", "sentence", ".", "endswith", "(", "list_pattern", ")", ":", "\n", "#splited_sentence = [str for str in sentence.encode('utf-8').split(\"\\n\" + wiki_utils.get_list_token() + \".\") if len(str) > 0]", "\n", "        ", "splited_sentence", "=", "[", "str", "for", "str", "in", "sentence", ".", "split", "(", "\"\\n\"", "+", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", ")", "if", "\n", "len", "(", "str", ")", ">", "0", "]", "\n", "splited_sentence", ".", "append", "(", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", ")", "\n", "return", "splited_sentence", "\n", "", "else", ":", "\n", "        ", "return", "[", "sentence", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentece_colon_new_line": [[52, 65], ["sentence.split", "range", "len", "len", "new_sentences.append", "len", "len", "new_sentences.append"], "function", ["None"], ["", "", "def", "split_sentece_colon_new_line", "(", "sentence", ")", ":", "\n", "\n", "    ", "splited_sentence", "=", "sentence", ".", "split", "(", "\":\\n\"", ")", "\n", "if", "(", "len", "(", "splited_sentence", ")", "==", "1", ")", ":", "\n", "        ", "return", "splited_sentence", "\n", "", "new_sentences", "=", "[", "]", "\n", "# -1 . not to add \":\" to last sentence", "\n", "for", "i", "in", "range", "(", "len", "(", "splited_sentence", ")", "-", "1", ")", ":", "\n", "        ", "if", "(", "len", "(", "splited_sentence", "[", "i", "]", ")", ">", "0", ")", ":", "\n", "            ", "new_sentences", ".", "append", "(", "splited_sentence", "[", "i", "]", "+", "\":\"", ")", "\n", "", "", "if", "(", "len", "(", "splited_sentence", "[", "-", "1", "]", ")", ">", "0", ")", ":", "\n", "        ", "new_sentences", ".", "append", "(", "splited_sentence", "[", "-", "1", "]", ")", "\n", "", "return", "new_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_long_sentences_with_backslash_n": [[66, 80], ["text_manipulation.extract_sentence_words", "len", "sentence.split", "new_sentences.extend", "new_sentences.append"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words"], ["", "def", "split_long_sentences_with_backslash_n", "(", "max_words_in_sentence", ",", "sentences", ",", "doc_id", ")", ":", "\n", "    ", "new_sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "sentence_words", "=", "extract_sentence_words", "(", "sentence", ")", "\n", "if", "len", "(", "sentence_words", ")", ">", "max_words_in_sentence", ":", "\n", "            ", "splitted_sentences", "=", "sentence", ".", "split", "(", "'\\n'", ")", "\n", "# if len(splitted_sentences) > 1:", "\n", "# logger.info(\"Sentence with backslash was splitted. Doc Id: \" + str(doc_id) +\"   Sentence:  \" + sentence)", "\n", "new_sentences", ".", "extend", "(", "splitted_sentences", ")", "\n", "", "else", ":", "\n", "# if \"\\n\" in sentence:", "\n", "# logger.info(\"No split for sentence with backslash n. Doc Id: \" + str(doc_id) +\"   Sentence:  \" + sentence)", "\n", "            ", "new_sentences", ".", "append", "(", "sentence", ")", "\n", "", "", "return", "new_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentences": [[81, 102], ["get_punkt().tokenize", "text_manipulation.split_long_sentences_with_backslash_n", "text_manipulation.split_sentence_with_list", "senteces_list_fix.extend", "text_manipulation.split_sentece_colon_new_line", "sentence_colon_fix.extend", "ret_sentences.append", "text_manipulation.get_punkt", "sentence.replace"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_long_sentences_with_backslash_n", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentence_with_list", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentece_colon_new_line", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.get_punkt"], ["", "def", "split_sentences", "(", "text", ",", "doc_id", ")", ":", "\n", "    ", "sentences", "=", "get_punkt", "(", ")", ".", "tokenize", "(", "text", ")", "\n", "senteces_list_fix", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "seplited_list_sentence", "=", "split_sentence_with_list", "(", "sentence", ")", "\n", "senteces_list_fix", ".", "extend", "(", "seplited_list_sentence", ")", "\n", "\n", "", "sentence_colon_fix", "=", "[", "]", "\n", "for", "sentence", "in", "senteces_list_fix", ":", "\n", "        ", "splitted_colon_sentence", "=", "split_sentece_colon_new_line", "(", "sentence", ")", "\n", "sentence_colon_fix", ".", "extend", "(", "splitted_colon_sentence", ")", "\n", "\n", "", "sentences_without_backslash_n", "=", "split_long_sentences_with_backslash_n", "(", "\n", "max_words_in_sentence_with_backslash_n", ",", "sentence_colon_fix", ",", "doc_id", ")", "\n", "\n", "ret_sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences_without_backslash_n", ":", "\n", "        ", "ret_sentences", ".", "append", "(", "sentence", ".", "replace", "(", "'\\n'", ",", "' '", ")", ")", "\n", "\n", "\n", "", "return", "ret_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words": [[103, 114], ["text_manipulation.get_words_tokenizer", "get_words_tokenizer.tokenize", "wiki_utils.get_special_tokens", "sentence.replace.replace"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.get_words_tokenizer", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_special_tokens"], ["", "def", "extract_sentence_words", "(", "sentence", ",", "remove_missing_emb_words", "=", "False", ",", "remove_special_tokens", "=", "False", ")", ":", "\n", "    ", "if", "(", "remove_special_tokens", ")", ":", "\n", "        ", "for", "token", "in", "wiki_utils", ".", "get_special_tokens", "(", ")", ":", "\n", "# Can't do on sentence words because tokenizer delete '***' of tokens.", "\n", "            ", "sentence", "=", "sentence", ".", "replace", "(", "token", ",", "\"\"", ")", "\n", "", "", "tokenizer", "=", "get_words_tokenizer", "(", ")", "\n", "sentence_words", "=", "tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "if", "remove_missing_emb_words", ":", "\n", "        ", "sentence_words", "=", "[", "w", "for", "w", "in", "sentence_words", "if", "w", "not", "in", "missing_stop_words", "]", "\n", "\n", "", "return", "sentence_words", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.word_model": [[116, 125], ["numpy.random.randn", "model[].reshape", "model[].reshape"], "function", ["None"], ["", "def", "word_model", "(", "word", ",", "model", ")", ":", "\n", "    ", "if", "model", "is", "None", ":", "\n", "        ", "return", "np", ".", "random", ".", "randn", "(", "1", ",", "300", ")", "\n", "", "else", ":", "\n", "        ", "if", "word", "in", "model", ":", "\n", "            ", "return", "model", "[", "word", "]", ".", "reshape", "(", "1", ",", "300", ")", "\n", "", "else", ":", "\n", "#print ('Word missing w2v: ' + word)", "\n", "            ", "return", "model", "[", "'UNK'", "]", ".", "reshape", "(", "1", ",", "300", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.TestDataSet.__init__": [[83, 95], ["pathlib2.Path", "test_loader.get_files", "len", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_files"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "word2vec", ",", "train", "=", "False", ",", "manifesto", "=", "False", ",", "folder", "=", "False", ",", "high_granularity", "=", "False", ")", ":", "\n", "\n", "\n", "        ", "root_path", "=", "Path", "(", "root", ")", "\n", "self", ".", "textfiles", "=", "get_files", "(", "root_path", ")", "\n", "\n", "if", "len", "(", "self", ".", "textfiles", ")", "==", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Found 0 images in subfolders of: {}'", ".", "format", "(", "root", ")", ")", "\n", "", "self", ".", "train", "=", "train", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "word2vec", "=", "word2vec", "\n", "self", ".", "high_granularity", "=", "high_granularity", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.TestDataSet.__getitem__": [[96, 101], ["test_loader.read_file", "pathlib2.Path"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.read_file"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", "=", "self", ".", "textfiles", "[", "index", "]", "\n", "\n", "return", "read_file", "(", "Path", "(", "path", ")", ",", "self", ".", "word2vec", ",", "ignore_list", "=", "True", ",", "remove_special_tokens", "=", "True", ",", "\n", "high_granularity", "=", "self", ".", "high_granularity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.TestDataSet.__len__": [[102, 104], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "textfiles", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_scections_from_text": [[12, 29], ["re.split", "baselines.get_seperator_foramt", "baselines.get_seperator_foramt", "baselines.get_seperator_foramt", "re.sub", "re.sub.strip().split", "len", "re.sub.strip", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_seperator_foramt"], ["def", "get_scections_from_text", "(", "txt", ",", "high_granularity", "=", "True", ")", ":", "\n", "    ", "sections_to_keep_pattern", "=", "wiki_utils", ".", "get_seperator_foramt", "(", ")", "if", "high_granularity", "else", "wiki_utils", ".", "get_seperator_foramt", "(", "\n", "(", "1", ",", "2", ")", ")", "\n", "if", "not", "high_granularity", ":", "\n", "# if low granularity required we should flatten segments within segemnt level 2", "\n", "        ", "pattern_to_ommit", "=", "wiki_utils", ".", "get_seperator_foramt", "(", "(", "3", ",", "999", ")", ")", "\n", "txt", "=", "re", ".", "sub", "(", "pattern_to_ommit", ",", "\"\"", ",", "txt", ")", "\n", "\n", "#delete empty lines after re.sub()", "\n", "sentences", "=", "[", "s", "for", "s", "in", "txt", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "if", "len", "(", "s", ")", ">", "0", "and", "s", "!=", "\"\\n\"", "]", "\n", "txt", "=", "'\\n'", ".", "join", "(", "sentences", ")", ".", "strip", "(", "'\\n'", ")", "\n", "\n", "\n", "", "all_sections", "=", "re", ".", "split", "(", "sections_to_keep_pattern", ",", "txt", ")", "\n", "non_empty_sections", "=", "[", "s", "for", "s", "in", "all_sections", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "\n", "return", "non_empty_sections", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_sections": [[31, 41], ["open", "open.read", "open.close", "file.read.decode().strip", "str", "test_loader.clean_section", "file.read.decode", "test_loader.get_scections_from_text"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.close", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.clean_section", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_scections_from_text"], ["", "def", "get_sections", "(", "path", ",", "high_granularity", "=", "True", ")", ":", "\n", "    ", "file", "=", "open", "(", "str", "(", "path", ")", ",", "\"r\"", ")", "\n", "raw_content", "=", "file", ".", "read", "(", ")", "\n", "file", ".", "close", "(", ")", "\n", "\n", "clean_txt", "=", "raw_content", ".", "decode", "(", "'utf-8'", ")", ".", "strip", "(", ")", "\n", "\n", "sections", "=", "[", "clean_section", "(", "s", ")", "for", "s", "in", "get_scections_from_text", "(", "clean_txt", ",", "high_granularity", ")", "]", "\n", "\n", "return", "sections", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.clean_section": [[42, 45], ["section.strip"], "function", ["None"], ["", "def", "clean_section", "(", "section", ")", ":", "\n", "    ", "cleaned_section", "=", "section", ".", "strip", "(", "'\\n'", ")", "\n", "return", "cleaned_section", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.read_file": [[46, 76], ["test_loader.get_sections", "section.split", "targets.append", "re.sub.encode", "baselines.textseg.extract_sentence_words", "baselines.get_list_token", "len", "data.append", "logger.info", "re.sub", "data.append", "data.append", "len", "baselines.textseg.word_model"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_sections", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.word_model"], ["", "def", "read_file", "(", "path", ",", "word2vec", ",", "remove_preface_segment", "=", "True", ",", "ignore_list", "=", "False", ",", "remove_special_tokens", "=", "False", ",", "\n", "return_as_sentences", "=", "False", ",", "high_granularity", "=", "True", ",", "only_letters", "=", "False", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "targets", "=", "[", "]", "\n", "all_sections", "=", "get_sections", "(", "path", ",", "high_granularity", ")", "\n", "\n", "for", "section", "in", "required_non_empty_sections", ":", "\n", "        ", "sentences", "=", "section", ".", "split", "(", "'\\n'", ")", "\n", "if", "sentences", ":", "\n", "            ", "for", "sentence", "in", "sentences", ":", "\n", "                ", "is_list_sentence", "=", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", "==", "sentence", ".", "encode", "(", "'utf-8'", ")", "\n", "if", "ignore_list", "and", "is_list_sentence", ":", "\n", "                    ", "continue", "\n", "", "if", "not", "return_as_sentences", ":", "\n", "                    ", "sentence_words", "=", "extract_sentence_words", "(", "sentence", ",", "remove_special_tokens", "=", "remove_special_tokens", ")", "\n", "if", "1", "<=", "len", "(", "sentence_words", ")", ":", "\n", "                        ", "data", ".", "append", "(", "[", "word_model", "(", "word", ",", "word2vec", ")", "for", "word", "in", "sentence_words", "]", ")", "\n", "", "else", ":", "\n", "#raise ValueError('Sentence in wikipedia file is empty')", "\n", "                        ", "logger", ".", "info", "(", "'Sentence in wikipedia file is empty'", ")", "\n", "", "", "else", ":", "# for the annotation. keep sentence as is.", "\n", "                    ", "if", "(", "only_letters", ")", ":", "\n", "                        ", "sentence", "=", "re", ".", "sub", "(", "'[^a-zA-Z0-9 ]+'", ",", "''", ",", "sentence", ")", "\n", "data", ".", "append", "(", "sentence", ")", "\n", "", "else", ":", "\n", "                        ", "data", ".", "append", "(", "sentence", ")", "\n", "", "", "", "if", "data", ":", "\n", "                ", "targets", ".", "append", "(", "len", "(", "data", ")", "-", "1", ")", "\n", "\n", "", "", "", "return", "data", ",", "targets", ",", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.test_loader.get_files": [[77, 81], ["pathlib2.Path().glob", "str", "pathlib2.Path", "p.is_file"], "function", ["None"], ["", "def", "get_files", "(", "path", ")", ":", "\n", "    ", "all_objects", "=", "Path", "(", "path", ")", ".", "glob", "(", "'*'", ")", "\n", "files", "=", "[", "str", "(", "p", ")", "for", "p", "in", "all_objects", "if", "p", ".", "is_file", "(", ")", "]", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences": [[33, 36], ["len", "str.split"], "function", ["None"], ["def", "count_str_occurrences", "(", "str", ",", "findStr", ")", ":", "\n", "\n", "    ", "return", "len", "(", "str", ".", "split", "(", "findStr", ")", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_file_path": [[37, 53], ["str", "range", "len", "chopped_id.append", "os.path.join"], "function", ["None"], ["", "def", "get_file_path", "(", "id", ")", ":", "\n", "    ", "chopped_id", "=", "[", "]", "\n", "id_str", "=", "str", "(", "id", ")", "\n", "padding_count", "=", "id_parts", "-", "len", "(", "id_str", ")", "\n", "while", "padding_count", ">", "0", ":", "\n", "        ", "id_str", "=", "\"0\"", "+", "id_str", "\n", "padding_count", "-=", "1", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "3", ")", ":", "\n", "        ", "chopped_id", ".", "append", "(", "id_str", "[", ":", "2", "]", ")", "\n", "id_str", "=", "id_str", "[", "2", ":", "]", "\n", "\n", "", "path", "=", "\"\"", "\n", "for", "sub_path", "in", "chopped_id", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "sub_path", ")", "\n", "", "return", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_header": [[54, 65], ["re.search", "re.search", "re.search.groups", "re.search.groups", "title.isdigit", "any", "title.endswith", "title.startswith"], "function", ["None"], ["", "def", "process_header", "(", "header", ")", ":", "\n", "    ", "id_match", "=", "re", ".", "search", "(", "r'<doc id=\"(\\d+)\" url'", ",", "header", ")", "\n", "id", "=", "id_match", ".", "groups", "(", ")", "[", "0", "]", "\n", "\n", "\n", "title_match", "=", "re", ".", "search", "(", "r'title=\"(.*)\">'", ",", "header", ")", "\n", "title", "=", "title_match", ".", "groups", "(", ")", "[", "0", "]", "\n", "\n", "not_valid", "=", "title", ".", "isdigit", "(", ")", "or", "any", "(", "title", ".", "startswith", "(", "prefix", "+", "':'", "or", "prefix", "+", "' talk:'", ")", "for", "prefix", "in", "wikipedia_namespaces", ")", "or", "title", ".", "endswith", "(", "disambigutaiton_pattern", ")", "\n", "\n", "return", "id", ",", "not", "not_valid", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_sections": [[66, 87], ["content.split", "sections.append", "baselines.get_segment_seperator", "baselines.is_seperator_line", "len", "sections.append", "sections.append", "len", "sections.append"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_seperator", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.is_seperator_line"], ["", "def", "get_sections", "(", "content", ")", ":", "\n", "    ", "lines", "=", "content", ".", "split", "(", "'\\n'", ")", "\n", "section", "=", "\"\"", "\n", "# sections include headers", "\n", "sections", "=", "[", "]", "\n", "sections", ".", "append", "(", "wiki_utils", ".", "get_segment_seperator", "(", "1", ",", "\"preface.\"", ")", ")", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "if", "(", "wiki_utils", ".", "is_seperator_line", "(", "line", ")", ")", ":", "\n", "            ", "if", "len", "(", "section", ")", ">", "0", ":", "\n", "                ", "sections", ".", "append", "(", "section", ")", "\n", "", "section", "=", "\"\"", "\n", "sections", ".", "append", "(", "line", ")", "\n", "\n", "", "else", ":", "\n", "            ", "section", "+=", "line", "\n", "section", "+=", "'\\n'", "\n", "\n", "", "", "if", "len", "(", "section", ")", ">", "0", ":", "\n", "        ", "sections", ".", "append", "(", "section", ")", "\n", "\n", "", "return", "sections", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_section": [[90, 158], ["baselines.textseg.split_sentences", "wiki_processor.count_str_occurrences", "wiki_processor.count_str_occurrences", "section_sentences.append", "len", "sentence.encode", "logger.info", "logger.info", "baselines.get_formula_token", "baselines.get_codesnipet_token", "float", "float", "re.findall", "str", "str", "baselines.get_list_token", "baselines.get_list_token", "sentence.encode", "baselines.textseg.extract_sentence_words", "len", "str", "baselines.get_list_token", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.split_sentences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.count_str_occurrences", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_formula_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_codesnipet_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.text_manipulation.extract_sentence_words", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_list_token"], ["", "def", "process_section", "(", "section", ",", "id", ")", ":", "\n", "    ", "global", "num_sentences_for_avg", "\n", "global", "sum_sentences_for_avg", "\n", "sentences", "=", "text_manipulation", ".", "split_sentences", "(", "section", ",", "id", ")", "\n", "section_sentences", "=", "[", "]", "\n", "num_lists", "=", "0", "\n", "num_sentences", "=", "0", "\n", "num_formulas", "=", "0", "\n", "num_codes", "=", "0", "\n", "last_sentence_was_list", "=", "False", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "is_list_sentence", "=", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", "==", "sentence", ".", "encode", "(", "'utf-8'", ")", "\n", "if", "'\\n'", "in", "sentence", ":", "\n", "            ", "logger", ".", "info", "(", "\"DocId: \"", "+", "str", "(", "id", ")", "+", "\"   back slash in sentence: \"", "+", "sentence", ")", "\n", "", "if", "(", "wiki_utils", ".", "get_list_token", "(", ")", "in", "sentence", ")", "and", "(", "wiki_utils", ".", "get_list_token", "(", ")", "+", "\".\"", ")", "!=", "sentence", ".", "encode", "(", "'utf-8'", ")", ":", "\n", "# TODO: delete this if section, since it is not suupposed to happen any more - but still happen", "\n", "            ", "num_lists", "+=", "1", "\n", "last_sentence_was_list", "=", "True", "\n", "logger", ".", "info", "(", "\"DocId: \"", "+", "str", "(", "id", ")", "+", "\"     Special case 1: \"", "+", "sentence", ")", "\n", "continue", "\n", "", "elif", "is_list_sentence", ":", "\n", "            ", "if", "(", "last_sentence_was_list", ")", ":", "\n", "                ", "continue", "\n", "", "last_sentence_was_list", "=", "True", "\n", "num_lists", "+=", "1", "\n", "", "else", ":", "\n", "            ", "last_sentence_was_list", "=", "False", "\n", "sentence_words", "=", "text_manipulation", ".", "extract_sentence_words", "(", "sentence", ")", "\n", "if", "len", "(", "sentence_words", ")", "<", "wiki_thresholds", ".", "min_words_in_sentence", ":", "\n", "# ignore this sentence", "\n", "                ", "continue", "\n", "", "sum_sentneces_for_avg", "+=", "len", "(", "sentence_words", ")", "\n", "num_sentneces_for_avg", "+=", "1", "\n", "\n", "\n", "", "num_formulas", "+=", "count_str_occurrences", "(", "sentence", ",", "wiki_utils", ".", "get_formula_token", "(", ")", ")", "\n", "num_codes", "+=", "count_str_occurrences", "(", "sentence", ",", "wiki_utils", ".", "get_codesnipet_token", "(", ")", ")", "\n", "num_sentences", "+=", "1", "\n", "section_sentences", ".", "append", "(", "sentence", ")", "\n", "\n", "\n", "", "valid_section", "=", "True", "\n", "error_message", "=", "None", "\n", "if", "(", "num_sentences", "<", "wiki_thresholds", ".", "min_sentence_in_section", ")", ":", "\n", "        ", "valid_section", "=", "False", "\n", "error_message", "=", "\"sentences count in section is too low\"", "\n", "\n", "", "if", "(", "num_sentences", ">", "0", ")", ":", "\n", "        ", "lists_perentage", "=", "float", "(", "num_lists", ")", "/", "float", "(", "num_sentences", ")", "\n", "if", "lists_perentage", ">=", "wiki_thresholds", ".", "max_list_in_section_percentage", ":", "\n", "            ", "valid_section", "=", "False", "\n", "error_message", "=", "\"list percentage in section is too high: \"", "+", "str", "(", "lists_perentage", ")", "\n", "\n", "", "", "section_text", "=", "''", ".", "join", "(", "section_sentences", ")", "\n", "if", "len", "(", "re", ".", "findall", "(", "'[a-zA-Z]'", ",", "section_text", ")", ")", "<", "wiki_thresholds", ".", "min_section_char_count", ":", "\n", "        ", "valid_section", "=", "False", "\n", "error_message", "=", "\"char count in section is too low\"", "\n", "\n", "", "if", "num_formulas", ">=", "wiki_thresholds", ".", "max_section_formulas_count", ":", "\n", "        ", "valid_section", "=", "False", "\n", "error_message", "=", "\"number of formulas in section is too high: \"", "+", "str", "(", "num_formulas", ")", "\n", "\n", "", "if", "num_codes", ">=", "wiki_thresholds", ".", "max_section_code_snipet_count", ":", "\n", "        ", "valid_section", "=", "False", "\n", "error_message", "=", "\"number of code snippets in section is too high: \"", "+", "str", "(", "num_codes", ")", "\n", "\n", "\n", "", "return", "valid_section", ",", "section_sentences", ",", "error_message", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.is_valid_article": [[159, 169], ["float", "float", "str", "str"], "function", ["None"], ["", "def", "is_valid_article", "(", "valid_section_count", ",", "section_count", ")", ":", "\n", "    ", "if", "valid_section_count", "<", "wiki_thresholds", ".", "min_valid_section_count", ":", "\n", "        ", "return", "False", ",", "\"Valid section count is too low: \"", "+", "str", "(", "valid_section_count", ")", "\n", "\n", "", "valid_section_percentage", "=", "float", "(", "valid_section_count", ")", "/", "float", "(", "section_count", ")", "\n", "if", "valid_section_percentage", "<", "wiki_thresholds", ".", "min_valid_section_percentage", ":", "\n", "        ", "return", "False", ",", "\"Valid section percentage is too low: \"", "+", "str", "(", "valid_section_percentage", ")", "\n", "\n", "\n", "", "return", "True", ",", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.max_level_in_article": [[172, 180], ["baselines.is_seperator_line", "baselines.get_segment_level"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.is_seperator_line", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_level"], ["", "def", "max_level_in_article", "(", "content", ")", ":", "\n", "    ", "max_lavel", "=", "-", "1", "\n", "for", "line", "in", "content", ":", "\n", "        ", "if", "(", "wiki_utils", ".", "is_seperator_line", "(", "line", ")", ")", ":", "\n", "            ", "current_level", "=", "wiki_utils", ".", "get_segment_level", "(", "line", ")", "\n", "if", "current_level", ">", "max_lavel", ":", "\n", "                ", "max_lavel", "=", "current_level", "\n", "", "", "", "return", "max_lavel", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.delete_empty_segment_headers": [[182, 205], ["wiki_processor.max_level_in_article", "range", "range", "baselines.is_seperator_line", "len", "baselines.get_segment_level"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.max_level_in_article", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.is_seperator_line", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.get_segment_level"], ["", "def", "delete_empty_segment_headers", "(", "content", ")", ":", "\n", "    ", "num_of_deletions", "=", "0", "\n", "max_level", "=", "max_level_in_article", "(", "content", ")", "\n", "for", "handle_level", "in", "range", "(", "max_level", ",", "0", ",", "-", "1", ")", ":", "\n", "        ", "last_section_level", "=", "-", "1", "\n", "last_section_header", "=", "True", "\n", "for", "i", "in", "range", "(", "len", "(", "content", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "section", "=", "content", "[", "i", "]", "\n", "if", "(", "wiki_utils", ".", "is_seperator_line", "(", "section", ")", ")", ":", "\n", "                ", "section_level", "=", "wiki_utils", ".", "get_segment_level", "(", "section", ")", "\n", "if", "(", "section_level", "==", "handle_level", ")", ":", "\n", "\n", "# empty section if last seciont was also a header", "\n", "                    ", "is_empty", "=", "last_section_header", "\n", "if", "(", "is_empty", "&", "(", "last_section_level", "<=", "section_level", ")", ")", ":", "\n", "                        ", "del", "content", "[", "i", "]", "\n", "num_of_deletions", "+=", "1", "\n", "", "", "last_section_level", "=", "section_level", "\n", "last_section_header", "=", "True", "\n", "", "else", ":", "\n", "                ", "last_section_header", "=", "False", "\n", "\n", "", "", "", "return", "content", ",", "num_of_deletions", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.vec_to_text": [[207, 212], ["None"], "function", ["None"], ["", "def", "vec_to_text", "(", "sections_with_headers", ")", ":", "\n", "    ", "adjusted_content", "=", "\"\"", "\n", "for", "section", "in", "sections_with_headers", ":", "\n", "        ", "adjusted_content", "+=", "section", "+", "'\\n'", "\n", "", "return", "adjusted_content", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_content": [[214, 242], ["wiki_processor.get_sections", "range", "wiki_processor.is_valid_article", "len", "baselines.is_seperator_line", "wiki_processor.delete_empty_segment_headers", "wiki_processor.vec_to_text", "article_lines.append", "wiki_processor.process_section", "article_lines.extend", "logger.info", "vec_to_text().strip", "wiki_processor.vec_to_text"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_sections", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.is_valid_article", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_utils.is_seperator_line", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.delete_empty_segment_headers", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.vec_to_text", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_section", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.vec_to_text"], ["", "def", "process_content", "(", "content", ",", "id", ")", ":", "\n", "    ", "sections_with_headers", "=", "get_sections", "(", "content", ")", "\n", "adjueted_content_text", "=", "\"\"", "\n", "article_lines", "=", "[", "]", "\n", "section_count", "=", "0", "\n", "valid_section_count", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sections_with_headers", ")", ")", ":", "\n", "        ", "section", "=", "sections_with_headers", "[", "i", "]", "\n", "if", "wiki_utils", ".", "is_seperator_line", "(", "section", ")", ":", "\n", "            ", "article_lines", ".", "append", "(", "section", ")", "\n", "", "else", ":", "\n", "            ", "is_valid_section", ",", "section_sentences", ",", "message", "=", "process_section", "(", "section", ",", "id", ")", "\n", "section_count", "+=", "1", "\n", "if", "(", "is_valid_section", ")", ":", "\n", "                ", "valid_section_count", "+=", "1", "\n", "article_lines", ".", "extend", "(", "section_sentences", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "'Invalid section in article id: '", "+", "id", "+", "\n", "'    Reason: '", "+", "message", "+", "'    Content: '", "+", "vec_to_text", "(", "section_sentences", ")", ".", "strip", "(", "'\\n'", ")", ")", "\n", "\n", "", "", "", "is_valid", ",", "reason", "=", "is_valid_article", "(", "valid_section_count", ",", "section_count", ")", "\n", "\n", "if", "is_valid", ":", "\n", "        ", "article_content", ",", "_", "=", "delete_empty_segment_headers", "(", "article_lines", ")", "\n", "adjueted_content_text", "=", "vec_to_text", "(", "article_content", ")", "\n", "\n", "\n", "", "return", "is_valid", ",", "adjueted_content_text", ",", "reason", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_article": [[264, 281], ["wiki_processor.process_header", "wiki_processor.process_content", "logger.info", "logger.info", "logger.info", "article.strip().split", "article.strip", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_header", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_content"], ["", "def", "process_article", "(", "article", ")", ":", "\n", "    ", "non_empty_lines", "=", "[", "l", "for", "l", "in", "article", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "if", "l", "!=", "\"\"", "]", "\n", "header", "=", "non_empty_lines", "[", "0", "]", "\n", "id", ",", "is_valid_header", "=", "process_header", "(", "header", ")", "\n", "\n", "if", "not", "is_valid_header", ":", "\n", "        ", "logger", ".", "info", "(", "'Invalid header in doc id: '", "+", "str", "(", "id", ")", "+", "'     header:   '", "+", "header", ")", "\n", "return", "\"\"", ",", "id", ",", "False", "\n", "\n", "", "content", "=", "\"\\n\"", ".", "join", "(", "non_empty_lines", "[", "2", ":", "]", ")", "\n", "is_valid_content", ",", "processed_content", ",", "debug", "=", "process_content", "(", "content", ",", "id", ")", "\n", "if", "not", "(", "is_valid_content", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Invalid article in doc id: '", "+", "str", "(", "id", ")", "+", "'.  '", "+", "debug", "+", "'\\n\\n'", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "'Valid article , id: '", "+", "str", "(", "id", ")", "+", "'\\n\\n'", ")", "\n", "\n", "", "return", "processed_content", ",", "id", ",", "is_valid_content", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_wiki_file": [[283, 318], ["open", "file.read", "wiki_processor.process_article", "random.uniform", "os.path.join", "os.path.join", "file.read.decode().strip().split", "wiki_processor.get_file_path", "os.path.exists", "os.makedirs", "str", "open", "output_file.write", "len", "int", "processed_article.encode", "file.read.decode().strip", "file.read.decode"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_article", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_file_path", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.write"], ["", "def", "process_wiki_file", "(", "path", ",", "output_folder", ",", "train_ratio", ",", "test_ratio", ",", "forbidden_train_ids", ")", ":", "\n", "    ", "train_size", "=", "0", "\n", "dev_size", "=", "0", "\n", "test_size", "=", "0", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "file", ":", "\n", "        ", "raw_content", "=", "file", ".", "read", "(", ")", "\n", "\n", "", "articles", "=", "[", "s", "for", "s", "in", "raw_content", ".", "decode", "(", "'utf-8'", ")", ".", "strip", "(", ")", ".", "split", "(", "doc_split_delimiter", ")", "if", "len", "(", "s", ")", ">", "0", "]", "\n", "created_articles_count", "=", "0", "\n", "processed_articles_count", "=", "0", "\n", "\n", "for", "article", "in", "articles", ":", "\n", "        ", "processed_article", ",", "id", ",", "is_valid", "=", "process_article", "(", "article", ")", "\n", "processed_articles_count", "+=", "1", "\n", "if", "not", "is_valid", ":", "\n", "            ", "continue", ";", "\n", "", "random_num", "=", "uniform", "(", "0", ",", "1", ")", "\n", "if", "(", "random_num", ">", "train_ratio", "and", "random_num", "<=", "train_ratio", "+", "test_ratio", ")", "or", "int", "(", "id", ")", "in", "forbidden_train_ids", ":", "\n", "            ", "partition", "=", "\"test\"", "\n", "test_size", "+=", "1", "\n", "", "elif", "(", "random_num", ">", "train_ratio", "+", "test_ratio", ")", ":", "\n", "            ", "partition", "=", "\"dev\"", "\n", "dev_size", "+=", "1", "\n", "", "else", ":", "\n", "            ", "partition", "=", "\"train\"", "\n", "train_size", "+=", "1", "\n", "", "output_sub_folder", "=", "os", ".", "path", ".", "join", "(", "output_folder", ",", "partition", ",", "get_file_path", "(", "id", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_sub_folder", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_sub_folder", ")", "\n", "", "output_file_path", "=", "os", ".", "path", ".", "join", "(", "output_sub_folder", ",", "str", "(", "id", ")", ")", "\n", "with", "open", "(", "output_file_path", ",", "\"w\"", ")", "as", "output_file", ":", "\n", "            ", "output_file", ".", "write", "(", "processed_article", ".", "encode", "(", "'utf-8'", ")", ",", ")", "\n", "", "created_articles_count", "+=", "1", "\n", "\n", "", "return", "created_articles_count", ",", "processed_articles_count", ",", "train_size", ",", "dev_size", ",", "test_size", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_forbidden_train_ids": [[320, 337], ["json.load.iteritems", "json.load.iteritems", "set", "open", "json.load", "open", "json.load", "forbidden_train_ids.append", "forbidden_train_ids.append", "int", "int"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["", "def", "get_forbidden_train_ids", "(", ")", ":", "\n", "# Return ids of article which must be in test set (and not train/dev)", "\n", "    ", "with", "open", "(", "'wikicities_article_names_to_ids'", ")", "as", "f", ":", "\n", "        ", "wiki_cities", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "with", "open", "(", "'wikielements_article_names_to_ids'", ")", "as", "f", ":", "\n", "        ", "wiki_elements", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "forbidden_train_ids", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "wiki_cities", ".", "iteritems", "(", ")", ":", "\n", "        ", "forbidden_train_ids", ".", "append", "(", "int", "(", "v", ")", ")", "\n", "", "for", "k", ",", "v", "in", "wiki_elements", ".", "iteritems", "(", ")", ":", "\n", "        ", "forbidden_train_ids", ".", "append", "(", "int", "(", "v", ")", ")", "\n", "\n", "", "unique_ids", "=", "set", "(", "forbidden_train_ids", ")", "\n", "\n", "return", "unique_ids", ";", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_wiki_files": [[340, 344], ["pathlib2.Path().glob", "str", "pathlib2.Path", "p.is_file"], "function", ["None"], ["", "def", "get_wiki_files", "(", "path", ")", ":", "\n", "    ", "all_objects", "=", "Path", "(", "path", ")", ".", "glob", "(", "'**/*'", ")", "\n", "files", "=", "(", "str", "(", "p", ")", "for", "p", "in", "all_objects", "if", "p", ".", "is_file", "(", ")", ")", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_wiki_folder": [[346, 375], ["wiki_processor.get_forbidden_train_ids", "print", "print", "print", "print", "os.path.join", "wiki_processor.get_wiki_files", "str", "os.listdir", "os.path.isdir", "os.path.exists", "os.makedirs", "wiki_processor.process_wiki_file", "os.path.join", "float", "float", "print", "float", "float", "float", "str", "str"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_forbidden_train_ids", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_wiki_files", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_wiki_file"], ["", "def", "process_wiki_folder", "(", "input_folder", ",", "output_folder", ",", "train_ratio", ",", "test_ratio", ")", ":", "\n", "    ", "total_train_size", "=", "0", "\n", "total_dev_size", "=", "0", "\n", "total_test_size", "=", "0", "\n", "folders", "=", "[", "o", "for", "o", "in", "os", ".", "listdir", "(", "input_folder", ")", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "input_folder", ",", "o", ")", ")", "]", "\n", "total_created_articles", "=", "0", "\n", "total_processed_articles", "=", "0", "\n", "previous_debug", "=", "0", "\n", "forbidden_train_ids", "=", "get_forbidden_train_ids", "(", ")", "\n", "for", "folder", "in", "folders", ":", "\n", "        ", "full_folder_path", "=", "os", ".", "path", ".", "join", "(", "input_folder", ",", "folder", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "output_folder", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_folder", ")", "\n", "", "files", "=", "get_wiki_files", "(", "full_folder_path", ")", "\n", "for", "file", "in", "files", ":", "\n", "            ", "created_articles", ",", "processed_articles", ",", "train_size", ",", "dev_size", ",", "test_size", "=", "process_wiki_file", "(", "file", ",", "output_folder", ",", "float", "(", "train_ratio", ")", ",", "float", "(", "test_ratio", ")", ",", "forbidden_train_ids", ")", "\n", "total_train_size", "+=", "train_size", "\n", "total_dev_size", "+=", "dev_size", "\n", "total_test_size", "+=", "test_size", "\n", "total_created_articles", "+=", "created_articles", "\n", "total_processed_articles", "+=", "processed_articles", "\n", "if", "(", "total_created_articles", "-", "previous_debug", ">", "2500", ")", ":", "\n", "                ", "previous_debug", "=", "total_created_articles", "\n", "print", "(", "'Created '", "+", "str", "(", "total_created_articles", ")", "+", "' wiki articles, out of '", "+", "str", "(", "total_processed_articles", ")", "+", "' processed articles'", ")", "\n", "", "", "", "total_samples", "=", "total_train_size", "+", "total_dev_size", "+", "total_test_size", "\n", "print", "(", "'total_samples = '", ",", "str", "(", "total_samples", ")", ")", "\n", "print", "(", "\"#train = \"", ",", "total_train_size", ",", "\"ratio: \"", ",", "\"{:.2f}\"", ".", "format", "(", "total_train_size", "/", "float", "(", "total_samples", ")", ")", ")", "\n", "print", "(", "\"#dev = \"", ",", "total_dev_size", ",", "\"ratio: \"", ",", "\"{:.2f}\"", ".", "format", "(", "total_dev_size", "/", "float", "(", "total_samples", ")", ")", ")", "\n", "print", "(", "\"#test = \"", ",", "total_test_size", ",", "\"ratio: \"", ",", "\"{:.2f}\"", ".", "format", "(", "total_test_size", "/", "float", "(", "total_samples", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.move_wiki_file": [[377, 387], ["os.path.relpath", "os.path.join", "os.path.dirname", "shutil.move", "os.path.exists", "os.makedirs"], "function", ["None"], ["", "def", "move_wiki_file", "(", "src", ",", "folder", ",", "partition", ")", ":", "\n", "# get relative path to inputFolder", "\n", "    ", "file", "=", "os", ".", "path", ".", "relpath", "(", "src", ",", "folder", ")", "\n", "\n", "# extract file path in train folder", "\n", "dstFile", "=", "os", ".", "path", ".", "join", "(", "folder", ",", "partition", ",", "file", ")", "\n", "dstdir", "=", "os", ".", "path", ".", "dirname", "(", "dstFile", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dstdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dstdir", ")", "\n", "", "move", "(", "src", ",", "dstFile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.removeEmptyFolders": [[389, 405], ["os.listdir", "os.listdir", "os.path.isdir", "os.path.join", "os.path.isdir", "os.rmdir", "wiki_processor.removeEmptyFolders", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.removeEmptyFolders"], ["", "def", "removeEmptyFolders", "(", "path", ",", "removeRoot", "=", "True", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "\n", "        ", "return", "\n", "\n", "# remove empty subfolders", "\n", "", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "for", "f", "in", "files", ":", "\n", "        ", "fullpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "f", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "fullpath", ")", ":", "\n", "            ", "removeEmptyFolders", "(", "fullpath", ")", "\n", "\n", "# if folder empty, delete it", "\n", "", "", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "if", "len", "(", "files", ")", "==", "0", "and", "removeRoot", ":", "\n", "#print \"Removing empty folder:\", path", "\n", "        ", "os", ".", "rmdir", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.trainTestDev": [[408, 444], ["float", "float", "print", "random.shuffle", "int", "int", "range", "range", "print", "print", "print", "wiki_processor.removeEmptyFolders", "os.path.exists", "print", "os.path.join", "wiki_processor.get_wiki_files", "allFiles.extend", "math.floor", "math.floor", "wiki_processor.move_wiki_file", "range", "len", "wiki_processor.move_wiki_file", "os.listdir", "os.path.isdir", "wiki_processor.move_wiki_file", "os.path.join", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.removeEmptyFolders", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.get_wiki_files", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.move_wiki_file", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.move_wiki_file", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.move_wiki_file"], ["", "", "def", "trainTestDev", "(", "destFolder", ",", "train_size", ",", "test_size", ")", ":", "\n", "    ", "train_size_ratio", "=", "float", "(", "train_size", ")", "\n", "test_size_ratio", "=", "float", "(", "test_size", ")", "\n", "dev_size_ratio", "=", "1", "-", "train_size_ratio", "-", "test_size_ratio", "\n", "\n", "print", "(", "destFolder", ",", "train_size", ",", "test_size", ")", "\n", "\n", "allFiles", "=", "[", "]", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "destFolder", ")", ":", "\n", "        ", "print", "(", "\"Output folder does not exist\"", ")", "\n", "return", "\n", "", "folders", "=", "[", "o", "for", "o", "in", "os", ".", "listdir", "(", "destFolder", ")", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "destFolder", ",", "o", ")", ")", "]", "\n", "for", "folder", "in", "folders", ":", "\n", "        ", "full_folder_path", "=", "os", ".", "path", ".", "join", "(", "destFolder", ",", "folder", ")", "\n", "files", "=", "get_wiki_files", "(", "full_folder_path", ")", "\n", "allFiles", ".", "extend", "(", "files", ")", "\n", "\n", "\n", "", "shuffle", "(", "allFiles", ")", "\n", "\n", "trainSize", "=", "int", "(", "math", ".", "floor", "(", "len", "(", "allFiles", ")", "*", "train_size_ratio", ")", ")", "\n", "devSize", "=", "int", "(", "math", ".", "floor", "(", "len", "(", "allFiles", ")", "*", "dev_size_ratio", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "trainSize", ")", ":", "\n", "        ", "move_wiki_file", "(", "allFiles", "[", "i", "]", ",", "destFolder", ",", "partition", "=", "\"train\"", ")", "\n", "\n", "", "if", "devSize", ">", "0", ":", "\n", "        ", "for", "i", "in", "range", "(", "trainSize", ",", "trainSize", "+", "devSize", ")", ":", "\n", "            ", "move_wiki_file", "(", "allFiles", "[", "i", "]", ",", "destFolder", ",", "partition", "=", "\"dev\"", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "trainSize", "+", "devSize", ",", "len", "(", "allFiles", ")", ")", ":", "\n", "        ", "move_wiki_file", "(", "allFiles", "[", "i", "]", ",", "destFolder", ",", "partition", "=", "\"test\"", ")", "\n", "", "print", "(", "\"#train = \"", ",", "trainSize", ")", "\n", "print", "(", "\"#dev = \"", ",", "devSize", ")", "\n", "print", "(", "\"#test = \"", ",", "len", "(", "allFiles", ")", "-", "trainSize", "-", "devSize", ")", "\n", "\n", "removeEmptyFolders", "(", "destFolder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.main": [[446, 473], ["wiki_processor.process_wiki_folder", "print", "print", "print", "os.path.exists", "os.makedirs", "str", "str", "subprocess.call", "print", "os.path.exists", "os.makedirs", "str", "str", "pathlib2.Path", "float"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_processor.process_wiki_folder"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "global", "num_sentences_for_avg", "\n", "global", "sum_sentences_for_avg", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "temp", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "temp", ")", "\n", "# execute extraction of wikipedia dump", "\n", "", "cmd", "=", "[", "'python'", ",", "str", "(", "Path", "(", "__file__", ")", ".", "parent", "/", "'wiki_extractor.py'", ")", ",", "'-s'", ",", "'-o'", ",", "args", ".", "temp", ",", "'--article_count'", ",", "str", "(", "args", ".", "article_count", ")", ",", "'--lists'", "]", "\n", "\n", "if", "args", ".", "processes", ":", "\n", "        ", "cmd", "+=", "[", "'--processes'", ",", "args", ".", "processes", "]", "\n", "\n", "", "cmd", "+=", "[", "args", ".", "input", "]", "\n", "\n", "if", "not", "args", ".", "no_extractor", ":", "\n", "        ", "subprocess", ".", "call", "(", "cmd", ")", "\n", "print", "(", "\"Finisehd extractor\"", ")", "\n", "\n", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output", ")", "\n", "# create file per each wiki value from the extracted dump", "\n", "", "process_wiki_folder", "(", "args", ".", "temp", ",", "args", ".", "output", ",", "args", ".", "train", ",", "args", ".", "test", ")", "\n", "\n", "print", "(", "\"Number of processed sentences: \"", "+", "str", "(", "num_sentneces_for_avg", ")", ")", "\n", "print", "(", "\"avg len sentence = \"", "+", "str", "(", "sum_sentneces_for_avg", "/", "float", "(", "num_sentneces_for_avg", ")", ")", ")", "\n", "print", "(", "'done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.f1.__init__": [[57, 62], ["numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["\"copyright complaints\"", ",", "\"claims of copyright infringement\"", ",", "\"dmca notice\"", ",", "\"copyright information\"", ",", "\n", "\"notice and procedure for making claims of copyright infringement\"", ",", "\n", "\"notice of copyright infringement\"", ",", "\"notification of copyright infringement\"", ",", "\n", "\"copyright and content ownership\"", ",", "\n", "\"copyrights and copyright agents\"", ",", "\"content ownership\"", "]", ",", "\n", "\"introduction\"", ":", "[", "\"introduction\"", ",", "\"preamble\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.f1.add": [[63, 85], ["numpy.argmax", "range", "range", "len", "range"], "methods", ["None"], ["\"contact\"", ":", "[", "\"contact us\"", ",", "\"contact information\"", ",", "\"contacting us\"", ",", "\"contact\"", ",", "\"how to contact us\"", ",", "\"contact details\"", ",", "\n", "\"information about us\"", ",", "\"about us\"", ",", "\"our details\"", ",", "\"who we are\"", ",", "\"who we are and how to contact us\"", "]", ",", "\n", "\"changes\"", ":", "[", "\"changes\"", ",", "\"changes to terms of service\"", ",", "\"changes to these terms\"", ",", "\"changes to this agreement\"", ",", "\n", "\"changes to the terms of use\"", ",", "\"changes to terms\"", ",", "\"modifications\"", ",", "\"changes and amendments\"", ",", "\n", "\"site terms of use modifications\"", ",", "\"modification\"", ",", "\"modifications to service\"", ",", "\"changes to the terms\"", ",", "\n", "\"modifications to the service and prices\"", ",", "\"modification of these terms of use\"", ",", "\"amendment\"", ",", "\n", "\"modifications to services\"", ",", "\"modification of terms\"", ",", "\"modifications to terms\"", ",", "\n", "\"changes to terms of use\"", ",", "\"changes to these terms of use\"", ",", "\"changes to terms and conditions\"", ",", "\n", "\"modification of terms of use\"", ",", "\"changes to these terms and conditions\"", ",", "\"changes to the terms of service\"", ",", "\n", "\"modification of these terms\"", ",", "\"changes to the terms of service\"", ",", "\"updates to terms\"", ",", "\"updates\"", ",", "\n", "\"changes to the website\"", "]", ",", "\n", "\"force majeure\"", ":", "[", "\"force majeure\"", ",", "\"events outside of our control\"", ",", "\"events outside our control\"", "]", ",", "\n", "\"definitions\"", ":", "[", "\"definitions\"", ",", "\"definitions and interpretation\"", "]", ",", "\n", "\"user content\"", ":", "[", "\"user content\"", ",", "\"your content\"", ",", "\"user generated content\"", ",", "\"content\"", ",", "\"user-generated content\"", "]", ",", "\n", "\"registration\"", ":", "[", "\"registration\"", ",", "\"your registration obligations\"", ",", "\"account registration\"", ",", "\"user registration\"", ",", "\n", "\"registration obligations & passwords\"", ",", "\"account creation\"", ",", "\"registration and password\"", ",", "\n", "\"registration information\"", "]", ",", "\n", "\"payment\"", ":", "[", "\"payment\"", ",", "\"payments\"", ",", "\"payment terms\"", ",", "\"fees\"", ",", "\"fee\"", ",", "\"fees and payment\"", ",", "\"fees and payments\"", ",", "\"price\"", ",", "\n", "\"pricing\"", ",", "\"prices\"", ",", "\"price and payment\"", ",", "\"billing\"", ",", "\"charges\"", ",", "\"payment methods\"", ",", "\"fees; payment\"", ",", "\n", "\"payment of fees\"", ",", "\"terms of payment\"", ",", "\"pricing information\"", ",", "\"late payments\"", ",", "\"payment method\"", ",", "\n", "\"prices and payment\"", "]", ",", "\n", "\"license\"", ":", "[", "\"license\"", ",", "\"license and site access\"", ",", "\"license grant\"", ",", "\"use license\"", ",", "\"limited license\"", ",", "\"licenses\"", ",", "\n", "\"license to use website\"", ",", "\"your license to use the products\"", ",", "\"grant of license\"", ",", "\"licence\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.f1.score": [[87, 103], ["range", "print", "precision.append", "recall.append", "fscore.append"], "methods", ["None"], ["\"cookies\"", ":", "[", "\"cookies\"", ",", "\"use of cookies\"", ",", "\"cookie policy\"", ",", "\"how we use cookies\"", "]", ",", "\n", "\"notice\"", ":", "[", "\"notices\"", ",", "\"notice\"", ",", "\"electronic notices\"", ",", "\"legal notice\"", "]", ",", "\n", "\"security\"", ":", "[", "\"security\"", ",", "\"account security\"", ",", "\"data security\"", ",", "\"site security\"", ",", "\"security and password\"", "]", ",", "\n", "\"general terms\"", ":", "[", "\"general\"", ",", "\"general conditions\"", ",", "\"general terms\"", ",", "\"general information\"", ",", "\"general terms of use\"", ",", "\n", "\"general provisions\"", ",", "\"general disclaimer\"", ",", "\"general terms and conditions\"", ",", "\"generally\"", "]", ",", "\n", "\"terms and conditions\"", ":", "[", "\"terms\"", ",", "\"terms and conditions\"", ",", "\"terms & conditions\"", ",", "\"terms of use\"", ",", "\"term\"", "\n", "\"terms of service\"", ",", "\"acceptance of terms\"", ",", "\"acceptance of these terms\"", ",", "\"acceptance\"", ",", "\n", "\"acceptance of terms of use\"", ",", "\"entire agreement\"", ",", "\"agreement\"", ",", "\"complete agreement\"", ",", "\n", "\"about these website terms of service\"", ",", "\"your acceptance\"", ",", "\"acceptance of agreement\"", ",", "\n", "\"acceptance of the terms of use\"", ",", "\"terms and conditions of use\"", ",", "\"agreement to terms\"", ",", "\n", "\"website terms of use\"", ",", "\"acceptance of terms and conditions\"", ",", "\"terms of website use\"", ",", "\n", "\"acceptance of the terms\"", ",", "\"acceptance of terms of service\"", ",", "\"user agreement\"", ",", "\n", "\"conditions of use\"", "]", ",", "\n", "\"refunds\"", ":", "[", "\"refund policy\"", ",", "\"refunds\"", ",", "\"refund\"", ",", "\"money back guarantee\"", ",", "\"guarantee\"", "]", ",", "\n", "\"eligibility\"", ":", "[", "\"eligibility\"", "]", ",", "\n", "\"prohibited use\"", ":", "[", "\"prohibited uses\"", ",", "\"no unlawful or prohibited use\"", ",", "\"prohibited activities\"", ",", "\"restrictions\"", ",", "\n", "\"restrictions on use\"", ",", "\"use restrictions\"", ",", "\"restrictions on use of materials\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.__init__": [[107, 112], ["None"], "methods", ["None"], ["\"third party links\"", ",", "\"links to third party websites\"", ",", "\"third party websites\"", ",", "\n", "\"third party sites\"", ",", "\"third party content\"", ",", "\"third party services\"", ",", "\n", "\"third parties\"", ",", "\"content posted on other websites\"", ",", "\"third party rights\"", ",", "\n", "\"external links\"", ",", "\"third-party services\"", ",", "\"links to third-party sites\"", ",", "\n", "\"linked sites\"", ",", "\"third-party content\"", ",", "\"links to third-party websites\"", ",", "\n", "\"links from this website\"", ",", "\"third-party links and resources\"", ",", "\"third-party sites\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.add": [[114, 119], ["None"], "methods", ["None"], ["\"links to third party sites/third party services\"", ",", "\"linked websites\"", ",", "\n", "\"third party sites and information\"", ",", "\n", "\"linked websites\"", ",", "\"third party sites and information\"", ",", "\"links to external sites\"", "]", ",", "\n", "\"disputes\"", ":", "[", "\"dispute resolution\"", ",", "\"disputes\"", ",", "\"arbitration\"", ",", "\"binding arbitration\"", ",", "\"member disputes\"", ",", "\n", "\"arbitration agreement\"", ",", "\"legal disputes\"", ",", "\"dispute resolution; arbitration\"", ",", "\n", "\"dispute resolution and arbitration\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.calc_recall": [[121, 126], ["numpy.true_divide"], "methods", ["None"], ["\"communications\"", ":", "[", "\"electronic communications\"", ",", "\"communications\"", ",", "\"use of communication services\"", ",", "\n", "\"communication\"", ",", "\"electronic communication\"", "]", ",", "\n", "\"ownership\"", ":", "[", "\"ownership\"", ",", "\"proprietary rights\"", ",", "\"proprietary information\"", ",", "\"our proprietary rights\"", "]", ",", "\n", "\"reservation of rights\"", ":", "[", "\"reservation of rights\"", "]", ",", "\n", "\"acceptable use\"", ":", "[", "\"acceptable use\"", ",", "\"acceptable use policy\"", ",", "\"use of site\"", ",", "\"use of the site\"", ",", "\"permitted use\"", ",", "\n", "\"use of the website\"", ",", "\"use of content\"", ",", "\"use of website\"", ",", "\"use of services\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.calc_precision": [[127, 132], ["numpy.true_divide"], "methods", ["None"], ["\"use of the services\"", ",", "\"use of the service\"", ",", "\"use of this site\"", ",", "\"permitted uses\"", ",", "\n", "\"your use of the site\"", ",", "\"use of service\"", ",", "\"use of our website\"", ",", "\"using our services\"", "]", ",", "\n", "\"revisions and errata\"", ":", "[", "\"revisions and errata\"", ",", "\"revisions\"", ",", "\"revision\"", "]", ",", "\n", "\"submissions\"", ":", "[", "\"submissions\"", ",", "\"user submissions\"", ",", "\"unsolicited submissions\"", "]", ",", "\n", "\"accounts\"", ":", "[", "\"accounts\"", ",", "\"your account\"", ",", "\"account\"", ",", "\"user accounts\"", ",", "\"user account\"", ",", "\"account information\"", "]", ",", "\n", "\"overview\"", ":", "[", "\"overview\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.get_f1": [[136, 149], ["utils.predictions_analysis.calc_precision", "utils.predictions_analysis.calc_recall"], "methods", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.calc_precision", "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.calc_recall"], ["\"personal data\"", ":", "[", "\"personal information\"", ",", "\"personal data\"", ",", "\"user information\"", ",", "\"user data\"", ",", "\"customer data\"", ",", "\n", "\"your personal information\"", "]", ",", "\n", "\"delivery\"", ":", "[", "\"delivery\"", ",", "\"shipping\"", ",", "\"shipping policy\"", ",", "\"delivery policy\"", ",", "\"product delivery\"", ",", "\n", "\"shipping and delivery\"", "]", ",", "\n", "\"variation\"", ":", "[", "\"variation\"", ",", "\"variation of terms\"", ",", "\"variations\"", "]", ",", "\n", "\"conduct\"", ":", "[", "\"user conduct\"", ",", "\"your conduct\"", ",", "\"member conduct\"", ",", "\"prohibited conduct\"", ",", "\"conduct\"", ",", "\n", "\"code of conduct\"", ",", "\"rules of conduct\"", ",", "\"your conduct and responsible use of the digital services\"", "]", ",", "\n", "\"services\"", ":", "[", "\"services\"", ",", "\"description of services\"", ",", "\"description of service\"", ",", "\"product descriptions\"", ",", "\n", "\"products or services (if applicable)\"", ",", "\"service\"", ",", "\"products\"", ",", "\"product\"", ",", "\"our services\"", ",", "\n", "\"product information\"", ",", "\"the services\"", ",", "\"the service\"", ",", "\"product description\"", "]", ",", "\n", "\"removal of links from our website\"", ":", "[", "\"removal of links from our website\"", "]", ",", "\n", "\"iframes\"", ":", "[", "\"iframes\"", "]", ",", "\n", "\"access\"", ":", "[", "\"restricted access\"", ",", "\"access\"", ",", "\"accessing our website\"", ",", "\"access to the site\"", ",", "\"access and inference\"", ",", "\n", "\"accessing our site\"", ",", "\"accessibility\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.get_accuracy": [[150, 157], ["numpy.true_divide"], "methods", ["None"], ["\"support\"", ":", "[", "\"support\"", ",", "\"technical support\"", ",", "\"customer support\"", ",", "\"support services\"", ",", "\"customer service\"", "]", ",", "\n", "\"hyperlinks\"", ":", "[", "\"hyperlinking to our content\"", ",", "\"hyperlinks\"", ",", "\"linking\"", ",", "\"linking to our site\"", ",", "\n", "\"linking to the website and social media features\"", ",", "\"linking to the website\"", ",", "\n", "\"linking to our website\"", ",", "\"linking to this website\"", ",", "\"links and linking\"", ",", "\"links to this website\"", "]", ",", "\n", "\"exceptions\"", ":", "[", "\"exceptions\"", ",", "\"exclusions\"", "]", ",", "\n", "\"violations\"", ":", "[", "\"violations\"", ",", "\"breaches of these terms and conditions\"", ",", "\"breach of terms\"", ",", "\"breach\"", "]", ",", "\n", "\"advertisements\"", ":", "[", "\"advertisements\"", ",", "\"advertising\"", ",", "\"advertisers\"", ",", "\"promotions\"", ",", "\"publicity\"", ",", "\n", "\"dealings with advertisers\"", ",", "\"advertisments and promotions\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.predictions_analysis.reset": [[159, 164], ["None"], "methods", ["None"], ["\"complaints\"", ":", "[", "\"complaints\"", "]", ",", "\n", "\"accuracy\"", ":", "[", "\"accuracy of billing and account information\"", ",", "\"accuracy of materials\"", ",", "\"accuracy of information\"", ",", "\n", "\"accuracy, completeness and timeliness of information\"", ",", "\"reliance on information posted\"", "]", ",", "\n", "\"errors\"", ":", "[", "\"typographical errors\"", ",", "\"errors, inaccuracies and omissions\"", ",", "\"errors, inaccuracies, and omissions\"", "]", ",", "\n", "\"returns\"", ":", "[", "\"returns\"", ",", "\"return policy\"", ",", "\"returns policy\"", ",", "\"returns and refunds\"", "]", ",", "\n", "\"availability\"", ":", "[", "\"availability\"", ",", "\"availability, errors and inaccuracies\"", ",", "\"product availability\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.read_config_file": [[14, 19], ["open", "config.update", "json.load"], "function", ["home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.wiki_extractor.OutputSplitter.open"], ["\"limitations on liability\"", ",", "\"limitation of our liability\"", ",", "\"liability limitation\"", ",", "\n", "\"limitations and exclusions of liability\"", ",", "\"no liability\"", ",", "\n", "\"disclaimers and limitations of liability\"", ",", "\n", "\"disclaimers; limitation of liability\"", ",", "\"limitation of liabilities\"", "]", ",", "\n", "\"indemnification\"", ":", "[", "\"indemnification\"", ",", "\"indemnity\"", "]", ",", "\n", "\"termination\"", ":", "[", "\"termination\"", ",", "\"termination of use\"", ",", "\"termination of service\"", ",", "\"term; termination\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.maybe_cuda": [[21, 30], ["x.cuda"], "function", ["None"], ["\"cancellations\"", ",", "\"termination/access restriction\"", ",", "\"term and termination\"", ",", "\n", "\"suspension and termination\"", ",", "\"effect of termination\"", ",", "\"account termination policy\"", ",", "\n", "\"termination of access\"", ",", "\"account termination\"", ",", "\"termination and access restriction\"", ",", "\n", "\"cancellation rights\"", "]", ",", "\n", "\"disclaimer\"", ":", "[", "\"disclaimer\"", ",", "\"disclaimers\"", "]", ",", "\n", "\"warranty disclaimer\"", ":", "[", "\"disclaimer of warranties\"", ",", "\"no warranties\"", ",", "\"no warranty\"", ",", "\"warranty disclaimer\"", ",", "\n", "\"warranty\"", ",", "\"warranties\"", ",", "\"disclaimer of warranty\"", ",", "\"limited warranties\"", ",", "\n", "\"disclaimer of warranties; limitation of liability\"", ",", "\"warranty disclaimers\"", ",", "\n", "\"disclaimer of warranties and limitation of liability\"", ",", "\"warranties and disclaimers\"", ",", "\n", "\"representations and warranties\"", ",", "\"general representation and warranty\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.setup_logger": [[32, 45], ["logging.getLogger", "logging.getLogger.setLevel", "logging.StreamHandler", "file_handler.setLevel", "logging.StreamHandler.setLevel", "logging.Formatter", "logging.StreamHandler.setFormatter", "file_handler.setFormatter", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "logging.FileHandler", "logging.FileHandler"], "function", ["None"], ["\"limitation of warranties\"", ",", "\"your representations and warranties\"", ",", "\"disclaimers of warranties\"", "]", ",", "\n", "\"law and jurisdiction\"", ":", "[", "\"governing law\"", ",", "\"governing law and jurisdiction\"", ",", "\"applicable law\"", ",", "\"jurisdiction\"", ",", "\n", "\"law and jurisdiction\"", ",", "\"choice of law\"", ",", "\"governing law & jurisdiction\"", ",", "\n", "\"applicable law and jurisdiction\"", ",", "\"jurisdictional issues\"", ",", "\"applicable laws\"", ",", "\n", "\"governing law; dispute resolution\"", ",", "\"jurisdiction and applicable law\"", ",", "\n", "\"choice of law and forum\"", ",", "\"governing law; jurisdiction\"", ",", "\"governing law and venue\"", ",", "\n", "\"governing law and dispute resolution\"", ",", "\"choice of law and venue\"", ",", "\"law\"", ",", "\n", "\"applicable law and venue\"", ",", "\"jurisdiction and venue\"", ",", "\"choice of law and jurisdiction\"", ",", "\n", "\"governing laws\"", ",", "\"final provisions\"", "]", ",", "\n", "\"miscellaneous\"", ":", "[", "\"miscellaneous\"", ",", "\"other\"", ",", "\"miscellaneous provisions\"", ",", "\"other terms\"", ",", "\n", "\"other applicable terms\"", ",", "\"other provisions\"", ",", "\"other important terms\"", ",", "\"additional information\"", "]", ",", "\n", "\"severability\"", ":", "[", "\"severability\"", ",", "\"waiver and severability\"", ",", "\"waiver & severability\"", ",", "\"severability; waiver\"", ",", "\n", "\"severability and integration\"", "]", ",", "\n", "\"privacy\"", ":", "[", "\"privacy\"", ",", "\"privacy policy\"", ",", "\"your privacy\"", ",", "\"privacy statement\"", ",", "\"data protection\"", ",", "\"privacy notice\"", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.unsort": [[47, 54], ["enumerate", "len"], "function", ["None"], ["\"intellectual property\"", ":", "[", "\"intellectual property\"", ",", "\"intellectual property rights\"", ",", "\n", "\"compliance with intellectual property laws\"", ",", "\"intellectual property information\"", ",", "\n", "\"intellectual and other proprietary rights\"", ",", "\"our intellectual property\"", ",", "\n", "\"intellectual property ownership\"", "]", ",", "\n", "\"trademark\"", ":", "[", "\"trademarks\"", ",", "\"trademark information\"", ",", "\"copyright and trademarks\"", ",", "\"trade marks\"", ",", "\n", "\"copyright and trademark notices\"", ",", "\"trademarks and copyrights\"", ",", "\"copyrights and trademarks\"", ",", "\n", "\"trademark & patents\"", ",", "\"copyrights; restrictions on use\"", ",", "\"trademark notice\"", ",", "\"trademarks & patents\"", "]", ",", "\n", "\"assignment\"", ":", "[", "\"assignment\"", ",", "\"waiver\"", ",", "\"no waiver\"", ",", "\"class action waiver\"", ",", "\"non-waiver\"", ",", "\"no class actions\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.dennlinger_TopicalChange.textseg.utils.get_random_files": [[166, 177], ["random.sample", "pathlib2.Path().glob", "pathlib2.Path().glob", "file_paths.append", "pathlib2.Path().joinpath", "shutil.copy", "str", "str", "pathlib2.Path", "pathlib2.Path", "pathlib2.Path"], "function", ["None"], ["\"purchases\"", ":", "[", "\"purchases\"", ",", "\"orders\"", ",", "\"ordering\"", ",", "\"making purchases\"", "]", ",", "\n", "\"international use\"", ":", "[", "\"international use\"", ",", "\"international users\"", ",", "\"special admonitions for international use\"", "]", ",", "\n", "\"your responsibilities\"", ":", "[", "\"your responsibility\"", ",", "\"responsibility of website visitors\"", ",", "\"responsibility\"", ",", "\n", "\"responsibility of contributors\"", ",", "\"your obligations\"", ",", "\"user obligations\"", ",", "\n", "\"obligations of the user\"", ",", "\"obligations of the visitor\"", ",", "\"responsibilities\"", ",", "\n", "\"user responsibilities\"", "]", ",", "\n", "\"monitoring\"", ":", "[", "\"monitoring\"", "]", ",", "\n", "\"release\"", ":", "[", "\"release\"", ",", "\"release of information\"", "]", ",", "\n", "\"additional terms\"", ":", "[", "\"additional terms\"", "]", ",", "\n", "\"risk of loss\"", ":", "[", "\"risk of loss\"", "]", ",", "\n", "\"interpretation\"", ":", "[", "\"interpretation\"", "]", ",", "\n", "\"export control\"", ":", "[", "\"export control\"", ",", "\"export controls\"", ",", "\"export\"", ",", "\"export restrictions\"", ",", "\"export compliance\"", "]", ",", "\n"]]}