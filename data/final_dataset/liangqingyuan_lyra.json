{"home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch.__init__": [[29, 56], ["torch.Module.__init__", "beamsearch.BeamSearch.model.eval", "beamsearch.BeamSearch.register_buffer", "beamsearch.BeamSearch.register_buffer", "beamsearch.BeamSearch.register_buffer", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "params", ",", "beam_size", ",", "max_seq_len", ",", "\n", "src_pad_idx", ",", "trg_pad_idx", ",", "trg_bos_idx", ",", "trg_eos_idx", ")", ":", "\n", "\n", "        ", "super", "(", "BeamSearch", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "alpha", "=", "0.7", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "src_pad_idx", "=", "src_pad_idx", "\n", "self", ".", "trg_bos_idx", "=", "trg_bos_idx", "\n", "self", ".", "trg_eos_idx", "=", "trg_eos_idx", "\n", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "#init_seq: [1,1] : [[bos_idx]]", "\n", "self", ".", "register_buffer", "(", "'init_seq'", ",", "torch", ".", "LongTensor", "(", "[", "[", "trg_bos_idx", "]", "]", ")", ")", "\n", "#blank_seqs: [beam_size, max_seq_len], [bos_idx, pad_idx, pad_idx, ......]", "\n", "self", ".", "register_buffer", "(", "\n", "'blank_seqs'", ",", "\n", "torch", ".", "full", "(", "(", "beam_size", ",", "max_seq_len", ")", ",", "trg_pad_idx", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "blank_seqs", "[", ":", ",", "0", "]", "=", "self", ".", "trg_bos_idx", "\n", "#len_map: [1, max_seq_len] : [1, 2, ... , max_seq_len]", "\n", "self", ".", "register_buffer", "(", "\n", "'len_map'", ",", "\n", "torch", ".", "arange", "(", "1", ",", "max_seq_len", "+", "1", ",", "dtype", "=", "torch", ".", "long", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._model_decode": [[60, 68], ["beamsearch.get_subsequent_mask", "beamsearch.BeamSearch.model.decode"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_subsequent_mask", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode"], ["def", "_model_decode", "(", "self", ",", "trg_seq", ",", "enc_output", ",", "src_mask", ")", ":", "\n", "# trg_seq: [beam_size, step]", "\n", "# trg_mask: [beam_size, step, step]", "\n", "# enc_output: [beam_size, seq_len, d_model]", "\n", "        ", "trg_mask", "=", "get_subsequent_mask", "(", "trg_seq", ")", "\n", "# dec_output: [beam_size, step, d_model]", "\n", "preb", "=", "self", ".", "model", ".", "decode", "(", "trg_seq", ",", "trg_mask", ",", "enc_output", ",", "src_mask", ",", "self", ".", "enc_wiht_extend_vocab", ",", "self", ".", "max_ext_len", ")", "\n", "return", "preb", "\n", "#return self.model.trg_word_prj(dec_output)*(enc_output.size(2) ** -0.5)", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._get_init_state": [[73, 108], ["beamsearch.BeamSearch.model.encode", "beamsearch.BeamSearch._model_decode", "dec_output[].topk", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "beamsearch.BeamSearch.blank_seqs.clone().detach", "beamsearch.BeamSearch.blank_seqs.clone().detach", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "enc_output.repeat.repeat.repeat", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "beamsearch.BeamSearch.blank_seqs.clone", "beamsearch.BeamSearch.blank_seqs.clone", "best_k_idx_list.append", "best_k_idx_list.append"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._model_decode"], ["def", "_get_init_state", "(", "self", ",", "src_seq", ",", "src_mask", ")", ":", "\n", "#src_seq: [1, seq_len]", "\n", "        ", "beam_size", "=", "self", ".", "beam_size", "\n", "#enc_output: [1, seq_len, d_model]", "\n", "enc_output", "=", "self", ".", "model", ".", "encode", "(", "src_seq", ",", "src_mask", ")", "\n", "#dec_output: [1, 1, vocab_size]", "\n", "dec_output", "=", "self", ".", "_model_decode", "(", "self", ".", "init_seq", ",", "enc_output", ",", "src_mask", ")", "#use self.init_seq", "\n", "#best_k: [[1, beam_size]]", "\n", "best_k_probs", ",", "best_k_idx", "=", "dec_output", "[", ":", ",", "-", "1", ",", ":", "]", ".", "topk", "(", "beam_size", ")", "\n", "#print(\">>>>>>>>>src_seq: \", src_seq)", "\n", "#print(\">>>>>>>>>best_k_probs: \",best_k_probs)", "\n", "#print(\">>>>>>>>>best_k_idx: \",best_k_idx)", "\n", "\n", "#socres: [beam_size]", "\n", "scores", "=", "torch", ".", "log", "(", "best_k_probs", ")", ".", "view", "(", "beam_size", ")", "\n", "#gen_seq: [beam_size, max_seq_len]", "\n", "gen_seq", "=", "self", ".", "blank_seqs", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "gen_seq_ext", "=", "self", ".", "blank_seqs", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "#[ [bos_id, pred_r0_1, pad, pad, ... ],", "\n", "#  [bos_id, pred_r1_1, pad, pad, ... ],", "\n", "#  [bos_id, pred_r2_1, pad, pad, ... ],", "\n", "#  [bos_id, pred_r3_1, pad, pad, ... ] ]", "\n", "best_k_idx_list", "=", "[", "]", "\n", "#print(\"best_k_idx: \", best_k_idx)", "\n", "for", "i", "in", "best_k_idx", "[", "0", "]", ":", "\n", "            ", "if", "i", ">=", "self", ".", "params", ".", "dec_vocab_size", ":", "\n", "                ", "best_k_idx_list", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "best_k_idx_list", ".", "append", "(", "i", ")", "\n", "", "", "best_k_idx_no_oovs", "=", "torch", ".", "LongTensor", "(", "best_k_idx_list", ")", "\n", "gen_seq", "[", ":", ",", "1", "]", "=", "best_k_idx_no_oovs", "\n", "gen_seq_ext", "[", ":", ",", "1", "]", "=", "best_k_idx", "[", "0", "]", "\n", "#enc_output: [beam_size, seq_len, d_model]", "\n", "enc_output", "=", "enc_output", ".", "repeat", "(", "beam_size", ",", "1", ",", "1", ")", "\n", "return", "enc_output", ",", "gen_seq", ",", "gen_seq_ext", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._get_the_best_score_and_idx": [[110, 144], ["dec_output[].topk", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "scores.view().topk", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "scores.view", "print", "scores.size", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "scores.view", "best_k_idx_list.append", "best_k_idx_list.append", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "_get_the_best_score_and_idx", "(", "self", ",", "gen_seq", ",", "gen_seq_ext", ",", "dec_output", ",", "scores", ",", "step", ")", ":", "\n", "#gen_seq: [beam_size, max_seq_len]", "\n", "#dec_output: [beam_size, step, vocab_size]", "\n", "        ", "assert", "len", "(", "scores", ".", "size", "(", ")", ")", "==", "1", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "# Get k candidates for each beam, k^2 candidates in total.", "\n", "# dec_output[:, -1, :]: [beam_size, vocab_size], -1 indicate the last step", "\n", "# best_k2_probs,best_k2_idx : [beam_size,beam_size]", "\n", "best_k2_probs", ",", "best_k2_idx", "=", "dec_output", "[", ":", ",", "-", "1", ",", ":", "]", ".", "topk", "(", "beam_size", ")", "\n", "# Include the previous scores.", "\n", "scores", "=", "torch", ".", "log", "(", "best_k2_probs", ")", ".", "view", "(", "beam_size", ",", "-", "1", ")", "+", "scores", ".", "view", "(", "beam_size", ",", "1", ")", "\n", "if", "torch", ".", "isnan", "(", "scores", ")", ".", "any", "(", ")", ":", "\n", "            ", "print", "(", "\"Error: log probs contains NAN!\"", ")", "\n", "# Get the best k candidates from k^2 candidates.", "\n", "# best_k_idx_in_k2: [4]", "\n", "", "scores", ",", "best_k_idx_in_k2", "=", "scores", ".", "view", "(", "-", "1", ")", ".", "topk", "(", "beam_size", ")", "\n", "# Get the corresponding positions of the best k candidiates.", "\n", "best_k_r_idxs", ",", "best_k_c_idxs", "=", "best_k_idx_in_k2", "//", "beam_size", ",", "best_k_idx_in_k2", "%", "beam_size", "\n", "best_k_idx", "=", "best_k2_idx", "[", "best_k_r_idxs", ",", "best_k_c_idxs", "]", "\n", "# Copy the corresponding previous tokens.", "\n", "gen_seq", "[", ":", ",", ":", "step", "]", "=", "gen_seq", "[", "best_k_r_idxs", ",", ":", "step", "]", "\n", "gen_seq_ext", "[", ":", ",", ":", "step", "]", "=", "gen_seq_ext", "[", "best_k_r_idxs", ",", ":", "step", "]", "\n", "# Set the best tokens in this beam search step", "\n", "best_k_idx_list", "=", "[", "]", "\n", "for", "i", "in", "best_k_idx", ":", "\n", "            ", "if", "i", ">=", "self", ".", "params", ".", "dec_vocab_size", ":", "\n", "                ", "best_k_idx_list", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "best_k_idx_list", ".", "append", "(", "i", ")", "\n", "", "", "best_k_idx_no_oovs", "=", "torch", ".", "LongTensor", "(", "best_k_idx_list", ")", "\n", "gen_seq", "[", ":", ",", "step", "]", "=", "best_k_idx_no_oovs", "\n", "gen_seq_ext", "[", ":", ",", "step", "]", "=", "best_k_idx", "\n", "\n", "return", "gen_seq", ",", "gen_seq_ext", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch.generate_sentence": [[146, 190], ["[].tolist", "src_seq.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "beamsearch.get_pad_mask", "beamsearch.BeamSearch._get_init_state", "range", "beamsearch.BeamSearch._model_decode", "beamsearch.BeamSearch._get_the_best_score_and_idx", "beamsearch.BeamSearch.len_map.masked_fill().min", "scores.div().max", "ans_idx.item.item.item", "beamsearch.BeamSearch.len_map.masked_fill", "scores.div", "seq_lens.float", "eos_locs.sum"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_pad_mask", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._get_init_state", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._model_decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch._get_the_best_score_and_idx"], ["", "def", "generate_sentence", "(", "self", ",", "src_seq", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", ":", "\n", "# Only accept batch size equals to 1 in this function.", "\n", "# TODO: expand to batch operation.", "\n", "        ", "assert", "src_seq", ".", "size", "(", "0", ")", "==", "1", "\n", "# set params", "\n", "src_pad_idx", ",", "trg_eos_idx", "=", "self", ".", "src_pad_idx", ",", "self", ".", "trg_eos_idx", "\n", "max_seq_len", ",", "beam_size", ",", "alpha", "=", "self", ".", "max_seq_len", ",", "self", ".", "beam_size", ",", "self", ".", "alpha", "\n", "self", ".", "enc_wiht_extend_vocab", "=", "enc_wiht_extend_vocab", "\n", "self", ".", "max_ext_len", "=", "max_ext_len", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "#get mask of src_seq", "\n", "            ", "src_mask", "=", "get_pad_mask", "(", "src_seq", ",", "src_pad_idx", ")", "\n", "#get init enc_output, gen_seq and scores:", "\n", "# - enc_output: [beam_size, seq_len, d_model]", "\n", "# - gen_seq: [beam_size, seq_len]", "\n", "# - scores: [beam_size]", "\n", "enc_output", ",", "gen_seq", ",", "gen_seq_ext", ",", "scores", "=", "self", ".", "_get_init_state", "(", "src_seq", ",", "src_mask", ")", "\n", "\n", "ans_idx", "=", "0", "# default", "\n", "for", "step", "in", "range", "(", "2", ",", "max_seq_len", ")", ":", "# decode up to max length", "\n", "# dec_output: [beam_size, step, vocab_size]", "\n", "                ", "dec_output", "=", "self", ".", "_model_decode", "(", "gen_seq", "[", ":", ",", ":", "step", "]", ",", "enc_output", ",", "src_mask", ")", "\n", "# gen_seq: [beam_size, max_seq_len]", "\n", "# scores: [beam_size]", "\n", "gen_seq", ",", "gen_seq_ext", ",", "scores", "=", "self", ".", "_get_the_best_score_and_idx", "(", "gen_seq", ",", "gen_seq_ext", ",", "dec_output", ",", "scores", ",", "step", ")", "\n", "\n", "# Check if all path finished", "\n", "# -- locate the eos in the generated sequences", "\n", "# -- eos_locs size: [beam_size, max_seq_len], True of False", "\n", "eos_locs", "=", "gen_seq_ext", "==", "trg_eos_idx", "\n", "# -- replace the eos with its position for the length penalty use", "\n", "# -- seq_lens size: [beam_size]", "\n", "# \u5bf9eos_locs\u53d6\u53cd\uff0c\u7136\u540e\u5c06\u4e3aTrue\u7684\u5730\u65b9\u586b\u5145\u6570\u503c\u4e3amax_seq_len\uff0c\u6700\u7ec8\u539f\u6765eos_locs\u4e3aTrue\u7684\u4fdd\u6301\u4e0d\u53d8", "\n", "# self.len_map.size(): [1, max_seq_len]", "\n", "# self.len_map.masked_fill(~eos_locs, max_seq_len).size(): [beam_size, max_seq_len]", "\n", "seq_lens", ",", "_", "=", "self", ".", "len_map", ".", "masked_fill", "(", "~", "eos_locs", ",", "max_seq_len", ")", ".", "min", "(", "1", ")", "\n", "# -- check if all beams contain eos", "\n", "if", "(", "eos_locs", ".", "sum", "(", "1", ")", ">", "0", ")", ".", "sum", "(", "0", ")", ".", "item", "(", ")", "==", "beam_size", ":", "\n", "# TODO: Try different terminate conditions.", "\n", "# \u6bcf\u4e2a\u5206\u6570\u5904\u4ee5\u5e8f\u5217\u957f\u5ea6\u7684alpha\u6b21\u65b9\uff0c\u9009\u51fa\u5f97\u5206\u6700\u5927\u7684beam", "\n", "                    ", "_", ",", "ans_idx", "=", "scores", ".", "div", "(", "seq_lens", ".", "float", "(", ")", "**", "alpha", ")", ".", "max", "(", "0", ")", "\n", "ans_idx", "=", "ans_idx", ".", "item", "(", ")", "\n", "break", "\n", "", "", "", "return", "gen_seq_ext", "[", "ans_idx", "]", "[", ":", "seq_lens", "[", "ans_idx", "]", "]", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.__init__": [[193, 220], ["Dataset.Vocab", "beamsearch.Search.vocab.tokenizer._convert_token_to_id", "beamsearch.Search.vocab.tokenizer._convert_token_to_id", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "Model.Model.Model().to", "beamsearch.Search.model.load_state_dict", "print", "BeamSearch().to", "Model.Model.Model", "beamsearch.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "model_file_path", ",", "dataloader", ",", "data_file_prefix", "=", "\"test\"", ")", ":", "\n", "        ", "self", ".", "model_file_path", "=", "model_file_path", "\n", "# param", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "data_file_prefix", "=", "data_file_prefix", "\n", "self", ".", "vocab", "=", "Vocab", "(", "params", ",", "mode", "=", "\"decoder\"", ")", "\n", "self", ".", "dataloader", "=", "dataloader", "\n", "self", ".", "bos_id", "=", "self", ".", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "Dataset", ".", "CODEBERT_START_DECODING", ")", "\n", "self", ".", "eos_id", "=", "self", ".", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "Dataset", ".", "CODEBERT_STOP_DECODING", ")", "\n", "\n", "# model", "\n", "checkpoint", "=", "torch", ".", "load", "(", "model_file_path", ")", "\n", "model_opt", "=", "checkpoint", "[", "'settings'", "]", "\n", "self", ".", "model", "=", "Model", "(", "params", ")", ".", "to", "(", "self", ".", "params", ".", "device", ")", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ")", "\n", "print", "(", "'[Info] Trained model state loaded.'", ")", "\n", "\n", "# generator", "\n", "self", ".", "generator", "=", "BeamSearch", "(", "\n", "model", "=", "self", ".", "model", ",", "\n", "params", "=", "params", ",", "\n", "beam_size", "=", "self", ".", "params", ".", "beam_size", ",", "\n", "max_seq_len", "=", "self", ".", "params", ".", "max_dec_len", ",", "\n", "src_pad_idx", "=", "0", ",", "\n", "trg_pad_idx", "=", "1", ",", "\n", "trg_bos_idx", "=", "self", ".", "bos_id", ",", "\n", "trg_eos_idx", "=", "self", ".", "eos_id", ")", ".", "to", "(", "self", ".", "params", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.get_evaluation": [[221, 232], ["utils.get_rouge_dict", "utils.get_bleu4_score", "utils.get_executable_rate", "utils.get_func_correctness", "utils.get_func_correctness"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_rouge_dict", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_bleu4_score", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_executable_rate", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness"], ["", "def", "get_evaluation", "(", "self", ",", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "result_dict", "=", "get_rouge_dict", "(", "hyps_list", ",", "gold_list", ")", "\n", "result_dict", "[", "'bleu'", "]", "=", "get_bleu4_score", "(", "hyps_list", ",", "gold_list", ",", "self", ".", "vocab", ".", "tokenizer", ")", "\n", "result_dict", "[", "'excutable_rate'", "]", "=", "get_executable_rate", "(", "hyps_list", ")", "\n", "rate", ",", "index", "=", "get_func_correctness", "(", "hyps_list", ",", "gold_list", ",", "repalce_string", "=", "True", ",", "need_index", "=", "True", ")", "\n", "result_dict", "[", "'AST_same_without_str_rate'", "]", "=", "rate", "\n", "result_dict", "[", "'exact_without_str_index'", "]", "=", "index", "\n", "rate", ",", "index", "=", "get_func_correctness", "(", "hyps_list", ",", "gold_list", ",", "need_index", "=", "True", ")", "\n", "result_dict", "[", "'func_correctness_rate'", "]", "=", "rate", "\n", "result_dict", "[", "'exact_index'", "]", "=", "index", "\n", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.store_res": [[233, 251], ["beamsearch.Search.get_evaluation", "pandas.DataFrame", "beamsearch.Search.model_file_path.rindex", "beamsearch.Search.model_file_path.rindex", "pandas.DataFrame.to_csv", "os.path.exists", "os.makedirs", "os.path.join", "open", "json.dump", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.get_evaluation"], ["", "def", "store_res", "(", "self", ",", "hyps_list", ",", "refs_list", ")", ":", "\n", "# rouge and bleu", "\n", "        ", "result_dict", "=", "self", ".", "get_evaluation", "(", "hyps_list", ",", "refs_list", ")", "\n", "# store the decode result", "\n", "sotre_res", "=", "pd", ".", "DataFrame", "(", ")", "\n", "sotre_res", "[", "'hyps'", "]", "=", "hyps_list", "\n", "sotre_res", "[", "'refs'", "]", "=", "refs_list", "\n", "i", "=", "self", ".", "model_file_path", ".", "rindex", "(", "'model/'", ")", "\n", "j", "=", "self", ".", "model_file_path", ".", "rindex", "(", "'.'", ")", "\n", "decode_res_name", "=", "self", ".", "model_file_path", "[", "i", "+", "6", ":", "j", "]", "+", "'_decode.csv'", "\n", "decode_path", "=", "self", ".", "model_file_path", "[", ":", "i", "]", "+", "\"decode\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "decode_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "decode_path", ")", "\n", "", "sotre_res", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "decode_path", ",", "decode_res_name", ")", ")", "\n", "# store the performance", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "decode_path", ",", "self", ".", "model_file_path", "[", "i", "+", "6", ":", "j", "]", "+", "\"_result_dict.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "result_dict", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.decode": [[252, 286], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "tqdm.tqdm.tqdm", "beamsearch.Search.store_res", "beamsearch.Search.generator.generate_sentence", "print", "print", "print", "print", "print", "print", "print", "hyps_list.append", "refs_list.append", "beamsearch.Search.vocab.convert_tokens_to_string().replace().replace", "beamsearch.Search.vocab.tokenizer.id2sentence().replace", "beamsearch.Search.vocab.tokenizer.convert_tokens_to_string", "beamsearch.Search.vocab.tokenizer.convert_tokens_to_string", "beamsearch.Search.vocab.tokenizer.tokenize", "beamsearch.Search.vocab.tokenizer.tokenize", "beamsearch.Search.vocab.convert_tokens_to_string().replace", "beamsearch.Search.vocab.tokenizer.id2sentence", "beamsearch.Search.vocab.convert_tokens_to_string", "Dataset.PGprocess.outputids2words"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.Search.store_res", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.BeamSearch.generate_sentence", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.id2sentence", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.outputids2words"], ["", "def", "decode", "(", "self", ")", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "self", ".", "params", ".", "device", ")", "\n", "hyps_list", "=", "[", "]", "\n", "refs_list", "=", "[", "]", "\n", "desc", "=", "'  - (Testing) '", "\n", "for", "batch", "in", "tqdm", "(", "self", ".", "dataloader", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dce_input", ",", "dce_target", "=", "batch", "\n", "max_ext_len", "=", "self", ".", "dataloader", ".", "dataset", ".", "max_src_oovs", "\n", "src_oovs", "=", "[", "self", ".", "dataloader", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dce_input", ",", "dce_target", "=", "batch", "\n", "# print(\"input data: \", enc_batch, dce_input, dce_target)", "\n", "", "pred_seq", "=", "self", ".", "generator", ".", "generate_sentence", "(", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "# print(\"pred_seq: \", pred_seq)", "\n", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "pred_line", "=", "self", ".", "vocab", ".", "convert_tokens_to_string", "(", "PGprocess", ".", "outputids2words", "(", "pred_seq", ",", "self", ".", "vocab", ",", "src_oovs", ")", ")", ".", "replace", "(", "Dataset", ".", "CODEBERT_START_DECODING", ",", "\"\"", ")", ".", "replace", "(", "Dataset", ".", "CODEBERT_PAD_TOKEN", ",", "\"\"", ")", "\n", "", "else", ":", "\n", "                ", "pred_line", "=", "self", ".", "vocab", ".", "tokenizer", ".", "id2sentence", "(", "pred_seq", ")", ".", "replace", "(", "Dataset", ".", "CODEBERT_START_DECODING", ",", "\"\"", ")", "\n", "", "ref_line", "=", "self", ".", "dataloader", ".", "dataset", ".", "example_list", "[", "data_index", "]", ".", "original_trg", "\n", "ecn_line", "=", "self", ".", "dataloader", ".", "dataset", ".", "example_list", "[", "data_index", "]", ".", "original_src", "\n", "print", "(", ")", "\n", "print", "(", "\">>>>>>data_index: \"", ",", "data_index", ")", "\n", "print", "(", "\">>>>>>ref_line: \"", ",", "ref_line", ")", "\n", "print", "(", "\">>>>>>ecn_line: \\n\"", ",", "ecn_line", ")", "\n", "print", "(", "\">>>>>>pred_seq: \"", ",", "pred_seq", ")", "\n", "print", "(", "\">>>>>>pred_line: \\n\"", ",", "pred_line", ")", "\n", "print", "(", ")", "\n", "\n", "hyps_list", ".", "append", "(", "self", ".", "vocab", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "self", ".", "vocab", ".", "tokenizer", ".", "tokenize", "(", "pred_line", ")", ")", ")", "\n", "refs_list", ".", "append", "(", "self", ".", "vocab", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "self", ".", "vocab", ".", "tokenizer", ".", "tokenize", "(", "ref_line", ")", ")", ")", "\n", "", "result_dict", "=", "self", ".", "store_res", "(", "hyps_list", ",", "refs_list", ")", "\n", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.get_pad_mask": [[17, 19], ["None"], "function", ["None"], ["def", "get_pad_mask", "(", "seq", ",", "pad_idx", ")", ":", "\n", "    ", "return", "(", "seq", "!=", "pad_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.beamsearch.get_subsequent_mask": [[20, 26], ["seq.size", "torch.triu", "torch.triu", "torch.triu", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "get_subsequent_mask", "(", "seq", ")", ":", "\n", "    ", "''' For masking out the subsequent info. '''", "\n", "sz_b", ",", "len_s", "=", "seq", ".", "size", "(", ")", "\n", "subsequent_mask", "=", "(", "1", "-", "torch", ".", "triu", "(", "\n", "torch", ".", "ones", "(", "(", "1", ",", "len_s", ",", "len_s", ")", ",", "device", "=", "seq", ".", "device", ")", ",", "diagonal", "=", "1", ")", ")", ".", "bool", "(", ")", "\n", "return", "subsequent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.label_smoothing.LabelSmoothing.__init__": [[7, 19], ["torch.Module.__init__", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["def", "__init__", "(", "self", ",", "device", ",", "size", ",", "padding_idx", ",", "label_smoothing", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "LabelSmoothing", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "0.0", "<", "label_smoothing", "<=", "1.0", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "device", "=", "device", "\n", "\n", "self", ".", "smoothing_value", "=", "label_smoothing", "/", "(", "size", "-", "2", ")", "\n", "self", ".", "one_hot", "=", "torch", ".", "full", "(", "(", "1", ",", "size", ")", ",", "self", ".", "smoothing_value", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "one_hot", "[", "0", ",", "self", ".", "padding_idx", "]", "=", "0", "\n", "\n", "self", ".", "confidence", "=", "1.0", "-", "label_smoothing", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.label_smoothing.LabelSmoothing.forward": [[20, 35], ["output.size", "label_smoothing.LabelSmoothing.one_hot.repeat", "torch.cat.scatter_", "torch.cat.scatter_", "torch.cat.scatter_", "torch.cat.masked_fill_", "torch.cat.masked_fill_", "torch.cat.masked_fill_", "torch.kl_div", "torch.kl_div", "torch.kl_div", "target.size", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.full().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.cat.size", "torch.cat.size", "torch.cat.size"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "real_size", "=", "output", ".", "size", "(", "1", ")", "\n", "if", "real_size", ">", "self", ".", "size", ":", "\n", "            ", "real_size", "-=", "self", ".", "size", "\n", "", "else", ":", "\n", "            ", "real_size", "=", "0", "\n", "\n", "", "model_prob", "=", "self", ".", "one_hot", ".", "repeat", "(", "target", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "if", "real_size", ">", "0", ":", "\n", "            ", "ext_zeros", "=", "torch", ".", "full", "(", "(", "model_prob", ".", "size", "(", "0", ")", ",", "real_size", ")", ",", "self", ".", "smoothing_value", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "model_prob", "=", "torch", ".", "cat", "(", "(", "model_prob", ",", "ext_zeros", ")", ",", "-", "1", ")", "\n", "", "model_prob", ".", "scatter_", "(", "1", ",", "target", ",", "self", ".", "confidence", ")", "\n", "model_prob", ".", "masked_fill_", "(", "(", "target", "==", "self", ".", "padding_idx", ")", ",", "0.", ")", "\n", "\n", "return", "F", ".", "kl_div", "(", "output", ",", "model_prob", ",", "reduction", "=", "\"sum\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.__init__": [[7, 13], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "optimizer", ",", "init_lr", ",", "d_model", ",", "n_warmup_steps", ")", ":", "\n", "        ", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "init_lr", "=", "init_lr", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "n_warmup_steps", "=", "n_warmup_steps", "\n", "self", ".", "n_steps", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step": [[15, 19], ["ScheduledOptim.ScheduledOptim._update_learning_rate", "ScheduledOptim.ScheduledOptim._optimizer.step"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim._update_learning_rate", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "\"Step with the inner optimizer\"", "\n", "self", ".", "_update_learning_rate", "(", ")", "\n", "self", ".", "_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad": [[21, 24], ["ScheduledOptim.ScheduledOptim._optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"Zero out the gradients with the inner optimizer\"", "\n", "self", ".", "_optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim._get_lr_scale": [[26, 30], ["min"], "methods", ["None"], ["", "def", "_get_lr_scale", "(", "self", ")", ":", "\n", "        ", "d_model", "=", "self", ".", "d_model", "\n", "n_steps", ",", "n_warmup_steps", "=", "self", ".", "n_steps", ",", "self", ".", "n_warmup_steps", "\n", "return", "(", "d_model", "**", "-", "0.5", ")", "*", "min", "(", "n_steps", "**", "(", "-", "0.5", ")", ",", "n_steps", "*", "n_warmup_steps", "**", "(", "-", "1.5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim._update_learning_rate": [[32, 40], ["ScheduledOptim.ScheduledOptim._get_lr_scale"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim._get_lr_scale"], ["", "def", "_update_learning_rate", "(", "self", ")", ":", "\n", "        ", "''' Learning rate scheduling per step '''", "\n", "\n", "self", ".", "n_steps", "+=", "1", "\n", "lr", "=", "self", ".", "init_lr", "*", "self", ".", "_get_lr_scale", "(", ")", "\n", "\n", "for", "param_group", "in", "self", ".", "_optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.__init__": [[26, 38], ["object.__init__", "transformers.BertTokenizer.from_pretrained", "Dataset.Vocab.tokenizer.get_vocab", "transformers.BertTokenizer.from_pretrained", "Dataset.Vocab.tokenizer.get_vocab", "Dataset.CodeTokenizer"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "mode", "=", "'encoder-en'", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "if", "self", ".", "mode", "==", "\"encoder-en\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "self", ".", "vocab", "=", "self", ".", "tokenizer", ".", "get_vocab", "(", ")", "\n", "", "elif", "self", ".", "mode", "==", "\"encoder-zh\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"bert-base-chinese\"", ")", "\n", "self", ".", "vocab", "=", "self", ".", "tokenizer", ".", "get_vocab", "(", ")", "\n", "", "elif", "self", ".", "mode", "==", "\"decoder\"", ":", "\n", "            ", "self", ".", "tokenizer", "=", "CodeTokenizer", "(", "config", ".", "vocab_path", ",", "bpe_mode", "=", "config", ".", "bpe_mode", ")", "\n", "self", ".", "vocab", "=", "self", ".", "tokenizer", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size": [[39, 41], ["len"], "methods", ["None"], ["", "", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.src2ids": [[45, 60], ["src_vocab.tokenizer._convert_token_to_id", "src_vocab.tokenizer._convert_token_to_id", "oovs.index", "ids.append", "ids.append", "oovs.append", "src_vocab.size"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["    ", "@", "classmethod", "\n", "def", "src2ids", "(", "self", ",", "src_words", ",", "src_vocab", ")", ":", "\n", "        ", "ids", "=", "[", "]", "\n", "oovs", "=", "[", "]", "\n", "unk_id", "=", "src_vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "src_words", ":", "\n", "            ", "i", "=", "src_vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is OOV", "\n", "                ", "if", "w", "not", "in", "oovs", ":", "# Add to list of OOVs", "\n", "                    ", "oovs", ".", "append", "(", "w", ")", "\n", "", "oov_num", "=", "oovs", ".", "index", "(", "w", ")", "# This is 0 for the first article OOV, 1 for the second article OOV...", "\n", "ids", ".", "append", "(", "src_vocab", ".", "size", "(", ")", "+", "oov_num", ")", "# This is e.g. 50000 for the first article OOV, 50001 for the second...", "\n", "", "else", ":", "\n", "                ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", ",", "oovs", "\n", "", "@", "classmethod", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.trg2ids": [[60, 75], ["tgt_vocab.tokenizer._convert_token_to_id", "tgt_vocab.tokenizer._convert_token_to_id", "ids.append", "ids.append", "ids.append", "tgt_vocab.size", "src_oovs.index"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "@", "classmethod", "\n", "def", "trg2ids", "(", "self", ",", "trg_words", ",", "tgt_vocab", ",", "src_oovs", ")", ":", "\n", "        ", "ids", "=", "[", "]", "\n", "unk_id", "=", "tgt_vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "CODEBERT_UNKNOWN_TOKEN", ")", "\n", "for", "w", "in", "trg_words", ":", "\n", "            ", "i", "=", "tgt_vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "w", ")", "\n", "if", "i", "==", "unk_id", ":", "# If w is an OOV word", "\n", "                ", "if", "w", "in", "src_oovs", ":", "# If w is an in-article OOV", "\n", "                    ", "vocab_idx", "=", "tgt_vocab", ".", "size", "(", ")", "+", "src_oovs", ".", "index", "(", "w", ")", "# Map to its temporary article OOV number", "\n", "ids", ".", "append", "(", "vocab_idx", ")", "\n", "", "else", ":", "# If w is an out-of-article OOV", "\n", "                    ", "ids", ".", "append", "(", "unk_id", ")", "# Map to the UNK token id", "\n", "", "", "else", ":", "\n", "                ", "ids", ".", "append", "(", "i", ")", "\n", "", "", "return", "ids", "\n", "", "@", "classmethod", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.outputids2words": [[75, 93], ["words.append", "tgt_vocab.tokenizer._convert_id_to_token", "tgt_vocab.size", "print", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_id_to_token", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "@", "classmethod", "\n", "def", "outputids2words", "(", "self", ",", "id_list", ",", "tgt_vocab", ",", "src_oovs", ")", ":", "\n", "        ", "words", "=", "[", "]", "\n", "for", "i", "in", "id_list", ":", "\n", "            ", "try", ":", "\n", "                ", "w", "=", "tgt_vocab", ".", "tokenizer", ".", "_convert_id_to_token", "(", "i", ")", "# might be [UNK]", "\n", "", "except", "ValueError", "as", "e", ":", "# w is OOV", "\n", "                ", "assert", "src_oovs", "is", "not", "None", ",", "\"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"", "\n", "src_oov_idx", "=", "i", "-", "tgt_vocab", ".", "size", "(", ")", "\n", "try", ":", "\n", "                    ", "w", "=", "src_oovs", "[", "src_oov_idx", "]", "\n", "print", "(", "\"_________Copy Worked_________\"", ")", "\n", "", "except", "ValueError", "as", "e", ":", "# i doesn't correspond to an article oov", "\n", "                    ", "raise", "ValueError", "(", "\n", "'Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs'", "%", "(", "\n", "i", ",", "src_oov_idx", ",", "len", "(", "src_oovs", ")", ")", ")", "\n", "", "", "words", ".", "append", "(", "w", ")", "\n", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Code4SQLDataset2PG.__init__": [[97, 111], ["os.path.join", "Dataset.Vocab", "Dataset.Vocab", "Dataset.Code4SQLDataset2PG.get_data", "len", "max", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Code4SQLDataset2PG.get_data"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "dataName", "=", "'train'", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "encoder_language", "=", "config", ".", "encoder_language", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "base_path", "=", "config", ".", "data_dir", "\n", "self", ".", "data_file_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_path", ",", "dataName", "+", "'.'", "+", "config", ".", "data_file_suffix", ")", "\n", "self", ".", "src_vocab", "=", "Vocab", "(", "config", ",", "self", ".", "encoder_language", ")", "\n", "self", ".", "trg_vocab", "=", "Vocab", "(", "config", ",", "\"decoder\"", ")", "\n", "self", ".", "example_list", "=", "self", ".", "get_data", "(", "self", ".", "data_file_path", ")", "\n", "self", ".", "length", "=", "len", "(", "self", ".", "example_list", ")", "\n", "self", ".", "max_src_oovs", "=", "None", "\n", "if", "config", ".", "pointer_gen", ":", "\n", "            ", "self", ".", "max_src_oovs", "=", "max", "(", "[", "len", "(", "ex", ".", "enc_oovs", ")", "for", "ex", "in", "self", ".", "example_list", "]", ")", "\n", "self", ".", "src_oovs", "=", "[", "ex", ".", "enc_oovs", "for", "ex", "in", "self", ".", "example_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Code4SQLDataset2PG.get_data": [[112, 125], ["pandas.read_csv", "range", "Dataset.Example", "example_list.append"], "methods", ["None"], ["", "", "def", "get_data", "(", "self", ",", "data_file_path", ")", ":", "\n", "        ", "example_list", "=", "[", "]", "\n", "data_df", "=", "pd", ".", "read_csv", "(", "data_file_path", ")", "\n", "if", "self", ".", "encoder_language", "==", "\"encoder-en\"", ":", "\n", "            ", "col", "=", "\"comm_en\"", "\n", "", "elif", "self", ".", "encoder_language", "==", "\"encoder-zh\"", ":", "\n", "            ", "col", "=", "\"comm_zh\"", "\n", "", "for", "i", "in", "range", "(", "data_df", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "src_seq", "=", "data_df", "[", "col", "]", "[", "i", "]", "\n", "trg_seq", "=", "data_df", "[", "\"code\"", "]", "[", "i", "]", "\n", "ex", "=", "Example", "(", "self", ".", "config", ",", "i", ",", "src_seq", ",", "trg_seq", ",", "self", ".", "src_vocab", ",", "self", ".", "trg_vocab", ")", "\n", "example_list", ".", "append", "(", "ex", ")", "\n", "", "return", "example_list", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Code4SQLDataset2PG.__getitem__": [[126, 139], ["torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "len", "len", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "data_index", "=", "self", ".", "example_list", "[", "index", "]", ".", "id", "\n", "enc_tensor", "=", "torch", ".", "tensor", "(", "self", ".", "example_list", "[", "index", "]", ".", "enc_padded", ")", ".", "to", "(", "self", ".", "device", ")", "#as enc input", "\n", "dec_input", "=", "torch", ".", "tensor", "(", "self", ".", "example_list", "[", "index", "]", ".", "dec_input", ")", ".", "to", "(", "self", ".", "device", ")", "#as dec input", "\n", "dec_target", "=", "torch", ".", "tensor", "(", "self", ".", "example_list", "[", "index", "]", ".", "dec_target", ")", ".", "to", "(", "self", ".", "device", ")", "#as dec target", "\n", "assert", "len", "(", "dec_input", ")", "==", "len", "(", "dec_target", ")", "\n", "if", "self", ".", "config", ".", "pointer_gen", ":", "\n", "            ", "enc_wiht_extend_vocab", "=", "torch", ".", "tensor", "(", "self", ".", "example_list", "[", "index", "]", ".", "enc_input_extend_vocab", ")", ".", "to", "(", "self", ".", "device", ")", "#as index of scatter_add func", "\n", "dec_target", "=", "torch", ".", "tensor", "(", "self", ".", "example_list", "[", "index", "]", ".", "dec_target", ")", ".", "to", "(", "self", ".", "device", ")", "#as ground truth", "\n", "assert", "len", "(", "enc_tensor", ")", "==", "len", "(", "enc_wiht_extend_vocab", ")", "\n", "return", "data_index", ",", "enc_tensor", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "\n", "", "else", ":", "\n", "            ", "return", "data_index", ",", "enc_tensor", ",", "dec_input", ",", "dec_target", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Code4SQLDataset2PG.__len__": [[140, 142], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.__init__": [[145, 175], ["src_vocab.tokenizer.tokenize", "src_vocab.tokenizer.encode", "trg_vocab.tokenizer.tokenize", "trg_vocab.tokenizer.encode", "Dataset.Example.init_seq", "Dataset.PGprocess.src2ids", "Dataset.Example.init_seq", "Dataset.PGprocess.trg2ids", "Dataset.Example.init_seq", "Dataset.Example.init_seq", "dec.replace"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.init_seq", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.src2ids", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.init_seq", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.trg2ids", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.init_seq", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.init_seq"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "id", ",", "enc", ",", "dec", ",", "src_vocab", ",", "trg_vocab", ")", ":", "\n", "        ", "self", ".", "id", "=", "id", "# use id to track each example", "\n", "self", ".", "max_enc_len", "=", "config", ".", "max_enc_len", "\n", "self", ".", "max_dec_len", "=", "config", ".", "max_dec_len", "\n", "# seq to words", "\n", "self", ".", "enc_words", "=", "src_vocab", ".", "tokenizer", ".", "tokenize", "(", "enc", ")", "\n", "enc_ids", "=", "src_vocab", ".", "tokenizer", ".", "encode", "(", "self", ".", "enc_words", ")", "\n", "self", ".", "dec_words", "=", "trg_vocab", ".", "tokenizer", ".", "tokenize", "(", "dec", ")", "\n", "dec_ids", "=", "trg_vocab", ".", "tokenizer", ".", "encode", "(", "dec", ")", "\n", "# pad the encoder seq", "\n", "self", ".", "enc_padded", "=", "self", ".", "init_seq", "(", "enc_ids", ",", "None", ",", "src_vocab", ",", "self", ".", "max_enc_len", ")", "\n", "\n", "# add some info if pg network is allow", "\n", "if", "config", ".", "pointer_gen", ":", "\n", "            ", "self", ".", "enc_input_extend_vocab", ",", "self", ".", "enc_oovs", "=", "PGprocess", ".", "src2ids", "(", "self", ".", "enc_words", ",", "src_vocab", ")", "\n", "#pad enc_input_extend_vocab", "\n", "self", ".", "enc_input_extend_vocab", "=", "self", ".", "init_seq", "(", "self", ".", "enc_input_extend_vocab", ",", "None", ",", "src_vocab", ",", "self", ".", "max_enc_len", ")", "\n", "#get decode target", "\n", "trg_ids_extend_vocab", "=", "PGprocess", ".", "trg2ids", "(", "self", ".", "dec_words", ",", "trg_vocab", ",", "self", ".", "enc_oovs", ")", "\n", "self", ".", "dec_input", ",", "self", ".", "dec_target", "=", "self", ".", "init_seq", "(", "dec_ids", ",", "trg_ids_extend_vocab", ",", "trg_vocab", ",", "self", ".", "max_dec_len", ",", "type", "=", "\"decode\"", ")", "\n", "", "else", ":", "\n", "# pad the decoder seq", "\n", "            ", "self", ".", "dec_input", ",", "self", ".", "dec_target", "=", "self", ".", "init_seq", "(", "dec_ids", ",", "dec_ids", ",", "trg_vocab", ",", "self", ".", "max_dec_len", ",", "type", "=", "\"decode\"", ")", "\n", "\n", "#original text", "\n", "", "self", ".", "original_src", "=", "enc", "\n", "if", "config", ".", "bpe_mode", ":", "\n", "            ", "self", ".", "original_trg", "=", "dec", "\n", "", "else", ":", "\n", "            ", "self", ".", "original_trg", "=", "dec", ".", "replace", "(", "\"\\t\"", ",", "\"    \"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.init_seq": [[176, 183], ["Dataset.Example.pad_enc_seq", "vocab.tokenizer._convert_token_to_id", "Dataset.Example.pad_dec_seq", "vocab.tokenizer._convert_token_to_id", "vocab.tokenizer._convert_token_to_id", "vocab.tokenizer._convert_token_to_id"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.pad_enc_seq", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.pad_dec_seq", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id"], ["", "", "def", "init_seq", "(", "self", ",", "seq", ",", "seq_ext", ",", "vocab", ",", "max_len", ",", "type", "=", "\"encode\"", ")", ":", "\n", "        ", "if", "type", "==", "\"encode\"", ":", "\n", "            ", "return", "self", ".", "pad_enc_seq", "(", "seq", ",", "max_len", ",", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "PAD_TOKEN", ")", ")", "\n", "", "elif", "type", "==", "\"decode\"", ":", "\n", "            ", "return", "self", ".", "pad_dec_seq", "(", "seq", ",", "seq_ext", ",", "max_len", ",", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "CODEBERT_PAD_TOKEN", ")", ",", "\n", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "CODEBERT_START_DECODING", ")", ",", "\n", "vocab", ".", "tokenizer", ".", "_convert_token_to_id", "(", "CODEBERT_STOP_DECODING", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.pad_enc_seq": [[184, 191], ["len", "seq.extend", "len", "len"], "methods", ["None"], ["", "", "def", "pad_enc_seq", "(", "self", ",", "seq", ",", "max_len", ",", "pad_id", ")", ":", "\n", "        ", "if", "len", "(", "seq", ")", ">", "max_len", ":", "\n", "            ", "seq", "=", "seq", "[", ":", "max_len", "]", "\n", "", "else", ":", "\n", "            ", "seq", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "max_len", "-", "len", "(", "seq", ")", ")", ")", "\n", "", "assert", "len", "(", "seq", ")", "==", "max_len", "\n", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Example.pad_dec_seq": [[192, 203], ["len", "inp.extend", "trg.extend", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "pad_dec_seq", "(", "self", ",", "seq", ",", "seq_ext", ",", "max_len", ",", "pad_id", ",", "start_decoding", ",", "stop_decoding", ")", ":", "\n", "        ", "inp", "=", "[", "start_decoding", "]", "+", "seq", "[", ":", "]", "\n", "trg", "=", "seq_ext", "[", ":", "]", "+", "[", "stop_decoding", "]", "\n", "if", "len", "(", "inp", ")", ">", "max_len", ":", "\n", "            ", "inp", "=", "inp", "[", ":", "max_len", "]", "\n", "trg", "=", "trg", "[", ":", "max_len", "]", "\n", "", "else", ":", "\n", "            ", "inp", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "max_len", "-", "len", "(", "inp", ")", ")", ")", "\n", "trg", ".", "extend", "(", "[", "pad_id", "]", "*", "(", "max_len", "-", "len", "(", "trg", ")", ")", ")", "\n", "", "assert", "len", "(", "inp", ")", "==", "len", "(", "trg", ")", "==", "max_len", "\n", "return", "inp", ",", "trg", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.__init__": [[206, 218], ["transformers.RobertaTokenizer.from_pretrained", "Dataset.CodeTokenizer.tokenizer.get_vocab", "Dataset.CodeTokenizer.get_vocab_from_file", "Dataset.CodeTokenizer._word_to_id.items"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.get_vocab_from_file"], ["    ", "def", "__init__", "(", "self", ",", "vocab_path", ",", "bpe_mode", "=", "False", ")", "->", "None", ":", "\n", "        ", "self", ".", "bpe_mode", "=", "bpe_mode", "\n", "if", "vocab_path", "==", "None", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "if", "bpe_mode", ":", "\n", "                ", "self", ".", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "\"microsoft/codebert-base\"", ")", "\n", "self", ".", "vocab", "=", "self", ".", "tokenizer", ".", "get_vocab", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "vocab", "=", "self", ".", "get_vocab_from_file", "(", "vocab_path", ")", "\n", "self", ".", "_word_to_id", "=", "self", ".", "vocab", "\n", "self", ".", "_id_to_word", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "_word_to_id", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id": [[219, 226], ["Dataset.CodeTokenizer.tokenizer._convert_token_to_id"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id"], ["", "", "", "def", "_convert_token_to_id", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "_convert_token_to_id", "(", "word", ")", "\n", "", "else", ":", "\n", "            ", "if", "word", "not", "in", "self", ".", "_word_to_id", ":", "\n", "                ", "return", "self", ".", "_word_to_id", "[", "CODEBERT_UNKNOWN_TOKEN", "]", "\n", "", "return", "self", ".", "_word_to_id", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_id_to_token": [[227, 234], ["Dataset.CodeTokenizer.tokenizer._convert_id_to_token", "ValueError"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_id_to_token"], ["", "", "def", "_convert_id_to_token", "(", "self", ",", "word_id", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "_convert_id_to_token", "(", "word_id", ")", "\n", "", "else", ":", "\n", "            ", "if", "word_id", "not", "in", "self", ".", "_id_to_word", ":", "\n", "                ", "raise", "ValueError", "(", "'Id not found in vocab: %d'", "%", "word_id", ")", "\n", "", "return", "self", ".", "_id_to_word", "[", "word_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.encode": [[235, 240], ["Dataset.CodeTokenizer.tokenizer.encode", "Dataset.CodeTokenizer._convert_token_to_id", "Dataset.CodeTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_token_to_id", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["", "", "def", "encode", "(", "self", ",", "s", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "encode", "(", "s", ")", "[", "1", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "self", ".", "_convert_token_to_id", "(", "i", ")", "for", "i", "in", "self", ".", "tokenize", "(", "s", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_ids_to_tokens": [[241, 246], ["Dataset.CodeTokenizer.tokenizer.convert_ids_to_tokens", "Dataset.CodeTokenizer._convert_id_to_token"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer._convert_id_to_token"], ["", "", "def", "convert_ids_to_tokens", "(", "self", ",", "id_list", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "id_list", ")", "\n", "", "else", ":", "\n", "            ", "return", "[", "self", ".", "_convert_id_to_token", "(", "i", ")", "for", "i", "in", "id_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string": [[247, 252], ["Dataset.CodeTokenizer.tokenizer.convert_tokens_to_string", "Dataset.CodeTokenizer.convert_code_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_code_tokens_to_string"], ["", "", "def", "convert_tokens_to_string", "(", "self", ",", "token_list", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "token_list", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "convert_code_tokens_to_string", "(", "token_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_code_tokens_to_string": [[253, 255], ["None"], "methods", ["None"], ["", "", "def", "convert_code_tokens_to_string", "(", "self", ",", "token_list", ")", ":", "\n", "        ", "return", "\"\"", ".", "join", "(", "token_list", ")", ".", "replace", "(", "\"\ud83d\ude1c\"", ",", "\" \"", ")", ".", "replace", "(", "\"<INDENT>\"", ",", "\"    \"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.id2sentence": [[256, 268], ["Dataset.CodeTokenizer.convert_tokens_to_string().replace", "Dataset.CodeTokenizer.index", "Dataset.CodeTokenizer.convert_tokens_to_string", "Dataset.CodeTokenizer.convert_ids_to_tokens", "len", "len", "len", "len", "s.replace", "Dataset.CodeTokenizer.replace"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_ids_to_tokens"], ["", "def", "id2sentence", "(", "self", ",", "word_ids", ")", ":", "\n", "        ", "sentence", "=", "self", ".", "convert_tokens_to_string", "(", "self", ".", "convert_ids_to_tokens", "(", "word_ids", ")", ")", ".", "replace", "(", "CODEBERT_PAD_TOKEN", ",", "\"\"", ")", "\n", "if", "CODEBERT_STOP_DECODING", "in", "sentence", ":", "\n", "            ", "stop_index", "=", "sentence", ".", "index", "(", "CODEBERT_STOP_DECODING", ")", "\n", "s", "=", "sentence", "[", ":", "stop_index", "]", "\n", "if", "len", "(", "s", ")", "==", "0", "or", "len", "(", "s", ".", "replace", "(", "'.'", ",", "\"\"", ")", ")", "==", "'.'", ":", "\n", "                ", "return", "\" \"", "\n", "", "return", "s", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "sentence", ")", "==", "0", "or", "len", "(", "sentence", ".", "replace", "(", "'.'", ",", "\"\"", ")", ")", "==", "0", ":", "\n", "                ", "return", "\" \"", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize": [[269, 275], ["Dataset.CodeTokenizer.tokenizer.tokenize", "s.replace.replace.replace", "Dataset.CodeTokenizer.tokenize_code"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize_code"], ["", "", "def", "tokenize", "(", "self", ",", "s", ")", ":", "\n", "        ", "if", "self", ".", "bpe_mode", ":", "\n", "            ", "return", "self", ".", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "", "else", ":", "\n", "            ", "s", "=", "s", ".", "replace", "(", "\"\\t\"", ",", "\"    \"", ")", "\n", "return", "self", ".", "tokenize_code", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize_code": [[276, 373], ["parso.utils.parse_version_string", "Dataset.CodeTokenizer.tokenize_code.get_space_list"], "methods", ["None"], ["", "", "def", "tokenize_code", "(", "self", ",", "s", ")", ":", "\n", "        ", "token_list", "=", "[", "]", "\n", "DEFAULT_INDENT", "=", "4", "\n", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "\n", "def", "get_space_list", "(", "s", ")", ":", "\n", "            ", "space_list", "=", "[", "]", "\n", "for", "i", "in", "s", ".", "split", "(", "\"\\n\"", ")", ":", "\n", "                ", "space_list", ".", "append", "(", "[", "i", ".", "start", "(", ")", "for", "i", "in", "re", ".", "finditer", "(", "' '", ",", "i", ")", "]", ")", "\n", "", "return", "space_list", "\n", "\n", "", "def", "add_sapce", "(", "word", ",", "space_list", ",", "l_pos", ",", "s_pos", ")", ":", "\n", "# print(word, space_list, l_pos, s_pos)", "\n", "            ", "if", "len", "(", "word", ")", "+", "s_pos", "in", "space_list", "[", "l_pos", "-", "1", "]", ":", "\n", "                ", "token_list", ".", "append", "(", "word", "+", "\"\ud83d\ude1c\"", ")", "\n", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "word", ")", "\n", "\n", "", "", "def", "split_underscore", "(", "words", ",", "space_list", ",", "l_pos", ",", "s_pos", ")", ":", "\n", "            ", "w_list", "=", "words", ".", "split", "(", "\"_\"", ")", "\n", "if", "\"_\"", "in", "words", ":", "\n", "                ", "for", "index", "in", "range", "(", "len", "(", "w_list", ")", ")", ":", "\n", "                    ", "if", "index", "<", "len", "(", "w_list", ")", "-", "1", ":", "\n", "                        ", "token_list", ".", "append", "(", "w_list", "[", "index", "]", ")", "\n", "token_list", ".", "append", "(", "\"_\"", ")", "\n", "", "else", ":", "\n", "                        ", "token_list", ".", "append", "(", "w_list", "[", "index", "]", ")", "\n", "", "", "if", "len", "(", "words", ")", "+", "s_pos", "in", "space_list", "[", "l_pos", "-", "1", "]", ":", "\n", "                    ", "token_list", "[", "-", "1", "]", "=", "token_list", "[", "-", "1", "]", "+", "\"\ud83d\ude1c\"", "\n", "", "", "else", ":", "\n", "                ", "add_sapce", "(", "w_list", "[", "0", "]", ",", "space_list", ",", "l_pos", ",", "s_pos", ")", "\n", "\n", "", "", "code_space_list", "=", "get_space_list", "(", "s", ")", "\n", "dict_DENT", "=", "{", "}", "#line_num : INDENT", "\n", "newline_list", "=", "[", "]", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "STRING", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", "[", "0", "]", ")", "#the symble of '\"'", "\n", "temp_string", "=", "i", ".", "string", "[", "1", ":", "-", "1", "]", "\n", "space_index", "=", "0", "\n", "while", "space_index", "<", "len", "(", "temp_string", ")", "and", "temp_string", "[", "space_index", "]", "==", "\" \"", ":", "\n", "                    ", "token_list", ".", "append", "(", "\"\ud83d\ude1c\"", ")", "\n", "temp_string", "=", "temp_string", "[", "space_index", "+", "1", ":", "]", "\n", "", "space_index", "=", "-", "1", "\n", "end_space", "=", "0", "\n", "while", "abs", "(", "space_index", ")", "<=", "len", "(", "temp_string", ")", "and", "temp_string", "[", "space_index", "]", "==", "\" \"", ":", "\n", "# token_list.append(\"\ud83d\ude1c\") ", "\n", "                    ", "end_space", "+=", "1", "\n", "temp_string", "=", "temp_string", "[", ":", "space_index", "]", "\n", "\n", "", "string_space_list", "=", "get_space_list", "(", "temp_string", ")", "\n", "for", "j", "in", "tokenize", ".", "tokenize", "(", "temp_string", ",", "version_info", ")", ":", "\n", "                    ", "if", "j", ".", "type", "!=", "tokenize", ".", "ENDMARKER", ":", "\n", "# print(j.string)", "\n", "                        ", "split_underscore", "(", "j", ".", "string", ",", "string_space_list", ",", "j", ".", "start_pos", "[", "0", "]", ",", "j", ".", "start_pos", "[", "1", "]", ")", "\n", "", "", "if", "end_space", "!=", "0", ":", "\n", "                    ", "[", "token_list", ".", "append", "(", "\"\ud83d\ude1c\"", ")", "for", "_", "in", "range", "(", "end_space", ")", "]", "\n", "", "token_list", ".", "append", "(", "i", ".", "string", "[", "-", "1", "]", ")", "#the symble of '\"'", "\n", "\n", "", "elif", "i", ".", "type", "==", "tokenize", ".", "INDENT", "or", "i", ".", "type", "==", "tokenize", ".", "DEDENT", ":", "\n", "                ", "s_line", ",", "s_pos", "=", "i", ".", "start_pos", "\n", "#recalculate \"<INDENT>\"", "\n", "while", "len", "(", "token_list", ")", "!=", "0", "and", "token_list", "[", "-", "1", "]", "==", "\"<INDENT>\"", ":", "\n", "                    ", "token_list", "=", "token_list", "[", ":", "-", "1", "]", "\n", "\n", "", "if", "s_pos", "%", "4", "==", "0", ":", "\n", "                    ", "temp_DENT", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "int", "(", "s_pos", "/", "DEFAULT_INDENT", ")", ")", ":", "\n", "                        ", "token_list", ".", "append", "(", "\"<INDENT>\"", ")", "\n", "temp_DENT", ".", "append", "(", "\"<INDENT>\"", ")", "\n", "", "dict_DENT", "[", "s_line", "]", "=", "temp_DENT", "\n", "", "elif", "s_pos", "%", "2", "==", "0", "and", "s_pos", "<", "4", ":", "\n", "                    ", "DEFAULT_INDENT", "=", "2", "\n", "temp_DENT", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "int", "(", "s_pos", "/", "DEFAULT_INDENT", ")", ")", ":", "\n", "                        ", "token_list", ".", "append", "(", "\"<INDENT>\"", ")", "\n", "temp_DENT", ".", "append", "(", "\"<INDENT>\"", ")", "\n", "", "dict_DENT", "[", "s_line", "]", "=", "temp_DENT", "\n", "\n", "", "", "elif", "i", ".", "type", "==", "tokenize", ".", "NEWLINE", ":", "\n", "# Set the default to keep the last line indented", "\n", "                ", "token_list", ".", "append", "(", "\"\\n\"", ")", "\n", "if", "len", "(", "dict_DENT", ".", "keys", "(", ")", ")", "!=", "0", ":", "\n", "                    ", "for", "_", "in", "dict_DENT", "[", "list", "(", "dict_DENT", ".", "keys", "(", ")", ")", "[", "-", "1", "]", "]", ":", "\n", "                        ", "token_list", ".", "append", "(", "\"<INDENT>\"", ")", "\n", "\n", "", "", "", "else", ":", "\n", "                ", "split_underscore", "(", "i", ".", "string", ",", "code_space_list", ",", "i", ".", "start_pos", "[", "0", "]", ",", "i", ".", "start_pos", "[", "1", "]", ")", "\n", "\n", "", "", "if", "len", "(", "token_list", ")", "!=", "0", ":", "\n", "#remove the tail", "\n", "            ", "while", "token_list", "[", "-", "1", "]", "==", "''", "or", "token_list", "[", "-", "1", "]", "==", "'<INDENT>'", ":", "\n", "                ", "token_list", "=", "token_list", "[", ":", "-", "1", "]", "\n", "if", "len", "(", "token_list", ")", "==", "0", ":", "return", "[", "\"\ud83d\ude1c\"", "]", "\n", "", "return", "token_list", "\n", "", "else", ":", "\n", "            ", "return", "[", "\"\ud83d\ude1c\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.get_vocab_from_file": [[374, 406], ["sorted", "sorted.sort", "open", "collections.Counter", "collections.Counter.items", "json.load", "print"], "methods", ["None"], ["", "", "def", "get_vocab_from_file", "(", "self", ",", "vocab_file", ",", "max_size", "=", "50000", ")", ":", "\n", "        ", "self", ".", "_word_to_id", "=", "{", "}", "\n", "self", ".", "_id_to_word", "=", "{", "}", "\n", "self", ".", "_count", "=", "0", "# keeps track of total number of words in the Vocab", "\n", "\n", "# [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.", "\n", "for", "w", "in", "[", "CODEBERT_UNKNOWN_TOKEN", ",", "CODEBERT_PAD_TOKEN", ",", "CODEBERT_START_DECODING", ",", "CODEBERT_STOP_DECODING", "]", ":", "\n", "            ", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "\n", "# Read the vocab file and add words up to max_size", "\n", "", "with", "open", "(", "vocab_file", ",", "'r'", ")", "as", "vocab_f", ":", "\n", "# the dict of counter", "\n", "            ", "counter", "=", "Counter", "(", "json", ".", "load", "(", "vocab_f", ")", ")", "\n", "", "for", "token", "in", "[", "CODEBERT_UNKNOWN_TOKEN", ",", "CODEBERT_PAD_TOKEN", ",", "CODEBERT_START_DECODING", ",", "CODEBERT_STOP_DECODING", "]", ":", "\n", "            ", "del", "counter", "[", "token", "]", "\n", "\n", "# alphabetical", "\n", "", "words_and_frequencies", "=", "sorted", "(", "counter", ".", "items", "(", ")", ",", "key", "=", "lambda", "tup", ":", "tup", "[", "0", "]", ")", "\n", "# list of (word, freq)", "\n", "words_and_frequencies", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "for", "w", ",", "freq", "in", "words_and_frequencies", ":", "\n", "            ", "self", ".", "_word_to_id", "[", "w", "]", "=", "self", ".", "_count", "\n", "self", ".", "_id_to_word", "[", "self", ".", "_count", "]", "=", "w", "\n", "self", ".", "_count", "+=", "1", "\n", "if", "max_size", ">", "0", "and", "self", ".", "_count", ">=", "max_size", ":", "\n", "                ", "print", "(", "\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\"", "%", "(", "max_size", ",", "self", ".", "_count", ")", ")", "\n", "break", "\n", "\n", "", "", "return", "self", ".", "_word_to_id", "", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.__init__": [[18, 55], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "transformer.PositionalEmbedding", "transformer.PositionalEmbedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "transformer.LayerNorm", "word_prob_layer.WordProbLayer", "label_smoothing.LabelSmoothing", "Model.Model.init_weights", "Model.Model.enc_layers.append", "Model.Model.dec_layers.append", "transformer.TransformerLayer", "transformer.TransformerLayer"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "self", ".", "copy", "=", "config", ".", "pointer_gen", "\n", "self", ".", "d_model", "=", "config", ".", "d_model", "#embeding size", "\n", "self", ".", "enc_dict_size", "=", "config", ".", "enc_vocab_size", "#vocab_size", "\n", "self", ".", "dec_dict_size", "=", "config", ".", "dec_vocab_size", "#vocab_size", "\n", "self", ".", "enc_pad_token_idx", "=", "0", "#pad id", "\n", "self", ".", "dec_pad_token_idx", "=", "1", "#pad id", "\n", "self", ".", "num_layers", "=", "config", ".", "n_layers", "#num layers", "\n", "self", ".", "d_ff", "=", "config", ".", "d_inner", "#d_inner", "\n", "self", ".", "num_heads", "=", "config", ".", "n_head", "#num heads", "\n", "self", ".", "dropout", "=", "config", ".", "dropout", "#dropout", "\n", "self", ".", "smoothing_factor", "=", "config", ".", "smoothing_factor", "#label_smoothing", "\n", "\n", "self", ".", "enc_tok_embed", "=", "nn", ".", "Embedding", "(", "self", ".", "enc_dict_size", ",", "self", ".", "d_model", ",", "self", ".", "enc_pad_token_idx", ")", "\n", "self", ".", "dec_tok_embed", "=", "nn", ".", "Embedding", "(", "self", ".", "dec_dict_size", ",", "self", ".", "d_model", ",", "self", ".", "dec_pad_token_idx", ")", "\n", "self", ".", "enc_pos_embed", "=", "PositionalEmbedding", "(", "self", ".", "d_model", ")", "\n", "self", ".", "dec_pos_embed", "=", "PositionalEmbedding", "(", "self", ".", "d_model", ")", "\n", "\n", "self", ".", "enc_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "self", ".", "enc_layers", ".", "append", "(", "TransformerLayer", "(", "self", ".", "d_model", ",", "self", ".", "d_ff", ",", "self", ".", "num_heads", ",", "self", ".", "dropout", ")", ")", "\n", "\n", "", "self", ".", "dec_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "self", ".", "dec_layers", ".", "append", "(", "TransformerLayer", "(", "self", ".", "d_model", ",", "self", ".", "d_ff", ",", "self", ".", "num_heads", ",", "self", ".", "dropout", ",", "with_external", "=", "True", ")", ")", "\n", "\n", "", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "self", ".", "d_model", ")", "\n", "\n", "self", ".", "word_prob", "=", "WordProbLayer", "(", "self", ".", "d_model", ",", "self", ".", "dec_dict_size", ",", "self", ".", "device", ",", "self", ".", "copy", ",", "self", ".", "dropout", ")", "\n", "\n", "self", ".", "smoothing", "=", "LabelSmoothing", "(", "self", ".", "device", ",", "self", ".", "dec_dict_size", ",", "self", ".", "dec_pad_token_idx", ",", "self", ".", "smoothing_factor", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.init_weights": [[56, 59], ["utils.init_uniform_weight", "utils.init_uniform_weight"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_uniform_weight", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_uniform_weight"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "init_uniform_weight", "(", "self", ".", "enc_tok_embed", ".", "weight", ")", "\n", "init_uniform_weight", "(", "self", ".", "dec_tok_embed", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_pad_mask": [[60, 62], ["None"], "methods", ["None"], ["", "def", "get_pad_mask", "(", "self", ",", "seq", ",", "pad_idx", ")", ":", "\n", "        ", "return", "(", "seq", "!=", "pad_idx", ")", "\n", "#return torch.cat([torch.gt(seq[:, 1],0).unsqueeze(1), torch.lt(seq[:, 1:], 0)], 1)", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_subsequent_mask": [[64, 70], ["seq.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "get_subsequent_mask", "(", "self", ",", "seq", ")", ":", "\n", "        ", "''' For masking out the subsequent info. '''", "\n", "sz_b", ",", "len_s", "=", "seq", ".", "size", "(", ")", "\n", "subsequent_mask", "=", "(", "1", "-", "torch", ".", "triu", "(", "\n", "torch", ".", "ones", "(", "(", "1", ",", "len_s", ",", "len_s", ")", ",", "device", "=", "seq", ".", "device", ")", ",", "diagonal", "=", "1", ")", ")", "#.bool()", "\n", "return", "subsequent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.cal_loss": [[71, 74], ["torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_().mean", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze().masked_fill_", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log().squeeze", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "y_pred.clamp", "y.unsqueeze"], "methods", ["None"], ["", "def", "cal_loss", "(", "self", ",", "y_pred", ",", "y", ")", ":", "\n", "        ", "loss", "=", "-", "torch", ".", "log", "(", "torch", ".", "gather", "(", "y_pred", ".", "clamp", "(", "min", "=", "1e-8", ")", ",", "-", "1", ",", "y", ".", "unsqueeze", "(", "-", "1", ")", ")", ")", ".", "squeeze", "(", "-", "1", ")", ".", "masked_fill_", "(", "(", "y", "==", "self", ".", "dec_pad_token_idx", ")", ",", "0.", ")", ".", "mean", "(", "-", "1", ")", ".", "mean", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.label_smotthing_loss": [[75, 80], ["y.size", "torch.log", "torch.log", "torch.log", "torch.log", "Model.Model.smoothing", "torch.log.clamp", "torch.log.view", "y.contiguous().view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "y.contiguous"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "label_smotthing_loss", "(", "self", ",", "y_pred", ",", "y", ",", "y_padding_mask", ")", ":", "\n", "        ", "bsz", ",", "seq_len", "=", "y", ".", "size", "(", ")", "\n", "y_pred", "=", "T", ".", "log", "(", "y_pred", ".", "clamp", "(", "min", "=", "1e-8", ")", ")", "\n", "loss", "=", "self", ".", "smoothing", "(", "y_pred", ".", "view", "(", "seq_len", "*", "bsz", ",", "-", "1", ")", ",", "y", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", "*", "bsz", ",", "-", "1", ")", ")", "\n", "return", "loss", "/", "T", ".", "sum", "(", "y_padding_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode": [[81, 86], ["enumerate", "Model.Model.enc_tok_embed", "Model.Model.enc_pos_embed", "layer"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "inp", ",", "mask_x", ")", ":", "\n", "        ", "x", "=", "self", ".", "enc_tok_embed", "(", "inp", ")", "+", "self", ".", "enc_pos_embed", "(", "inp", ")", "\n", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "enc_layers", ")", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "x", ",", "x", ",", "mask", "=", "mask_x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.decode": [[87, 100], ["enumerate", "Model.Model.dec_tok_embed", "Model.Model.dec_pos_embed", "layer", "Model.Model.word_prob", "Model.Model.word_prob"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "y_inp", ",", "mask_y", ",", "enc_output", ",", "mask_x", ",", "x_ext", "=", "None", ",", "max_ext_len", "=", "None", ")", ":", "\n", "        ", "dec_embedding", "=", "self", ".", "dec_tok_embed", "(", "y_inp", ")", "+", "self", ".", "dec_pos_embed", "(", "y_inp", ")", "\n", "dec_output", "=", "dec_embedding", "\n", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "dec_layers", ")", ":", "\n", "            ", "dec_output", "=", "layer", "(", "query", "=", "dec_output", ",", "key", "=", "enc_output", ",", "value", "=", "enc_output", ",", "mask", "=", "mask_y", ",", "src_mask", "=", "mask_x", ")", "\n", "\n", "", "if", "self", ".", "copy", ":", "\n", "#dec_output, dec_embedding, enc_output", "\n", "            ", "y_pred", "=", "self", ".", "word_prob", "(", "dec_output", ",", "dec_embedding", ",", "enc_output", ",", "mask_x", ",", "x_ext", ",", "max_ext_len", ")", "\n", "", "else", ":", "\n", "            ", "y_pred", "=", "self", ".", "word_prob", "(", "dec_output", ")", "\n", "\n", "", "return", "y_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.forward": [[101, 120], ["Model.Model.get_pad_mask", "Model.Model.get_subsequent_mask", "Model.Model.encode", "Model.Model.label_smotthing_loss", "Model.Model.decode", "Model.Model.decode", "Model.Model.get_pad_mask"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_pad_mask", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_subsequent_mask", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.label_smotthing_loss", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.get_pad_mask"], ["", "def", "forward", "(", "self", ",", "x", ",", "y_inp", ",", "y_tgt", ",", "x_ext", ",", "max_ext_len", ")", ":", "\n", "\n", "        ", "mask_x", "=", "self", ".", "get_pad_mask", "(", "x", ",", "self", ".", "enc_pad_token_idx", ")", "\n", "mask_y", "=", "self", ".", "get_subsequent_mask", "(", "y_inp", ")", "\n", "\n", "# print(\"***************** Encoder start *****************\")", "\n", "enc_output", "=", "self", ".", "encode", "(", "x", ",", "mask_x", ")", "\n", "# print(\"***************** Encoder end *****************\")", "\n", "\n", "# print(\"***************** Decoder start *****************\")", "\n", "if", "self", ".", "copy", ":", "\n", "            ", "y_pred", "=", "self", ".", "decode", "(", "y_inp", ",", "mask_y", ",", "enc_output", ",", "mask_x", ",", "x_ext", ",", "max_ext_len", ")", "\n", "", "else", ":", "\n", "            ", "y_pred", "=", "self", ".", "decode", "(", "y_inp", ",", "mask_y", ",", "enc_output", ",", "mask_x", ")", "\n", "# cost = self.cal_loss(y_pred, y_tgt)", "\n", "", "cost", "=", "self", ".", "label_smotthing_loss", "(", "y_pred", ",", "y_tgt", ",", "self", ".", "get_pad_mask", "(", "y_tgt", ",", "self", ".", "dec_pad_token_idx", ")", ")", "\n", "# print(\"***************** Decoder end *****************\")", "\n", "\n", "return", "y_pred", ",", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.word_prob_layer.WordProbLayer.__init__": [[13, 28], ["torch.Module.__init__", "transformer.MultiheadAttention", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "dict_size", ",", "device", ",", "copy", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "WordProbLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "dict_size", "=", "dict_size", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "copy", "=", "copy", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "if", "self", ".", "copy", ":", "\n", "            ", "self", ".", "external_attn", "=", "MultiheadAttention", "(", "self", ".", "hidden_size", ",", "1", ",", "self", ".", "dropout", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", "*", "3", ",", "self", ".", "dict_size", ")", "\n", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "self", ".", "hidden_size", "*", "3", ")", ")", "\n", "self", ".", "bv", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "hidden_size", ",", "self", ".", "dict_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.word_prob_layer.WordProbLayer.forward": [[29, 43], ["word_prob_layer.WordProbLayer.external_attn", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "xids.unsqueeze().repeat().transpose.unsqueeze().repeat().transpose.unsqueeze().repeat().transpose", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "word_prob_layer.WordProbLayer.proj", "torch.autograd.Variable().to", "torch.autograd.Variable().to", "torch.autograd.Variable().to", "torch.autograd.Variable().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "word_prob_layer.WordProbLayer.proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "xids.unsqueeze().repeat().transpose.unsqueeze().repeat().transpose.unsqueeze().repeat", "dists.squeeze", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.cat.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "xids.unsqueeze().repeat().transpose.unsqueeze().repeat().transpose.unsqueeze", "torch.cat.size", "torch.cat.size"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "", "def", "forward", "(", "self", ",", "dec_output", ",", "y_emb", "=", "None", ",", "memory", "=", "None", ",", "mask_x", "=", "None", ",", "xids", "=", "None", ",", "max_ext_len", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "copy", ":", "\n", "            ", "atts", ",", "dists", "=", "self", ".", "external_attn", "(", "query", "=", "dec_output", ",", "key", "=", "memory", ",", "value", "=", "memory", ",", "mask", "=", "mask_x", ",", "need_attn", "=", "True", ")", "\n", "pred", "=", "T", ".", "softmax", "(", "self", ".", "proj", "(", "T", ".", "cat", "(", "[", "dec_output", ",", "y_emb", ",", "atts", "]", ",", "-", "1", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "if", "max_ext_len", ">", "0", ":", "\n", "                ", "ext_zeros", "=", "Variable", "(", "torch", ".", "zeros", "(", "pred", ".", "size", "(", "0", ")", ",", "pred", ".", "size", "(", "1", ")", ",", "max_ext_len", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "pred", "=", "T", ".", "cat", "(", "(", "pred", ",", "ext_zeros", ")", ",", "-", "1", ")", "\n", "", "g", "=", "T", ".", "sigmoid", "(", "F", ".", "linear", "(", "T", ".", "cat", "(", "[", "dec_output", ",", "y_emb", ",", "atts", "]", ",", "-", "1", ")", ",", "self", ".", "v", ",", "self", ".", "bv", ")", ")", "\n", "xids", "=", "xids", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "pred", ".", "size", "(", "1", ")", ",", "1", ",", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "pred", "=", "(", "g", "*", "pred", ")", ".", "scatter_add", "(", "2", ",", "xids", ",", "(", "1", "-", "g", ")", "*", "dists", ".", "squeeze", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "pred", "=", "T", ".", "softmax", "(", "self", ".", "proj", "(", "dec_output", ")", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "pred", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.__init__": [[29, 33], ["open", "json.load", "run.Params.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.update"], ["    ", "def", "__init__", "(", "self", ",", "json_path", ")", ":", "\n", "        ", "with", "open", "(", "json_path", ")", "as", "f", ":", "\n", "            ", "params", "=", "json", ".", "load", "(", "f", ")", "\n", "self", ".", "__dict__", ".", "update", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save": [[34, 37], ["open", "json.dump"], "methods", ["None"], ["", "", "def", "save", "(", "self", ",", "json_path", ")", ":", "\n", "        ", "with", "open", "(", "json_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "__dict__", ",", "f", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.update": [[38, 43], ["open", "json.load", "run.Params.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.update"], ["", "", "def", "update", "(", "self", ",", "json_path", ")", ":", "\n", "        ", "\"\"\"Loads parameters from json file\"\"\"", "\n", "with", "open", "(", "json_path", ")", "as", "f", ":", "\n", "            ", "params", "=", "json", ".", "load", "(", "f", ")", "\n", "self", ".", "__dict__", ".", "update", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.add": [[44, 46], ["None"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.dict": [[47, 51], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']\"\"\"", "\n", "return", "self", ".", "__dict__", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.__init__": [[55, 63], ["Dataset.Vocab", "Dataset.Vocab", "params.add", "params.add", "wandb.config.update", "run.Procedure.src_vocab.size", "run.Procedure.trg_vocab.size"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.add", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.add", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.update", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "# data", "\n", "        ", "self", ".", "src_vocab", "=", "Vocab", "(", "params", ",", "mode", "=", "params", ".", "encoder_language", ")", "\n", "self", ".", "trg_vocab", "=", "Vocab", "(", "params", ",", "mode", "=", "\"decoder\"", ")", "\n", "params", ".", "add", "(", "\"enc_vocab_size\"", ",", "self", ".", "src_vocab", ".", "size", "(", ")", ")", "\n", "params", ".", "add", "(", "\"dec_vocab_size\"", ",", "self", ".", "trg_vocab", ".", "size", "(", ")", ")", "\n", "wandb", ".", "config", ".", "update", "(", "params", ".", "dict", ")", "\n", "self", ".", "params", "=", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.get_model_dir": [[64, 77], ["time.time", "os.path.join", "os.path.join", "os.path.join", "print", "run.Procedure.params.save", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save"], ["", "def", "get_model_dir", "(", "self", ")", ":", "\n", "#save path", "\n", "        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_root", ",", "'train_%d'", "%", "(", "cur_time", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "'model'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "param_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'params.json'", ")", "\n", "print", "(", "\"Dump hyper-parameters to {}.\"", ".", "format", "(", "param_path", ")", ")", "\n", "self", ".", "params", ".", "save", "(", "param_path", ")", "\n", "return", "self", ".", "model_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.get_save_path": [[78, 85], ["time.time", "os.path.join", "os.path.join", "prefix.format", "param_prefix.format"], "methods", ["None"], ["", "def", "get_save_path", "(", "self", ",", "iter", ")", ":", "\n", "        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n", "prefix", "=", "'model_{}_{}'", "\n", "param_prefix", "=", "'params_{}_{}'", "\n", "model_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "prefix", ".", "format", "(", "iter", ",", "cur_time", ")", ")", "\n", "param_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "param_prefix", ".", "format", "(", "iter", ",", "cur_time", ")", ")", "\n", "return", "model_save_path", ",", "param_save_path", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_data": [[86, 97], ["Dataset.Code4SQLDataset2PG", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "Dataset.Code4SQLDataset2PG", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "Dataset.Code4SQLDataset2PG", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "setup_data", "(", "self", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'train'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "True", ",", "drop_last", "=", "True", ")", "\n", "", "elif", "mode", "==", "'valid'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'valid'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "False", ",", "drop_last", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'test'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_train": [[98, 112], ["Model.Model.Model", "model.to.to.to", "wandb.watch", "ScheduledOptim.ScheduledOptim.ScheduledOptim", "torch.Adam", "torch.Adam", "torch.Adam", "model.to.to.parameters"], "methods", ["None"], ["", "def", "setup_train", "(", "self", ")", ":", "\n", "# model", "\n", "        ", "model", "=", "Model", "(", "self", ".", "params", ")", "\n", "# parallel", "\n", "#model = torch.nn.DataParallel(model, device_ids=[5, 1, 2]).to(self.params.device)", "\n", "model", "=", "model", ".", "to", "(", "self", ".", "params", ".", "device", ")", "\n", "wandb", ".", "watch", "(", "model", ")", "\n", "# optim", "\n", "# optimizer = optim.Adam(model.parameters(), lr=self.params.lr)", "\n", "#optimizer = optim.Adagrad(model.parameters(), lr=self.params.lr, initial_accumulator_value=0.1)", "\n", "optimizer", "=", "ScheduledOptim", "(", "\n", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "eps", "=", "1e-09", ",", "lr", "=", "self", ".", "params", ".", "lr", ")", ",", "\n", "0.1", ",", "self", ".", "params", ".", "d_model", ",", "self", ".", "params", ".", "n_warmup_steps", ")", "\n", "return", "model", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.train_epoch": [[113, 143], ["model.train", "tqdm.tqdm.tqdm", "optimizer.zero_grad", "model", "loss.backward", "optimizer.step", "run.Procedure.cal_bleu_rouge", "loss.item"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.cal_bleu_rouge"], ["", "def", "train_epoch", "(", "self", ",", "model", ",", "training_data", ",", "max_ext_len", ",", "optimizer", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Training)   '", "\n", "for", "batch", "in", "tqdm", "(", "training_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "src_oovs", "=", "[", "training_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#optim", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "training_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.eval_epoch": [[144, 172], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "tqdm.tqdm.tqdm", "model", "run.Procedure.cal_bleu_rouge", "loss.item"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.cal_bleu_rouge"], ["", "def", "eval_epoch", "(", "self", ",", "model", ",", "validation_data", ",", "max_ext_len", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Validation) '", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "tqdm", "(", "validation_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "                ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                    ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "src_oovs", "=", "[", "validation_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                    ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#print(\"xxxxxxxxxxx\",loss)", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "validation_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.cal_bleu_rouge": [[173, 200], ["range", "print", "print", "print", "print", "print", "utils.get_bleu4_score", "utils.get_func_correctness", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "pred_sentences.append", "pred_sentences.append", "utils.get_rouge_dict", "run.Procedure.trg_vocab.tokenizer.convert_tokens_to_string", "run.Procedure.trg_vocab.tokenizer.id2sentence", "Dataset.PGprocess.outputids2words", "pred_batch.cpu().numpy().tolist", "pred_batch.cpu().numpy().tolist", "pred_batch.cpu().numpy", "pred_batch.cpu().numpy", "pred_batch.cpu", "pred_batch.cpu"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_bleu4_score", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_rouge_dict", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.convert_tokens_to_string", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.id2sentence", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.PGprocess.outputids2words"], ["", "", "def", "cal_bleu_rouge", "(", "self", ",", "pred", ",", "original_trg_batch", ",", "src_oovs", "=", "None", ")", ":", "\n", "# print(\"pred: \\n\", pred)", "\n", "# print(\"pred: \\n\", pred.shape)", "\n", "        ", "pred_batch", "=", "torch", ".", "max", "(", "pred", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "\n", "# print(\"pred_batch: \\n\", pred_batch)", "\n", "pred_sentences", "=", "[", "]", "\n", "gold_sentences", "=", "original_trg_batch", "\n", "for", "i", "in", "range", "(", "pred_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "PGprocess", ".", "outputids2words", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ",", "self", ".", "vocab", ",", "src_oovs", "[", "i", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "id2sentence", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ")", ")", "\n", "\n", "", "", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Pred_sentences example: \\n\"", ",", "pred_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Gold_sentences example: \\n\"", ",", "gold_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "\n", "#rouge l", "\n", "r_score", "=", "get_rouge_dict", "(", "pred_sentences", ",", "gold_sentences", ")", "[", "\"rouge-l\"", "]", "[", "'f'", "]", "\n", "#bleu 4", "\n", "b_score", "=", "get_bleu4_score", "(", "pred_sentences", ",", "gold_sentences", ",", "self", ".", "trg_vocab", ".", "tokenizer", ")", "\n", "#func_correctness", "\n", "func_corr", "=", "get_func_correctness", "(", "pred_sentences", ",", "gold_sentences", ")", "\n", "\n", "return", "r_score", ",", "b_score", ",", "func_corr", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.__init__": [[204, 208], ["run.Params", "run.Procedure"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "param_path", "=", "None", ")", ":", "\n", "        ", "if", "param_path", "!=", "None", ":", "\n", "            ", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.train": [[209, 262], ["run.Main.base.get_model_dir", "run.Main.params.add", "wandb.config.update", "run.Main.base.setup_data", "run.Main.base.setup_data", "run.Main.base.setup_train", "range", "print", "run.Main.base.train_epoch", "run.Main.base.eval_epoch", "wandb.log", "model.state_dict", "min", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "max", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "max", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "max", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.get_model_dir", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.add", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.update", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_data", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_data", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.train_epoch", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.eval_epoch", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save"], ["", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "model_dir", "=", "self", ".", "base", ".", "get_model_dir", "(", ")", "\n", "self", ".", "params", ".", "add", "(", "\"model_dir\"", ",", "self", ".", "model_dir", ")", "\n", "wandb", ".", "config", ".", "update", "(", "self", ".", "params", ".", "dict", ")", "\n", "training_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'train'", ")", "\n", "validation_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'valid'", ")", "\n", "model", ",", "optimizer", "=", "self", ".", "base", ".", "setup_train", "(", ")", "\n", "params", "=", "self", ".", "params", "\n", "valid_losses", "=", "[", "]", "\n", "valid_rouges", "=", "[", "]", "\n", "valid_bleus", "=", "[", "]", "\n", "valid_fun_corrs", "=", "[", "]", "\n", "for", "epoch_i", "in", "range", "(", "params", ".", "epoch", ")", ":", "\n", "            ", "print", "(", "'[ Epoch'", ",", "epoch_i", "+", "1", ",", "']'", ")", "\n", "# train", "\n", "train_loss", ",", "train_rouge", ",", "train_bleu", ",", "train_func_corr", "=", "self", ".", "base", ".", "train_epoch", "(", "model", ",", "training_data", ",", "training_data", ".", "dataset", ".", "max_src_oovs", ",", "optimizer", ")", "\n", "# eval", "\n", "valid_loss", ",", "valid_rouge", ",", "valid_bleu", ",", "valid_func_corr", "=", "self", ".", "base", ".", "eval_epoch", "(", "model", ",", "validation_data", ",", "validation_data", ".", "dataset", ".", "max_src_oovs", ")", "\n", "#print(\"***************\",valid_loss)", "\n", "# log", "\n", "wandb", ".", "log", "(", "{", "\"Training loss: \"", ":", "train_loss", ",", "\n", "\"Training rouge: \"", ":", "train_rouge", ",", "\n", "\"Training bleu: \"", ":", "train_bleu", ",", "\n", "\"Training func_corr\"", ":", "train_func_corr", ",", "\n", "\"Validation loss: \"", ":", "valid_loss", ",", "\n", "\"Validation rouge: \"", ":", "valid_rouge", ",", "\n", "\"Validation bleu: \"", ":", "valid_bleu", ",", "\n", "\"Validation func_corr: \"", ":", "valid_func_corr", "}", ")", "\n", "\n", "checkpoint", "=", "{", "'epoch'", ":", "epoch_i", ",", "'settings'", ":", "params", ",", "'model'", ":", "model", ".", "state_dict", "(", ")", "}", "\n", "\n", "if", "params", ".", "save_mode", ":", "\n", "                ", "valid_losses", "+=", "[", "valid_loss", "]", "\n", "valid_rouges", "+=", "[", "valid_rouge", "]", "\n", "valid_bleus", "+=", "[", "valid_bleu", "]", "\n", "valid_fun_corrs", "+=", "[", "valid_func_corr", "]", "\n", "if", "params", ".", "save_mode", "==", "'best'", ":", "\n", "                    ", "model_name", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_loss_model.chkpt'", "\n", "if", "valid_loss", "<=", "min", "(", "valid_losses", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by loss.'", ")", "\n", "", "model_name_rouge", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_rouge_model.chkpt'", "\n", "if", "valid_rouge", ">=", "max", "(", "valid_rouges", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_rouge", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by rouge.'", ")", "\n", "", "model_name_bleu", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_bleu_model.chkpt'", "\n", "if", "valid_bleu", ">=", "max", "(", "valid_bleus", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_bleu", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n", "", "model_name_func_corr", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_func_corr_model.chkpt'", "\n", "if", "valid_func_corr", ">=", "max", "(", "valid_fun_corrs", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_func_corr", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.basic_decode": [[263, 269], ["run.Procedure", "run.Main.base.setup_data", "beamsearch.Search", "beamsearch.Search.decode", "print"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Procedure.setup_data", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode"], ["", "", "", "", "", "def", "basic_decode", "(", "self", ",", "best_model_on_validation", ",", "data_file_prefix", ")", ":", "\n", "        ", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "test_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "data_file_prefix", ")", "\n", "decode_processor", "=", "Search", "(", "self", ".", "params", ",", "best_model_on_validation", ",", "test_data", ",", "data_file_prefix", ")", "\n", "result_dict", "=", "decode_processor", ".", "decode", "(", ")", "\n", "print", "(", "result_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode": [[270, 286], ["model_path.split", "os.path.join", "run.Params", "os.path.join", "run.Params", "os.path.join", "run.Main.basic_decode", "os.path.join", "run.Main.basic_decode"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.basic_decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.basic_decode"], ["", "def", "decode", "(", "self", ",", "model_path", ",", "data_file_prefix", "=", "\"test\"", ")", ":", "\n", "        ", "if", "\";\"", "in", "model_path", ":", "\n", "            ", "for", "path", "in", "model_path", ".", "split", "(", "\";\"", ")", ":", "\n", "                ", "param_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                    ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "", "", "", "else", ":", "\n", "            ", "param_path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.TransformerLayer.__init__": [[9, 21], ["torch.nn.Module.__init__", "transformer.MultiheadAttention", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "transformer.LayerNorm", "transformer.LayerNorm", "transformer.MultiheadAttention", "transformer.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embed_dim", ",", "ff_embed_dim", ",", "num_heads", ",", "dropout", ",", "with_external", "=", "False", ")", ":", "\n", "        ", "super", "(", "TransformerLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "embed_dim", ",", "num_heads", ",", "dropout", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "ff_embed_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ff_embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "attn_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "self", ".", "ff_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "self", ".", "with_external", "=", "with_external", "\n", "self", ".", "dropout", "=", "dropout", "\n", "if", "self", ".", "with_external", ":", "\n", "            ", "self", ".", "external_attn", "=", "MultiheadAttention", "(", "embed_dim", ",", "num_heads", ",", "dropout", ")", "\n", "self", ".", "external_layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.TransformerLayer.forward": [[22, 49], ["transformer.TransformerLayer.self_attn", "torch.dropout", "torch.dropout", "transformer.TransformerLayer.attn_layer_norm", "transformer.TransformerLayer.fc1", "torch.dropout", "torch.dropout", "transformer.gelu", "torch.dropout", "torch.dropout", "transformer.TransformerLayer.ff_layer_norm", "transformer.TransformerLayer.external_attn", "torch.dropout", "torch.dropout", "transformer.TransformerLayer.attn_layer_norm", "transformer.TransformerLayer.fc2"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.gelu"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "mask", "=", "None", ",", "src_mask", "=", "None", ")", ":", "\n", "# query: bsz x seq_len x embed_dim", "\n", "        ", "residual", "=", "query", "\n", "\n", "#encoder and decoder self attention", "\n", "x", "=", "self", ".", "self_attn", "(", "query", ",", "query", ",", "query", ",", "mask", ")", "\n", "#Add and Norm for attention", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ")", "\n", "x", "=", "self", ".", "attn_layer_norm", "(", "residual", "+", "x", ")", "\n", "\n", "#encoder-decoder attention", "\n", "if", "self", ".", "with_external", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "external_attn", "(", "x", ",", "key", ",", "value", ",", "src_mask", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ")", "\n", "x", "=", "self", ".", "attn_layer_norm", "(", "residual", "+", "x", ")", "\n", "\n", "#feed forward", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ")", "\n", "x", "=", "gelu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ")", "\n", "x", "=", "self", ".", "ff_layer_norm", "(", "residual", "+", "x", ")", "\n", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.MultiheadAttention.__init__": [[56, 69], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "transformer.Attention", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "h", ",", "dropout", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "d_model", "%", "h", "==", "0", "\n", "\n", "# We assume d_v always equals d_k", "\n", "self", ".", "d_k", "=", "d_model", "//", "h", "\n", "self", ".", "h", "=", "h", "\n", "\n", "self", ".", "linear_layers", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ")", "for", "_", "in", "range", "(", "3", ")", "]", ")", "\n", "self", ".", "output_linear", "=", "nn", ".", "Linear", "(", "d_model", ",", "d_model", ")", "\n", "self", ".", "attention", "=", "Attention", "(", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.MultiheadAttention.forward": [[70, 86], ["query.size", "transformer.MultiheadAttention.attention", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "l().view().transpose", "transformer.MultiheadAttention.output_linear", "zip", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "transformer.MultiheadAttention.output_linear", "l().view", "x.transpose().contiguous().view.transpose().contiguous().view.transpose", "l"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "mask", "=", "None", ",", "need_attn", "=", "False", ")", ":", "\n", "        ", "batch_size", "=", "query", ".", "size", "(", "0", ")", "\n", "\n", "# 1) Do all the linear projections in batch from d_model => h x d_k", "\n", "query", ",", "key", ",", "value", "=", "[", "l", "(", "x", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "self", ".", "h", ",", "self", ".", "d_k", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "l", ",", "x", "in", "zip", "(", "self", ".", "linear_layers", ",", "(", "query", ",", "key", ",", "value", ")", ")", "]", "\n", "\n", "# 2) Apply attention on all the projected vectors in batch.", "\n", "x", ",", "attn", "=", "self", ".", "attention", "(", "query", ",", "key", ",", "value", ",", "mask", "=", "mask", ",", "dropout", "=", "self", ".", "dropout", ")", "\n", "\n", "# 3) \"Concat\" using a view and apply a final linear.", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "self", ".", "h", "*", "self", ".", "d_k", ")", "\n", "if", "need_attn", ":", "\n", "            ", "return", "self", ".", "output_linear", "(", "x", ")", ",", "attn", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "output_linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.Attention.forward": [[92, 114], ["torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "scores.masked_fill.masked_fill.masked_fill", "dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "key.transpose", "query.size", "len", "mask.unsqueeze.unsqueeze.unsqueeze().repeat().unsqueeze", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "list", "len", "mask.unsqueeze.unsqueeze.unsqueeze", "mask.unsqueeze.unsqueeze.size", "mask.unsqueeze.unsqueeze.unsqueeze().repeat", "list", "query.size", "mask.unsqueeze.unsqueeze.size", "mask.unsqueeze.unsqueeze.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "mask", "=", "None", ",", "dropout", "=", "None", ")", ":", "\n", "        ", "scores", "=", "torch", ".", "matmul", "(", "query", ",", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "/", "math", ".", "sqrt", "(", "query", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "#src mask", "\n", "            ", "if", "len", "(", "list", "(", "mask", ".", "size", "(", ")", ")", ")", "==", "2", ":", "\n", "                ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "query", ".", "size", "(", "2", ")", ",", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "#trg mask", "\n", "", "elif", "len", "(", "list", "(", "mask", ".", "size", "(", ")", ")", ")", "==", "3", ":", "\n", "                ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", "\n", "#print(mask.shape)", "\n", "", "scores", "=", "scores", ".", "masked_fill", "(", "mask", "==", "0", ",", "-", "1e9", ")", "\n", "\n", "", "p_attn", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "            ", "attn_value", "=", "dropout", "(", "torch", ".", "matmul", "(", "p_attn", ",", "value", ")", ")", "\n", "", "else", ":", "\n", "            ", "attn_value", "=", "torch", ".", "matmul", "(", "p_attn", ",", "value", ")", "\n", "\n", "", "return", "attn_value", ",", "p_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.LayerNorm.__init__": [[122, 127], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "features", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", "LayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "a_2", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "features", ")", ")", "\n", "self", ".", "b_2", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "features", ")", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.LayerNorm.forward": [[128, 132], ["x.mean", "x.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "a_2", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "b_2", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.PositionalEmbedding.__init__": [[135, 150], ["torch.nn.Module.__init__", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "pe.unsqueeze.unsqueeze.unsqueeze", "transformer.PositionalEmbedding.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "max_len", "=", "1024", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Compute the positional encodings once in log space.", "\n", "pe", "=", "torch", ".", "zeros", "(", "max_len", ",", "d_model", ")", ".", "float", "(", ")", "\n", "pe", ".", "require_grad", "=", "False", "\n", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "(", "torch", ".", "arange", "(", "0", ",", "d_model", ",", "2", ")", ".", "float", "(", ")", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "d_model", ")", ")", ".", "exp", "(", ")", "\n", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "register_buffer", "(", "'pe'", ",", "pe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.PositionalEmbedding.forward": [[151, 153], ["x.size"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "-", "1", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.transformer.gelu": [[116, 118], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_seeds": [[18, 28], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.is_available", "str", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "init_seeds", "(", ")", ":", "\n", "    ", "random", ".", "seed", "(", "123", ")", "\n", "np", ".", "random", ".", "seed", "(", "123", ")", "\n", "torch", ".", "manual_seed", "(", "123", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "123", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "123", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "123", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_lstm_weight": [[29, 35], ["lstm.parameters", "len", "utils.init_ortho_weight", "utils.init_bias"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_ortho_weight", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_bias"], ["", "def", "init_lstm_weight", "(", "lstm", ")", ":", "\n", "    ", "for", "param", "in", "lstm", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_gru_weight": [[36, 42], ["gru.parameters", "len", "utils.init_ortho_weight", "utils.init_bias"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_ortho_weight", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_bias"], ["", "", "", "def", "init_gru_weight", "(", "gru", ")", ":", "\n", "    ", "for", "param", "in", "gru", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_linear_weight": [[43, 47], ["utils.init_xavier_weight", "utils.init_bias"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_xavier_weight", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_bias"], ["", "", "", "def", "init_linear_weight", "(", "linear", ")", ":", "\n", "    ", "init_xavier_weight", "(", "linear", ".", "weight", ")", "\n", "if", "linear", ".", "bias", "is", "not", "None", ":", "\n", "        ", "init_bias", "(", "linear", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_normal_weight": [[48, 50], ["torch.nn.init.normal_"], "function", ["None"], ["", "", "def", "init_normal_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "normal_", "(", "w", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_uniform_weight": [[51, 53], ["torch.nn.init.uniform_"], "function", ["None"], ["", "def", "init_uniform_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "uniform_", "(", "w", ",", "-", "0.1", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_ortho_weight": [[54, 56], ["torch.nn.init.orthogonal_"], "function", ["None"], ["", "def", "init_ortho_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "orthogonal_", "(", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_xavier_weight": [[57, 59], ["torch.nn.init.xavier_normal_"], "function", ["None"], ["", "def", "init_xavier_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "xavier_normal_", "(", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.init_bias": [[60, 62], ["torch.nn.init.constant_"], "function", ["None"], ["", "def", "init_bias", "(", "b", ")", ":", "\n", "    ", "nn", ".", "init", ".", "constant_", "(", "b", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.get_bleu4_score": [[65, 72], ["nltk.translate.bleu_score.corpus_bleu", "tokenizer.tokenize", "tokenizer.tokenize", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["", "def", "get_bleu4_score", "(", "hyps_list", ",", "gold_list", ",", "tokenizer", ")", ":", "\n", "    ", "b_score", "=", "corpus_bleu", "(", "\n", "[", "[", "tokenizer", ".", "tokenize", "(", "ref", ")", "]", "for", "ref", "in", "gold_list", "]", ",", "\n", "[", "tokenizer", ".", "tokenize", "(", "pred", ")", "for", "pred", "in", "hyps_list", "]", ",", "\n", "smoothing_function", "=", "bleu_score", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", "weights", "=", "(", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", ")", ")", "\n", "return", "b_score", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.get_rouge_dict": [[73, 77], ["rouge.Rouge", "rouge.Rouge.get_scores"], "function", ["None"], ["", "def", "get_rouge_dict", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "    ", "rouge", "=", "Rouge", "(", ")", "\n", "result_dict", "=", "rouge", ".", "get_scores", "(", "hyps_list", ",", "gold_list", ",", "avg", "=", "True", ")", "\n", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.get_var_replacing": [[78, 114], ["parso.utils.parse_version_string", "parso.python.tokenize.tokenize", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "token_list.append", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "str", "re.findall", "utils.sqlparse", "re.findall", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.sqlparse"], ["", "def", "get_var_replacing", "(", "code_string", ",", "repalce_string", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "var_dict", "=", "{", "}", "\n", "token_list", "=", "[", "]", "\n", "var_index", "=", "0", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "code_string", ",", "version_info", ")", ":", "\n", "        ", "if", "not", "repalce_string", ":", "\n", "# print(i)", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", ":", "\n", "                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "elif", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "i", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "i", ".", "string", "\n", "", "token_list", ".", "append", "(", "sql_parsed", ")", "\n", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", "or", "(", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ")", ":", "\n", "                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "", "return", "token_list", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.get_func_correctness": [[116, 133], ["zip", "print", "print", "len", "len", "utils.get_var_replacing", "utils.get_var_replacing", "index_list.append", "len", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing"], ["", "def", "get_func_correctness", "(", "hyps_list", ",", "gold_list", ",", "repalce_string", "=", "False", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "ast_match_num", "=", "0", "\n", "index", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "i", ":", "\n", "            ", "i", ",", "j", "=", "get_var_replacing", "(", "i", ",", "repalce_string", ")", ",", "get_var_replacing", "(", "j", ",", "repalce_string", ")", "\n", "if", "i", "==", "j", ":", "\n", "                ", "ast_match_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "", "index", "+=", "1", "\n", "", "print", "(", "\"Number of AST matching\"", ",", "ast_match_num", ")", "\n", "print", "(", "\"Accuration of AST matching\"", ",", "ast_match_num", "/", "len", "(", "hyps_list", ")", ")", "\n", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.embedded_ast_matching": [[134, 160], ["parso.utils.parse_version_string", "zip", "parso.python.tokenize.tokenize", "index_list.append", "len", "sqls.append", "utils.embedded_ast_matching.get_sql_parser"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["", "", "def", "embedded_ast_matching", "(", "hyps_list", ",", "gold_list", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "def", "get_sql_parser", "(", "s", ")", ":", "\n", "        ", "sqls", "=", "[", "]", "\n", "for", "t", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "            ", "if", "t", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "t", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "t", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "t", ".", "string", "\n", "", "sqls", ".", "append", "(", "sql_parsed", ")", "\n", "", "", "return", "sqls", "\n", "", "m_num", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "index", "=", "0", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "get_sql_parser", "(", "i", ")", "!=", "[", "]", "and", "get_sql_parser", "(", "i", ")", "==", "get_sql_parser", "(", "j", ")", ":", "\n", "# print(i, j)", "\n", "# print(\"---------------\")", "\n", "            ", "m_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "index", "+=", "1", "\n", "", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.code_staticAnaylsis": [[161, 173], ["str().replace", "pylint.epylint.py_run", "os.remove", "pylint_stdout.read", "open", "f.write", "f.write", "str", "time.time", "str", "str", "str"], "function", ["None"], ["", "", "def", "code_staticAnaylsis", "(", "code", ",", "id", ")", ":", "\n", "    ", "cur_time", "=", "str", "(", "time", ".", "time", "(", ")", ")", ".", "replace", "(", "\".\"", ",", "\"\"", ")", "\n", "with", "open", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"# pylint: disable=E1101\\n\"", ")", "\n", "f", ".", "write", "(", "code", ")", "\n", "", "(", "pylint_stdout", ",", "pylint_stderr", ")", "=", "lint", ".", "py_run", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py -s yes\"", ",", "return_std", "=", "True", ")", "\n", "os", ".", "remove", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ")", "\n", "pylint_stdout_str", "=", "pylint_stdout", ".", "read", "(", ")", "\n", "# pylint_stderr_str = pylint_stderr.read()", "\n", "if", "\"E0\"", "in", "pylint_stdout_str", "or", "\"E1\"", "in", "pylint_stdout_str", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.get_executable_rate": [[174, 183], ["range", "len", "len", "utils.code_staticAnaylsis", "len", "hyps_list[].replace"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.code_staticAnaylsis"], ["", "def", "get_executable_rate", "(", "hyps_list", ")", ":", "\n", "    ", "executable_wrong_num", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "hyps_list", "[", "i", "]", ":", "\n", "            ", "if", "code_staticAnaylsis", "(", "hyps_list", "[", "i", "]", ".", "replace", "(", "\"\\t\"", ",", "\"    \"", ")", ",", "i", ")", ":", "\n", "                ", "executable_wrong_num", "+=", "1", "\n", "", "", "else", ":", "\n", "            ", "executable_wrong_num", "+=", "1", "\n", "", "", "return", "(", "len", "(", "hyps_list", ")", "-", "executable_wrong_num", ")", "/", "len", "(", "hyps_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.utils.sqlparse": [[184, 287], ["s.replace().replace.replace().replace", "re.findall", "re.findall", "re.findall", "s.replace().replace.index", "s.replace().replace.index", "parso.python.tokenize.tokenize", "s.replace().replace.index", "len", "range", "s.replace().replace.replace", "len", "len", "query_tokens.append", "len", "len", "cond_list.append", "temp.append", "op_list.append", "col_list.append", "col_list.append", "from_toknes.append", "where_tokens.append", "temp.append", "cond_list.append", "dict", "len", "len", "col_list.append", "col_list.append", "col_list.append", "temp.append", "len", "len", "col_list.append", "len", "temp.append", "temp.append"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.dict"], ["", "def", "sqlparse", "(", "s", ")", ":", "\n", "    ", "s", "=", "s", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ".", "replace", "(", "\"\\\\\"", ",", "\" \"", ")", "\n", "# split index", "\n", "se", "=", "re", ".", "findall", "(", "r\"(SELECT )|(select )\"", ",", "s", ")", "\n", "fr", "=", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "s", ")", "\n", "wh", "=", "re", ".", "findall", "(", "r\"( WHERE )|( where )\"", ",", "s", ")", "\n", "try", ":", "\n", "        ", "assert", "len", "(", "se", ")", "==", "len", "(", "fr", ")", "==", "1", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "", "s_index", "=", "s", ".", "index", "(", "se", "[", "0", "]", "[", "0", "]", "if", "se", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "se", "[", "0", "]", "[", "1", "]", ")", "\n", "f_index", "=", "s", ".", "index", "(", "fr", "[", "0", "]", "[", "0", "]", "if", "fr", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "fr", "[", "0", "]", "[", "1", "]", ")", "\n", "if", "wh", "!=", "[", "]", ":", "\n", "        ", "w_index", "=", "s", ".", "index", "(", "wh", "[", "0", "]", "[", "0", "]", "if", "wh", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "wh", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "w_index", "=", "9999", "\n", "\n", "# split tokens", "\n", "", "query_tokens", "=", "[", "]", "\n", "from_toknes", "=", "[", "]", "\n", "where_tokens", "=", "[", "]", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "# print(i)", "\n", "        ", "if", "i", ".", "type", "in", "[", "tokenize", ".", "INDENT", ",", "tokenize", ".", "DEDENT", ",", "tokenize", ".", "ENDMARKER", "]", ":", "\n", "            ", "continue", "\n", "", "if", "i", ".", "start_pos", "[", "1", "]", "<", "f_index", ":", "\n", "            ", "query_tokens", ".", "append", "(", "i", ")", "\n", "", "elif", "i", ".", "start_pos", "[", "1", "]", "<", "w_index", ":", "\n", "            ", "from_toknes", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "where_tokens", ".", "append", "(", "i", ")", "\n", "\n", "# condition list", "\n", "", "", "if", "len", "(", "where_tokens", ")", "!=", "0", ":", "\n", "#where_list = [where_tokens[0].string]", "\n", "        ", "where_list", "=", "[", "]", "\n", "temp", "=", "[", "]", "\n", "temp_op", "=", "''", "\n", "for", "i", "in", "where_tokens", "[", "1", ":", "]", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "==", "','", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "", "elif", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "!=", "':'", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "i", ".", "string", "\n", "", "elif", "i", ".", "type", "!=", "tokenize", ".", "OP", ":", "\n", "                ", "if", "temp_op", "!=", "''", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"PARAMETER\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "''", "\n", "", "else", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"COLUMN\"", ":", "i", ".", "string", "}", ")", "\n", "", "", "", "cond", "=", "{", "}", "\n", "cond_list", "=", "[", "]", "\n", "# print(\"temp: \",temp)", "\n", "for", "i", "in", "range", "(", "len", "(", "temp", ")", ")", ":", "\n", "# print(temp[i])", "\n", "            ", "if", "(", "i", "+", "1", ")", "%", "4", "==", "0", ":", "\n", "                ", "cond_list", ".", "append", "(", "cond", ")", "\n", "# cond_list.append(temp[i])", "\n", "cond", "=", "{", "}", "\n", "", "else", ":", "\n", "                ", "cond", "=", "dict", "(", "cond", ",", "**", "temp", "[", "i", "]", ")", "\n", "", "", "if", "len", "(", "cond", ")", "!=", "0", ":", "\n", "            ", "cond_list", ".", "append", "(", "cond", ")", "\n", "# where_list.append(cond_list)", "\n", "", "where_list", "=", "{", "\"CONDITION\"", ":", "cond_list", "}", "\n", "", "else", ":", "\n", "        ", "where_list", "=", "[", "]", "\n", "\n", "# from list", "\n", "# from_lsit = [from_toknes[0].string, [from_toknes[1].string, where_list]]", "\n", "", "from_lsit", "=", "{", "\"TABLE\"", ":", "from_toknes", "[", "1", "]", ".", "string", "}", "\n", "\n", "# query list", "\n", "# query_list = [query_tokens[0].string]", "\n", "query_list", "=", "[", "]", "\n", "temp", "=", "[", "]", "\n", "op_list", "=", "[", "]", "\n", "col_list", "=", "[", "]", "\n", "for", "i", "in", "query_tokens", "[", "1", ":", "]", ":", "\n", "        ", "if", "i", ".", "type", "!=", "tokenize", ".", "OP", ":", "\n", "            ", "temp", ".", "append", "(", "i", ".", "string", ")", "\n", "", "else", ":", "\n", "            ", "op_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "if", "','", "in", "op_list", ":", "\n", "        ", "for", "i", "in", "temp", ":", "\n", "            ", "col_list", ".", "append", "(", "{", "\"COLUMN\"", ":", "i", "}", ")", "\n", "", "", "elif", "\"(\"", "in", "op_list", "and", "\")\"", "in", "op_list", "and", "len", "(", "temp", ")", "<=", "2", ":", "\n", "        ", "col_list", ".", "append", "(", "{", "\"AGG\"", ":", "temp", "[", "0", "]", "}", ")", "\n", "if", "len", "(", "temp", ")", "==", "1", ":", "\n", "            ", "col_list", ".", "append", "(", "{", "\"COLUMN\"", ":", "op_list", "[", "1", "]", "}", ")", "\n", "", "else", ":", "\n", "            ", "col_list", ".", "append", "(", "{", "\"COLUMN\"", ":", "temp", "[", "1", "]", "}", ")", "\n", "", "", "elif", "len", "(", "op_list", ")", "==", "0", "and", "len", "(", "temp", ")", "==", "1", ":", "\n", "        ", "col_list", ".", "append", "(", "{", "\"COLUMN\"", ":", "temp", "[", "0", "]", "}", ")", "\n", "", "elif", "op_list", "[", "0", "]", "==", "\"*\"", "and", "len", "(", "temp", ")", "==", "0", ":", "\n", "        ", "col_list", ".", "append", "(", "{", "\"COLUMN\"", ":", "op_list", "[", "0", "]", "}", ")", "\n", "", "else", ":", "\n", "# print(col_list, temp)", "\n", "        ", "return", "\"Error\"", "\n", "\n", "", "query_list", "=", "[", "{", "\"COLUMNS\"", ":", "col_list", "}", ",", "from_lsit", ",", "where_list", "]", "\n", "\n", "return", "query_list", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.bleu._get_ngrams": [[28, 46], ["collections.Counter", "range", "range", "tuple", "len"], "function", ["None"], ["def", "_get_ngrams", "(", "segment", ",", "max_order", ")", ":", "\n", "  ", "\"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"", "\n", "ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "segment", ")", "-", "order", "+", "1", ")", ":", "\n", "      ", "ngram", "=", "tuple", "(", "segment", "[", "i", ":", "i", "+", "order", "]", ")", "\n", "ngram_counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "ngram_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.bleu.compute_bleu": [[48, 113], ["zip", "range", "min", "len", "collections.Counter", "bleu._get_ngrams", "range", "min", "sum", "math.exp", "float", "math.exp", "bleu._get_ngrams", "len", "len", "float", "math.log", "len"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._get_ngrams", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._get_ngrams"], ["", "def", "compute_bleu", "(", "reference_corpus", ",", "translation_corpus", ",", "max_order", "=", "4", ",", "\n", "smooth", "=", "False", ")", ":", "\n", "  ", "\"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"", "\n", "matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "possible_matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "reference_length", "=", "0", "\n", "translation_length", "=", "0", "\n", "for", "(", "references", ",", "translation", ")", "in", "zip", "(", "reference_corpus", ",", "\n", "translation_corpus", ")", ":", "\n", "    ", "reference_length", "+=", "min", "(", "len", "(", "r", ")", "for", "r", "in", "references", ")", "\n", "translation_length", "+=", "len", "(", "translation", ")", "\n", "\n", "merged_ref_ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "merged_ref_ngram_counts", "|=", "_get_ngrams", "(", "reference", ",", "max_order", ")", "\n", "", "translation_ngram_counts", "=", "_get_ngrams", "(", "translation", ",", "max_order", ")", "\n", "overlap", "=", "translation_ngram_counts", "&", "merged_ref_ngram_counts", "\n", "for", "ngram", "in", "overlap", ":", "\n", "      ", "matches_by_order", "[", "len", "(", "ngram", ")", "-", "1", "]", "+=", "overlap", "[", "ngram", "]", "\n", "", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "      ", "possible_matches", "=", "len", "(", "translation", ")", "-", "order", "+", "1", "\n", "if", "possible_matches", ">", "0", ":", "\n", "        ", "possible_matches_by_order", "[", "order", "-", "1", "]", "+=", "possible_matches", "\n", "\n", "", "", "", "precisions", "=", "[", "0", "]", "*", "max_order", "\n", "for", "i", "in", "range", "(", "0", ",", "max_order", ")", ":", "\n", "    ", "if", "smooth", ":", "\n", "      ", "precisions", "[", "i", "]", "=", "(", "(", "matches_by_order", "[", "i", "]", "+", "1.", ")", "/", "\n", "(", "possible_matches_by_order", "[", "i", "]", "+", "1.", ")", ")", "\n", "", "else", ":", "\n", "      ", "if", "possible_matches_by_order", "[", "i", "]", ">", "0", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "(", "float", "(", "matches_by_order", "[", "i", "]", ")", "/", "\n", "possible_matches_by_order", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "0.0", "\n", "\n", "", "", "", "if", "min", "(", "precisions", ")", ">", "0", ":", "\n", "    ", "p_log_sum", "=", "sum", "(", "(", "1.", "/", "max_order", ")", "*", "math", ".", "log", "(", "p", ")", "for", "p", "in", "precisions", ")", "\n", "geo_mean", "=", "math", ".", "exp", "(", "p_log_sum", ")", "\n", "", "else", ":", "\n", "    ", "geo_mean", "=", "0", "\n", "\n", "", "ratio", "=", "float", "(", "translation_length", ")", "/", "reference_length", "\n", "\n", "if", "ratio", ">", "1.0", ":", "\n", "    ", "bp", "=", "1.", "\n", "", "else", ":", "\n", "    ", "bp", "=", "math", ".", "exp", "(", "1", "-", "1.", "/", "ratio", ")", "\n", "\n", "", "bleu", "=", "geo_mean", "*", "bp", "\n", "\n", "return", "(", "bleu", ",", "precisions", ",", "bp", ",", "ratio", ",", "translation_length", ",", "reference_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.bleu._bleu": [[115, 135], ["zip", "bleu.compute_bleu", "round", "per_segment_references.append", "open", "open", "reference_text.append", "reference_list.append", "translations.append", "fh.readlines", "reference.strip().split", "line.strip().split", "reference.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu.compute_bleu"], ["", "def", "_bleu", "(", "ref_file", ",", "trans_file", ",", "subword_option", "=", "None", ")", ":", "\n", "    ", "max_order", "=", "4", "\n", "smooth", "=", "True", "\n", "ref_files", "=", "[", "ref_file", "]", "\n", "reference_text", "=", "[", "]", "\n", "for", "reference_filename", "in", "ref_files", ":", "\n", "        ", "with", "open", "(", "reference_filename", ")", "as", "fh", ":", "\n", "            ", "reference_text", ".", "append", "(", "fh", ".", "readlines", "(", ")", ")", "\n", "", "", "per_segment_references", "=", "[", "]", "\n", "for", "references", "in", "zip", "(", "*", "reference_text", ")", ":", "\n", "        ", "reference_list", "=", "[", "]", "\n", "for", "reference", "in", "references", ":", "\n", "            ", "reference_list", ".", "append", "(", "reference", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "per_segment_references", ".", "append", "(", "reference_list", ")", "\n", "", "translations", "=", "[", "]", "\n", "with", "open", "(", "trans_file", ")", "as", "fh", ":", "\n", "        ", "for", "line", "in", "fh", ":", "\n", "            ", "translations", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "", "bleu_score", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "compute_bleu", "(", "per_segment_references", ",", "translations", ",", "max_order", ",", "smooth", ")", "\n", "return", "round", "(", "100", "*", "bleu_score", ",", "2", ")", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq.__init__": [[21, 36], ["torch.Module.__init__", "model.Seq2Seq.register_buffer", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax", "torch.LogSoftmax", "model.Seq2Seq.tie_weights", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq.tie_weights"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "config", ",", "beam_size", "=", "None", ",", "max_length", "=", "None", ",", "sos_id", "=", "None", ",", "eos_id", "=", "None", ")", ":", "\n", "        ", "super", "(", "Seq2Seq", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "register_buffer", "(", "\"bias\"", ",", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "2048", ",", "2048", ")", ")", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "lsm", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq._tie_or_clone_weights": [[37, 44], ["torch.Parameter", "torch.Parameter", "torch.Parameter", "second_module.weight.clone"], "methods", ["None"], ["", "def", "_tie_or_clone_weights", "(", "self", ",", "first_module", ",", "second_module", ")", ":", "\n", "        ", "\"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n        \"\"\"", "\n", "if", "self", ".", "config", ".", "torchscript", ":", "\n", "            ", "first_module", ".", "weight", "=", "nn", ".", "Parameter", "(", "second_module", ".", "weight", ".", "clone", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "first_module", ".", "weight", "=", "second_module", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq.tie_weights": [[45, 51], ["model.Seq2Seq._tie_or_clone_weights"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq._tie_or_clone_weights"], ["", "", "def", "tie_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\"", "\n", "self", ".", "_tie_or_clone_weights", "(", "self", ".", "lm_head", ",", "\n", "self", ".", "encoder", ".", "embeddings", ".", "word_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Seq2Seq.forward": [[52, 102], ["model.Seq2Seq.encoder", "outputs[].permute().contiguous", "model.Seq2Seq.encoder.embeddings().permute().contiguous", "model.Seq2Seq.decoder", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "torch.tanh().permute().contiguous", "model.Seq2Seq.lm_head", "lm_logits[].contiguous", "target_ids[].contiguous", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor().fill_", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "outputs[].permute", "target_mask[].ne().view", "active_loss.sum", "model.Beam", "model.Beam.getCurrentState", "context.repeat.repeat.repeat", "context_mask.repeat.repeat.repeat", "range", "model.Beam.getHyp", "torch.cat.append", "torch.cat.append", "torch.cat.append", "model.Seq2Seq.encoder.embeddings().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "torch.tanh().permute", "lm_logits[].contiguous.view", "target_ids[].contiguous.view", "active_loss.sum", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "torch.cuda.LongTensor", "model.Beam.done", "model.Seq2Seq.encoder.embeddings().permute().contiguous", "model.Seq2Seq.decoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "model.Beam.advance", "torch.cat.data.copy_", "torch.cat.data.copy_", "torch.cat.data.copy_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.Beam.getFinal", "model.Beam.buildTargetTokens", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "target_mask[].ne", "lm_logits[].contiguous.size", "model.Seq2Seq.dense", "torch.tanh.permute().contiguous", "torch.tanh.permute().contiguous", "torch.tanh.permute().contiguous", "model.Seq2Seq.lsm", "torch.cat.data.index_select", "torch.cat.data.index_select", "torch.cat.data.index_select", "model.Seq2Seq.encoder.embeddings", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "model.Seq2Seq.encoder.embeddings().permute", "model.Seq2Seq.lm_head", "model.Beam.getCurrentOrigin", "model.Beam.getCurrentState", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.Seq2Seq.dense", "torch.tanh.permute", "torch.tanh.permute", "torch.tanh.permute", "model.Seq2Seq.encoder.embeddings", "x.view", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentState", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getHyp", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.done", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.advance", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getFinal", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.buildTargetTokens", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentOrigin", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentState"], ["", "def", "forward", "(", "self", ",", "source_ids", "=", "None", ",", "source_mask", "=", "None", ",", "target_ids", "=", "None", ",", "target_mask", "=", "None", ",", "args", "=", "None", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "encoder", "(", "source_ids", ",", "attention_mask", "=", "source_mask", ")", "\n", "encoder_output", "=", "outputs", "[", "0", "]", ".", "permute", "(", "[", "1", ",", "0", ",", "2", "]", ")", ".", "contiguous", "(", ")", "\n", "if", "target_ids", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "-", "1e4", "*", "(", "1", "-", "self", ".", "bias", "[", ":", "target_ids", ".", "shape", "[", "1", "]", ",", ":", "target_ids", ".", "shape", "[", "1", "]", "]", ")", "\n", "tgt_embeddings", "=", "self", ".", "encoder", ".", "embeddings", "(", "target_ids", ")", ".", "permute", "(", "[", "1", ",", "0", ",", "2", "]", ")", ".", "contiguous", "(", ")", "\n", "out", "=", "self", ".", "decoder", "(", "tgt_embeddings", ",", "encoder_output", ",", "tgt_mask", "=", "attn_mask", ",", "memory_key_padding_mask", "=", "(", "1", "-", "source_mask", ")", ".", "bool", "(", ")", ")", "\n", "hidden_states", "=", "torch", ".", "tanh", "(", "self", ".", "dense", "(", "out", ")", ")", ".", "permute", "(", "[", "1", ",", "0", ",", "2", "]", ")", ".", "contiguous", "(", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "hidden_states", ")", "\n", "# Shift so that tokens < n predict n", "\n", "active_loss", "=", "target_mask", "[", "...", ",", "1", ":", "]", ".", "ne", "(", "0", ")", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "shift_logits", "=", "lm_logits", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "target_ids", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "# Flatten the tokens", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "loss", "=", "loss_fct", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", "[", "active_loss", "]", ",", "\n", "shift_labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", ")", "\n", "\n", "outputs", "=", "loss", ",", "loss", "*", "active_loss", ".", "sum", "(", ")", ",", "active_loss", ".", "sum", "(", ")", "\n", "return", "outputs", "\n", "", "else", ":", "\n", "#Predict ", "\n", "            ", "preds", "=", "[", "]", "\n", "zero", "=", "torch", ".", "cuda", ".", "LongTensor", "(", "1", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "source_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "context", "=", "encoder_output", "[", ":", ",", "i", ":", "i", "+", "1", "]", "\n", "context_mask", "=", "source_mask", "[", "i", ":", "i", "+", "1", ",", ":", "]", "\n", "beam", "=", "Beam", "(", "self", ".", "beam_size", ",", "self", ".", "sos_id", ",", "self", ".", "eos_id", ")", "\n", "input_ids", "=", "beam", ".", "getCurrentState", "(", ")", "\n", "context", "=", "context", ".", "repeat", "(", "1", ",", "self", ".", "beam_size", ",", "1", ")", "\n", "context_mask", "=", "context_mask", ".", "repeat", "(", "self", ".", "beam_size", ",", "1", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "max_length", ")", ":", "\n", "                    ", "if", "beam", ".", "done", "(", ")", ":", "\n", "                        ", "break", "\n", "", "attn_mask", "=", "-", "1e4", "*", "(", "1", "-", "self", ".", "bias", "[", ":", "input_ids", ".", "shape", "[", "1", "]", ",", ":", "input_ids", ".", "shape", "[", "1", "]", "]", ")", "\n", "tgt_embeddings", "=", "self", ".", "encoder", ".", "embeddings", "(", "input_ids", ")", ".", "permute", "(", "[", "1", ",", "0", ",", "2", "]", ")", ".", "contiguous", "(", ")", "\n", "out", "=", "self", ".", "decoder", "(", "tgt_embeddings", ",", "context", ",", "tgt_mask", "=", "attn_mask", ",", "memory_key_padding_mask", "=", "(", "1", "-", "context_mask", ")", ".", "bool", "(", ")", ")", "\n", "out", "=", "torch", ".", "tanh", "(", "self", ".", "dense", "(", "out", ")", ")", "\n", "hidden_states", "=", "out", ".", "permute", "(", "[", "1", ",", "0", ",", "2", "]", ")", ".", "contiguous", "(", ")", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "out", "=", "self", ".", "lsm", "(", "self", ".", "lm_head", "(", "hidden_states", ")", ")", ".", "data", "\n", "beam", ".", "advance", "(", "out", ")", "\n", "input_ids", ".", "data", ".", "copy_", "(", "input_ids", ".", "data", ".", "index_select", "(", "0", ",", "beam", ".", "getCurrentOrigin", "(", ")", ")", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "(", "input_ids", ",", "beam", ".", "getCurrentState", "(", ")", ")", ",", "-", "1", ")", "\n", "", "hyp", "=", "beam", ".", "getHyp", "(", "beam", ".", "getFinal", "(", ")", ")", "\n", "pred", "=", "beam", ".", "buildTargetTokens", "(", "hyp", ")", "[", ":", "self", ".", "beam_size", "]", "\n", "pred", "=", "[", "torch", ".", "cat", "(", "[", "x", ".", "view", "(", "-", "1", ")", "for", "x", "in", "p", "]", "+", "[", "zero", "]", "*", "(", "self", ".", "max_length", "-", "len", "(", "p", ")", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", "for", "p", "in", "pred", "]", "\n", "preds", ".", "append", "(", "torch", ".", "cat", "(", "pred", ",", "0", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "preds", "=", "torch", ".", "cat", "(", "preds", ",", "0", ")", "\n", "return", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.__init__": [[105, 121], ["model.Beam.tt.FloatTensor().zero_", "model.Beam.tt.LongTensor().fill_", "model.Beam.tt.FloatTensor", "model.Beam.tt.LongTensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "sos", ",", "eos", ")", ":", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prevKs", "=", "[", "]", "\n", "# The outputs at each time-step.", "\n", "self", ".", "nextYs", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", "\n", ".", "fill_", "(", "0", ")", "]", "\n", "self", ".", "nextYs", "[", "0", "]", "[", "0", "]", "=", "sos", "\n", "# Has EOS topped the beam yet.", "\n", "self", ".", "_eos", "=", "eos", "\n", "self", ".", "eosTop", "=", "False", "\n", "# Time and k pair for finished.", "\n", "self", ".", "finished", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.getCurrentState": [[122, 126], ["model.Beam.tt.LongTensor().view", "model.Beam.tt.LongTensor"], "methods", ["None"], ["", "def", "getCurrentState", "(", "self", ")", ":", "\n", "        ", "\"Get the outputs for the current timestep.\"", "\n", "batch", "=", "self", ".", "tt", ".", "LongTensor", "(", "self", ".", "nextYs", "[", "-", "1", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.getCurrentOrigin": [[127, 130], ["None"], "methods", ["None"], ["", "def", "getCurrentOrigin", "(", "self", ")", ":", "\n", "        ", "\"Get the backpointers for the current timestep.\"", "\n", "return", "self", ".", "prevKs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.advance": [[131, 175], ["wordLk.size", "beamLk.view", "beamLk.view.topk", "model.Beam.prevKs.append", "model.Beam.nextYs.append", "range", "len", "range", "model.Beam.nextYs[].size", "model.Beam.scores.unsqueeze().expand_as", "model.Beam.nextYs[].size", "model.Beam.finished.append", "model.Beam.scores.unsqueeze", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "advance", "(", "self", ",", "wordLk", ")", ":", "\n", "        ", "\"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attnOut`: Compute and update the beam search.\n\n        Parameters:\n\n        * `wordLk`- probs of advancing from the last step (K x words)\n        * `attnOut`- attention at the last step\n\n        Returns: True if beam search is complete.\n        \"\"\"", "\n", "numWords", "=", "wordLk", ".", "size", "(", "1", ")", "\n", "\n", "# Sum the previous scores.", "\n", "if", "len", "(", "self", ".", "prevKs", ")", ">", "0", ":", "\n", "            ", "beamLk", "=", "wordLk", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "wordLk", ")", "\n", "\n", "# Don't let EOS have children.", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                    ", "beamLk", "[", "i", "]", "=", "-", "1e20", "\n", "", "", "", "else", ":", "\n", "            ", "beamLk", "=", "wordLk", "[", "0", "]", "\n", "", "flatBeamLk", "=", "beamLk", ".", "view", "(", "-", "1", ")", "\n", "bestScores", ",", "bestScoresId", "=", "flatBeamLk", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "True", ",", "True", ")", "\n", "\n", "self", ".", "scores", "=", "bestScores", "\n", "\n", "# bestScoresId is flattened beam x word array, so calculate which", "\n", "# word and beam each score came from", "\n", "prevK", "=", "bestScoresId", "//", "numWords", "\n", "self", ".", "prevKs", ".", "append", "(", "prevK", ")", "\n", "self", ".", "nextYs", ".", "append", "(", "(", "bestScoresId", "-", "prevK", "*", "numWords", ")", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                ", "s", "=", "self", ".", "scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "i", ")", ")", "\n", "\n", "# End condition is when top-of-beam is EOS and no global score.", "\n", "", "", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "0", "]", "==", "self", ".", "_eos", ":", "\n", "            ", "self", ".", "eosTop", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.done": [[176, 178], ["len"], "methods", ["None"], ["", "", "def", "done", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eosTop", "and", "len", "(", "self", ".", "finished", ")", ">=", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.getFinal": [[179, 192], ["model.Beam.finished.sort", "len", "model.Beam.finished.append", "len", "range", "unfinished.sort", "model.Beam.nextYs[].size", "unfinished.append", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "getFinal", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "finished", ")", "==", "0", ":", "\n", "            ", "self", ".", "finished", ".", "append", "(", "(", "self", ".", "scores", "[", "0", "]", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "0", ")", ")", "\n", "", "self", ".", "finished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "if", "len", "(", "self", ".", "finished", ")", "!=", "self", ".", "size", ":", "\n", "            ", "unfinished", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "!=", "self", ".", "_eos", ":", "\n", "                    ", "s", "=", "self", ".", "scores", "[", "i", "]", "\n", "unfinished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "i", ")", ")", "\n", "", "", "unfinished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "self", ".", "finished", "+=", "unfinished", "[", ":", "self", ".", "size", "-", "len", "(", "self", ".", "finished", ")", "]", "\n", "", "return", "self", ".", "finished", "[", ":", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.getHyp": [[193, 205], ["range", "hyps.append", "hyp.append", "len"], "methods", ["None"], ["", "def", "getHyp", "(", "self", ",", "beam_res", ")", ":", "\n", "        ", "\"\"\"\n        Walk back to construct the full hypothesis.\n        \"\"\"", "\n", "hyps", "=", "[", "]", "\n", "for", "_", ",", "timestep", ",", "k", "in", "beam_res", ":", "\n", "            ", "hyp", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prevKs", "[", ":", "timestep", "]", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "hyp", ".", "append", "(", "self", ".", "nextYs", "[", "j", "+", "1", "]", "[", "k", "]", ")", "\n", "k", "=", "self", ".", "prevKs", "[", "j", "]", "[", "k", "]", "\n", "", "hyps", ".", "append", "(", "hyp", "[", ":", ":", "-", "1", "]", ")", "\n", "", "return", "hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.model.Beam.buildTargetTokens": [[206, 216], ["sentence.append", "tokens.append"], "methods", ["None"], ["", "def", "buildTargetTokens", "(", "self", ",", "preds", ")", ":", "\n", "        ", "sentence", "=", "[", "]", "\n", "for", "pred", "in", "preds", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "for", "tok", "in", "pred", ":", "\n", "                ", "if", "tok", "==", "self", ".", "_eos", ":", "\n", "                    ", "break", "\n", "", "tokens", ".", "append", "(", "tok", ")", "\n", "", "sentence", ".", "append", "(", "tokens", ")", "\n", "", "return", "sentence", "", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.Example.__init__": [[53, 61], ["None"], "methods", ["None"], ["", "", "\"\"\"Base class of all process-related classes in order to share similar process\"\"\"", "\n", "class", "Procedure", "(", "object", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "# data", "\n", "        ", "self", ".", "src_vocab", "=", "Vocab", "(", "params", ",", "mode", "=", "params", ".", "encoder_language", ")", "\n", "self", ".", "trg_vocab", "=", "Vocab", "(", "params", ",", "mode", "=", "\"decoder\"", ")", "\n", "params", ".", "add", "(", "\"enc_vocab_size\"", ",", "self", ".", "src_vocab", ".", "size", "(", ")", ")", "\n", "params", ".", "add", "(", "\"dec_vocab_size\"", ",", "self", ".", "trg_vocab", ".", "size", "(", ")", ")", "\n", "wandb", ".", "config", ".", "update", "(", "params", ".", "dict", ")", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.InputFeatures.__init__": [[83, 96], ["None"], "methods", ["None"], ["param_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "param_prefix", ".", "format", "(", "iter", ",", "cur_time", ")", ")", "\n", "return", "model_save_path", ",", "param_save_path", "\n", "\n", "", "def", "setup_data", "(", "self", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'train'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "True", ",", "drop_last", "=", "True", ")", "\n", "", "elif", "mode", "==", "'valid'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'valid'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "False", ",", "drop_last", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'test'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "", "return", "dataloader", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.read_examples": [[62, 80], ["pandas.read_csv", "zip", "data[].tolist", "data[].tolist", "examples.append", "data[].tolist", "run.Example"], "function", ["None"], ["self", ".", "params", "=", "params", "\n", "\n", "", "def", "get_model_dir", "(", "self", ")", ":", "\n", "#save path", "\n", "        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_root", ",", "'train_%d'", "%", "(", "cur_time", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "'model'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "param_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'params.json'", ")", "\n", "print", "(", "\"Dump hyper-parameters to {}.\"", ".", "format", "(", "param_path", ")", ")", "\n", "self", ".", "params", ".", "save", "(", "param_path", ")", "\n", "return", "self", ".", "model_dir", "\n", "\n", "", "def", "get_save_path", "(", "self", ",", "iter", ")", ":", "\n", "        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n", "prefix", "=", "'model_{}_{}'", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.convert_examples_to_features": [[97, 144], ["enumerate", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "len", "len", "tokenizer.tokenize", "len", "len", "run.InputFeatures", "tokenizer.tokenize", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "x.replace", "map", "map", "x.replace", "map", "map"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["\n", "", "def", "setup_train", "(", "self", ")", ":", "\n", "# model", "\n", "        ", "model", "=", "Model", "(", "self", ".", "params", ")", "\n", "# parallel", "\n", "#model = torch.nn.DataParallel(model, device_ids=[5, 1, 2]).to(self.params.device)", "\n", "model", "=", "model", ".", "to", "(", "self", ".", "params", ".", "device", ")", "\n", "wandb", ".", "watch", "(", "model", ")", "\n", "# optim", "\n", "# optimizer = optim.Adam(model.parameters(), lr=self.params.lr)", "\n", "#optimizer = optim.Adagrad(model.parameters(), lr=self.params.lr, initial_accumulator_value=0.1)", "\n", "optimizer", "=", "ScheduledOptim", "(", "\n", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "eps", "=", "1e-09", ",", "lr", "=", "self", ".", "params", ".", "lr", ")", ",", "\n", "0.1", ",", "self", ".", "params", ".", "d_model", ",", "self", ".", "params", ".", "n_warmup_steps", ")", "\n", "return", "model", ",", "optimizer", "\n", "\n", "", "def", "train_epoch", "(", "self", ",", "model", ",", "training_data", ",", "max_ext_len", ",", "optimizer", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Training)   '", "\n", "for", "batch", "in", "tqdm", "(", "training_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "src_oovs", "=", "[", "training_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#optim", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "training_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n", "", "def", "eval_epoch", "(", "self", ",", "model", ",", "validation_data", ",", "max_ext_len", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.set_seed": [[145, 152], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["        ", "model", ".", "eval", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Validation) '", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "tqdm", "(", "validation_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "                ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                    ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.main": [[153, 518], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.FileHandler", "logger.addHandler", "logger.info", "logger.warning", "run.set_seed", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "torch.TransformerDecoderLayer", "torch.TransformerDecoder", "model.Seq2Seq", "torch.nn.DataParallel.to", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "bool", "os.path.exists", "os.makedirs", "logger.info", "torch.nn.DataParallel.load_state_dict", "DDP", "run.read_examples", "run.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.train", "range", "itertools.cycle", "enumerate", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "len", "next", "tuple", "torch.nn.DataParallel.", "loss.mean.item", "round", "source_ids.size", "loss.mean.backward", "files.append", "files.append", "logger.info", "run.read_examples", "run.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "tqdm.tqdm", "utils.get_rouge_dict", "utils.get_bleu4_score", "utils.get_executable_rate", "utils.get_func_correctness", "utils.get_func_correctness", "ImportError", "len", "loss.mean.mean", "logger.info", "transformers.AdamW.step", "transformers.AdamW.zero_grad", "transformers.get_linear_schedule_with_warmup.step", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.eval", "torch.nn.DataParallel.train", "sorted", "logger.info", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "torch.nn.DataParallel.train", "utils.get_func_correctness", "round", "logger.info", "logger.info", "tuple", "len", "len", "io.open", "json.dump", "io.open", "io.open", "zip", "torch.cuda.is_available", "torch.cuda.is_available", "list.to", "run.read_examples", "run.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "len", "tuple", "loss.mean.sum().item", "num.sum().item", "round", "round", "result.keys", "logger.info", "os.path.exists", "os.makedirs", "hasattr", "model_to_save.state_dict", "logger.info", "logger.info", "os.path.join", "os.path.join", "torch.save", "torch.save", "run.read_examples", "random.sample", "run.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "tuple", "len", "len", "logger.info", "logger.info", "os.path.join", "os.path.join", "torch.save", "torch.save", "len", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "os.path.join", "os.path.join", "os.path.join", "f.write", "f1.write", "torch.nn.DataParallel.named_parameters", "torch.nn.DataParallel.named_parameters", "any", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "numpy.exp", "str", "round", "os.path.exists", "os.makedirs", "hasattr", "model_to_save.state_dict", "min", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "os.path.exists", "os.makedirs", "hasattr", "model_to_save.state_dict", "file.rindex", "file.rindex", "list.to", "pred[].cpu().numpy", "list", "tokenizer_class.from_pretrained.decode", "preds.append", "any", "list.to", "loss.mean.sum", "num.sum", "numpy.exp", "len", "list.to", "pred[].cpu().numpy", "list", "tokenizer_class.from_pretrained.decode", "preds.append", "str", "pred[].cpu", "pred[].cpu", "list.index", "list.index"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.set_seed", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.read_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.convert_examples_to_features", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.read_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.convert_examples_to_features", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_rouge_dict", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_bleu4_score", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_executable_rate", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.read_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.convert_examples_to_features", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.read_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.run.convert_examples_to_features", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode"], ["src_oovs", "=", "[", "validation_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                    ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#print(\"xxxxxxxxxxx\",loss)", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "validation_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n", "", "", "def", "cal_bleu_rouge", "(", "self", ",", "pred", ",", "original_trg_batch", ",", "src_oovs", "=", "None", ")", ":", "\n", "# print(\"pred: \\n\", pred)", "\n", "# print(\"pred: \\n\", pred.shape)", "\n", "        ", "pred_batch", "=", "torch", ".", "max", "(", "pred", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "\n", "# print(\"pred_batch: \\n\", pred_batch)", "\n", "pred_sentences", "=", "[", "]", "\n", "gold_sentences", "=", "original_trg_batch", "\n", "for", "i", "in", "range", "(", "pred_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "PGprocess", ".", "outputids2words", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ",", "self", ".", "vocab", ",", "src_oovs", "[", "i", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "id2sentence", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ")", ")", "\n", "\n", "", "", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Pred_sentences example: \\n\"", ",", "pred_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Gold_sentences example: \\n\"", ",", "gold_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "\n", "#rouge l", "\n", "r_score", "=", "get_rouge_dict", "(", "pred_sentences", ",", "gold_sentences", ")", "[", "\"rouge-l\"", "]", "[", "'f'", "]", "\n", "#bleu 4", "\n", "b_score", "=", "get_bleu4_score", "(", "pred_sentences", ",", "gold_sentences", ",", "self", ".", "trg_vocab", ".", "tokenizer", ")", "\n", "#func_correctness", "\n", "func_corr", "=", "get_func_correctness", "(", "pred_sentences", ",", "gold_sentences", ")", "\n", "\n", "return", "r_score", ",", "b_score", ",", "func_corr", "\n", "\n", "\n", "", "", "'''Main class'''", "\n", "class", "Main", "(", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "param_path", "=", "None", ")", ":", "\n", "        ", "if", "param_path", "!=", "None", ":", "\n", "            ", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "\n", "", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "model_dir", "=", "self", ".", "base", ".", "get_model_dir", "(", ")", "\n", "self", ".", "params", ".", "add", "(", "\"model_dir\"", ",", "self", ".", "model_dir", ")", "\n", "wandb", ".", "config", ".", "update", "(", "self", ".", "params", ".", "dict", ")", "\n", "training_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'train'", ")", "\n", "validation_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'valid'", ")", "\n", "model", ",", "optimizer", "=", "self", ".", "base", ".", "setup_train", "(", ")", "\n", "params", "=", "self", ".", "params", "\n", "valid_losses", "=", "[", "]", "\n", "valid_rouges", "=", "[", "]", "\n", "valid_bleus", "=", "[", "]", "\n", "valid_fun_corrs", "=", "[", "]", "\n", "for", "epoch_i", "in", "range", "(", "params", ".", "epoch", ")", ":", "\n", "            ", "print", "(", "'[ Epoch'", ",", "epoch_i", "+", "1", ",", "']'", ")", "\n", "# train", "\n", "train_loss", ",", "train_rouge", ",", "train_bleu", ",", "train_func_corr", "=", "self", ".", "base", ".", "train_epoch", "(", "model", ",", "training_data", ",", "training_data", ".", "dataset", ".", "max_src_oovs", ",", "optimizer", ")", "\n", "# eval", "\n", "valid_loss", ",", "valid_rouge", ",", "valid_bleu", ",", "valid_func_corr", "=", "self", ".", "base", ".", "eval_epoch", "(", "model", ",", "validation_data", ",", "validation_data", ".", "dataset", ".", "max_src_oovs", ")", "\n", "#print(\"***************\",valid_loss)", "\n", "# log", "\n", "wandb", ".", "log", "(", "{", "\"Training loss: \"", ":", "train_loss", ",", "\n", "\"Training rouge: \"", ":", "train_rouge", ",", "\n", "\"Training bleu: \"", ":", "train_bleu", ",", "\n", "\"Training func_corr\"", ":", "train_func_corr", ",", "\n", "\"Validation loss: \"", ":", "valid_loss", ",", "\n", "\"Validation rouge: \"", ":", "valid_rouge", ",", "\n", "\"Validation bleu: \"", ":", "valid_bleu", ",", "\n", "\"Validation func_corr: \"", ":", "valid_func_corr", "}", ")", "\n", "\n", "checkpoint", "=", "{", "'epoch'", ":", "epoch_i", ",", "'settings'", ":", "params", ",", "'model'", ":", "model", ".", "state_dict", "(", ")", "}", "\n", "\n", "if", "params", ".", "save_mode", ":", "\n", "                ", "valid_losses", "+=", "[", "valid_loss", "]", "\n", "valid_rouges", "+=", "[", "valid_rouge", "]", "\n", "valid_bleus", "+=", "[", "valid_bleu", "]", "\n", "valid_fun_corrs", "+=", "[", "valid_func_corr", "]", "\n", "if", "params", ".", "save_mode", "==", "'best'", ":", "\n", "                    ", "model_name", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_loss_model.chkpt'", "\n", "if", "valid_loss", "<=", "min", "(", "valid_losses", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by loss.'", ")", "\n", "", "model_name_rouge", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_rouge_model.chkpt'", "\n", "if", "valid_rouge", ">=", "max", "(", "valid_rouges", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_rouge", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by rouge.'", ")", "\n", "", "model_name_bleu", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_bleu_model.chkpt'", "\n", "if", "valid_bleu", ">=", "max", "(", "valid_bleus", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_bleu", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n", "", "model_name_func_corr", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_func_corr_model.chkpt'", "\n", "if", "valid_func_corr", ">=", "max", "(", "valid_fun_corrs", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_func_corr", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n", "\n", "", "", "", "", "", "def", "basic_decode", "(", "self", ",", "best_model_on_validation", ",", "data_file_prefix", ")", ":", "\n", "        ", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "test_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "data_file_prefix", ")", "\n", "decode_processor", "=", "Search", "(", "self", ".", "params", ",", "best_model_on_validation", ",", "test_data", ",", "data_file_prefix", ")", "\n", "result_dict", "=", "decode_processor", ".", "decode", "(", ")", "\n", "print", "(", "result_dict", ")", "\n", "\n", "", "def", "decode", "(", "self", ",", "model_path", ",", "data_file_prefix", "=", "\"test\"", ")", ":", "\n", "        ", "if", "\";\"", "in", "model_path", ":", "\n", "            ", "for", "path", "in", "model_path", ".", "split", "(", "\";\"", ")", ":", "\n", "                ", "param_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                    ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "", "", "", "else", ":", "\n", "            ", "param_path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "\n", "", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "init_seeds", "(", ")", "\n", "fire", ".", "Fire", "(", "Main", ")", "\n", "#python -m run train --param-path params.json", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.get_bleu4_score": [[18, 25], ["nltk.translate.bleu_score.corpus_bleu", "tokenizer.tokenize", "tokenizer.tokenize", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["def", "init_seeds", "(", ")", ":", "\n", "    ", "random", ".", "seed", "(", "123", ")", "\n", "np", ".", "random", ".", "seed", "(", "123", ")", "\n", "torch", ".", "manual_seed", "(", "123", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "123", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "123", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.get_rouge_dict": [[26, 30], ["rouge.Rouge", "rouge.Rouge.get_scores"], "function", ["None"], ["torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "123", ")", "\n", "\n", "", "def", "init_lstm_weight", "(", "lstm", ")", ":", "\n", "    ", "for", "param", "in", "lstm", ".", "parameters", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.get_var_replacing": [[31, 67], ["parso.utils.parse_version_string", "parso.python.tokenize.tokenize", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "token_list.append", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "str", "re.findall", "utils.sqlparse", "re.findall", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.sqlparse"], ["        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n", "", "", "", "def", "init_gru_weight", "(", "gru", ")", ":", "\n", "    ", "for", "param", "in", "gru", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n", "", "", "", "def", "init_linear_weight", "(", "linear", ")", ":", "\n", "    ", "init_xavier_weight", "(", "linear", ".", "weight", ")", "\n", "if", "linear", ".", "bias", "is", "not", "None", ":", "\n", "        ", "init_bias", "(", "linear", ".", "bias", ")", "\n", "\n", "", "", "def", "init_normal_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "normal_", "(", "w", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "\n", "", "def", "init_uniform_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "uniform_", "(", "w", ",", "-", "0.1", ",", "0.1", ")", "\n", "\n", "", "def", "init_ortho_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "orthogonal_", "(", "w", ")", "\n", "\n", "", "def", "init_xavier_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "xavier_normal_", "(", "w", ")", "\n", "\n", "", "def", "init_bias", "(", "b", ")", ":", "\n", "    ", "nn", ".", "init", ".", "constant_", "(", "b", ",", "0.", ")", "\n", "\n", "\n", "# evaluation metrics", "\n", "", "def", "get_bleu4_score", "(", "hyps_list", ",", "gold_list", ",", "tokenizer", ")", ":", "\n", "    ", "b_score", "=", "corpus_bleu", "(", "\n", "[", "[", "tokenizer", ".", "tokenize", "(", "ref", ")", "]", "for", "ref", "in", "gold_list", "]", ",", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.get_func_correctness": [[69, 86], ["zip", "print", "print", "len", "len", "utils.get_var_replacing", "utils.get_var_replacing", "index_list.append", "len", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing"], ["smoothing_function", "=", "bleu_score", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", "weights", "=", "(", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", ")", ")", "\n", "return", "b_score", "\n", "\n", "", "def", "get_rouge_dict", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "    ", "rouge", "=", "Rouge", "(", ")", "\n", "result_dict", "=", "rouge", ".", "get_scores", "(", "hyps_list", ",", "gold_list", ",", "avg", "=", "True", ")", "\n", "return", "result_dict", "\n", "\n", "", "def", "get_var_replacing", "(", "code_string", ",", "repalce_string", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "var_dict", "=", "{", "}", "\n", "token_list", "=", "[", "]", "\n", "var_index", "=", "0", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "code_string", ",", "version_info", ")", ":", "\n", "        ", "if", "not", "repalce_string", ":", "\n", "# print(i)", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.embedded_ast_matching": [[87, 113], ["parso.utils.parse_version_string", "zip", "parso.python.tokenize.tokenize", "index_list.append", "len", "sqls.append", "utils.embedded_ast_matching.get_sql_parser"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "elif", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "i", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "i", ".", "string", "\n", "", "token_list", ".", "append", "(", "sql_parsed", ")", "\n", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", "or", "(", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ")", ":", "\n", "                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "", "return", "token_list", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.code_staticAnaylsis": [[114, 126], ["str().replace", "pylint.epylint.py_run", "os.remove", "pylint_stdout.read", "open", "f.write", "f.write", "str", "time.time", "str", "str", "str"], "function", ["None"], ["\n", "\n", "", "def", "get_func_correctness", "(", "hyps_list", ",", "gold_list", ",", "repalce_string", "=", "False", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "ast_match_num", "=", "0", "\n", "index", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "i", ":", "\n", "            ", "i", ",", "j", "=", "get_var_replacing", "(", "i", ",", "repalce_string", ")", ",", "get_var_replacing", "(", "j", ",", "repalce_string", ")", "\n", "if", "i", "==", "j", ":", "\n", "                ", "ast_match_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "", "index", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.get_executable_rate": [[127, 136], ["range", "len", "len", "utils.code_staticAnaylsis", "len", "hyps_list[].replace"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.code_staticAnaylsis"], ["", "print", "(", "\"Number of AST matching\"", ",", "ast_match_num", ")", "\n", "print", "(", "\"Accuration of AST matching\"", ",", "ast_match_num", "/", "len", "(", "hyps_list", ")", ")", "\n", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "", "def", "embedded_ast_matching", "(", "hyps_list", ",", "gold_list", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "def", "get_sql_parser", "(", "s", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeBert.utils.sqlparse": [[137, 240], ["s.replace().replace.replace().replace", "re.findall", "re.findall", "re.findall", "s.replace().replace.index", "s.replace().replace.index", "parso.python.tokenize.tokenize", "s.replace().replace.index", "len", "range", "s.replace().replace.replace", "len", "len", "query_tokens.append", "len", "len", "cond_list.append", "temp.append", "op_list.append", "col_list.append", "col_list.append", "from_toknes.append", "where_tokens.append", "temp.append", "cond_list.append", "dict", "len", "len", "col_list.append", "col_list.append", "col_list.append", "temp.append", "len", "len", "col_list.append", "len", "temp.append", "temp.append"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.dict"], ["        ", "sqls", "=", "[", "]", "\n", "for", "t", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "            ", "if", "t", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "t", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "t", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "t", ".", "string", "\n", "", "sqls", ".", "append", "(", "sql_parsed", ")", "\n", "", "", "return", "sqls", "\n", "", "m_num", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "index", "=", "0", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "get_sql_parser", "(", "i", ")", "!=", "[", "]", "and", "get_sql_parser", "(", "i", ")", "==", "get_sql_parser", "(", "j", ")", ":", "\n", "# print(i, j)", "\n", "# print(\"---------------\")", "\n", "            ", "m_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "index", "+=", "1", "\n", "", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "", "def", "code_staticAnaylsis", "(", "code", ",", "id", ")", ":", "\n", "    ", "cur_time", "=", "str", "(", "time", ".", "time", "(", ")", ")", ".", "replace", "(", "\".\"", ",", "\"\"", ")", "\n", "with", "open", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"# pylint: disable=E1101\\n\"", ")", "\n", "f", ".", "write", "(", "code", ")", "\n", "", "(", "pylint_stdout", ",", "pylint_stderr", ")", "=", "lint", ".", "py_run", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py -s yes\"", ",", "return_std", "=", "True", ")", "\n", "os", ".", "remove", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ")", "\n", "pylint_stdout_str", "=", "pylint_stdout", ".", "read", "(", ")", "\n", "# pylint_stderr_str = pylint_stderr.read()", "\n", "if", "\"E0\"", "in", "pylint_stdout_str", "or", "\"E1\"", "in", "pylint_stdout_str", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "get_executable_rate", "(", "hyps_list", ")", ":", "\n", "    ", "executable_wrong_num", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "hyps_list", "[", "i", "]", ":", "\n", "            ", "if", "code_staticAnaylsis", "(", "hyps_list", "[", "i", "]", ".", "replace", "(", "\"\\t\"", ",", "\"    \"", ")", ",", "i", ")", ":", "\n", "                ", "executable_wrong_num", "+=", "1", "\n", "", "", "else", ":", "\n", "            ", "executable_wrong_num", "+=", "1", "\n", "", "", "return", "(", "len", "(", "hyps_list", ")", "-", "executable_wrong_num", ")", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "def", "sqlparse", "(", "s", ")", ":", "\n", "    ", "s", "=", "s", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ".", "replace", "(", "\"\\\\\"", ",", "\" \"", ")", "\n", "# split index", "\n", "se", "=", "re", ".", "findall", "(", "r\"(SELECT )|(select )\"", ",", "s", ")", "\n", "fr", "=", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "s", ")", "\n", "wh", "=", "re", ".", "findall", "(", "r\"( WHERE )|( where )\"", ",", "s", ")", "\n", "try", ":", "\n", "        ", "assert", "len", "(", "se", ")", "==", "len", "(", "fr", ")", "==", "1", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "", "s_index", "=", "s", ".", "index", "(", "se", "[", "0", "]", "[", "0", "]", "if", "se", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "se", "[", "0", "]", "[", "1", "]", ")", "\n", "f_index", "=", "s", ".", "index", "(", "fr", "[", "0", "]", "[", "0", "]", "if", "fr", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "fr", "[", "0", "]", "[", "1", "]", ")", "\n", "if", "wh", "!=", "[", "]", ":", "\n", "        ", "w_index", "=", "s", ".", "index", "(", "wh", "[", "0", "]", "[", "0", "]", "if", "wh", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "wh", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "w_index", "=", "9999", "\n", "\n", "# split tokens", "\n", "", "query_tokens", "=", "[", "]", "\n", "from_toknes", "=", "[", "]", "\n", "where_tokens", "=", "[", "]", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "# print(i)", "\n", "        ", "if", "i", ".", "type", "in", "[", "tokenize", ".", "INDENT", ",", "tokenize", ".", "DEDENT", ",", "tokenize", ".", "ENDMARKER", "]", ":", "\n", "            ", "continue", "\n", "", "if", "i", ".", "start_pos", "[", "1", "]", "<", "f_index", ":", "\n", "            ", "query_tokens", ".", "append", "(", "i", ")", "\n", "", "elif", "i", ".", "start_pos", "[", "1", "]", "<", "w_index", ":", "\n", "            ", "from_toknes", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "where_tokens", ".", "append", "(", "i", ")", "\n", "\n", "# condition list", "\n", "", "", "if", "len", "(", "where_tokens", ")", "!=", "0", ":", "\n", "#where_list = [where_tokens[0].string]", "\n", "        ", "where_list", "=", "[", "]", "\n", "temp", "=", "[", "]", "\n", "temp_op", "=", "''", "\n", "for", "i", "in", "where_tokens", "[", "1", ":", "]", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "==", "','", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "", "elif", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "!=", "':'", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "i", ".", "string", "\n", "", "elif", "i", ".", "type", "!=", "tokenize", ".", "OP", ":", "\n", "                ", "if", "temp_op", "!=", "''", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"PARAMETER\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "''", "\n", "", "else", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"COLUMN\"", ":", "i", ".", "string", "}", ")", "\n", "", "", "", "cond", "=", "{", "}", "\n", "cond_list", "=", "[", "]", "\n", "# print(\"temp: \",temp)", "\n", "for", "i", "in", "range", "(", "len", "(", "temp", ")", ")", ":", "\n", "# print(temp[i])", "\n", "            ", "if", "(", "i", "+", "1", ")", "%", "4", "==", "0", ":", "\n", "                ", "cond_list", ".", "append", "(", "cond", ")", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.__init__": [[8, 25], ["beam.Beam.tt.FloatTensor().zero_", "beam.Beam.tt.LongTensor().fill_", "beam.Beam.tt.FloatTensor", "beam.Beam.tt.LongTensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "sos", ",", "eos", ")", ":", "\n", "# The size of beam", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "tt", "=", "torch", ".", "cuda", "\n", "# The score for each translation on the beam.", "\n", "self", ".", "scores", "=", "self", ".", "tt", ".", "FloatTensor", "(", "size", ")", ".", "zero_", "(", ")", "\n", "# The backpointers at each time-step.", "\n", "self", ".", "prevKs", "=", "[", "]", "\n", "# The outputs at each time-step.", "\n", "self", ".", "nextYs", "=", "[", "self", ".", "tt", ".", "LongTensor", "(", "size", ")", "\n", ".", "fill_", "(", "0", ")", "]", "\n", "self", ".", "nextYs", "[", "0", "]", "[", ":", "]", "=", "sos", "\n", "# Has EOS topped the beam yet.", "\n", "self", ".", "_eos", "=", "eos", "\n", "self", ".", "eosTop", "=", "False", "\n", "# Time and k pair for finished.", "\n", "self", ".", "finished", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentState": [[26, 30], ["beam.Beam.tt.LongTensor().view", "beam.Beam.tt.LongTensor"], "methods", ["None"], ["", "def", "getCurrentState", "(", "self", ")", ":", "\n", "        ", "\"Get the outputs for the current timestep.\"", "\n", "batch", "=", "self", ".", "tt", ".", "LongTensor", "(", "self", ".", "nextYs", "[", "-", "1", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentOrigin": [[31, 34], ["None"], "methods", ["None"], ["", "def", "getCurrentOrigin", "(", "self", ")", ":", "\n", "        ", "\"Get the backpointers for the current timestep.\"", "\n", "return", "self", ".", "prevKs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.advance": [[35, 79], ["wordLk.size", "beamLk.view", "beamLk.view.topk", "beam.Beam.prevKs.append", "beam.Beam.nextYs.append", "range", "len", "range", "beam.Beam.nextYs[].size", "beam.Beam.scores.unsqueeze().expand_as", "beam.Beam.nextYs[].size", "beam.Beam.finished.append", "beam.Beam.scores.unsqueeze", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "advance", "(", "self", ",", "wordLk", ")", ":", "\n", "        ", "\"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attnOut`: Compute and update the beam search.\n\n        Parameters:\n\n        * `wordLk`- probs of advancing from the last step (K x words)\n        * `attnOut`- attention at the last step\n\n        Returns: True if beam search is complete.\n        \"\"\"", "\n", "numWords", "=", "wordLk", ".", "size", "(", "1", ")", "\n", "\n", "# Sum the previous scores.", "\n", "if", "len", "(", "self", ".", "prevKs", ")", ">", "0", ":", "\n", "            ", "beamLk", "=", "wordLk", "+", "self", ".", "scores", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "wordLk", ")", "\n", "\n", "# Don't let EOS have children.", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                    ", "beamLk", "[", "i", "]", "=", "-", "1e20", "\n", "", "", "", "else", ":", "\n", "            ", "beamLk", "=", "wordLk", "[", "0", "]", "\n", "", "flatBeamLk", "=", "beamLk", ".", "view", "(", "-", "1", ")", "\n", "bestScores", ",", "bestScoresId", "=", "flatBeamLk", ".", "topk", "(", "self", ".", "size", ",", "0", ",", "True", ",", "True", ")", "\n", "\n", "self", ".", "scores", "=", "bestScores", "\n", "\n", "# bestScoresId is flattened beam x word array, so calculate which", "\n", "# word and beam each score came from", "\n", "prevK", "=", "bestScoresId", "//", "numWords", "\n", "self", ".", "prevKs", ".", "append", "(", "prevK", ")", "\n", "self", ".", "nextYs", ".", "append", "(", "(", "bestScoresId", "-", "prevK", "*", "numWords", ")", ")", "\n", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "==", "self", ".", "_eos", ":", "\n", "                ", "s", "=", "self", ".", "scores", "[", "i", "]", "\n", "self", ".", "finished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "i", ")", ")", "\n", "\n", "# End condition is when top-of-beam is EOS and no global score.", "\n", "", "", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "0", "]", "==", "self", ".", "_eos", ":", "\n", "            ", "self", ".", "eosTop", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.done": [[80, 82], ["len"], "methods", ["None"], ["", "", "def", "done", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "eosTop", "and", "len", "(", "self", ".", "finished", ")", ">=", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getFinal": [[83, 96], ["beam.Beam.finished.sort", "len", "beam.Beam.finished.append", "len", "range", "unfinished.sort", "beam.Beam.nextYs[].size", "unfinished.append", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["", "def", "getFinal", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "finished", ")", "==", "0", ":", "\n", "            ", "self", ".", "finished", ".", "append", "(", "(", "self", ".", "scores", "[", "0", "]", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "0", ")", ")", "\n", "", "self", ".", "finished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "if", "len", "(", "self", ".", "finished", ")", "!=", "self", ".", "size", ":", "\n", "            ", "unfinished", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "nextYs", "[", "-", "1", "]", ".", "size", "(", "0", ")", ")", ":", "\n", "                ", "if", "self", ".", "nextYs", "[", "-", "1", "]", "[", "i", "]", "!=", "self", ".", "_eos", ":", "\n", "                    ", "s", "=", "self", ".", "scores", "[", "i", "]", "\n", "unfinished", ".", "append", "(", "(", "s", ",", "len", "(", "self", ".", "nextYs", ")", "-", "1", ",", "i", ")", ")", "\n", "", "", "unfinished", ".", "sort", "(", "key", "=", "lambda", "a", ":", "-", "a", "[", "0", "]", ")", "\n", "self", ".", "finished", "+=", "unfinished", "[", ":", "self", ".", "size", "-", "len", "(", "self", ".", "finished", ")", "]", "\n", "", "return", "self", ".", "finished", "[", ":", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getHyp": [[97, 109], ["range", "hyps.append", "hyp.append", "len"], "methods", ["None"], ["", "def", "getHyp", "(", "self", ",", "beam_res", ")", ":", "\n", "        ", "\"\"\"\n        Walk back to construct the full hypothesis.\n        \"\"\"", "\n", "hyps", "=", "[", "]", "\n", "for", "_", ",", "timestep", ",", "k", "in", "beam_res", ":", "\n", "            ", "hyp", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "prevKs", "[", ":", "timestep", "]", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "hyp", ".", "append", "(", "self", ".", "nextYs", "[", "j", "+", "1", "]", "[", "k", "]", ")", "\n", "k", "=", "self", ".", "prevKs", "[", "j", "]", "[", "k", "]", "\n", "", "hyps", ".", "append", "(", "hyp", "[", ":", ":", "-", "1", "]", ")", "\n", "", "return", "hyps", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.buildTargetTokens": [[110, 120], ["sentence.append", "tokens.append"], "methods", ["None"], ["", "def", "buildTargetTokens", "(", "self", ",", "preds", ")", ":", "\n", "        ", "sentence", "=", "[", "]", "\n", "for", "pred", "in", "preds", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "for", "tok", "in", "pred", ":", "\n", "                ", "if", "tok", "==", "self", ".", "_eos", ":", "\n", "                    ", "break", "\n", "", "tokens", ".", "append", "(", "tok", ")", "\n", "", "sentence", ".", "append", "(", "tokens", ")", "\n", "", "return", "sentence", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._get_ngrams": [[28, 46], ["collections.Counter", "range", "range", "tuple", "len"], "function", ["None"], ["def", "_get_ngrams", "(", "segment", ",", "max_order", ")", ":", "\n", "  ", "\"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"", "\n", "ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "segment", ")", "-", "order", "+", "1", ")", ":", "\n", "      ", "ngram", "=", "tuple", "(", "segment", "[", "i", ":", "i", "+", "order", "]", ")", "\n", "ngram_counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "ngram_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu.compute_bleu": [[48, 113], ["zip", "range", "min", "len", "collections.Counter", "bleu._get_ngrams", "range", "min", "sum", "math.exp", "float", "math.exp", "bleu._get_ngrams", "len", "len", "float", "math.log", "len"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._get_ngrams", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._get_ngrams"], ["", "def", "compute_bleu", "(", "reference_corpus", ",", "translation_corpus", ",", "max_order", "=", "4", ",", "\n", "smooth", "=", "False", ")", ":", "\n", "  ", "\"\"\"Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  \"\"\"", "\n", "matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "possible_matches_by_order", "=", "[", "0", "]", "*", "max_order", "\n", "reference_length", "=", "0", "\n", "translation_length", "=", "0", "\n", "for", "(", "references", ",", "translation", ")", "in", "zip", "(", "reference_corpus", ",", "\n", "translation_corpus", ")", ":", "\n", "    ", "reference_length", "+=", "min", "(", "len", "(", "r", ")", "for", "r", "in", "references", ")", "\n", "translation_length", "+=", "len", "(", "translation", ")", "\n", "\n", "merged_ref_ngram_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "reference", "in", "references", ":", "\n", "      ", "merged_ref_ngram_counts", "|=", "_get_ngrams", "(", "reference", ",", "max_order", ")", "\n", "", "translation_ngram_counts", "=", "_get_ngrams", "(", "translation", ",", "max_order", ")", "\n", "overlap", "=", "translation_ngram_counts", "&", "merged_ref_ngram_counts", "\n", "for", "ngram", "in", "overlap", ":", "\n", "      ", "matches_by_order", "[", "len", "(", "ngram", ")", "-", "1", "]", "+=", "overlap", "[", "ngram", "]", "\n", "", "for", "order", "in", "range", "(", "1", ",", "max_order", "+", "1", ")", ":", "\n", "      ", "possible_matches", "=", "len", "(", "translation", ")", "-", "order", "+", "1", "\n", "if", "possible_matches", ">", "0", ":", "\n", "        ", "possible_matches_by_order", "[", "order", "-", "1", "]", "+=", "possible_matches", "\n", "\n", "", "", "", "precisions", "=", "[", "0", "]", "*", "max_order", "\n", "for", "i", "in", "range", "(", "0", ",", "max_order", ")", ":", "\n", "    ", "if", "smooth", ":", "\n", "      ", "precisions", "[", "i", "]", "=", "(", "(", "matches_by_order", "[", "i", "]", "+", "1.", ")", "/", "\n", "(", "possible_matches_by_order", "[", "i", "]", "+", "1.", ")", ")", "\n", "", "else", ":", "\n", "      ", "if", "possible_matches_by_order", "[", "i", "]", ">", "0", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "(", "float", "(", "matches_by_order", "[", "i", "]", ")", "/", "\n", "possible_matches_by_order", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "        ", "precisions", "[", "i", "]", "=", "0.0", "\n", "\n", "", "", "", "if", "min", "(", "precisions", ")", ">", "0", ":", "\n", "    ", "p_log_sum", "=", "sum", "(", "(", "1.", "/", "max_order", ")", "*", "math", ".", "log", "(", "p", ")", "for", "p", "in", "precisions", ")", "\n", "geo_mean", "=", "math", ".", "exp", "(", "p_log_sum", ")", "\n", "", "else", ":", "\n", "    ", "geo_mean", "=", "0", "\n", "\n", "", "ratio", "=", "float", "(", "translation_length", ")", "/", "reference_length", "\n", "\n", "if", "ratio", ">", "1.0", ":", "\n", "    ", "bp", "=", "1.", "\n", "", "else", ":", "\n", "    ", "bp", "=", "math", ".", "exp", "(", "1", "-", "1.", "/", "ratio", ")", "\n", "\n", "", "bleu", "=", "geo_mean", "*", "bp", "\n", "\n", "return", "(", "bleu", ",", "precisions", ",", "bp", ",", "ratio", ",", "translation_length", ",", "reference_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu._bleu": [[115, 135], ["zip", "bleu.compute_bleu", "round", "per_segment_references.append", "open", "open", "reference_text.append", "reference_list.append", "translations.append", "fh.readlines", "reference.strip().split", "line.strip().split", "reference.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.bleu.compute_bleu"], ["", "def", "_bleu", "(", "ref_file", ",", "trans_file", ",", "subword_option", "=", "None", ")", ":", "\n", "    ", "max_order", "=", "4", "\n", "smooth", "=", "True", "\n", "ref_files", "=", "[", "ref_file", "]", "\n", "reference_text", "=", "[", "]", "\n", "for", "reference_filename", "in", "ref_files", ":", "\n", "        ", "with", "open", "(", "reference_filename", ")", "as", "fh", ":", "\n", "            ", "reference_text", ".", "append", "(", "fh", ".", "readlines", "(", ")", ")", "\n", "", "", "per_segment_references", "=", "[", "]", "\n", "for", "references", "in", "zip", "(", "*", "reference_text", ")", ":", "\n", "        ", "reference_list", "=", "[", "]", "\n", "for", "reference", "in", "references", ":", "\n", "            ", "reference_list", ".", "append", "(", "reference", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "per_segment_references", ".", "append", "(", "reference_list", ")", "\n", "", "translations", "=", "[", "]", "\n", "with", "open", "(", "trans_file", ")", "as", "fh", ":", "\n", "        ", "for", "line", "in", "fh", ":", "\n", "            ", "translations", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "", "bleu_score", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "compute_bleu", "(", "per_segment_references", ",", "translations", ",", "max_order", ",", "smooth", ")", "\n", "return", "round", "(", "100", "*", "bleu_score", ",", "2", ")", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__init__": [[23, 81], ["os.path.join", "torch.distributed.get_world_size", "os.path.exists", "os.makedirs", "os.path.exists", "os.path.join", "pandas.read_csv", "len", "logger.info", "range", "logger.warning", "open", "pickle.load", "logger.warning", "logger.info", "tokenizer.encode", "dataset.lyraDataset.pad_and_get_mask", "dataset.lyraDataset.inputs.append", "dataset.lyraDataset.token_labels.append", "logger.warning", "logger.warning", "logger.warning", "tokenizer.encode", "open", "pickle.dump", "tokenizer.encode", "len"], "methods", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.pad_and_get_mask", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ",", "args", ",", "logger", ",", "file_type", "=", "'train'", ",", "block_size", "=", "512", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "local_rank", "=", "0", "\n", "world_size", "=", "1", "\n", "", "else", ":", "\n", "            ", "local_rank", "=", "args", ".", "local_rank", "\n", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "mode", "=", "mode", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "", "cached_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "file_type", "+", "\"_blocksize_%d\"", "%", "(", "block_size", ")", "+", "\"_wordsize_%d\"", "%", "(", "world_size", ")", "+", "\"_rank_%d\"", "%", "(", "local_rank", ")", ")", "\n", "if", "mode", "!=", "'test'", "and", "os", ".", "path", ".", "exists", "(", "cached_file", ")", "and", "not", "args", ".", "overwrite_cache", ":", "\n", "            ", "if", "file_type", "==", "'train'", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Loading features from cached file %s\"", ",", "cached_file", ")", "\n", "", "with", "open", "(", "cached_file", ",", "'rb'", ")", "as", "handle", ":", "\n", "                ", "data", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "self", ".", "inputs", "=", "data", "[", "'inputs'", "]", "\n", "self", ".", "token_labels", "=", "data", "[", "'token_labels'", "]", "\n", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "inputs", "=", "[", "]", "\n", "self", ".", "token_labels", "=", "[", "]", "\n", "# read train or dev file", "\n", "datafile", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "f\"{file_type}.lyra.csv\"", ")", "\n", "if", "file_type", "==", "'train'", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Creating features from dataset file at %s\"", ",", "datafile", ")", "\n", "", "datas", "=", "pd", ".", "read_csv", "(", "datafile", ")", "\n", "\n", "length", "=", "len", "(", "datas", ")", "\n", "logger", ".", "info", "(", "\"Data size: %d\"", "%", "(", "length", ")", ")", "\n", "if", "args", ".", "nl_langs", "==", "'zh'", ":", "\n", "                ", "logger", ".", "info", "(", "'Using chinese comm in code.'", ")", "\n", "", "for", "idx", "in", "range", "(", "length", ")", ":", "\n", "                ", "if", "idx", "%", "(", "length", "//", "10", ")", "==", "0", ":", "\n", "                    ", "percent", "=", "idx", "/", "(", "length", "//", "10", ")", "*", "10", "\n", "logger", ".", "warning", "(", "\"Rank %d, load %d\"", "%", "(", "local_rank", ",", "percent", ")", ")", "\n", "", "if", "idx", "%", "world_size", "!=", "local_rank", ":", "\n", "                    ", "continue", "\n", "\n", "", "code", "=", "tokenizer", ".", "encode", "(", "datas", "[", "\"code\"", "]", "[", "idx", "]", ")", "\n", "if", "args", ".", "nl_langs", "==", "'zh'", ":", "\n", "                    ", "nl", "=", "tokenizer", ".", "encode", "(", "datas", "[", "\"comm_zh\"", "]", "[", "idx", "]", ")", "\n", "", "elif", "args", ".", "nl_langs", "==", "'en'", ":", "\n", "                    ", "nl", "=", "tokenizer", ".", "encode", "(", "datas", "[", "\"comm_en\"", "]", "[", "idx", "]", ")", "\n", "\n", "", "input_ids", ",", "input_labels", "=", "self", ".", "pad_and_get_mask", "(", "code", ",", "nl", ",", "tokenizer", ")", "\n", "self", ".", "inputs", ".", "append", "(", "input_ids", ")", "\n", "self", ".", "token_labels", ".", "append", "(", "input_labels", ")", "\n", "\n", "", "if", "file_type", "==", "'train'", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Rank %d Training %d token, %d samples\"", "%", "(", "local_rank", ",", "length", ",", "len", "(", "self", ".", "inputs", ")", ")", ")", "\n", "logger", ".", "warning", "(", "\"Saving features into cached file %s\"", ",", "cached_file", ")", "\n", "", "if", "mode", "!=", "'test'", ":", "\n", "                ", "with", "open", "(", "cached_file", ",", "'wb'", ")", "as", "handle", ":", "\n", "                    ", "pickle", ".", "dump", "(", "{", "'inputs'", ":", "self", ".", "inputs", ",", "'token_labels'", ":", "self", ".", "token_labels", "}", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.pad_and_get_mask": [[82, 103], ["len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "", "", "", "def", "pad_and_get_mask", "(", "self", ",", "code", ",", "nl", ",", "tokenizer", ")", ":", "\n", "        ", "if", "self", ".", "mode", "==", "'test'", ":", "\n", "            ", "code", "=", "[", "]", "\n", "", "while", "(", "len", "(", "code", ")", "+", "len", "(", "nl", ")", "+", "2", ">", "self", ".", "block_size", ")", ":", "\n", "            ", "if", "(", "len", "(", "code", ")", ">", "len", "(", "nl", ")", ")", ":", "\n", "                ", "code", "=", "code", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "nl", "=", "nl", "[", ":", "-", "1", "]", "\n", "", "", "if", "self", ".", "mode", "==", "'train'", ":", "\n", "            ", "inputs", "=", "nl", "+", "[", "tokenizer", ".", "bos_token_id", "]", "+", "code", "+", "[", "tokenizer", ".", "eos_token_id", "]", "\n", "labels", "=", "[", "1", "]", "*", "len", "(", "nl", ")", "+", "[", "2", "]", "*", "(", "len", "(", "code", ")", "+", "1", ")", "+", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "inputs", "=", "nl", "+", "[", "tokenizer", ".", "bos_token_id", "]", "\n", "labels", "=", "[", "1", "]", "*", "len", "(", "nl", ")", "+", "[", "2", "]", "\n", "return", "inputs", ",", "labels", "\n", "", "assert", "len", "(", "inputs", ")", "<=", "self", ".", "block_size", "\n", "pad_len", "=", "self", ".", "block_size", "-", "len", "(", "inputs", ")", "\n", "inputs", "+=", "[", "tokenizer", ".", "pad_token_id", "]", "*", "pad_len", "\n", "labels", "+=", "[", "0", "]", "*", "pad_len", "\n", "assert", "len", "(", "inputs", ")", "==", "len", "(", "labels", ")", "\n", "return", "inputs", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__len__": [[105, 107], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.dataset.lyraDataset.__getitem__": [[108, 110], ["torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "self", ".", "inputs", "[", "item", "]", ")", ",", "torch", ".", "tensor", "(", "self", ".", "token_labels", "[", "item", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.load_and_cache_examples": [[62, 66], ["dataset.lyraDataset"], "function", ["None"], ["self", ".", "params", "=", "params", "\n", "\n", "", "def", "get_model_dir", "(", "self", ")", ":", "\n", "#save path", "\n", "        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.set_seed": [[67, 73], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["train_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "params", ".", "model_root", ",", "'train_%d'", "%", "(", "cur_time", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "train_dir", ")", "\n", "", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "'model'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "param_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'params.json'", ")", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.update_config": [[74, 78], ["None"], "function", ["None"], ["print", "(", "\"Dump hyper-parameters to {}.\"", ".", "format", "(", "param_path", ")", ")", "\n", "self", ".", "params", ".", "save", "(", "param_path", ")", "\n", "return", "self", ".", "model_dir", "\n", "\n", "", "def", "get_save_path", "(", "self", ",", "iter", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train": [[79, 257], ["torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "torch.nn.parallel.DistributedDataParallel.to", "transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "os.path.exists", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.parallel.DistributedDataParallel.zero_grad", "run.set_seed", "range", "os.path.join", "tensorboardX.SummaryWriter", "max", "len", "torch.distributed.barrier", "transformers.get_linear_schedule_with_warmup.load_state_dict", "transformers.AdamW.load_state_dict", "torch.distributed.barrier", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "int", "enumerate", "tensorboardX.SummaryWriter.close", "os.path.exists", "os.makedirs", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.load", "torch.load", "batch.to", "torch.tensor", "torch.tensor", "torch.nn.parallel.DistributedDataParallel.train", "torch.nn.parallel.DistributedDataParallel.", "logits[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "loss_mask[].contiguous().view", "torch.nonzero().view", "torch.nn.CrossEntropyLoss.", "loss.mean.item", "ImportError", "loss.mean.mean", "torch.nn.utils.clip_grad_norm_", "loss.mean.backward", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.AdamW.zero_grad", "transformers.get_linear_schedule_with_warmup.step", "round", "torch.nn.parallel.DistributedDataParallel.named_parameters", "torch.nn.parallel.DistributedDataParallel.named_parameters", "any", "token_labels.clone().detach", "token_labels.clone().detach", "loss_mask[].contiguous", "torch.nonzero", "logits[].contiguous.view", "labels[].contiguous.view", "amp.scale_loss", "scaled_loss.backward", "amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "numpy.exp", "logger.info", "tensorboardX.SummaryWriter.add_scalar", "tensorboardX.SummaryWriter.add_scalar", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "torch.save", "logger.info", "os.path.join", "model_to_save.save_pretrained", "tokenizer.save_pretrained", "os.path.join", "torch.save", "torch.save", "logger.info", "os.path.join", "any", "logits[].contiguous.size", "round", "run.eval_bleu", "eval_bleu.items", "os.path.join", "os.path.join", "os.path.exists", "os.makedirs", "hasattr", "os.path.join", "os.path.exists", "os.makedirs", "open", "idxf.write", "transformers.AdamW.state_dict", "os.path.join", "transformers.get_linear_schedule_with_warmup.state_dict", "os.path.join", "open", "stepf.write", "token_labels.clone", "token_labels.clone", "transformers.get_linear_schedule_with_warmup.get_last_lr", "tensorboardX.SummaryWriter.add_scalar", "logger.info", "round", "str", "str", "type", "tensorboardX.SummaryWriter.add_scalar", "logger.info", "tensorboardX.SummaryWriter.add_scalar", "logger.info", "round", "round", "round"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.set_seed", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.step", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.save", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.eval_bleu"], ["        ", "cur_time", "=", "time", ".", "time", "(", ")", "\n", "prefix", "=", "'model_{}_{}'", "\n", "param_prefix", "=", "'params_{}_{}'", "\n", "model_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "prefix", ".", "format", "(", "iter", ",", "cur_time", ")", ")", "\n", "param_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "param_prefix", ".", "format", "(", "iter", ",", "cur_time", ")", ")", "\n", "return", "model_save_path", ",", "param_save_path", "\n", "\n", "", "def", "setup_data", "(", "self", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'train'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "True", ",", "drop_last", "=", "True", ")", "\n", "", "elif", "mode", "==", "'valid'", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'valid'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "self", ".", "params", ".", "batch_size", ",", "shuffle", "=", "False", ",", "drop_last", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "Code4SQLDataset2PG", "(", "self", ".", "params", ",", "'test'", ")", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "", "return", "dataloader", "\n", "\n", "", "def", "setup_train", "(", "self", ")", ":", "\n", "# model", "\n", "        ", "model", "=", "Model", "(", "self", ".", "params", ")", "\n", "# parallel", "\n", "#model = torch.nn.DataParallel(model, device_ids=[5, 1, 2]).to(self.params.device)", "\n", "model", "=", "model", ".", "to", "(", "self", ".", "params", ".", "device", ")", "\n", "wandb", ".", "watch", "(", "model", ")", "\n", "# optim", "\n", "# optimizer = optim.Adam(model.parameters(), lr=self.params.lr)", "\n", "#optimizer = optim.Adagrad(model.parameters(), lr=self.params.lr, initial_accumulator_value=0.1)", "\n", "optimizer", "=", "ScheduledOptim", "(", "\n", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "eps", "=", "1e-09", ",", "lr", "=", "self", ".", "params", ".", "lr", ")", ",", "\n", "0.1", ",", "self", ".", "params", ".", "d_model", ",", "self", ".", "params", ".", "n_warmup_steps", ")", "\n", "return", "model", ",", "optimizer", "\n", "\n", "", "def", "train_epoch", "(", "self", ",", "model", ",", "training_data", ",", "max_ext_len", ",", "optimizer", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Training)   '", "\n", "for", "batch", "in", "tqdm", "(", "training_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "src_oovs", "=", "[", "training_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#optim", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "training_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n", "", "def", "eval_epoch", "(", "self", ",", "model", ",", "validation_data", ",", "max_ext_len", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "total_loss", ",", "total_rouge", ",", "total_bleu", ",", "n_batch", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "total_func_corr", "=", "0", "\n", "desc", "=", "'  - (Validation) '", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "tqdm", "(", "validation_data", ",", "mininterval", "=", "2", ",", "desc", "=", "desc", ")", ":", "\n", "                ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                    ", "data_index", ",", "enc_batch", ",", "enc_wiht_extend_vocab", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "src_oovs", "=", "[", "validation_data", ".", "dataset", ".", "src_oovs", "[", "i", "]", "for", "i", "in", "data_index", "]", "\n", "", "else", ":", "\n", "                    ", "src_oovs", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", "=", "None", ",", "None", ",", "None", "\n", "data_index", ",", "enc_batch", ",", "dec_input", ",", "dec_target", "=", "batch", "\n", "\n", "# forward", "\n", "", "pred", ",", "loss", "=", "model", "(", "enc_batch", ",", "dec_input", ",", "dec_target", ",", "enc_wiht_extend_vocab", ",", "max_ext_len", ")", "\n", "#print(\"xxxxxxxxxxx\",loss)", "\n", "#rouge, bleu", "\n", "original_trg_batch", "=", "[", "validation_data", ".", "dataset", ".", "example_list", "[", "i", "]", ".", "original_trg", "for", "i", "in", "data_index", "]", "\n", "r_score", ",", "b_score", ",", "func_corr", "=", "self", ".", "cal_bleu_rouge", "(", "pred", ",", "original_trg_batch", ",", "src_oovs", ")", "\n", "#sum and averge", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "total_rouge", "+=", "r_score", "\n", "total_bleu", "+=", "b_score", "\n", "total_func_corr", "+=", "func_corr", "\n", "n_batch", "+=", "1", "\n", "\n", "", "return", "total_loss", "/", "n_batch", ",", "total_rouge", "/", "n_batch", ",", "total_bleu", "/", "n_batch", ",", "total_func_corr", "/", "n_batch", "\n", "\n", "", "", "def", "cal_bleu_rouge", "(", "self", ",", "pred", ",", "original_trg_batch", ",", "src_oovs", "=", "None", ")", ":", "\n", "# print(\"pred: \\n\", pred)", "\n", "# print(\"pred: \\n\", pred.shape)", "\n", "        ", "pred_batch", "=", "torch", ".", "max", "(", "pred", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "\n", "# print(\"pred_batch: \\n\", pred_batch)", "\n", "pred_sentences", "=", "[", "]", "\n", "gold_sentences", "=", "original_trg_batch", "\n", "for", "i", "in", "range", "(", "pred_batch", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "self", ".", "params", ".", "pointer_gen", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "convert_tokens_to_string", "(", "PGprocess", ".", "outputids2words", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ",", "self", ".", "vocab", ",", "src_oovs", "[", "i", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "pred_sentences", ".", "append", "(", "self", ".", "trg_vocab", ".", "tokenizer", ".", "id2sentence", "(", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "[", "i", "]", ")", ")", "\n", "\n", "", "", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Pred_sentences example: \\n\"", ",", "pred_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "print", "(", "\">>>>>>>>>>>Gold_sentences example: \\n\"", ",", "gold_sentences", "[", "0", "]", ")", "\n", "print", "(", ")", "\n", "\n", "#rouge l", "\n", "r_score", "=", "get_rouge_dict", "(", "pred_sentences", ",", "gold_sentences", ")", "[", "\"rouge-l\"", "]", "[", "'f'", "]", "\n", "#bleu 4", "\n", "b_score", "=", "get_bleu4_score", "(", "pred_sentences", ",", "gold_sentences", ",", "self", ".", "trg_vocab", ".", "tokenizer", ")", "\n", "#func_correctness", "\n", "func_corr", "=", "get_func_correctness", "(", "pred_sentences", ",", "gold_sentences", ")", "\n", "\n", "return", "r_score", ",", "b_score", ",", "func_corr", "\n", "\n", "\n", "", "", "'''Main class'''", "\n", "class", "Main", "(", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "param_path", "=", "None", ")", ":", "\n", "        ", "if", "param_path", "!=", "None", ":", "\n", "            ", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "\n", "", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "model_dir", "=", "self", ".", "base", ".", "get_model_dir", "(", ")", "\n", "self", ".", "params", ".", "add", "(", "\"model_dir\"", ",", "self", ".", "model_dir", ")", "\n", "wandb", ".", "config", ".", "update", "(", "self", ".", "params", ".", "dict", ")", "\n", "training_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'train'", ")", "\n", "validation_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "'valid'", ")", "\n", "model", ",", "optimizer", "=", "self", ".", "base", ".", "setup_train", "(", ")", "\n", "params", "=", "self", ".", "params", "\n", "valid_losses", "=", "[", "]", "\n", "valid_rouges", "=", "[", "]", "\n", "valid_bleus", "=", "[", "]", "\n", "valid_fun_corrs", "=", "[", "]", "\n", "for", "epoch_i", "in", "range", "(", "params", ".", "epoch", ")", ":", "\n", "            ", "print", "(", "'[ Epoch'", ",", "epoch_i", "+", "1", ",", "']'", ")", "\n", "# train", "\n", "train_loss", ",", "train_rouge", ",", "train_bleu", ",", "train_func_corr", "=", "self", ".", "base", ".", "train_epoch", "(", "model", ",", "training_data", ",", "training_data", ".", "dataset", ".", "max_src_oovs", ",", "optimizer", ")", "\n", "# eval", "\n", "valid_loss", ",", "valid_rouge", ",", "valid_bleu", ",", "valid_func_corr", "=", "self", ".", "base", ".", "eval_epoch", "(", "model", ",", "validation_data", ",", "validation_data", ".", "dataset", ".", "max_src_oovs", ")", "\n", "#print(\"***************\",valid_loss)", "\n", "# log", "\n", "wandb", ".", "log", "(", "{", "\"Training loss: \"", ":", "train_loss", ",", "\n", "\"Training rouge: \"", ":", "train_rouge", ",", "\n", "\"Training bleu: \"", ":", "train_bleu", ",", "\n", "\"Training func_corr\"", ":", "train_func_corr", ",", "\n", "\"Validation loss: \"", ":", "valid_loss", ",", "\n", "\"Validation rouge: \"", ":", "valid_rouge", ",", "\n", "\"Validation bleu: \"", ":", "valid_bleu", ",", "\n", "\"Validation func_corr: \"", ":", "valid_func_corr", "}", ")", "\n", "\n", "checkpoint", "=", "{", "'epoch'", ":", "epoch_i", ",", "'settings'", ":", "params", ",", "'model'", ":", "model", ".", "state_dict", "(", ")", "}", "\n", "\n", "if", "params", ".", "save_mode", ":", "\n", "                ", "valid_losses", "+=", "[", "valid_loss", "]", "\n", "valid_rouges", "+=", "[", "valid_rouge", "]", "\n", "valid_bleus", "+=", "[", "valid_bleu", "]", "\n", "valid_fun_corrs", "+=", "[", "valid_func_corr", "]", "\n", "if", "params", ".", "save_mode", "==", "'best'", ":", "\n", "                    ", "model_name", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_loss_model.chkpt'", "\n", "if", "valid_loss", "<=", "min", "(", "valid_losses", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by loss.'", ")", "\n", "", "model_name_rouge", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_rouge_model.chkpt'", "\n", "if", "valid_rouge", ">=", "max", "(", "valid_rouges", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_rouge", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by rouge.'", ")", "\n", "", "model_name_bleu", "=", "params", ".", "encoder_language", "+", "\"_\"", "+", "params", ".", "save_mode", "+", "'_bleu_model.chkpt'", "\n", "if", "valid_bleu", ">=", "max", "(", "valid_bleus", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_bleu", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.evaluate": [[259, 311], ["run.load_and_cache_examples", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "enumerate", "torch.exp", "os.path.join", "os.makedirs", "max", "torch.nn.DataParallel", "batch.to", "torch.tensor", "torch.tensor", "torch.tensor", "float", "float", "open", "sorted", "os.path.exists", "torch.no_grad", "torch.nn.DataParallel.", "logits[].contiguous", "labels[].contiguous", "torch.nn.CrossEntropyLoss", "loss_mask[].contiguous().view", "torch.nonzero().view", "torch.nn.CrossEntropyLoss.", "loss_fct.mean().item", "result.keys", "writer.write", "token_labels.clone().detach", "token_labels.clone().detach", "loss_mask[].contiguous", "torch.nonzero", "logits[].contiguous.view", "labels[].contiguous.view", "loss_fct.mean", "token_labels.clone", "token_labels.clone", "logits[].contiguous.size", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.load_and_cache_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], ["if", "valid_func_corr", ">=", "max", "(", "valid_fun_corrs", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "checkpoint", ",", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "model_name_func_corr", ")", ")", "\n", "print", "(", "'    - [Info] The checkpoint file has been updated by bleu.'", ")", "\n", "\n", "", "", "", "", "", "def", "basic_decode", "(", "self", ",", "best_model_on_validation", ",", "data_file_prefix", ")", ":", "\n", "        ", "self", ".", "base", "=", "Procedure", "(", "self", ".", "params", ")", "\n", "test_data", "=", "self", ".", "base", ".", "setup_data", "(", "mode", "=", "data_file_prefix", ")", "\n", "decode_processor", "=", "Search", "(", "self", ".", "params", ",", "best_model_on_validation", ",", "test_data", ",", "data_file_prefix", ")", "\n", "result_dict", "=", "decode_processor", ".", "decode", "(", ")", "\n", "print", "(", "result_dict", ")", "\n", "\n", "", "def", "decode", "(", "self", ",", "model_path", ",", "data_file_prefix", "=", "\"test\"", ")", ":", "\n", "        ", "if", "\";\"", "in", "model_path", ":", "\n", "            ", "for", "path", "in", "model_path", ".", "split", "(", "\";\"", ")", ":", "\n", "                ", "param_path", "=", "os", ".", "path", ".", "join", "(", "path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                    ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "", "", "", "else", ":", "\n", "            ", "param_path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"params.json\"", ")", "\n", "self", ".", "params", "=", "Params", "(", "param_path", ")", "\n", "model_names", "=", "[", "self", ".", "params", ".", "encoder_language", "+", "\"_best_func_corr_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_bleu_model.chkpt\"", ",", "self", ".", "params", ".", "encoder_language", "+", "\"_best_rouge_model.chkpt\"", "]", "\n", "for", "m_name", "in", "model_names", ":", "\n", "                ", "best_model_on_validation", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "m_name", ")", "\n", "self", ".", "basic_decode", "(", "best_model_on_validation", ",", "data_file_prefix", ")", "\n", "\n", "", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "init_seeds", "(", ")", "\n", "fire", ".", "Fire", "(", "Main", ")", "\n", "#python -m run train --param-path params.json", "", "", ""]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.eval_bleu": [[313, 391], ["dataset.lyraDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "model.to", "model.zero_grad", "model.eval", "enumerate", "os.path.join", "pandas.read_csv", "utils.get_rouge_dict", "utils.get_bleu4_score", "utils.get_executable_rate", "utils.get_func_correctness", "utils.get_func_correctness", "batch.to", "datas[].tolist", "len", "len", "open", "json.dump", "open", "open", "zip", "torch.no_grad", "torch.nn.LogSoftmax", "torch.cuda.LongTensor().fill_", "range", "torch.cat", "logger.info", "os.path.join", "os.path.join", "os.path.join", "f.write", "f.write", "f1.write", "f1.write", "model", "beam.Beam", "range", "beam.Beam.getHyp", "torch.cat.append", "pred[].cpu().numpy", "list", "tokenizer.decode", "preds.append", "torch.cuda.LongTensor", "x[].expand", "beam.Beam.done", "beam.Beam.getCurrentState", "model", "beam.Beam.advance", "beam.Beam.getFinal", "beam.Beam.buildTargetTokens", "torch.cat().view", "torch.cat().unsqueeze", "torch.cat", "torch.nn.LogSoftmax.", "x.data.index_select", "pred[].cpu", "type", "torch.cat", "beam.Beam.getCurrentOrigin", "torch.cat", "torch.cat", "list.index", "x[].unsqueeze", "x[].unsqueeze", "type", "x[].unsqueeze", "x[].unsqueeze", "x.view", "len"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.ScheduledOptim.ScheduledOptim.zero_grad", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_rouge_dict", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_bleu4_score", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_executable_rate", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getHyp", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Main.decode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.done", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentState", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.advance", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getFinal", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.buildTargetTokens", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.beam.Beam.getCurrentOrigin"], []], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.main": [[393, 611], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.warning", "logging.basicConfig", "logger.warning", "logging.FileHandler", "logger.addHandler", "run.set_seed", "os.path.join", "model_class.parameters", "sum", "logger.info", "logger.info", "ValueError", "os.path.exists", "os.listdir", "ValueError", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "os.path.exists", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.exists", "logger.info", "tokenizer_class.from_pretrained", "logger.info", "model_class.from_pretrained", "model_class.resize_token_embeddings", "run.update_config", "logger.info", "tokenizer_class.from_pretrained", "config_class.from_pretrained", "model_class", "model_class.resize_token_embeddings", "run.update_config", "torch.distributed.barrier", "run.load_and_cache_examples", "run.train", "logger.info", "run.eval_bleu", "logger.info", "run.eval_bleu", "logger.info", "torch.distributed.get_world_size", "open", "tokenizer_class.from_pretrained.encode", "len", "len", "numpy.prod", "int", "open", "int", "p.size", "torch.cuda.is_available", "[].strip", "[].strip", "idxf.readlines", "stepf.readlines"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.set_seed", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.update_config", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.update_config", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.load_and_cache_examples", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.train", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.eval_bleu", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.run.eval_bleu", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Model.Model.encode", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.Vocab.size"], []], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_bleu4_score": [[18, 25], ["nltk.translate.bleu_score.corpus_bleu", "tokenizer.tokenize", "tokenizer.tokenize", "nltk.translate.bleu_score.SmoothingFunction"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["def", "init_seeds", "(", ")", ":", "\n", "    ", "random", ".", "seed", "(", "123", ")", "\n", "np", ".", "random", ".", "seed", "(", "123", ")", "\n", "torch", ".", "manual_seed", "(", "123", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed", "(", "123", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "123", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_rouge_dict": [[26, 30], ["rouge.Rouge", "rouge.Rouge.get_scores"], "function", ["None"], ["torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "123", ")", "\n", "\n", "", "def", "init_lstm_weight", "(", "lstm", ")", ":", "\n", "    ", "for", "param", "in", "lstm", ".", "parameters", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing": [[31, 67], ["parso.utils.parse_version_string", "parso.python.tokenize.tokenize", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "token_list.append", "token_list.append", "var_dict.keys", "token_list.append", "token_list.append", "str", "re.findall", "utils.sqlparse", "re.findall", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.sqlparse"], ["        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n", "", "", "", "def", "init_gru_weight", "(", "gru", ")", ":", "\n", "    ", "for", "param", "in", "gru", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "len", "(", "param", ".", "shape", ")", ">=", "2", ":", "# weights", "\n", "            ", "init_ortho_weight", "(", "param", ".", "data", ")", "\n", "", "else", ":", "# bias", "\n", "            ", "init_bias", "(", "param", ".", "data", ")", "\n", "\n", "", "", "", "def", "init_linear_weight", "(", "linear", ")", ":", "\n", "    ", "init_xavier_weight", "(", "linear", ".", "weight", ")", "\n", "if", "linear", ".", "bias", "is", "not", "None", ":", "\n", "        ", "init_bias", "(", "linear", ".", "bias", ")", "\n", "\n", "", "", "def", "init_normal_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "normal_", "(", "w", ",", "mean", "=", "0", ",", "std", "=", "0.01", ")", "\n", "\n", "", "def", "init_uniform_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "uniform_", "(", "w", ",", "-", "0.1", ",", "0.1", ")", "\n", "\n", "", "def", "init_ortho_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "orthogonal_", "(", "w", ")", "\n", "\n", "", "def", "init_xavier_weight", "(", "w", ")", ":", "\n", "    ", "nn", ".", "init", ".", "xavier_normal_", "(", "w", ")", "\n", "\n", "", "def", "init_bias", "(", "b", ")", ":", "\n", "    ", "nn", ".", "init", ".", "constant_", "(", "b", ",", "0.", ")", "\n", "\n", "\n", "# evaluation metrics", "\n", "", "def", "get_bleu4_score", "(", "hyps_list", ",", "gold_list", ",", "tokenizer", ")", ":", "\n", "    ", "b_score", "=", "corpus_bleu", "(", "\n", "[", "[", "tokenizer", ".", "tokenize", "(", "ref", ")", "]", "for", "ref", "in", "gold_list", "]", ",", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_func_correctness": [[69, 86], ["zip", "print", "print", "len", "len", "utils.get_var_replacing", "utils.get_var_replacing", "index_list.append", "len", "str"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_var_replacing"], ["smoothing_function", "=", "bleu_score", ".", "SmoothingFunction", "(", "epsilon", "=", "1e-12", ")", ".", "method1", ",", "\n", "weights", "=", "(", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", ")", ")", "\n", "return", "b_score", "\n", "\n", "", "def", "get_rouge_dict", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "    ", "rouge", "=", "Rouge", "(", ")", "\n", "result_dict", "=", "rouge", ".", "get_scores", "(", "hyps_list", ",", "gold_list", ",", "avg", "=", "True", ")", "\n", "return", "result_dict", "\n", "\n", "", "def", "get_var_replacing", "(", "code_string", ",", "repalce_string", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "var_dict", "=", "{", "}", "\n", "token_list", "=", "[", "]", "\n", "var_index", "=", "0", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "code_string", ",", "version_info", ")", ":", "\n", "        ", "if", "not", "repalce_string", ":", "\n", "# print(i)", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.embedded_ast_matching": [[87, 113], ["parso.utils.parse_version_string", "zip", "parso.python.tokenize.tokenize", "index_list.append", "len", "sqls.append", "utils.embedded_ast_matching.get_sql_parser"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize"], ["                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "elif", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "i", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "i", ".", "string", "\n", "", "token_list", ".", "append", "(", "sql_parsed", ")", "\n", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "NAME", "or", "(", "i", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "i", ".", "string", ")", "!=", "[", "]", ")", ":", "\n", "                ", "if", "i", ".", "string", "in", "var_dict", ".", "keys", "(", ")", ":", "\n", "                    ", "token_list", ".", "append", "(", "var_dict", "[", "i", ".", "string", "]", ")", "\n", "", "else", ":", "\n", "                    ", "var", "=", "\"var_\"", "+", "str", "(", "var_index", ")", "\n", "var_dict", "[", "i", ".", "string", "]", "=", "var", "\n", "token_list", ".", "append", "(", "var", ")", "\n", "var_index", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "token_list", ".", "append", "(", "i", ".", "string", ")", "\n", "", "", "", "return", "token_list", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.code_staticAnaylsis": [[114, 126], ["str().replace", "pylint.epylint.py_run", "os.remove", "pylint_stdout.read", "open", "f.write", "f.write", "str", "time.time", "str", "str", "str"], "function", ["None"], ["\n", "\n", "", "def", "get_func_correctness", "(", "hyps_list", ",", "gold_list", ",", "repalce_string", "=", "False", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "ast_match_num", "=", "0", "\n", "index", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "i", ":", "\n", "            ", "i", ",", "j", "=", "get_var_replacing", "(", "i", ",", "repalce_string", ")", ",", "get_var_replacing", "(", "j", ",", "repalce_string", ")", "\n", "if", "i", "==", "j", ":", "\n", "                ", "ast_match_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "", "index", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.get_executable_rate": [[127, 136], ["range", "len", "len", "utils.code_staticAnaylsis", "len", "hyps_list[].replace"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.code_staticAnaylsis"], ["", "print", "(", "\"Number of AST matching\"", ",", "ast_match_num", ")", "\n", "print", "(", "\"Accuration of AST matching\"", ",", "ast_match_num", "/", "len", "(", "hyps_list", ")", ")", "\n", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "ast_match_num", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "", "def", "embedded_ast_matching", "(", "hyps_list", ",", "gold_list", ",", "need_index", "=", "False", ")", ":", "\n", "    ", "version_info", "=", "parso", ".", "utils", ".", "parse_version_string", "(", "\"3.8\"", ")", "\n", "def", "get_sql_parser", "(", "s", ")", ":", "\n"]], "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_CodeGPT.utils.sqlparse": [[137, 240], ["s.replace().replace.replace().replace", "re.findall", "re.findall", "re.findall", "s.replace().replace.index", "s.replace().replace.index", "parso.python.tokenize.tokenize", "s.replace().replace.index", "len", "range", "s.replace().replace.replace", "len", "len", "query_tokens.append", "len", "len", "cond_list.append", "temp.append", "op_list.append", "col_list.append", "col_list.append", "from_toknes.append", "where_tokens.append", "temp.append", "cond_list.append", "dict", "len", "len", "col_list.append", "col_list.append", "col_list.append", "temp.append", "len", "len", "col_list.append", "len", "temp.append", "temp.append"], "function", ["home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.Dataset.CodeTokenizer.tokenize", "home.repos.pwc.inspect_result.liangqingyuan_lyra.Lyra_Transformer.run.Params.dict"], ["        ", "sqls", "=", "[", "]", "\n", "for", "t", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "            ", "if", "t", ".", "type", "==", "tokenize", ".", "STRING", "and", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "t", ".", "string", ")", "!=", "[", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "sql_parsed", "=", "sqlparse", "(", "t", ".", "string", "[", "1", ":", "-", "1", "]", ")", "\n", "", "except", ":", "\n", "                    ", "sql_parsed", "=", "t", ".", "string", "\n", "", "sqls", ".", "append", "(", "sql_parsed", ")", "\n", "", "", "return", "sqls", "\n", "", "m_num", "=", "0", "\n", "index_list", "=", "[", "]", "\n", "index", "=", "0", "\n", "for", "i", ",", "j", "in", "zip", "(", "hyps_list", ",", "gold_list", ")", ":", "\n", "        ", "if", "get_sql_parser", "(", "i", ")", "!=", "[", "]", "and", "get_sql_parser", "(", "i", ")", "==", "get_sql_parser", "(", "j", ")", ":", "\n", "# print(i, j)", "\n", "# print(\"---------------\")", "\n", "            ", "m_num", "+=", "1", "\n", "index_list", ".", "append", "(", "index", ")", "\n", "", "index", "+=", "1", "\n", "", "if", "need_index", "==", "True", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", ",", "\" \"", ".", "join", "(", "[", "str", "(", "k", ")", "for", "k", "in", "index_list", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "m_num", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "", "def", "code_staticAnaylsis", "(", "code", ",", "id", ")", ":", "\n", "    ", "cur_time", "=", "str", "(", "time", ".", "time", "(", ")", ")", ".", "replace", "(", "\".\"", ",", "\"\"", ")", "\n", "with", "open", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"# pylint: disable=E1101\\n\"", ")", "\n", "f", ".", "write", "(", "code", ")", "\n", "", "(", "pylint_stdout", ",", "pylint_stderr", ")", "=", "lint", ".", "py_run", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py -s yes\"", ",", "return_std", "=", "True", ")", "\n", "os", ".", "remove", "(", "cur_time", "+", "str", "(", "id", ")", "+", "\".py\"", ")", "\n", "pylint_stdout_str", "=", "pylint_stdout", ".", "read", "(", ")", "\n", "# pylint_stderr_str = pylint_stderr.read()", "\n", "if", "\"E0\"", "in", "pylint_stdout_str", "or", "\"E1\"", "in", "pylint_stdout_str", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "get_executable_rate", "(", "hyps_list", ")", ":", "\n", "    ", "executable_wrong_num", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "hyps_list", ")", ")", ":", "\n", "        ", "if", "'<unk>'", "not", "in", "hyps_list", "[", "i", "]", ":", "\n", "            ", "if", "code_staticAnaylsis", "(", "hyps_list", "[", "i", "]", ".", "replace", "(", "\"\\t\"", ",", "\"    \"", ")", ",", "i", ")", ":", "\n", "                ", "executable_wrong_num", "+=", "1", "\n", "", "", "else", ":", "\n", "            ", "executable_wrong_num", "+=", "1", "\n", "", "", "return", "(", "len", "(", "hyps_list", ")", "-", "executable_wrong_num", ")", "/", "len", "(", "hyps_list", ")", "\n", "\n", "", "def", "sqlparse", "(", "s", ")", ":", "\n", "    ", "s", "=", "s", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ".", "replace", "(", "\"\\\\\"", ",", "\" \"", ")", "\n", "# split index", "\n", "se", "=", "re", ".", "findall", "(", "r\"(SELECT )|(select )\"", ",", "s", ")", "\n", "fr", "=", "re", ".", "findall", "(", "r\"( FROM )|( from )\"", ",", "s", ")", "\n", "wh", "=", "re", ".", "findall", "(", "r\"( WHERE )|( where )\"", ",", "s", ")", "\n", "try", ":", "\n", "        ", "assert", "len", "(", "se", ")", "==", "len", "(", "fr", ")", "==", "1", "\n", "", "except", ":", "\n", "        ", "return", "s", "\n", "", "s_index", "=", "s", ".", "index", "(", "se", "[", "0", "]", "[", "0", "]", "if", "se", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "se", "[", "0", "]", "[", "1", "]", ")", "\n", "f_index", "=", "s", ".", "index", "(", "fr", "[", "0", "]", "[", "0", "]", "if", "fr", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "fr", "[", "0", "]", "[", "1", "]", ")", "\n", "if", "wh", "!=", "[", "]", ":", "\n", "        ", "w_index", "=", "s", ".", "index", "(", "wh", "[", "0", "]", "[", "0", "]", "if", "wh", "[", "0", "]", "[", "0", "]", "!=", "\"\"", "else", "wh", "[", "0", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "w_index", "=", "9999", "\n", "\n", "# split tokens", "\n", "", "query_tokens", "=", "[", "]", "\n", "from_toknes", "=", "[", "]", "\n", "where_tokens", "=", "[", "]", "\n", "for", "i", "in", "tokenize", ".", "tokenize", "(", "s", ",", "version_info", ")", ":", "\n", "# print(i)", "\n", "        ", "if", "i", ".", "type", "in", "[", "tokenize", ".", "INDENT", ",", "tokenize", ".", "DEDENT", ",", "tokenize", ".", "ENDMARKER", "]", ":", "\n", "            ", "continue", "\n", "", "if", "i", ".", "start_pos", "[", "1", "]", "<", "f_index", ":", "\n", "            ", "query_tokens", ".", "append", "(", "i", ")", "\n", "", "elif", "i", ".", "start_pos", "[", "1", "]", "<", "w_index", ":", "\n", "            ", "from_toknes", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "where_tokens", ".", "append", "(", "i", ")", "\n", "\n", "# condition list", "\n", "", "", "if", "len", "(", "where_tokens", ")", "!=", "0", ":", "\n", "#where_list = [where_tokens[0].string]", "\n", "        ", "where_list", "=", "[", "]", "\n", "temp", "=", "[", "]", "\n", "temp_op", "=", "''", "\n", "for", "i", "in", "where_tokens", "[", "1", ":", "]", ":", "\n", "            ", "if", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "==", "','", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "", "elif", "i", ".", "type", "==", "tokenize", ".", "OP", "and", "i", ".", "string", "!=", "':'", ":", "\n", "                ", "temp", ".", "append", "(", "{", "\"OP\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "i", ".", "string", "\n", "", "elif", "i", ".", "type", "!=", "tokenize", ".", "OP", ":", "\n", "                ", "if", "temp_op", "!=", "''", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"PARAMETER\"", ":", "i", ".", "string", "}", ")", "\n", "temp_op", "=", "''", "\n", "", "else", ":", "\n", "                    ", "temp", ".", "append", "(", "{", "\"COLUMN\"", ":", "i", ".", "string", "}", ")", "\n", "", "", "", "cond", "=", "{", "}", "\n", "cond_list", "=", "[", "]", "\n", "# print(\"temp: \",temp)", "\n", "for", "i", "in", "range", "(", "len", "(", "temp", ")", ")", ":", "\n", "# print(temp[i])", "\n", "            ", "if", "(", "i", "+", "1", ")", "%", "4", "==", "0", ":", "\n", "                ", "cond_list", ".", "append", "(", "cond", ")", "\n"]]}