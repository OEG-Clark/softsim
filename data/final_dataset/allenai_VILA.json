{"home.repos.pwc.inspect_result.allenai_VILA.None.setup.get_requirements": [[5, 14], ["open", "fp.readlines", "line.startswith", "reqs.append", "line.strip", "line.strip"], "function", ["None"], ["def", "get_requirements", "(", "req_file", ")", ":", "\n", "    ", "reqs", "=", "[", "]", "\n", "with", "open", "(", "req_file", ",", "\"r\"", ")", "as", "fp", ":", "\n", "        ", "for", "line", "in", "fp", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "line", ".", "startswith", "(", "\"#\"", ")", "or", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "reqs", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "", "return", "reqs", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.__init__": [[95, 108], ["predictors.BasePDFPredictor.model.eval", "predictors.columns_used_in_model_inputs", "model.to"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.columns_used_in_model_inputs"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "preprocessor", ",", "device", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "preprocessor", "=", "preprocessor", "\n", "self", ".", "id2label", "=", "self", ".", "model", ".", "config", ".", "id2label", "\n", "\n", "if", "device", "is", "None", ":", "\n", "            ", "self", ".", "device", "=", "model", ".", "device", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "device", "\n", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "_used_cols", "=", "columns_used_in_model_inputs", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.from_pretrained": [[109, 124], ["automodel.AutoModelForTokenClassification.from_pretrained", "automodel.AutoTokenizer.from_pretrained", "cls", "dataset.preprocessors.VILAPreprocessorConfig.from_pretrained", "cls.initialize_preprocessor"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.HierarchicalPDFPredictor.initialize_preprocessor"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "\n", "cls", ",", "model_path", ",", "preprocessor", "=", "None", ",", "device", "=", "None", ",", "**", "preprocessor_config", "\n", ")", ":", "\n", "\n", "        ", "model", "=", "AutoModelForTokenClassification", ".", "from_pretrained", "(", "model_path", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_path", ")", "\n", "\n", "if", "preprocessor", "is", "None", ":", "\n", "            ", "preprocessor_config", "=", "VILAPreprocessorConfig", ".", "from_pretrained", "(", "\n", "model_path", ",", "**", "preprocessor_config", "\n", ")", "\n", "preprocessor", "=", "cls", ".", "initialize_preprocessor", "(", "tokenizer", ",", "preprocessor_config", ")", "\n", "\n", "", "return", "cls", "(", "model", ",", "preprocessor", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.initialize_preprocessor": [[125, 129], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "@", "abstractmethod", "\n", "def", "initialize_preprocessor", "(", "tokenizer", ",", "config", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict": [[130, 171], ["predictors.BasePDFPredictor.preprocess_pdf_data", "predictors.BasePDFPredictor.model_input_collator", "numpy.vstack", "predictors.BasePDFPredictor.postprocess_model_outputs", "predictors.BasePDFPredictor.model", "numpy.vstack.append", "predictors.BasePDFPredictor.get_category_prediction"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.preprocess_pdf_data", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.model_input_collator", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.HierarchicalPDFPredictor.postprocess_model_outputs", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.get_category_prediction"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "page_data", ":", "Dict", ",", "\n", "page_size", ":", "Tuple", ",", "\n", "batch_size", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "return_type", ":", "Optional", "[", "str", "]", "=", "\"layout\"", ",", "\n", ")", "->", "Union", "[", "lp", ".", "Layout", ",", "List", "]", ":", "\n", "        ", "\"\"\"This is a generalized predict function that runs vila on a PDF page.\n\n        Args:\n            page_data (Dict):\n                The page-level data as a dict in the form of\n                {\n                    'words': ['word1', 'word2', ...],\n                    'bbox': [[x1, y1, x2, y2], [x1, y1, x2, y2], ...],\n                    'block_ids': [0, 0, 0, 1 ...],\n                    'line_ids': [0, 1, 1, 2 ...],\n                    'labels': [0, 0, 0, 1 ...], # could be empty\n                }\n            page_size (Tuple):\n                A tuple of (width, height) for this page\n            batch_size (Optional[int]):\n                Specifying the maximum number of batches for each model run.\n                By default it will encode all pages all at once.\n            return_type (Optional[str]):\n                It can be either \"layout\", for a structured token output,\n                or \"list\" for a list of predicted classes. Default is \"layout\".\n        \"\"\"", "\n", "\n", "# page_size is (page_token.width, page_token.height)", "\n", "model_inputs", "=", "self", ".", "preprocess_pdf_data", "(", "page_data", ",", "page_size", ")", "\n", "batched_inputs", "=", "self", ".", "model_input_collator", "(", "model_inputs", ",", "batch_size", ")", "\n", "\n", "model_predictions", "=", "[", "]", "\n", "for", "batch", "in", "batched_inputs", ":", "\n", "            ", "model_outputs", "=", "self", ".", "model", "(", "**", "batch", ")", "\n", "model_predictions", ".", "append", "(", "self", ".", "get_category_prediction", "(", "model_outputs", ")", ")", "\n", "\n", "", "model_predictions", "=", "np", ".", "vstack", "(", "model_predictions", ")", "\n", "return", "self", ".", "postprocess_model_outputs", "(", "\n", "page_data", ",", "model_inputs", ",", "model_predictions", ",", "return_type", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict_page": [[173, 223], ["copy.copy", "copy.copy.to_pagedata().to_dict", "predictors.BasePDFPredictor.predict", "getattr", "warnings.warn", "visual_group_detector.detect", "copy.copy.annotate", "ValueError", "copy.copy.to_pagedata"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata"], ["", "def", "predict_page", "(", "\n", "self", ",", "\n", "page_tokens", ":", "PDFPlumberPageData", ",", "\n", "page_image", ":", "Optional", "[", "\"PIL.Image\"", "]", "=", "None", ",", "\n", "visual_group_detector", ":", "Optional", "[", "Any", "]", "=", "None", ",", "\n", "page_size", "=", "None", ",", "\n", "batch_size", "=", "None", ",", "\n", ")", "->", "lp", ".", "Layout", ":", "\n", "        ", "\"\"\"The predict_page function is used for running the model on a single page\n        in the vila page_token objects.\n\n        Args:\n            page_tokens (PDFPlumberPageData):\n                The page-level data as an PDFPlumberPageData object.\n            visual_group_detector:\n                The visual group model to use for detecting the required visual groups.\n            page_size (Tuple):\n                A tuple of (width, height) for this page. By default it will use the \n                page_size from the page_tokens directly unless the page_size is explicitly \n                specified.\n            batch_size (Optional[int]):\n                Specifying the maximum number of batches for each model run.\n                By default it will encode all pages all at once.\n        \"\"\"", "\n", "page_tokens", "=", "copy", ".", "copy", "(", "page_tokens", ")", "\n", "required_agg_level", "=", "self", ".", "preprocessor", ".", "config", ".", "agg_level", "\n", "required_group", "=", "AGG_LEVEL_TO_GROUP_NAME", "[", "required_agg_level", "]", "\n", "\n", "if", "not", "getattr", "(", "page_tokens", ",", "required_group", "+", "\"s\"", ")", ":", "# either none or empty", "\n", "            ", "if", "page_image", "is", "not", "None", "and", "visual_group_detector", "is", "not", "None", ":", "\n", "                ", "warnings", ".", "warn", "(", "\n", "f\"The required_group {required_group} is missing in page_tokens.\"", "\n", "f\"Using the page_image and visual_group_detector to detect.\"", "\n", ")", "\n", "detected_groups", "=", "visual_group_detector", ".", "detect", "(", "page_image", ")", "\n", "page_tokens", ".", "annotate", "(", "**", "{", "required_group", "+", "\"s\"", ":", "detected_groups", "}", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"The required_group {required_group} is missing in page_tokens.\"", "\n", ")", "\n", "\n", "", "", "pdf_data", "=", "page_tokens", ".", "to_pagedata", "(", ")", ".", "to_dict", "(", ")", "\n", "predicted_tokens", "=", "self", ".", "predict", "(", "\n", "page_data", "=", "pdf_data", ",", "\n", "page_size", "=", "page_tokens", ".", "page_size", "if", "page_size", "is", "None", "else", "page_size", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "return_type", "=", "\"layout\"", ",", "\n", ")", "\n", "\n", "return", "predicted_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.get_category_prediction": [[224, 227], ["model_outputs.logits.argmax().cpu().detach().numpy", "model_outputs.logits.argmax().cpu().detach", "model_outputs.logits.argmax().cpu", "model_outputs.logits.argmax"], "methods", ["None"], ["", "def", "get_category_prediction", "(", "self", ",", "model_outputs", ")", ":", "\n", "        ", "predictions", "=", "model_outputs", ".", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "return", "predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.preprocess_pdf_data": [[228, 243], ["pdf_data.get", "predictors.BasePDFPredictor.preprocessor.preprocess_sample", "len", "predictors.normalize_bbox"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.normalize_bbox"], ["", "def", "preprocess_pdf_data", "(", "self", ",", "pdf_data", ",", "page_size", ")", ":", "\n", "        ", "_labels", "=", "pdf_data", ".", "get", "(", "\"labels\"", ")", "\n", "pdf_data", "[", "\"labels\"", "]", "=", "[", "0", "]", "*", "len", "(", "pdf_data", "[", "\"words\"", "]", ")", "\n", "page_width", ",", "page_height", "=", "page_size", "\n", "\n", "sample", "=", "self", ".", "preprocessor", ".", "preprocess_sample", "(", "pdf_data", ")", "\n", "sample", "[", "\"bbox\"", "]", "=", "[", "\n", "[", "normalize_bbox", "(", "bbox", ",", "page_width", ",", "page_height", ")", "for", "bbox", "in", "batch", "]", "\n", "for", "batch", "in", "sample", "[", "\"bbox\"", "]", "\n", "]", "\n", "\n", "# Change back to the original pdf_data", "\n", "pdf_data", "[", "\"labels\"", "]", "=", "_labels", "\n", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.model_input_collator": [[244, 257], ["len", "range", "len", "torch.tensor().type", "list", "sample.items", "sample.keys", "list", "torch.tensor", "sample.keys"], "methods", ["None"], ["", "def", "model_input_collator", "(", "self", ",", "sample", ",", "batch_size", "=", "None", ")", ":", "\n", "\n", "        ", "n_samples", "=", "len", "(", "sample", "[", "list", "(", "sample", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "len", "(", "sample", "[", "list", "(", "sample", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "\n", "", "for", "idx", "in", "range", "(", "0", ",", "n_samples", ",", "batch_size", ")", ":", "\n", "            ", "yield", "{", "\n", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", ":", "idx", "+", "batch_size", "]", ",", "device", "=", "self", ".", "device", ")", ".", "type", "(", "\n", "torch", ".", "int64", "\n", ")", "# Make the conversion more robust", "\n", "for", "key", ",", "val", "in", "sample", ".", "items", "(", ")", "\n", "if", "key", "in", "self", ".", "_used_cols", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.postprocess_model_outputs": [[259, 262], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "postprocess_model_outputs", "(", "self", ",", "pdf_data", ",", "model_inputs", ",", "model_predictions", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.SimplePDFPredictor.initialize_preprocessor": [[267, 270], ["dataset.preprocessors.instantiate_dataset_preprocessor"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor"], ["@", "staticmethod", "\n", "def", "initialize_preprocessor", "(", "tokenizer", ",", "config", ")", ":", "\n", "        ", "return", "instantiate_dataset_preprocessor", "(", "\"base\"", ",", "tokenizer", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.SimplePDFPredictor.postprocess_model_outputs": [[271, 299], ["list", "itertools.chain.from_iterable", "predictors.SimplePDFPredictor.id2label.get", "zip", "zip", "layoutparser.Layout", "zip", "generated_tokens.append", "layoutparser.TextBlock", "layoutparser.Rectangle"], "methods", ["None"], ["", "def", "postprocess_model_outputs", "(", "\n", "self", ",", "pdf_data", ",", "model_inputs", ",", "model_predictions", ",", "return_type", "\n", ")", ":", "\n", "\n", "        ", "encoded_labels", "=", "model_inputs", "[", "\"labels\"", "]", "\n", "\n", "true_predictions", "=", "[", "\n", "[", "(", "p", ",", "l", ")", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "model_predictions", ",", "encoded_labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "preds", "=", "[", "self", ".", "id2label", ".", "get", "(", "ele", "[", "0", "]", ",", "ele", "[", "0", "]", ")", "for", "ele", "in", "true_predictions", "]", "\n", "\n", "if", "return_type", "==", "\"list\"", ":", "\n", "            ", "return", "preds", "\n", "\n", "", "elif", "return_type", "==", "\"layout\"", ":", "\n", "            ", "words", "=", "[", "pdf_data", "[", "\"words\"", "]", "[", "idx", "]", "for", "idx", "in", "model_inputs", "[", "\"encoded_word_ids\"", "]", "]", "\n", "bboxes", "=", "[", "pdf_data", "[", "\"bbox\"", "]", "[", "idx", "]", "for", "idx", "in", "model_inputs", "[", "\"encoded_word_ids\"", "]", "]", "\n", "\n", "generated_tokens", "=", "[", "]", "\n", "for", "word", ",", "pred", ",", "bbox", "in", "zip", "(", "words", ",", "preds", ",", "bboxes", ")", ":", "\n", "                ", "generated_tokens", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "block", "=", "lp", ".", "Rectangle", "(", "*", "bbox", ")", ",", "text", "=", "word", ",", "type", "=", "pred", ")", "\n", ")", "\n", "\n", "", "return", "lp", ".", "Layout", "(", "generated_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.LayoutIndicatorPDFPredictor.initialize_preprocessor": [[305, 308], ["dataset.preprocessors.instantiate_dataset_preprocessor"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor"], ["@", "staticmethod", "\n", "def", "initialize_preprocessor", "(", "tokenizer", ",", "config", ")", ":", "\n", "        ", "return", "instantiate_dataset_preprocessor", "(", "\"layout_indicator\"", ",", "tokenizer", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.HierarchicalPDFPredictor.initialize_preprocessor": [[313, 317], ["dataset.preprocessors.instantiate_dataset_preprocessor"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor"], ["@", "staticmethod", "\n", "def", "initialize_preprocessor", "(", "tokenizer", ",", "config", ")", ":", "\n", "        ", "return", "instantiate_dataset_preprocessor", "(", "\n", "\"hierarchical_modeling\"", ",", "tokenizer", ",", "config", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.HierarchicalPDFPredictor.postprocess_model_outputs": [[319, 349], ["predictors.flatten_group_level_prediction", "predictors.HierarchicalPDFPredictor.id2label.get", "zip", "zip", "layoutparser.Layout", "zip", "generated_tokens.append", "layoutparser.TextBlock", "layoutparser.Rectangle"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.flatten_group_level_prediction"], ["", "def", "postprocess_model_outputs", "(", "\n", "self", ",", "pdf_data", ",", "model_inputs", ",", "model_predictions", ",", "return_type", "\n", ")", ":", "\n", "\n", "        ", "encoded_labels", "=", "model_inputs", "[", "\"labels\"", "]", "\n", "\n", "true_predictions", "=", "[", "\n", "[", "(", "p", ",", "l", ")", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "model_predictions", ",", "encoded_labels", ")", "\n", "]", "\n", "\n", "flatten_predictions", "=", "flatten_group_level_prediction", "(", "\n", "true_predictions", ",", "model_inputs", "[", "\"group_word_count\"", "]", "\n", ")", "\n", "\n", "preds", "=", "[", "self", ".", "id2label", ".", "get", "(", "ele", "[", "0", "]", ",", "ele", "[", "0", "]", ")", "for", "ele", "in", "flatten_predictions", "]", "\n", "if", "return_type", "==", "\"list\"", ":", "\n", "            ", "return", "preds", "\n", "\n", "", "elif", "return_type", "==", "\"layout\"", ":", "\n", "            ", "words", "=", "pdf_data", "[", "\"words\"", "]", "\n", "bboxes", "=", "pdf_data", "[", "\"bbox\"", "]", "\n", "\n", "generated_tokens", "=", "[", "]", "\n", "for", "word", ",", "pred", ",", "bbox", "in", "zip", "(", "words", ",", "preds", ",", "bboxes", ")", ":", "\n", "                ", "generated_tokens", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "block", "=", "lp", ".", "Rectangle", "(", "*", "bbox", ")", ",", "text", "=", "word", ",", "type", "=", "pred", ")", "\n", ")", "\n", "\n", "", "return", "lp", ".", "Layout", "(", "generated_tokens", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.columns_used_in_model_inputs": [[27, 31], ["inspect.signature", "list", "inspect.signature.parameters.keys"], "function", ["None"], ["def", "columns_used_in_model_inputs", "(", "model", ")", ":", "\n", "    ", "signature", "=", "inspect", ".", "signature", "(", "model", ".", "forward", ")", "\n", "signature_columns", "=", "list", "(", "signature", ".", "parameters", ".", "keys", "(", ")", ")", "\n", "return", "signature_columns", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.flatten_group_level_prediction": [[33, 43], ["zip", "list", "zip", "itertools.chain.from_iterable", "len", "len", "final_flatten_pred.append"], "function", ["None"], ["", "def", "flatten_group_level_prediction", "(", "batched_group_pred", ",", "batched_group_word_count", ")", ":", "\n", "    ", "final_flatten_pred", "=", "[", "]", "\n", "for", "group_pred", ",", "group_word_count", "in", "zip", "(", "\n", "batched_group_pred", ",", "batched_group_word_count", "\n", ")", ":", "\n", "        ", "assert", "len", "(", "group_pred", ")", "==", "len", "(", "group_word_count", ")", "\n", "for", "(", "pred", ",", "label", ")", ",", "(", "line_id", ",", "count", ")", "in", "zip", "(", "group_pred", ",", "group_word_count", ")", ":", "\n", "            ", "final_flatten_pred", ".", "append", "(", "[", "[", "pred", ",", "label", ",", "line_id", "]", "]", "*", "count", ")", "\n", "\n", "", "", "return", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "final_flatten_pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.normalize_bbox": [[45, 68], ["float", "float", "float", "float"], "function", ["None"], ["", "def", "normalize_bbox", "(", "\n", "bbox", ",", "\n", "page_width", ",", "\n", "page_height", ",", "\n", "target_width", "=", "MODEL_PDF_WIDTH", ",", "\n", "target_height", "=", "MODEL_PDF_HEIGHT", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Normalize bounding box to the target size.\n    \"\"\"", "\n", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "bbox", "\n", "\n", "# Right now only execute this for only \"large\" PDFs", "\n", "# TODO: Change it for all PDFs", "\n", "if", "page_width", ">", "target_width", "or", "page_height", ">", "target_height", ":", "\n", "\n", "        ", "x1", "=", "float", "(", "x1", ")", "/", "page_width", "*", "target_width", "\n", "x2", "=", "float", "(", "x2", ")", "/", "page_width", "*", "target_width", "\n", "y1", "=", "float", "(", "y1", ")", "/", "page_height", "*", "target_height", "\n", "y2", "=", "float", "(", "y2", ")", "/", "page_height", "*", "target_height", "\n", "\n", "", "return", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.unnormalize_bbox": [[70, 92], ["float", "float", "float", "float"], "function", ["None"], ["", "def", "unnormalize_bbox", "(", "\n", "bbox", ",", "\n", "page_width", ",", "\n", "page_height", ",", "\n", "target_width", "=", "MODEL_PDF_WIDTH", ",", "\n", "target_height", "=", "MODEL_PDF_HEIGHT", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Unnormalize bounding box to the target size.\n    \"\"\"", "\n", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "bbox", "\n", "\n", "# Right now only execute this for only \"large\" PDFs", "\n", "# TODO: Change it for all PDFs", "\n", "if", "page_width", ">", "target_width", "or", "page_height", ">", "target_height", ":", "\n", "        ", "x1", "=", "float", "(", "x1", ")", "/", "target_width", "*", "page_width", "\n", "x2", "=", "float", "(", "x2", ")", "/", "target_width", "*", "page_width", "\n", "y1", "=", "float", "(", "y1", ")", "/", "target_height", "*", "page_height", "\n", "y2", "=", "float", "(", "y2", ")", "/", "target_height", "*", "page_height", "\n", "\n", "", "return", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.union_box": [[8, 20], ["len", "logging.warning", "float", "float", "float", "float", "min", "min", "max", "max", "int", "int", "int", "int"], "function", ["None"], ["def", "union_box", "(", "blocks", ")", "->", "List", ":", "\n", "    ", "if", "len", "(", "blocks", ")", "==", "0", ":", "\n", "        ", "logging", ".", "warning", "(", "\"The length of blocks is 0!\"", ")", "\n", "return", "[", "0", ",", "0", ",", "0", ",", "0", "]", "\n", "\n", "", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"-inf\"", ")", ",", "float", "(", "\"-inf\"", ")", "\n", "for", "bbox", "in", "blocks", ":", "\n", "        ", "x1", "=", "min", "(", "x1", ",", "bbox", "[", "0", "]", ")", "\n", "y1", "=", "min", "(", "y1", ",", "bbox", "[", "1", "]", ")", "\n", "x2", "=", "max", "(", "x2", ",", "bbox", "[", "2", "]", ")", "\n", "y2", "=", "max", "(", "y2", ",", "bbox", "[", "3", "]", ")", "\n", "", "return", "[", "int", "(", "x1", ")", ",", "int", "(", "y1", ")", ",", "int", "(", "x2", ")", ",", "int", "(", "y2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.union_lp_box": [[22, 34], ["layoutparser.TextBlock", "float", "float", "float", "float", "min", "min", "max", "max", "layoutparser.Rectangle"], "function", ["None"], ["", "def", "union_lp_box", "(", "blocks", ":", "List", "[", "lp", ".", "TextBlock", "]", ")", "->", "List", ":", "\n", "\n", "    ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"-inf\"", ")", ",", "float", "(", "\"-inf\"", ")", "\n", "\n", "for", "bbox", "in", "blocks", ":", "\n", "        ", "_x1", ",", "_y1", ",", "_x2", ",", "_y2", "=", "bbox", ".", "coordinates", "\n", "x1", "=", "min", "(", "x1", ",", "_x1", ")", "\n", "y1", "=", "min", "(", "y1", ",", "_y1", ")", "\n", "x2", "=", "max", "(", "x2", ",", "_x2", ")", "\n", "y2", "=", "max", "(", "y2", ",", "_y2", ")", "\n", "\n", "", "return", "lp", ".", "TextBlock", "(", "lp", ".", "Rectangle", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.is_in": [[36, 44], ["block_a.is_in"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "def", "is_in", "(", "block_a", ",", "block_b", ")", ":", "\n", "    ", "\"\"\"A rewrite of the lp.LayoutElement.is_in function.\n    We will use a soft_margin and center function by default.\n    \"\"\"", "\n", "return", "block_a", ".", "is_in", "(", "\n", "block_b", ",", "\n", "soft_margin", "=", "{", "\"top\"", ":", "1", ",", "\"bottom\"", ":", "1", ",", "\"left\"", ":", "1", ",", "\"right\"", ":", "1", "}", ",", "\n", "center", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.assign_tokens_to_blocks": [[47, 102], ["copy.deepcopy", "copy.deepcopy", "enumerate", "sorted", "enumerate", "copy.deepcopy.extend", "copy.deepcopy.append", "copy.deepcopy.extend", "sorted", "utils.is_in", "all_token_groups.append", "token_group.append", "remaining_tokens.append", "len", "min", "float", "len"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "def", "assign_tokens_to_blocks", "(", "\n", "blocks", ":", "List", ",", "tokens", ":", "List", ",", "keep_empty_blocks", "=", "False", "\n", ")", "->", "Tuple", "[", "List", ",", "List", "]", ":", "\n", "    ", "\"\"\"It will assign the token to the corresponding blocks,\n    sort the blocks based on the token ids (the blocks are\n    ordered based on the minimum id of the contained tokens),\n    and then assign the correspinding block_id to the tokens \n    within the block. \n    \n    It will return a Tuple for blocks (in correct order) and \n    tokens (with block_id assigned and in the original order).\n    \"\"\"", "\n", "blocks", "=", "deepcopy", "(", "blocks", ")", "\n", "tokens", "=", "deepcopy", "(", "tokens", ")", "\n", "left_tokens_last", "=", "tokens", "\n", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "token", ".", "id", "=", "idx", "\n", "\n", "", "all_token_groups", "=", "[", "]", "\n", "\n", "for", "block", "in", "blocks", ":", "\n", "        ", "token_group", "=", "[", "]", "\n", "remaining_tokens", "=", "[", "]", "\n", "for", "token", "in", "left_tokens_last", ":", "\n", "            ", "if", "is_in", "(", "token", ",", "block", ")", ":", "\n", "                ", "token_group", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "remaining_tokens", ".", "append", "(", "token", ")", "\n", "", "", "if", "len", "(", "token_group", ")", ">", "0", "or", "keep_empty_blocks", ":", "\n", "            ", "all_token_groups", ".", "append", "(", "(", "block", ",", "token_group", ")", ")", "\n", "\n", "", "left_tokens_last", "=", "remaining_tokens", "\n", "\n", "", "remaining_tokens", "=", "[", "token", "for", "token", "in", "left_tokens_last", "]", "\n", "sorted_token_groups", "=", "sorted", "(", "\n", "all_token_groups", ",", "\n", "key", "=", "lambda", "ele", ":", "min", "(", "tok", ".", "id", "for", "tok", "in", "ele", "[", "1", "]", ")", "\n", "if", "len", "(", "ele", "[", "1", "]", ")", ">", "0", "# Avoid empty blocks", "\n", "else", "float", "(", "\"-inf\"", ")", ",", "\n", ")", "\n", "\n", "blocks", ",", "tokens", "=", "[", "]", ",", "[", "]", "\n", "for", "bid", ",", "(", "block", ",", "gp_tokens", ")", "in", "enumerate", "(", "sorted_token_groups", ")", ":", "\n", "        ", "block", ".", "id", "=", "bid", "\n", "blocks", ".", "append", "(", "block", ")", "\n", "for", "token", "in", "gp_tokens", ":", "\n", "            ", "token", ".", "block_id", "=", "bid", "\n", "", "tokens", ".", "extend", "(", "gp_tokens", ")", "\n", "\n", "", "for", "token", "in", "remaining_tokens", ":", "\n", "        ", "token", ".", "block_id", "=", "None", "\n", "", "tokens", ".", "extend", "(", "remaining_tokens", ")", "\n", "\n", "return", "blocks", ",", "sorted", "(", "tokens", ",", "key", "=", "lambda", "ele", ":", "ele", ".", "id", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_text_segments": [[25, 66], ["lines.append", "token_in_this_line.append", "lines.append", "abs"], "methods", ["None"], ["def", "get_text_segments", "(", "self", ",", "x_tolerance", "=", "10", ",", "y_tolerance", "=", "10", ")", "->", "List", "[", "List", "]", ":", "\n", "        ", "\"\"\"Get text segments from the current page.\n        It will automatically add new lines for 1) line breaks\n        2) big horizontal gaps\n        \"\"\"", "\n", "prev_y", "=", "None", "\n", "prev_x", "=", "None", "\n", "\n", "lines", "=", "[", "]", "\n", "token_in_this_line", "=", "[", "]", "\n", "n", "=", "0", "\n", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "cur_y", "=", "token", ".", "block", ".", "center", "[", "1", "]", "\n", "cur_x", "=", "token", ".", "coordinates", "[", "0", "]", "\n", "\n", "if", "prev_y", "is", "None", ":", "\n", "                ", "prev_y", "=", "cur_y", "\n", "prev_x", "=", "cur_x", "\n", "\n", "", "if", "abs", "(", "cur_y", "-", "prev_y", ")", "<=", "y_tolerance", "and", "cur_x", "-", "prev_x", "<=", "x_tolerance", ":", "\n", "\n", "                ", "token_in_this_line", ".", "append", "(", "token", ")", "\n", "if", "n", "==", "0", ":", "\n", "                    ", "prev_y", "=", "cur_y", "\n", "", "else", ":", "\n", "                    ", "prev_y", "=", "(", "prev_y", "*", "n", "+", "cur_y", ")", "/", "(", "n", "+", "1", ")", "\n", "", "n", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                ", "lines", ".", "append", "(", "token_in_this_line", ")", "\n", "token_in_this_line", "=", "[", "token", "]", "\n", "n", "=", "1", "\n", "prev_y", "=", "cur_y", "\n", "\n", "", "prev_x", "=", "token", ".", "coordinates", "[", "2", "]", "\n", "\n", "", "if", "token_in_this_line", ":", "\n", "            ", "lines", ".", "append", "(", "token_in_this_line", ")", "\n", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_text": [[67, 76], ["pdfplumber_extractor.PDFPlumberPageData.get_text_segments"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_text_segments"], ["", "def", "get_text", "(", "self", ",", "x_tolerance", "=", "10", ",", "y_tolerance", "=", "10", ")", "->", "str", ":", "\n", "        ", "\"\"\"Returns the page text by instering '\\n' between text segments\n        returned by `self.get_text_segments` .\n        \"\"\"", "\n", "\n", "return", "\"\\n\"", ".", "join", "(", "\n", "[", "\n", "\" \"", ".", "join", "(", "[", "e", ".", "text", "for", "e", "in", "ele", "]", ")", "\n", "for", "ele", "in", "self", ".", "get_text_segments", "(", "x_tolerance", ",", "y_tolerance", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_lines": [[79, 96], ["enumerate", "layoutparser.Layout", "pdfplumber_extractor.PDFPlumberPageData.get_text_segments", "utils.union_lp_box().set", "lines.append", "utils.union_lp_box", "hasattr"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_text_segments", "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.union_lp_box"], ["", "def", "get_lines", "(", "self", ",", "x_tolerance", "=", "10", ",", "y_tolerance", "=", "10", ")", "->", "lp", ".", "Layout", ":", "\n", "        ", "\"\"\"Get the text line bounding boxes from the current page.\"\"\"", "\n", "\n", "lines", "=", "[", "]", "\n", "for", "idx", ",", "line_tokens", "in", "enumerate", "(", "\n", "self", ".", "get_text_segments", "(", "x_tolerance", ",", "y_tolerance", ")", "\n", ")", ":", "\n", "            ", "line", "=", "union_lp_box", "(", "line_tokens", ")", ".", "set", "(", "id", "=", "idx", ")", "\n", "lines", ".", "append", "(", "line", ")", "\n", "for", "t", "in", "line_tokens", ":", "\n", "                ", "t", ".", "line_id", "=", "idx", "\n", "\n", "if", "not", "hasattr", "(", "t", ",", "\"block_id\"", ")", ":", "\n", "                    ", "t", ".", "block_id", "=", "None", "\n", "", "line", ".", "block_id", "=", "t", ".", "block_id", "\n", "\n", "", "", "return", "lp", ".", "Layout", "(", "lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate": [[97, 104], ["kwargs.items", "utils.assign_tokens_to_blocks", "setattr"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.vila.utils.assign_tokens_to_blocks"], ["", "def", "annotate", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "for", "key", ",", "blocks", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "[", "\"lines\"", ",", "\"blocks\"", "]", ":", "\n", "                ", "blocks", ",", "tokens", "=", "assign_tokens_to_blocks", "(", "blocks", ",", "self", ".", "tokens", ")", "\n", "setattr", "(", "self", ",", "key", ",", "blocks", ")", "\n", "self", ".", "tokens", "=", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata": [[105, 114], ["datamodel.PageData", "len", "pdfplumber_extractor.PDFPlumberPageData.get_lines"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_lines"], ["", "", "", "def", "to_pagedata", "(", "self", ",", "x_tolerance", "=", "10", ",", "y_tolerance", "=", "10", ")", ":", "\n", "        ", "\"\"\"Convert the layout to a PageData object.\"\"\"", "\n", "\n", "if", "len", "(", "self", ".", "lines", ")", "==", "0", ":", "\n", "            ", "lines", "=", "self", ".", "get_lines", "(", "x_tolerance", ",", "y_tolerance", ")", "\n", "", "else", ":", "\n", "            ", "lines", "=", "self", ".", "lines", "\n", "\n", "", "return", "PageData", "(", "words", "=", "self", ".", "tokens", ",", "lines", "=", "lines", ",", "blocks", "=", "self", ".", "blocks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.page_size": [[115, 118], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "page_size", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "width", ",", "self", ".", "height", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.convert_to_pagetoken": [[175, 185], ["dict", "row.get"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "convert_to_pagetoken", "(", "row", ":", "pd", ".", "Series", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Convert a row in a DataFrame to pagetoken\"\"\"", "\n", "return", "dict", "(", "\n", "text", "=", "row", "[", "\"text\"", "]", ",", "\n", "x", "=", "row", "[", "\"x0\"", "]", ",", "\n", "width", "=", "row", "[", "\"width\"", "]", ",", "\n", "y", "=", "row", "[", "\"top\"", "]", ",", "\n", "height", "=", "row", "[", "\"height\"", "]", ",", "\n", "font", "=", "row", ".", "get", "(", "\"fontname\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_word_tokens": [[187, 226], ["cur_page.extract_words", "pandas.DataFrame", "df[].clip().astype", "df[].clip().astype", "pandas.DataFrame.apply().tolist", "len", "df[].clip", "df[].clip", "pandas.DataFrame.apply", "int", "int"], "methods", ["None"], ["", "def", "obtain_word_tokens", "(", "self", ",", "cur_page", ":", "pdfplumber", ".", "page", ".", "Page", ")", "->", "List", "[", "Dict", "]", ":", "\n", "        ", "\"\"\"Obtain all words from the current page.\n        Args:\n            cur_page (pdfplumber.page.Page):\n                the pdfplumber.page.Page object with PDF token information\n        Returns:\n            List[PageToken]:\n                A list of page tokens stored in PageToken format.\n        \"\"\"", "\n", "words", "=", "cur_page", ".", "extract_words", "(", "\n", "x_tolerance", "=", "1.5", ",", "\n", "y_tolerance", "=", "3", ",", "\n", "keep_blank_chars", "=", "False", ",", "\n", "use_text_flow", "=", "True", ",", "\n", "horizontal_ltr", "=", "True", ",", "\n", "vertical_ttb", "=", "True", ",", "\n", "extra_attrs", "=", "[", "\"fontname\"", ",", "\"size\"", "]", ",", "\n", ")", "\n", "\n", "if", "len", "(", "words", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "df", "=", "pd", ".", "DataFrame", "(", "words", ")", "\n", "\n", "# Avoid boxes outside the page", "\n", "df", "[", "[", "\"x0\"", ",", "\"x1\"", "]", "]", "=", "(", "\n", "df", "[", "[", "\"x0\"", ",", "\"x1\"", "]", "]", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "int", "(", "cur_page", ".", "width", ")", ")", ".", "astype", "(", "\"float\"", ")", "\n", ")", "\n", "df", "[", "[", "\"top\"", ",", "\"bottom\"", "]", "]", "=", "(", "\n", "df", "[", "[", "\"top\"", ",", "\"bottom\"", "]", "]", "\n", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "int", "(", "cur_page", ".", "height", ")", ")", "\n", ".", "astype", "(", "\"float\"", ")", "\n", ")", "\n", "\n", "df", "[", "\"height\"", "]", "=", "df", "[", "\"bottom\"", "]", "-", "df", "[", "\"top\"", "]", "\n", "df", "[", "\"width\"", "]", "=", "df", "[", "\"x1\"", "]", "-", "df", "[", "\"x0\"", "]", "\n", "\n", "word_tokens", "=", "df", ".", "apply", "(", "self", ".", "convert_to_pagetoken", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", "\n", "return", "word_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_page_hyperlinks": [[227, 249], ["pandas.DataFrame", "df[].clip().astype", "df[].clip().astype", "df[].astype", "pandas.DataFrame.rename().apply().tolist", "len", "df[].clip", "df[].clip", "pandas.DataFrame.rename().apply", "int", "int", "pandas.DataFrame.rename"], "methods", ["None"], ["", "def", "obtain_page_hyperlinks", "(", "self", ",", "cur_page", ":", "pdfplumber", ".", "page", ".", "Page", ")", "->", "List", "[", "Dict", "]", ":", "\n", "\n", "        ", "if", "len", "(", "cur_page", ".", "hyperlinks", ")", "==", "0", ":", "\n", "            ", "return", "[", "]", "\n", "\n", "", "df", "=", "pd", ".", "DataFrame", "(", "cur_page", ".", "hyperlinks", ")", "\n", "df", "[", "[", "\"x0\"", ",", "\"x1\"", "]", "]", "=", "(", "\n", "df", "[", "[", "\"x0\"", ",", "\"x1\"", "]", "]", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "int", "(", "cur_page", ".", "width", ")", ")", ".", "astype", "(", "\"float\"", ")", "\n", ")", "\n", "df", "[", "[", "\"top\"", ",", "\"bottom\"", "]", "]", "=", "(", "\n", "df", "[", "[", "\"top\"", ",", "\"bottom\"", "]", "]", "\n", ".", "clip", "(", "lower", "=", "0", ",", "upper", "=", "int", "(", "cur_page", ".", "height", ")", ")", "\n", ".", "astype", "(", "\"float\"", ")", "\n", ")", "\n", "df", "[", "[", "\"height\"", ",", "\"width\"", "]", "]", "=", "df", "[", "[", "\"height\"", ",", "\"width\"", "]", "]", ".", "astype", "(", "\"float\"", ")", "\n", "\n", "hyperlink_tokens", "=", "(", "\n", "df", ".", "rename", "(", "columns", "=", "{", "\"uri\"", ":", "\"text\"", "}", ")", "\n", ".", "apply", "(", "self", ".", "convert_to_pagetoken", ",", "axis", "=", "1", ")", "\n", ".", "tolist", "(", ")", "\n", ")", "\n", "return", "hyperlink_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_page_lines": [[250, 268], ["float", "dict", "filter", "float", "float", "float", "float"], "methods", ["None"], ["", "def", "obtain_page_lines", "(", "self", ",", "cur_page", ":", "pdfplumber", ".", "page", ".", "Page", ")", "->", "List", "[", "Dict", "]", ":", "\n", "\n", "        ", "height", "=", "float", "(", "cur_page", ".", "height", ")", "\n", "page_objs", "=", "cur_page", ".", "rects", "+", "cur_page", ".", "lines", "\n", "possible_underlines", "=", "[", "\n", "dict", "(", "\n", "x", "=", "float", "(", "ele", "[", "\"x0\"", "]", ")", ",", "\n", "y", "=", "height", "-", "float", "(", "ele", "[", "\"y0\"", "]", ")", ",", "\n", "height", "=", "float", "(", "ele", "[", "\"height\"", "]", ")", ",", "\n", "width", "=", "float", "(", "ele", "[", "\"width\"", "]", ")", ",", "\n", ")", "\n", "for", "ele", "in", "filter", "(", "\n", "lambda", "obj", ":", "obj", "[", "\"height\"", "]", "<", "self", ".", "UNDERLINE_HEIGHT_THRESHOLD", "\n", "and", "obj", "[", "\"width\"", "]", "<", "self", ".", "UNDERLINE_WIDTH_THRESHOLD", ",", "\n", "page_objs", ",", "\n", ")", "\n", "]", "\n", "return", "possible_underlines", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.extract": [[269, 300], ["pdfplumber.open", "range", "pdfplumber_extractor.load_page_data_from_dict", "len", "pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_word_tokens", "pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_page_hyperlinks", "dict", "pages.append", "dict", "float", "float"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.load_page_data_from_dict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_word_tokens", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberTokenExtractor.obtain_page_hyperlinks"], ["", "def", "extract", "(", "self", ",", "pdf_path", ":", "str", ")", "->", "List", "[", "Dict", "]", ":", "\n", "        ", "\"\"\"Extracts token text, positions, and style information from a PDF file.\n        Args:\n            pdf_path (str): the path to the pdf file.\n            include_lines (bool, optional): Whether to include line tokens. Defaults to False.\n            target_data (str, optional): {\"token\", \"hyperlink\"}\n        Returns:\n            PdfAnnotations: A `PdfAnnotations` containing all the paper token information.\n        \"\"\"", "\n", "plumber_pdf_object", "=", "pdfplumber", ".", "open", "(", "pdf_path", ")", "\n", "\n", "pages", "=", "[", "]", "\n", "for", "page_id", "in", "range", "(", "len", "(", "plumber_pdf_object", ".", "pages", ")", ")", ":", "\n", "            ", "cur_page", "=", "plumber_pdf_object", ".", "pages", "[", "page_id", "]", "\n", "\n", "tokens", "=", "self", ".", "obtain_word_tokens", "(", "cur_page", ")", "\n", "url_tokens", "=", "self", ".", "obtain_page_hyperlinks", "(", "cur_page", ")", "\n", "\n", "page", "=", "dict", "(", "\n", "page", "=", "dict", "(", "\n", "width", "=", "float", "(", "cur_page", ".", "width", ")", ",", "\n", "height", "=", "float", "(", "cur_page", ".", "height", ")", ",", "\n", "index", "=", "page_id", ",", "\n", ")", ",", "\n", "tokens", "=", "tokens", ",", "\n", "url_tokens", "=", "url_tokens", ",", "\n", "lines", "=", "[", "]", ",", "\n", ")", "\n", "pages", ".", "append", "(", "page", ")", "\n", "\n", "", "return", "load_page_data_from_dict", "(", "pages", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.convert_token_dict_to_layout": [[120, 137], ["layoutparser.Layout", "layoutparser.TextBlock", "token.get", "lp_tokens.append", "layoutparser.Rectangle"], "function", ["None"], ["", "", "def", "convert_token_dict_to_layout", "(", "tokens", ")", ":", "\n", "\n", "    ", "lp_tokens", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "lp_token", "=", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "x_1", "=", "token", "[", "\"x\"", "]", ",", "\n", "y_1", "=", "token", "[", "\"y\"", "]", ",", "\n", "x_2", "=", "token", "[", "\"x\"", "]", "+", "token", "[", "\"width\"", "]", ",", "\n", "y_2", "=", "token", "[", "\"y\"", "]", "+", "token", "[", "\"height\"", "]", ",", "\n", ")", ",", "\n", "text", "=", "token", "[", "\"text\"", "]", ",", "\n", ")", "\n", "lp_token", ".", "font", "=", "token", ".", "get", "(", "\"font\"", ")", "\n", "lp_tokens", ".", "append", "(", "lp_token", ")", "\n", "\n", "", "return", "lp", ".", "Layout", "(", "lp_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.load_page_data_from_dict": [[139, 166], ["pdfplumber_extractor.PDFPlumberPageData", "page_token.get_lines", "pdfplumber_extractor.convert_token_dict_to_layout", "pdfplumber_extractor.convert_token_dict_to_layout", "layoutparser.Layout", "layoutparser.Rectangle"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.get_lines", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.convert_token_dict_to_layout", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.convert_token_dict_to_layout"], ["", "def", "load_page_data_from_dict", "(", "source_data", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "List", "[", "Dict", "]", ":", "\n", "\n", "    ", "page_data", "=", "[", "\n", "PDFPlumberPageData", "(", "\n", "height", "=", "page_data", "[", "\"page\"", "]", "[", "\"height\"", "]", ",", "\n", "width", "=", "page_data", "[", "\"page\"", "]", "[", "\"width\"", "]", ",", "\n", "tokens", "=", "convert_token_dict_to_layout", "(", "page_data", "[", "\"tokens\"", "]", ")", ",", "\n", "url_tokens", "=", "convert_token_dict_to_layout", "(", "page_data", "[", "\"url_tokens\"", "]", ")", ",", "\n", "lines", "=", "lp", ".", "Layout", "(", "\n", "[", "\n", "lp", ".", "Rectangle", "(", "\n", "x_1", "=", "line", "[", "\"x\"", "]", ",", "\n", "y_1", "=", "line", "[", "\"y\"", "]", ",", "\n", "x_2", "=", "line", "[", "\"x\"", "]", "+", "line", "[", "\"width\"", "]", ",", "\n", "y_2", "=", "line", "[", "\"y\"", "]", "+", "line", "[", "\"height\"", "]", ",", "\n", ")", "\n", "for", "line", "in", "page_data", "[", "\"lines\"", "]", "\n", "]", "\n", ")", ",", "\n", ")", "\n", "for", "page_data", "in", "source_data", "\n", "]", "\n", "\n", "for", "page_token", "in", "page_data", ":", "\n", "        ", "page_token", ".", "lines", "=", "page_token", ".", "get_lines", "(", ")", "\n", "\n", "", "return", "page_data", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.base.BasePDFTokenExtractor.__call__": [[7, 9], ["base.BasePDFTokenExtractor.extract"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.base.BasePDFTokenExtractor.extract"], ["def", "__call__", "(", "self", ",", "pdf_path", ":", "str", ")", ":", "\n", "        ", "return", "self", ".", "extract", "(", "pdf_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.base.BasePDFTokenExtractor.extract": [[10, 19], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "extract", "(", "self", ",", "pdf_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"Extract PDF Tokens from the input pdf_path\n        Args:\n            pdf_path (str):\n                The path to a PDF file\n        Returns:\n        \"\"\"", "\n", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.__post_init__": [[17, 27], ["hasattr", "hasattr", "hasattr"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "for", "w", "in", "self", ".", "words", ":", "\n", "            ", "if", "not", "hasattr", "(", "w", ",", "\"block_id\"", ")", ":", "# type: ignore", "\n", "                ", "w", ".", "block_id", "=", "None", "\n", "", "if", "not", "hasattr", "(", "w", ",", "\"line_id\"", ")", ":", "\n", "                ", "w", ".", "line_id", "=", "None", "\n", "\n", "", "", "for", "l", "in", "self", ".", "words", ":", "\n", "            ", "if", "not", "hasattr", "(", "l", ",", "\"block_id\"", ")", ":", "\n", "                ", "l", ".", "block_id", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dataframe": [[28, 104], ["pandas.DataFrame", "enumerate", "len"], "methods", ["None"], ["", "", "", "def", "to_dataframe", "(", "\n", "self", ",", "\n", "keep_token_index", "=", "True", ",", "\n", "normalize_coordinates", "=", "False", ",", "\n", "canvas_width", "=", "None", ",", "\n", "canvas_height", "=", "None", ",", "\n", ")", "->", "pd", ".", "DataFrame", ":", "\n", "\n", "        ", "blocks_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "-", "1", ",", "\n", "-", "1", ",", "\n", "True", ",", "\n", "False", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "blocks", "\n", "]", "\n", "lines_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "block_id", ",", "\n", "-", "1", ",", "\n", "False", ",", "\n", "True", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "lines", "\n", "]", "\n", "\n", "tokens_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", "if", "keep_token_index", "else", "idx", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "block_id", ",", "\n", "ele", ".", "line_id", ",", "\n", "False", ",", "\n", "False", ",", "\n", "]", "\n", "for", "idx", ",", "ele", "in", "enumerate", "(", "self", ".", "words", ",", "start", "=", "len", "(", "blocks_to_save", ")", ")", "\n", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "blocks_to_save", "+", "lines_to_save", "+", "tokens_to_save", ",", "\n", "columns", "=", "[", "\n", "\"id\"", ",", "\n", "\"x_1\"", ",", "\n", "\"y_1\"", ",", "\n", "\"x_2\"", ",", "\n", "\"y_2\"", ",", "\n", "\"text\"", ",", "\n", "\"category\"", ",", "\n", "\"block_id\"", ",", "\n", "\"line_id\"", ",", "\n", "\"is_block\"", ",", "\n", "\"is_line\"", ",", "\n", "]", ",", "\n", ")", "\n", "\n", "if", "normalize_coordinates", ":", "\n", "            ", "assert", "canvas_width", "is", "not", "None", "\n", "assert", "canvas_height", "is", "not", "None", "\n", "df", "[", "[", "\"x_1\"", ",", "\"x_2\"", "]", "]", "=", "(", "df", "[", "[", "\"x_1\"", ",", "\"x_2\"", "]", "]", "/", "canvas_width", "*", "1000", ")", ".", "astype", "(", "\n", "\"int\"", "\n", ")", "\n", "df", "[", "[", "\"y_1\"", ",", "\"y_2\"", "]", "]", "=", "(", "df", "[", "[", "\"y_1\"", ",", "\"y_2\"", "]", "]", "/", "canvas_height", "*", "1000", ")", ".", "astype", "(", "\n", "\"int\"", "\n", ")", "\n", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict": [[105, 149], ["datamodel.PageData.to_dataframe", "df.dropna.dropna.dropna", "df[].fillna().astype", "df[].fillna().astype", "len", "df[].tolist", "df.dropna.dropna.apply().tolist", "df[].tolist", "df[].tolist", "df[].map().tolist", "df[].tolist", "df[].fillna", "df[].fillna", "df.dropna.dropna.text.str.isspace", "df.dropna.dropna.apply", "df[].map"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.PageData.to_dataframe"], ["", "def", "to_dict", "(", "\n", "self", ",", "\n", "keep_token_index", "=", "True", ",", "\n", "normalize_coordinates", "=", "False", ",", "\n", "canvas_width", "=", "None", ",", "\n", "canvas_height", "=", "None", ",", "\n", "category_map", ":", "Dict", "=", "None", ",", "\n", ")", "->", "Dict", ":", "\n", "\n", "        ", "df", "=", "self", ".", "to_dataframe", "(", "\n", "keep_token_index", "=", "keep_token_index", ",", "\n", "normalize_coordinates", "=", "normalize_coordinates", ",", "\n", "canvas_width", "=", "canvas_width", ",", "\n", "canvas_height", "=", "canvas_height", ",", "\n", ")", "\n", "\n", "# Only select text bocks", "\n", "df", "=", "df", "[", "~", "df", ".", "is_block", "&", "~", "df", ".", "is_line", "]", "\n", "\n", "# Filter empty text", "\n", "df", "=", "df", ".", "dropna", "(", "axis", "=", "0", ",", "subset", "=", "[", "\"text\"", "]", ")", "\n", "df", "=", "df", "[", "~", "df", ".", "text", ".", "str", ".", "isspace", "(", ")", "]", "\n", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "df", "[", "\"block_id\"", "]", "=", "df", "[", "\"block_id\"", "]", ".", "fillna", "(", "-", "1", ")", ".", "astype", "(", "\"int\"", ")", "\n", "df", "[", "\"line_id\"", "]", "=", "df", "[", "\"line_id\"", "]", ".", "fillna", "(", "-", "1", ")", ".", "astype", "(", "\"int\"", ")", "\n", "\n", "row_item", "=", "{", "\n", "\"words\"", ":", "df", "[", "\"text\"", "]", ".", "tolist", "(", ")", ",", "\n", "\"bbox\"", ":", "df", ".", "apply", "(", "\n", "lambda", "row", ":", "(", "row", "[", "\"x_1\"", "]", ",", "row", "[", "\"y_1\"", "]", ",", "row", "[", "\"x_2\"", "]", ",", "row", "[", "\"y_2\"", "]", ")", ",", "axis", "=", "1", "\n", ")", ".", "tolist", "(", ")", ",", "\n", "\"block_ids\"", ":", "df", "[", "\"block_id\"", "]", ".", "tolist", "(", ")", ",", "\n", "\"line_ids\"", ":", "df", "[", "\"line_id\"", "]", ".", "tolist", "(", ")", ",", "\n", "}", "\n", "\n", "if", "category_map", "is", "not", "None", ":", "\n", "            ", "row_item", "[", "\"labels\"", "]", "=", "df", "[", "\"category\"", "]", ".", "map", "(", "category_map", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "row_item", "[", "\"labels\"", "]", "=", "df", "[", "\"category\"", "]", ".", "tolist", "(", ")", "\n", "\n", "", "return", "row_item", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.from_dict": [[150, 188], ["collections.defaultdict", "collections.defaultdict", "enumerate", "collections.defaultdict.pop", "collections.defaultdict.pop", "cls", "zip", "layoutparser.TextBlock", "lines[].append", "blocks[].append", "words.append", "utils.union_lp_box().set", "utils.union_lp_box().set", "sorted", "sorted", "layoutparser.Rectangle", "utils.union_lp_box", "collections.defaultdict.items", "utils.union_lp_box", "collections.defaultdict.items"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.vila.utils.union_lp_box", "home.repos.pwc.inspect_result.allenai_VILA.vila.utils.union_lp_box"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_data", ",", "default_line_id", "=", "-", "1", ",", "default_block_id", "=", "-", "1", ")", ":", "\n", "\n", "        ", "words", "=", "[", "]", "\n", "lines", "=", "defaultdict", "(", "list", ")", "\n", "blocks", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "idx", ",", "(", "word", ",", "bbox", ",", "line_id", ",", "block_id", ",", "label", ")", "in", "enumerate", "(", "zip", "(", "\n", "json_data", "[", "\"words\"", "]", ",", "json_data", "[", "\"bbox\"", "]", ",", "json_data", "[", "\"line_ids\"", "]", ",", "json_data", "[", "\"block_ids\"", "]", ",", "json_data", "[", "\"labels\"", "]", "\n", ")", ")", ":", "\n", "            ", "word", "=", "lp", ".", "TextBlock", "(", "\n", "id", "=", "idx", ",", "\n", "block", "=", "lp", ".", "Rectangle", "(", "bbox", "[", "0", "]", ",", "bbox", "[", "1", "]", ",", "bbox", "[", "2", "]", ",", "bbox", "[", "3", "]", ")", ",", "\n", "text", "=", "word", ",", "\n", "type", "=", "label", ",", "\n", ")", "\n", "word", ".", "line_id", "=", "line_id", "\n", "word", ".", "block_id", "=", "block_id", "\n", "\n", "lines", "[", "line_id", "]", ".", "append", "(", "word", ")", "\n", "blocks", "[", "block_id", "]", ".", "append", "(", "word", ")", "\n", "\n", "words", ".", "append", "(", "word", ")", "\n", "\n", "", "lines", ".", "pop", "(", "default_line_id", ",", "None", ")", "\n", "blocks", ".", "pop", "(", "default_block_id", ",", "None", ")", "\n", "\n", "lines", "=", "[", "\n", "union_lp_box", "(", "contained_words", ")", ".", "set", "(", "id", "=", "id", ")", "\n", "for", "id", ",", "contained_words", "in", "sorted", "(", "lines", ".", "items", "(", ")", ")", "\n", "]", "\n", "\n", "blocks", "=", "[", "\n", "union_lp_box", "(", "contained_words", ")", ".", "set", "(", "id", "=", "id", ")", "\n", "for", "id", ",", "contained_words", "in", "sorted", "(", "blocks", ".", "items", "(", ")", ")", "\n", "]", "\n", "\n", "return", "cls", "(", "blocks", "=", "blocks", ",", "lines", "=", "lines", ",", "words", "=", "words", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.__init__": [[9, 18], ["pdf_extractor_name.lower", "pdfplumber_extractor.PDFPlumberTokenExtractor", "NotImplementedError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "pdf_extractor_name", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "self", ".", "pdf_extractor_name", "=", "pdf_extractor_name", ".", "lower", "(", ")", "\n", "\n", "if", "self", ".", "pdf_extractor_name", "==", "PDFPlumberTokenExtractor", ".", "NAME", ":", "\n", "            ", "self", ".", "pdf_extractor", "=", "PDFPlumberTokenExtractor", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "f\"Unknown pdf_extractor_name {pdf_extractor_name}\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image": [[20, 49], ["pdf_extractor.PDFExtractor.pdf_extractor", "pdf2image.convert_from_path", "zip", "page.tokens.scale", "image.resize", "zip", "int", "int"], "methods", ["None"], ["", "", "def", "load_tokens_and_image", "(", "\n", "self", ",", "pdf_path", ":", "str", ",", "resize_image", "=", "False", ",", "resize_layout", "=", "False", ",", "dpi", "=", "72", ",", "**", "kwargs", "\n", ")", ":", "\n", "\n", "        ", "pdf_tokens", "=", "self", ".", "pdf_extractor", "(", "pdf_path", ",", "**", "kwargs", ")", "\n", "\n", "page_images", "=", "pdf2image", ".", "convert_from_path", "(", "pdf_path", ",", "dpi", "=", "dpi", ")", "\n", "\n", "assert", "not", "(", "\n", "resize_image", "and", "resize_layout", "\n", ")", ",", "\"You could not resize image and layout simultaneously.\"", "\n", "\n", "if", "resize_layout", ":", "\n", "            ", "for", "image", ",", "page", "in", "zip", "(", "page_images", ",", "pdf_tokens", ")", ":", "\n", "                ", "width", ",", "height", "=", "image", ".", "size", "\n", "resize_factor", "=", "width", "/", "page", ".", "width", ",", "height", "/", "page", ".", "height", "\n", "page", ".", "tokens", "=", "page", ".", "tokens", ".", "scale", "(", "resize_factor", ")", "\n", "page", ".", "image_height", "=", "height", "\n", "page", ".", "image_width", "=", "width", "\n", "\n", "", "", "elif", "resize_image", ":", "\n", "            ", "page_images", "=", "[", "\n", "image", ".", "resize", "(", "(", "int", "(", "page", ".", "width", ")", ",", "int", "(", "page", ".", "height", ")", ")", ")", "\n", "if", "page", ".", "width", "!=", "image", ".", "size", "[", "0", "]", "\n", "else", "image", "\n", "for", "image", ",", "page", "in", "zip", "(", "page_images", ",", "pdf_tokens", ")", "\n", "]", "\n", "\n", "", "return", "pdf_tokens", ",", "page_images", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.__init__": [[11, 15], ["None"], "methods", ["None"], ["def", "extract", "(", "self", ",", "pdf_path", ":", "str", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.preprocess_sample": [[16, 21], ["None"], "methods", ["None"], ["\n", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.preprocess_batch": [[22, 27], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.iter_example": [[28, 34], ["list", "len", "range", "examples.keys"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.batchsize_examples": [[35, 43], ["list", "examples[].keys", "list", "itertools.chain.from_iterable"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.SimplePDFDataPreprocessor.__init__": [[47, 63], ["zip"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.SimplePDFDataPreprocessor.preprocess_sample": [[65, 148], ["base.SimplePDFDataPreprocessor.tokenizer", "range", "list", "len", "base.SimplePDFDataPreprocessor.word_ids", "enumerate", "batched_labels.append", "batched_bboxes.append", "reversed", "set", "cur_label_ids.append", "encoded_word_ids.append", "cur_bboxes.append", "cur_bboxes.append", "cur_label_ids.append", "cur_bboxes.append", "cur_label_ids.append", "cur_bboxes.append", "int", "int"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.SimplePDFDataPreprocessor.preprocess_batch": [[149, 162], ["base.SimplePDFDataPreprocessor.iter_example", "base.SimplePDFDataPreprocessor.batchsize_examples", "base.SimplePDFDataPreprocessor.preprocess_sample", "base.SimplePDFDataPreprocessor.pop", "all_processed_examples.append"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.iter_example", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.batchsize_examples", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BaseHierarchicalPDFDataPreprocessor.__init__": [[49, 64], ["getattr", "getattr", "getattr", "getattr"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "config", ")", ":", "\n", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "group_bbox_agg_func", "=", "self", ".", "GROUP_BBOX_AGG_FUNC", "[", "config", ".", "group_bbox_agg", "]", "\n", "\n", "self", ".", "max_line_per_page", "=", "getattr", "(", "config", ",", "\"max_line_per_page\"", ",", "MAX_LINE_PER_PAGE", ")", "\n", "self", ".", "max_tokens_per_line", "=", "getattr", "(", "\n", "config", ",", "\"max_tokens_per_line\"", ",", "MAX_TOKENS_PER_LINE", "\n", ")", "\n", "self", ".", "max_block_per_page", "=", "getattr", "(", "\n", "config", ",", "\"max_block_per_page\"", ",", "MAX_BLOCK_PER_PAGE", "\n", ")", "\n", "self", ".", "max_tokens_per_block", "=", "getattr", "(", "\n", "config", ",", "\"max_tokens_per_block\"", ",", "MAX_TOKENS_PER_BLOCK", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BaseHierarchicalPDFDataPreprocessor.preprocess_batch": [[66, 75], ["hierarchical_modeling.BaseHierarchicalPDFDataPreprocessor.iter_example", "hierarchical_modeling.BaseHierarchicalPDFDataPreprocessor.batchsize_examples", "hierarchical_modeling.BaseHierarchicalPDFDataPreprocessor.preprocess_sample", "all_processed_examples.append"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.iter_example", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.batchsize_examples", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample"], ["", "def", "preprocess_batch", "(", "self", ",", "examples", ")", ":", "\n", "\n", "        ", "all_processed_examples", "=", "[", "]", "\n", "for", "example", "in", "self", ".", "iter_example", "(", "examples", ")", ":", "\n", "            ", "processed_example", "=", "self", ".", "preprocess_sample", "(", "example", ")", "\n", "# encoded_word_ids will only be used in test time", "\n", "all_processed_examples", ".", "append", "(", "processed_example", ")", "\n", "\n", "", "return", "self", ".", "batchsize_examples", "(", "all_processed_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample": [[78, 148], ["float", "zip", "enumerate", "hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.tokenizer", "list", "processed_batches.append", "itertools.groupby", "len", "line_words.append", "line_bbox.append", "line_labels.append", "max", "line_words.extend", "line_bbox.extend", "line_labels.extend", "hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.group_bbox_agg_func", "line_word_cnt.items", "processed_batches[].keys", "list", "hierarchical_modeling.get_most_common_element"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_element"], ["    ", "def", "preprocess_chunked_sample", "(", "self", ",", "examples", ")", ":", "\n", "\n", "        ", "processed_batches", "=", "[", "]", "\n", "\n", "max_textline_len", "=", "float", "(", "\"-inf\"", ")", "\n", "for", "line_ids", ",", "words", ",", "bbox", ",", "labels", "in", "zip", "(", "\n", "examples", "[", "\"line_ids\"", "]", ",", "\n", "examples", "[", "\"words\"", "]", ",", "\n", "examples", "[", "\"bbox\"", "]", ",", "\n", "examples", "[", "\"labels\"", "]", ",", "\n", ")", ":", "\n", "            ", "line_words", "=", "[", "]", "\n", "line_bbox", "=", "[", "]", "\n", "line_labels", "=", "[", "]", "\n", "line_word_cnt", "=", "{", "}", "# Dict is ordered since Python 3.5", "\n", "\n", "pre_index", "=", "0", "\n", "\n", "for", "line_id", ",", "(", "_orig_line_id", ",", "gp", ")", "in", "enumerate", "(", "itertools", ".", "groupby", "(", "line_ids", ")", ")", ":", "\n", "                ", "if", "line_id", ">=", "self", ".", "max_line_per_page", ":", "\n", "                    ", "line_id", "-=", "1", "# Recover the correct line_id for generating the line_attention_mask", "\n", "break", "\n", "", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "line_word_cnt", "[", "_orig_line_id", "]", "=", "cur_len", "\n", "line_words", ".", "append", "(", "words", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", "line_bbox", ".", "append", "(", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", "line_labels", ".", "append", "(", "\n", "get_most_common_element", "(", "labels", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", ")", "# Because all tokens in the same line have the same labels", "\n", "pre_index", "+=", "cur_len", "\n", "max_textline_len", "=", "max", "(", "max_textline_len", ",", "cur_len", ")", "\n", "\n", "", "line_attention_mask", "=", "[", "1", "]", "*", "(", "line_id", "+", "1", ")", "+", "[", "0", "]", "*", "(", "\n", "self", ".", "max_line_per_page", "-", "line_id", "-", "1", "\n", ")", "\n", "\n", "if", "line_id", "<", "self", ".", "max_line_per_page", ":", "\n", "                ", "line_words", ".", "extend", "(", "\n", "[", "[", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "\"pad_token\"", "]", "]", "]", "\n", "*", "(", "self", ".", "max_line_per_page", "-", "line_id", "-", "1", ")", "\n", ")", "\n", "line_bbox", ".", "extend", "(", "\n", "[", "[", "[", "0", ",", "0", ",", "0", ",", "0", "]", "]", "]", "*", "(", "self", ".", "max_line_per_page", "-", "line_id", "-", "1", ")", "\n", ")", "\n", "line_labels", ".", "extend", "(", "[", "-", "100", "]", "*", "(", "self", ".", "max_line_per_page", "-", "line_id", "-", "1", ")", ")", "\n", "\n", "", "tokenized_line", "=", "self", ".", "tokenizer", "(", "\n", "line_words", ",", "\n", "padding", "=", "\"max_length\"", ",", "\n", "max_length", "=", "self", ".", "max_tokens_per_line", ",", "\n", "truncation", "=", "True", ",", "\n", "is_split_into_words", "=", "True", ",", "\n", ")", "\n", "\n", "tokenized_line", "[", "\"bbox\"", "]", "=", "[", "\n", "self", ".", "group_bbox_agg_func", "(", "bboxes", ")", "for", "bboxes", "in", "line_bbox", "\n", "]", "\n", "tokenized_line", "[", "\"labels\"", "]", "=", "line_labels", "\n", "tokenized_line", "[", "\"group_level_attention_mask\"", "]", "=", "line_attention_mask", "\n", "tokenized_line", "[", "\"group_word_count\"", "]", "=", "list", "(", "line_word_cnt", ".", "items", "(", ")", ")", "\n", "\n", "processed_batches", ".", "append", "(", "tokenized_line", ")", "\n", "\n", "", "condensed_batch", "=", "{", "\n", "key", ":", "[", "ele", "[", "key", "]", "for", "ele", "in", "processed_batches", "]", "\n", "for", "key", "in", "processed_batches", "[", "0", "]", ".", "keys", "(", ")", "\n", "}", "\n", "\n", "del", "processed_batches", "\n", "return", "condensed_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.preprocess_sample": [[149, 217], ["len", "max", "hierarchical_modeling.clean_group_ids", "hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "math.ceil", "range", "hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "sum", "len", "set", "hierarchical_modeling.remap_group_id", "max", "hierarchical_modeling.find_idx_in_list", "newly_batched_words.append", "newly_batched_labels.append", "newly_batched_bbox.append", "newly_batched_line_ids.append"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.clean_group_ids", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.remap_group_id", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.find_idx_in_list"], ["", "def", "preprocess_sample", "(", "self", ",", "example", ")", ":", "\n", "\n", "        ", "if", "False", ":", "\n", "# Keep for future reference", "\n", "            ", "line_count_this_page", "=", "len", "(", "set", "(", "example", "[", "\"line_ids\"", "]", ")", ")", "\n", "max_line_id", "=", "max", "(", "example", "[", "\"line_ids\"", "]", ")", "\n", "\n", "if", "max_line_id", "!=", "line_count_this_page", ":", "\n", "                ", "line_ids", "=", "remap_group_id", "(", "example", "[", "\"line_ids\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "line_ids", "=", "example", "[", "\"line_ids\"", "]", "\n", "", "", "else", ":", "\n", "            ", "line_ids", "=", "clean_group_ids", "(", "example", "[", "\"line_ids\"", "]", ")", "\n", "line_count_this_page", "=", "max", "(", "line_ids", ")", "+", "1", "\n", "\n", "", "if", "line_count_this_page", "<=", "self", ".", "max_line_per_page", ":", "\n", "# In this case, we just need to send it to the regular tokenizer", "\n", "            ", "batched_input", "=", "self", ".", "preprocess_chunked_sample", "(", "\n", "{", "\n", "\"words\"", ":", "[", "example", "[", "\"words\"", "]", "]", ",", "\n", "\"labels\"", ":", "[", "example", "[", "\"labels\"", "]", "]", ",", "\n", "\"bbox\"", ":", "[", "example", "[", "\"bbox\"", "]", "]", ",", "\n", "\"line_ids\"", ":", "[", "line_ids", "]", ",", "\n", "}", "\n", ")", "\n", "", "else", ":", "\n", "# We need to split the input and regroup it into multiple batches", "\n", "            ", "num_splits", "=", "math", ".", "ceil", "(", "line_count_this_page", "/", "self", ".", "max_line_per_page", ")", "\n", "assert", "num_splits", ">", "1", "\n", "\n", "words", "=", "example", "[", "\"words\"", "]", "\n", "labels", "=", "example", "[", "\"labels\"", "]", "\n", "bbox", "=", "example", "[", "\"bbox\"", "]", "\n", "\n", "newly_batched_words", "=", "[", "]", "\n", "newly_batched_labels", "=", "[", "]", "\n", "newly_batched_bbox", "=", "[", "]", "\n", "newly_batched_line_ids", "=", "[", "]", "\n", "\n", "prev_idx", "=", "0", "\n", "for", "line_id_split", "in", "range", "(", "\n", "self", ".", "max_line_per_page", ",", "\n", "(", "num_splits", "+", "1", ")", "*", "self", ".", "max_line_per_page", ",", "\n", "self", ".", "max_line_per_page", ",", "\n", ")", ":", "\n", "\n", "                ", "cur_idx", "=", "find_idx_in_list", "(", "line_ids", ",", "line_id_split", ",", "prev_idx", ")", "\n", "assert", "cur_idx", ">", "prev_idx", "# Ensure the new segment is not empty", "\n", "\n", "newly_batched_words", ".", "append", "(", "words", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_labels", ".", "append", "(", "labels", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_bbox", ".", "append", "(", "bbox", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_line_ids", ".", "append", "(", "line_ids", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "prev_idx", "=", "cur_idx", "+", "1", "\n", "\n", "", "new_examples", "=", "{", "\n", "\"words\"", ":", "newly_batched_words", ",", "\n", "\"labels\"", ":", "newly_batched_labels", ",", "\n", "\"bbox\"", ":", "newly_batched_bbox", ",", "\n", "\"line_ids\"", ":", "newly_batched_line_ids", ",", "\n", "}", "\n", "batched_input", "=", "self", ".", "preprocess_chunked_sample", "(", "new_examples", ")", "\n", "\n", "", "group_word_count", "=", "[", "\n", "ele", "[", "1", "]", "for", "batch", "in", "batched_input", "[", "\"group_word_count\"", "]", "for", "ele", "in", "batch", "\n", "]", "\n", "assert", "sum", "(", "group_word_count", ")", "==", "len", "(", "example", "[", "\"words\"", "]", ")", "\n", "return", "batched_input", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample": [[220, 292], ["float", "zip", "enumerate", "hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.tokenizer", "list", "processed_batches.append", "itertools.groupby", "len", "block_words.append", "block_bbox.append", "block_labels.append", "max", "block_words.extend", "block_bbox.extend", "block_labels.extend", "hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.group_bbox_agg_func", "block_word_cnt.items", "processed_batches[].keys", "list", "hierarchical_modeling.get_most_common_element"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_element"], ["    ", "def", "preprocess_chunked_sample", "(", "self", ",", "examples", ")", ":", "\n", "\n", "        ", "processed_batches", "=", "[", "]", "\n", "\n", "max_textblock_len", "=", "float", "(", "\"-inf\"", ")", "\n", "for", "block_ids", ",", "words", ",", "bbox", ",", "labels", "in", "zip", "(", "\n", "examples", "[", "\"block_ids\"", "]", ",", "\n", "examples", "[", "\"words\"", "]", ",", "\n", "examples", "[", "\"bbox\"", "]", ",", "\n", "examples", "[", "\"labels\"", "]", ",", "\n", ")", ":", "\n", "            ", "block_words", "=", "[", "]", "\n", "block_bbox", "=", "[", "]", "\n", "block_labels", "=", "[", "]", "\n", "block_word_cnt", "=", "{", "}", "# Dict is ordered since Python 3.5", "\n", "\n", "pre_index", "=", "0", "\n", "\n", "for", "block_id", ",", "(", "_orig_block_id", ",", "gp", ")", "in", "enumerate", "(", "\n", "itertools", ".", "groupby", "(", "block_ids", ")", "\n", ")", ":", "\n", "                ", "if", "block_id", ">=", "self", ".", "max_block_per_page", ":", "\n", "                    ", "block_id", "-=", "1", "# Recover the correct block_id for generating the block_attention_mask", "\n", "break", "\n", "", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "block_word_cnt", "[", "_orig_block_id", "]", "=", "cur_len", "\n", "block_words", ".", "append", "(", "words", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", "block_bbox", ".", "append", "(", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", "block_labels", ".", "append", "(", "\n", "get_most_common_element", "(", "labels", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "\n", ")", "# Because all tokens in the same block have the same labels", "\n", "pre_index", "+=", "cur_len", "\n", "max_textblock_len", "=", "max", "(", "max_textblock_len", ",", "cur_len", ")", "\n", "\n", "", "block_attention_mask", "=", "[", "1", "]", "*", "(", "block_id", "+", "1", ")", "+", "[", "0", "]", "*", "(", "\n", "self", ".", "max_block_per_page", "-", "block_id", "-", "1", "\n", ")", "\n", "\n", "if", "block_id", "<", "self", ".", "max_block_per_page", ":", "\n", "                ", "block_words", ".", "extend", "(", "\n", "[", "[", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "\"pad_token\"", "]", "]", "]", "\n", "*", "(", "self", ".", "max_block_per_page", "-", "block_id", "-", "1", ")", "\n", ")", "\n", "block_bbox", ".", "extend", "(", "\n", "[", "[", "[", "0", ",", "0", ",", "0", ",", "0", "]", "]", "]", "*", "(", "self", ".", "max_block_per_page", "-", "block_id", "-", "1", ")", "\n", ")", "\n", "block_labels", ".", "extend", "(", "[", "-", "100", "]", "*", "(", "self", ".", "max_block_per_page", "-", "block_id", "-", "1", ")", ")", "\n", "\n", "", "tokenized_block", "=", "self", ".", "tokenizer", "(", "\n", "block_words", ",", "\n", "padding", "=", "\"max_length\"", ",", "\n", "max_length", "=", "self", ".", "max_tokens_per_block", ",", "\n", "truncation", "=", "True", ",", "\n", "is_split_into_words", "=", "True", ",", "\n", ")", "\n", "\n", "tokenized_block", "[", "\"bbox\"", "]", "=", "[", "\n", "self", ".", "group_bbox_agg_func", "(", "bboxes", ")", "for", "bboxes", "in", "block_bbox", "\n", "]", "\n", "tokenized_block", "[", "\"labels\"", "]", "=", "block_labels", "\n", "tokenized_block", "[", "\"group_level_attention_mask\"", "]", "=", "block_attention_mask", "\n", "tokenized_block", "[", "\"group_word_count\"", "]", "=", "list", "(", "block_word_cnt", ".", "items", "(", ")", ")", "\n", "\n", "processed_batches", ".", "append", "(", "tokenized_block", ")", "\n", "\n", "", "condensed_batch", "=", "{", "\n", "key", ":", "[", "ele", "[", "key", "]", "for", "ele", "in", "processed_batches", "]", "\n", "for", "key", "in", "processed_batches", "[", "0", "]", ".", "keys", "(", ")", "\n", "}", "\n", "\n", "del", "processed_batches", "\n", "return", "condensed_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_sample": [[293, 361], ["len", "max", "hierarchical_modeling.clean_group_ids", "hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "math.ceil", "range", "hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "sum", "len", "set", "hierarchical_modeling.remap_group_id", "max", "hierarchical_modeling.find_idx_in_list", "newly_batched_words.append", "newly_batched_labels.append", "newly_batched_bbox.append", "newly_batched_block_ids.append"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.clean_group_ids", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor.preprocess_chunked_sample", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.remap_group_id", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.find_idx_in_list"], ["", "def", "preprocess_sample", "(", "self", ",", "example", ")", ":", "\n", "\n", "        ", "if", "False", ":", "\n", "# Keep for future reference", "\n", "            ", "block_count_this_page", "=", "len", "(", "set", "(", "example", "[", "\"block_ids\"", "]", ")", ")", "\n", "max_block_id", "=", "max", "(", "example", "[", "\"block_ids\"", "]", ")", "\n", "\n", "if", "max_block_id", "!=", "block_count_this_page", ":", "\n", "                ", "block_ids", "=", "remap_group_id", "(", "example", "[", "\"block_ids\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "block_ids", "=", "example", "[", "\"block_ids\"", "]", "\n", "", "", "else", ":", "\n", "            ", "block_ids", "=", "clean_group_ids", "(", "example", "[", "\"block_ids\"", "]", ")", "\n", "block_count_this_page", "=", "max", "(", "block_ids", ")", "+", "1", "\n", "\n", "", "if", "block_count_this_page", "<=", "self", ".", "max_block_per_page", ":", "\n", "# In this case, we just need to send it to the regular tokenizer", "\n", "            ", "batched_input", "=", "self", ".", "preprocess_chunked_sample", "(", "\n", "{", "\n", "\"words\"", ":", "[", "example", "[", "\"words\"", "]", "]", ",", "\n", "\"labels\"", ":", "[", "example", "[", "\"labels\"", "]", "]", ",", "\n", "\"bbox\"", ":", "[", "example", "[", "\"bbox\"", "]", "]", ",", "\n", "\"block_ids\"", ":", "[", "block_ids", "]", ",", "\n", "}", "\n", ")", "\n", "", "else", ":", "\n", "# We need to split the input and regroup it into multiple batches", "\n", "            ", "num_splits", "=", "math", ".", "ceil", "(", "block_count_this_page", "/", "self", ".", "max_block_per_page", ")", "\n", "assert", "num_splits", ">", "1", "\n", "\n", "words", "=", "example", "[", "\"words\"", "]", "\n", "labels", "=", "example", "[", "\"labels\"", "]", "\n", "bbox", "=", "example", "[", "\"bbox\"", "]", "\n", "\n", "newly_batched_words", "=", "[", "]", "\n", "newly_batched_labels", "=", "[", "]", "\n", "newly_batched_bbox", "=", "[", "]", "\n", "newly_batched_block_ids", "=", "[", "]", "\n", "\n", "prev_idx", "=", "0", "\n", "for", "block_id_split", "in", "range", "(", "\n", "self", ".", "max_block_per_page", ",", "\n", "(", "num_splits", "+", "1", ")", "*", "self", ".", "max_block_per_page", ",", "\n", "self", ".", "max_block_per_page", ",", "\n", ")", ":", "\n", "\n", "                ", "cur_idx", "=", "find_idx_in_list", "(", "block_ids", ",", "block_id_split", ",", "prev_idx", ")", "\n", "assert", "cur_idx", ">", "prev_idx", "# Ensure the new segment is not empty", "\n", "\n", "newly_batched_words", ".", "append", "(", "words", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_labels", ".", "append", "(", "labels", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_bbox", ".", "append", "(", "bbox", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "newly_batched_block_ids", ".", "append", "(", "block_ids", "[", "prev_idx", ":", "cur_idx", "+", "1", "]", ")", "\n", "prev_idx", "=", "cur_idx", "+", "1", "\n", "\n", "", "new_examples", "=", "{", "\n", "\"words\"", ":", "newly_batched_words", ",", "\n", "\"labels\"", ":", "newly_batched_labels", ",", "\n", "\"bbox\"", ":", "newly_batched_bbox", ",", "\n", "\"block_ids\"", ":", "newly_batched_block_ids", ",", "\n", "}", "\n", "batched_input", "=", "self", ".", "preprocess_chunked_sample", "(", "new_examples", ")", "\n", "\n", "", "group_word_count", "=", "[", "\n", "ele", "[", "1", "]", "for", "batch", "in", "batched_input", "[", "\"group_word_count\"", "]", "for", "ele", "in", "batch", "\n", "]", "\n", "assert", "sum", "(", "group_word_count", ")", "==", "len", "(", "example", "[", "\"words\"", "]", ")", "\n", "return", "batched_input", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.find_idx_in_list": [[10, 20], ["range", "len", "len"], "function", ["None"], ["def", "find_idx_in_list", "(", "lst", ",", "target_value", ",", "start", "=", "0", ")", ":", "\n", "    ", "\"\"\"Find the max idx in the list such that lst[idx] < target_value.\n    If list is empty or the first value is larger than the target value, return -1\n    \"\"\"", "\n", "if", "len", "(", "lst", ")", "==", "0", "or", "lst", "[", "0", "]", ">", "target_value", ":", "\n", "        ", "return", "-", "1", "\n", "", "for", "idx", "in", "range", "(", "start", ",", "len", "(", "lst", ")", ")", ":", "\n", "        ", "if", "lst", "[", "idx", "]", ">=", "target_value", ":", "\n", "            ", "return", "idx", "-", "1", "\n", "", "", "return", "idx", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.get_most_common_element": [[22, 24], ["collections.Counter().most_common", "collections.Counter"], "function", ["None"], ["", "def", "get_most_common_element", "(", "lst", ")", ":", "\n", "    ", "return", "Counter", "(", "lst", ")", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.remap_group_id": [[26, 29], ["enumerate", "set"], "function", ["None"], ["", "def", "remap_group_id", "(", "lst", ")", ":", "\n", "    ", "reindex_element", "=", "{", "val", ":", "idx", "for", "idx", ",", "val", "in", "enumerate", "(", "set", "(", "lst", ")", ")", "}", "\n", "return", "[", "reindex_element", "[", "ele", "]", "for", "ele", "in", "lst", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.hierarchical_modeling.clean_group_ids": [[31, 36], ["enumerate", "itertools.groupby", "_lst.extend", "len", "list"], "function", ["None"], ["", "def", "clean_group_ids", "(", "lst", ")", ":", "\n", "    ", "_lst", "=", "[", "]", "\n", "for", "idx", ",", "(", "_", ",", "gp", ")", "in", "enumerate", "(", "itertools", ".", "groupby", "(", "lst", ")", ")", ":", "\n", "        ", "_lst", ".", "extend", "(", "[", "idx", "]", "*", "len", "(", "list", "(", "gp", ")", ")", ")", "\n", "", "return", "_lst", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.to_json": [[18, 21], ["open", "json.dump", "vars"], "methods", ["None"], ["def", "to_json", "(", "self", ",", "path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"w\"", ")", "as", "fp", ":", "\n", "            ", "json", ".", "dump", "(", "vars", "(", "self", ")", ",", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained": [[22, 40], ["transformers.AutoConfig.from_pretrained", "hasattr", "warnings.warn", "cls", "transformers.AutoConfig.from_pretrained.vila_preprocessor_config.copy", "AutoConfig.from_pretrained.vila_preprocessor_config.copy.update", "cls", "AutoConfig.from_pretrained.vila_preprocessor_config.copy.pop"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "model_path", ":", "str", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "model_path", ")", "\n", "\n", "if", "hasattr", "(", "config", ",", "\"vila_preprocessor_config\"", ")", ":", "\n", "            ", "data_json", "=", "config", ".", "vila_preprocessor_config", ".", "copy", "(", ")", "\n", "if", "\"added_special_sepration_token\"", "in", "data_json", ":", "\n", "                ", "data_json", "[", "\"added_special_separation_token\"", "]", "=", "data_json", ".", "pop", "(", "\"added_special_sepration_token\"", ")", "\n", "# Fix an old typo in the config", "\n", "", "data_json", ".", "update", "(", "kwargs", ")", "\n", "return", "cls", "(", "**", "data_json", ")", "\n", "# We store the vila-preprocessor configs inside", "\n", "# a typical hf config json.", "\n", "\n", "# If not present, we use the default config", "\n", "", "warnings", ".", "warn", "(", "\"The vila_preprocessor_config is not present in the config, using the default one.\"", ")", "\n", "return", "cls", "(", "**", "kwargs", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.__init__": [[47, 61], ["base.SimplePDFDataPreprocessor.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "tokenizer", ",", "\n", "config", ",", "\n", "text_column_name", "=", "\"words\"", ",", "\n", "label_column_name", "=", "\"labels\"", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tokenizer", ",", "config", ",", "text_column_name", ",", "label_column_name", ")", "\n", "\n", "self", ".", "added_special_separation_token", "=", "config", ".", "added_special_separation_token", "\n", "if", "self", ".", "added_special_separation_token", "==", "\"default\"", ":", "\n", "            ", "self", ".", "added_special_separation_token", "=", "tokenizer", ".", "special_tokens_map", "[", "\n", "\"sep_token\"", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator": [[63, 66], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "insert_layout_indicator", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"It should be implemented differently for the functions\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.preprocess_sample": [[67, 157], ["layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator", "layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.tokenizer", "range", "len", "layout_indicator.BaseLayoutIndicatorPDFDataPreprocessor.word_ids", "enumerate", "batched_labels.append", "batched_bboxes.append", "reversed", "enumerate", "set", "cur_label_ids.append", "cur_bboxes.append", "cur_bboxes.append", "cur_label_ids.append", "cur_bboxes.append", "cur_label_ids.append", "cur_bboxes.append", "encoded_word_ids.append", "int", "int"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.SentenceLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator"], ["", "def", "preprocess_sample", "(", "self", ",", "example", ":", "Dict", ",", "padding", "=", "\"max_length\"", ")", "->", "Dict", ":", "\n", "        ", "example", ",", "token_id_mapping_table", "=", "self", ".", "insert_layout_indicator", "(", "example", ")", "\n", "\n", "tokenized_inputs", "=", "self", ".", "tokenizer", "(", "\n", "example", "[", "self", ".", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "truncation", "=", "True", ",", "\n", "is_split_into_words", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "True", ",", "\n", ")", "\n", "\n", "# original label and bbox from the input", "\n", "labels", "=", "example", "[", "self", ".", "label_column_name", "]", "\n", "bboxes", "=", "example", "[", "\"bbox\"", "]", "\n", "\n", "# batched labels and bboxes", "\n", "batched_labels", "=", "[", "]", "\n", "batched_bboxes", "=", "[", "]", "\n", "previous_word_idx", "=", "None", "\n", "encoded_word_ids", "=", "[", "]", "\n", "\n", "for", "batch_id", "in", "range", "(", "len", "(", "tokenized_inputs", "[", "\"input_ids\"", "]", ")", ")", ":", "\n", "\n", "            ", "word_ids", "=", "tokenized_inputs", ".", "word_ids", "(", "batch_index", "=", "batch_id", ")", "\n", "\n", "cur_label_ids", "=", "[", "]", "\n", "cur_bboxes", "=", "[", "]", "\n", "\n", "for", "_i", ",", "word_idx", "in", "enumerate", "(", "word_ids", ")", ":", "\n", "\n", "                ", "if", "word_idx", "is", "None", ":", "\n", "                    ", "cur_label_ids", ".", "append", "(", "-", "100", ")", "\n", "if", "(", "\n", "tokenized_inputs", "[", "\"input_ids\"", "]", "[", "batch_id", "]", "[", "_i", "]", "\n", "==", "self", ".", "special_tokens_map", "[", "\n", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "\"sep_token\"", "]", "\n", "]", "\n", ")", ":", "\n", "                        ", "cur_bboxes", ".", "append", "(", "[", "1000", ",", "1000", ",", "1000", ",", "1000", "]", ")", "\n", "", "else", ":", "\n", "                        ", "cur_bboxes", ".", "append", "(", "[", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "\n", "", "", "elif", "word_idx", "!=", "previous_word_idx", ":", "\n", "                    ", "cur_label_ids", ".", "append", "(", "int", "(", "labels", "[", "word_idx", "]", ")", ")", "\n", "cur_bboxes", ".", "append", "(", "bboxes", "[", "word_idx", "]", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "cur_label_ids", ".", "append", "(", "\n", "int", "(", "labels", "[", "word_idx", "]", ")", "if", "self", ".", "config", ".", "label_all_tokens", "else", "-", "100", "\n", ")", "\n", "cur_bboxes", ".", "append", "(", "bboxes", "[", "word_idx", "]", ")", "\n", "\n", "", "if", "not", "(", "_i", "==", "0", "and", "word_idx", "is", "None", ")", ":", "\n", "# Only updates the word_idx after the 0th item", "\n", "# This is important because there would be cross-batch", "\n", "# tokens.", "\n", "                    ", "previous_word_idx", "=", "word_idx", "\n", "\n", "", "if", "word_idx", "is", "not", "None", ":", "\n", "                    ", "if", "tokenized_inputs", "[", "\"input_ids\"", "]", "[", "batch_id", "]", "[", "_i", "]", "not", "in", "[", "\n", "self", ".", "special_tokens_map", "[", "\n", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "\"sep_token\"", "]", "\n", "]", ",", "\n", "self", ".", "special_tokens_map", "[", "self", ".", "added_special_separation_token", "]", ",", "\n", "]", ":", "\n", "# Because we could possibly insert [SEP] or [BLK] tokens in", "\n", "# this process.", "\n", "                        ", "encoded_word_ids", ".", "append", "(", "word_idx", ")", "\n", "\n", "", "", "", "batched_labels", ".", "append", "(", "cur_label_ids", ")", "\n", "batched_bboxes", ".", "append", "(", "cur_bboxes", ")", "\n", "\n", "# Find the last word id in this batch to handle", "\n", "# multi-batch samples", "\n", "for", "word_id", "in", "reversed", "(", "word_ids", ")", ":", "\n", "                ", "if", "word_id", "is", "not", "None", ":", "\n", "                    ", "previous_word_idx", "=", "word_id", "\n", "break", "\n", "\n", "", "", "", "new_id_to_original_id", "=", "{", "\n", "ele", ":", "idx", "for", "idx", ",", "ele", "in", "enumerate", "(", "token_id_mapping_table", ")", "\n", "}", "\n", "\n", "tokenized_inputs", "[", "\"labels\"", "]", "=", "batched_labels", "\n", "tokenized_inputs", "[", "\"bbox\"", "]", "=", "batched_bboxes", "\n", "tokenized_inputs", "[", "\"encoded_word_ids\"", "]", "=", "[", "\n", "new_id_to_original_id", "[", "ele", "]", "for", "ele", "in", "set", "(", "encoded_word_ids", ")", "\n", "]", "\n", "\n", "return", "tokenized_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.BlockLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator": [[160, 203], ["itertools.groupby", "len", "len", "list", "processed_words.extend", "processed_bbox.extend", "processed_labels.extend", "len", "list", "range", "union_box"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["    ", "def", "insert_layout_indicator", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "\n", "        ", "processed_words", "=", "[", "]", "\n", "processed_bbox", "=", "[", "]", "\n", "processed_labels", "=", "[", "]", "\n", "\n", "block_ids", "=", "example", "[", "\"block_ids\"", "]", "\n", "words", "=", "example", "[", "\"words\"", "]", "\n", "bbox", "=", "example", "[", "\"bbox\"", "]", "\n", "labels", "=", "example", "[", "\"labels\"", "]", "\n", "\n", "token_id_mapping_table", "=", "[", "None", "]", "*", "len", "(", "words", ")", "\n", "\n", "pre_index", "=", "0", "\n", "new_sequence_len", "=", "0", "\n", "\n", "for", "block_id", ",", "gp", "in", "itertools", ".", "groupby", "(", "block_ids", ")", ":", "\n", "            ", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "token_id_mapping_table", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "=", "list", "(", "\n", "range", "(", "new_sequence_len", ",", "new_sequence_len", "+", "cur_len", ")", "\n", ")", "\n", "processed_words", ".", "extend", "(", "\n", "words", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "\n", "+", "[", "self", ".", "added_special_separation_token", "]", "\n", ")", "\n", "processed_bbox", ".", "extend", "(", "\n", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "\n", "+", "[", "union_box", "(", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "]", "\n", ")", "\n", "processed_labels", ".", "extend", "(", "labels", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "+", "[", "-", "100", "]", ")", "\n", "pre_index", "+=", "cur_len", "\n", "new_sequence_len", "=", "len", "(", "processed_labels", ")", "\n", "\n", "# There will be an extra [SEP] token at the end of the iterations", "\n", "", "processed_words", "=", "processed_words", "[", ":", "-", "1", "]", "\n", "processed_bbox", "=", "processed_bbox", "[", ":", "-", "1", "]", "\n", "processed_labels", "=", "processed_labels", "[", ":", "-", "1", "]", "\n", "\n", "return", "{", "\n", "self", ".", "text_column_name", ":", "processed_words", ",", "\n", "self", ".", "label_column_name", ":", "processed_labels", ",", "\n", "\"bbox\"", ":", "processed_bbox", ",", "\n", "}", ",", "token_id_mapping_table", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.RowLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator": [[206, 249], ["itertools.groupby", "len", "len", "list", "processed_words.extend", "processed_bbox.extend", "processed_labels.extend", "len", "list", "range", "union_box"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["    ", "def", "insert_layout_indicator", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "\n", "        ", "processed_words", "=", "[", "]", "\n", "processed_bbox", "=", "[", "]", "\n", "processed_labels", "=", "[", "]", "\n", "\n", "line_ids", "=", "example", "[", "\"line_ids\"", "]", "# Changed", "\n", "words", "=", "example", "[", "\"words\"", "]", "\n", "bbox", "=", "example", "[", "\"bbox\"", "]", "\n", "labels", "=", "example", "[", "\"labels\"", "]", "\n", "\n", "token_id_mapping_table", "=", "[", "None", "]", "*", "len", "(", "words", ")", "\n", "\n", "pre_index", "=", "0", "\n", "new_sequence_len", "=", "0", "\n", "\n", "for", "line_id", ",", "gp", "in", "itertools", ".", "groupby", "(", "line_ids", ")", ":", "# Changed", "\n", "            ", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "token_id_mapping_table", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "=", "list", "(", "\n", "range", "(", "new_sequence_len", ",", "new_sequence_len", "+", "cur_len", ")", "\n", ")", "\n", "processed_words", ".", "extend", "(", "\n", "words", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "\n", "+", "[", "self", ".", "added_special_separation_token", "]", "\n", ")", "\n", "processed_bbox", ".", "extend", "(", "\n", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "\n", "+", "[", "union_box", "(", "bbox", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", ")", "]", "\n", ")", "\n", "processed_labels", ".", "extend", "(", "labels", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "+", "[", "-", "100", "]", ")", "\n", "pre_index", "+=", "cur_len", "\n", "new_sequence_len", "=", "len", "(", "processed_labels", ")", "\n", "\n", "# There will be an extra [SEP] token at the end of the iterations", "\n", "", "processed_words", "=", "processed_words", "[", ":", "-", "1", "]", "\n", "processed_bbox", "=", "processed_bbox", "[", ":", "-", "1", "]", "\n", "processed_labels", "=", "processed_labels", "[", ":", "-", "1", "]", "\n", "\n", "return", "{", "\n", "self", ".", "text_column_name", ":", "processed_words", ",", "\n", "self", ".", "label_column_name", ":", "processed_labels", ",", "\n", "\"bbox\"", ":", "processed_bbox", ",", "\n", "}", ",", "token_id_mapping_table", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.SentenceLayoutIndicatorPDFDataPreprocessor.insert_layout_indicator": [[254, 291], ["layout_indicator.split_token_based_on_sentences_boundary", "len", "list", "processed_words.extend", "processed_bbox.extend", "processed_labels.extend", "len", "range", "union_box"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.split_token_based_on_sentences_boundary", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["    ", "def", "insert_layout_indicator", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "\n", "        ", "processed_words", "=", "[", "]", "\n", "processed_bbox", "=", "[", "]", "\n", "processed_labels", "=", "[", "]", "\n", "\n", "words", "=", "example", "[", "\"words\"", "]", "\n", "bbox", "=", "example", "[", "\"bbox\"", "]", "\n", "labels", "=", "example", "[", "\"labels\"", "]", "\n", "\n", "token_id_mapping_table", "=", "[", "None", "]", "*", "len", "(", "words", ")", "\n", "\n", "token_splits", "=", "split_token_based_on_sentences_boundary", "(", "words", ")", "\n", "\n", "new_sequence_len", "=", "0", "\n", "for", "(", "start", ",", "end", ")", "in", "token_splits", ":", "\n", "            ", "token_id_mapping_table", "[", "start", ":", "end", "]", "=", "list", "(", "\n", "range", "(", "new_sequence_len", ",", "new_sequence_len", "+", "end", "-", "start", ")", "\n", ")", "\n", "processed_words", ".", "extend", "(", "\n", "words", "[", "start", ":", "end", "]", "+", "[", "self", ".", "added_special_separation_token", "]", "\n", ")", "\n", "processed_bbox", ".", "extend", "(", "bbox", "[", "start", ":", "end", "]", "+", "[", "union_box", "(", "bbox", "[", "start", ":", "end", "]", ")", "]", ")", "\n", "processed_labels", ".", "extend", "(", "labels", "[", "start", ":", "end", "]", "+", "[", "-", "100", "]", ")", "\n", "\n", "new_sequence_len", "=", "len", "(", "processed_labels", ")", "\n", "\n", "# There will be an extra [SEP] token at the end of the iterations", "\n", "", "processed_words", "=", "processed_words", "[", ":", "-", "1", "]", "\n", "processed_bbox", "=", "processed_bbox", "[", ":", "-", "1", "]", "\n", "processed_labels", "=", "processed_labels", "[", ":", "-", "1", "]", "\n", "\n", "return", "{", "\n", "self", ".", "text_column_name", ":", "processed_words", ",", "\n", "self", ".", "label_column_name", ":", "processed_labels", ",", "\n", "\"bbox\"", ":", "processed_bbox", ",", "\n", "}", ",", "token_id_mapping_table", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.layout_indicator.split_token_based_on_sentences_boundary": [[14, 44], ["numpy.zeros", "enumerate", "Segmenter.segment", "len", "len", "char2token_mask[].max", "split.append", "len", "len"], "function", ["None"], ["def", "split_token_based_on_sentences_boundary", "(", "words", ":", "List", "[", "str", "]", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"\n    Returns: List[Tuple(int, int)]\n        a list of (start, end) for token indices within each sentence\n    \"\"\"", "\n", "\n", "if", "len", "(", "words", ")", "==", "0", ":", "\n", "        ", "return", "[", "(", "0", ",", "0", ")", "]", "\n", "", "combined_words", "=", "\" \"", ".", "join", "(", "words", ")", "\n", "\n", "char2token_mask", "=", "np", ".", "zeros", "(", "len", "(", "combined_words", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "acc_word_len", "=", "0", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "        ", "word_len", "=", "len", "(", "word", ")", "+", "1", "\n", "char2token_mask", "[", "acc_word_len", ":", "acc_word_len", "+", "word_len", "]", "=", "idx", "\n", "acc_word_len", "+=", "word_len", "\n", "\n", "", "segmented_sentences", "=", "Segmenter", ".", "segment", "(", "combined_words", ")", "\n", "sent_boundary", "=", "[", "(", "ele", ".", "start", ",", "ele", ".", "end", ")", "for", "ele", "in", "segmented_sentences", "]", "\n", "\n", "split", "=", "[", "]", "\n", "token_id_start", "=", "0", "\n", "for", "(", "start", ",", "end", ")", "in", "sent_boundary", ":", "\n", "        ", "token_id_end", "=", "char2token_mask", "[", "start", ":", "end", "]", ".", "max", "(", ")", "\n", "if", "end", "+", "1", ">=", "len", "(", "char2token_mask", ")", "or", "char2token_mask", "[", "end", "+", "1", "]", "!=", "token_id_end", ":", "\n", "            ", "token_id_end", "+=", "1", "# (Including the end)", "\n", "", "split", ".", "append", "(", "(", "token_id_start", ",", "token_id_end", ")", ")", "\n", "token_id_start", "=", "token_id_end", "\n", "", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor": [[15, 43], ["hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor", "hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor", "ValueError", "layout_indicator.RowLayoutIndicatorPDFDataPreprocessor", "layout_indicator.BlockLayoutIndicatorPDFDataPreprocessor", "grouping.RowGroupingPDFDataPreprocessor", "base.SimplePDFDataPreprocessor", "layout_indicator.SentenceLayoutIndicatorPDFDataPreprocessor", "grouping.BlockGroupingPDFDataPreprocessor"], "function", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sequence": [[44, 47], ["None"], "methods", ["None"], ["    ", "@", "abstractmethod", "\n", "def", "preprocess_sequence", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"It should be implemented differently for the functions\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample": [[48, 120], ["grouping.BaseGroupingPDFDataPreprocessor.preprocess_sequence", "grouping.BaseGroupingPDFDataPreprocessor.tokenizer", "enumerate", "list", "zip", "grouping.BaseGroupingPDFDataPreprocessor.word_ids", "enumerate", "labels.append", "bboxes.append", "num_tokens_per_group.items", "len", "collections.Counter", "label_ids.append", "collections.Counter.pop", "cur_bboxes.append", "cur_bboxes.append", "label_ids.append", "cur_bboxes.append", "label_ids.append", "cur_bboxes.append", "len", "int", "collections.Counter.most_common", "int"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BlockGroupingPDFDataPreprocessor.preprocess_sequence"], ["", "def", "preprocess_sample", "(", "self", ",", "example", ":", "Dict", ",", "padding", "=", "\"max_length\"", ")", "->", "Dict", ":", "\n", "\n", "        ", "splitted_example", ",", "num_tokens_per_group", "=", "self", ".", "preprocess_sequence", "(", "example", ")", "\n", "\n", "tokenized_inputs", "=", "self", ".", "tokenizer", "(", "\n", "splitted_example", "[", "self", ".", "text_column_name", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "truncation", "=", "True", ",", "\n", "is_split_into_words", "=", "True", ",", "\n", ")", "\n", "\n", "# original label and bbox from the input", "\n", "labels", "=", "[", "]", "\n", "bboxes", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "label", ",", "bbox", ")", "in", "enumerate", "(", "\n", "zip", "(", "splitted_example", "[", "self", ".", "label_column_name", "]", ",", "splitted_example", "[", "\"bbox\"", "]", ")", "\n", ")", ":", "\n", "            ", "word_ids", "=", "tokenized_inputs", ".", "word_ids", "(", "batch_index", "=", "i", ")", "\n", "previous_word_idx", "=", "None", "\n", "label_ids", "=", "[", "]", "\n", "cur_bboxes", "=", "[", "]", "\n", "for", "_i", ",", "word_idx", "in", "enumerate", "(", "word_ids", ")", ":", "\n", "# Special tokens have a word id that is None. We set the label to -100 so they are automatically", "\n", "# ignored in the loss function.", "\n", "                ", "if", "word_idx", "is", "None", ":", "\n", "                    ", "label_ids", ".", "append", "(", "-", "100", ")", "\n", "if", "(", "\n", "tokenized_inputs", "[", "\"input_ids\"", "]", "[", "i", "]", "[", "_i", "]", "\n", "==", "self", ".", "special_tokens_map", "[", "\n", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "\"pad_token\"", "]", "\n", "]", "\n", ")", ":", "\n", "                        ", "cur_bboxes", ".", "append", "(", "[", "1000", ",", "1000", ",", "1000", ",", "1000", "]", ")", "\n", "# A lazy solution for now as only [SEP] token", "\n", "# has the [1000, 1000, 1000, 1000] bbox", "\n", "\n", "", "else", ":", "\n", "                        ", "cur_bboxes", ".", "append", "(", "[", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "\n", "# We set the label for the first token of each word.", "\n", "", "", "elif", "word_idx", "!=", "previous_word_idx", ":", "\n", "                    ", "label_ids", ".", "append", "(", "int", "(", "label", "[", "word_idx", "]", ")", ")", "\n", "cur_bboxes", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "\n", "# For the other tokens in a word, we set the label to either the current label or -100, depending on", "\n", "# the label_all_tokens flag.", "\n", "", "else", ":", "\n", "                    ", "label_ids", ".", "append", "(", "\n", "int", "(", "label", "[", "word_idx", "]", ")", "if", "self", ".", "config", ".", "label_all_tokens", "else", "-", "100", "\n", ")", "\n", "cur_bboxes", ".", "append", "(", "bbox", "[", "word_idx", "]", ")", "\n", "\n", "", "previous_word_idx", "=", "word_idx", "\n", "\n", "", "if", "len", "(", "label_ids", ")", "==", "0", ":", "\n", "                ", "label_id", "=", "-", "100", "\n", "", "else", ":", "\n", "                ", "label_freq", "=", "Counter", "(", "label_ids", ")", "\n", "if", "len", "(", "label_freq", ")", "==", "1", "and", "-", "100", "in", "label_freq", ":", "\n", "                    ", "label_id", "=", "-", "100", "\n", "", "else", ":", "\n", "                    ", "label_freq", ".", "pop", "(", "-", "100", ")", "\n", "label_id", "=", "label_freq", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "\n", "", "", "labels", ".", "append", "(", "label_id", ")", "\n", "bboxes", ".", "append", "(", "cur_bboxes", ")", "\n", "\n", "", "tokenized_inputs", "[", "\"labels\"", "]", "=", "labels", "\n", "tokenized_inputs", "[", "\"bbox\"", "]", "=", "bboxes", "\n", "tokenized_inputs", "[", "\"num_tokens_per_group\"", "]", "=", "list", "(", "num_tokens_per_group", ".", "items", "(", ")", ")", "\n", "return", "tokenized_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_batch": [[121, 134], ["grouping.BaseGroupingPDFDataPreprocessor.iter_example", "grouping.BaseGroupingPDFDataPreprocessor.batchsize_examples", "grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample", "grouping.BaseGroupingPDFDataPreprocessor.pop", "all_processed_examples.append"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.iter_example", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.base.BasePDFDataPreprocessor.batchsize_examples", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BaseGroupingPDFDataPreprocessor.preprocess_sample"], ["", "def", "preprocess_batch", "(", "self", ",", "examples", ":", "Dict", "[", "str", ",", "List", "]", ")", "->", "Dict", "[", "str", ",", "List", "]", ":", "\n", "        ", "\"\"\"This is a wrapper based on the preprocess_sample. There might be\n        considerable performance loss, but we don't need to rewrite the main\n        iteration loop again.\n        \"\"\"", "\n", "all_processed_examples", "=", "[", "]", "\n", "for", "example", "in", "self", ".", "iter_example", "(", "examples", ")", ":", "\n", "            ", "processed_example", "=", "self", ".", "preprocess_sample", "(", "example", ",", "padding", "=", "False", ")", "\n", "processed_example", ".", "pop", "(", "\"num_tokens_per_group\"", ")", "\n", "# encoded_word_ids will only be used in test time", "\n", "all_processed_examples", ".", "append", "(", "processed_example", ")", "\n", "\n", "", "return", "self", ".", "batchsize_examples", "(", "all_processed_examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.RowGroupingPDFDataPreprocessor.preprocess_sequence": [[137, 139], ["grouping.split_example_based_on"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.split_example_based_on"], ["    ", "def", "preprocess_sequence", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "        ", "return", "split_example_based_on", "(", "example", ",", "\"line\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.BlockGroupingPDFDataPreprocessor.preprocess_sequence": [[142, 144], ["grouping.split_example_based_on"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.split_example_based_on"], ["    ", "def", "preprocess_sequence", "(", "self", ",", "example", ":", "Dict", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "        ", "return", "split_example_based_on", "(", "example", ",", "\"block\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.clean_group_ids": [[11, 16], ["enumerate", "itertools.groupby", "_lst.extend", "len", "list"], "function", ["None"], ["def", "clean_group_ids", "(", "lst", ")", ":", "\n", "    ", "_lst", "=", "[", "]", "\n", "for", "idx", ",", "(", "_", ",", "gp", ")", "in", "enumerate", "(", "itertools", ".", "groupby", "(", "lst", ")", ")", ":", "\n", "        ", "_lst", ".", "extend", "(", "[", "idx", "]", "*", "len", "(", "list", "(", "gp", ")", ")", ")", "\n", "", "return", "_lst", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.split_example_based_on": [[18, 41], ["grouping.clean_group_ids", "list", "enumerate", "example.keys", "itertools.groupby", "len", "list", "regrouped_sequence[].append"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.grouping.clean_group_ids"], ["", "def", "split_example_based_on", "(", "example", ":", "Dict", ",", "level", ":", "str", ")", "->", "List", "[", "Dict", "]", ":", "\n", "\n", "    ", "level_target", "=", "example", "[", "f\"{level}_ids\"", "]", "\n", "level_target", "=", "clean_group_ids", "(", "level_target", ")", "\n", "keys", "=", "list", "(", "example", ".", "keys", "(", ")", ")", "\n", "\n", "pre_index", "=", "0", "\n", "regrouped_sequence", "=", "{", "key", ":", "[", "]", "for", "key", "in", "keys", "}", "\n", "\n", "num_tokens_per_group", "=", "{", "}", "\n", "\n", "for", "level_idx", ",", "(", "orig_level_id", ",", "gp", ")", "in", "enumerate", "(", "itertools", ".", "groupby", "(", "level_target", ")", ")", ":", "\n", "        ", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "\n", "for", "key", "in", "keys", ":", "\n", "            ", "regrouped_sequence", "[", "key", "]", ".", "append", "(", "\n", "example", "[", "key", "]", "[", "pre_index", ":", "pre_index", "+", "cur_len", "]", "\n", ")", "\n", "\n", "", "pre_index", "+=", "cur_len", "\n", "num_tokens_per_group", "[", "orig_level_id", "]", "=", "cur_len", "\n", "\n", "", "return", "regrouped_sequence", ",", "num_tokens_per_group", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.HierarchicalPreTrainedModel._init_weights": [[125, 140], ["isinstance", "module.weight.data.normal_", "isinstance", "module.bias.data.zero_", "module.weight.data.normal_", "isinstance", "module.weight.data[].zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "if", "module", ".", "padding_idx", "is", "not", "None", ":", "\n", "                ", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.SimpleHierarchicalModel.__init__": [[143, 154], ["transformers.modeling_utils.PreTrainedModel.__init__", "hierarchical_model.instantiate_textline_encoder", "hierarchical_model.instantiate_textline_model", "hierarchical_model.SimpleHierarchicalModel._check_use_bbox_for_textline_model", "hierarchical_model.SimpleHierarchicalModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.__init__", "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.instantiate_textline_encoder", "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.instantiate_textline_model", "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.SimpleHierarchicalModel._check_use_bbox_for_textline_model"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "textline_encoder", "=", "instantiate_textline_encoder", "(", "config", ")", "\n", "self", ".", "textline_model", "=", "instantiate_textline_model", "(", "config", ")", "\n", "\n", "self", ".", "textline_encoder_output", "=", "config", ".", "textline_encoder_output", "\n", "self", ".", "use_bbox_for_textline_model", ":", "bool", "=", "self", ".", "_check_use_bbox_for_textline_model", "(", ")", "\n", "if", "not", "config", ".", "load_weights_from_existing_model", ":", "\n", "            ", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.SimpleHierarchicalModel._check_use_bbox_for_textline_model": [[155, 160], ["inspect.signature", "list", "inspect.signature.parameters.keys"], "methods", ["None"], ["", "", "def", "_check_use_bbox_for_textline_model", "(", "self", ")", "->", "bool", ":", "\n", "# TODO: Explain", "\n", "        ", "signature", "=", "inspect", ".", "signature", "(", "self", ".", "textline_model", ".", "forward", ")", "\n", "signature_columns", "=", "list", "(", "signature", ".", "parameters", ".", "keys", "(", ")", ")", "\n", "return", "\"bbox\"", "in", "signature_columns", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.SimpleHierarchicalModel.forward": [[163, 235], ["input_ids.reshape.reshape.size", "input_ids.reshape.reshape.reshape", "torch.ones.reshape", "torch.zeros.reshape", "hierarchical_model.SimpleHierarchicalModel.textline_encoder", "encoded_textlines.last_hidden_state.mean.last_hidden_state.mean.reshape", "hierarchical_model.SimpleHierarchicalModel.textline_model", "torch.ones", "torch.zeros", "torch.ones", "torch.zeros", "hierarchical_model.SimpleHierarchicalModel.textline_encoder_output.lower", "hierarchical_model.SimpleHierarchicalModel.textline_model.embeddings", "hierarchical_model.SimpleHierarchicalModel.textline_model.embeddings", "tuple", "hierarchical_model.SimpleHierarchicalModel.textline_encoder_output.lower", "hierarchical_model.SimpleHierarchicalModel.textline_encoder_output.lower", "hierarchical_model.SimpleHierarchicalModel.textline_encoder_output.lower", "encoded_textlines.last_hidden_state.mean.last_hidden_state.mean.last_hidden_state.mean", "list"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "group_level_attention_mask", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "return_dict", "=", "(", "\n", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", ")", "\n", "\n", "# Preprocess the input", "\n", "#TODO: Change Ls to something about groups", "\n", "B", ",", "Ls", ",", "Ts", "=", "input_ids", ".", "shape", "# Lines, Tokens", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "#TODO: Change this name to group_level_attention_mask", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "", "if", "group_level_attention_mask", "is", "None", ":", "# TODO: page_level_attention_mask", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "(", "B", ",", "Ls", ")", ",", "device", "=", "device", ")", "\n", "", "if", "bbox", "is", "None", ":", "\n", "            ", "bbox", "=", "torch", ".", "zeros", "(", "\n", "tuple", "(", "list", "(", "input_shape", ")", "[", ":", "-", "1", "]", "+", "[", "4", "]", ")", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", "\n", ")", "\n", "# There are 4 elements for each bounding box (left, top, right, bottom)", "\n", "# Currently we only accept line-level bbox", "\n", "\n", "", "input_ids", "=", "input_ids", ".", "reshape", "(", "B", "*", "Ls", ",", "-", "1", ")", "\n", "attention_mask", "=", "attention_mask", ".", "reshape", "(", "B", "*", "Ls", ",", "-", "1", ")", "\n", "token_type_ids", "=", "token_type_ids", ".", "reshape", "(", "B", "*", "Ls", ",", "-", "1", ")", "\n", "encoded_textlines", "=", "self", ".", "textline_encoder", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", ")", "# (B * Ls, Ts, hidden_state)", "\n", "\n", "# TODO: Change this to a pooling method ", "\n", "if", "self", ".", "textline_encoder_output", ".", "lower", "(", ")", "==", "\"cls\"", ":", "\n", "            ", "encoded_textlines", "=", "encoded_textlines", ".", "last_hidden_state", "[", ":", ",", "0", ",", ":", "]", "\n", "", "elif", "self", ".", "textline_encoder_output", ".", "lower", "(", ")", "==", "\"sep\"", ":", "\n", "# the sep token is 102 by default", "\n", "# for debugging ", "\n", "# _, ys = torch.where(input_ids == 102)", "\n", "# print(ys) ", "\n", "            ", "encoded_textlines", "=", "encoded_textlines", ".", "last_hidden_state", "[", "input_ids", "==", "102", "]", "\n", "#TODO: Move this into config_class", "\n", "", "elif", "self", ".", "textline_encoder_output", ".", "lower", "(", ")", "==", "\"last\"", ":", "\n", "            ", "encoded_textlines", "=", "encoded_textlines", ".", "last_hidden_state", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "elif", "self", ".", "textline_encoder_output", ".", "lower", "(", ")", "==", "\"average\"", ":", "\n", "            ", "encoded_textlines", "=", "encoded_textlines", ".", "last_hidden_state", ".", "mean", "(", "dim", "=", "1", ")", "\n", "\n", "", "encoded_textlines", "=", "encoded_textlines", ".", "reshape", "(", "B", ",", "Ls", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "use_bbox_for_textline_model", ":", "\n", "            ", "embedded_lines", "=", "self", ".", "textline_model", ".", "embeddings", "(", "\n", "inputs_embeds", "=", "encoded_textlines", ",", "bbox", "=", "bbox", "\n", ")", "\n", "", "else", ":", "\n", "            ", "embedded_lines", "=", "self", ".", "textline_model", ".", "embeddings", "(", "\n", "inputs_embeds", "=", "encoded_textlines", "\n", ")", "\n", "\n", "", "outputs", "=", "self", ".", "textline_model", "(", "\n", "inputs_embeds", "=", "embedded_lines", ",", "attention_mask", "=", "group_level_attention_mask", "\n", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.HierarchicalModelForTokenClassification.__init__": [[238, 248], ["transformers.modeling_utils.PreTrainedModel.__init__", "hierarchical_model.SimpleHierarchicalModel", "torch.nn.Dropout", "torch.nn.Linear", "hierarchical_model.HierarchicalModelForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "hierarchical_model", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.HierarchicalModelForTokenClassification.forward": [[249, 302], ["hierarchical_model.HierarchicalModelForTokenClassification.hierarchical_model", "hierarchical_model.HierarchicalModelForTokenClassification.dropout", "hierarchical_model.HierarchicalModelForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "torch.nn.CrossEntropyLoss", "hierarchical_model.HierarchicalModelForTokenClassification.view", "torch.where", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "group_level_attention_mask.view", "labels.view", "torch.tensor().type_as", "hierarchical_model.HierarchicalModelForTokenClassification.view", "labels.view", "torch.tensor"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", ",", "# This shouldn't be none", "\n", "bbox", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "group_level_attention_mask", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "return_dict", "=", "(", "\n", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", ")", "\n", "\n", "outputs", "=", "self", ".", "hierarchical_model", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "bbox", "=", "bbox", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "group_level_attention_mask", "=", "group_level_attention_mask", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "# last_hidden_state", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "100", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "group_level_attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "group_level_attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "\n", "active_labels", "=", "torch", ".", "where", "(", "\n", "active_loss", ",", "\n", "labels", ".", "view", "(", "-", "1", ")", ",", "\n", "torch", ".", "tensor", "(", "loss_fct", ".", "ignore_index", ")", ".", "type_as", "(", "labels", ")", ",", "\n", ")", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", ""]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.instantiate_textline_encoder": [[25, 57], ["transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "transformers.AutoModel.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "torch.nn.ModuleList", "torch.nn.ModuleList"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained"], ["def", "instantiate_textline_encoder", "(", "config", ")", ":", "\n", "\n", "    ", "if", "not", "config", ".", "load_weights_from_existing_model", ":", "\n", "\n", "        ", "if", "config", ".", "textline_encoder_type", "==", "\"bert-base-uncased\"", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "elif", "config", ".", "textline_encoder_type", "==", "\"bert-layer\"", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "config", ".", "num_hidden_layers", "=", "1", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "else", ":", "\n", "            ", "raise", "(", "\n", "f\"Invalid textline_encoder_type: {config.textline_encoder_type}.\"", "\n", "\"Must be one of {bert-base-uncased, bert-layer}\"", "\n", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "        ", "if", "config", ".", "textline_encoder_type", "in", "[", "\"bert-layer\"", ",", "\"bert-base-uncased\"", "]", ":", "\n", "            ", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "if", "config", ".", "textline_encoder_type", "==", "\"bert-layer\"", ":", "\n", "                ", "if", "config", ".", "textline_encoder_used_bert_layer", "==", "\"first\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "0", "]", "]", ")", "\n", "", "elif", "config", ".", "textline_encoder_used_bert_layer", "==", "\"last\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "-", "1", "]", "]", ")", "\n", "", "", "return", "model", "\n", "", "else", ":", "\n", "            ", "raise", "(", "\n", "f\"Invalid textline_encoder_type: {config.textline_encoder_type}.\"", "\n", "\"Must be one of {bert-base-uncased, bert-layer}\"", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.hierarchical_model.instantiate_textline_model": [[61, 111], ["transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "transformers.AutoModel.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "transformers.AutoModel.from_pretrained", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "torch.nn.ModuleList", "transformers.AutoConfig.from_pretrained", "transformers.AutoModel.from_config", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained"], ["", "", "", "def", "instantiate_textline_model", "(", "config", ")", ":", "\n", "    ", "if", "not", "config", ".", "load_weights_from_existing_model", ":", "\n", "        ", "if", "config", ".", "textline_model_type", "==", "\"bert-base-uncased\"", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "elif", "config", ".", "textline_model_type", "==", "\"bert-layer\"", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "config", ".", "num_hidden_layers", "=", "1", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "elif", "\"layoutlm-base-uncased\"", "in", "config", ".", "textline_model_type", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"microsoft/layoutlm-base-uncased\"", ")", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "elif", "config", ".", "textline_model_type", "==", "\"layoutlm-layer\"", ":", "\n", "            ", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\"microsoft/layoutlm-base-uncased\"", ")", "\n", "config", ".", "num_hidden_layers", "=", "1", "\n", "model", "=", "AutoModel", ".", "from_config", "(", "config", ")", "\n", "return", "model", "\n", "", "else", ":", "\n", "            ", "raise", "(", "\n", "f\"Invalid textline_model_type: {config.textline_model_type}.\"", "\n", "\"Must be one of {bert-base-uncased, bert-layer, \"", "\n", "\"layoutlm-base-uncased, layoutlm-layer}\"", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "config", ".", "textline_model_type", "in", "[", "\"bert-layer\"", ",", "\"bert-base-uncased\"", "]", ":", "\n", "            ", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "if", "config", ".", "textline_model_type", "==", "\"bert-layer\"", ":", "\n", "                ", "if", "config", ".", "textline_model_used_bert_layer", "==", "\"first\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "0", "]", "]", ")", "\n", "", "elif", "config", ".", "textline_model_used_bert_layer", "==", "\"last\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "-", "1", "]", "]", ")", "\n", "", "", "return", "model", "\n", "", "elif", "config", ".", "textline_model_type", "in", "[", "\n", "\"layoutlm-layer\"", ",", "\n", "\"layoutlm-base-uncased\"", ",", "\n", "\"microsoft/layoutlm-base-uncased\"", ",", "\n", "]", ":", "\n", "            ", "model", "=", "AutoModel", ".", "from_pretrained", "(", "\"microsoft/layoutlm-base-uncased\"", ")", "\n", "if", "config", ".", "textline_model_type", "==", "\"layoutlm-layer\"", ":", "\n", "                ", "if", "config", ".", "textline_model_used_bert_layer", "==", "\"first\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "0", "]", "]", ")", "\n", "", "elif", "config", ".", "textline_model_used_bert_layer", "==", "\"last\"", ":", "\n", "                    ", "model", ".", "encoder", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "model", ".", "encoder", ".", "layer", "[", "-", "1", "]", "]", ")", "\n", "", "", "return", "model", "\n", "", "else", ":", "\n", "            ", "raise", "(", "\n", "f\"Invalid textline_encoder_type: {config.textline_encoder_type}.\"", "\n", "\"Must be one of {bert-base-uncased, bert-layer}\"", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.models.configuration_hierarchical_model.HierarchicalModelConfig.__init__": [[11, 43], ["transformers.configuration_utils.PretrainedConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "textline_encoder_type", "=", "\"bert-layer\"", ",", "\n", "textline_model_type", "=", "\"bert-layer\"", ",", "\n", "textline_encoder_output", "=", "\"cls\"", ",", "\n", "load_weights_from_existing_model", "=", "False", ",", "\n", "textline_encoder_used_bert_layer", "=", "\"first\"", ",", "\n", "textline_model_used_bert_layer", "=", "\"first\"", ",", "\n", "pad_token_id", "=", "0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "hidden_size", "=", "768", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad_token_id", "=", "pad_token_id", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "\n", "self", ".", "textline_encoder_type", "=", "textline_encoder_type", "\n", "self", ".", "textline_model_type", "=", "textline_model_type", "\n", "self", ".", "textline_encoder_output", "=", "textline_encoder_output", "\n", "\n", "self", ".", "load_weights_from_existing_model", "=", "load_weights_from_existing_model", "\n", "self", ".", "textline_encoder_used_bert_layer", "=", "textline_encoder_used_bert_layer", "\n", "self", ".", "textline_model_used_bert_layer", "=", "textline_model_used_bert_layer", "\n", "\n", "self", ".", "pad_token_id", "=", "pad_token_id", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "hidden_size", "=", "hidden_size", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.argsort": [[21, 24], ["sorted", "range", "len"], "function", ["None"], ["def", "argsort", "(", "seq", ")", ":", "\n", "# http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python", "\n", "    ", "return", "sorted", "(", "range", "(", "len", "(", "seq", ")", ")", ",", "key", "=", "seq", ".", "__getitem__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_element": [[25, 27], ["collections.Counter().most_common", "collections.Counter"], "function", ["None"], ["", "def", "get_most_common_element", "(", "lst", ")", ":", "\n", "    ", "return", "Counter", "(", "lst", ")", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_token_type": [[28, 30], ["vision_postprocessor.get_most_common_element"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_element"], ["", "def", "get_most_common_token_type", "(", "tokens", ")", ":", "\n", "    ", "return", "get_most_common_element", "(", "[", "ele", ".", "type", "for", "ele", "in", "tokens", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.union_box": [[31, 46], ["len", "layoutparser.Rectangle", "layoutparser.TextBlock", "layoutparser.Rectangle", "layoutparser.TextBlock", "float", "float", "float", "float", "min", "min", "max", "max", "int", "int", "int", "int"], "function", ["None"], ["", "def", "union_box", "(", "blocks", ")", ":", "\n", "    ", "if", "len", "(", "blocks", ")", "==", "0", ":", "\n", "# print(\"Warning: The length of blocks is 0!\")", "\n", "        ", "rect", "=", "lp", ".", "Rectangle", "(", "0", ",", "0", ",", "0", ",", "0", ")", "\n", "return", "lp", ".", "TextBlock", "(", "rect", ")", "\n", "", "else", ":", "\n", "        ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"-inf\"", ")", ",", "float", "(", "\"-inf\"", ")", "\n", "for", "block", "in", "blocks", ":", "\n", "            ", "bbox", "=", "block", ".", "coordinates", "\n", "x1", "=", "min", "(", "x1", ",", "bbox", "[", "0", "]", ")", "\n", "y1", "=", "min", "(", "y1", ",", "bbox", "[", "1", "]", ")", "\n", "x2", "=", "max", "(", "x2", ",", "bbox", "[", "2", "]", ")", "\n", "y2", "=", "max", "(", "y2", ",", "bbox", "[", "3", "]", ")", "\n", "", "rect", "=", "lp", ".", "Rectangle", "(", "int", "(", "x1", ")", ",", "int", "(", "y1", ")", ",", "int", "(", "x2", ")", ",", "int", "(", "y2", ")", ")", "\n", "return", "lp", ".", "TextBlock", "(", "rect", ",", "type", "=", "blocks", "[", "0", "]", ".", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in": [[47, 65], ["block_a.is_in", "vision_postprocessor.calculate_overlapping_coefficient", "vision_postprocessor.is_in", "vision_postprocessor.is_in"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "", "def", "is_in", "(", "block_a", ",", "block_b", ",", "metric", "=", "\"center\"", ")", ":", "\n", "    ", "\"\"\"A rewrite of the lp.LayoutElement.is_in function.\n    We will use a soft_margin and center function by default.\n    \"\"\"", "\n", "if", "metric", "==", "\"center\"", ":", "\n", "        ", "return", "block_a", ".", "is_in", "(", "\n", "block_b", ",", "\n", "soft_margin", "=", "{", "\"top\"", ":", "1", ",", "\"bottom\"", ":", "1", ",", "\"left\"", ":", "1", ",", "\"right\"", ":", "1", "}", ",", "\n", "center", "=", "True", ",", "\n", ")", "\n", "", "elif", "metric", "==", "\"coef\"", ":", "\n", "        ", "return", "(", "\n", "calculate_overlapping_coefficient", "(", "block_a", ",", "block_b", ")", "\n", ">", "MIN_OVERLAPPING_THRESHOLD", "\n", ")", "\n", "", "elif", "metric", "==", "\"any\"", ":", "\n", "        ", "return", "is_in", "(", "block_a", ",", "block_b", ",", "metric", "=", "\"center\"", ")", "or", "is_in", "(", "\n", "block_a", ",", "block_b", ",", "metric", "=", "\"coef\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_non_textual_type": [[67, 72], ["isinstance", "ValueError", "type"], "function", ["None"], ["", "", "def", "is_non_textual_type", "(", "block", ")", ":", "\n", "    ", "if", "isinstance", "(", "block", ".", "type", ",", "str", ")", ":", "\n", "        ", "return", "block", ".", "type", "in", "NON_TEXTUAL_TYPES", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"strange block type data type {type(block.type)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.cvt_cermine_df_to_layout": [[73, 85], ["layoutparser.TextBlock", "layoutparser.Rectangle"], "function", ["None"], ["", "", "def", "cvt_cermine_df_to_layout", "(", "row", ")", ":", "\n", "\n", "    ", "return", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "row", "[", "\"x_1\"", "]", ",", "\n", "row", "[", "\"y_1\"", "]", ",", "\n", "row", "[", "\"x_2\"", "]", ",", "\n", "row", "[", "\"y_2\"", "]", ",", "\n", ")", ",", "\n", "id", "=", "row", "[", "\"id\"", "]", ",", "\n", "type", "=", "row", "[", "\"category\"", "]", ",", "\n", "text", "=", "row", "[", "\"text\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.cvt_line_df_to_layout": [[87, 97], ["layoutparser.TextBlock", "layoutparser.Rectangle"], "function", ["None"], ["", "def", "cvt_line_df_to_layout", "(", "row", ")", ":", "\n", "\n", "    ", "return", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "row", "[", "\"x_1\"", "]", ",", "\n", "row", "[", "\"y_1\"", "]", ",", "\n", "row", "[", "\"x_2\"", "]", ",", "\n", "row", "[", "\"y_2\"", "]", ",", "\n", ")", ",", "\n", "id", "=", "row", "[", "\"id\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.cvt_block_df_to_layout": [[99, 110], ["layoutparser.TextBlock", "layoutparser.Rectangle"], "function", ["None"], ["", "def", "cvt_block_df_to_layout", "(", "row", ")", ":", "\n", "\n", "    ", "return", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "row", "[", "\"x_1\"", "]", ",", "\n", "row", "[", "\"y_1\"", "]", ",", "\n", "row", "[", "\"x_2\"", "]", ",", "\n", "row", "[", "\"y_2\"", "]", ",", "\n", ")", ",", "\n", "id", "=", "row", "[", "\"id\"", "]", ",", "\n", "type", "=", "row", "[", "\"category\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_cermine_data_from_csv": [[112, 124], ["pandas.read_csv", "layoutparser.Layout", "len", "len", "tokens_df.apply().tolist", "pd.read_csv.text.isna", "tokens_df.apply"], "function", ["None"], ["", "def", "load_cermine_data_from_csv", "(", "filename", ")", ":", "\n", "    ", "df", "=", "pd", ".", "read_csv", "(", "filename", ")", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "\n", "", "df", "=", "df", "[", "~", "df", ".", "text", ".", "isna", "(", ")", "]", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "\n", "", "tokens_df", "=", "df", "[", "~", "df", ".", "is_line", "&", "~", "df", ".", "is_block", "]", "\n", "\n", "return", "lp", ".", "Layout", "(", "tokens_df", ".", "apply", "(", "cvt_cermine_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_line_data_from_csv": [[125, 129], ["pandas.read_csv", "layoutparser.Layout", "pd.read_csv.apply().tolist", "pd.read_csv.apply"], "function", ["None"], ["", "def", "load_line_data_from_csv", "(", "filename", ")", ":", "\n", "    ", "df", "=", "pd", ".", "read_csv", "(", "filename", ")", "\n", "return", "lp", ".", "Layout", "(", "\n", "df", ".", "apply", "(", "cvt_line_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_block_data_from_csv": [[131, 135], ["pandas.read_csv", "layoutparser.Layout", "pd.read_csv.apply().tolist", "pd.read_csv.apply"], "function", ["None"], ["", "def", "load_block_data_from_csv", "(", "filename", ")", ":", "\n", "    ", "df", "=", "pd", ".", "read_csv", "(", "filename", ")", "\n", "return", "lp", ".", "Layout", "(", "\n", "df", ".", "apply", "(", "cvt_block_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.calculate_overlapping_coefficient": [[137, 152], ["min", "layoutparser.Rectangle", "max", "max", "min", "min"], "function", ["None"], ["", "def", "calculate_overlapping_coefficient", "(", "box1", ",", "box2", ")", ":", "\n", "    ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "box1", ".", "coordinates", "\n", "a1", ",", "b1", ",", "a2", ",", "b2", "=", "box2", ".", "coordinates", "\n", "\n", "if", "x2", "<", "a1", "or", "x1", ">", "a2", "or", "y1", ">", "b2", "or", "y2", "<", "b1", ":", "# Bottom or top", "\n", "        ", "return", "0", "\n", "\n", "", "min_area", "=", "min", "(", "box1", ".", "area", ",", "box2", ".", "area", ")", "\n", "if", "min_area", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "intersection", "=", "lp", ".", "Rectangle", "(", "\n", "x_1", "=", "max", "(", "x1", ",", "a1", ")", ",", "y_1", "=", "max", "(", "y1", ",", "b1", ")", ",", "x_2", "=", "min", "(", "x2", ",", "a2", ")", ",", "y_2", "=", "min", "(", "y2", ",", "b2", ")", "\n", ")", "\n", "return", "intersection", ".", "area", "/", "min_area", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.calculate_pairwise_overlapping_coefficient": [[153, 174], ["numpy.array", "len", "numpy.zeros", "range", "numpy.tril_indices", "range", "vision_postprocessor.calculate_overlapping_coefficient", "vision_postprocessor.calculate_overlapping_coefficient"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient"], ["", "", "def", "calculate_pairwise_overlapping_coefficient", "(", "blocks_A", ",", "blocks_B", "=", "None", ")", ":", "\n", "    ", "if", "blocks_B", "is", "not", "None", ":", "\n", "        ", "return", "np", ".", "array", "(", "\n", "[", "\n", "[", "calculate_overlapping_coefficient", "(", "box1", ",", "box2", ")", "for", "box2", "in", "blocks_B", "]", "\n", "for", "box1", "in", "blocks_A", "\n", "]", "\n", ")", "\n", "", "else", ":", "\n", "        ", "n", "=", "len", "(", "blocks_A", ")", "\n", "overlapping", "=", "np", ".", "zeros", "(", "(", "n", ",", "n", ")", ")", "\n", "for", "row", "in", "range", "(", "n", ")", ":", "\n", "            ", "for", "col", "in", "range", "(", "row", "+", "1", ",", "n", ")", ":", "\n", "                ", "overlapping", "[", "row", ",", "col", "]", "=", "calculate_overlapping_coefficient", "(", "\n", "blocks_A", "[", "row", "]", ",", "blocks_A", "[", "col", "]", "\n", ")", "\n", "\n", "", "", "i_lower", "=", "np", ".", "tril_indices", "(", "n", ",", "k", "=", "-", "1", ")", "\n", "overlapping", "[", "i_lower", "]", "=", "overlapping", ".", "T", "[", "i_lower", "]", "\n", "# A trick learned from https://stackoverflow.com/a/42209263", "\n", "return", "overlapping", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.remove_overlapping_textual_blocks_for_non_textual_blocks": [[177, 199], ["vision_postprocessor.calculate_pairwise_overlapping_coefficient", "numpy.where", "calculate_pairwise_overlapping_coefficient.any", "vision_postprocessor.is_non_textual_type", "len", "len", "vision_postprocessor.is_non_textual_type", "enumerate", "numpy.unique"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.calculate_pairwise_overlapping_coefficient", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_non_textual_type", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_non_textual_type"], ["def", "remove_overlapping_textual_blocks_for_non_textual_blocks", "(", "blocks", ")", ":", "\n", "# Firstly checking paragraph and non-paragraph blocks", "\n", "\n", "    ", "textual_blocks", "=", "[", "b", "for", "b", "in", "blocks", "if", "not", "is_non_textual_type", "(", "b", ")", "]", "\n", "non_textual_blocks", "=", "[", "b", "for", "b", "in", "blocks", "if", "is_non_textual_type", "(", "b", ")", "]", "\n", "\n", "if", "len", "(", "textual_blocks", ")", "==", "0", "or", "len", "(", "non_textual_blocks", ")", "==", "0", ":", "\n", "        ", "return", "textual_blocks", "+", "non_textual_blocks", "\n", "\n", "", "overlapping", "=", "calculate_pairwise_overlapping_coefficient", "(", "\n", "non_textual_blocks", ",", "textual_blocks", "\n", ")", "\n", "overlapping", "=", "overlapping", ">", "0.8", "\n", "\n", "if", "not", "overlapping", ".", "any", "(", ")", ":", "\n", "        ", "return", "textual_blocks", "+", "non_textual_blocks", "\n", "\n", "", "nids", ",", "tids", "=", "np", ".", "where", "(", "overlapping", ")", "\n", "\n", "return", "[", "\n", "b", "for", "idx", ",", "b", "in", "enumerate", "(", "textual_blocks", ")", "if", "idx", "not", "in", "np", ".", "unique", "(", "tids", ")", "\n", "]", "+", "non_textual_blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_elements": [[200, 214], ["setattr", "vision_postprocessor.is_in", "setattr", "remaining_tokens.append", "int"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "def", "find_parent_for_elements", "(", "block_layout", ",", "token_layout", ",", "target_attr", "=", "\"parent\"", ")", ":", "\n", "\n", "    ", "for", "block", "in", "block_layout", ":", "\n", "        ", "remaining_tokens", "=", "[", "]", "\n", "for", "token", "in", "token_layout", ":", "\n", "            ", "if", "is_in", "(", "token", ",", "block", ")", ":", "\n", "                ", "setattr", "(", "token", ",", "target_attr", ",", "int", "(", "block", ".", "id", ")", ")", "\n", "", "else", ":", "\n", "                ", "remaining_tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "", "token_layout", "=", "remaining_tokens", "\n", "\n", "", "for", "token", "in", "token_layout", ":", "\n", "        ", "setattr", "(", "token", ",", "target_attr", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.block_snapping": [[215, 225], ["vision_postprocessor.is_non_textual_type", "vision_postprocessor.union_box", "tokens_in_this_group.append"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_non_textual_type", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "", "def", "block_snapping", "(", "blocks", ",", "tokens", ")", ":", "\n", "\n", "    ", "for", "block", "in", "blocks", ":", "\n", "        ", "if", "is_non_textual_type", "(", "block", ")", ":", "\n", "            ", "continue", "\n", "", "tokens_in_this_group", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", ".", "parent", "==", "block", ".", "id", ":", "\n", "                ", "tokens_in_this_group", ".", "append", "(", "token", ")", "\n", "", "", "block", ".", "block", "=", "union_box", "(", "tokens_in_this_group", ")", ".", "block", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.filter_out_overlapping_block": [[226, 249], ["vision_postprocessor.calculate_overlapping_coefficient"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient"], ["", "", "def", "filter_out_overlapping_block", "(", "blocks", ")", ":", "\n", "\n", "    ", "boxes_to_remove", "=", "{", "b", ".", "id", ":", "0", "for", "b", "in", "blocks", "}", "\n", "for", "box2", "in", "blocks", ":", "\n", "        ", "for", "box1", "in", "blocks", ":", "\n", "\n", "            ", "if", "box1", ".", "id", "==", "box2", ".", "id", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "boxes_to_remove", "[", "box1", ".", "id", "]", "or", "boxes_to_remove", "[", "box2", ".", "id", "]", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "(", "\n", "calculate_overlapping_coefficient", "(", "box1", ",", "box2", ")", "\n", ">", "MIN_OVERLAPPING_THRESHOLD", "\n", ")", ":", "\n", "\n", "                ", "if", "box1", ".", "area", ">=", "box2", ".", "area", ":", "\n", "                    ", "boxes_to_remove", "[", "box2", ".", "id", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "boxes_to_remove", "[", "box1", ".", "id", "]", "=", "1", "\n", "\n", "", "", "", "", "return", "[", "b", "for", "b", "in", "blocks", "if", "not", "boxes_to_remove", "[", "b", ".", "id", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.filter_out_overlapping_block_and_union": [[250, 272], ["vision_postprocessor.calculate_pairwise_overlapping_coefficient", "scipy.sparse.csgraph.connected_components", "itertools.groupby", "len", "new_blocks.append", "list", "union_box().set", "vision_postprocessor.union_box", "sorted"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.calculate_pairwise_overlapping_coefficient", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "filter_out_overlapping_block_and_union", "(", "blocks", ")", ":", "\n", "\n", "    ", "overlapping", "=", "calculate_pairwise_overlapping_coefficient", "(", "blocks", ")", "\n", "n_components", ",", "labels", "=", "connected_components", "(", "\n", "csgraph", "=", "overlapping", ">", "MIN_OVERLAPPING_THRESHOLD", ",", "\n", "directed", "=", "False", ",", "\n", "return_labels", "=", "True", ",", "\n", ")", "\n", "\n", "new_blocks", "=", "[", "]", "\n", "prev_len", "=", "0", "\n", "for", "idx", ",", "gp", "in", "groupby", "(", "labels", ")", ":", "\n", "        ", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "cur_blocks", "=", "blocks", "[", "prev_len", ":", "prev_len", "+", "cur_len", "]", "\n", "new_blocks", ".", "append", "(", "\n", "union_box", "(", "sorted", "(", "cur_blocks", ",", "key", "=", "lambda", "b", ":", "b", ".", "area", ",", "reverse", "=", "True", ")", ")", ".", "set", "(", "\n", "id", "=", "idx", "\n", ")", "\n", ")", "\n", "prev_len", "+=", "cur_len", "\n", "\n", "", "return", "new_blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_close": [[273, 286], ["abs"], "function", ["None"], ["", "def", "is_close", "(", "block1", ",", "block2", ",", "x_tolerance", "=", "15", ",", "y_tolerance", "=", "16", ")", ":", "\n", "# horizontal difference", "\n", "    ", "bbox0", "=", "block1", ".", "coordinates", "\n", "bbox1", "=", "block2", ".", "coordinates", "\n", "if", "bbox1", "[", "0", "]", "-", "bbox0", "[", "2", "]", ">", "x_tolerance", "or", "bbox0", "[", "0", "]", "-", "bbox1", "[", "2", "]", ">", "x_tolerance", ":", "\n", "        ", "return", "False", "\n", "\n", "# line difference", "\n", "", "_", ",", "y1", "=", "block1", ".", "block", ".", "center", "\n", "_", ",", "y2", "=", "block2", ".", "block", ".", "center", "\n", "if", "abs", "(", "y1", "-", "y2", ")", ">", "y_tolerance", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.group_contents": [[287, 320], ["random.choice", "grouped_tokens.append", "current_group.append", "queue.pop", "vision_postprocessor.is_close", "queue.append"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_close"], ["", "def", "group_contents", "(", "tokens", ",", "x_tolerance", "=", "15", ",", "y_tolerance", "=", "16", ")", ":", "\n", "\n", "    ", "selected_mask", "=", "{", "b", ".", "id", ":", "0", "for", "b", "in", "tokens", "}", "\n", "\n", "grouped_tokens", "=", "[", "]", "\n", "\n", "cur_tokens", "=", "tokens", "\n", "\n", "while", "cur_tokens", ":", "\n", "\n", "        ", "current_group", "=", "[", "]", "\n", "\n", "# start from a random sample to improve robustness", "\n", "start_token", "=", "random", ".", "choice", "(", "cur_tokens", ")", "\n", "\n", "queue", "=", "[", "start_token", "]", "\n", "selected_mask", "[", "start_token", ".", "id", "]", "=", "1", "\n", "\n", "while", "queue", ":", "\n", "            ", "cur_token", "=", "queue", "[", "0", "]", "\n", "for", "candidate_token", "in", "cur_tokens", ":", "\n", "                ", "if", "not", "selected_mask", "[", "candidate_token", ".", "id", "]", "and", "is_close", "(", "\n", "cur_token", ",", "candidate_token", ",", "x_tolerance", ",", "y_tolerance", "\n", ")", ":", "\n", "                    ", "queue", ".", "append", "(", "candidate_token", ")", "\n", "selected_mask", "[", "candidate_token", ".", "id", "]", "=", "1", "\n", "\n", "", "", "current_group", ".", "append", "(", "queue", ".", "pop", "(", "0", ")", ")", "\n", "\n", "", "grouped_tokens", ".", "append", "(", "current_group", ")", "\n", "cur_tokens", "=", "[", "token", "for", "token", "in", "cur_tokens", "if", "not", "selected_mask", "[", "token", ".", "id", "]", "]", "\n", "\n", "", "return", "grouped_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.group_ungrouped_elements": [[321, 332], ["vision_postprocessor.group_contents", "union_box().set", "getattr", "vision_postprocessor.union_box"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.group_contents", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "group_ungrouped_elements", "(", "\n", "tokens", ",", "attr_name", "=", "\"parent\"", ",", "x_tolerance", "=", "15", ",", "y_tolerance", "=", "16", "\n", ")", ":", "\n", "    ", "selected_tokens", "=", "[", "\n", "b", "\n", "for", "b", "in", "tokens", "\n", "if", "getattr", "(", "b", ",", "attr_name", ",", "None", ")", "is", "None", "\n", "]", "\n", "\n", "results", "=", "group_contents", "(", "selected_tokens", ",", "x_tolerance", ",", "y_tolerance", ")", "\n", "return", "[", "union_box", "(", "ele", ")", ".", "set", "(", "text", "=", "\" \"", ".", "join", "(", "i", ".", "text", "for", "i", "in", "ele", ")", ")", "for", "ele", "in", "results", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.absorb_additional_blocks_into_existing_blocks": [[341, 413], ["vision_postprocessor.calculate_pairwise_overlapping_coefficient", "numpy.where", "len", "itertools.groupby", "len", "len", "len", "block_ids_to_remove.append", "additional_block_ids_to_remove.extend", "newly_added_blocks.append", "list", "vision_postprocessor.union_box", "enumerate", "enumerate"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.calculate_pairwise_overlapping_coefficient", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["def", "absorb_additional_blocks_into_existing_blocks", "(", "\n", "blocks", ",", "additional_blocks", ",", "threshold", "=", "MIN_OVERLAPPING_THRESHOLD", "\n", ")", ":", "\n", "\n", "    ", "if", "len", "(", "blocks", ")", "==", "0", "or", "len", "(", "additional_blocks", ")", "==", "0", ":", "\n", "        ", "return", "blocks", ",", "additional_blocks", "\n", "\n", "", "overlapping", "=", "calculate_pairwise_overlapping_coefficient", "(", "additional_blocks", ",", "blocks", ")", "\n", "block_indices", ",", "add_block_indices", "=", "np", ".", "where", "(", "overlapping", ".", "T", ">=", "threshold", ")", "\n", "# Ensure the block_indices are appropriately ordered", "\n", "\n", "if", "len", "(", "add_block_indices", ")", "==", "0", ":", "\n", "        ", "return", "blocks", ",", "additional_blocks", "\n", "", "else", ":", "\n", "        ", "block_ids_to_remove", "=", "[", "]", "\n", "additional_block_ids_to_remove", "=", "[", "]", "\n", "newly_added_blocks", "=", "[", "]", "\n", "\n", "prev_len", "=", "0", "\n", "for", "orig_idx", ",", "gp", "in", "groupby", "(", "block_indices", ")", ":", "\n", "            ", "cur_len", "=", "len", "(", "list", "(", "gp", ")", ")", "\n", "additional_block_indices_in_this_group", "=", "add_block_indices", "[", "\n", "prev_len", ":", "prev_len", "+", "cur_len", "\n", "]", "\n", "block_ids_to_remove", ".", "append", "(", "orig_idx", ")", "\n", "additional_block_ids_to_remove", ".", "extend", "(", "\n", "additional_block_indices_in_this_group", "\n", ")", "\n", "\n", "newly_added_blocks", ".", "append", "(", "\n", "union_box", "(", "\n", "[", "blocks", "[", "orig_idx", "]", "]", "\n", "+", "[", "\n", "additional_blocks", "[", "ad_idx", "]", "\n", "for", "ad_idx", "in", "additional_block_indices_in_this_group", "\n", "]", "\n", ")", "\n", "# it will keep the category from the original block", "\n", ")", "\n", "prev_len", "+=", "cur_len", "\n", "\n", "# for ad_idx, orig_idx in zip(add_block_indices, block_indices):", "\n", "#     if overlapping[ad_idx, orig_idx] < MIN_OVERLAPPING_THRESHOLD:", "\n", "#         continue", "\n", "\n", "#     block_ids_to_remove.append(orig_idx)", "\n", "#     additional_block_ids_to_remove.append(orig_idx)", "\n", "\n", "#     newly_added_blocks.append(", "\n", "#         union_box([blocks[orig_idx], additional_blocks[ad_idx]])", "\n", "#         # it will keep the category from the original block", "\n", "#     )", "\n", "\n", "# for ad_idx, orig_idx in zip(add_block_indices, block_indices):", "\n", "#     if overlapping[ad_idx, orig_idx] < MIN_OVERLAPPING_THRESHOLD:", "\n", "#         continue", "\n", "\n", "#     block_ids_to_remove.append(orig_idx)", "\n", "#     additional_block_ids_to_remove.append(orig_idx)", "\n", "\n", "#     newly_added_blocks.append(", "\n", "#         union_box([blocks[orig_idx], additional_blocks[ad_idx]])", "\n", "#         # it will keep the category from the original block", "\n", "#     )", "\n", "", "return", "(", "\n", "[", "b", "for", "idx", ",", "b", "in", "enumerate", "(", "blocks", ")", "if", "idx", "not", "in", "block_ids_to_remove", "]", ",", "\n", "[", "\n", "b", "\n", "for", "idx", ",", "b", "in", "enumerate", "(", "additional_blocks", ")", "\n", "if", "idx", "not", "in", "additional_block_ids_to_remove", "\n", "]", "\n", "+", "newly_added_blocks", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_all_elements_and_reassign_block_category": [[415, 468], ["enumerate", "len", "float", "block_min_token_ids.append", "setattr", "setattr", "vision_postprocessor.is_in", "sorted_block_token_indices.get", "int", "setattr", "tokens_in_this_block.append", "min", "remaining_tokens.append", "enumerate", "getattr", "vision_postprocessor.argsort", "vision_postprocessor.find_closet_block_for_token"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.argsort", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_closet_block_for_token"], ["", "", "def", "find_parent_for_all_elements_and_reassign_block_category", "(", "\n", "block_layout", ",", "token_layout", ",", "target_attr", "=", "\"parent\"", ",", "block_ordering_method", "=", "\"token_id\"", "\n", ")", ":", "\n", "\n", "    ", "assert", "len", "(", "block_layout", ")", ">", "0", "\n", "\n", "block_min_token_ids", "=", "[", "]", "\n", "\n", "iterating_tokens", "=", "token_layout", "\n", "for", "idx", ",", "block", "in", "enumerate", "(", "block_layout", ")", ":", "\n", "        ", "block", ".", "id", "=", "idx", "\n", "remaining_tokens", "=", "[", "]", "\n", "tokens_in_this_block", "=", "[", "]", "\n", "block_min_token_id", "=", "float", "(", "\"inf\"", ")", "\n", "for", "token", "in", "iterating_tokens", ":", "\n", "            ", "if", "is_in", "(", "token", ",", "block", ")", ":", "\n", "                ", "setattr", "(", "token", ",", "target_attr", ",", "idx", ")", "\n", "tokens_in_this_block", ".", "append", "(", "token", ")", "\n", "block_min_token_id", "=", "min", "(", "token", ".", "id", ",", "block_min_token_id", ")", "\n", "", "else", ":", "\n", "                ", "remaining_tokens", ".", "append", "(", "token", ")", "\n", "\n", "", "", "iterating_tokens", "=", "remaining_tokens", "\n", "# if is_non_textual_type(block):", "\n", "#     for token in tokens_in_this_block:", "\n", "#         token.type = block.type", "\n", "# else:", "\n", "#     token_types_in_this_block = [b.type for b in tokens_in_this_block]", "\n", "#     block.type = get_most_common_element(token_types_in_this_block)", "\n", "\n", "block_min_token_ids", ".", "append", "(", "block_min_token_id", ")", "\n", "\n", "", "if", "block_ordering_method", "==", "\"token_id\"", ":", "\n", "        ", "sorted_block_token_indices", "=", "{", "\n", "orig_id", ":", "new_id", "\n", "for", "new_id", ",", "orig_id", "in", "enumerate", "(", "argsort", "(", "block_min_token_ids", ")", ")", "\n", "}", "\n", "\n", "", "for", "token", "in", "token_layout", ":", "\n", "        ", "setattr", "(", "\n", "token", ",", "\n", "target_attr", ",", "\n", "sorted_block_token_indices", ".", "get", "(", "getattr", "(", "token", ",", "target_attr", ",", "None", ")", ",", "None", ")", ",", "\n", ")", "\n", "", "for", "block", "in", "block_layout", ":", "\n", "        ", "block", ".", "id", "=", "sorted_block_token_indices", "[", "block", ".", "id", "]", "\n", "\n", "# print(", "\n", "#     f\"Searching the closet blocks for the remaining {len(remaining_tokens)} tokens\"", "\n", "# )", "\n", "", "for", "token", "in", "remaining_tokens", ":", "\n", "        ", "setattr", "(", "\n", "token", ",", "target_attr", ",", "int", "(", "find_closet_block_for_token", "(", "token", ",", "block_layout", ")", ".", "id", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_minimum_gap": [[470, 476], ["sum", "abs", "zip"], "function", ["None"], ["", "", "def", "find_minimum_gap", "(", "block_A", ",", "block_B", ")", ":", "\n", "# just the manhattan distance", "\n", "\n", "    ", "center_A", "=", "block_A", ".", "block", ".", "center", "\n", "center_B", "=", "block_B", ".", "block", ".", "center", "\n", "return", "sum", "(", "abs", "(", "a", "-", "b", ")", "for", "a", ",", "b", "in", "zip", "(", "center_A", ",", "center_B", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_closet_block_for_token": [[477, 487], ["float", "vision_postprocessor.find_minimum_gap"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_minimum_gap"], ["", "def", "find_closet_block_for_token", "(", "token", ",", "blocks", ")", ":", "\n", "    ", "gap", "=", "float", "(", "\"inf\"", ")", "\n", "target_block", "=", "None", "\n", "for", "block", "in", "blocks", ":", "\n", "        ", "cur_gap", "=", "find_minimum_gap", "(", "token", ",", "block", ")", "\n", "if", "cur_gap", "<", "gap", ":", "\n", "            ", "gap", "=", "cur_gap", "\n", "target_block", "=", "block", "\n", "", "", "assert", "target_block", "is", "not", "None", "\n", "return", "target_block", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.intersect": [[488, 494], ["layoutparser.Rectangle", "max", "max", "min", "min"], "function", ["None"], ["", "def", "intersect", "(", "self", ",", "other", ")", ":", "\n", "    ", "return", "lp", ".", "Rectangle", "(", "\n", "max", "(", "self", ".", "x_1", ",", "other", ".", "x_1", ")", ",", "\n", "max", "(", "self", ".", "y_1", ",", "other", ".", "y_1", ")", ",", "\n", "min", "(", "self", ".", "x_2", ",", "other", ".", "x_2", ")", ",", "\n", "min", "(", "self", ".", "y_2", ",", "other", ".", "y_2", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.trim_elements_based_on_parents": [[496, 504], ["block_layout.get", "vision_postprocessor.intersect"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.intersect"], ["", "def", "trim_elements_based_on_parents", "(", "block_layout", ",", "token_layout", ")", ":", "\n", "\n", "    ", "block_layout", "=", "{", "b", ".", "id", ":", "b", "for", "b", "in", "block_layout", "}", "\n", "\n", "for", "token", "in", "token_layout", ":", "\n", "        ", "block", "=", "block_layout", ".", "get", "(", "token", ".", "parent", ",", "None", ")", "\n", "if", "block", "is", "not", "None", ":", "\n", "            ", "token", ".", "block", "=", "intersect", "(", "token", ".", "block", ",", "block", ".", "block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_tokens_in_block": [[505, 508], ["vision_postprocessor.is_in"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "", "", "def", "get_tokens_in_block", "(", "tokens", ",", "block", ",", "metric", "=", "\"center\"", ")", ":", "\n", "\n", "    ", "return", "[", "tok", "for", "tok", "in", "tokens", "if", "is_in", "(", "tok", ",", "block", ",", "metric", "=", "metric", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.reorder_lines": [[509, 599], ["sorted", "enumerate", "vision_postprocessor.get_tokens_in_block", "vision_postprocessor.find_closet_block_for_token", "vision_postprocessor.is_in", "vision_postprocessor.get_tokens_in_block", "vision_postprocessor.get_most_common_token_type", "len", "vision_postprocessor.find_closet_block_for_token", "vision_postprocessor.get_most_common_token_type", "lines_in_current_block.append", "remaining_lines.append", "min", "float", "enumerate", "len", "len", "vision_postprocessor.argsort"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_tokens_in_block", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_closet_block_for_token", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_tokens_in_block", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_token_type", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_closet_block_for_token", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.get_most_common_token_type", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.argsort"], ["", "def", "reorder_lines", "(", "lines", ",", "blocks", ",", "tokens", ")", ":", "\n", "# We firstly group lines by blocks, then order", "\n", "# lines within each group using the token indices", "\n", "\n", "    ", "ordered_blocks", "=", "sorted", "(", "blocks", ",", "key", "=", "lambda", "b", ":", "b", ".", "id", ")", "\n", "\n", "tokens_groupby_blocks", "=", "{", "\n", "block", ".", "id", ":", "[", "tok", "for", "tok", "in", "tokens", "if", "tok", ".", "parent", "==", "block", ".", "id", "]", "for", "block", "in", "blocks", "\n", "}", "\n", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "token", ".", "line_id", "=", "None", "\n", "\n", "", "line_id", "=", "0", "\n", "iter_lines", "=", "lines", "\n", "\n", "for", "block", "in", "ordered_blocks", ":", "\n", "\n", "        ", "lines_in_current_block", "=", "[", "]", "\n", "remaining_lines", "=", "[", "]", "\n", "for", "line", "in", "iter_lines", ":", "\n", "            ", "if", "is_in", "(", "line", ",", "block", ")", ":", "\n", "                ", "lines_in_current_block", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                ", "remaining_lines", ".", "append", "(", "line", ")", "\n", "", "", "iter_lines", "=", "remaining_lines", "\n", "\n", "tokens_in_current_block", "=", "tokens_groupby_blocks", "[", "block", ".", "id", "]", "\n", "\n", "tokens_in_each_line", "=", "[", "\n", "get_tokens_in_block", "(", "tokens_in_current_block", ",", "line", ",", "metric", "=", "\"any\"", ")", "\n", "for", "line", "in", "lines_in_current_block", "\n", "]", "\n", "\n", "min_token_indices_in_each_line", "=", "[", "\n", "(", "min", "(", "tok", ".", "id", "for", "tok", "in", "tokens", ")", "if", "len", "(", "tokens", ")", ">", "0", "else", "float", "(", "\"inf\"", ")", ")", "\n", "for", "tokens", "in", "tokens_in_each_line", "\n", "]", "\n", "\n", "# print(min_token_indices_in_each_line)", "\n", "\n", "sorted_line_token_indices", "=", "{", "\n", "orig_id", ":", "new_id", "\n", "for", "new_id", ",", "orig_id", "in", "enumerate", "(", "argsort", "(", "min_token_indices_in_each_line", ")", ")", "\n", "}", "\n", "\n", "used_line_id", "=", "0", "\n", "for", "idx", ",", "line", "in", "enumerate", "(", "lines_in_current_block", ")", ":", "\n", "\n", "            ", "tokens_in_this_line", "=", "tokens_in_each_line", "[", "idx", "]", "\n", "\n", "if", "len", "(", "tokens_in_this_line", ")", "==", "0", ":", "\n", "                ", "line", ".", "id", "=", "None", "\n", "continue", "\n", "\n", "", "line", ".", "id", "=", "cur_line_id", "=", "sorted_line_token_indices", "[", "idx", "]", "+", "line_id", "\n", "line", ".", "parent", "=", "block", ".", "id", "\n", "line", ".", "type", "=", "get_most_common_token_type", "(", "tokens_in_this_line", ")", "\n", "used_line_id", "+=", "1", "\n", "\n", "for", "token", "in", "tokens_in_this_line", ":", "\n", "                ", "token", ".", "line_id", "=", "cur_line_id", "\n", "\n", "", "", "line_id", "+=", "used_line_id", "\n", "\n", "# print(", "\n", "#     \"Searching the closet blocks for the remaining\", len(remaining_lines), \"lines\"", "\n", "# )", "\n", "\n", "", "for", "line", "in", "remaining_lines", ":", "\n", "        ", "tokens_in_this_line", "=", "get_tokens_in_block", "(", "tokens", ",", "line", ",", "metric", "=", "\"coef\"", ")", "\n", "if", "len", "(", "tokens_in_this_line", ")", "==", "0", ":", "\n", "            ", "line", ".", "id", "=", "None", "\n", "", "else", ":", "\n", "            ", "block", "=", "find_closet_block_for_token", "(", "line", ",", "blocks", ")", "\n", "line", ".", "id", "=", "cur_line_id", "=", "line_id", "\n", "line", ".", "parent", "=", "block", ".", "id", "\n", "line", ".", "type", "=", "get_most_common_token_type", "(", "tokens_in_this_line", ")", "\n", "for", "token", "in", "tokens_in_this_line", ":", "\n", "                ", "token", ".", "line_id", "=", "cur_line_id", "\n", "\n", "", "line_id", "+=", "1", "\n", "\n", "", "", "tokens_without_line_ids", "=", "[", "token", "for", "token", "in", "tokens", "if", "token", ".", "line_id", "is", "None", "]", "\n", "# print(", "\n", "#     \"Searching the closet lines for the remaining\", len(tokens_without_line_ids), \"tokens\"", "\n", "# )", "\n", "for", "token", "in", "tokens_without_line_ids", ":", "\n", "        ", "line", "=", "find_closet_block_for_token", "(", "token", ",", "lines", ")", "\n", "token", ".", "line_id", "=", "line", ".", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.replace_non_text_lines_with_block": [[600, 643], ["itertools.groupby", "list", "vision_postprocessor.is_non_textual_type", "copy.copy", "synthesized_lines.append", "enumerate", "synthesized_lines.extend", "vision_postprocessor.is_in", "vision_postprocessor.find_closet_block_for_token"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_non_textual_type", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_closet_block_for_token"], ["", "", "def", "replace_non_text_lines_with_block", "(", "lines", ",", "blocks", ",", "tokens", ")", ":", "\n", "\n", "    ", "blocks", "=", "{", "b", ".", "id", ":", "b", "for", "b", "in", "blocks", "}", "\n", "\n", "new_line_id", "=", "0", "\n", "synthesized_lines", "=", "[", "]", "\n", "line_id_conversion", "=", "{", "}", "\n", "\n", "for", "bid", ",", "gp", "in", "groupby", "(", "lines", ",", "key", "=", "lambda", "ele", ":", "ele", ".", "parent", ")", ":", "\n", "        ", "lines_in_this_block", "=", "list", "(", "gp", ")", "\n", "cur_block", "=", "blocks", "[", "bid", "]", "\n", "if", "is_non_textual_type", "(", "cur_block", ")", ":", "\n", "            ", "cur_block", "=", "copy", "(", "cur_block", ")", "\n", "\n", "cur_block", ".", "parent", "=", "cur_block", ".", "id", "\n", "cur_block", ".", "id", "=", "new_line_id", "\n", "synthesized_lines", ".", "append", "(", "cur_block", ")", "\n", "\n", "for", "line", "in", "lines_in_this_block", ":", "\n", "                ", "line_id_conversion", "[", "line", ".", "id", "]", "=", "new_line_id", "\n", "\n", "", "new_line_id", "+=", "1", "\n", "\n", "", "else", ":", "\n", "            ", "for", "idx", ",", "line", "in", "enumerate", "(", "lines_in_this_block", ",", "start", "=", "new_line_id", ")", ":", "\n", "                ", "line_id_conversion", "[", "line", ".", "id", "]", "=", "idx", "\n", "line", ".", "id", "=", "idx", "\n", "", "synthesized_lines", ".", "extend", "(", "lines_in_this_block", ")", "\n", "new_line_id", "=", "idx", "+", "1", "\n", "\n", "", "", "for", "token", "in", "tokens", ":", "\n", "        ", "if", "token", ".", "line_id", "not", "in", "line_id_conversion", ":", "\n", "            ", "for", "line", "in", "synthesized_lines", ":", "\n", "                ", "if", "is_in", "(", "token", ",", "line", ",", "metric", "=", "\"coef\"", ")", ":", "\n", "                    ", "token", ".", "line_id", "=", "line", ".", "id", "\n", "continue", "\n", "", "", "if", "token", ".", "line_id", "is", "None", ":", "\n", "                ", "line", "=", "find_closet_block_for_token", "(", "token", ",", "synthesized_lines", ")", "\n", "token", ".", "line_id", "=", "line", ".", "id", "\n", "", "", "else", ":", "\n", "            ", "token", ".", "line_id", "=", "line_id_conversion", "[", "token", ".", "line_id", "]", "\n", "\n", "", "", "return", "synthesized_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.pipeline": [[645, 692], ["vision_postprocessor.load_cermine_data_from_csv", "vision_postprocessor.load_block_data_from_csv", "vision_postprocessor.load_line_data_from_csv", "vision_postprocessor.remove_overlapping_textual_blocks_for_non_textual_blocks", "vision_postprocessor.find_parent_for_elements", "vision_postprocessor.block_snapping", "vision_postprocessor.filter_out_overlapping_block_and_union", "vision_postprocessor.find_parent_for_elements", "group_token_to_blocks", "vision_postprocessor.absorb_additional_blocks_into_existing_blocks", "vision_postprocessor.find_parent_for_all_elements_and_reassign_block_category", "vision_postprocessor.filter_out_overlapping_block", "vision_postprocessor.find_parent_for_elements", "vision_postprocessor.trim_elements_based_on_parents", "vision_postprocessor.find_parent_for_elements", "group_token_to_lines", "vision_postprocessor.absorb_additional_blocks_into_existing_blocks", "vision_postprocessor.reorder_lines", "sorted", "sorted", "vision_postprocessor.replace_non_text_lines_with_block", "len"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_cermine_data_from_csv", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_block_data_from_csv", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.load_line_data_from_csv", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.remove_overlapping_textual_blocks_for_non_textual_blocks", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_elements", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.block_snapping", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.filter_out_overlapping_block_and_union", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_elements", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.absorb_additional_blocks_into_existing_blocks", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_all_elements_and_reassign_block_category", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.filter_out_overlapping_block", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_elements", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.trim_elements_based_on_parents", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.find_parent_for_elements", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.absorb_additional_blocks_into_existing_blocks", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.reorder_lines", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.replace_non_text_lines_with_block"], ["", "def", "pipeline", "(", "base_path", ",", "pdf_sha", ",", "pid", ")", ":", "\n", "\n", "    ", "csv_name", "=", "f\"{pdf_sha}-{pid}.csv\"", "\n", "tokens", "=", "load_cermine_data_from_csv", "(", "f'{base_path}/tokens/{csv_name}'", ")", "\n", "\n", "if", "tokens", "is", "None", "or", "len", "(", "tokens", ")", "==", "0", ":", "\n", "# Nothing for empty tokens", "\n", "        ", "return", "[", "[", "]", "]", "*", "5", "\n", "\n", "", "blocks", "=", "load_block_data_from_csv", "(", "f'{base_path}/blocks/{csv_name}'", ")", "\n", "lines", "=", "load_line_data_from_csv", "(", "f'{base_path}/lines/{csv_name}'", ")", "\n", "\n", "blocks", "=", "remove_overlapping_textual_blocks_for_non_textual_blocks", "(", "blocks", ")", "\n", "find_parent_for_elements", "(", "blocks", ",", "tokens", ")", "\n", "block_snapping", "(", "blocks", ",", "tokens", ")", "\n", "blocks", "=", "filter_out_overlapping_block_and_union", "(", "blocks", ")", "\n", "\n", "find_parent_for_elements", "(", "blocks", ",", "tokens", ")", "\n", "\n", "additional_blocks", "=", "group_token_to_blocks", "(", "tokens", ")", "\n", "blocks", ",", "additional_blocks", "=", "absorb_additional_blocks_into_existing_blocks", "(", "\n", "blocks", ",", "additional_blocks", "\n", ")", "\n", "\n", "find_parent_for_all_elements_and_reassign_block_category", "(", "\n", "blocks", "+", "additional_blocks", ",", "tokens", "\n", ")", "\n", "\n", "lines", "=", "filter_out_overlapping_block", "(", "lines", ")", "\n", "find_parent_for_elements", "(", "blocks", ",", "lines", ")", "\n", "trim_elements_based_on_parents", "(", "blocks", ",", "lines", ")", "\n", "\n", "find_parent_for_elements", "(", "lines", ",", "tokens", ",", "target_attr", "=", "\"line_id\"", ")", "\n", "additional_lines", "=", "group_token_to_lines", "(", "tokens", ")", "\n", "lines", ",", "additional_lines", "=", "absorb_additional_blocks_into_existing_blocks", "(", "\n", "lines", ",", "additional_lines", ",", "threshold", "=", "0.3", "\n", ")", "\n", "reorder_lines", "(", "lines", "+", "additional_lines", ",", "blocks", "+", "additional_blocks", ",", "tokens", ")", "\n", "\n", "blocks", "=", "sorted", "(", "[", "ele", "for", "ele", "in", "blocks", "+", "additional_blocks", "]", ",", "key", "=", "lambda", "ele", ":", "ele", ".", "id", ")", "\n", "lines", "=", "sorted", "(", "\n", "[", "ele", "for", "ele", "in", "lines", "+", "additional_lines", "if", "ele", ".", "id", "is", "not", "None", "]", ",", "\n", "key", "=", "lambda", "ele", ":", "ele", ".", "id", ",", "\n", ")", "\n", "lines", "=", "replace_non_text_lines_with_block", "(", "lines", ",", "blocks", ",", "tokens", ")", "\n", "\n", "return", "(", "blocks", ",", "lines", ",", "tokens", ",", "additional_blocks", ",", "additional_lines", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.create_structure_df": [[693, 750], ["pandas.DataFrame"], "function", ["None"], ["", "def", "create_structure_df", "(", "tokens", ",", "blocks", ",", "lines", ")", ":", "\n", "    ", "blocks_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "-", "1", ",", "\n", "-", "1", ",", "\n", "True", ",", "\n", "False", ",", "\n", "]", "\n", "for", "ele", "in", "blocks", "\n", "]", "\n", "lines_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "parent", ",", "\n", "-", "1", ",", "\n", "False", ",", "\n", "True", ",", "\n", "]", "\n", "for", "ele", "in", "lines", "\n", "]", "\n", "tokens_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "parent", ",", "\n", "ele", ".", "line_id", ",", "\n", "False", ",", "\n", "False", ",", "\n", "]", "\n", "for", "ele", "in", "tokens", "\n", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "blocks_to_save", "+", "lines_to_save", "+", "tokens_to_save", ",", "\n", "columns", "=", "[", "\n", "\"id\"", ",", "\n", "\"x_1\"", ",", "\n", "\"y_1\"", ",", "\n", "\"x_2\"", ",", "\n", "\"y_2\"", ",", "\n", "\"text\"", ",", "\n", "\"category\"", ",", "\n", "\"block_id\"", ",", "\n", "\"line_id\"", ",", "\n", "\"is_block\"", ",", "\n", "\"is_line\"", ",", "\n", "]", ",", "\n", ")", "\n", "return", "df", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.S2VLLoader.__init__": [[15, 19], ["glob.glob.glob"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "pdf_path", ")", ":", "\n", "\n", "        ", "self", ".", "pdf_path", "=", "pdf_path", "\n", "self", ".", "all_pdfs", "=", "glob", "(", "f\"{self.pdf_path}/*.pdf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.S2VLLoader.__getitem__": [[20, 23], ["vision_model_loader.S2VLLoader.load_sample", "len"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.S2VLLoader.load_sample"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "idx", "<", "len", "(", "self", ".", "all_pdfs", ")", "\n", "return", "self", ".", "load_sample", "(", "self", ".", "all_pdfs", "[", "idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.S2VLLoader.load_sample": [[24, 26], ["pdf2image.convert_from_path"], "methods", ["None"], ["", "def", "load_sample", "(", "self", ",", "pdf_path", ")", ":", "\n", "        ", "return", "pdf_path", ",", "convert_from_path", "(", "pdf_path", ",", "dpi", "=", "72", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.S2VLLoader.__len__": [[27, 29], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "all_pdfs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient": [[31, 43], ["layoutparser.Rectangle", "min", "max", "max", "min", "min"], "function", ["None"], ["", "", "def", "calculate_overlapping_coefficient", "(", "box1", ",", "box2", ")", ":", "\n", "    ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "box1", ".", "coordinates", "\n", "a1", ",", "b1", ",", "a2", ",", "b2", "=", "box2", ".", "coordinates", "\n", "\n", "if", "x2", "<", "a1", "or", "x1", ">", "a2", "or", "y1", ">", "b2", "or", "y2", "<", "b1", ":", "# Bottom or top", "\n", "        ", "return", "0", "\n", "\n", "", "else", ":", "\n", "        ", "intersection", "=", "lp", ".", "Rectangle", "(", "\n", "x_1", "=", "max", "(", "x1", ",", "a1", ")", ",", "y_1", "=", "max", "(", "y1", ",", "b1", ")", ",", "x_2", "=", "min", "(", "x2", ",", "a2", ")", ",", "y_2", "=", "min", "(", "y2", ",", "b2", ")", "\n", ")", "\n", "return", "intersection", ".", "area", "/", "min", "(", "box1", ".", "area", ",", "box2", ".", "area", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.filter_out_non_overlapping_block": [[48, 70], ["numpy.zeros", "len", "vision_model_loader.calculate_overlapping_coefficient"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.calculate_overlapping_coefficient"], ["def", "filter_out_non_overlapping_block", "(", "blocks", ")", ":", "\n", "\n", "    ", "boxes_to_remove", "=", "np", ".", "zeros", "(", "len", "(", "blocks", ")", ")", "\n", "for", "box2", "in", "blocks", ":", "\n", "        ", "for", "box1", "in", "blocks", ":", "\n", "\n", "            ", "if", "box1", ".", "id", "==", "box2", ".", "id", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "boxes_to_remove", "[", "box1", ".", "id", "]", "or", "boxes_to_remove", "[", "box2", ".", "id", "]", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "(", "\n", "calculate_overlapping_coefficient", "(", "box1", ",", "box2", ")", "\n", ">", "THRESHOLD_FOR_OVERLAPPING_BLOCKS", "\n", ")", ":", "\n", "                ", "if", "box1", ".", "area", ">=", "box2", ".", "area", ":", "\n", "                    ", "boxes_to_remove", "[", "box2", ".", "id", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "boxes_to_remove", "[", "box1", ".", "id", "]", "=", "1", "\n", "\n", "", "", "", "", "return", "[", "b", "for", "b", "in", "blocks", "if", "not", "boxes_to_remove", "[", "b", ".", "id", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.convert_blocks_to_df": [[72, 90], ["pandas.DataFrame", "df[].astype"], "function", ["None"], ["", "def", "convert_blocks_to_df", "(", "blocks_line", ")", ":", "\n", "    ", "blocks_to_save", "=", "[", "[", "ele", ".", "id", ",", "*", "ele", ".", "coordinates", ",", "ele", ".", "type", ",", "ele", ".", "score", "]", "for", "ele", "in", "blocks_line", "]", "\n", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "blocks_to_save", ",", "\n", "columns", "=", "[", "\n", "\"id\"", ",", "\n", "\"x_1\"", ",", "\n", "\"y_1\"", ",", "\n", "\"x_2\"", ",", "\n", "\"y_2\"", ",", "\n", "\"category\"", ",", "\n", "\"confidence\"", ",", "\n", "]", ",", "\n", ")", "\n", "\n", "df", "[", "[", "\"x_1\"", ",", "\"y_1\"", ",", "\"x_2\"", ",", "\"y_2\"", "]", "]", "=", "df", "[", "[", "\"x_1\"", ",", "\"y_1\"", ",", "\"x_2\"", ",", "\"y_2\"", "]", "]", ".", "astype", "(", "\"int\"", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.textline_detection": [[92, 125], ["layoutparser.Detectron2LayoutModel", "vision_model_loader.S2VLLoader", "tqdm.tqdm", "lp.Detectron2LayoutModel.detect", "vision_model_loader.filter_out_non_overlapping_block", "[].replace", "pdf2image.convert_from_path", "enumerate", "token.set", "vision_model_loader.textline_detection.detect_lines_for_image"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.filter_out_non_overlapping_block"], ["", "def", "textline_detection", "(", "base_path", ")", ":", "\n", "    ", "def", "detect_lines_for_image", "(", "pdf_image", ")", ":", "\n", "        ", "blocks_line", "=", "model_line", ".", "detect", "(", "pdf_image", ")", "\n", "blocks_line", "=", "[", "token", ".", "set", "(", "id", "=", "idx", ")", "for", "idx", ",", "token", "in", "enumerate", "(", "blocks_line", ")", "]", "\n", "blocks_line", "=", "filter_out_non_overlapping_block", "(", "blocks_line", ")", "\n", "return", "blocks_line", "\n", "\n", "", "model_line", "=", "lp", ".", "Detectron2LayoutModel", "(", "\n", "config_path", "=", "f\"https://www.dropbox.com/s/hd21tarnhbj1p1o/config.yaml?dl=1\"", ",", "# This is the line detection model trained using the GROTOAP2 dataset", "\n", "extra_config", "=", "[", "\n", "\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\"", ",", "\n", "0.35", ",", "\n", "\"MODEL.ROI_HEADS.NMS_THRESH_TEST\"", ",", "\n", "0.8", ",", "\n", "]", ",", "\n", "label_map", "=", "{", "0", ":", "\"line\"", "}", ",", "\n", ")", "\n", "\n", "loader", "=", "S2VLLoader", "(", "f\"{base_path}/pdfs\"", ")", "\n", "\n", "for", "pdf_path", "in", "tqdm", "(", "loader", ".", "all_pdfs", ")", ":", "\n", "        ", "pdf_name", "=", "pdf_path", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\".pdf\"", ",", "\"\"", ")", "\n", "pdf_images", "=", "convert_from_path", "(", "pdf_path", ",", "dpi", "=", "72", ")", "\n", "for", "pid", ",", "pdf_image", "in", "enumerate", "(", "pdf_images", ")", ":", "\n", "\n", "            ", "blocks_line", "=", "detect_lines_for_image", "(", "pdf_image", ")", "\n", "\n", "df", "=", "convert_blocks_to_df", "(", "blocks_line", ")", "\n", "\n", "if", "len", "(", "pdf_images", ")", "==", "1", "and", "len", "(", "pdf_name", ".", "split", "(", "\"-\"", ")", ")", "==", "2", ":", "\n", "                ", "df", ".", "to_csv", "(", "f\"{base_path}/lines/{pdf_name}.csv\"", ",", "index", "=", "None", ")", "\n", "", "else", ":", "\n", "                ", "df", ".", "to_csv", "(", "f\"{base_path}/lines/{pdf_name}-{pid:02d}.csv\"", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.textblock_detection": [[127, 174], ["layoutparser.Detectron2LayoutModel", "layoutparser.Detectron2LayoutModel", "vision_model_loader.S2VLLoader", "tqdm.tqdm", "lp.Detectron2LayoutModel.detect", "lp.Detectron2LayoutModel.detect", "sorted", "vision_model_loader.filter_out_non_overlapping_block", "[].replace", "pdf2image.convert_from_path", "enumerate", "token.set", "vision_model_loader.textblock_detection.detect_blocks_for_image"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_model_loader.filter_out_non_overlapping_block"], ["", "", "", "", "def", "textblock_detection", "(", "base_path", ")", ":", "\n", "    ", "def", "detect_blocks_for_image", "(", "pdf_image", ")", ":", "\n", "        ", "blocks1", "=", "block_predictorA", ".", "detect", "(", "pdf_image", ")", "\n", "blocks2", "=", "block_predictorB", ".", "detect", "(", "pdf_image", ")", "\n", "\n", "blocks", "=", "blocks1", "+", "blocks2", "\n", "blocks", "=", "sorted", "(", "blocks", ",", "key", "=", "lambda", "ele", ":", "ele", ".", "coordinates", "[", "1", "]", ")", "\n", "blocks", "=", "[", "token", ".", "set", "(", "id", "=", "idx", ")", "for", "idx", ",", "token", "in", "enumerate", "(", "blocks", ")", "]", "\n", "blocks", "=", "filter_out_non_overlapping_block", "(", "blocks", ")", "\n", "return", "blocks", "\n", "\n", "", "block_predictorA", "=", "lp", ".", "Detectron2LayoutModel", "(", "\n", "config_path", "=", "\"lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config\"", ",", "\n", "extra_config", "=", "[", "\n", "\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\"", ",", "\n", "0.50", ",", "\n", "\"MODEL.ROI_HEADS.NMS_THRESH_TEST\"", ",", "\n", "0.4", ",", "\n", "]", ",", "\n", "label_map", "=", "{", "0", ":", "\"text\"", ",", "1", ":", "\"title\"", ",", "2", ":", "\"list\"", ",", "3", ":", "\"table\"", ",", "4", ":", "\"figure\"", "}", ",", "\n", ")", "\n", "\n", "block_predictorB", "=", "lp", ".", "Detectron2LayoutModel", "(", "\n", "config_path", "=", "\"lp://MFD/faster_rcnn_R_50_FPN_3x/config\"", ",", "\n", "extra_config", "=", "[", "\n", "\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\"", ",", "\n", "0.6", ",", "\n", "\"MODEL.ROI_HEADS.NMS_THRESH_TEST\"", ",", "\n", "0.2", ",", "\n", "]", ",", "\n", "label_map", "=", "{", "1", ":", "\"equation\"", "}", ",", "\n", ")", "\n", "\n", "loader", "=", "S2VLLoader", "(", "f\"{base_path}/pdfs\"", ")", "\n", "\n", "for", "pdf_path", "in", "tqdm", "(", "loader", ".", "all_pdfs", ")", ":", "\n", "        ", "pdf_name", "=", "pdf_path", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "replace", "(", "\".pdf\"", ",", "\"\"", ")", "\n", "pdf_images", "=", "convert_from_path", "(", "pdf_path", ",", "dpi", "=", "72", ")", "\n", "\n", "for", "pid", ",", "pdf_image", "in", "enumerate", "(", "pdf_images", ")", ":", "\n", "            ", "blocks", "=", "detect_blocks_for_image", "(", "pdf_image", ")", "\n", "df", "=", "convert_blocks_to_df", "(", "blocks", ")", "\n", "\n", "if", "len", "(", "pdf_images", ")", "==", "1", "and", "len", "(", "pdf_name", ".", "split", "(", "\"-\"", ")", ")", "==", "2", ":", "\n", "                ", "df", ".", "to_csv", "(", "f\"{base_path}/blocks/{pdf_name}.csv\"", ",", "index", "=", "None", ")", "\n", "", "else", ":", "\n", "                ", "df", ".", "to_csv", "(", "f\"{base_path}/blocks/{pdf_name}-{pid:02d}.csv\"", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.PageData.to_dataframe": [[23, 162], ["pandas.DataFrame", "pandas.DataFrame", "enumerate", "enumerate", "len", "len"], "methods", ["None"], ["def", "to_dataframe", "(", "\n", "self", ",", "\n", "keep_token_index", "=", "True", ",", "\n", "export_font", "=", "False", ",", "\n", "normalize_coordinates", "=", "False", ",", "\n", "canvas_width", "=", "None", ",", "\n", "canvas_height", "=", "None", ",", "\n", ")", "->", "pd", ".", "DataFrame", ":", "\n", "\n", "        ", "if", "not", "export_font", ":", "\n", "            ", "blocks_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "-", "1", ",", "\n", "-", "1", ",", "\n", "True", ",", "\n", "False", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "blocks", "\n", "]", "\n", "lines_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "parent", ",", "\n", "-", "1", ",", "\n", "False", ",", "\n", "True", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "lines", "\n", "]", "\n", "parent_block_id_for_line_id", "=", "{", "ele", ".", "id", ":", "ele", ".", "parent", "for", "ele", "in", "self", ".", "lines", "}", "\n", "tokens_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", "if", "keep_token_index", "else", "idx", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "type", ",", "\n", "parent_block_id_for_line_id", "[", "ele", ".", "parent", "]", ",", "# Cvt to block-level id", "\n", "ele", ".", "parent", ",", "\n", "False", ",", "\n", "False", ",", "\n", "]", "\n", "for", "idx", ",", "ele", "in", "enumerate", "(", "self", ".", "words", ",", "start", "=", "len", "(", "blocks_to_save", ")", ")", "\n", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "blocks_to_save", "+", "lines_to_save", "+", "tokens_to_save", ",", "\n", "columns", "=", "[", "\n", "\"id\"", ",", "\n", "\"x_1\"", ",", "\n", "\"y_1\"", ",", "\n", "\"x_2\"", ",", "\n", "\"y_2\"", ",", "\n", "\"text\"", ",", "\n", "\"category\"", ",", "\n", "\"block_id\"", ",", "\n", "\"line_id\"", ",", "\n", "\"is_block\"", ",", "\n", "\"is_line\"", ",", "\n", "]", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "blocks_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "None", ",", "\n", "ele", ".", "type", ",", "\n", "-", "1", ",", "\n", "-", "1", ",", "\n", "True", ",", "\n", "False", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "blocks", "\n", "]", "\n", "lines_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "None", ",", "\n", "ele", ".", "type", ",", "\n", "ele", ".", "parent", ",", "\n", "-", "1", ",", "\n", "False", ",", "\n", "True", ",", "\n", "]", "\n", "for", "ele", "in", "self", ".", "lines", "\n", "]", "\n", "parent_block_id_for_line_id", "=", "{", "ele", ".", "id", ":", "ele", ".", "parent", "for", "ele", "in", "self", ".", "lines", "}", "\n", "tokens_to_save", "=", "[", "\n", "[", "\n", "ele", ".", "id", "if", "keep_token_index", "else", "idx", ",", "\n", "*", "ele", ".", "coordinates", ",", "\n", "ele", ".", "text", ",", "\n", "ele", ".", "font", ",", "\n", "ele", ".", "type", ",", "\n", "parent_block_id_for_line_id", "[", "ele", ".", "parent", "]", ",", "# Cvt to block-level id", "\n", "ele", ".", "parent", ",", "\n", "False", ",", "\n", "False", ",", "\n", "]", "\n", "for", "idx", ",", "ele", "in", "enumerate", "(", "self", ".", "words", ",", "start", "=", "len", "(", "blocks_to_save", ")", ")", "\n", "]", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "blocks_to_save", "+", "lines_to_save", "+", "tokens_to_save", ",", "\n", "columns", "=", "[", "\n", "\"id\"", ",", "\n", "\"x_1\"", ",", "\n", "\"y_1\"", ",", "\n", "\"x_2\"", ",", "\n", "\"y_2\"", ",", "\n", "\"text\"", ",", "\n", "\"font\"", ",", "\n", "\"category\"", ",", "\n", "\"block_id\"", ",", "\n", "\"line_id\"", ",", "\n", "\"is_block\"", ",", "\n", "\"is_line\"", ",", "\n", "]", ",", "\n", ")", "\n", "\n", "", "if", "normalize_coordinates", ":", "\n", "            ", "assert", "canvas_width", "is", "not", "None", "\n", "assert", "canvas_height", "is", "not", "None", "\n", "df", "[", "[", "\"x_1\"", ",", "\"x_2\"", "]", "]", "=", "(", "df", "[", "[", "\"x_1\"", ",", "\"x_2\"", "]", "]", "/", "canvas_width", "*", "1000", ")", ".", "astype", "(", "\n", "\"int\"", "\n", ")", "\n", "df", "[", "[", "\"y_1\"", ",", "\"y_2\"", "]", "]", "=", "(", "df", "[", "[", "\"y_1\"", ",", "\"y_2\"", "]", "]", "/", "canvas_height", "*", "1000", ")", ".", "astype", "(", "\n", "\"int\"", "\n", ")", "\n", "\n", "", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.__init__": [[165, 171], ["glob.glob.glob"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "base_dir", ":", "str", ",", "dataset_folder_name", ":", "str", "=", "\"dataset\"", ")", ":", "\n", "\n", "        ", "self", ".", "base_dir", "=", "base_dir", "\n", "self", ".", "dataset_folder_name", "=", "dataset_folder_name", "\n", "self", ".", "all_xml_files", "=", "glob", "(", "\n", "f\"{self.base_dir}/{self.dataset_folder_name}/*/*.cxml\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.load_xml": [[173, 184], ["bs4.BeautifulSoup.find_all", "open", "bs4.BeautifulSoup", "cermine_loader.GrotoapDataset.parse_page_xml", "enumerate"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.parse_page_xml"], ["", "def", "load_xml", "(", "self", ",", "xml_filename", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "xml_filename", ",", "\"r\"", ")", "as", "fp", ":", "\n", "            ", "soup", "=", "BeautifulSoup", "(", "fp", ",", "\"lxml\"", ")", "\n", "\n", "", "pages", "=", "soup", ".", "find_all", "(", "\"page\"", ")", "\n", "\n", "parsed_page_data", "=", "{", "\n", "idx", ":", "self", ".", "parse_page_xml", "(", "page", ")", "for", "idx", ",", "page", "in", "enumerate", "(", "pages", ")", "\n", "}", "\n", "\n", "return", "parsed_page_data", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.parse_page_xml": [[185, 268], ["page.find_all", "enumerate", "cermine_loader.PageData", "cermine_loader.PageData", "zone.find().find_all", "layoutparser.TextBlock", "zone.find_all", "blocks.append", "words.extend", "zone.find().find", "layoutparser.Rectangle", "line.find().find_all", "layoutparser.TextBlock", "line.find_all", "line.find_all", "words_in_this_block.extend", "lines.append", "zone.find", "float", "float", "float", "float", "layoutparser.Rectangle", "word.find().find_all", "words_in_this_line.append", "zone.find", "line.find", "float", "float", "float", "float", "layoutparser.TextBlock", "word.find", "layoutparser.Rectangle", "float", "float", "float", "float", "word.find_all"], "methods", ["None"], ["", "def", "parse_page_xml", "(", "self", ",", "page", ":", "\"bs4.element.Tag\"", ")", "->", "PageData", ":", "\n", "\n", "        ", "blocks", "=", "[", "]", "\n", "lines", "=", "[", "]", "\n", "words", "=", "[", "]", "\n", "\n", "word_id", "=", "0", "\n", "line_id", "=", "0", "\n", "all_zones", "=", "page", ".", "find_all", "(", "\"zone\"", ")", "\n", "if", "all_zones", "is", "None", ":", "\n", "            ", "return", "PageData", "(", ")", "\n", "\n", "", "for", "zone_id", ",", "zone", "in", "enumerate", "(", "all_zones", ")", ":", "\n", "\n", "            ", "words_in_this_block", "=", "[", "]", "\n", "# Fetch the zone", "\n", "v1", ",", "v2", "=", "zone", ".", "find", "(", "\"zonecorners\"", ")", ".", "find_all", "(", "\"vertex\"", ")", "\n", "block_type", "=", "zone", ".", "find", "(", "\"classification\"", ")", ".", "find", "(", "\"category\"", ")", "[", "\"value\"", "]", "\n", "block", "=", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "float", "(", "v1", "[", "\"x\"", "]", ")", ",", "float", "(", "v1", "[", "\"y\"", "]", ")", ",", "float", "(", "v2", "[", "\"x\"", "]", ")", ",", "float", "(", "v2", "[", "\"y\"", "]", ")", "\n", ")", ",", "\n", "type", "=", "block_type", ",", "\n", "id", "=", "zone_id", ",", "\n", ")", "\n", "\n", "# Fetch lines", "\n", "all_lines", "=", "zone", ".", "find_all", "(", "\"line\"", ")", "\n", "if", "all_lines", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "line", "in", "all_lines", ":", "\n", "\n", "                ", "words_in_this_line", "=", "[", "]", "\n", "\n", "v1", ",", "v2", "=", "line", ".", "find", "(", "\"linecorners\"", ")", ".", "find_all", "(", "\"vertex\"", ")", "\n", "current_line", "=", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "float", "(", "v1", "[", "\"x\"", "]", ")", ",", "\n", "float", "(", "v1", "[", "\"y\"", "]", ")", ",", "\n", "float", "(", "v2", "[", "\"x\"", "]", ")", ",", "\n", "float", "(", "v2", "[", "\"y\"", "]", ")", ",", "\n", ")", ",", "\n", "type", "=", "block_type", ",", "\n", "parent", "=", "zone_id", ",", "\n", "id", "=", "line_id", ",", "\n", ")", "\n", "\n", "# Fetch words", "\n", "all_words", "=", "line", ".", "find_all", "(", "\"word\"", ")", "\n", "if", "all_words", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "for", "word", "in", "line", ".", "find_all", "(", "\"word\"", ")", ":", "\n", "                    ", "v1", ",", "v2", "=", "word", ".", "find", "(", "\"wordcorners\"", ")", ".", "find_all", "(", "\"vertex\"", ")", "\n", "words_in_this_line", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "float", "(", "v1", "[", "\"x\"", "]", ")", ",", "\n", "float", "(", "v1", "[", "\"y\"", "]", ")", ",", "\n", "float", "(", "v2", "[", "\"x\"", "]", ")", ",", "\n", "float", "(", "v2", "[", "\"y\"", "]", ")", ",", "\n", ")", ",", "\n", "type", "=", "block_type", ",", "\n", "text", "=", "\"\"", ".", "join", "(", "\n", "[", "ele", "[", "\"value\"", "]", "for", "ele", "in", "word", ".", "find_all", "(", "\"gt_text\"", ")", "]", "\n", ")", ",", "\n", "id", "=", "word_id", ",", "\n", "parent", "=", "line_id", ",", "\n", ")", "\n", ")", "\n", "word_id", "+=", "1", "\n", "\n", "", "current_line", ".", "text", "=", "\" \"", ".", "join", "(", "ele", ".", "text", "for", "ele", "in", "words_in_this_line", ")", "\n", "line_id", "+=", "1", "\n", "words_in_this_block", ".", "extend", "(", "words_in_this_line", ")", "\n", "lines", ".", "append", "(", "current_line", ")", "\n", "\n", "", "block", ".", "text", "=", "\" \"", ".", "join", "(", "ele", ".", "text", "for", "ele", "in", "words_in_this_block", ")", "\n", "blocks", ".", "append", "(", "block", ")", "\n", "words", ".", "extend", "(", "words_in_this_block", ")", "\n", "\n", "", "return", "PageData", "(", "blocks", ",", "lines", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.convert_xml_to_page_token": [[269, 281], ["cermine_loader.GrotoapDataset.load_xml", "print", "cermine_loader.GrotoapDataset.items", "os.path.exists", "page_data.to_dataframe", "page_data.to_dataframe.to_csv", "xml_filename.split"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.load_xml", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.PageData.to_dataframe"], ["", "def", "convert_xml_to_page_token", "(", "self", ",", "xml_filename", ",", "export_path", ")", ":", "\n", "\n", "        ", "savename", "=", "\"-\"", ".", "join", "(", "xml_filename", ".", "split", "(", "\"/\"", ")", "[", "-", "2", ":", "]", ")", ".", "rstrip", "(", "\".cxml\"", ")", "\n", "parsed_page_data", "=", "self", ".", "load_xml", "(", "xml_filename", ")", "\n", "print", "(", "f\"Processing {savename}\"", ")", "\n", "for", "page_id", ",", "page_data", "in", "parsed_page_data", ".", "items", "(", ")", ":", "\n", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "f\"{export_path}/{savename}-{page_id}.csv\"", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "df", "=", "page_data", ".", "to_dataframe", "(", ")", "\n", "df", ".", "to_csv", "(", "f\"{export_path}/{savename}-{page_id}.csv\"", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.convert_to_page_token_table": [[282, 293], ["os.path.exists", "os.makedirs", "print", "print", "joblib.Parallel", "joblib.delayed", "tqdm.tqdm.tqdm"], "methods", ["None"], ["", "", "def", "convert_to_page_token_table", "(", "self", ",", "export_path", ":", "str", ",", "n_jobs", "=", "20", ")", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "export_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "export_path", ")", "\n", "print", "(", "f\"Creating the export directory {export_path}\"", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f\"Overwriting existing exports in {export_path}\"", ")", "\n", "\n", "", "Parallel", "(", "n_jobs", "=", "n_jobs", ")", "(", "\n", "delayed", "(", "self", ".", "convert_xml_to_page_token", ")", "(", "xml_filename", ",", "export_path", ")", "\n", "for", "xml_filename", "in", "tqdm", "(", "self", ".", "all_xml_files", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.__init__": [[297, 299], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.corner_to_rectangle": [[300, 307], ["numpy.array.find_all", "numpy.array", "numpy.array.min", "numpy.array.max", "layoutparser.Rectangle", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "corner_to_rectangle", "(", "corners", ")", ":", "\n", "        ", "corners", "=", "corners", ".", "find_all", "(", "\"vertex\"", ")", "\n", "corners", "=", "np", ".", "array", "(", "[", "(", "float", "(", "ele", "[", "\"x\"", "]", ")", ",", "float", "(", "ele", "[", "\"y\"", "]", ")", ")", "for", "ele", "in", "corners", "]", ")", "\n", "x1", ",", "y1", "=", "corners", ".", "min", "(", "axis", "=", "0", ")", "\n", "x2", ",", "y2", "=", "corners", ".", "max", "(", "axis", "=", "0", ")", "\n", "return", "lp", ".", "Rectangle", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.parse_page_xml": [[308, 379], ["page.find_all", "enumerate", "cermine_loader.PageData", "cermine_loader.PageData", "cermine_loader.CERMINELoader.corner_to_rectangle", "layoutparser.TextBlock", "zone.find_all", "blocks.append", "words.extend", "zone.find", "zone.find().find", "cermine_loader.CERMINELoader.corner_to_rectangle", "layoutparser.TextBlock", "line.find_all", "line.find_all", "words_in_this_block.extend", "lines.append", "line.find", "cermine_loader.CERMINELoader.corner_to_rectangle", "words_in_this_line.append", "zone.find", "word.find", "layoutparser.TextBlock", "word.find_all"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.corner_to_rectangle", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.corner_to_rectangle", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.CERMINELoader.corner_to_rectangle"], ["", "def", "parse_page_xml", "(", "self", ",", "page", ":", "\"bs4.element.Tag\"", ")", "->", "PageData", ":", "\n", "\n", "        ", "blocks", "=", "[", "]", "\n", "lines", "=", "[", "]", "\n", "words", "=", "[", "]", "\n", "\n", "word_id", "=", "0", "\n", "line_id", "=", "0", "\n", "all_zones", "=", "page", ".", "find_all", "(", "\"zone\"", ")", "\n", "if", "all_zones", "is", "None", ":", "\n", "            ", "return", "PageData", "(", ")", "\n", "\n", "", "for", "zone_id", ",", "zone", "in", "enumerate", "(", "all_zones", ")", ":", "\n", "\n", "            ", "words_in_this_block", "=", "[", "]", "\n", "# Fetch the zone", "\n", "rect", "=", "self", ".", "corner_to_rectangle", "(", "zone", ".", "find", "(", "\"zonecorners\"", ")", ")", "\n", "block_type", "=", "zone", ".", "find", "(", "\"classification\"", ")", ".", "find", "(", "\"category\"", ")", "[", "\"value\"", "]", "\n", "block", "=", "lp", ".", "TextBlock", "(", "\n", "rect", ",", "\n", "type", "=", "block_type", ",", "\n", "id", "=", "zone_id", ",", "\n", ")", "\n", "\n", "# Fetch lines", "\n", "all_lines", "=", "zone", ".", "find_all", "(", "\"line\"", ")", "\n", "if", "all_lines", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "line", "in", "all_lines", ":", "\n", "\n", "                ", "words_in_this_line", "=", "[", "]", "\n", "\n", "rect", "=", "self", ".", "corner_to_rectangle", "(", "line", ".", "find", "(", "\"linecorners\"", ")", ")", "\n", "current_line", "=", "lp", ".", "TextBlock", "(", "\n", "rect", ",", "\n", "type", "=", "block_type", ",", "\n", "parent", "=", "zone_id", ",", "\n", "id", "=", "line_id", ",", "\n", ")", "\n", "\n", "# Fetch words", "\n", "all_words", "=", "line", ".", "find_all", "(", "\"word\"", ")", "\n", "if", "all_words", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "for", "word", "in", "line", ".", "find_all", "(", "\"word\"", ")", ":", "\n", "                    ", "rect", "=", "self", ".", "corner_to_rectangle", "(", "word", ".", "find", "(", "\"wordcorners\"", ")", ")", "\n", "words_in_this_line", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "\n", "rect", ",", "\n", "type", "=", "block_type", ",", "\n", "text", "=", "\"\"", ".", "join", "(", "\n", "[", "ele", "[", "\"value\"", "]", "for", "ele", "in", "word", ".", "find_all", "(", "\"gt_text\"", ")", "]", "\n", ")", ",", "\n", "id", "=", "word_id", ",", "\n", "parent", "=", "line_id", ",", "\n", ")", "\n", ")", "\n", "word_id", "+=", "1", "\n", "\n", "", "current_line", ".", "text", "=", "\" \"", ".", "join", "(", "ele", ".", "text", "for", "ele", "in", "words_in_this_line", ")", "\n", "line_id", "+=", "1", "\n", "words_in_this_block", ".", "extend", "(", "words_in_this_line", ")", "\n", "lines", ".", "append", "(", "current_line", ")", "\n", "\n", "", "block", ".", "text", "=", "\" \"", ".", "join", "(", "ele", ".", "text", "for", "ele", "in", "words_in_this_block", ")", "\n", "blocks", ".", "append", "(", "block", ")", "\n", "words", ".", "extend", "(", "words_in_this_block", ")", "\n", "\n", "", "return", "PageData", "(", "blocks", ",", "lines", ",", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.process_cermine_annotation": [[384, 407], ["CERMINE_LOADER.load_xml", "xml_data[].to_dataframe().to_csv", "range", "print", "len", "len", "len", "xml_data[].to_dataframe().to_csv", "sha.split", "xml_data[].to_dataframe", "xml_data[].to_dataframe"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.GrotoapDataset.load_xml", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.PageData.to_dataframe", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.PageData.to_dataframe"], ["def", "process_cermine_annotation", "(", "sha", ",", "pdf_path", ",", "token_path", ")", ":", "\n", "    ", "filename", "=", "f\"{pdf_path}/{sha}.cxml\"", "\n", "\n", "try", ":", "\n", "        ", "xml_data", "=", "CERMINE_LOADER", ".", "load_xml", "(", "filename", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "\"error CERMINE parsing for \"", ",", "sha", ")", "\n", "return", "None", "\n", "\n", "# _, pdf_images = pdf_extractor.load_tokens_and_image(filename.replace('.cxml', '.pdf'), resize_image=True)", "\n", "\n", "# if len(xml_data) != len(pdf_images):", "\n", "#     print(\"error CERMINE parsing for \", sha)", "\n", "#     return None", "\n", "\n", "", "if", "(", "\n", "len", "(", "xml_data", ")", "==", "1", "and", "len", "(", "sha", ".", "split", "(", "\"-\"", ")", ")", "==", "2", "\n", ")", ":", "# it is a single page pdf for an individual page", "\n", "        ", "xml_data", "[", "0", "]", ".", "to_dataframe", "(", ")", ".", "to_csv", "(", "f\"{token_path}/{sha}.csv\"", ",", "index", "=", "None", ")", "\n", "", "else", ":", "\n", "        ", "for", "page_id", "in", "range", "(", "len", "(", "xml_data", ")", ")", ":", "\n", "            ", "xml_data", "[", "page_id", "]", ".", "to_dataframe", "(", ")", ".", "to_csv", "(", "\n", "f\"{token_path}/{sha}-{page_id:02d}.csv\"", ",", "index", "=", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.cermine_loader.get_file_sha": [[409, 411], ["[].split", "filename.split"], "function", ["None"], ["", "", "", "def", "get_file_sha", "(", "filename", ")", ":", "\n", "    ", "return", "filename", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.__init__": [[49, 53], ["pandas.read_csv().set_index", "pandas.read_csv"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "annotation_table", ",", "annotation_dir", ")", ":", "\n", "\n", "        ", "self", ".", "annotation_table", "=", "pd", ".", "read_csv", "(", "annotation_table", ")", ".", "set_index", "(", "\"sha\"", ")", "\n", "self", ".", "annotation_dir", "=", "annotation_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_annotation_for_sha": [[54, 71], ["len", "glob.glob.glob", "glob.glob.glob", "glob.glob.glob", "int", "condense_dataset.RawAnnotation.load_page_data_from_json", "condense_dataset.RawAnnotation.load_all_page_data_from_json", "filename.replace().replace", "filename.replace"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_page_data_from_json", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_all_page_data_from_json"], ["", "def", "load_annotation_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "all_page_annotations", "=", "{", "}", "\n", "\n", "if", "len", "(", "glob", "(", "f\"{self.annotation_dir}/{sha}-*.json\"", ")", ")", ">", "0", ":", "\n", "# Load annotation for sha-pageid.json like files ", "\n", "            ", "for", "filename", "in", "glob", "(", "f\"{self.annotation_dir}/{sha}-*.json\"", ")", ":", "\n", "                ", "page_id", "=", "int", "(", "filename", ".", "replace", "(", "f\"{self.annotation_dir}/{sha}-\"", ",", "\"\"", ")", ".", "replace", "(", "\".json\"", ",", "\"\"", ")", ")", "\n", "res", "=", "self", ".", "load_page_data_from_json", "(", "filename", ")", "\n", "if", "res", "is", "not", "None", ":", "\n", "                    ", "all_page_annotations", "[", "page_id", "]", "=", "res", "\n", "", "", "", "else", ":", "\n", "# load annotations for sha.json like files ", "\n", "            ", "for", "filename", "in", "glob", "(", "f\"{self.annotation_dir}/{sha}.json\"", ")", ":", "\n", "                ", "all_page_annotations", "=", "self", ".", "load_all_page_data_from_json", "(", "filename", ")", "\n", "\n", "", "", "return", "all_page_annotations", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_all_page_data_from_json": [[72, 89], ["condense_dataset.load_json", "collections.defaultdict", "results_by_page[].append", "layoutparser.TextBlock", "layoutparser.Rectangle"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.load_json"], ["", "def", "load_all_page_data_from_json", "(", "self", ",", "filename", ")", ":", "\n", "        ", "raw", "=", "load_json", "(", "filename", ")", "\n", "results_by_page", "=", "defaultdict", "(", "list", ")", "\n", "for", "ele", "in", "raw", "[", "\"annotations\"", "]", ":", "\n", "\n", "            ", "results_by_page", "[", "ele", "[", "\"page\"", "]", "]", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"left\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"top\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"right\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"bottom\"", "]", ",", "\n", ")", ",", "\n", "type", "=", "ele", "[", "\"label\"", "]", "[", "\"text\"", "]", ",", "\n", ")", "\n", ")", "\n", "", "return", "results_by_page", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_page_data_from_json": [[90, 106], ["condense_dataset.load_json", "page_annotation.append", "layoutparser.TextBlock", "layoutparser.Rectangle"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.load_json"], ["", "def", "load_page_data_from_json", "(", "self", ",", "filename", ")", ":", "\n", "        ", "raw", "=", "load_json", "(", "filename", ")", "\n", "page_annotation", "=", "[", "]", "\n", "for", "ele", "in", "raw", "[", "\"annotations\"", "]", ":", "\n", "            ", "page_annotation", ".", "append", "(", "\n", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"left\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"top\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"right\"", "]", ",", "\n", "ele", "[", "\"bounds\"", "]", "[", "\"bottom\"", "]", ",", "\n", ")", ",", "\n", "type", "=", "ele", "[", "\"label\"", "]", "[", "\"text\"", "]", ",", "\n", ")", "\n", ")", "\n", "", "return", "page_annotation", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.__init__": [[116, 123], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "pdf_directory", ",", "\n", "csv_directory", ",", "\n", ")", ":", "\n", "        ", "self", ".", "csv_dir", "=", "csv_directory", "\n", "self", ".", "pdf_dir", "=", "pdf_directory", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_page_data_from_csv": [[124, 142], ["pandas.read_csv", "condense_dataset.PageData", "len", "len", "layoutparser.Layout", "layoutparser.Layout", "layoutparser.Layout", "pandas.read_csv.text.isna", "blocks_df.apply().tolist", "lines_df.apply().tolist", "tokens_df.apply().tolist", "blocks_df.apply", "lines_df.apply", "tokens_df.apply"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_page_data_from_csv", "(", "filename", ")", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "filename", ")", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "df", "=", "df", "[", "~", "df", ".", "text", ".", "isna", "(", ")", "]", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "blocks_df", "=", "df", "[", "df", ".", "is_block", "]", "\n", "lines_df", "=", "df", "[", "df", ".", "is_line", "]", "\n", "tokens_df", "=", "df", "[", "~", "df", ".", "is_line", "&", "~", "df", ".", "is_block", "]", "\n", "\n", "return", "PageData", "(", "\n", "blocks", "=", "lp", ".", "Layout", "(", "blocks_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", "lines", "=", "lp", ".", "Layout", "(", "lines_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", "words", "=", "lp", ".", "Layout", "(", "tokens_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha": [[144, 154], ["glob.glob.glob", "int", "condense_dataset.CERMINEAnnotation.load_page_data_from_csv", "filename.replace().replace", "filename.replace"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.VISIONAnnotation.load_page_data_from_csv"], ["", "def", "load_annotations_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "xml_data", "=", "{", "}", "\n", "for", "filename", "in", "glob", "(", "f\"{self.csv_dir}/{sha}-*.csv\"", ")", ":", "\n", "            ", "page_id", "=", "int", "(", "filename", ".", "replace", "(", "f\"{self.csv_dir}/{sha}-\"", ",", "\"\"", ")", ".", "replace", "(", "\".csv\"", ",", "\"\"", ")", ")", "\n", "res", "=", "self", ".", "load_page_data_from_csv", "(", "filename", ")", "\n", "if", "res", "is", "not", "None", ":", "\n", "                ", "xml_data", "[", "page_id", "]", "=", "res", "\n", "\n", "", "", "return", "xml_data", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.VISIONAnnotation.load_page_data_from_csv": [[157, 172], ["pandas.read_csv", "condense_dataset.PageData", "len", "layoutparser.Layout", "layoutparser.Layout", "layoutparser.Layout", "blocks_df.apply().tolist", "lines_df.apply().tolist", "tokens_df.apply().tolist", "blocks_df.apply", "lines_df.apply", "tokens_df.apply"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "load_page_data_from_csv", "(", "filename", ")", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "filename", ")", "\n", "if", "len", "(", "df", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "# Not dropping empty tokens ", "\n", "\n", "", "blocks_df", "=", "df", "[", "df", ".", "is_block", "]", "\n", "lines_df", "=", "df", "[", "df", ".", "is_line", "]", "\n", "tokens_df", "=", "df", "[", "~", "df", ".", "is_line", "&", "~", "df", ".", "is_block", "]", "\n", "\n", "return", "PageData", "(", "\n", "blocks", "=", "lp", ".", "Layout", "(", "blocks_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", "lines", "=", "lp", ".", "Layout", "(", "lines_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", "words", "=", "lp", ".", "Layout", "(", "tokens_df", ".", "apply", "(", "cvt_df_to_layout", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.__init__": [[175, 193], ["pandas.read_csv", "enumerate", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "annotation_table", ",", "\n", "raw_annotation", ",", "\n", "cermine_annotation", ",", "\n", "selected_categories", ",", "\n", "default_category", ",", "\n", "vision_annotation", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "annotation_table", "=", "pd", ".", "read_csv", "(", "annotation_table", ")", "\n", "self", ".", "raw_annotation", "=", "raw_annotation", "\n", "self", ".", "cermine_annotation", "=", "cermine_annotation", "\n", "self", ".", "vision_annotation", "=", "vision_annotation", "\n", "\n", "self", ".", "selected_categories", "=", "selected_categories", "\n", "self", ".", "default_category", "=", "default_category", "\n", "self", ".", "cat2id", "=", "{", "cat", ":", "idx", "for", "idx", ",", "cat", "in", "enumerate", "(", "selected_categories", ")", "}", "\n", "self", ".", "id2cat", "=", "{", "idx", ":", "cat", "for", "idx", ",", "cat", "in", "enumerate", "(", "selected_categories", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.get_unique_shas": [[194, 196], ["condense_dataset.S2VLAnnotationGenerator.annotation_table.sha.unique"], "methods", ["None"], ["", "def", "get_unique_shas", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "annotation_table", ".", "sha", ".", "unique", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json": [[197, 226], ["pandas.DataFrame", "token_df[].tolist", "token_df[].tolist", "token_df[].map().tolist", "token_df[].astype().tolist", "token_df[].astype().tolist", "str", "token_df[].map", "token_df[].astype", "token_df[].astype", "int", "pandas.DataFrame.text.str.isspace", "pandas.DataFrame.text.isnull", "pandas.DataFrame.text.isna"], "methods", ["None"], ["", "def", "convert_token_data_to_json", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "token_df", "=", "pd", ".", "DataFrame", "(", "\n", "[", "\n", "[", "\n", "e", ".", "id", ",", "\n", "str", "(", "e", ".", "text", ")", ",", "\n", "[", "int", "(", "_", ")", "for", "_", "in", "e", ".", "coordinates", "]", ",", "\n", "e", ".", "type", ",", "\n", "e", ".", "block_id", ",", "\n", "e", ".", "line_id", ",", "\n", "]", "\n", "for", "e", "in", "tokens", "\n", "]", ",", "\n", "columns", "=", "[", "\"id\"", ",", "\"text\"", ",", "\"bbox\"", ",", "\"category\"", ",", "\"block_id\"", ",", "\"line_id\"", "]", ",", "\n", ")", "\n", "token_df", "=", "token_df", "[", "\n", "~", "token_df", ".", "text", ".", "isnull", "(", ")", "\n", "&", "~", "token_df", ".", "text", ".", "isna", "(", ")", "\n", "&", "~", "token_df", ".", "text", ".", "str", ".", "isspace", "(", ")", "\n", "]", "\n", "row_item", "=", "{", "\n", "\"words\"", ":", "token_df", "[", "\"text\"", "]", ".", "tolist", "(", ")", ",", "\n", "\"bbox\"", ":", "token_df", "[", "\"bbox\"", "]", ".", "tolist", "(", ")", ",", "\n", "\"labels\"", ":", "token_df", "[", "\"category\"", "]", ".", "map", "(", "self", ".", "cat2id", ")", ".", "tolist", "(", ")", ",", "\n", "\"block_ids\"", ":", "token_df", "[", "\"block_id\"", "]", ".", "astype", "(", "\"int\"", ")", ".", "tolist", "(", ")", ",", "\n", "\"line_ids\"", ":", "token_df", "[", "\"line_id\"", "]", ".", "astype", "(", "\"int\"", ")", ".", "tolist", "(", ")", ",", "\n", "}", "\n", "\n", "return", "row_item", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.create_annotation_for_sha": [[227, 278], ["condense_dataset.S2VLAnnotationGenerator.raw_annotation.load_annotation_for_sha", "condense_dataset.S2VLAnnotationGenerator.cermine_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGenerator.keys", "condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json", "len", "all_token_data.append", "all_files.append", "word.is_in", "word.is_in", "word.is_in"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_annotation_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "def", "create_annotation_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "all_token_data", "=", "[", "]", "\n", "all_files", "=", "[", "]", "\n", "\n", "raw_blocks", "=", "self", ".", "raw_annotation", ".", "load_annotation_for_sha", "(", "sha", ")", "\n", "cermine_data", "=", "self", ".", "cermine_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "\n", "for", "page_id", "in", "cermine_data", ".", "keys", "(", ")", ":", "\n", "            ", "blocks", "=", "[", "\n", "b", "for", "b", "in", "raw_blocks", "[", "page_id", "]", "if", "b", ".", "type", "in", "self", ".", "selected_categories", "\n", "]", "\n", "\n", "# Pass 1: O(n) Initialize ids and categories", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "word", ".", "line_id", "=", "-", "1", "\n", "word", ".", "block_id", "=", "-", "1", "\n", "word", ".", "type", "=", "self", ".", "default_category", "\n", "\n", "# Pass 2: O(mn) Assign token categories", "\n", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "block", "in", "blocks", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "block", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "type", "=", "block", ".", "type", "\n", "\n", "# Pass 3: O(mn) Assign token block-category ids", "\n", "", "", "", "used_lines_for_assign_line_ids", "=", "cermine_data", "[", "page_id", "]", ".", "lines", "\n", "used_blocks_for_assign_block_ids", "=", "cermine_data", "[", "page_id", "]", ".", "blocks", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "_l", "in", "used_lines_for_assign_line_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_l", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "line_id", "=", "_l", ".", "id", "\n", "\n", "", "", "for", "_b", "in", "used_blocks_for_assign_block_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_b", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "block_id", "=", "_b", ".", "id", "\n", "\n", "# Pass 4: O(n) In case some blocks are not assigned with the", "\n", "# appropriate block indices, we assign the line ids", "\n", "", "", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "if", "word", ".", "block_id", "==", "-", "1", ":", "\n", "                    ", "word", ".", "block_id", "=", "word", ".", "line_id", "+", "PADDING_CONSTANT", "\n", "\n", "", "", "row_item", "=", "self", ".", "convert_token_data_to_json", "(", "cermine_data", "[", "page_id", "]", ".", "words", ")", "\n", "\n", "if", "len", "(", "row_item", "[", "\"words\"", "]", ")", ">", "0", ":", "\n", "\n", "                ", "all_token_data", ".", "append", "(", "row_item", ")", "\n", "all_files", ".", "append", "(", "f\"{sha}-{page_id}\"", ")", "\n", "\n", "", "", "return", "all_token_data", ",", "all_files", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.create_annotation_for_shas": [[279, 289], ["tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm.set_description", "condense_dataset.S2VLAnnotationGenerator.create_annotation_for_sha", "all_token_data.extend", "all_files.extend"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithVisionLine.create_annotation_for_sha"], ["", "def", "create_annotation_for_shas", "(", "self", ",", "shas", ")", ":", "\n", "        ", "all_token_data", "=", "[", "]", "\n", "all_files", "=", "[", "]", "\n", "pbar", "=", "tqdm", "(", "shas", ")", "\n", "for", "sha", "in", "pbar", ":", "\n", "            ", "pbar", ".", "set_description", "(", "sha", ")", "\n", "token_data", ",", "files", "=", "self", ".", "create_annotation_for_sha", "(", "sha", ")", "\n", "all_token_data", ".", "extend", "(", "token_data", ")", "\n", "all_files", ".", "extend", "(", "files", ")", "\n", "", "return", "all_token_data", ",", "all_files", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.create_annotations": [[290, 301], ["condense_dataset.S2VLAnnotationGenerator.get_unique_shas", "condense_dataset.S2VLAnnotationGenerator.create_annotation_for_shas", "list", "set", "enumerate", "ele.split"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.get_unique_shas", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.create_annotation_for_shas"], ["", "def", "create_annotations", "(", "self", ")", ":", "\n", "        ", "shas", "=", "self", ".", "get_unique_shas", "(", ")", "\n", "all_token_data", ",", "all_files", "=", "self", ".", "create_annotation_for_shas", "(", "shas", ")", "\n", "all_valid_shas", "=", "list", "(", "set", "(", "[", "ele", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "for", "ele", "in", "all_files", "]", ")", ")", "\n", "\n", "self", ".", "all_token_data", "=", "all_token_data", "\n", "self", ".", "all_files", "=", "all_files", "\n", "self", ".", "all_valid_shas", "=", "all_valid_shas", "\n", "self", ".", "sha_to_sample_mapping", "=", "{", "\n", "sha", ":", "[", "idx", "for", "idx", ",", "file", "in", "enumerate", "(", "all_files", ")", "if", "file", "[", ":", "40", "]", "==", "sha", "]", "\n", "for", "sha", "in", "all_valid_shas", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_annotation_cv": [[303, 329], ["sklearn.model_selection.KFold", "enumerate", "tqdm.tqdm.tqdm", "condense_dataset.S2VLAnnotationGenerator.save_json", "sklearn.model_selection.KFold.split", "list", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_json"], ["", "def", "save_annotation_cv", "(", "self", ",", "export_folder", ",", "n_fold", "=", "5", ")", ":", "\n", "\n", "        ", "kf", "=", "KFold", "(", "n_splits", "=", "n_fold", ",", "shuffle", "=", "True", ",", "random_state", "=", "42", ")", "\n", "\n", "for", "idx", ",", "(", "train_idx", ",", "test_idx", ")", "in", "enumerate", "(", "\n", "tqdm", "(", "kf", ".", "split", "(", "self", ".", "all_valid_shas", ")", ",", "total", "=", "n_fold", ")", "\n", ")", ":", "\n", "            ", "annotation_data", "=", "{", "}", "\n", "train_test_split", "=", "{", "}", "\n", "\n", "for", "name", ",", "indices", "in", "[", "(", "\"train\"", ",", "train_idx", ")", ",", "(", "\"test\"", ",", "test_idx", ")", "]", ":", "\n", "                ", "cur_shas", "=", "[", "self", ".", "all_valid_shas", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "selected_data_item_indices", "=", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "[", "self", ".", "sha_to_sample_mapping", "[", "sha", "]", "for", "sha", "in", "cur_shas", "]", "\n", ")", "\n", ")", "\n", "\n", "annotation_data", "[", "name", "]", "=", "(", "\n", "[", "self", ".", "all_token_data", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", "[", "self", ".", "all_files", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", ")", "\n", "train_test_split", "[", "name", "]", "=", "annotation_data", "[", "name", "]", "[", "1", "]", "\n", "\n", "", "cur_export_folder", "=", "f\"{export_folder}/{idx}\"", "\n", "self", ".", "save_json", "(", "annotation_data", ",", "train_test_split", ",", "cur_export_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_annotation_few_shot": [[330, 356], ["tqdm.tqdm.tqdm", "sklearn.model_selection.train_test_split", "condense_dataset.S2VLAnnotationGenerator.save_json", "list", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_json"], ["", "", "def", "save_annotation_few_shot", "(", "self", ",", "export_folder", ",", "sample_sizes", "=", "[", "5", ",", "10", ",", "15", "]", ")", ":", "\n", "\n", "        ", "for", "sample_size", "in", "tqdm", "(", "sample_sizes", ")", ":", "\n", "\n", "            ", "train_sha", ",", "test_sha", "=", "train_test_split", "(", "\n", "self", ".", "all_valid_shas", ",", "train_size", "=", "sample_size", ",", "random_state", "=", "42", "\n", ")", "\n", "\n", "annotation_data", "=", "{", "}", "\n", "train_test_files", "=", "{", "}", "\n", "\n", "for", "name", ",", "cur_shas", "in", "[", "(", "\"train\"", ",", "train_sha", ")", ",", "(", "\"test\"", ",", "test_sha", ")", "]", ":", "\n", "                ", "selected_data_item_indices", "=", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "[", "self", ".", "sha_to_sample_mapping", "[", "sha", "]", "for", "sha", "in", "cur_shas", "]", "\n", ")", "\n", ")", "\n", "\n", "annotation_data", "[", "name", "]", "=", "(", "\n", "[", "self", ".", "all_token_data", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", "[", "self", ".", "all_files", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", ")", "\n", "train_test_files", "[", "name", "]", "=", "annotation_data", "[", "name", "]", "[", "1", "]", "\n", "\n", "", "cur_export_folder", "=", "f\"{export_folder}/{sample_size}\"", "\n", "self", ".", "save_json", "(", "annotation_data", ",", "train_test_files", ",", "cur_export_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_annotation_few_shot_with_mutual_test_set": [[357, 394], ["max", "sklearn.model_selection.train_test_split", "tqdm.tqdm.tqdm", "len", "random.sample", "condense_dataset.S2VLAnnotationGenerator.save_json", "list", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_json"], ["", "", "def", "save_annotation_few_shot_with_mutual_test_set", "(", "\n", "self", ",", "export_folder", ",", "sample_sizes", "=", "[", "5", ",", "10", ",", "15", "]", "\n", ")", ":", "\n", "\n", "        ", "maximum_training_samples", "=", "max", "(", "sample_sizes", ")", "\n", "maximum_remaining_test_samples", "=", "(", "\n", "len", "(", "self", ".", "all_valid_shas", ")", "-", "maximum_training_samples", "\n", ")", "\n", "\n", "all_train_sha", ",", "test_sha", "=", "train_test_split", "(", "\n", "self", ".", "all_valid_shas", ",", "\n", "test_size", "=", "maximum_remaining_test_samples", ",", "\n", "random_state", "=", "42", ",", "\n", ")", "\n", "\n", "for", "sample_size", "in", "tqdm", "(", "sample_sizes", ")", ":", "\n", "\n", "            ", "train_sha", "=", "random", ".", "sample", "(", "all_train_sha", ",", "sample_size", ")", "\n", "\n", "annotation_data", "=", "{", "}", "\n", "train_test_files", "=", "{", "}", "\n", "\n", "for", "name", ",", "cur_shas", "in", "[", "(", "\"train\"", ",", "train_sha", ")", ",", "(", "\"test\"", ",", "test_sha", ")", "]", ":", "\n", "                ", "selected_data_item_indices", "=", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "[", "self", ".", "sha_to_sample_mapping", "[", "sha", "]", "for", "sha", "in", "cur_shas", "]", "\n", ")", "\n", ")", "\n", "\n", "annotation_data", "[", "name", "]", "=", "(", "\n", "[", "self", ".", "all_token_data", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", "[", "self", ".", "all_files", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", ")", "\n", "train_test_files", "[", "name", "]", "=", "annotation_data", "[", "name", "]", "[", "1", "]", "\n", "\n", "", "cur_export_folder", "=", "f\"{export_folder}/{sample_size}\"", "\n", "self", ".", "save_json", "(", "annotation_data", ",", "train_test_files", ",", "cur_export_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_annotation_few_shot_and_cv": [[395, 422], ["enumerate", "tqdm.tqdm.tqdm", "condense_dataset.S2VLAnnotationGenerator.save_json", "list", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_json"], ["", "", "def", "save_annotation_few_shot_and_cv", "(", "\n", "self", ",", "export_folder", ",", "train_test_shas", ",", "sample_sizes", "=", "[", "5", ",", "10", ",", "15", ",", "25", ",", "45", ",", "70", "]", "\n", ")", ":", "\n", "\n", "        ", "for", "cv_index", ",", "_shas", "in", "enumerate", "(", "tqdm", "(", "train_test_shas", ")", ")", ":", "\n", "            ", "all_train_sha", ",", "test_sha", "=", "_shas", "[", "\"train\"", "]", ",", "_shas", "[", "\"test\"", "]", "\n", "for", "sample_size", "in", "sample_sizes", ":", "\n", "                ", "train_sha", "=", "all_train_sha", "[", ":", "sample_size", "]", "\n", "\n", "annotation_data", "=", "{", "}", "\n", "train_test_files", "=", "{", "}", "\n", "\n", "for", "name", ",", "cur_shas", "in", "[", "(", "\"train\"", ",", "train_sha", ")", ",", "(", "\"test\"", ",", "test_sha", ")", "]", ":", "\n", "                    ", "selected_data_item_indices", "=", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "[", "self", ".", "sha_to_sample_mapping", "[", "sha", "]", "for", "sha", "in", "cur_shas", "]", "\n", ")", "\n", ")", "\n", "\n", "annotation_data", "[", "name", "]", "=", "(", "\n", "[", "self", ".", "all_token_data", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", "[", "self", ".", "all_files", "[", "i", "]", "for", "i", "in", "selected_data_item_indices", "]", ",", "\n", ")", "\n", "train_test_files", "[", "name", "]", "=", "annotation_data", "[", "name", "]", "[", "1", "]", "\n", "\n", "", "cur_export_folder", "=", "f\"{export_folder}/{sample_size}/{cv_index}\"", "\n", "self", ".", "save_json", "(", "annotation_data", ",", "train_test_files", ",", "cur_export_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.save_json": [[423, 436], ["os.makedirs", "annotation_data.items", "condense_dataset.write_json", "condense_dataset.write_json", "condense_dataset.write_json"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.write_json", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.write_json", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.write_json"], ["", "", "", "def", "save_json", "(", "self", ",", "annotation_data", ",", "train_test_split", ",", "export_folder", ")", ":", "\n", "\n", "        ", "os", ".", "makedirs", "(", "export_folder", ",", "exist_ok", "=", "True", ")", "\n", "\n", "for", "subset", ",", "(", "all_token_data", ",", "all_files", ")", "in", "annotation_data", ".", "items", "(", ")", ":", "\n", "\n", "            ", "write_json", "(", "\n", "{", "\"data\"", ":", "all_token_data", ",", "\"labels\"", ":", "self", ".", "cat2id", ",", "\"files\"", ":", "all_files", "}", ",", "\n", "f\"{export_folder}/{subset}-token.json\"", ",", "\n", ")", "\n", "\n", "", "write_json", "(", "train_test_split", ",", "f\"{export_folder}/train-test-split.json\"", ")", "\n", "write_json", "(", "self", ".", "id2cat", ",", "f\"{export_folder}/labels.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithGTBox.order_blocks_based_on_token_ids": [[439, 465], ["x.set", "token.is_in", "len", "token_ids_in_blocks.append", "token_ids_in_blocks.append", "enumerate", "token_ids_in_this_block.append", "float", "min", "sorted", "zip"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["    ", "@", "staticmethod", "\n", "def", "order_blocks_based_on_token_ids", "(", "blocks", ",", "tokens", ")", ":", "\n", "\n", "        ", "token_ids_in_blocks", "=", "[", "]", "\n", "\n", "for", "block", "in", "blocks", ":", "\n", "\n", "            ", "token_ids_in_this_block", "=", "[", "]", "\n", "\n", "for", "token", "in", "tokens", ":", "\n", "                ", "if", "token", ".", "is_in", "(", "block", ",", "center", "=", "True", ")", ":", "\n", "                    ", "token_ids_in_this_block", ".", "append", "(", "token", ".", "id", ")", "\n", "\n", "", "", "if", "len", "(", "token_ids_in_this_block", ")", "==", "0", ":", "\n", "                ", "token_ids_in_blocks", ".", "append", "(", "float", "(", "\"inf\"", ")", ")", "\n", "", "else", ":", "\n", "                ", "token_ids_in_blocks", ".", "append", "(", "min", "(", "token_ids_in_this_block", ")", ")", "\n", "\n", "", "", "sorted_blocks", "=", "[", "\n", "x", ".", "set", "(", "id", "=", "idx", ")", "\n", "for", "idx", ",", "(", "_", ",", "x", ")", "in", "enumerate", "(", "\n", "sorted", "(", "zip", "(", "token_ids_in_blocks", ",", "blocks", ")", ",", "key", "=", "lambda", "pair", ":", "pair", "[", "0", "]", ")", "\n", ")", "\n", "]", "\n", "\n", "return", "sorted_blocks", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithGTBox.create_annotation_for_sha": [[466, 521], ["condense_dataset.S2VLAnnotationGeneratorWithGTBox.raw_annotation.load_annotation_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithGTBox.cermine_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithGTBox.keys", "condense_dataset.S2VLAnnotationGeneratorWithGTBox.order_blocks_based_on_token_ids", "condense_dataset.S2VLAnnotationGeneratorWithGTBox.convert_token_data_to_json", "len", "all_token_data.append", "all_files.append", "word.is_in", "word.is_in", "word.is_in"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_annotation_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithGTBox.order_blocks_based_on_token_ids", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["", "def", "create_annotation_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "all_token_data", "=", "[", "]", "\n", "all_files", "=", "[", "]", "\n", "\n", "raw_blocks", "=", "self", ".", "raw_annotation", ".", "load_annotation_for_sha", "(", "sha", ")", "\n", "cermine_data", "=", "self", ".", "cermine_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "\n", "for", "page_id", "in", "cermine_data", ".", "keys", "(", ")", ":", "\n", "            ", "blocks", "=", "[", "\n", "b", "for", "b", "in", "raw_blocks", "[", "page_id", "]", "if", "b", ".", "type", "in", "self", ".", "selected_categories", "\n", "]", "\n", "blocks", "=", "self", ".", "order_blocks_based_on_token_ids", "(", "\n", "blocks", ",", "cermine_data", "[", "page_id", "]", ".", "words", "\n", ")", "\n", "\n", "# Pass 1: O(n) Initialize ids and categories", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "word", ".", "line_id", "=", "-", "1", "\n", "word", ".", "block_id", "=", "-", "1", "\n", "word", ".", "type", "=", "self", ".", "default_category", "\n", "\n", "# Pass 2: O(mn) Assign token categories", "\n", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "block", "in", "blocks", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "block", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "type", "=", "block", ".", "type", "\n", "\n", "# Pass 3: O(mn) Assign token block-category ids", "\n", "", "", "", "used_lines_for_assign_line_ids", "=", "cermine_data", "[", "page_id", "]", ".", "lines", "\n", "used_blocks_for_assign_block_ids", "=", "blocks", "\n", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "_l", "in", "used_lines_for_assign_line_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_l", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "line_id", "=", "_l", ".", "id", "\n", "\n", "", "", "for", "_b", "in", "used_blocks_for_assign_block_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_b", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "block_id", "=", "_b", ".", "id", "\n", "\n", "# Pass 4: O(n) In case some blocks are not assigned with the", "\n", "# appropriate block indices, we assign the line ids", "\n", "", "", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "if", "word", ".", "block_id", "==", "-", "1", ":", "\n", "                    ", "word", ".", "block_id", "=", "word", ".", "line_id", "+", "PADDING_CONSTANT", "\n", "\n", "", "", "row_item", "=", "self", ".", "convert_token_data_to_json", "(", "cermine_data", "[", "page_id", "]", ".", "words", ")", "\n", "\n", "if", "len", "(", "row_item", "[", "\"words\"", "]", ")", ">", "0", ":", "\n", "\n", "                ", "all_token_data", ".", "append", "(", "row_item", ")", "\n", "all_files", ".", "append", "(", "f\"{sha}-{page_id}\"", ")", "\n", "\n", "", "", "return", "all_token_data", ",", "all_files", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithVisionBox.create_annotation_for_sha": [[524, 576], ["condense_dataset.S2VLAnnotationGeneratorWithVisionBox.raw_annotation.load_annotation_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionBox.cermine_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionBox.vision_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionBox.keys", "condense_dataset.S2VLAnnotationGeneratorWithVisionBox.convert_token_data_to_json", "len", "all_token_data.append", "all_files.append", "word.is_in", "word.is_in", "word.is_in"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_annotation_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["    ", "def", "create_annotation_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "all_token_data", "=", "[", "]", "\n", "all_files", "=", "[", "]", "\n", "\n", "raw_blocks", "=", "self", ".", "raw_annotation", ".", "load_annotation_for_sha", "(", "sha", ")", "\n", "cermine_data", "=", "self", ".", "cermine_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "vision_data", "=", "self", ".", "vision_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "\n", "for", "page_id", "in", "cermine_data", ".", "keys", "(", ")", ":", "\n", "            ", "blocks", "=", "[", "\n", "b", "for", "b", "in", "raw_blocks", "[", "page_id", "]", "if", "b", ".", "type", "in", "self", ".", "selected_categories", "\n", "]", "\n", "\n", "# Pass 1: O(n) Initialize ids and categories", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "word", ".", "line_id", "=", "-", "1", "\n", "word", ".", "block_id", "=", "-", "1", "\n", "word", ".", "type", "=", "self", ".", "default_category", "\n", "\n", "# Pass 2: O(mn) Assign token categories", "\n", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "block", "in", "blocks", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "block", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "type", "=", "block", ".", "type", "\n", "\n", "# Pass 3: O(mn) Assign token block-category ids", "\n", "", "", "", "used_lines_for_assign_line_ids", "=", "cermine_data", "[", "page_id", "]", ".", "lines", "\n", "used_blocks_for_assign_block_ids", "=", "vision_data", "[", "page_id", "]", ".", "blocks", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "_l", "in", "used_lines_for_assign_line_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_l", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "line_id", "=", "_l", ".", "id", "\n", "\n", "", "", "for", "_b", "in", "used_blocks_for_assign_block_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_b", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "block_id", "=", "_b", ".", "id", "\n", "\n", "# Pass 4: O(n) In case some blocks are not assigned with the", "\n", "# appropriate block indices, we assign the line ids", "\n", "", "", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "if", "word", ".", "block_id", "==", "-", "1", ":", "\n", "                    ", "word", ".", "block_id", "=", "word", ".", "line_id", "+", "PADDING_CONSTANT", "\n", "\n", "", "", "row_item", "=", "self", ".", "convert_token_data_to_json", "(", "cermine_data", "[", "page_id", "]", ".", "words", ")", "\n", "\n", "if", "len", "(", "row_item", "[", "\"words\"", "]", ")", ">", "0", ":", "\n", "\n", "                ", "all_token_data", ".", "append", "(", "row_item", ")", "\n", "all_files", ".", "append", "(", "f\"{sha}-{page_id}\"", ")", "\n", "\n", "", "", "return", "all_token_data", ",", "all_files", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithVisionLine.create_annotation_for_sha": [[579, 635], ["condense_dataset.S2VLAnnotationGeneratorWithVisionLine.raw_annotation.load_annotation_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionLine.cermine_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionLine.vision_annotation.load_annotations_for_sha", "condense_dataset.S2VLAnnotationGeneratorWithVisionLine.keys", "condense_dataset.S2VLAnnotationGeneratorWithVisionLine.order_blocks_based_on_token_ids", "condense_dataset.S2VLAnnotationGeneratorWithVisionLine.convert_token_data_to_json", "len", "all_token_data.append", "all_files.append", "word.is_in", "word.is_in", "word.is_in"], "methods", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.RawAnnotation.load_annotation_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.CERMINEAnnotation.load_annotations_for_sha", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGeneratorWithGTBox.order_blocks_based_on_token_ids", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.S2VLAnnotationGenerator.convert_token_data_to_json", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.vision_postprocessor.is_in"], ["    ", "def", "create_annotation_for_sha", "(", "self", ",", "sha", ")", ":", "\n", "\n", "        ", "all_token_data", "=", "[", "]", "\n", "all_files", "=", "[", "]", "\n", "\n", "raw_blocks", "=", "self", ".", "raw_annotation", ".", "load_annotation_for_sha", "(", "sha", ")", "\n", "cermine_data", "=", "self", ".", "cermine_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "vision_data", "=", "self", ".", "vision_annotation", ".", "load_annotations_for_sha", "(", "sha", ")", "\n", "\n", "for", "page_id", "in", "cermine_data", ".", "keys", "(", ")", ":", "\n", "            ", "blocks", "=", "[", "\n", "b", "for", "b", "in", "raw_blocks", "[", "page_id", "]", "if", "b", ".", "type", "in", "self", ".", "selected_categories", "\n", "]", "\n", "blocks", "=", "self", ".", "order_blocks_based_on_token_ids", "(", "\n", "blocks", ",", "cermine_data", "[", "page_id", "]", ".", "words", "\n", ")", "\n", "\n", "# Pass 1: O(n) Initialize ids and categories", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "word", ".", "line_id", "=", "-", "1", "\n", "word", ".", "block_id", "=", "-", "1", "\n", "word", ".", "type", "=", "self", ".", "default_category", "\n", "\n", "# Pass 2: O(mn) Assign token categories", "\n", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "block", "in", "blocks", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "block", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "type", "=", "block", ".", "type", "\n", "\n", "# Pass 3: O(mn) Assign token block-category ids", "\n", "", "", "", "used_lines_for_assign_line_ids", "=", "vision_data", "[", "page_id", "]", ".", "lines", "\n", "used_blocks_for_assign_block_ids", "=", "vision_data", "[", "page_id", "]", ".", "blocks", "\n", "\n", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "for", "_l", "in", "used_lines_for_assign_line_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_l", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "line_id", "=", "_l", ".", "id", "\n", "\n", "", "", "for", "_b", "in", "used_blocks_for_assign_block_ids", ":", "\n", "                    ", "if", "word", ".", "is_in", "(", "_b", ",", "center", "=", "True", ")", ":", "\n", "                        ", "word", ".", "block_id", "=", "_b", ".", "id", "\n", "\n", "# Pass 4: O(n) In case some blocks are not assigned with the", "\n", "# appropriate block indices, we assign the line ids", "\n", "", "", "", "for", "word", "in", "cermine_data", "[", "page_id", "]", ".", "words", ":", "\n", "                ", "if", "word", ".", "block_id", "==", "-", "1", ":", "\n", "                    ", "word", ".", "block_id", "=", "word", ".", "line_id", "+", "PADDING_CONSTANT", "\n", "\n", "", "", "row_item", "=", "self", ".", "convert_token_data_to_json", "(", "cermine_data", "[", "page_id", "]", ".", "words", ")", "\n", "\n", "if", "len", "(", "row_item", "[", "\"words\"", "]", ")", ">", "0", ":", "\n", "\n", "                ", "all_token_data", ".", "append", "(", "row_item", ")", "\n", "all_files", ".", "append", "(", "f\"{sha}-{page_id}\"", ")", "\n", "\n", "", "", "return", "all_token_data", ",", "all_files", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.load_json": [[23, 26], ["open", "json.load"], "function", ["None"], ["def", "load_json", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "fp", ":", "\n", "        ", "return", "json", ".", "load", "(", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.write_json": [[28, 31], ["open", "json.dump", "sklearn.model_selection.train_test_split"], "function", ["None"], ["", "", "def", "write_json", "(", "data", ",", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "fp", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.condense_dataset.cvt_df_to_layout": [[33, 45], ["layoutparser.TextBlock", "layoutparser.Rectangle"], "function", ["None"], ["", "", "def", "cvt_df_to_layout", "(", "row", ")", ":", "\n", "\n", "    ", "return", "lp", ".", "TextBlock", "(", "\n", "lp", ".", "Rectangle", "(", "\n", "row", "[", "\"x_1\"", "]", ",", "\n", "row", "[", "\"y_1\"", "]", ",", "\n", "row", "[", "\"x_2\"", "]", ",", "\n", "row", "[", "\"y_2\"", "]", ",", "\n", ")", ",", "\n", "id", "=", "row", "[", "\"id\"", "]", ",", "\n", "type", "=", "row", "[", "\"category\"", "]", ",", "\n", "text", "=", "row", "[", "\"text\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.bulk_fetch_pdf_for_urls": [[29, 72], ["os.makedirs", "paper_table.groupby().first().reset_index.groupby().first().reset_index", "tqdm.tqdm", "paper_table.groupby().first().reset_index.iterrows", "tqdm.tqdm.set_description", "paper_table.groupby().first().reset_index.groupby().first", "len", "os.path.join", "os.path.exists", "requests.get", "requests.get", "sha1.update", "sha1.hexdigest", "paper_download_status.append", "print", "paper_download_status.append", "print", "paper_download_status.append", "paper_table.groupby().first().reset_index.groupby", "str", "open", "fh.write"], "function", ["None"], ["def", "bulk_fetch_pdf_for_urls", "(", "\n", "paper_table", ":", "pd", ".", "DataFrame", ",", "\n", "target_dir", ":", "str", ",", "\n", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "\n", "    ", "os", ".", "makedirs", "(", "target_dir", ",", "exist_ok", "=", "True", ")", "\n", "paper_download_status", "=", "[", "]", "\n", "\n", "paper_table", "=", "paper_table", ".", "groupby", "(", "\"sha\"", ")", ".", "first", "(", ")", ".", "reset_index", "(", ")", "# Remove duplicates", "\n", "pbar", "=", "tqdm", "(", "paper_table", ".", "iterrows", "(", ")", ",", "total", "=", "len", "(", "paper_table", ")", ")", "\n", "\n", "for", "_", ",", "row", "in", "pbar", ":", "\n", "\n", "        ", "sha_in_table", "=", "row", "[", "\"sha\"", "]", "\n", "download_link", "=", "row", "[", "\"url\"", "]", "\n", "\n", "pbar", ".", "set_description", "(", "desc", "=", "download_link", ")", "\n", "\n", "try", ":", "\n", "            ", "pdf_path", "=", "os", ".", "path", ".", "join", "(", "target_dir", ",", "str", "(", "sha_in_table", ")", "+", "\".pdf\"", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "pdf_path", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "r", "=", "requests", ".", "get", "(", "download_link", ",", "headers", "=", "headers", ")", "\n", "\n", "if", "r", ".", "status_code", "==", "200", ":", "\n", "                ", "sha1", ".", "update", "(", "r", ".", "content", ")", "\n", "downloaded_sha", "=", "sha1", ".", "hexdigest", "(", ")", "\n", "\n", "\n", "with", "open", "(", "pdf_path", ",", "\"wb\"", ")", "as", "fh", ":", "\n", "                    ", "fh", ".", "write", "(", "r", ".", "content", ")", "\n", "\n", "", "paper_download_status", ".", "append", "(", "[", "sha_in_table", ",", "downloaded_sha", ",", "\"success\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "f\"Fail to download due to HTTP error {r.status_code} for {download_link}\"", ")", "\n", "paper_download_status", ".", "append", "(", "[", "sha_in_table", ",", "None", ",", "\"download_error\"", "]", ")", "\n", "", "", "except", ":", "\n", "            ", "print", "(", "f\"Fail to download due to HTTP error {r.status_code} for {download_link}\"", ")", "\n", "paper_download_status", ".", "append", "(", "[", "sha_in_table", ",", "None", ",", "\"download_error\"", "]", ")", "\n", "\n", "", "", "return", "paper_download_status", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.split_pdf_to_each_page_and_check": [[74, 139], ["range", "PyPDF2.PdfFileReader", "PyPDF2.PdfFileReader.getNumPages", "PyPDF2.PdfFileWriter", "PyPDF2.PdfFileWriter.addPage", "os.path.join", "len", "glob.glob", "print", "shutil.rmtree", "PyPDF2.PdfFileReader.getPage", "os.path.splitext", "is_page_successfully_saved.append", "len", "os.path.basename", "open", "PyPDF2.PdfFileWriter.write", "exit", "print", "layoutparser.load_pdf", "ok_files.append", "exit"], "function", ["None"], ["", "def", "split_pdf_to_each_page_and_check", "(", "pdf_file", ",", "target_folder", ",", "remove_problematic", "=", "False", ")", ":", "\n", "    ", "\"\"\"Split a pdf file into separate pages.\n\n    Args:\n        pdf_file (str): The name of the PDF file to be split.\n        target_folder (str): The target folder to save the splitted pages.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "pdf", "=", "PdfFileReader", "(", "pdf_file", ")", "\n", "# Sometimes the downloaded PDF is corrupted.", "\n", "total_pages", "=", "pdf", ".", "getNumPages", "(", ")", "\n", "# Sometimes some strange errors would occur if the pdf engine", "\n", "# thinks the pdf is corrupted.", "\n", "", "except", ":", "\n", "        ", "return", "False", "\n", "\n", "", "is_page_successfully_saved", "=", "[", "]", "\n", "\n", "# Try to save individual pages", "\n", "for", "i", "in", "range", "(", "total_pages", ")", ":", "\n", "        ", "pdf_writer", "=", "PdfFileWriter", "(", ")", "\n", "pdf_writer", ".", "addPage", "(", "pdf", ".", "getPage", "(", "i", ")", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "pdf_file", ")", ")", "[", "0", "]", "\n", "save_name", "=", "os", ".", "path", ".", "join", "(", "target_folder", ",", "f\"{filename}-{i:02d}.pdf\"", ")", "\n", "\n", "try", ":", "\n", "            ", "with", "open", "(", "save_name", ",", "\"wb\"", ")", "as", "outputStream", ":", "\n", "                ", "pdf_writer", ".", "write", "(", "outputStream", ")", "\n", "", "is_page_successfully_saved", ".", "append", "(", "i", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "exit", "(", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "f\"Failed to save {save_name}\"", ")", "\n", "\n", "", "del", "pdf_writer", "\n", "", "del", "pdf", "\n", "\n", "# If individual pages", "\n", "if", "len", "(", "is_page_successfully_saved", ")", "!=", "total_pages", ":", "\n", "        ", "is_pdf_successfully_saved", "=", "False", "\n", "\n", "", "else", ":", "\n", "        ", "ok_files", "=", "[", "]", "\n", "saved_pdf_files", "=", "glob", "(", "f\"{target_folder}/{filename}*.pdf\"", ")", "\n", "for", "saved_pdf_file", "in", "saved_pdf_files", ":", "\n", "            ", "try", ":", "\n", "                ", "lp", ".", "load_pdf", "(", "saved_pdf_file", ")", "\n", "ok_files", ".", "append", "(", "saved_pdf_file", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "                ", "exit", "(", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "", "", "if", "len", "(", "ok_files", ")", "!=", "total_pages", ":", "\n", "            ", "is_pdf_successfully_saved", "=", "False", "\n", "", "else", ":", "\n", "            ", "is_pdf_successfully_saved", "=", "True", "\n", "\n", "", "", "if", "not", "is_pdf_successfully_saved", "and", "remove_problematic", ":", "\n", "        ", "print", "(", "\n", "f\"The current PDF {pdf_file} cannot be appropriately parsed. Removing the saved folders\"", "\n", ")", "\n", "shutil", ".", "rmtree", "(", "target_folder", ")", "\n", "\n", "", "return", "is_pdf_successfully_saved", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download._generalized_paper_downloading_and_processing_protocol": [[141, 193], ["tempfile.TemporaryDirectory", "print", "pandas.DataFrame", "tqdm.tqdm", "pd.DataFrame.copy", "download.create_download_report", "os.path.exists", "os.makedirs", "download.create_download_report", "glob.glob", "pd.DataFrame.iterrows", "os.path.join", "shutil.move", "len", "tqdm.tqdm.set_description", "glob.glob", "tempfile.TemporaryDirectory", "download.split_pdf_to_each_page_and_check", "os.path.join", "paper_table.loc[].tolist", "shutil.move", "os.path.join", "download.bulk_fetch_pdf_for_urls"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.create_download_report", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.create_download_report", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.split_pdf_to_each_page_and_check", "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.bulk_fetch_pdf_for_urls"], ["", "def", "_generalized_paper_downloading_and_processing_protocol", "(", "\n", "paper_table", ",", "\n", "target_folder", ",", "\n", "download_func", ",", "\n", ")", ":", "\n", "\n", "    ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "combined_pdf_save_path", ":", "\n", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "target_folder", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "target_folder", ")", "\n", "\n", "", "print", "(", "\"Downloading the Papers\"", ")", "\n", "paper_download_status", "=", "download_func", "(", "paper_table", ",", "combined_pdf_save_path", ")", "\n", "paper_download_status", "=", "pd", ".", "DataFrame", "(", "\n", "paper_download_status", ",", "columns", "=", "[", "\"sha_in_table\"", ",", "\"downloaded_sha\"", ",", "\"status\"", "]", "\n", ")", "\n", "\n", "if", "\"page\"", "not", "in", "paper_table", ".", "columns", ":", "\n", "            ", "create_download_report", "(", "paper_download_status", ")", "\n", "for", "file", "in", "glob", "(", "os", ".", "path", ".", "join", "(", "combined_pdf_save_path", ",", "\"*\"", ")", ")", ":", "\n", "                ", "shutil", ".", "move", "(", "file", ",", "target_folder", ")", "\n", "", "return", "paper_download_status", "\n", "\n", "", "pbar", "=", "tqdm", "(", "paper_download_status", ".", "iterrows", "(", ")", ",", "total", "=", "len", "(", "paper_download_status", ")", ")", "\n", "updated_paper_download_status", "=", "paper_download_status", ".", "copy", "(", ")", "\n", "\n", "for", "idx", ",", "row", "in", "pbar", ":", "\n", "            ", "if", "row", "[", "\"status\"", "]", "==", "\"success\"", ":", "\n", "                ", "sha", "=", "row", "[", "\"sha_in_table\"", "]", "\n", "pbar", ".", "set_description", "(", "f\"Processing {sha}\"", ")", "\n", "\n", "if", "glob", "(", "f\"{target_folder}/{sha}-*\"", ")", ":", "\n", "                    ", "continue", "# Skip already processed", "\n", "\n", "", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tempdir", ":", "\n", "                    ", "is_pdf_successfully_saved", "=", "split_pdf_to_each_page_and_check", "(", "\n", "os", ".", "path", ".", "join", "(", "combined_pdf_save_path", ",", "sha", "+", "\".pdf\"", ")", ",", "tempdir", "\n", ")", "\n", "\n", "# In this command, it will save all the processed files in a tmp folder", "\n", "# As such, when the PDFs are successfully downloaded, we need to move them", "\n", "# to the actual target folder", "\n", "if", "is_pdf_successfully_saved", ":", "\n", "# The tempdir contains all the pages, but we only want to move the target pages", "\n", "                        ", "all_pages", "=", "paper_table", ".", "loc", "[", "paper_table", "[", "\"sha\"", "]", "==", "sha", ",", "\"page\"", "]", ".", "tolist", "(", ")", "\n", "for", "page", "in", "all_pages", ":", "\n", "                            ", "shutil", ".", "move", "(", "os", ".", "path", ".", "join", "(", "tempdir", ",", "f\"{sha}-{page:02d}.pdf\"", ")", ",", "target_folder", ")", "\n", "", "", "else", ":", "\n", "                        ", "updated_paper_download_status", ".", "iloc", "[", "idx", ",", "-", "1", "]", "=", "\"pdf_parsing_failure\"", "\n", "\n", "", "", "", "", "create_download_report", "(", "paper_download_status", ")", "\n", "return", "updated_paper_download_status", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.create_download_report": [[195, 219], ["print", "print", "print", "incompatible_papers.iterrows", "unsuccessful_papers.groupby", "print", "print", "gp.iterrows", "print", "len", "len", "len", "len"], "function", ["None"], ["", "", "def", "create_download_report", "(", "paper_download_status", ")", ":", "\n", "    ", "\"\"\"Create a report of the downloaded papers.\n\n    Args:\n        paper_download_status (pd.DataFrame): The status of the downloaded papers.\n    \"\"\"", "\n", "print", "(", "\"PDF Download Report\"", ")", "\n", "incompatible_papers", "=", "paper_download_status", "[", "paper_download_status", "[", "\"sha_in_table\"", "]", "!=", "paper_download_status", "[", "\"downloaded_sha\"", "]", "]", "\n", "\n", "print", "(", "f\"Total mismatch: {len(incompatible_papers)}/{len(paper_download_status)}\"", ")", "\n", "print", "(", "\"Note: The mismatch between SHA doesn't necessarily mean\\n\"", "\n", "\"the PDF files have different contents.\"", ")", "\n", "for", "_", ",", "row", "in", "incompatible_papers", ".", "iterrows", "(", ")", ":", "\n", "        ", "print", "(", "\n", "f\"Original SHA: {row['sha_in_table']} -> Actual SHA: {row['downloaded_sha']}\"", "\n", ")", "\n", "\n", "", "unsuccessful_papers", "=", "paper_download_status", "[", "\n", "paper_download_status", "[", "\"status\"", "]", "!=", "\"success\"", "\n", "]", "\n", "for", "error_name", ",", "gp", "in", "unsuccessful_papers", ".", "groupby", "(", "\"status\"", ")", ":", "\n", "        ", "print", "(", "f\"Total {error_name}: {len(gp)}/{len(paper_download_status)}\"", ")", "\n", "for", "_", ",", "row", "in", "gp", ".", "iterrows", "(", ")", ":", "\n", "            ", "print", "(", "f\"Fail to download SHA: {row['sha_in_table']}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download.fetch_and_process_papers_based_on_urls": [[221, 226], ["download._generalized_paper_downloading_and_processing_protocol"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.s2-vl-utils.download._generalized_paper_downloading_and_processing_protocol"], ["", "", "", "def", "fetch_and_process_papers_based_on_urls", "(", "\n", "paper_table", ",", "target_folder", "\n", ")", ":", "\n", "    ", "return", "_generalized_paper_downloading_and_processing_protocol", "(", "\n", "paper_table", ",", "target_folder", ",", "bulk_fetch_pdf_for_urls", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.push_model_to_hf_hub.load_json": [[10, 13], ["open", "json.load"], "function", ["None"], ["def", "load_json", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "fp", ":", "\n", "        ", "return", "json", ".", "load", "(", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.push_model_to_hf_hub.write_json": [[15, 18], ["open", "json.dump"], "function", ["None"], ["", "", "def", "write_json", "(", "data", ",", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "\"w\"", ")", "as", "fp", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-baseline.main": [[49, 391], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "len", "transformers.AutoConfig.from_pretrained", "transformers.AutoModelForTokenClassification.from_pretrained", "vila.dataset.preprocessors.instantiate_dataset_preprocessor", "utils.DataCollatorForTokenClassification", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "datasets.load_dataset", "datasets.load_dataset", "set", "list", "get_label_list.sort", "list", "isinstance", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "isinstance", "ValueError", "train_dataset.select.map", "eval_dataset.select.map", "test_dataset.select.map", "numpy.argmax", "list", "list", "utils.classification_report", "utils.classification_report.pop", "transformers.Trainer.train", "transformers.Trainer.save_model", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "transformers.Trainer.save_state", "logger.info", "transformers.Trainer.evaluate", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "logger.info", "os.path.join", "logger.info", "utils.columns_used_in_model_inputs", "len", "ValueError", "transformers.trainer_utils.is_main_process", "data_args.validation_file.split", "utils.load_json().values", "train-baseline.main.get_label_list"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.classification_report", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.columns_used_in_model_inputs"], ["def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "\n", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", "\n", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "\n", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "(", "\n", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "filename", "=", "\"./log.txt\"", ",", "\n", "filemode", "=", "\"a\"", ",", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "# handlers=[logging.StreamHandler(sys.stdout)],", "\n", ")", "\n", "logger", ".", "setLevel", "(", "\n", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", "\n", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub).", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called", "\n", "# 'text' is found. You can easily tweak this behavior (see below).", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "        ", "datasets", "=", "load_dataset", "(", "data_args", ".", "dataset_name", ",", "data_args", ".", "dataset_config_name", ")", "\n", "", "else", ":", "\n", "        ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "", "if", "data_args", ".", "test_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"test\"", "]", "=", "data_args", ".", "test_file", "\n", "", "extension", "=", "data_args", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ",", "field", "=", "\"data\"", ")", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"train\"", "]", ".", "features", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"validation\"", "]", ".", "features", "\n", "", "text_column_name", "=", "\"words\"", "\n", "label_column_name", "=", "\"labels\"", "\n", "\n", "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the", "\n", "# unique labels.", "\n", "def", "get_label_list", "(", "labels", ")", ":", "\n", "        ", "unique_labels", "=", "set", "(", ")", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "unique_labels", "=", "unique_labels", "|", "set", "(", "label", ")", "\n", "", "label_list", "=", "list", "(", "unique_labels", ")", "\n", "label_list", ".", "sort", "(", ")", "\n", "return", "label_list", "\n", "\n", "", "if", "data_args", ".", "label_map_file", "is", "not", "None", ":", "\n", "        ", "label_list", "=", "list", "(", "load_json", "(", "data_args", ".", "label_map_file", ")", ".", "values", "(", ")", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "features", "[", "label_column_name", "]", ".", "feature", ",", "ClassLabel", ")", ":", "\n", "            ", "label_list", "=", "features", "[", "label_column_name", "]", ".", "feature", ".", "names", "\n", "# No need to convert the labels since they are already ints.", "\n", "label_to_id", "=", "{", "i", ":", "i", "for", "i", "in", "range", "(", "len", "(", "label_list", ")", ")", "}", "\n", "", "else", ":", "\n", "            ", "label_list", "=", "get_label_list", "(", "datasets", "[", "\"train\"", "]", "[", "label_column_name", "]", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "label_list", "=", "[", "str", "(", "ele", ")", "for", "ele", "in", "label_list", "]", "# Ensure the ele is a string", "\n", "\n", "", "", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "\n", "if", "model_args", ".", "config_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "finetuning_task", "=", "data_args", ".", "task_name", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "if", "config", ".", "model_type", "in", "{", "\"gpt2\"", ",", "\"roberta\"", "}", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "\n", "if", "model_args", ".", "tokenizer_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "True", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "add_prefix_space", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "\n", "if", "model_args", ".", "tokenizer_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "True", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "model", "=", "AutoModelForTokenClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "\n", "# Tokenizer check: this script requires a fast tokenizer.", "\n", "if", "not", "isinstance", "(", "tokenizer", ",", "PreTrainedTokenizerFast", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"", "\n", "\"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"", "\n", "\"requirement\"", "\n", ")", "\n", "\n", "", "preprocessor", "=", "instantiate_dataset_preprocessor", "(", "\"base\"", ",", "tokenizer", ",", "data_args", ")", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "train_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_val_samples", ")", ")", "\n", "", "eval_dataset", "=", "eval_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "eval_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "\"test\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_predict requires a test dataset\"", ")", "\n", "", "test_dataset", "=", "datasets", "[", "\"test\"", "]", "\n", "if", "data_args", ".", "max_test_samples", "is", "not", "None", ":", "\n", "            ", "test_dataset", "=", "test_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_test_samples", ")", ")", "\n", "", "test_dataset", "=", "test_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_sample", ",", "\n", "batched", "=", "False", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Data collator", "\n", "", "data_collator", "=", "DataCollatorForTokenClassification", "(", "\n", "tokenizer", ",", "pad_to_multiple_of", "=", "8", "if", "training_args", ".", "fp16", "else", "None", "\n", ")", "\n", "\n", "def", "compute_metrics", "(", "p", ")", ":", "\n", "        ", "predictions", ",", "labels", "=", "p", "\n", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "# Remove ignored index (special tokens)", "\n", "true_predictions", "=", "[", "\n", "[", "label_list", "[", "p", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "true_labels", "=", "[", "\n", "[", "label_list", "[", "l", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "true_labels", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_labels", ")", ")", "\n", "results", "=", "classification_report", "(", "y_true", "=", "true_labels", ",", "y_pred", "=", "true_predictions", ")", "\n", "results", ".", "pop", "(", "\"detailed\"", ")", "\n", "return", "results", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "compute_metrics", "=", "compute_metrics", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "not", "model_args", ".", "not_resume_training", ":", "\n", "            ", "if", "last_checkpoint", "is", "not", "None", ":", "\n", "                ", "checkpoint", "=", "last_checkpoint", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", ":", "\n", "                ", "checkpoint", "=", "model_args", ".", "model_name_or_path", "\n", "", "else", ":", "\n", "                ", "checkpoint", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "checkpoint", "=", "None", "\n", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "\n", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_val_samples", "=", "(", "\n", "data_args", ".", "max_val_samples", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", "\n", "else", "len", "(", "eval_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_val_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Predict", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "\n", "output_test_predictions_file", "=", "os", ".", "path", ".", "join", "(", "\n", "training_args", ".", "output_dir", ",", "data_args", ".", "pred_file_name", "\n", ")", "\n", "logger", ".", "info", "(", "f\"The test file will be saved to {output_test_predictions_file}\"", ")", "\n", "\n", "_used_cols", "=", "columns_used_in_model_inputs", "(", "test_dataset", ",", "model", ")", "\n", "_used_cols", "=", "[", "ele", "for", "ele", "in", "_used_cols", "if", "\"label\"", "not", "in", "ele", "]", "\n", "\n", "all_pred_df", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "idx", ",", "sample", "in", "enumerate", "(", "tqdm", "(", "test_dataset", ")", ")", ":", "\n", "                ", "_sample", "=", "{", "\n", "key", ":", "torch", ".", "tensor", "(", "val", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "model", ".", "device", ")", "\n", "for", "key", ",", "val", "in", "sample", ".", "items", "(", ")", "\n", "if", "key", "in", "_used_cols", "\n", "}", "\n", "model_outputs", "=", "trainer", ".", "model", "(", "**", "_sample", ")", "\n", "predictions", "=", "model_outputs", ".", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "sample", "[", "\"labels\"", "]", "\n", "\n", "true_predictions", "=", "[", "\n", "[", "(", "p", ",", "l", ")", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "words", "=", "[", "sample", "[", "\"words\"", "]", "[", "idx", "]", "for", "idx", "in", "sample", "[", "\"encoded_word_ids\"", "]", "]", "\n", "block_ids", "=", "[", "\n", "sample", "[", "\"block_ids\"", "]", "[", "idx", "]", "for", "idx", "in", "sample", "[", "\"encoded_word_ids\"", "]", "\n", "]", "\n", "\n", "assert", "len", "(", "true_predictions", ")", "==", "len", "(", "words", ")", "\n", "df", "=", "pd", ".", "DataFrame", "(", "true_predictions", ",", "columns", "=", "[", "\"pred\"", ",", "\"gt\"", "]", ")", "\n", "df", "[", "\"word\"", "]", "=", "words", "\n", "df", "[", "\"block_id\"", "]", "=", "block_ids", "\n", "df", "[", "\"word_id\"", "]", "=", "sample", "[", "\"encoded_word_ids\"", "]", "\n", "df", "[", "\"sample_id\"", "]", "=", "idx", "\n", "all_pred_df", ".", "append", "(", "df", ")", "\n", "\n", "", "pd", ".", "concat", "(", "all_pred_df", ")", ".", "to_csv", "(", "output_test_predictions_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-baseline._mp_fn": [[393, 396], ["train-baseline.main"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila.main"], ["", "", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.arguments.ModelArguments.__post_init__": [[73, 82], ["None"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "\n", "        ", "assert", "self", ".", "added_special_separation_token", "in", "[", "\"BLK\"", ",", "\"SEP\"", "]", "\n", "\n", "if", "self", ".", "added_special_separation_token", "==", "\"BLK\"", ":", "\n", "            ", "self", ".", "added_special_separation_token", "=", "\"[BLK]\"", "\n", "\n", "", "if", "self", ".", "added_special_separation_token", "==", "\"SEP\"", ":", "\n", "            ", "self", ".", "added_special_separation_token", "=", "\"[SEP]\"", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.arguments.DataTrainingArguments.__post_init__": [[215, 270], ["arguments.DataTrainingArguments.task_name.lower", "ValueError", "arguments.DataTrainingArguments.dataset_name.lower", "arguments.DataTrainingArguments.train_file.split", "arguments.DataTrainingArguments.validation_file.split", "arguments.DataTrainingArguments.dataset_name.lower"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "\n", "        ", "if", "(", "\n", "self", ".", "dataset_name", "is", "None", "\n", "and", "self", ".", "train_file", "is", "None", "\n", "and", "self", ".", "validation_file", "is", "None", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Need either a dataset name or a training/validation file.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "train_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "train_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\n", "\"csv\"", ",", "\n", "\"json\"", ",", "\n", "]", ",", "\"`train_file` should be a csv or a json file.\"", "\n", "", "if", "self", ".", "validation_file", "is", "not", "None", ":", "\n", "                ", "extension", "=", "self", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "assert", "extension", "in", "[", "\n", "\"csv\"", ",", "\n", "\"json\"", ",", "\n", "]", ",", "\"`validation_file` should be a csv or a json file.\"", "\n", "\n", "", "", "self", ".", "task_name", "=", "self", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "self", ".", "dataset_name", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "dataset_name", ".", "lower", "(", ")", "==", "\"grotoap2\"", ":", "\n", "# fmt:off", "\n", "                ", "self", ".", "train_file", "=", "\"../data/grotoap2/train-token.json\"", "\n", "self", ".", "validation_file", "=", "\"../data/grotoap2/dev-token.json\"", "\n", "self", ".", "test_file", "=", "\"../data/grotoap2/test-token.json\"", "\n", "self", ".", "label_map_file", "=", "\"../data/grotoap2/labels.json\"", "\n", "self", ".", "dataset_name", "=", "None", "\n", "# fmt:on", "\n", "\n", "self", ".", "max_line_per_page", "=", "MAX_LINE_PER_PAGE", "\n", "self", ".", "max_tokens_per_line", "=", "MAX_TOKENS_PER_LINE", "\n", "self", ".", "max_block_per_page", "=", "MAX_BLOCK_PER_PAGE", "\n", "self", ".", "max_tokens_per_block", "=", "MAX_TOKENS_PER_BLOCK", "\n", "\n", "", "elif", "self", ".", "dataset_name", ".", "lower", "(", ")", "==", "\"docbank\"", ":", "\n", "\n", "# fmt:off", "\n", "                ", "self", ".", "train_file", "=", "\"../data/docbank/train-token.json\"", "\n", "self", ".", "validation_file", "=", "\"../data/docbank/dev-token.json\"", "\n", "self", ".", "test_file", "=", "\"../data/docbank/test-token.json\"", "\n", "self", ".", "label_map_file", "=", "\"../data/docbank/labels.json\"", "\n", "self", ".", "dataset_name", "=", "None", "\n", "# fmt:on", "\n", "\n", "self", ".", "max_line_per_page", "=", "100", "\n", "self", ".", "max_tokens_per_line", "=", "25", "\n", "self", ".", "max_block_per_page", "=", "30", "\n", "self", ".", "max_tokens_per_block", "=", "100", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-ivila.main": [[51, 406], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "len", "transformers.AutoConfig.from_pretrained", "transformers.AutoModelForTokenClassification.from_pretrained", "logger.info", "vila.dataset.preprocessors.instantiate_dataset_preprocessor", "utils.DataCollatorForTokenClassification", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "datasets.load_dataset", "datasets.load_dataset", "set", "list", "get_label_list.sort", "list", "isinstance", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "AutoTokenizer.from_pretrained.special_tokens_map.values", "AutoTokenizer.from_pretrained.add_special_tokens", "AutoModelForTokenClassification.from_pretrained.resize_token_embeddings", "isinstance", "ValueError", "train_dataset.select.map", "eval_dataset.select.map", "test_dataset.select.map", "numpy.argmax", "list", "list", "utils.classification_report", "utils.classification_report.pop", "transformers.Trainer.train", "transformers.Trainer.save_model", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "transformers.Trainer.save_state", "logger.info", "transformers.Trainer.evaluate", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "logger.info", "os.path.join", "logger.info", "utils.columns_used_in_model_inputs", "len", "ValueError", "transformers.trainer_utils.is_main_process", "data_args.validation_file.split", "utils.load_json().values", "train-ivila.main.get_label_list"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.classification_report", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.columns_used_in_model_inputs"], ["def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "\n", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", "\n", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "\n", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "(", "\n", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "filename", "=", "\"./log.txt\"", ",", "\n", "filemode", "=", "\"a\"", ",", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "# handlers=[logging.StreamHandler(sys.stdout)],", "\n", ")", "\n", "logger", ".", "setLevel", "(", "\n", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", "\n", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub).", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called", "\n", "# 'text' is found. You can easily tweak this behavior (see below).", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "        ", "datasets", "=", "load_dataset", "(", "data_args", ".", "dataset_name", ",", "data_args", ".", "dataset_config_name", ")", "\n", "", "else", ":", "\n", "        ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "", "if", "data_args", ".", "test_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"test\"", "]", "=", "data_args", ".", "test_file", "\n", "", "extension", "=", "data_args", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ",", "field", "=", "\"data\"", ")", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"train\"", "]", ".", "features", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"validation\"", "]", ".", "features", "\n", "", "text_column_name", "=", "\"words\"", "\n", "label_column_name", "=", "\"labels\"", "\n", "\n", "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the", "\n", "# unique labels.", "\n", "def", "get_label_list", "(", "labels", ")", ":", "\n", "        ", "unique_labels", "=", "set", "(", ")", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "unique_labels", "=", "unique_labels", "|", "set", "(", "label", ")", "\n", "", "label_list", "=", "list", "(", "unique_labels", ")", "\n", "label_list", ".", "sort", "(", ")", "\n", "return", "label_list", "\n", "\n", "", "if", "data_args", ".", "label_map_file", "is", "not", "None", ":", "\n", "        ", "label_list", "=", "list", "(", "load_json", "(", "data_args", ".", "label_map_file", ")", ".", "values", "(", ")", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "features", "[", "label_column_name", "]", ".", "feature", ",", "ClassLabel", ")", ":", "\n", "            ", "label_list", "=", "features", "[", "label_column_name", "]", ".", "feature", ".", "names", "\n", "# No need to convert the labels since they are already ints.", "\n", "label_to_id", "=", "{", "i", ":", "i", "for", "i", "in", "range", "(", "len", "(", "label_list", ")", ")", "}", "\n", "", "else", ":", "\n", "            ", "label_list", "=", "get_label_list", "(", "datasets", "[", "\"train\"", "]", "[", "label_column_name", "]", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "label_list", "=", "[", "str", "(", "ele", ")", "for", "ele", "in", "label_list", "]", "# Ensure the ele is a string", "\n", "\n", "", "", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "config_name", "\n", "if", "model_args", ".", "config_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "finetuning_task", "=", "data_args", ".", "task_name", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "if", "config", ".", "model_type", "in", "{", "\"gpt2\"", ",", "\"roberta\"", "}", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "\n", "if", "model_args", ".", "tokenizer_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "True", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", "add_prefix_space", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "model_args", ".", "tokenizer_name", "\n", "if", "model_args", ".", "tokenizer_name", "\n", "else", "model_args", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "True", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "\n", "", "model", "=", "AutoModelForTokenClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "\n", "if", "model_args", ".", "added_special_separation_token", "not", "in", "tokenizer", ".", "special_tokens_map", ".", "values", "(", ")", ":", "\n", "        ", "tokenizer", ".", "add_special_tokens", "(", "{", "\"additional_special_tokens\"", ":", "[", "model_args", ".", "added_special_separation_token", "]", "}", ")", "\n", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "# In a previous version, we try to avoid resizing the token embeddings by directly ", "\n", "# modifying the unused tokens in vocab. However, this is not possible as not all tokenizer ", "\n", "# have such unused vocabs (e.g., Roberta). Ultimately, we still need to resize the token vocab.", "\n", "\n", "\n", "# Tokenizer check: this script requires a fast tokenizer.", "\n", "", "if", "not", "isinstance", "(", "tokenizer", ",", "PreTrainedTokenizerFast", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"", "\n", "\"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"", "\n", "\"requirement\"", "\n", ")", "\n", "\n", "", "logger", ".", "info", "(", "f\"The used agg level is {data_args.agg_level}\"", ")", "\n", "data_args", ".", "added_special_separation_token", "=", "model_args", ".", "added_special_separation_token", "\n", "preprocessor", "=", "instantiate_dataset_preprocessor", "(", "\n", "\"layout_indicator\"", ",", "tokenizer", ",", "data_args", "\n", ")", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "train_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_val_samples", ")", ")", "\n", "", "eval_dataset", "=", "eval_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "eval_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "\"test\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_predict requires a test dataset\"", ")", "\n", "", "test_dataset", "=", "datasets", "[", "\"test\"", "]", "\n", "if", "data_args", ".", "max_test_samples", "is", "not", "None", ":", "\n", "            ", "test_dataset", "=", "test_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_test_samples", ")", ")", "\n", "", "test_dataset", "=", "test_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_sample", ",", "\n", "batched", "=", "False", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Data collator", "\n", "", "data_collator", "=", "DataCollatorForTokenClassification", "(", "\n", "tokenizer", ",", "pad_to_multiple_of", "=", "8", "if", "training_args", ".", "fp16", "else", "None", "\n", ")", "\n", "\n", "def", "compute_metrics", "(", "p", ")", ":", "\n", "        ", "predictions", ",", "labels", "=", "p", "\n", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "# Remove ignored index (special tokens)", "\n", "true_predictions", "=", "[", "\n", "[", "label_list", "[", "p", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "true_labels", "=", "[", "\n", "[", "label_list", "[", "l", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "true_labels", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_labels", ")", ")", "\n", "results", "=", "classification_report", "(", "y_true", "=", "true_labels", ",", "y_pred", "=", "true_predictions", ")", "\n", "results", ".", "pop", "(", "\"detailed\"", ")", "\n", "return", "results", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "compute_metrics", "=", "compute_metrics", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "not", "model_args", ".", "not_resume_training", ":", "\n", "            ", "if", "last_checkpoint", "is", "not", "None", ":", "\n", "                ", "checkpoint", "=", "last_checkpoint", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", ":", "\n", "                ", "checkpoint", "=", "model_args", ".", "model_name_or_path", "\n", "", "else", ":", "\n", "                ", "checkpoint", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "checkpoint", "=", "None", "\n", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "\n", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_val_samples", "=", "(", "\n", "data_args", ".", "max_val_samples", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", "\n", "else", "len", "(", "eval_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_val_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Predict", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "\n", "output_test_predictions_file", "=", "os", ".", "path", ".", "join", "(", "\n", "training_args", ".", "output_dir", ",", "data_args", ".", "pred_file_name", "\n", ")", "\n", "logger", ".", "info", "(", "f\"The test file will be saved to {output_test_predictions_file}\"", ")", "\n", "\n", "_used_cols", "=", "columns_used_in_model_inputs", "(", "test_dataset", ",", "model", ")", "\n", "_used_cols", "=", "[", "ele", "for", "ele", "in", "_used_cols", "if", "\"label\"", "not", "in", "ele", "]", "\n", "\n", "all_pred_df", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "idx", ",", "sample", "in", "enumerate", "(", "tqdm", "(", "test_dataset", ")", ")", ":", "\n", "                ", "_sample", "=", "{", "\n", "key", ":", "torch", ".", "tensor", "(", "val", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "model", ".", "device", ")", "\n", "for", "key", ",", "val", "in", "sample", ".", "items", "(", ")", "\n", "if", "key", "in", "_used_cols", "\n", "}", "\n", "model_outputs", "=", "trainer", ".", "model", "(", "**", "_sample", ")", "\n", "predictions", "=", "model_outputs", ".", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "sample", "[", "\"labels\"", "]", "\n", "\n", "true_predictions", "=", "[", "\n", "[", "(", "p", ",", "l", ")", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "words", "=", "[", "sample", "[", "\"words\"", "]", "[", "idx", "]", "for", "idx", "in", "sample", "[", "\"encoded_word_ids\"", "]", "]", "\n", "block_ids", "=", "[", "\n", "sample", "[", "\"block_ids\"", "]", "[", "idx", "]", "for", "idx", "in", "sample", "[", "\"encoded_word_ids\"", "]", "\n", "]", "\n", "\n", "assert", "len", "(", "true_predictions", ")", "==", "len", "(", "words", ")", "\n", "df", "=", "pd", ".", "DataFrame", "(", "true_predictions", ",", "columns", "=", "[", "\"pred\"", ",", "\"gt\"", "]", ")", "\n", "df", "[", "\"word\"", "]", "=", "words", "\n", "df", "[", "\"block_id\"", "]", "=", "block_ids", "\n", "df", "[", "\"word_id\"", "]", "=", "sample", "[", "\"encoded_word_ids\"", "]", "\n", "df", "[", "\"sample_id\"", "]", "=", "idx", "\n", "all_pred_df", ".", "append", "(", "df", ")", "\n", "\n", "", "pd", ".", "concat", "(", "all_pred_df", ")", ".", "to_csv", "(", "output_test_predictions_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-ivila._mp_fn": [[408, 411], ["train-ivila.main"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila.main"], ["", "", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.dataset_configs.instiantiate_dataset": [[16, 21], ["dataset_name.lower", "dataset_configs.GROTOAP2", "dataset_name.lower", "dataset_configs.DocBank"], "function", ["None"], ["", "def", "instiantiate_dataset", "(", "dataset_name", ")", ":", "\n", "    ", "if", "dataset_name", ".", "lower", "(", ")", "==", "GROTOAP2", ".", "_name", ":", "\n", "        ", "return", "GROTOAP2", "(", ")", "\n", "", "elif", "dataset_name", ".", "lower", "(", ")", "==", "DocBank", ".", "_name", ":", "\n", "        ", "return", "DocBank", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.DataCollatorForTokenClassification.__call__": [[106, 153], ["utils.DataCollatorForTokenClassification.tokenizer.pad", "torch.tensor", "features[].keys", "features[].keys", "torch.tensor", "utils.DataCollatorForTokenClassification.items", "len", "len", "len", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.DataCollatorForSequenceClassification.__call__": [[187, 224], ["utils.DataCollatorForSequenceClassification.tokenizer.pad", "torch.tensor", "features[].keys", "features[].keys", "torch.tensor", "utils.DataCollatorForSequenceClassification.items", "len", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.load_json": [[17, 20], ["open", "json.load"], "function", ["None"], ["x2", "=", "max", "(", "x2", ",", "bbox", "[", "2", "]", ")", "\n", "y2", "=", "max", "(", "y2", ",", "bbox", "[", "3", "]", ")", "\n", "", "return", "[", "int", "(", "x1", ")", ",", "int", "(", "y1", ")", ",", "int", "(", "x2", ")", ",", "int", "(", "y2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.write_json": [[22, 25], ["open", "json.dump"], "function", ["None"], ["", "def", "union_lp_box", "(", "blocks", ":", "List", "[", "lp", ".", "TextBlock", "]", ")", "->", "List", ":", "\n", "\n", "    ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"inf\"", ")", ",", "float", "(", "\"-inf\"", ")", ",", "float", "(", "\"-inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box": [[27, 39], ["len", "logging.warning", "float", "float", "float", "float", "min", "min", "max", "max", "int", "int", "int", "int"], "function", ["None"], ["        ", "_x1", ",", "_y1", ",", "_x2", ",", "_y2", "=", "bbox", ".", "coordinates", "\n", "x1", "=", "min", "(", "x1", ",", "_x1", ")", "\n", "y1", "=", "min", "(", "y1", ",", "_y1", ")", "\n", "x2", "=", "max", "(", "x2", ",", "_x2", ")", "\n", "y2", "=", "max", "(", "y2", ",", "_y2", ")", "\n", "\n", "", "return", "lp", ".", "TextBlock", "(", "lp", ".", "Rectangle", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ")", ")", "\n", "\n", "\n", "", "def", "is_in", "(", "block_a", ",", "block_b", ")", ":", "\n", "    ", "\"\"\"A rewrite of the lp.LayoutElement.is_in function.\n    We will use a soft_margin and center function by default.\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.columns_used_in_model_inputs": [[41, 46], ["inspect.signature", "list", "inspect.signature.parameters.keys"], "function", ["None"], ["block_b", ",", "\n", "soft_margin", "=", "{", "\"top\"", ":", "1", ",", "\"bottom\"", ":", "1", ",", "\"left\"", ":", "1", ",", "\"right\"", ":", "1", "}", ",", "\n", "center", "=", "True", ",", "\n", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.classification_report": [[48, 71], ["isinstance", "isinstance", "numpy.unique", "sklearn.metrics.precision_recall_fscore_support", "pandas.DataFrame", "pd.DataFrame.mean", "numpy.array", "numpy.array", "pd.DataFrame.to_dict", "list"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict"], ["blocks", ":", "List", ",", "tokens", ":", "List", ",", "keep_empty_blocks", "=", "False", "\n", ")", "->", "Tuple", "[", "List", ",", "List", "]", ":", "\n", "    ", "\"\"\"It will assign the token to the corresponding blocks,\n    sort the blocks based on the token ids (the blocks are\n    ordered based on the minimum id of the contained tokens),\n    and then assign the correspinding block_id to the tokens \n    within the block. \n    \n    It will return a Tuple for blocks (in correct order) and \n    tokens (with block_id assigned and in the original order).\n    \"\"\"", "\n", "blocks", "=", "deepcopy", "(", "blocks", ")", "\n", "tokens", "=", "deepcopy", "(", "tokens", ")", "\n", "left_tokens_last", "=", "tokens", "\n", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "token", ".", "id", "=", "idx", "\n", "\n", "", "all_token_groups", "=", "[", "]", "\n", "\n", "for", "block", "in", "blocks", ":", "\n", "        ", "token_group", "=", "[", "]", "\n", "remaining_tokens", "=", "[", "]", "\n", "for", "token", "in", "left_tokens_last", ":", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila.flatten_line_level_prediction": [[52, 60], ["zip", "list", "zip", "itertools.chain.from_iterable", "len", "len", "final_flattend_pred.append"], "function", ["None"], ["def", "flatten_line_level_prediction", "(", "batched_line_pred", ",", "batched_line_word_count", ")", ":", "\n", "    ", "final_flattend_pred", "=", "[", "]", "\n", "for", "line_pred", ",", "line_word_count", "in", "zip", "(", "batched_line_pred", ",", "batched_line_word_count", ")", ":", "\n", "        ", "assert", "len", "(", "line_pred", ")", "==", "len", "(", "line_word_count", ")", "\n", "for", "(", "pred", ",", "label", ")", ",", "(", "line_id", ",", "count", ")", "in", "zip", "(", "line_pred", ",", "line_word_count", ")", ":", "\n", "            ", "final_flattend_pred", ".", "append", "(", "[", "[", "pred", ",", "label", ",", "line_id", "]", "]", "*", "count", ")", "\n", "\n", "", "", "return", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "final_flattend_pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila.main": [[62, 388], ["transformers.HfArgumentParser", "logging.basicConfig", "logger.setLevel", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "len", "vila.models.configuration_hierarchical_model.HierarchicalModelConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.from_pretrained", "logger.info", "vila.dataset.preprocessors.instantiate_dataset_preprocessor", "transformers.Trainer", "sys.argv[].endswith", "transformers.HfArgumentParser.parse_json_file", "transformers.HfArgumentParser.parse_args_into_dataclasses", "os.path.isdir", "transformers.trainer_utils.get_last_checkpoint", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "datasets.load_dataset", "datasets.load_dataset", "set", "list", "get_label_list.sort", "list", "isinstance", "isinstance", "ValueError", "train_dataset.select.map", "eval_dataset.select.map", "test_dataset.select.map", "numpy.argmax", "list", "list", "utils.classification_report", "utils.classification_report.pop", "transformers.Trainer.train", "transformers.Trainer.save_model", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "transformers.Trainer.save_state", "logger.info", "transformers.Trainer.evaluate", "min", "transformers.Trainer.log_metrics", "transformers.Trainer.save_metrics", "logger.info", "os.path.join", "logger.info", "utils.columns_used_in_model_inputs", "len", "ValueError", "transformers.trainer_utils.is_main_process", "data_args.validation_file.split", "utils.load_json().values", "train-hvila.main.get_label_list"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.__init__.instantiate_dataset_preprocessor", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.classification_report", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.columns_used_in_model_inputs"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "parser", "=", "HfArgumentParser", "(", "\n", "(", "ModelArguments", ",", "DataTrainingArguments", ",", "TrainingArguments", ")", "\n", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "2", "and", "sys", ".", "argv", "[", "1", "]", ".", "endswith", "(", "\".json\"", ")", ":", "\n", "# If we pass only one argument to the script and it's the path to a json file,", "\n", "# let's parse it to get our arguments.", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_json_file", "(", "\n", "json_file", "=", "os", ".", "path", ".", "abspath", "(", "sys", ".", "argv", "[", "1", "]", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "model_args", ",", "data_args", ",", "training_args", "=", "parser", ".", "parse_args_into_dataclasses", "(", ")", "\n", "\n", "# Detecting last checkpoint.", "\n", "", "last_checkpoint", "=", "None", "\n", "if", "(", "\n", "os", ".", "path", ".", "isdir", "(", "training_args", ".", "output_dir", ")", "\n", "and", "training_args", ".", "do_train", "\n", "and", "not", "training_args", ".", "overwrite_output_dir", "\n", ")", ":", "\n", "        ", "last_checkpoint", "=", "get_last_checkpoint", "(", "training_args", ".", "output_dir", ")", "\n", "if", "last_checkpoint", "is", "None", "and", "len", "(", "os", ".", "listdir", "(", "training_args", ".", "output_dir", ")", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"", "\n", "\"Use --overwrite_output_dir to overcome.\"", "\n", ")", "\n", "", "elif", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"", "\n", "\"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"", "\n", ")", "\n", "\n", "# Setup logging", "\n", "", "", "logging", ".", "basicConfig", "(", "\n", "filename", "=", "\"./log.txt\"", ",", "\n", "filemode", "=", "\"a\"", ",", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "# handlers=[logging.StreamHandler(sys.stdout)],", "\n", ")", "\n", "logger", ".", "setLevel", "(", "\n", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", "\n", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)", "\n", "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/", "\n", "# (the dataset will be downloaded automatically from the datasets Hub).", "\n", "#", "\n", "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called", "\n", "# 'text' is found. You can easily tweak this behavior (see below).", "\n", "#", "\n", "# In distributed training, the load_dataset function guarantee that only one local process can concurrently", "\n", "# download the dataset.", "\n", "if", "data_args", ".", "dataset_name", "is", "not", "None", ":", "\n", "# Downloading and loading a dataset from the hub.", "\n", "        ", "datasets", "=", "load_dataset", "(", "data_args", ".", "dataset_name", ",", "data_args", ".", "dataset_config_name", ")", "\n", "", "else", ":", "\n", "        ", "data_files", "=", "{", "}", "\n", "if", "data_args", ".", "train_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"train\"", "]", "=", "data_args", ".", "train_file", "\n", "", "if", "data_args", ".", "validation_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"validation\"", "]", "=", "data_args", ".", "validation_file", "\n", "", "if", "data_args", ".", "test_file", "is", "not", "None", ":", "\n", "            ", "data_files", "[", "\"test\"", "]", "=", "data_args", ".", "test_file", "\n", "", "extension", "=", "data_args", ".", "validation_file", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", "\n", "datasets", "=", "load_dataset", "(", "extension", ",", "data_files", "=", "data_files", ",", "field", "=", "\"data\"", ")", "\n", "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at", "\n", "# https://huggingface.co/docs/datasets/loading_datasets.html.", "\n", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"train\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"train\"", "]", ".", "features", "\n", "", "else", ":", "\n", "        ", "column_names", "=", "datasets", "[", "\"validation\"", "]", ".", "column_names", "\n", "features", "=", "datasets", "[", "\"validation\"", "]", ".", "features", "\n", "", "text_column_name", "=", "\"words\"", "\n", "label_column_name", "=", "\"labels\"", "\n", "\n", "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the", "\n", "# unique labels.", "\n", "def", "get_label_list", "(", "labels", ")", ":", "\n", "        ", "unique_labels", "=", "set", "(", ")", "\n", "for", "label", "in", "labels", ":", "\n", "            ", "unique_labels", "=", "unique_labels", "|", "set", "(", "label", ")", "\n", "", "label_list", "=", "list", "(", "unique_labels", ")", "\n", "label_list", ".", "sort", "(", ")", "\n", "return", "label_list", "\n", "\n", "", "if", "data_args", ".", "label_map_file", "is", "not", "None", ":", "\n", "        ", "label_list", "=", "list", "(", "load_json", "(", "data_args", ".", "label_map_file", ")", ".", "values", "(", ")", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "features", "[", "label_column_name", "]", ".", "feature", ",", "ClassLabel", ")", ":", "\n", "            ", "label_list", "=", "features", "[", "label_column_name", "]", ".", "feature", ".", "names", "\n", "# No need to convert the labels since they are already ints.", "\n", "label_to_id", "=", "{", "i", ":", "i", "for", "i", "in", "range", "(", "len", "(", "label_list", ")", ")", "}", "\n", "", "else", ":", "\n", "            ", "label_list", "=", "get_label_list", "(", "datasets", "[", "\"train\"", "]", "[", "label_column_name", "]", ")", "\n", "label_to_id", "=", "{", "l", ":", "i", "for", "i", ",", "l", "in", "enumerate", "(", "label_list", ")", "}", "\n", "label_list", "=", "[", "str", "(", "ele", ")", "for", "ele", "in", "label_list", "]", "# Ensure the ele is a string", "\n", "\n", "", "", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "#", "\n", "# Distributed training:", "\n", "# The .from_pretrained methods guarantee that only one local process can concurrently", "\n", "# download model & vocab.", "\n", "config", "=", "HierarchicalModelConfig", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "textline_encoder_output", "=", "model_args", ".", "textline_encoder_output", ",", "\n", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "use_fast", "=", "True", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "model", "=", "HierarchicalModelForTokenClassification", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "config", "=", "config", ",", "\n", ")", "\n", "\n", "# Tokenizer check: this script requires a fast tokenizer.", "\n", "if", "not", "isinstance", "(", "tokenizer", ",", "PreTrainedTokenizerFast", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"", "\n", "\"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"", "\n", "\"requirement\"", "\n", ")", "\n", "\n", "", "logger", ".", "info", "(", "f\"The used agg level is {data_args.agg_level}\"", ")", "\n", "preprocessor", "=", "instantiate_dataset_preprocessor", "(", "\n", "\"hierarchical_modeling\"", ",", "tokenizer", ",", "data_args", "\n", ")", "\n", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "\"train\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_train requires a train dataset\"", ")", "\n", "", "train_dataset", "=", "datasets", "[", "\"train\"", "]", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", ":", "\n", "            ", "train_dataset", "=", "train_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_train_samples", ")", ")", "\n", "", "train_dataset", "=", "train_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "train_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "if", "\"validation\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_eval requires a validation dataset\"", ")", "\n", "", "eval_dataset", "=", "datasets", "[", "\"validation\"", "]", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", ":", "\n", "            ", "eval_dataset", "=", "eval_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_val_samples", ")", ")", "\n", "", "eval_dataset", "=", "eval_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "remove_columns", "=", "eval_dataset", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "if", "\"test\"", "not", "in", "datasets", ":", "\n", "            ", "raise", "ValueError", "(", "\"--do_predict requires a test dataset\"", ")", "\n", "", "test_dataset", "=", "datasets", "[", "\"test\"", "]", "\n", "if", "data_args", ".", "max_test_samples", "is", "not", "None", ":", "\n", "            ", "test_dataset", "=", "test_dataset", ".", "select", "(", "range", "(", "data_args", ".", "max_test_samples", ")", ")", "\n", "", "test_dataset", "=", "test_dataset", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_sample", ",", "\n", "batched", "=", "False", ",", "\n", "num_proc", "=", "data_args", ".", "preprocessing_num_workers", ",", "\n", "load_from_cache_file", "=", "not", "data_args", ".", "overwrite_cache", ",", "\n", ")", "\n", "\n", "# Data collator", "\n", "", "def", "data_collator", "(", "batch", ")", ":", "\n", "        ", "condensed_batch", "=", "{", "\n", "key", ":", "torch", ".", "tensor", "(", "[", "ele", "[", "key", "]", "for", "ele", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "for", "key", "in", "batch", "[", "0", "]", ".", "keys", "(", ")", "\n", "}", "\n", "assert", "not", "(", "\n", "condensed_batch", "[", "\"group_level_attention_mask\"", "]", "[", "\n", "condensed_batch", "[", "\"labels\"", "]", "==", "-", "100", "\n", "]", "\n", ">", "0", "\n", ")", ".", "any", "(", ")", "\n", "return", "condensed_batch", "\n", "\n", "", "def", "compute_metrics", "(", "p", ")", ":", "\n", "        ", "predictions", ",", "labels", "=", "p", "\n", "predictions", "=", "np", ".", "argmax", "(", "predictions", ",", "axis", "=", "2", ")", "\n", "\n", "# Remove ignored index (special tokens)", "\n", "true_predictions", "=", "[", "\n", "[", "label_list", "[", "p", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "true_labels", "=", "[", "\n", "[", "label_list", "[", "l", "]", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "\n", "true_predictions", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_predictions", ")", ")", "\n", "true_labels", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "true_labels", ")", ")", "\n", "results", "=", "classification_report", "(", "y_true", "=", "true_labels", ",", "y_pred", "=", "true_predictions", ")", "\n", "results", ".", "pop", "(", "\"detailed\"", ")", "\n", "return", "results", "\n", "\n", "# Initialize our Trainer", "\n", "", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "train_dataset", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "eval_dataset", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", "compute_metrics", "=", "compute_metrics", ",", "\n", ")", "\n", "\n", "# Training", "\n", "if", "training_args", ".", "do_train", ":", "\n", "        ", "if", "last_checkpoint", "is", "not", "None", ":", "\n", "            ", "checkpoint", "=", "last_checkpoint", "\n", "# Disable this behavior as we are re-initializing the classifier weights", "\n", "# elif os.path.isdir(model_args.model_name_or_path):", "\n", "#     checkpoint = model_args.model_name_or_path", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "None", "\n", "", "train_result", "=", "trainer", ".", "train", "(", "resume_from_checkpoint", "=", "checkpoint", ")", "\n", "metrics", "=", "train_result", ".", "metrics", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "max_train_samples", "=", "(", "\n", "data_args", ".", "max_train_samples", "\n", "if", "data_args", ".", "max_train_samples", "is", "not", "None", "\n", "else", "len", "(", "train_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"train_samples\"", "]", "=", "min", "(", "max_train_samples", ",", "len", "(", "train_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"train\"", ",", "metrics", ")", "\n", "trainer", ".", "save_state", "(", ")", "\n", "\n", "# Evaluation", "\n", "", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "metrics", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "max_val_samples", "=", "(", "\n", "data_args", ".", "max_val_samples", "\n", "if", "data_args", ".", "max_val_samples", "is", "not", "None", "\n", "else", "len", "(", "eval_dataset", ")", "\n", ")", "\n", "metrics", "[", "\"eval_samples\"", "]", "=", "min", "(", "max_val_samples", ",", "len", "(", "eval_dataset", ")", ")", "\n", "\n", "trainer", ".", "log_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "trainer", ".", "save_metrics", "(", "\"eval\"", ",", "metrics", ")", "\n", "\n", "# Predict", "\n", "", "if", "training_args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Predict ***\"", ")", "\n", "\n", "output_test_predictions_file", "=", "os", ".", "path", ".", "join", "(", "\n", "training_args", ".", "output_dir", ",", "data_args", ".", "pred_file_name", "\n", ")", "\n", "logger", ".", "info", "(", "f\"The test file will be saved to {output_test_predictions_file}\"", ")", "\n", "\n", "_used_cols", "=", "columns_used_in_model_inputs", "(", "test_dataset", ",", "model", ")", "\n", "_used_cols", "=", "[", "ele", "for", "ele", "in", "_used_cols", "if", "\"label\"", "not", "in", "ele", "]", "\n", "\n", "all_pred_df", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "idx", ",", "sample", "in", "enumerate", "(", "tqdm", "(", "test_dataset", ")", ")", ":", "\n", "                ", "_sample", "=", "{", "\n", "key", ":", "torch", ".", "tensor", "(", "val", ",", "dtype", "=", "torch", ".", "int64", ",", "device", "=", "model", ".", "device", ")", "\n", "for", "key", ",", "val", "in", "sample", ".", "items", "(", ")", "\n", "if", "key", "in", "_used_cols", "\n", "}", "\n", "model_outputs", "=", "model", "(", "**", "_sample", ")", "\n", "predictions", "=", "model_outputs", ".", "logits", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "sample", "[", "\"labels\"", "]", "\n", "\n", "true_predictions", "=", "[", "\n", "[", "(", "p", ",", "l", ")", "for", "(", "p", ",", "l", ")", "in", "zip", "(", "prediction", ",", "label", ")", "if", "l", "!=", "-", "100", "]", "\n", "for", "prediction", ",", "label", "in", "zip", "(", "predictions", ",", "labels", ")", "\n", "]", "\n", "flatten_predictions", "=", "flatten_line_level_prediction", "(", "\n", "true_predictions", ",", "sample", "[", "\"group_word_count\"", "]", "\n", ")", "\n", "assert", "len", "(", "flatten_predictions", ")", "==", "len", "(", "sample", "[", "\"words\"", "]", ")", "\n", "df", "=", "pd", ".", "DataFrame", "(", "\n", "flatten_predictions", ",", "columns", "=", "[", "\"pred\"", ",", "\"gt\"", ",", "\"line_id\"", "]", "\n", ")", "\n", "df", "[", "\"word\"", "]", "=", "sample", "[", "\"words\"", "]", "\n", "df", "[", "\"block_id\"", "]", "=", "sample", "[", "\"block_ids\"", "]", "\n", "df", "[", "\"word_id\"", "]", "=", "df", ".", "index", "\n", "df", "[", "\"sample_id\"", "]", "=", "idx", "\n", "all_pred_df", ".", "append", "(", "df", ")", "\n", "\n", "", "pd", ".", "concat", "(", "all_pred_df", ")", ".", "to_csv", "(", "output_test_predictions_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila._mp_fn": [[390, 393], ["train-hvila.main"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.train-hvila.main"], ["", "", "", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_large_pdf.test_large_pdf": [[5, 17], ["vila.pdftools.pdf_extractor.PDFExtractor", "vila.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "layoutparser.EfficientDetLayoutModel", "vila.predictors.LayoutIndicatorPDFPredictor.from_pretrained", "enumerate", "lp.EfficientDetLayoutModel.detect", "page_token.annotate", "page_token.to_pagedata().to_dict", "LayoutIndicatorPDFPredictor.from_pretrained.predict", "page_token.to_pagedata"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata"], ["def", "test_large_pdf", "(", ")", ":", "\n", "    ", "pdf_extractor", "=", "PDFExtractor", "(", "\"pdfplumber\"", ")", "\n", "page_tokens", ",", "page_images", "=", "pdf_extractor", ".", "load_tokens_and_image", "(", "f\"tests/fixtures/large.pdf\"", ")", "\n", "\n", "vision_model", "=", "lp", ".", "EfficientDetLayoutModel", "(", "\"lp://PubLayNet\"", ")", "\n", "pdf_predictor", "=", "LayoutIndicatorPDFPredictor", ".", "from_pretrained", "(", "\"allenai/ivila-block-layoutlm-finetuned-docbank\"", ")", "\n", "\n", "for", "idx", ",", "page_token", "in", "enumerate", "(", "page_tokens", "[", ":", "1", "]", ")", ":", "\n", "        ", "blocks", "=", "vision_model", ".", "detect", "(", "page_images", "[", "idx", "]", ")", "\n", "page_token", ".", "annotate", "(", "blocks", "=", "blocks", ")", "\n", "pdf_data", "=", "page_token", ".", "to_pagedata", "(", ")", ".", "to_dict", "(", ")", "\n", "predicted_tokens", "=", "pdf_predictor", ".", "predict", "(", "pdf_data", ",", "page_token", ".", "page_size", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_sentence_indicator_processor": [[59, 84], ["vila.dataset.preprocessors.layout_indicator.SentenceLayoutIndicatorPDFDataPreprocessor", "dummy_sample.map", "pysbd.Segmenter", "vila.dataset.preprocessors.layout_indicator.union_box", "tokenizer", "pysbd.Segmenter.segment"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["def", "test_sentence_indicator_processor", "(", ")", ":", "\n", "\n", "    ", "preprocessor", "=", "SentenceLayoutIndicatorPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "Segmenter", "=", "pysbd", ".", "Segmenter", "(", "language", "=", "\"en\"", ",", "clean", "=", "False", ",", "char_span", "=", "True", ")", "\n", "combined_words", "=", "\" \"", ".", "join", "(", "dummy_sample", "[", "\"words\"", "]", "[", "0", "]", ")", "\n", "encoded_sentence", "=", "\"[SEP] \"", ".", "join", "(", "\n", "ele", ".", "sent", "for", "ele", "in", "Segmenter", ".", "segment", "(", "combined_words", ")", "\n", ")", "\n", "\n", "assert", "(", "\n", "tokenizer", "(", "encoded_sentence", ")", "[", "\"input_ids\"", "]", "\n", "==", "processed_version", "[", "0", "]", "[", "\"input_ids\"", "]", "[", ":", "9", "]", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", ":", "9", "]", "==", "[", "101", ",", "3231", ",", "1996", ",", "4326", ",", "1012", ",", "102", ",", "4119", ",", "6429", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "[", "0", "]", "[", ":", "9", "]", "==", "[", "-", "100", ",", "0", ",", "1", ",", "1", ",", "1", ",", "-", "100", ",", "1", ",", "1", ",", "-", "100", "]", "\n", "assert", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "5", "]", "==", "union_box", "(", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", ":", "5", "]", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_block_indicator_processor": [[87, 101], ["vila.dataset.preprocessors.layout_indicator.BlockLayoutIndicatorPDFDataPreprocessor", "dummy_sample.map", "vila.dataset.preprocessors.layout_indicator.union_box"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "test_block_indicator_processor", "(", ")", ":", "\n", "\n", "    ", "preprocessor", "=", "BlockLayoutIndicatorPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", ":", "9", "]", "==", "[", "101", ",", "3231", ",", "1996", ",", "4326", ",", "1012", ",", "4119", ",", "102", ",", "6429", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "[", "0", "]", "[", ":", "9", "]", "==", "[", "-", "100", ",", "0", ",", "1", ",", "1", ",", "1", ",", "1", ",", "-", "100", ",", "1", ",", "-", "100", "]", "\n", "assert", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "6", "]", "==", "union_box", "(", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", ":", "6", "]", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_row_indicator_processor": [[104, 118], ["vila.dataset.preprocessors.layout_indicator.RowLayoutIndicatorPDFDataPreprocessor", "dummy_sample.map", "vila.dataset.preprocessors.layout_indicator.union_box", "vila.dataset.preprocessors.layout_indicator.union_box"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "test_row_indicator_processor", "(", ")", ":", "\n", "    ", "preprocessor", "=", "RowLayoutIndicatorPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", ":", "10", "]", "==", "[", "101", ",", "3231", ",", "102", ",", "1996", ",", "4326", ",", "1012", ",", "102", ",", "4119", ",", "6429", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "[", "0", "]", "[", ":", "10", "]", "==", "[", "-", "100", ",", "0", ",", "-", "100", ",", "1", ",", "1", ",", "1", ",", "-", "100", ",", "1", ",", "1", ",", "-", "100", "]", "\n", "assert", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "2", "]", "==", "union_box", "(", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "1", ":", "2", "]", ")", "\n", "assert", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "6", "]", "==", "union_box", "(", "processed_version", "[", "\"bbox\"", "]", "[", "0", "]", "[", "3", ":", "6", "]", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_row_level_hierarchical_processor": [[121, 142], ["vila.dataset.preprocessors.hierarchical_modeling.RowLevelHierarchicalPDFDataPreprocessor", "dummy_sample.map", "len", "len", "vila.dataset.preprocessors.layout_indicator.union_box", "vila.dataset.preprocessors.layout_indicator.union_box", "vila.dataset.preprocessors.layout_indicator.union_box"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "test_row_level_hierarchical_processor", "(", ")", ":", "\n", "    ", "preprocessor", "=", "RowLevelHierarchicalPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", ")", "==", "MAX_LINE_PER_PAGE", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "0", "]", ")", "==", "MAX_TOKENS_PER_LINE", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "0", "]", "[", ":", "3", "]", "==", "[", "101", ",", "3231", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "1", "]", "[", ":", "5", "]", "==", "[", "101", ",", "1996", ",", "4326", ",", "1012", ",", "102", "]", "\n", "assert", "processed_version", "[", "'input_ids'", "]", "[", "0", "]", "[", "2", "]", "[", ":", "4", "]", "==", "[", "101", ",", "4119", ",", "6429", ",", "102", "]", "\n", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "0", "]", "==", "union_box", "(", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", ":", "1", "]", ")", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", "]", "==", "union_box", "(", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "1", ":", "4", "]", ")", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "2", "]", "==", "union_box", "(", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "4", ":", "6", "]", ")", "\n", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "[", "0", "]", "[", ":", "3", "]", "==", "[", "0", ",", "1", ",", "1", "]", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_block_level_hierarchical_processor": [[145, 164], ["vila.dataset.preprocessors.hierarchical_modeling.BlockLevelHierarchicalPDFDataPreprocessor", "dummy_sample.map", "len", "len", "vila.dataset.preprocessors.layout_indicator.union_box", "vila.dataset.preprocessors.layout_indicator.union_box"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box", "home.repos.pwc.inspect_result.allenai_VILA.tools.utils.union_box"], ["", "def", "test_block_level_hierarchical_processor", "(", ")", ":", "\n", "    ", "preprocessor", "=", "BlockLevelHierarchicalPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", ")", "==", "MAX_BLOCK_PER_PAGE", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "0", "]", ")", "==", "MAX_TOKENS_PER_BLOCK", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "0", "]", "[", ":", "7", "]", "==", "[", "101", ",", "3231", ",", "1996", ",", "4326", ",", "1012", ",", "4119", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "[", "1", "]", "[", ":", "3", "]", "==", "[", "101", ",", "6429", ",", "102", "]", "\n", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "0", "]", "==", "union_box", "(", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", ":", "5", "]", ")", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", "]", "==", "union_box", "(", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "5", ":", "6", "]", ")", "\n", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "[", "0", "]", "[", ":", "2", "]", "==", "[", "1", ",", "1", "]", "# The 1st block should be 1 due to the majority voting strategy", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_row_level_grouping_processor": [[167, 187], ["vila.dataset.preprocessors.grouping.RowGroupingPDFDataPreprocessor", "dummy_sample.map", "len"], "function", ["None"], ["", "def", "test_row_level_grouping_processor", "(", ")", ":", "\n", "    ", "preprocessor", "=", "RowGroupingPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", ")", "==", "3", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "==", "[", "101", ",", "3231", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "1", "]", "==", "[", "101", ",", "1996", ",", "4326", ",", "1012", ",", "102", "]", "\n", "assert", "processed_version", "[", "'input_ids'", "]", "[", "2", "]", "==", "[", "101", ",", "4119", ",", "6429", ",", "102", "]", "\n", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", ":", "-", "1", "]", "==", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", ":", "1", "]", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "1", "]", "[", "1", ":", "-", "1", "]", "==", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "1", ":", "4", "]", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "2", "]", "[", "1", ":", "-", "1", "]", "==", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "4", ":", "6", "]", "\n", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "==", "[", "0", ",", "1", ",", "1", "]", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_preprocessor.test_block_level_grouping_processor": [[190, 208], ["vila.dataset.preprocessors.grouping.BlockGroupingPDFDataPreprocessor", "dummy_sample.map", "len"], "function", ["None"], ["", "def", "test_block_level_grouping_processor", "(", ")", ":", "\n", "    ", "preprocessor", "=", "BlockGroupingPDFDataPreprocessor", "(", "tokenizer", ",", "config", ")", "\n", "processed_version", "=", "dummy_sample", ".", "map", "(", "\n", "preprocessor", ".", "preprocess_batch", ",", "\n", "batched", "=", "True", ",", "\n", "remove_columns", "=", "dummy_sample", ".", "column_names", ",", "\n", "load_from_cache_file", "=", "False", ",", "\n", ")", "\n", "\n", "# fmt: off", "\n", "assert", "len", "(", "processed_version", "[", "\"input_ids\"", "]", ")", "==", "2", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "0", "]", "==", "[", "101", ",", "3231", ",", "1996", ",", "4326", ",", "1012", ",", "4119", ",", "102", "]", "\n", "assert", "processed_version", "[", "\"input_ids\"", "]", "[", "1", "]", "==", "[", "101", ",", "6429", ",", "102", "]", "\n", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "0", "]", "[", "1", ":", "-", "1", "]", "==", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", ":", "5", "]", "\n", "assert", "processed_version", "[", "'bbox'", "]", "[", "1", "]", "[", "1", ":", "-", "1", "]", "==", "dummy_sample", "[", "0", "]", "[", "'bbox'", "]", "[", "5", ":", "6", "]", "\n", "\n", "assert", "processed_version", "[", "\"labels\"", "]", "==", "[", "1", ",", "1", "]", "\n", "# fmt: on", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_predictor.test_return_type": [[6, 25], ["vila.pdftools.pdf_extractor.PDFExtractor", "vila.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "layoutparser.EfficientDetLayoutModel", "vila.predictors.HierarchicalPDFPredictor.from_pretrained", "vila.predictors.LayoutIndicatorPDFPredictor.from_pretrained", "enumerate", "lp.EfficientDetLayoutModel.detect", "page_token.annotate", "page_token.to_pagedata().to_dict", "pdf_predictor.predict", "pdf_predictor.predict", "page_token.to_pagedata"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata"], ["def", "test_return_type", "(", ")", ":", "\n", "\n", "    ", "pdf_extractor", "=", "PDFExtractor", "(", "\"pdfplumber\"", ")", "\n", "page_tokens", ",", "page_images", "=", "pdf_extractor", ".", "load_tokens_and_image", "(", "f\"tests/fixtures/regular.pdf\"", ")", "\n", "\n", "vision_model", "=", "lp", ".", "EfficientDetLayoutModel", "(", "\"lp://PubLayNet\"", ")", "\n", "pdf_predictors", "=", "[", "\n", "HierarchicalPDFPredictor", ".", "from_pretrained", "(", "\"allenai/hvila-row-layoutlm-finetuned-docbank\"", ")", ",", "\n", "LayoutIndicatorPDFPredictor", ".", "from_pretrained", "(", "\"allenai/ivila-block-layoutlm-finetuned-docbank\"", ")", "\n", "]", "\n", "\n", "for", "pdf_predictor", "in", "pdf_predictors", ":", "\n", "        ", "for", "idx", ",", "page_token", "in", "enumerate", "(", "page_tokens", ")", ":", "\n", "            ", "blocks", "=", "vision_model", ".", "detect", "(", "page_images", "[", "idx", "]", ")", "\n", "page_token", ".", "annotate", "(", "blocks", "=", "blocks", ")", "\n", "pdf_data", "=", "page_token", ".", "to_pagedata", "(", ")", ".", "to_dict", "(", ")", "\n", "predicted_tokens", "=", "pdf_predictor", ".", "predict", "(", "pdf_data", ",", "page_token", ".", "page_size", ",", "return_type", "=", "\"layout\"", ")", "\n", "predicted_types", "=", "pdf_predictor", ".", "predict", "(", "pdf_data", ",", "page_token", ".", "page_size", ",", "return_type", "=", "\"list\"", ")", "\n", "assert", "predicted_types", "==", "[", "l", ".", "type", "for", "l", "in", "predicted_tokens", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_vila_run.test_hvila_run": [[7, 33], ["vila.pdftools.pdf_extractor.PDFExtractor", "vila.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "layoutparser.EfficientDetLayoutModel", "vila.predictors.HierarchicalPDFPredictor.from_pretrained", "enumerate", "HierarchicalPDFPredictor.from_pretrained.predict_page", "lp.EfficientDetLayoutModel.detect", "page_token.annotate", "page_token.to_pagedata().to_dict", "HierarchicalPDFPredictor.from_pretrained.predict", "page_token.to_pagedata"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict_page", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata"], ["def", "test_hvila_run", "(", ")", ":", "\n", "\n", "    ", "pdf_extractor", "=", "PDFExtractor", "(", "\"pdfplumber\"", ")", "\n", "page_tokens", ",", "page_images", "=", "pdf_extractor", ".", "load_tokens_and_image", "(", "\n", "f\"tests/fixtures/regular.pdf\"", "\n", ")", "\n", "\n", "vision_model", "=", "lp", ".", "EfficientDetLayoutModel", "(", "\"lp://PubLayNet\"", ")", "\n", "pdf_predictor", "=", "HierarchicalPDFPredictor", ".", "from_pretrained", "(", "\n", "\"allenai/hvila-row-layoutlm-finetuned-docbank\"", "\n", ")", "\n", "\n", "for", "idx", ",", "page_token", "in", "enumerate", "(", "page_tokens", ")", ":", "\n", "\n", "# Method 1", "\n", "        ", "predicted_tokens1", "=", "pdf_predictor", ".", "predict_page", "(", "\n", "page_token", ",", "page_image", "=", "page_images", "[", "idx", "]", ",", "visual_group_detector", "=", "vision_model", "\n", ")", "\n", "\n", "# Method 2", "\n", "blocks", "=", "vision_model", ".", "detect", "(", "page_images", "[", "idx", "]", ")", "\n", "page_token", ".", "annotate", "(", "blocks", "=", "blocks", ")", "\n", "pdf_data", "=", "page_token", ".", "to_pagedata", "(", ")", ".", "to_dict", "(", ")", "\n", "predicted_tokens2", "=", "pdf_predictor", ".", "predict", "(", "pdf_data", ",", "page_token", ".", "page_size", ")", "\n", "\n", "assert", "predicted_tokens1", "==", "predicted_tokens2", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_vila_run.test_ivila_run": [[35, 61], ["vila.pdftools.pdf_extractor.PDFExtractor", "vila.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "layoutparser.EfficientDetLayoutModel", "vila.predictors.LayoutIndicatorPDFPredictor.from_pretrained", "enumerate", "LayoutIndicatorPDFPredictor.from_pretrained.predict_page", "lp.EfficientDetLayoutModel.detect", "page_token.annotate", "page_token.to_pagedata().to_dict", "LayoutIndicatorPDFPredictor.from_pretrained.predict", "page_token.to_pagedata"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdf_extractor.PDFExtractor.load_tokens_and_image", "home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict_page", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.annotate", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.datamodel.PageData.to_dict", "home.repos.pwc.inspect_result.allenai_VILA.vila.predictors.BasePDFPredictor.predict", "home.repos.pwc.inspect_result.allenai_VILA.pdftools.pdfplumber_extractor.PDFPlumberPageData.to_pagedata"], ["", "", "def", "test_ivila_run", "(", ")", ":", "\n", "    ", "pdf_extractor", "=", "PDFExtractor", "(", "\"pdfplumber\"", ")", "\n", "page_tokens", ",", "page_images", "=", "pdf_extractor", ".", "load_tokens_and_image", "(", "\n", "f\"tests/fixtures/regular.pdf\"", "\n", ")", "\n", "\n", "vision_model", "=", "lp", ".", "EfficientDetLayoutModel", "(", "\"lp://PubLayNet\"", ")", "\n", "pdf_predictor", "=", "LayoutIndicatorPDFPredictor", ".", "from_pretrained", "(", "\n", "\"allenai/ivila-block-layoutlm-finetuned-docbank\"", "\n", ")", "\n", "\n", "for", "idx", ",", "page_token", "in", "enumerate", "(", "page_tokens", ")", ":", "\n", "\n", "# Method 1", "\n", "        ", "predicted_tokens1", "=", "pdf_predictor", ".", "predict_page", "(", "\n", "page_token", ",", "page_image", "=", "page_images", "[", "idx", "]", ",", "visual_group_detector", "=", "vision_model", "\n", ")", "\n", "assert", "page_token", ".", "blocks", "==", "[", "]", "\n", "\n", "# Method 2", "\n", "blocks", "=", "vision_model", ".", "detect", "(", "page_images", "[", "idx", "]", ")", "\n", "page_token", ".", "annotate", "(", "blocks", "=", "blocks", ")", "\n", "pdf_data", "=", "page_token", ".", "to_pagedata", "(", ")", ".", "to_dict", "(", ")", "\n", "predicted_tokens2", "=", "pdf_predictor", ".", "predict", "(", "pdf_data", ",", "page_token", ".", "page_size", ")", "\n", "\n", "assert", "predicted_tokens1", "==", "predicted_tokens2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models": [[14, 29], ["zip", "model_1.state_dict().items", "model_2.state_dict().items", "torch.equal", "model_1.state_dict", "model_2.state_dict", "print"], "function", ["None"], ["def", "compare_models", "(", "model_1", ",", "model_2", ")", ":", "\n", "# From https://discuss.pytorch.org/t/check-if-models-have-same-weights/4351/5", "\n", "    ", "models_differ", "=", "0", "\n", "for", "key_item_1", ",", "key_item_2", "in", "zip", "(", "\n", "model_1", ".", "state_dict", "(", ")", ".", "items", "(", ")", ",", "model_2", ".", "state_dict", "(", ")", ".", "items", "(", ")", "\n", ")", ":", "\n", "        ", "if", "torch", ".", "equal", "(", "key_item_1", "[", "1", "]", ",", "key_item_2", "[", "1", "]", ")", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "models_differ", "+=", "1", "\n", "if", "key_item_1", "[", "0", "]", "==", "key_item_2", "[", "0", "]", ":", "\n", "                ", "print", "(", "\"Mismtach found at\"", ",", "key_item_1", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "\n", "", "", "", "return", "models_differ", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.create_bbox": [[31, 36], ["torch.randint", "torch.randint", "torch.cat"], "function", ["None"], ["", "def", "create_bbox", "(", "bz", ",", "sequence_length", ")", ":", "\n", "    ", "_bbox", "=", "torch", ".", "randint", "(", "0", ",", "100", ",", "size", "=", "(", "bz", ",", "sequence_length", ",", "2", ")", ")", "\n", "bbox_", "=", "torch", ".", "randint", "(", "100", ",", "200", ",", "size", "=", "(", "bz", ",", "sequence_length", ",", "2", ")", ")", "\n", "bbox", "=", "torch", ".", "cat", "(", "[", "_bbox", ",", "bbox_", "]", ",", "dim", "=", "-", "1", ")", "\n", "return", "bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.test_reload_hierarchical_model": [[38, 48], ["tempfile.TemporaryDirectory", "vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.SimpleHierarchicalModel", "vila.models.hierarchical_model.SimpleHierarchicalModel.save_pretrained", "vila.models.hierarchical_model.SimpleHierarchicalModel.from_pretrained", "test_hierarchical_model.compare_models"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models"], ["", "def", "test_reload_hierarchical_model", "(", ")", ":", "\n", "\n", "    ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tempdir", ":", "\n", "        ", "config", "=", "HierarchicalModelConfig", "(", "textline_model_type", "=", "\"bert-layer\"", ")", "\n", "model1", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "model1", ".", "save_pretrained", "(", "f\"{tempdir}/tmp-model\"", ")", "\n", "\n", "model2", "=", "SimpleHierarchicalModel", ".", "from_pretrained", "(", "f\"{tempdir}/tmp-model\"", ")", "\n", "\n", "assert", "compare_models", "(", "model1", ",", "model2", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.test_load_hierarchical_with_preloaded_weights": [[50, 72], ["vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.SimpleHierarchicalModel", "transformers.AutoModel.from_pretrained", "test_hierarchical_model.compare_models", "test_hierarchical_model.compare_models", "test_hierarchical_model.compare_models"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models", "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models", "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models"], ["", "", "def", "test_load_hierarchical_with_preloaded_weights", "(", ")", ":", "\n", "\n", "    ", "config", "=", "HierarchicalModelConfig", "(", "\n", "textline_model_type", "=", "\"bert-base-uncased\"", ",", "\n", "load_weights_from_existing_model", "=", "True", ",", "\n", ")", "\n", "\n", "model1", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "base_bert_model", "=", "AutoModel", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ")", "\n", "\n", "assert", "(", "\n", "compare_models", "(", "model1", ".", "textline_encoder", ".", "embeddings", ",", "base_bert_model", ".", "embeddings", ")", "\n", "==", "0", "\n", ")", "\n", "assert", "(", "\n", "compare_models", "(", "\n", "model1", ".", "textline_encoder", ".", "encoder", ".", "layer", "[", "0", "]", ",", "\n", "base_bert_model", ".", "encoder", ".", "layer", "[", "0", "]", ",", "\n", ")", "\n", "==", "0", "\n", ")", "\n", "assert", "compare_models", "(", "model1", ".", "textline_model", ",", "base_bert_model", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.test_reload_hierarchical_with_preloaded_weights": [[74, 89], ["tempfile.TemporaryDirectory", "vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.SimpleHierarchicalModel", "vila.models.hierarchical_model.SimpleHierarchicalModel.init_weights", "vila.models.hierarchical_model.SimpleHierarchicalModel.save_pretrained", "vila.models.hierarchical_model.SimpleHierarchicalModel.from_pretrained", "test_hierarchical_model.compare_models"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.preprocessors.config.VILAPreprocessorConfig.from_pretrained", "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.compare_models"], ["", "def", "test_reload_hierarchical_with_preloaded_weights", "(", ")", ":", "\n", "# Ensures the reloading model weights does not influence the reloading", "\n", "# of the models", "\n", "    ", "with", "tempfile", ".", "TemporaryDirectory", "(", ")", "as", "tempdir", ":", "\n", "        ", "config", "=", "HierarchicalModelConfig", "(", "\n", "textline_model_type", "=", "\"bert-base-uncased\"", ",", "\n", "load_weights_from_existing_model", "=", "True", ",", "\n", ")", "\n", "\n", "model1", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "model1", ".", "init_weights", "(", ")", "# changes the current model weights", "\n", "model1", ".", "save_pretrained", "(", "f\"{tempdir}/tmp-model\"", ")", "\n", "\n", "model2", "=", "SimpleHierarchicalModel", ".", "from_pretrained", "(", "f\"{tempdir}/tmp-model\"", ")", "\n", "assert", "compare_models", "(", "model1", ",", "model2", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.test_hierarchical_model": [[91, 118], ["vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.SimpleHierarchicalModel", "vila.models.hierarchical_model.SimpleHierarchicalModel.eval", "torch.randint", "test_hierarchical_model.create_bbox", "vila.models.hierarchical_model.SimpleHierarchicalModel.", "vila.models.hierarchical_model.SimpleHierarchicalModel.", "torch.equal", "vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.SimpleHierarchicalModel", "vila.models.hierarchical_model.SimpleHierarchicalModel.eval", "vila.models.hierarchical_model.SimpleHierarchicalModel.", "vila.models.hierarchical_model.SimpleHierarchicalModel.", "torch.Size", "torch.equal"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.create_bbox"], ["", "", "def", "test_hierarchical_model", "(", ")", ":", "\n", "    ", "config", "=", "HierarchicalModelConfig", "(", "\n", "num_labels", "=", "10", ",", "\n", "textline_model_type", "=", "\"bert-layer\"", ",", "\n", ")", "\n", "model", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "input_ids", "=", "torch", ".", "randint", "(", "30522", ",", "size", "=", "(", "1", ",", "200", ",", "25", ")", ")", "\n", "bbox", "=", "create_bbox", "(", "1", ",", "200", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ")", "\n", "assert", "outputs", "[", "0", "]", ".", "shape", "==", "torch", ".", "Size", "(", "[", "1", ",", "200", ",", "config", ".", "hidden_size", "]", ")", "\n", "\n", "outputs2", "=", "model", "(", "input_ids", ",", "bbox", ")", "\n", "assert", "torch", ".", "equal", "(", "outputs", "[", "0", "]", ",", "outputs2", "[", "0", "]", ")", "\n", "\n", "config", "=", "HierarchicalModelConfig", "(", "\n", "num_labels", "=", "10", ",", "\n", "textline_model_type", "=", "\"layoutlm-layer\"", ",", "\n", ")", "\n", "model", "=", "SimpleHierarchicalModel", "(", "config", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ")", "\n", "outputs2", "=", "model", "(", "input_ids", ",", "bbox", ")", "\n", "assert", "not", "torch", ".", "equal", "(", "outputs", "[", "0", "]", ",", "outputs2", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.test_hierarchical_token_classification_model": [[120, 149], ["vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.eval", "torch.randint", "test_hierarchical_model.create_bbox", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.", "torch.equal", "vila.models.configuration_hierarchical_model.HierarchicalModelConfig", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.eval", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.", "vila.models.hierarchical_model.HierarchicalModelForTokenClassification.", "torch.Size", "torch.equal"], "function", ["home.repos.pwc.inspect_result.allenai_VILA.tests.test_hierarchical_model.create_bbox"], ["", "def", "test_hierarchical_token_classification_model", "(", ")", ":", "\n", "# Ensures the reloading model weights does not influence the reloading", "\n", "# of the models", "\n", "\n", "    ", "config", "=", "HierarchicalModelConfig", "(", "\n", "num_labels", "=", "10", ",", "textline_model_type", "=", "\"bert-layer\"", ",", "hidden_dropout_prob", "=", "0", "\n", ")", "\n", "model", "=", "HierarchicalModelForTokenClassification", "(", "config", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "input_ids", "=", "torch", ".", "randint", "(", "30522", ",", "size", "=", "(", "1", ",", "200", ",", "25", ")", ")", "\n", "bbox", "=", "create_bbox", "(", "1", ",", "200", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ",", "bbox", ")", "\n", "assert", "outputs", "[", "0", "]", ".", "shape", "==", "torch", ".", "Size", "(", "[", "1", ",", "200", ",", "10", "]", ")", "\n", "\n", "outputs2", "=", "model", "(", "input_ids", ",", "bbox", ")", "\n", "assert", "torch", ".", "equal", "(", "outputs", "[", "0", "]", ",", "outputs2", "[", "0", "]", ")", "\n", "\n", "config", "=", "HierarchicalModelConfig", "(", "\n", "num_labels", "=", "10", ",", "\n", "textline_model_type", "=", "\"layoutlm-layer\"", ",", "\n", ")", "\n", "model", "=", "HierarchicalModelForTokenClassification", "(", "config", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "outputs", "=", "model", "(", "input_ids", ")", "\n", "outputs2", "=", "model", "(", "input_ids", ",", "bbox", ")", "\n", "assert", "not", "torch", ".", "equal", "(", "outputs", "[", "0", "]", ",", "outputs2", "[", "0", "]", ")", "", "", ""]]}