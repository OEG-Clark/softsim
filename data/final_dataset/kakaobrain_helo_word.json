{"home.repos.pwc.inspect_result.kakaobrain_helo_word.None.error_type_control.parse": [[16, 33], ["open().read", "re.search", "re.search.group().split", "dict", "open().read.splitlines", "ValueError", "int", "int", "int", "float", "float", "float", "open", "re.search.group", "line.strip().split", "int", "int", "int", "line.strip"], "function", ["None"], ["def", "parse", "(", "report", ")", ":", "\n", "# get summary", "\n", "    ", "text", "=", "open", "(", "report", ",", "'r'", ")", ".", "read", "(", ")", "\n", "srch", "=", "re", ".", "search", "(", "\"(?s)\\nTP[^\\n]+F0\\.5\\n([^\\n]+)\"", ",", "text", ")", "\n", "if", "srch", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"There is no summary\"", ")", "\n", "", "TP", ",", "FP", ",", "FN", ",", "Prec", ",", "Rec", ",", "Fscore", "=", "srch", ".", "group", "(", "1", ")", ".", "split", "(", ")", "\n", "TP", ",", "FP", ",", "FN", ",", "Prec", ",", "Rec", ",", "Fscore", "=", "int", "(", "TP", ")", ",", "int", "(", "FP", ")", ",", "int", "(", "FN", ")", ",", "float", "(", "Prec", ")", ",", "float", "(", "Rec", ")", ",", "float", "(", "Fscore", ")", "\n", "\n", "# get edits", "\n", "error_type2scores", "=", "dict", "(", ")", "\n", "for", "line", "in", "text", ".", "splitlines", "(", ")", ":", "\n", "        ", "if", "line", "[", ":", "2", "]", "in", "(", "\"U:\"", ",", "\"M:\"", ",", "\"R:\"", ")", ":", "\n", "            ", "error_type", ",", "tp", ",", "fp", ",", "fn", ",", "_", ",", "_", ",", "_", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "error_type2scores", "[", "error_type", "]", "=", "int", "(", "tp", ")", ",", "int", "(", "fp", ")", ",", "int", "(", "fn", ")", "\n", "\n", "", "", "return", "TP", ",", "FP", ",", "FN", ",", "Prec", ",", "Rec", ",", "Fscore", ",", "error_type2scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.error_type_control.get_score": [[34, 61], ["error_type2scores.items", "round", "round", "round", "float", "float"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "get_score", "(", "error_type2scores", ",", "dropped_error_types", "=", "[", "]", ")", ":", "\n", "    ", "'''\n    error_type2scores: e.g., 'U:VERB' : (4, 17, 25)\n    dropped_error_types: list.\n\n    Returns\n        precision, recall, fscore\n    '''", "\n", "TP", ",", "FP", ",", "FN", "=", "0", ",", "0", ",", "0", "\n", "for", "error_type", ",", "(", "tp", ",", "fp", ",", "fn", ")", "in", "error_type2scores", ".", "items", "(", ")", ":", "\n", "        ", "if", "error_type", "in", "dropped_error_types", ":", "\n", "            ", "fn", "+=", "tp", "\n", "FN", "+=", "fn", "\n", "", "else", ":", "\n", "            ", "TP", "+=", "tp", "\n", "FP", "+=", "fp", "\n", "FN", "+=", "fn", "\n", "\n", "", "", "precision", "=", "float", "(", "TP", ")", "/", "(", "TP", "+", "FP", ")", "\n", "recall", "=", "float", "(", "TP", ")", "/", "(", "TP", "+", "FN", ")", "\n", "fscore", "=", "(", "1", "+", "0.5", "*", "0.5", ")", "*", "precision", "*", "recall", "/", "(", "0.5", "*", "0.5", "*", "precision", "+", "recall", ")", "\n", "\n", "precision", "=", "round", "(", "precision", ",", "4", ")", "\n", "recall", "=", "round", "(", "recall", ",", "4", ")", "\n", "fscore", "=", "round", "(", "fscore", ",", "4", ")", "\n", "\n", "return", "TP", ",", "FP", ",", "FN", ",", "precision", ",", "recall", ",", "fscore", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.main": [[16, 46], ["gec.track.choice_track", "gec.track.choice_track.get_databin_path", "gec.track.choice_track.get_subset_datapath", "logging.info", "evaluate.find_best", "bool", "bool", "bool", "gec.util.get_sorted_ckpts", "gec.track.choice_track.get_output_dir", "gec.track.choice_track.get_output_dir", "tqdm.tqdm", "evaluate.run_ckpt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.choice_track", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_databin_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.get_subset_datapath", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.find_best", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_sorted_ckpts", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_output_dir", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_output_dir", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.run_ckpt"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "track", "=", "choice_track", "(", "args", ".", "track", ")", "\n", "\n", "assert", "args", ".", "subset", "in", "track", ".", "subsets", "\n", "assert", "bool", "(", "args", ".", "ckpt_dir", ")", "^", "bool", "(", "args", ".", "ckpt_fpath", ")", "\n", "if", "args", ".", "find_best", ":", "\n", "        ", "assert", "bool", "(", "args", ".", "ckpt_dir", ")", "\n", "\n", "", "databin_path", "=", "track", ".", "get_databin_path", "(", "'pretrain'", ")", "\n", "gold_m2", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "scorer_type", "=", "track", ".", "get_subset_datapath", "(", "args", ".", "subset", ")", "\n", "\n", "# ckpt_dir", "\n", "if", "args", ".", "ckpt_dir", "is", "not", "None", ":", "\n", "        ", "ckpt_files", "=", "util", ".", "get_sorted_ckpts", "(", "args", ".", "ckpt_dir", ")", "\n", "output_dir", "=", "track", ".", "get_output_dir", "(", "args", ".", "ckpt_dir", ")", "\n", "\n", "# ckpt_fpath", "\n", "", "else", ":", "\n", "        ", "ckpt_files", "=", "[", "args", ".", "ckpt_fpath", "]", "\n", "output_dir", "=", "track", ".", "get_output_dir", "(", "args", ".", "ckpt_fpath", ")", "\n", "\n", "", "if", "not", "args", ".", "find_best", ":", "\n", "        ", "for", "ckpt", "in", "tqdm", "(", "ckpt_files", ")", ":", "\n", "            ", "run_ckpt", "(", "databin_path", ",", "ckpt", ",", "output_dir", ",", "scorer_type", ",", "\n", "gold_m2", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "\n", "args", ".", "remove_unk_edits", ",", "args", ".", "remove_error_type_lst", ",", "\n", "args", ".", "apply_rerank", ",", "args", ".", "preserve_spell", ",", "args", ".", "max_edits", ")", "\n", "\n", "", "", "logging", ".", "info", "(", "f\"[Evaluate] highest score on {ori_path}\"", ")", "\n", "find_best", "(", "output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.find_best": [[48, 57], ["gec.util.find_highest_score", "logging.info", "logging.error", "exit"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.find_highest_score"], ["", "def", "find_best", "(", "output_dir", ",", "ori_path", ",", "scorer_type", ")", ":", "\n", "    ", "if", "output_dir", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "highest_fscore", ",", "highest_ckpt", "=", "util", ".", "find_highest_score", "(", "output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "logging", ".", "info", "(", "f\"[Evaluate] highest fscore: {highest_fscore}, ckpt: {highest_ckpt}\"", ")", "\n", "if", "highest_fscore", "==", "0", "and", "highest_ckpt", "==", "'.pt'", ":", "\n", "        ", "logging", ".", "error", "(", "f\"[Evaluate] cannot find the highest ckpt\"", ")", "\n", "exit", "(", ")", "\n", "", "return", "highest_fscore", ",", "highest_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.evaluate": [[59, 78], ["logging.info", "logging.info", "os.system", "logging.info", "os.system", "logging.info", "os.system"], "function", ["None"], ["", "def", "evaluate", "(", "scorer_type", ",", "ori_file", ",", "cor_file", ",", "gold_m2_file", ",", "report_path", ")", ":", "\n", "    ", "if", "scorer_type", "==", "\"errant\"", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Evaluate] errant\"", ")", "\n", "scorer", "=", "Path", ".", "errant", "\n", "\n", "logging", ".", "info", "(", "\"[Evaluate] 1. parallel to m2\"", ")", "\n", "m2_file", "=", "f\"{cor_file}.m2\"", "\n", "prompt", "=", "f\"python {Path.parallel_to_m2} -orig {ori_file} -cor {cor_file} -out {m2_file}\"", "\n", "os", ".", "system", "(", "prompt", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Evaluate] 2. compare m2\"", ")", "\n", "prompt", "=", "f\"python {scorer} -hyp {m2_file} -ref {gold_m2_file} -cat 3 -v | tee {report_path}\"", "\n", "os", ".", "system", "(", "prompt", ")", "\n", "\n", "", "elif", "scorer_type", "==", "\"m2scorer\"", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Evaluate] m2scorer\"", ")", "\n", "scorer", "=", "Path", ".", "m2scorer", "\n", "prompt", "=", "f\"python2.7 {scorer} {cor_file} {gold_m2_file} > {report_path}\"", "\n", "os", ".", "system", "(", "prompt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.run_ckpt": [[80, 115], ["logging.info", "os.makedirs", "ckpt.split", "gec.util.get_basename", "os.path.join", "gec.util.get_cor_path", "gec.util.get_basename", "os.path.isfile", "logging.info", "gec.generate.generate", "os.path.isfile", "logging.info", "gec.postprocess.postprocess", "os.path.isfile", "logging.info", "gec.util.get_basename", "evaluate.evaluate"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_cor_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.postprocess.postprocess", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.evaluate.evaluate"], ["", "", "def", "run_ckpt", "(", "databin_path", ",", "ckpt", ",", "output_dir", ",", "scorer_type", ",", "\n", "gold_m2_file", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "\n", "remove_unk_edits", ",", "remove_error_type_lst", ",", "\n", "apply_rerank", ",", "preserve_spell", ",", "max_edits", ")", ":", "\n", "\n", "        ", "logging", ".", "info", "(", "f\"[Run-ckpt] working on {ckpt}\"", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "ckpt_lst", "=", "ckpt", ".", "split", "(", "\":\"", ")", "\n", "ckpt_basename", "=", "''", "\n", "for", "c", "in", "ckpt_lst", ":", "\n", "            ", "b", "=", "util", ".", "get_basename", "(", "c", ",", "include_path", "=", "False", ",", "include_extension", "=", "False", ")", "\n", "ckpt_basename", "+=", "b", "\n", "\n", "", "data_basename", "=", "util", ".", "get_basename", "(", "ori_path", ",", "include_path", "=", "False", ",", "include_extension", "=", "False", ")", "\n", "system_out_basename", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f\"{ckpt_basename}.{data_basename}\"", ")", "\n", "system_out", "=", "f\"{system_out_basename}.out\"", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "system_out", ")", ":", "\n", "            ", "logging", ".", "info", "(", "f\"[Run-ckpt] 1. generate into {system_out}\"", ")", "\n", "generate", "(", "databin_path", ",", "ckpt", ",", "system_out", ",", "ori_path", "=", "ori_bpe_path", ",", "gen_subset", "=", "gen_subset", ")", "\n", "\n", "", "cor_path", "=", "util", ".", "get_cor_path", "(", "system_out", ",", "remove_unk_edits", ",", "remove_error_type_lst", ",", "\n", "apply_rerank", ",", "preserve_spell", ",", "max_edits", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "cor_path", ")", ":", "\n", "            ", "logging", ".", "info", "(", "f\"[Run-ckpt] 2. postprocess into {cor_path}\"", ")", "\n", "postprocess", "(", "ori_path", ",", "system_out", ",", "cor_path", ",", "remove_unk_edits", ",", "remove_error_type_lst", ",", "\n", "apply_rerank", ",", "preserve_spell", ",", "max_edits", ")", "\n", "\n", "", "report_path", "=", "f\"{util.get_basename(cor_path, include_extension=False)}.report\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "report_path", ")", ":", "\n", "            ", "logging", ".", "info", "(", "f\"[Run-ckpt] 3. evaluation into {report_path}\"", ")", "\n", "if", "scorer_type", "is", "not", "None", "and", "gold_m2_file", "is", "not", "None", ":", "\n", "                ", "evaluate", "(", "scorer_type", ",", "ori_path", ",", "cor_path", ",", "gold_m2_file", ",", "report_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.train.main": [[11, 27], ["gec.track.choice_track", "gec.track.choice_track.get_databin_path", "gec.track.choice_track.get_model_config", "gec.track.choice_track.get_ckpt_dir", "gec.track.choice_track.get_subset_datapath", "train.find_restore", "train.train"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.choice_track", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_databin_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_model_config", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_ckpt_dir", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.get_subset_datapath", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.train.find_restore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "track", "=", "choice_track", "(", "args", ".", "track", ")", "\n", "\n", "assert", "args", ".", "train_mode", "in", "track", ".", "train_modes", "\n", "if", "args", ".", "train_mode", "==", "'pretrain'", ":", "\n", "        ", "assert", "args", ".", "prev_model_output_dir", "is", "None", "\n", "\n", "", "databin_path", "=", "track", ".", "get_databin_path", "(", "args", ".", "train_mode", ")", "\n", "model_config", "=", "track", ".", "get_model_config", "(", "args", ".", "model", ",", "args", ".", "lr", ",", "args", ".", "dropout", ",", "args", ".", "max_epoch", ",", "args", ".", "seed", ",", "args", ".", "reset", ")", "\n", "ckpt_dir", "=", "track", ".", "get_ckpt_dir", "(", "args", ".", "train_mode", ",", "args", ".", "model", ",", "args", ".", "lr", ",", "args", ".", "dropout", ",", "args", ".", "seed", ",", "\n", "args", ".", "prev_model_output_dir", ")", "\n", "\n", "_", ",", "ori_path", ",", "_", ",", "_", ",", "scorer_type", "=", "track", ".", "get_subset_datapath", "(", "'valid'", ")", "\n", "fscore", ",", "restore_ckpt", "=", "find_restore", "(", "args", ".", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "\n", "train", "(", "databin_path", ",", "model_config", ",", "ckpt_dir", ",", "restore_ckpt", ",", "args", ".", "ngpu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.train.train": [[29, 47], ["os.makedirs", "logging.info", "logging.info", "os.system", "os.path.basename", "logging.info", "os.system", "gec.util.change_ckpt_dir"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.change_ckpt_dir"], ["", "def", "train", "(", "databin_path", ",", "model_config", ",", "ckpt_dir", ",", "restore_ckpt", ",", "ngpu", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "ckpt_dir", ",", "exist_ok", "=", "True", ")", "\n", "logging", ".", "info", "(", "f\"[Train] working on {ckpt_dir}\"", ")", "\n", "\n", "if", "ngpu", ">", "1", ":", "\n", "        ", "prompt", "=", "f\"python -m torch.distributed.launch --nproc_per_node {ngpu} $(which fairseq-train) {databin_path} \"", "f\"{model_config} --save-dir {ckpt_dir} \"", "\n", "", "else", ":", "\n", "        ", "prompt", "=", "f\"fairseq-train {databin_path} {model_config} --save-dir {ckpt_dir} \"", "\n", "", "logging", ".", "info", "(", "f\"[Train] {prompt}\"", ")", "\n", "\n", "if", "restore_ckpt", "is", "not", "None", ":", "\n", "        ", "finetune_ckpt", "=", "os", ".", "path", ".", "basename", "(", "util", ".", "change_ckpt_dir", "(", "restore_ckpt", ",", "ckpt_dir", ")", ")", "\n", "logging", ".", "info", "(", "f\"[Train] copy the ckpt {restore_ckpt} into {finetune_ckpt}\"", ")", "\n", "os", ".", "system", "(", "f\"cp {restore_ckpt} {finetune_ckpt}\"", ")", "\n", "\n", "prompt", "+=", "f\"--restore-file {finetune_ckpt} \"", "\n", "", "os", ".", "system", "(", "prompt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.train.find_restore": [[49, 58], ["gec.util.find_highest_score", "logging.info", "logging.error", "exit"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.find_highest_score"], ["", "def", "find_restore", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", ":", "\n", "    ", "if", "prev_model_output_dir", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "highest_fscore", ",", "highest_ckpt", "=", "util", ".", "find_highest_score", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "logging", ".", "info", "(", "f\"[Train] highest fscore: {highest_fscore}, ckpt: {highest_ckpt}\"", ")", "\n", "if", "highest_fscore", "==", "0", "and", "highest_ckpt", "==", "'.pt'", ":", "\n", "        ", "logging", ".", "error", "(", "f\"[Train] cannot find the highest ckpt\"", ")", "\n", "exit", "(", ")", "\n", "", "return", "highest_fscore", ",", "highest_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.prepare.main": [[11, 20], ["gec.track.choice_track", "prepare.prepare_text", "gec.track.choice_track.get_databin_path", "gec.track.choice_track.get_pref", "prepare.prepare_binary"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.choice_track", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.prepare.prepare_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_databin_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.get_pref", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.prepare.prepare_binary"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "track", "=", "choice_track", "(", "args", ".", "track", ")", "\n", "\n", "prepare_text", "(", "track", ")", "\n", "\n", "for", "train_mode", "in", "track", ".", "train_modes", ":", "\n", "        ", "databin_path", "=", "track", ".", "get_databin_path", "(", "train_mode", ")", "\n", "trainpref", ",", "validpref", "=", "track", ".", "get_pref", "(", "train_mode", ")", "\n", "prepare_binary", "(", "databin_path", ",", "trainpref", ",", "validpref", ",", "track", ".", "fp", ".", "BPE_VOCAB", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.prepare.prepare_text": [[22, 74], ["logging.info", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt", "logging.info", "gec.util.maybe_prompt", "gec.util.maybe_prompt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt"], ["", "", "def", "prepare_text", "(", "track", ")", ":", "\n", "    ", "logging", ".", "info", "(", "f\"[Prepare] 1. prepare for the text data\"", ")", "\n", "fp", "=", "track", ".", "fp", "\n", "\n", "if", "track", ".", "TRACK_NUM", "==", "0", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Prepare] 1-1. pretrain\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_ORI0", ",", "f\"cat {fp.GUTENBERG_ORI0} {fp.TATOEBA_ORI0} {fp.WIKI103_ORI0} > {fp.DAE_ORI0}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_COR0", ",", "f\"cat {fp.GUTENBERG_COR0} {fp.TATOEBA_COR0} {fp.WIKI103_COR0} > {fp.DAE_COR0}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-2. train\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "TRAIN_ORI0", ",", "f\"cat {fp.FCE_TOK_ORI} {fp.LANG8_TOK_ORI} {fp.NUCLE_TOK_ORI} > {fp.TRAIN_ORI0}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "TRAIN_COR0", ",", "f\"cat {fp.FCE_TOK_COR} {fp.LANG8_TOK_COR} {fp.NUCLE_TOK_COR} > {fp.TRAIN_COR0}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-3. finetune\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_ORI0", ",", "f\"cp {fp.NUCLE_TOK_ORI} {fp.FINETUNE_ORI0}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_COR0", ",", "f\"cp {fp.NUCLE_TOK_COR} {fp.FINETUNE_COR0}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-4. valid\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_ORI0", ",", "f\"cp {fp.CONLL2013_TOK_ORI} {fp.VALID_ORI0}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_COR0", ",", "f\"cp {fp.CONLL2013_TOK_COR} {fp.VALID_COR0}\"", ")", "\n", "\n", "", "elif", "track", ".", "TRACK_NUM", "==", "1", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Prepare] 1-1. pretrain\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_ORI1", ",", "f\"cat {fp.GUTENBERG_ORI1} {fp.TATOEBA_ORI1} {fp.WIKI103_ORI1} > {fp.DAE_ORI1}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_COR1", ",", "f\"cat {fp.GUTENBERG_COR1} {fp.TATOEBA_COR1} {fp.WIKI103_COR1} > {fp.DAE_COR1}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-2. train\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "TRAIN_ORI1", ",", "\n", "f\"cat {fp.FCE_TOK_ORI} {fp.LANG8_TOK_ORI} {fp.NUCLE_TOK_ORI} {fp.WI_TRAIN_TOK_ORI} > {fp.TRAIN_ORI1}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "TRAIN_COR1", ",", "\n", "f\"cat {fp.FCE_TOK_COR} {fp.LANG8_TOK_COR} {fp.NUCLE_TOK_COR} {fp.WI_TRAIN_TOK_COR} > {fp.TRAIN_COR1}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-3. finetune\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_ORI1", ",", "f\"cp {fp.WI_TRAIN_TOK_ORI} {fp.FINETUNE_ORI1}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_COR1", ",", "f\"cp {fp.WI_TRAIN_TOK_COR} {fp.FINETUNE_COR1}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-4. valid\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_ORI1", ",", "f\"cp {fp.WI_DEV_TOK_ORI} {fp.VALID_ORI1}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_COR1", ",", "f\"cp {fp.WI_DEV_TOK_COR} {fp.VALID_COR1}\"", ")", "\n", "\n", "", "elif", "track", ".", "TRACK_NUM", "==", "3", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Prepare] 1-1. pretrain\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_ORI3", ",", "f\"cat {fp.GUTENBERG_ORI3} {fp.TATOEBA_ORI3} {fp.WIKI103_ORI3} > {fp.DAE_ORI3}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "DAE_COR3", ",", "f\"cat {fp.GUTENBERG_COR3} {fp.TATOEBA_COR3} {fp.WIKI103_COR3} > {fp.DAE_COR3}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-2. finetune\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_ORI3", ",", "f\"cp {fp.WI_DEV_3K_TOK_ORI} {fp.FINETUNE_ORI3}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "FINETUNE_COR3", ",", "f\"cp {fp.WI_DEV_3K_TOK_COR} {fp.FINETUNE_COR3}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Prepare] 1-3. valid\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_ORI3", ",", "f\"cp {fp.WI_DEV_1K_TOK_ORI} {fp.VALID_ORI3}\"", ")", "\n", "util", ".", "maybe_prompt", "(", "fp", ".", "VALID_COR3", ",", "f\"cp {fp.WI_DEV_1K_TOK_COR} {fp.VALID_COR3}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.prepare.prepare_binary": [[76, 83], ["logging.info", "gec.util.maybe_prompt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt"], ["", "", "def", "prepare_binary", "(", "databin_path", ",", "trainpref", ",", "validpref", ",", "vocab", ")", ":", "\n", "    ", "logging", ".", "info", "(", "f\"[Prepare] 2. create binary data on {databin_path}\"", ")", "\n", "prompt", "=", "f\"fairseq-preprocess --source-lang ori --target-lang cor \"", "f\"--trainpref {trainpref} --validpref {validpref} \"", "f\"--srcdict {vocab} --tgtdict {vocab} --destdir {databin_path}\"", "\n", "\n", "util", ".", "maybe_prompt", "(", "databin_path", ",", "prompt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.preprocess.maybe_do": [[11, 16], ["os.path.exists", "logging.info", "func"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["def", "maybe_do", "(", "fp", ",", "func", ",", "inputs", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "fp", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"skip this step as {fp} already exists\"", ")", "\n", "", "else", ":", "\n", "        ", "func", "(", "*", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.preprocess.maybe_download": [[17, 25], ["os.listdir", "logging.info", "print", "cmd.split", "print", "os.system"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "def", "maybe_download", "(", "dir", ",", "cmd", ")", ":", "\n", "    ", "if", "os", ".", "listdir", "(", "dir", ")", "!=", "[", "]", ":", "\n", "        ", "logging", ".", "info", "(", "f\"skip this step as {dir} is NOT empty\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "cmd", ")", "\n", "for", "sub_cmd", "in", "cmd", ".", "split", "(", "\"&\"", ")", ":", "\n", "            ", "print", "(", "sub_cmd", ")", "\n", "os", ".", "system", "(", "sub_cmd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.__init__": [[112, 115], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "function", "=", "lambda", "x", ":", "x", ",", "items", "=", "[", "]", ")", ":", "\n", "        ", "self", ".", "_f", "=", "function", "\n", "self", ".", "_a", "=", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items": [[116, 119], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "items", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_a", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.__repr__": [[120, 122], ["repr", "list", "iter"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "repr", "(", "list", "(", "iter", "(", "self", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.__getitem__": [[123, 125], ["tree.Map._f"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_f", "(", "self", ".", "_a", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.__len__": [[126, 128], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.__iter__": [[129, 134], ["len", "tree.Map._f"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "self", ".", "_a", ")", ":", "\n", "            ", "yield", "self", ".", "_f", "(", "self", ".", "_a", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__init__": [[148, 169], ["isinstance", "string.decode.decode.decode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sentence", ",", "string", ",", "lemma", "=", "None", ",", "type", "=", "None", ",", "index", "=", "0", ")", ":", "\n", "        ", "\"\"\" A word in the sentence.\n            - lemma: base form of the word; \"was\" => \"be\".\n            -  type: the part-of-speech tag; \"NN\" => a noun.\n            - chunk: the chunk (or phrase) this word belongs to.\n            - index: the index in the sentence.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "string", ",", "str", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "string", "=", "string", ".", "decode", "(", "\"utf-8\"", ")", "# ensure Unicode", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "", "", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "index", "=", "index", "\n", "self", ".", "string", "=", "string", "# \"was\"", "\n", "self", ".", "lemma", "=", "lemma", "# \"be\"", "\n", "self", ".", "type", "=", "type", "# VB", "\n", "self", ".", "chunk", "=", "None", "# Chunk object this word belongs to (i.e., a VP).", "\n", "self", ".", "pnp", "=", "None", "# PNP chunk object this word belongs to.", "\n", "# word.chunk and word.pnp are set in chunk.append().", "\n", "self", ".", "_custom_tags", "=", "None", "# Tags object, created on request.", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.copy": [[170, 183], ["tree.Word", "tree.Tags"], "methods", ["None"], ["", "def", "copy", "(", "self", ",", "chunk", "=", "None", ",", "pnp", "=", "None", ")", ":", "\n", "        ", "w", "=", "Word", "(", "\n", "self", ".", "sentence", ",", "\n", "self", ".", "string", ",", "\n", "self", ".", "lemma", ",", "\n", "self", ".", "type", ",", "\n", "self", ".", "index", "\n", ")", "\n", "w", ".", "chunk", "=", "chunk", "\n", "w", ".", "pnp", "=", "pnp", "\n", "if", "self", ".", "_custom_tags", ":", "\n", "            ", "w", ".", "_custom_tags", "=", "Tags", "(", "w", ",", "items", "=", "self", ".", "_custom_tags", ")", "\n", "", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word._get_tag": [[184, 186], ["None"], "methods", ["None"], ["", "def", "_get_tag", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "type", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word._set_tag": [[187, 189], ["None"], "methods", ["None"], ["", "def", "_set_tag", "(", "self", ",", "v", ")", ":", "\n", "        ", "self", ".", "type", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.phrase": [[192, 195], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "phrase", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.prepositional_phrase": [[196, 199], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "prepositional_phrase", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pnp", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.tags": [[202, 229], ["enumerate", "builtins.range", "encode_entities", "len", "len", "encode_entities", "builtins.str", "tree.Word.custom_tags.get", "list", "reversed"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["@", "property", "\n", "def", "tags", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields a list of all the token tags as they appeared when the word was parsed.\n            For example: [\"was\", \"VBD\", \"B-VP\", \"O\", \"VP-1\", \"A1\", \"be\"]\n        \"\"\"", "\n", "# See also. Sentence.__repr__().", "\n", "ch", ",", "I", ",", "O", ",", "B", "=", "self", ".", "chunk", ",", "INSIDE", "+", "\"-\"", ",", "OUTSIDE", ",", "BEGIN", "+", "\"-\"", "\n", "tags", "=", "[", "OUTSIDE", "for", "i", "in", "range", "(", "len", "(", "self", ".", "sentence", ".", "token", ")", ")", "]", "\n", "for", "i", ",", "tag", "in", "enumerate", "(", "self", ".", "sentence", ".", "token", ")", ":", "# Default: [WORD, POS, CHUNK, PNP, RELATION, ANCHOR, LEMMA]", "\n", "            ", "if", "tag", "==", "WORD", ":", "\n", "                ", "tags", "[", "i", "]", "=", "encode_entities", "(", "self", ".", "string", ")", "\n", "", "elif", "tag", "==", "POS", "or", "tag", "==", "\"pos\"", "and", "self", ".", "type", ":", "\n", "                ", "tags", "[", "i", "]", "=", "self", ".", "type", "\n", "", "elif", "tag", "==", "CHUNK", "and", "ch", "and", "ch", ".", "type", ":", "\n", "                ", "tags", "[", "i", "]", "=", "(", "self", "==", "ch", "[", "0", "]", "and", "B", "or", "I", ")", "+", "ch", ".", "type", "\n", "", "elif", "tag", "==", "PNP", "and", "self", ".", "pnp", ":", "\n", "                ", "tags", "[", "i", "]", "=", "(", "self", "==", "self", ".", "pnp", "[", "0", "]", "and", "B", "or", "I", ")", "+", "\"PNP\"", "\n", "", "elif", "tag", "==", "REL", "and", "ch", "and", "len", "(", "ch", ".", "relations", ")", ">", "0", ":", "\n", "                ", "tags", "[", "i", "]", "=", "[", "\"-\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "[", "ch", ".", "type", "]", "+", "list", "(", "reversed", "(", "r", ")", ")", "if", "x", "]", ")", "for", "r", "in", "ch", ".", "relations", "]", "\n", "tags", "[", "i", "]", "=", "\"*\"", ".", "join", "(", "tags", "[", "i", "]", ")", "\n", "", "elif", "tag", "==", "ANCHOR", "and", "ch", ":", "\n", "                ", "tags", "[", "i", "]", "=", "ch", ".", "anchor_id", "or", "OUTSIDE", "\n", "", "elif", "tag", "==", "LEMMA", ":", "\n", "                ", "tags", "[", "i", "]", "=", "encode_entities", "(", "self", ".", "lemma", "or", "\"\"", ")", "\n", "", "elif", "tag", "in", "self", ".", "custom_tags", ":", "\n", "                ", "tags", "[", "i", "]", "=", "self", ".", "custom_tags", ".", "get", "(", "tag", ")", "or", "OUTSIDE", "\n", "", "", "return", "tags", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.custom_tags": [[230, 235], ["tree.Tags"], "methods", ["None"], ["", "@", "property", "\n", "def", "custom_tags", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "_custom_tags", ":", "\n", "            ", "self", ".", "_custom_tags", "=", "Tags", "(", "self", ")", "\n", "", "return", "self", ".", "_custom_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.next": [[236, 245], ["len"], "methods", ["None"], ["", "def", "next", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\" Returns the next word in the sentence with the given type.\n        \"\"\"", "\n", "i", "=", "self", ".", "index", "+", "1", "\n", "s", "=", "self", ".", "sentence", "\n", "while", "i", "<", "len", "(", "s", ")", ":", "\n", "            ", "if", "type", "in", "(", "s", "[", "i", "]", ".", "type", ",", "None", ")", ":", "\n", "                ", "return", "s", "[", "i", "]", "\n", "", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.previous": [[246, 255], ["None"], "methods", ["None"], ["", "", "def", "previous", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\" Returns the next previous word in the sentence with the given type.\n        \"\"\"", "\n", "i", "=", "self", ".", "index", "-", "1", "\n", "s", "=", "self", ".", "sentence", "\n", "while", "i", ">", "0", ":", "\n", "            ", "if", "type", "in", "(", "s", "[", "i", "]", ".", "type", ",", "None", ")", ":", "\n", "                ", "return", "s", "[", "i", "]", "\n", "", "i", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__getattr__": [[257, 262], ["tree.Word.__dict__.get", "AttributeError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "", "def", "__getattr__", "(", "self", ",", "tag", ")", ":", "\n", "        ", "d", "=", "self", ".", "__dict__", ".", "get", "(", "\"_custom_tags\"", ",", "None", ")", "\n", "if", "d", "and", "tag", "in", "d", ":", "\n", "            ", "return", "d", "[", "tag", "]", "\n", "", "raise", "AttributeError", "(", "\"Word instance has no attribute '%s'\"", "%", "tag", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__str__": [[265, 267], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__repr__": [[268, 272], ["repr", "encode_entities"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"Word(%s)\"", "%", "repr", "(", "\"%s/%s\"", "%", "(", "\n", "encode_entities", "(", "self", ".", "string", ")", ",", "\n", "self", ".", "type", "is", "not", "None", "and", "self", ".", "type", "or", "OUTSIDE", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__eq__": [[273, 275], ["id", "id"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "word", ")", ":", "\n", "        ", "return", "id", "(", "self", ")", "==", "id", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__ne__": [[276, 278], ["id", "id"], "methods", ["None"], ["", "def", "__ne__", "(", "self", ",", "word", ")", ":", "\n", "        ", "return", "id", "(", "self", ")", "!=", "id", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.__init__": [[287, 297], ["builtins.dict.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "word", ",", "items", "=", "[", "]", ")", ":", "\n", "        ", "\"\"\" A dictionary of custom word tags.\n            A word may be annotated with its part-of-speech tag (e.g., \"cat/NN\"), \n            phrase tag (e.g., \"cat/NN/NP\"), the prepositional noun phrase it is part of etc.\n            An example of an extra custom slot is its semantic type, \n            e.g., gene type, topic, and so on: \"cat/NN/NP/genus_felis\"\n        \"\"\"", "\n", "if", "items", ":", "\n", "            ", "dict", ".", "__init__", "(", "self", ",", "items", ")", "\n", "", "self", ".", "word", "=", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.__setitem__": [[298, 304], ["builtins.dict.__setitem__", "reversed", "tree.Tags.word.sentence.token.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.__setitem__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "__setitem__", "(", "self", ",", "k", ",", "v", ")", ":", "\n", "# Ensure that the custom tag is also in Word.sentence.token,", "\n", "# so that it is not forgotten when exporting or importing XML.", "\n", "        ", "dict", ".", "__setitem__", "(", "self", ",", "k", ",", "v", ")", "\n", "if", "k", "not", "in", "reversed", "(", "self", ".", "word", ".", "sentence", ".", "token", ")", ":", "\n", "            ", "self", ".", "word", ".", "sentence", ".", "token", ".", "append", "(", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault": [[305, 309], ["tree.Tags.__setitem__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.__setitem__"], ["", "", "def", "setdefault", "(", "self", ",", "k", ",", "v", ")", ":", "\n", "        ", "if", "k", "not", "in", "self", ":", "\n", "            ", "self", ".", "__setitem__", "(", "k", ",", "v", ")", "\n", "return", "self", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__init__": [[315, 344], ["isinstance", "isinstance", "tree.Chunk.extend", "list", "builtins.zip", "list", "builtins.zip", "list", "builtins.zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["    ", "def", "__init__", "(", "self", ",", "sentence", ",", "words", "=", "[", "]", ",", "type", "=", "None", ",", "role", "=", "None", ",", "relation", "=", "None", ")", ":", "\n", "        ", "\"\"\" A list of words that make up a phrase in the sentence.\n            - type: the phrase tag; \"NP\" => a noun phrase (e.g., \"the black cat\").\n            - role: the function of the phrase; \"SBJ\" => sentence subject.\n            - relation: an id shared with other phrases, linking subject to object in the sentence.\n        \"\"\"", "\n", "# A chunk can have multiple roles or relations in the sentence,", "\n", "# so role and relation can also be given as lists.", "\n", "b1", "=", "isinstance", "(", "relation", ",", "(", "list", ",", "tuple", ")", ")", "\n", "b2", "=", "isinstance", "(", "role", ",", "(", "list", ",", "tuple", ")", ")", "\n", "if", "not", "b1", "and", "not", "b2", ":", "\n", "            ", "r", "=", "[", "(", "relation", ",", "role", ")", "]", "\n", "", "elif", "b1", "and", "b2", ":", "\n", "            ", "r", "=", "list", "(", "zip", "(", "relation", ",", "role", ")", ")", "\n", "", "elif", "b1", ":", "\n", "            ", "r", "=", "list", "(", "zip", "(", "relation", ",", "[", "role", "]", "*", "len", "(", "relation", ")", ")", ")", "\n", "", "elif", "b2", ":", "\n", "            ", "r", "=", "list", "(", "zip", "(", "[", "relation", "]", "*", "len", "(", "role", ")", ",", "role", ")", ")", "\n", "", "r", "=", "[", "(", "a", ",", "b", ")", "for", "a", ",", "b", "in", "r", "if", "a", "is", "not", "None", "or", "b", "is", "not", "None", "]", "\n", "self", ".", "sentence", "=", "sentence", "\n", "self", ".", "words", "=", "[", "]", "\n", "self", ".", "type", "=", "type", "# NP, VP, ADJP ...", "\n", "self", ".", "relations", "=", "r", "# NP-SBJ-1 => [(1, SBJ)]", "\n", "self", ".", "pnp", "=", "None", "# PNP chunk object this chunk belongs to.", "\n", "self", ".", "anchor", "=", "None", "# PNP chunk's anchor.", "\n", "self", ".", "attachments", "=", "[", "]", "# PNP chunks attached to this anchor.", "\n", "self", ".", "_conjunctions", "=", "None", "# Conjunctions object, created on request.", "\n", "self", ".", "_modifiers", "=", "None", "\n", "self", ".", "extend", "(", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.extend": [[345, 348], ["tree.Chunk.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "extend", "(", "self", ",", "words", ")", ":", "\n", "        ", "for", "w", "in", "words", ":", "\n", "            ", "self", ".", "append", "(", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.append": [[349, 352], ["tree.Chunk.words.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "append", "(", "self", ",", "word", ")", ":", "\n", "        ", "self", ".", "words", ".", "append", "(", "word", ")", "\n", "word", ".", "chunk", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__getitem__": [[353, 355], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "words", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__len__": [[356, 358], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__iter__": [[359, 361], ["tree.Chunk.words.__iter__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__iter__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "words", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk._get_tag": [[362, 364], ["None"], "methods", ["None"], ["", "def", "_get_tag", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "type", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk._set_tag": [[365, 367], ["None"], "methods", ["None"], ["", "def", "_set_tag", "(", "self", ",", "v", ")", ":", "\n", "        ", "self", ".", "type", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.start": [[370, 373], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "start", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "words", "[", "0", "]", ".", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.stop": [[374, 377], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "stop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "words", "[", "-", "1", "]", ".", "index", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range": [[378, 381], ["builtins.range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "@", "property", "\n", "def", "range", "(", "self", ")", ":", "\n", "        ", "return", "range", "(", "self", ".", "start", ",", "self", ".", "stop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.span": [[382, 385], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "span", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "start", ",", "self", ".", "stop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.lemmata": [[386, 389], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "lemmata", "(", "self", ")", ":", "\n", "        ", "return", "[", "word", ".", "lemma", "for", "word", "in", "self", ".", "words", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.tagged": [[390, 393], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tagged", "(", "self", ")", ":", "\n", "        ", "return", "[", "(", "word", ".", "string", ",", "word", ".", "type", ")", "for", "word", "in", "self", ".", "words", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.head": [[394, 413], ["any", "tree.find", "reversed", "tree.find", "find.type.startswith", "find.type.startswith", "reversed", "tree.find", "find.type.startswith", "reversed", "tree.find", "find.type.startswith", "tree.find", "find.type.startswith", "reversed", "find.type.startswith"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find"], ["", "@", "property", "\n", "def", "head", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the head of the chunk (usually, the last word in the chunk).\n        \"\"\"", "\n", "if", "self", ".", "type", "==", "\"NP\"", "and", "any", "(", "w", ".", "type", ".", "startswith", "(", "\"NNP\"", ")", "for", "w", "in", "self", ")", ":", "\n", "            ", "w", "=", "find", "(", "lambda", "w", ":", "w", ".", "type", ".", "startswith", "(", "\"NNP\"", ")", ",", "reversed", "(", "self", ")", ")", "\n", "", "elif", "self", ".", "type", "==", "\"NP\"", ":", "# \"the cat\" => \"cat\"", "\n", "            ", "w", "=", "find", "(", "lambda", "w", ":", "w", ".", "type", ".", "startswith", "(", "\"NN\"", ")", ",", "reversed", "(", "self", ")", ")", "\n", "", "elif", "self", ".", "type", "==", "\"VP\"", ":", "# \"is watching\" => \"watching\"", "\n", "            ", "w", "=", "find", "(", "lambda", "w", ":", "w", ".", "type", ".", "startswith", "(", "\"VB\"", ")", ",", "reversed", "(", "self", ")", ")", "\n", "", "elif", "self", ".", "type", "==", "\"PP\"", ":", "# \"from up on\" => \"from\"", "\n", "            ", "w", "=", "find", "(", "lambda", "w", ":", "w", ".", "type", ".", "startswith", "(", "(", "\"IN\"", ",", "\"PP\"", ")", ")", ",", "self", ")", "\n", "", "elif", "self", ".", "type", "==", "\"PNP\"", ":", "# \"from up on the roof\" => \"roof\"", "\n", "            ", "w", "=", "find", "(", "lambda", "w", ":", "w", ".", "type", ".", "startswith", "(", "\"NN\"", ")", ",", "reversed", "(", "self", ")", ")", "\n", "", "else", ":", "\n", "            ", "w", "=", "None", "\n", "", "if", "w", "is", "None", ":", "\n", "            ", "w", "=", "self", "[", "-", "1", "]", "\n", "", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.relation": [[414, 420], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "relation", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the first relation id of the chunk.\n        \"\"\"", "\n", "# [(2,OBJ), (3,OBJ)])] => 2", "\n", "return", "len", "(", "self", ".", "relations", ")", ">", "0", "and", "self", ".", "relations", "[", "0", "]", "[", "0", "]", "or", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.role": [[421, 427], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "role", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the first role of the chunk (SBJ, OBJ, ...).\n        \"\"\"", "\n", "# [(1,SBJ), (1,OBJ)])] => SBJ", "\n", "return", "len", "(", "self", ".", "relations", ")", ">", "0", "and", "self", ".", "relations", "[", "0", "]", "[", "1", "]", "or", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.subject": [[428, 433], ["tree.Chunk.sentence.relations[].get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "@", "property", "\n", "def", "subject", "(", "self", ")", ":", "\n", "        ", "ch", "=", "self", ".", "sentence", ".", "relations", "[", "\"SBJ\"", "]", ".", "get", "(", "self", ".", "relation", ",", "None", ")", "\n", "if", "ch", "!=", "self", ":", "\n", "            ", "return", "ch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.object": [[434, 439], ["tree.Chunk.sentence.relations[].get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "", "@", "property", "\n", "def", "object", "(", "self", ")", ":", "\n", "        ", "ch", "=", "self", ".", "sentence", ".", "relations", "[", "\"OBJ\"", "]", ".", "get", "(", "self", ".", "relation", ",", "None", ")", "\n", "if", "ch", "!=", "self", ":", "\n", "            ", "return", "ch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.verb": [[440, 445], ["tree.Chunk.sentence.relations[].get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "", "@", "property", "\n", "def", "verb", "(", "self", ")", ":", "\n", "        ", "ch", "=", "self", ".", "sentence", ".", "relations", "[", "\"VP\"", "]", ".", "get", "(", "self", ".", "relation", ",", "None", ")", "\n", "if", "ch", "!=", "self", ":", "\n", "            ", "return", "ch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.related": [[446, 452], ["tree.intersects", "tree.unzip", "tree.unzip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.intersects", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip"], ["", "", "@", "property", "\n", "def", "related", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields a list of all chunks in the sentence with the same relation id.\n        \"\"\"", "\n", "return", "[", "ch", "for", "ch", "in", "self", ".", "sentence", ".", "chunks", "\n", "if", "ch", "!=", "self", "and", "intersects", "(", "unzip", "(", "0", ",", "ch", ".", "relations", ")", ",", "unzip", "(", "0", ",", "self", ".", "relations", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.prepositional_phrase": [[453, 456], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "prepositional_phrase", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pnp", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.anchor_id": [[459, 476], ["list", "id.strip", "builtins.filter", "f", "f", "f"], "methods", ["None"], ["@", "property", "\n", "def", "anchor_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the anchor tag as parsed from the original token.\n            Chunks that are anchors have a tag with an \"A\" prefix (e.g., \"A1\").\n            Chunks that are PNP attachmens (or chunks inside a PNP) have \"P\" (e.g., \"P1\").\n            Chunks inside a PNP can be both anchor and attachment (e.g., \"P1-A2\"),\n            as in: \"clawed/A1 at/P1 mice/P1-A2 in/P2 the/P2 wall/P2\"\n        \"\"\"", "\n", "id", "=", "\"\"", "\n", "f", "=", "lambda", "ch", ":", "list", "(", "filter", "(", "lambda", "k", ":", "self", ".", "sentence", ".", "_anchors", "[", "k", "]", "==", "ch", ",", "self", ".", "sentence", ".", "_anchors", ")", ")", "\n", "if", "self", ".", "pnp", "and", "self", ".", "pnp", ".", "anchor", ":", "\n", "            ", "id", "+=", "\"-\"", "+", "\"-\"", ".", "join", "(", "f", "(", "self", ".", "pnp", ")", ")", "\n", "", "if", "self", ".", "anchor", ":", "\n", "            ", "id", "+=", "\"-\"", "+", "\"-\"", ".", "join", "(", "f", "(", "self", ")", ")", "\n", "", "if", "self", ".", "attachments", ":", "\n", "            ", "id", "+=", "\"-\"", "+", "\"-\"", ".", "join", "(", "f", "(", "self", ")", ")", "\n", "", "return", "id", ".", "strip", "(", "\"-\"", ")", "or", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.conjunctions": [[477, 482], ["tree.Conjunctions"], "methods", ["None"], ["", "@", "property", "\n", "def", "conjunctions", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "_conjunctions", ":", "\n", "            ", "self", ".", "_conjunctions", "=", "Conjunctions", "(", "self", ")", "\n", "", "return", "self", ".", "_conjunctions", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.modifiers": [[483, 497], ["builtins.filter", "chunk.nearest", "chunk.nearest._modifiers.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.nearest", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "@", "property", "\n", "def", "modifiers", "(", "self", ")", ":", "\n", "        ", "\"\"\" For verb phrases (VP), yields a list of the nearest adjectives and adverbs.\n        \"\"\"", "\n", "if", "self", ".", "_modifiers", "is", "None", ":", "\n", "# Iterate over all the chunks and attach modifiers to their VP-anchor.", "\n", "            ", "is_modifier", "=", "lambda", "ch", ":", "ch", ".", "type", "in", "(", "\"ADJP\"", ",", "\"ADVP\"", ")", "and", "ch", ".", "relation", "is", "None", "\n", "for", "chunk", "in", "self", ".", "sentence", ".", "chunks", ":", "\n", "                ", "chunk", ".", "_modifiers", "=", "[", "]", "\n", "", "for", "chunk", "in", "filter", "(", "is_modifier", ",", "self", ".", "sentence", ".", "chunks", ")", ":", "\n", "                ", "anchor", "=", "chunk", ".", "nearest", "(", "\"VP\"", ")", "\n", "if", "anchor", ":", "\n", "                    ", "anchor", ".", "_modifiers", ".", "append", "(", "chunk", ")", "\n", "", "", "", "return", "self", ".", "_modifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.nearest": [[498, 512], ["isinstance", "enumerate", "len", "tree.Chunk.sentence.chunks.index", "tree.Chunk.sentence.chunks.index", "chunk.type.startswith", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index"], ["", "def", "nearest", "(", "self", ",", "type", "=", "\"VP\"", ")", ":", "\n", "        ", "\"\"\" Returns the nearest chunk in the sentence with the given type.\n            This can be used (for example) to find adverbs and adjectives related to verbs,\n            as in: \"the cat is ravenous\" => is what? => \"ravenous\".\n        \"\"\"", "\n", "candidate", ",", "d", "=", "None", ",", "len", "(", "self", ".", "sentence", ".", "chunks", ")", "\n", "if", "isinstance", "(", "self", ",", "PNPChunk", ")", ":", "\n", "            ", "i", "=", "self", ".", "sentence", ".", "chunks", ".", "index", "(", "self", ".", "chunks", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "i", "=", "self", ".", "sentence", ".", "chunks", ".", "index", "(", "self", ")", "\n", "", "for", "j", ",", "chunk", "in", "enumerate", "(", "self", ".", "sentence", ".", "chunks", ")", ":", "\n", "            ", "if", "chunk", ".", "type", ".", "startswith", "(", "type", ")", "and", "abs", "(", "i", "-", "j", ")", "<", "d", ":", "\n", "                ", "candidate", ",", "d", "=", "chunk", ",", "abs", "(", "i", "-", "j", ")", "\n", "", "", "return", "candidate", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next": [[513, 522], ["len"], "methods", ["None"], ["", "def", "next", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\" Returns the next chunk in the sentence with the given type.\n        \"\"\"", "\n", "i", "=", "self", ".", "stop", "\n", "s", "=", "self", ".", "sentence", "\n", "while", "i", "<", "len", "(", "s", ")", ":", "\n", "            ", "if", "s", "[", "i", "]", ".", "chunk", "is", "not", "None", "and", "type", "in", "(", "s", "[", "i", "]", ".", "chunk", ".", "type", ",", "None", ")", ":", "\n", "                ", "return", "s", "[", "i", "]", ".", "chunk", "\n", "", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.previous": [[523, 532], ["None"], "methods", ["None"], ["", "", "def", "previous", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\" Returns the next previous chunk in the sentence with the given type.\n        \"\"\"", "\n", "i", "=", "self", ".", "start", "-", "1", "\n", "s", "=", "self", ".", "sentence", "\n", "while", "i", ">", "0", ":", "\n", "            ", "if", "s", "[", "i", "]", ".", "chunk", "is", "not", "None", "and", "type", "in", "(", "s", "[", "i", "]", ".", "chunk", ".", "type", ",", "None", ")", ":", "\n", "                ", "return", "s", "[", "i", "]", ".", "chunk", "\n", "", "i", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.string": [[535, 538], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "string", "(", "self", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "word", ".", "string", "for", "word", "in", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__str__": [[539, 541], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__repr__": [[542, 548], ["repr", "builtins.str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"Chunk(%s)\"", "%", "repr", "(", "\"%s/%s%s%s\"", ")", "%", "(", "\n", "self", ".", "string", ",", "\n", "self", ".", "type", "is", "not", "None", "and", "self", ".", "type", "or", "OUTSIDE", ",", "\n", "self", ".", "role", "is", "not", "None", "and", "(", "\"-\"", "+", "self", ".", "role", ")", "or", "\"\"", ",", "\n", "self", ".", "relation", "is", "not", "None", "and", "(", "\"-\"", "+", "str", "(", "self", ".", "relation", ")", ")", "or", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__eq__": [[549, 551], ["id", "id"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "chunk", ")", ":", "\n", "        ", "return", "id", "(", "self", ")", "==", "id", "(", "chunk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.__ne__": [[552, 554], ["id", "id"], "methods", ["None"], ["", "def", "__ne__", "(", "self", ",", "chunk", ")", ":", "\n", "        ", "return", "id", "(", "self", ")", "!=", "id", "(", "chunk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chink.__repr__": [[565, 567], ["Chunk.__repr__().replace", "tree.Chunk.__repr__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.conv_tbc.ConvTBC.__repr__"], ["    ", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "Chunk", ".", "__repr__", "(", "self", ")", ".", "replace", "(", "\"Chunk(\"", ",", "\"Chink(\"", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.PNPChunk.__init__": [[573, 584], ["tree.Chunk.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" A chunk of chunks that make up a prepositional noun phrase (i.e., PP + NP).\n            When the output of the parser includes PP-attachment,\n            PNPChunck.anchor will yield the chunk that is clarified by the preposition.\n            For example: \"the cat went [for the mouse] [with its claws]\":\n            - [went] what? => for the mouse,\n            - [went] how? => with its claws.\n        \"\"\"", "\n", "self", ".", "anchor", "=", "None", "# The anchor chunk (e.g., \"for the mouse\" => \"went\").", "\n", "self", ".", "chunks", "=", "[", "]", "# List of chunks in the prepositional noun phrase.", "\n", "Chunk", ".", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.PNPChunk.append": [[585, 592], ["tree.PNPChunk.words.append", "tree.PNPChunk.chunks.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "append", "(", "self", ",", "word", ")", ":", "\n", "        ", "self", ".", "words", ".", "append", "(", "word", ")", "\n", "word", ".", "pnp", "=", "self", "\n", "if", "word", ".", "chunk", "is", "not", "None", ":", "\n", "            ", "word", ".", "chunk", ".", "pnp", "=", "self", "\n", "if", "word", ".", "chunk", "not", "in", "self", ".", "chunks", ":", "\n", "                ", "self", ".", "chunks", ".", "append", "(", "word", ".", "chunk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.PNPChunk.preposition": [[593, 599], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "preposition", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the first chunk in the prepositional noun phrase, usually a PP-chunk.\n            PP-chunks contain words such as \"for\", \"with\", \"in\", ...\n        \"\"\"", "\n", "return", "self", ".", "chunks", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.PNPChunk.phrases": [[602, 605], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "phrases", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.PNPChunk.guess_anchor": [[606, 611], ["tree.PNPChunk.nearest"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.nearest"], ["", "def", "guess_anchor", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns an anchor chunk for this prepositional noun phrase (without a PP-attacher).\n            Often, the nearest verb phrase is a good candidate.\n        \"\"\"", "\n", "return", "self", ".", "nearest", "(", "\"VP\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Conjunctions.__init__": [[620, 625], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "chunk", ")", ":", "\n", "        ", "\"\"\" Chunk.conjunctions is a list of other chunks participating in a conjunction.\n            Each item in the list is a (chunk, conjunction)-tuple, with conjunction either AND or OR.\n        \"\"\"", "\n", "self", ".", "anchor", "=", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Conjunctions.append": [[626, 628], ["list.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "append", "(", "self", ",", "chunk", ",", "type", "=", "CONJUNCT", ")", ":", "\n", "        ", "list", ".", "append", "(", "self", ",", "(", "chunk", ",", "type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__init__": [[649, 682], ["tree._is_tokenstring", "tree._uid", "list", "string.decode.decode.split", "isinstance", "getattr", "tree.Sentence.append", "string.decode.decode.decode", "tree.Sentence.parse_token"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._is_tokenstring", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._uid", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.parse_token"], ["    ", "def", "__init__", "(", "self", ",", "string", "=", "\"\"", ",", "token", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ",", "language", "=", "\"en\"", ")", ":", "\n", "        ", "\"\"\" A nested tree of sentence words, chunks and prepositions.\n            The input is a tagged string from parse(). \n            The order in which token tags appear can be specified.\n        \"\"\"", "\n", "# Extract token format from TokenString or TaggedString if possible.", "\n", "if", "_is_tokenstring", "(", "string", ")", ":", "\n", "            ", "token", ",", "language", "=", "string", ".", "tags", ",", "getattr", "(", "string", ",", "\"language\"", ",", "language", ")", "\n", "# Convert to Unicode.", "\n", "", "if", "not", "isinstance", "(", "string", ",", "str", ")", ":", "\n", "            ", "for", "encoding", "in", "(", "(", "\"utf-8\"", ",", ")", ",", "(", "\"windows-1252\"", ",", ")", ",", "(", "\"utf-8\"", ",", "\"ignore\"", ")", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "string", "=", "string", ".", "decode", "(", "*", "encoding", ")", "\n", "", "except", ":", "\n", "                    ", "pass", "\n", "", "", "", "self", ".", "parent", "=", "None", "# A Slice refers to the Sentence it is part of.", "\n", "self", ".", "text", "=", "None", "# A Sentence refers to the Text it is part of.", "\n", "self", ".", "language", "=", "language", "\n", "self", ".", "id", "=", "_uid", "(", ")", "\n", "self", ".", "token", "=", "list", "(", "token", ")", "\n", "self", ".", "words", "=", "[", "]", "\n", "self", ".", "chunks", "=", "[", "]", "# Words grouped into chunks.", "\n", "self", ".", "pnp", "=", "[", "]", "# Words grouped into PNP chunks.", "\n", "self", ".", "_anchors", "=", "{", "}", "# Anchor tags related to anchor chunks or attached PNP's.", "\n", "self", ".", "_relation", "=", "None", "# Helper variable: the last chunk's relation and role.", "\n", "self", ".", "_attachment", "=", "None", "# Helper variable: the last attachment tag (e.g., \"P1\") parsed in _do_pnp().", "\n", "self", ".", "_previous", "=", "None", "# Helper variable: the last token parsed in parse_token().", "\n", "self", ".", "relations", "=", "{", "\"SBJ\"", ":", "{", "}", ",", "\"OBJ\"", ":", "{", "}", ",", "\"VP\"", ":", "{", "}", "}", "\n", "# Split the slash-formatted token into the separate tags in the given order.", "\n", "# Append Word and Chunk objects according to the token's tags.", "\n", "for", "chars", "in", "string", ".", "split", "(", "\" \"", ")", ":", "\n", "            ", "if", "chars", ":", "\n", "                ", "self", ".", "append", "(", "*", "self", ".", "parse_token", "(", "chars", ",", "token", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.word": [[683, 686], ["None"], "methods", ["None"], ["", "", "", "@", "property", "\n", "def", "word", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.lemmata": [[687, 690], ["tree.Map"], "methods", ["None"], ["", "@", "property", "\n", "def", "lemmata", "(", "self", ")", ":", "\n", "        ", "return", "Map", "(", "lambda", "w", ":", "w", ".", "lemma", ",", "self", ".", "words", ")", "\n", "#return [word.lemma for word in self.words]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.parts_of_speech": [[694, 697], ["tree.Map"], "methods", ["None"], ["@", "property", "\n", "def", "parts_of_speech", "(", "self", ")", ":", "\n", "        ", "return", "Map", "(", "lambda", "w", ":", "w", ".", "type", ",", "self", ".", "words", ")", "\n", "#return [word.type for word in self.words]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.tagged": [[701, 704], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "tagged", "(", "self", ")", ":", "\n", "        ", "return", "[", "(", "word", ".", "string", ",", "word", ".", "type", ")", "for", "word", "in", "self", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.phrases": [[705, 708], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "phrases", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.prepositional_phrases": [[711, 714], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "prepositional_phrases", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pnp", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.start": [[717, 720], ["None"], "methods", ["None"], ["@", "property", "\n", "def", "start", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.stop": [[721, 724], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "stop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "start", "+", "len", "(", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.nouns": [[725, 728], ["word.type.startswith"], "methods", ["None"], ["", "@", "property", "\n", "def", "nouns", "(", "self", ")", ":", "\n", "        ", "return", "[", "word", "for", "word", "in", "self", "if", "word", ".", "type", ".", "startswith", "(", "\"NN\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.verbs": [[745, 748], ["list", "tree.Sentence.relations[].values"], "methods", ["None"], ["", "@", "property", "\n", "def", "verbs", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "relations", "[", "\"VP\"", "]", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.adjectives": [[733, 736], ["word.type.startswith"], "methods", ["None"], ["", "@", "property", "\n", "def", "adjectives", "(", "self", ")", ":", "\n", "        ", "return", "[", "word", "for", "word", "in", "self", "if", "word", ".", "type", ".", "startswith", "(", "\"JJ\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.subjects": [[737, 740], ["list", "tree.Sentence.relations[].values"], "methods", ["None"], ["", "@", "property", "\n", "def", "subjects", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "relations", "[", "\"SBJ\"", "]", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.objects": [[741, 744], ["list", "tree.Sentence.relations[].values"], "methods", ["None"], ["", "@", "property", "\n", "def", "objects", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "relations", "[", "\"OBJ\"", "]", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.anchors": [[749, 752], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "anchors", "(", "self", ")", ":", "\n", "        ", "return", "[", "chunk", "for", "chunk", "in", "self", ".", "chunks", "if", "len", "(", "chunk", ".", "attachments", ")", ">", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.is_question": [[753, 756], ["len", "builtins.str"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_question", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ")", ">", "0", "and", "str", "(", "self", "[", "-", "1", "]", ")", "==", "\"?\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.is_exclamation": [[757, 760], ["len", "builtins.str"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_exclamation", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ")", ">", "0", "and", "str", "(", "self", "[", "-", "1", "]", ")", "==", "\"!\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__getitem__": [[761, 763], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "words", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__len__": [[764, 766], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__iter__": [[767, 769], ["tree.Sentence.words.__iter__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__iter__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "words", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.append": [[770, 791], ["tree.Sentence._do_word", "tree.Sentence._do_chunk", "tree.Sentence._do_conjunction", "tree.Sentence._do_relation", "tree.Sentence._do_pnp", "tree.Sentence._do_anchor", "tree.Sentence._do_custom"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_word", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_chunk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_conjunction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_relation", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_pnp", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_anchor", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_custom"], ["", "def", "append", "(", "self", ",", "word", ",", "lemma", "=", "None", ",", "type", "=", "None", ",", "chunk", "=", "None", ",", "role", "=", "None", ",", "relation", "=", "None", ",", "pnp", "=", "None", ",", "anchor", "=", "None", ",", "iob", "=", "None", ",", "custom", "=", "{", "}", ")", ":", "\n", "        ", "\"\"\" Appends the next word to the sentence / chunk / preposition.\n            For example: Sentence.append(\"clawed\", \"claw\", \"VB\", \"VP\", role=None, relation=1)\n            - word     : the current word,\n            - lemma    : the canonical form of the word,\n            - type     : part-of-speech tag for the word (NN, JJ, ...),\n            - chunk    : part-of-speech tag for the chunk this word is part of (NP, VP, ...),\n            - role     : the chunk's grammatical role (SBJ, OBJ, ...),\n            - relation : an id shared by other related chunks (e.g., SBJ-1 <=> VP-1),\n            - pnp      : PNP if this word is in a prepositional noun phrase (B- prefix optional),\n            - iob      : BEGIN if the word marks the start of a new chunk,\n                         INSIDE (optional) if the word is part of the previous chunk,\n            - custom   : a dictionary of (tag, value)-items for user-defined word tags.\n        \"\"\"", "\n", "self", ".", "_do_word", "(", "word", ",", "lemma", ",", "type", ")", "# Append Word object.", "\n", "self", ".", "_do_chunk", "(", "chunk", ",", "role", ",", "relation", ",", "iob", ")", "# Append Chunk, or add last word to last chunk.", "\n", "self", ".", "_do_conjunction", "(", ")", "\n", "self", ".", "_do_relation", "(", ")", "\n", "self", ".", "_do_pnp", "(", "pnp", ",", "anchor", ")", "\n", "self", ".", "_do_anchor", "(", "anchor", ")", "\n", "self", ".", "_do_custom", "(", "custom", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.parse_token": [[792, 861], ["builtins.zip", "token.split", "ch.startswith", "tree.Sentence._parse_relation", "v.replace.replace.replace"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_relation"], ["", "def", "parse_token", "(", "self", ",", "token", ",", "tags", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", ":", "\n", "        ", "\"\"\" Returns the arguments for Sentence.append() from a tagged token representation.\n            The order in which token tags appear can be specified.\n            The default order is (separated by slashes): \n            - word, \n            - part-of-speech, \n            - (IOB-)chunk, \n            - (IOB-)preposition, \n            - chunk(-relation)(-role), \n            - anchor, \n            - lemma.\n            Examples:\n            The/DT/B-NP/O/NP-SBJ-1/O/the\n            cats/NNS/I-NP/O/NP-SBJ-1/O/cat\n            clawed/VBD/B-VP/O/VP-1/A1/claw\n            at/IN/B-PP/B-PNP/PP/P1/at\n            the/DT/B-NP/I-PNP/NP/P1/the\n            sofa/NN/I-NP/I-PNP/NP/P1/sofa\n            ././O/O/O/O/.\n            Returns a (word, lemma, type, chunk, role, relation, preposition, anchor, iob, custom)-tuple,\n            which can be passed to Sentence.append(): Sentence.append(*Sentence.parse_token(\"cats/NNS/NP\"))\n            The custom value is a dictionary of (tag, value)-items of unrecognized tags in the token.\n        \"\"\"", "\n", "p", "=", "{", "WORD", ":", "\"\"", ",", "\n", "POS", ":", "None", ",", "\n", "IOB", ":", "None", ",", "\n", "CHUNK", ":", "None", ",", "\n", "PNP", ":", "None", ",", "\n", "REL", ":", "None", ",", "\n", "ROLE", ":", "None", ",", "\n", "ANCHOR", ":", "None", ",", "\n", "LEMMA", ":", "None", "}", "\n", "# Split the slash-formatted token into separate tags in the given order.", "\n", "# Decode &slash; characters (usually in words and lemmata).", "\n", "# Assume None for missing tags (except the word itself, which defaults to an empty string).", "\n", "custom", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "zip", "(", "tags", ",", "token", ".", "split", "(", "\"/\"", ")", ")", ":", "\n", "            ", "if", "SLASH0", "in", "v", ":", "\n", "                ", "v", "=", "v", ".", "replace", "(", "SLASH", ",", "\"/\"", ")", "\n", "", "if", "k", "==", "\"pos\"", ":", "\n", "                ", "k", "=", "POS", "\n", "", "if", "k", "not", "in", "p", ":", "\n", "                ", "custom", "[", "k", "]", "=", "None", "\n", "", "if", "v", "!=", "OUTSIDE", "or", "k", "==", "WORD", "or", "k", "==", "LEMMA", ":", "# \"type O negative\" => \"O\" != OUTSIDE.", "\n", "                ", "(", "p", "if", "k", "not", "in", "custom", "else", "custom", ")", "[", "k", "]", "=", "v", "\n", "# Split IOB-prefix from the chunk tag:", "\n", "# B- marks the start of a new chunk,", "\n", "# I- marks inside of a chunk.", "\n", "", "", "ch", "=", "p", "[", "CHUNK", "]", "\n", "if", "ch", "is", "not", "None", "and", "ch", ".", "startswith", "(", "(", "\"B-\"", ",", "\"I-\"", ")", ")", ":", "\n", "            ", "p", "[", "IOB", "]", ",", "p", "[", "CHUNK", "]", "=", "ch", "[", ":", "1", "]", ",", "ch", "[", "2", ":", "]", "# B-NP", "\n", "# Split the role from the relation:", "\n", "# NP-SBJ-1 => relation id is 1 and role is SBJ,", "\n", "# VP-1 => relation id is 1 with no role.", "\n", "# Tokens may be tagged with multiple relations (e.g., NP-OBJ-1*NP-OBJ-3).", "\n", "", "if", "p", "[", "REL", "]", "is", "not", "None", ":", "\n", "            ", "ch", ",", "p", "[", "REL", "]", ",", "p", "[", "ROLE", "]", "=", "self", ".", "_parse_relation", "(", "p", "[", "REL", "]", ")", "\n", "# Infer a missing chunk tag from the relation tag (e.g., NP-SBJ-1 => NP).", "\n", "# For PP relation tags (e.g., PP-CLR-1), the first chunk is PP, the following chunks NP.", "\n", "if", "ch", "==", "\"PP\"", "and", "self", ".", "_previous", "and", "self", ".", "_previous", "[", "REL", "]", "==", "p", "[", "REL", "]", "and", "self", ".", "_previous", "[", "ROLE", "]", "==", "p", "[", "ROLE", "]", ":", "\n", "                ", "ch", "=", "\"NP\"", "\n", "", "if", "p", "[", "CHUNK", "]", "is", "None", "and", "ch", "!=", "OUTSIDE", ":", "\n", "                ", "p", "[", "CHUNK", "]", "=", "ch", "\n", "", "", "self", ".", "_previous", "=", "p", "\n", "# Return the tags in the right order for Sentence.append().", "\n", "return", "p", "[", "WORD", "]", ",", "p", "[", "LEMMA", "]", ",", "p", "[", "POS", "]", ",", "p", "[", "CHUNK", "]", ",", "p", "[", "ROLE", "]", ",", "p", "[", "REL", "]", ",", "p", "[", "PNP", "]", ",", "p", "[", "ANCHOR", "]", ",", "p", "[", "IOB", "]", ",", "custom", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._parse_relation": [[862, 903], ["tag.split.split.replace", "tag.split.split.split", "s.split.split.split", "len", "id.startswith", "relation.append", "role.append", "relation.append", "role.append", "id.isdigit", "tag.split.split.split", "builtins.int"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "_parse_relation", "(", "self", ",", "tag", ")", ":", "\n", "        ", "\"\"\" Parses the chunk tag, role and relation id from the token relation tag.\n            - VP                => VP, [], []\n            - VP-1              => VP, [1], [None]\n            - ADJP-PRD          => ADJP, [None], [PRD]\n            - NP-SBJ-1          => NP, [1], [SBJ]\n            - NP-OBJ-1*NP-OBJ-2 => NP, [1,2], [OBJ,OBJ]\n            - NP-SBJ;NP-OBJ-1   => NP, [1,1], [SBJ,OBJ]\n        \"\"\"", "\n", "chunk", ",", "relation", ",", "role", "=", "None", ",", "[", "]", ",", "[", "]", "\n", "if", "\";\"", "in", "tag", ":", "\n", "# NP-SBJ;NP-OBJ-1 => 1 relates to both SBJ and OBJ.", "\n", "            ", "id", "=", "tag", ".", "split", "(", "\"*\"", ")", "[", "0", "]", "[", "-", "2", ":", "]", "\n", "id", "=", "id", "if", "id", ".", "startswith", "(", "\"-\"", ")", "else", "\"\"", "\n", "tag", "=", "tag", ".", "replace", "(", "\";\"", ",", "id", "+", "\"*\"", ")", "\n", "", "if", "\"*\"", "in", "tag", ":", "\n", "            ", "tag", "=", "tag", ".", "split", "(", "\"*\"", ")", "\n", "", "else", ":", "\n", "            ", "tag", "=", "[", "tag", "]", "\n", "", "for", "s", "in", "tag", ":", "\n", "            ", "s", "=", "s", ".", "split", "(", "\"-\"", ")", "\n", "n", "=", "len", "(", "s", ")", "\n", "if", "n", "==", "1", ":", "\n", "                ", "chunk", "=", "s", "[", "0", "]", "\n", "", "if", "n", "==", "2", ":", "\n", "                ", "chunk", "=", "s", "[", "0", "]", "\n", "relation", ".", "append", "(", "s", "[", "1", "]", ")", "\n", "role", ".", "append", "(", "None", ")", "\n", "", "if", "n", ">=", "3", ":", "\n", "                ", "chunk", "=", "s", "[", "0", "]", "\n", "relation", ".", "append", "(", "s", "[", "2", "]", ")", "\n", "role", ".", "append", "(", "s", "[", "1", "]", ")", "\n", "", "if", "n", ">", "1", ":", "\n", "                ", "id", "=", "relation", "[", "-", "1", "]", "\n", "if", "id", ".", "isdigit", "(", ")", ":", "\n", "                    ", "relation", "[", "-", "1", "]", "=", "int", "(", "id", ")", "\n", "", "else", ":", "\n", "# Correct \"ADJP-PRD\":", "\n", "# (ADJP, [PRD], [None]) => (ADJP, [None], [PRD])", "\n", "                    ", "relation", "[", "-", "1", "]", ",", "role", "[", "-", "1", "]", "=", "None", ",", "id", "\n", "", "", "", "return", "chunk", ",", "relation", ",", "role", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_word": [[904, 912], ["tree.Sentence.words.append", "tree.Word", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "_do_word", "(", "self", ",", "word", ",", "lemma", "=", "None", ",", "type", "=", "None", ")", ":", "\n", "        ", "\"\"\" Adds a new Word to the sentence.\n            Other Sentence._do_[tag] functions assume a new word has just been appended.\n        \"\"\"", "\n", "# Improve 3rd person singular \"'s\" lemma to \"be\", e.g., as in \"he's fine\".", "\n", "if", "lemma", "==", "\"'s\"", "and", "type", "in", "(", "\"VB\"", ",", "\"VBZ\"", ")", ":", "\n", "            ", "lemma", "=", "\"be\"", "\n", "", "self", ".", "words", ".", "append", "(", "Word", "(", "self", ",", "word", ",", "lemma", ",", "type", ",", "index", "=", "len", "(", "self", ".", "words", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_chunk": [[913, 932], ["tree.Sentence.chunks[].append", "tree.Chunk", "tree.Sentence.chunks.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "_do_chunk", "(", "self", ",", "type", ",", "role", "=", "None", ",", "relation", "=", "None", ",", "iob", "=", "None", ")", ":", "\n", "        ", "\"\"\" Adds a new Chunk to the sentence, or adds the last word to the previous chunk.\n            The word is attached to the previous chunk if both type and relation match,\n            and if the word's chunk tag does not start with \"B-\" (i.e., iob != BEGIN).\n            Punctuation marks (or other \"O\" chunk tags) are not chunked.\n        \"\"\"", "\n", "if", "(", "type", "is", "None", "or", "type", "==", "OUTSIDE", ")", "and", "(", "role", "is", "None", "or", "role", "==", "OUTSIDE", ")", "and", "(", "relation", "is", "None", "or", "relation", "==", "OUTSIDE", ")", ":", "\n", "            ", "return", "\n", "", "if", "iob", "!=", "BEGIN", "and", "self", ".", "chunks", "and", "self", ".", "chunks", "[", "-", "1", "]", ".", "type", "==", "type", "and", "self", ".", "_relation", "==", "(", "relation", ",", "role", ")", "and", "self", ".", "words", "[", "-", "2", "]", ".", "chunk", "is", "not", "None", ":", "# \"one, two\" => \"one\" & \"two\" different chunks.", "\n", "            ", "self", ".", "chunks", "[", "-", "1", "]", ".", "append", "(", "self", ".", "words", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "ch", "=", "Chunk", "(", "self", ",", "[", "self", ".", "words", "[", "-", "1", "]", "]", ",", "type", ",", "role", ",", "relation", ")", "\n", "self", ".", "chunks", ".", "append", "(", "ch", ")", "\n", "self", ".", "_relation", "=", "(", "relation", ",", "role", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_relation": [[933, 944], ["None"], "methods", ["None"], ["", "", "def", "_do_relation", "(", "self", ")", ":", "\n", "        ", "\"\"\" Attaches subjects, objects and verbs.\n            If the previous chunk is a subject/object/verb, it is stored in Sentence.relations{}.\n        \"\"\"", "\n", "if", "self", ".", "chunks", ":", "\n", "            ", "ch", "=", "self", ".", "chunks", "[", "-", "1", "]", "\n", "for", "relation", ",", "role", "in", "ch", ".", "relations", ":", "\n", "                ", "if", "role", "==", "\"SBJ\"", "or", "role", "==", "\"OBJ\"", ":", "\n", "                    ", "self", ".", "relations", "[", "role", "]", "[", "relation", "]", "=", "ch", "\n", "", "", "if", "ch", ".", "type", "in", "(", "\"VP\"", ",", ")", ":", "\n", "                ", "self", ".", "relations", "[", "ch", ".", "type", "]", "[", "ch", ".", "relation", "]", "=", "ch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_pnp": [[945, 967], ["pnp.endswith", "tree.find", "tree.Sentence.pnp[].append", "pnp.startswith", "tree.Sentence.pnp[].append", "tree.PNPChunk", "tree.Sentence.pnp.append", "x.startswith"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "", "def", "_do_pnp", "(", "self", ",", "pnp", ",", "anchor", "=", "None", ")", ":", "\n", "        ", "\"\"\" Attaches prepositional noun phrases.\n            Identifies PNP's from either the PNP tag or the P-attachment tag.\n            This does not determine the PP-anchor, it only groups words in a PNP chunk.\n        \"\"\"", "\n", "if", "anchor", "or", "pnp", "and", "pnp", ".", "endswith", "(", "\"PNP\"", ")", ":", "\n", "            ", "if", "anchor", "is", "not", "None", ":", "\n", "                ", "m", "=", "find", "(", "lambda", "x", ":", "x", ".", "startswith", "(", "\"P\"", ")", ",", "anchor", ")", "\n", "", "else", ":", "\n", "                ", "m", "=", "None", "\n", "", "if", "self", ".", "pnp", "and", "pnp", "and", "pnp", "!=", "OUTSIDE", "and", "pnp", ".", "startswith", "(", "\"B-\"", ")", "is", "False", "and", "self", ".", "words", "[", "-", "2", "]", ".", "pnp", "is", "not", "None", ":", "\n", "                ", "self", ".", "pnp", "[", "-", "1", "]", ".", "append", "(", "self", ".", "words", "[", "-", "1", "]", ")", "\n", "", "elif", "m", "is", "not", "None", "and", "m", "==", "self", ".", "_attachment", ":", "\n", "                ", "self", ".", "pnp", "[", "-", "1", "]", ".", "append", "(", "self", ".", "words", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "ch", "=", "PNPChunk", "(", "self", ",", "[", "self", ".", "words", "[", "-", "1", "]", "]", ",", "type", "=", "\"PNP\"", ")", "\n", "self", ".", "pnp", ".", "append", "(", "ch", ")", "\n", "", "self", ".", "_attachment", "=", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_anchor": [[968, 985], ["anchor.split", "x.startswith", "x.startswith", "pnp.anchor.attachments.append", "len", "x.replace", "len", "x.replace"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "_do_anchor", "(", "self", ",", "anchor", ")", ":", "\n", "        ", "\"\"\" Collects preposition anchors and attachments in a dictionary.\n            Once the dictionary has an entry for both the anchor and the attachment, they are linked.\n        \"\"\"", "\n", "if", "anchor", ":", "\n", "            ", "for", "x", "in", "anchor", ".", "split", "(", "\"-\"", ")", ":", "\n", "                ", "A", ",", "P", "=", "None", ",", "None", "\n", "if", "x", ".", "startswith", "(", "\"A\"", ")", "and", "len", "(", "self", ".", "chunks", ")", ">", "0", ":", "# anchor", "\n", "                    ", "A", ",", "P", "=", "x", ",", "x", ".", "replace", "(", "\"A\"", ",", "\"P\"", ")", "\n", "self", ".", "_anchors", "[", "A", "]", "=", "self", ".", "chunks", "[", "-", "1", "]", "\n", "", "if", "x", ".", "startswith", "(", "\"P\"", ")", "and", "len", "(", "self", ".", "pnp", ")", ">", "0", ":", "# attachment (PNP)", "\n", "                    ", "A", ",", "P", "=", "x", ".", "replace", "(", "\"P\"", ",", "\"A\"", ")", ",", "x", "\n", "self", ".", "_anchors", "[", "P", "]", "=", "self", ".", "pnp", "[", "-", "1", "]", "\n", "", "if", "A", "in", "self", ".", "_anchors", "and", "P", "in", "self", ".", "_anchors", "and", "not", "self", ".", "_anchors", "[", "P", "]", ".", "anchor", ":", "\n", "                    ", "pnp", "=", "self", ".", "_anchors", "[", "P", "]", "\n", "pnp", ".", "anchor", "=", "self", ".", "_anchors", "[", "A", "]", "\n", "pnp", ".", "anchor", ".", "attachments", ".", "append", "(", "pnp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_custom": [[986, 992], ["tree.Sentence.words[].custom_tags.update"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update"], ["", "", "", "", "def", "_do_custom", "(", "self", ",", "custom", ")", ":", "\n", "        ", "\"\"\" Adds the user-defined tags to the last word.\n            Custom tags can be used to add extra semantical meaning or metadata to words.\n        \"\"\"", "\n", "if", "custom", ":", "\n", "            ", "self", ".", "words", "[", "-", "1", "]", ".", "custom_tags", ".", "update", "(", "custom", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence._do_conjunction": [[993, 1006], ["len", "ch1.conjunctions.append", "ch2.conjunctions.append", "w[].string.lower"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "_do_conjunction", "(", "self", ",", "_and", "=", "(", "\"and\"", ",", "\"e\"", ",", "\"en\"", ",", "\"et\"", ",", "\"und\"", ",", "\"y\"", ")", ")", ":", "\n", "        ", "\"\"\" Attach conjunctions.\n            CC-words like \"and\" and \"or\" between two chunks indicate a conjunction.\n        \"\"\"", "\n", "w", "=", "self", ".", "words", "\n", "if", "len", "(", "w", ")", ">", "2", "and", "w", "[", "-", "2", "]", ".", "type", "==", "\"CC\"", "and", "w", "[", "-", "2", "]", ".", "chunk", "is", "None", ":", "\n", "            ", "cc", "=", "w", "[", "-", "2", "]", ".", "string", ".", "lower", "(", ")", "in", "_and", "and", "AND", "or", "OR", "\n", "ch1", "=", "w", "[", "-", "3", "]", ".", "chunk", "\n", "ch2", "=", "w", "[", "-", "1", "]", ".", "chunk", "\n", "if", "ch1", "is", "not", "None", "and", "ch2", "is", "not", "None", ":", "\n", "                ", "ch1", ".", "conjunctions", ".", "append", "(", "ch2", ",", "cc", ")", "\n", "ch2", ".", "conjunctions", ".", "append", "(", "ch1", ",", "cc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.get": [[1007, 1033], ["None"], "methods", ["None"], ["", "", "", "def", "get", "(", "self", ",", "index", ",", "tag", "=", "LEMMA", ")", ":", "\n", "        ", "\"\"\" Returns a tag for the word at the given index.\n            The tag can be WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.\n        \"\"\"", "\n", "if", "tag", "==", "WORD", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", "\n", "", "if", "tag", "==", "LEMMA", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", ".", "lemma", "\n", "", "if", "tag", "==", "POS", "or", "tag", "==", "\"pos\"", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", ".", "type", "\n", "", "if", "tag", "==", "CHUNK", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", ".", "chunk", "\n", "", "if", "tag", "==", "PNP", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", ".", "pnp", "\n", "", "if", "tag", "==", "REL", ":", "\n", "            ", "ch", "=", "self", ".", "words", "[", "index", "]", ".", "chunk", "\n", "return", "ch", "and", "ch", ".", "relation", "\n", "", "if", "tag", "==", "ROLE", ":", "\n", "            ", "ch", "=", "self", ".", "words", "[", "index", "]", ".", "chunk", "\n", "return", "ch", "and", "ch", ".", "role", "\n", "", "if", "tag", "==", "ANCHOR", ":", "\n", "            ", "ch", "=", "self", ".", "words", "[", "index", "]", ".", "pnp", "\n", "return", "ch", "and", "ch", ".", "anchor", "\n", "", "if", "tag", "in", "self", ".", "words", "[", "index", "]", ".", "custom_tags", ":", "\n", "            ", "return", "self", ".", "words", "[", "index", "]", ".", "custom_tags", "[", "tag", "]", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.loop": [[1034, 1042], ["builtins.range", "len", "tuple", "tree.Sentence.get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "loop", "(", "self", ",", "*", "tags", ")", ":", "\n", "        ", "\"\"\" Iterates over the tags in the entire Sentence,\n            For example, Sentence.loop(POS, LEMMA) yields tuples of the part-of-speech tags and lemmata. \n            Possible tags: WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.\n            Any order or combination of tags can be supplied.\n        \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "words", ")", ")", ":", "\n", "            ", "yield", "tuple", "(", "[", "self", ".", "get", "(", "i", ",", "tag", "=", "tag", ")", "for", "tag", "in", "tags", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.indexof": [[1043, 1056], ["builtins.range", "len", "match", "tree.Sentence.get", "indices.append", "a.endswith", "b.startswith"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "indexof", "(", "self", ",", "value", ",", "tag", "=", "WORD", ")", ":", "\n", "        ", "\"\"\" Returns the indices of tokens in the sentence where the given token tag equals the string.\n            The string can contain a wildcard \"*\" at the end (this way \"NN*\" will match \"NN\" and \"NNS\").\n            The tag can be WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.\n            For example: Sentence.indexof(\"VP\", tag=CHUNK) \n            returns the indices of all the words that are part of a VP chunk.\n        \"\"\"", "\n", "match", "=", "lambda", "a", ",", "b", ":", "a", ".", "endswith", "(", "\"*\"", ")", "and", "b", ".", "startswith", "(", "a", "[", ":", "-", "1", "]", ")", "or", "a", "==", "b", "\n", "indices", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "words", ")", ")", ":", "\n", "            ", "if", "match", "(", "value", ",", "self", ".", "get", "(", "i", ",", "tag", ")", ")", ":", "\n", "                ", "indices", ".", "append", "(", "i", ")", "\n", "", "", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.slice": [[1057, 1090], ["tree.Slice", "enumerate", "tree.Sentence.append", "tree.unzip", "tree.unzip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip"], ["", "def", "slice", "(", "self", ",", "start", ",", "stop", ")", ":", "\n", "        ", "\"\"\" Returns a portion of the sentence from word start index to word stop index.\n            The returned slice is a subclass of Sentence and a deep copy.\n        \"\"\"", "\n", "s", "=", "Slice", "(", "token", "=", "self", ".", "token", ",", "language", "=", "self", ".", "language", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "self", ".", "words", "[", "start", ":", "stop", "]", ")", ":", "\n", "# The easiest way to copy (part of) a sentence", "\n", "# is by unpacking all of the token tags and passing them to Sentence.append().", "\n", "            ", "p0", "=", "word", ".", "string", "# WORD", "\n", "p1", "=", "word", ".", "lemma", "# LEMMA", "\n", "p2", "=", "word", ".", "type", "# POS", "\n", "p3", "=", "word", ".", "chunk", "is", "not", "None", "and", "word", ".", "chunk", ".", "type", "or", "None", "# CHUNK", "\n", "p4", "=", "word", ".", "pnp", "is", "not", "None", "and", "\"PNP\"", "or", "None", "# PNP", "\n", "p5", "=", "word", ".", "chunk", "is", "not", "None", "and", "unzip", "(", "0", ",", "word", ".", "chunk", ".", "relations", ")", "or", "None", "# REL", "\n", "p6", "=", "word", ".", "chunk", "is", "not", "None", "and", "unzip", "(", "1", ",", "word", ".", "chunk", ".", "relations", ")", "or", "None", "# ROLE", "\n", "p7", "=", "word", ".", "chunk", "and", "word", ".", "chunk", ".", "anchor_id", "or", "None", "# ANCHOR", "\n", "p8", "=", "word", ".", "chunk", "and", "word", ".", "chunk", ".", "start", "==", "start", "+", "i", "and", "BEGIN", "or", "None", "# IOB", "\n", "p9", "=", "word", ".", "custom_tags", "# User-defined tags.", "\n", "# If the given range does not contain the chunk head, remove the chunk tags.", "\n", "if", "word", ".", "chunk", "is", "not", "None", "and", "(", "word", ".", "chunk", ".", "stop", ">", "stop", ")", ":", "\n", "                ", "p3", ",", "p4", ",", "p5", ",", "p6", ",", "p7", ",", "p8", "=", "None", ",", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "# If the word starts the preposition, add the IOB B-prefix (i.e., B-PNP).", "\n", "", "if", "word", ".", "pnp", "is", "not", "None", "and", "word", ".", "pnp", ".", "start", "==", "start", "+", "i", ":", "\n", "                ", "p4", "=", "BEGIN", "+", "\"-\"", "+", "\"PNP\"", "\n", "# If the given range does not contain the entire PNP, remove the PNP tags.", "\n", "# The range must contain the entire PNP,", "\n", "# since it starts with the PP and ends with the chunk head (and is meaningless without these).", "\n", "", "if", "word", ".", "pnp", "is", "not", "None", "and", "(", "word", ".", "pnp", ".", "start", "<", "start", "or", "word", ".", "chunk", ".", "stop", ">", "stop", ")", ":", "\n", "                ", "p4", ",", "p7", "=", "None", ",", "None", "\n", "", "s", ".", "append", "(", "word", "=", "p0", ",", "lemma", "=", "p1", ",", "type", "=", "p2", ",", "chunk", "=", "p3", ",", "pnp", "=", "p4", ",", "relation", "=", "p5", ",", "role", "=", "p6", ",", "anchor", "=", "p7", ",", "iob", "=", "p8", ",", "custom", "=", "p9", ")", "\n", "", "s", ".", "parent", "=", "self", "\n", "s", ".", "_start", "=", "start", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.copy": [[1091, 1093], ["tree.Sentence.slice", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.slice"], ["", "def", "copy", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "slice", "(", "0", ",", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.chunked": [[1094, 1096], ["tree.Sentence.chunked"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.chunked"], ["", "def", "chunked", "(", "self", ")", ":", "\n", "        ", "return", "chunked", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.constituents": [[1097, 1112], ["a.append", "a.append", "len", "a.append", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "constituents", "(", "self", ",", "pnp", "=", "False", ")", ":", "\n", "        ", "\"\"\" Returns an in-order list of mixed Chunk and Word objects.\n            With pnp=True, also contains PNPChunk objects whenever possible.\n        \"\"\"", "\n", "a", "=", "[", "]", "\n", "for", "word", "in", "self", ".", "words", ":", "\n", "            ", "if", "pnp", "and", "word", ".", "pnp", "is", "not", "None", ":", "\n", "                ", "if", "len", "(", "a", ")", "==", "0", "or", "a", "[", "-", "1", "]", "!=", "word", ".", "pnp", ":", "\n", "                    ", "a", ".", "append", "(", "word", ".", "pnp", ")", "\n", "", "", "elif", "word", ".", "chunk", "is", "not", "None", ":", "\n", "                ", "if", "len", "(", "a", ")", "==", "0", "or", "a", "[", "-", "1", "]", "!=", "word", ".", "chunk", ":", "\n", "                    ", "a", ".", "append", "(", "word", ".", "chunk", ")", "\n", "", "", "else", ":", "\n", "                ", "a", ".", "append", "(", "word", ")", "\n", "", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.string": [[1115, 1118], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "string", "(", "self", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "word", ".", "string", "for", "word", "in", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__str__": [[1119, 1121], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__repr__": [[1122, 1124], ["repr"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"Sentence(%s)\"", "%", "repr", "(", "\" \"", ".", "join", "(", "[", "\"/\"", ".", "join", "(", "word", ".", "tags", ")", "for", "word", "in", "self", ".", "words", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.__eq__": [[1125, 1129], ["isinstance", "len", "len", "repr", "repr"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "other", ",", "Sentence", ")", ":", "\n", "            ", "return", "False", "\n", "", "return", "len", "(", "self", ")", "==", "len", "(", "other", ")", "and", "repr", "(", "self", ")", "==", "repr", "(", "other", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.xml": [[1135, 1140], ["tree.parse_xml"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.parse_xml"], ["@", "property", "\n", "def", "xml", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the sentence as an XML-formatted string (plain bytestring, UTF-8 encoded).\n        \"\"\"", "\n", "return", "parse_xml", "(", "self", ",", "tab", "=", "\"\\t\"", ",", "id", "=", "self", ".", "id", "or", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.from_xml": [[1141, 1147], ["tree.parse_string", "tree.Sentence", "parse_string.split"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.parse_string"], ["", "@", "classmethod", "\n", "def", "from_xml", "(", "cls", ",", "xml", ")", ":", "\n", "        ", "\"\"\" Returns a new Text from the given XML string.\n        \"\"\"", "\n", "s", "=", "parse_string", "(", "xml", ")", "\n", "return", "Sentence", "(", "s", ".", "split", "(", "\"\\n\"", ")", "[", "0", "]", ",", "token", "=", "s", ".", "tags", ",", "language", "=", "s", ".", "language", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Sentence.nltk_tree": [[1150, 1154], ["tree.Sentence.nltk_tree"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.nltk_tree"], ["def", "nltk_tree", "(", "self", ")", ":", "\n", "        ", "\"\"\" The sentence as an nltk.tree object.\n        \"\"\"", "\n", "return", "nltk_tree", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Slice.__init__": [[1158, 1163], ["kwargs.pop", "tree.Sentence.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" A portion of the sentence returned by Sentence.slice().\n        \"\"\"", "\n", "self", ".", "_start", "=", "kwargs", ".", "pop", "(", "\"start\"", ",", "0", ")", "\n", "Sentence", ".", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Slice.start": [[1164, 1167], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "start", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_start", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Slice.stop": [[1168, 1171], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "stop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_start", "+", "len", "(", "self", ".", "words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.__init__": [[1204, 1218], ["tree._is_tokenstring", "isinstance", "tree.Text.extend", "getattr", "string.splitlines.splitlines.splitlines", "tree.Sentence"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._is_tokenstring", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend"], ["    ", "def", "__init__", "(", "self", ",", "string", ",", "token", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ",", "language", "=", "\"en\"", ",", "encoding", "=", "\"utf-8\"", ")", ":", "\n", "        ", "\"\"\" A list of Sentence objects parsed from the given string.\n            The string is the Unicode return value from parse().\n        \"\"\"", "\n", "self", ".", "encoding", "=", "encoding", "\n", "# Extract token format from TokenString if possible.", "\n", "if", "_is_tokenstring", "(", "string", ")", ":", "\n", "            ", "token", ",", "language", "=", "string", ".", "tags", ",", "getattr", "(", "string", ",", "\"language\"", ",", "language", ")", "\n", "", "if", "string", ":", "\n", "# From a string.", "\n", "            ", "if", "isinstance", "(", "string", ",", "str", ")", ":", "\n", "                ", "string", "=", "string", ".", "splitlines", "(", ")", "\n", "# From an iterable (e.g., string.splitlines(), open('parsed.txt')).", "\n", "", "self", ".", "extend", "(", "Sentence", "(", "s", ",", "token", ",", "language", ")", "for", "s", "in", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert": [[1219, 1222], ["list.insert"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert"], ["", "", "def", "insert", "(", "self", ",", "index", ",", "sentence", ")", ":", "\n", "        ", "list", ".", "insert", "(", "self", ",", "index", ",", "sentence", ")", "\n", "sentence", ".", "text", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append": [[1223, 1226], ["list.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "append", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "list", ".", "append", "(", "self", ",", "sentence", ")", "\n", "sentence", ".", "text", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend": [[1227, 1231], ["list.extend"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend"], ["", "def", "extend", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "list", ".", "extend", "(", "self", ",", "sentences", ")", "\n", "for", "s", "in", "sentences", ":", "\n", "            ", "s", ".", "text", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove": [[1232, 1235], ["list.remove"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove"], ["", "", "def", "remove", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "list", ".", "remove", "(", "self", ",", "sentence", ")", "\n", "sentence", ".", "text", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop": [[1236, 1240], ["list.pop"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop"], ["", "def", "pop", "(", "self", ",", "index", ")", ":", "\n", "        ", "sentence", "=", "list", ".", "pop", "(", "self", ",", "index", ")", "\n", "sentence", ".", "text", "=", "None", "\n", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.sentences": [[1241, 1244], ["list"], "methods", ["None"], ["", "@", "property", "\n", "def", "sentences", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.words": [[1245, 1248], ["list", "itertools.chain"], "methods", ["None"], ["", "@", "property", "\n", "def", "words", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "chain", "(", "*", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.copy": [[1249, 1254], ["tree.Text", "tree.Text.append", "sentence.copy"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.copy"], ["", "def", "copy", "(", "self", ")", ":", "\n", "        ", "t", "=", "Text", "(", "\"\"", ",", "encoding", "=", "self", ".", "encoding", ")", "\n", "for", "sentence", "in", "self", ":", "\n", "            ", "t", ".", "append", "(", "sentence", ".", "copy", "(", ")", ")", "\n", "", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.string": [[1256, 1259], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "string", "(", "self", ")", ":", "\n", "        ", "return", "\"\\n\"", ".", "join", "(", "sentence", ".", "string", "for", "sentence", "in", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.__str__": [[1260, 1262], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.xml": [[1266, 1277], ["xml.append", "xml.append", "xml.extend", "xml.append", "XML_ENCODING.get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "@", "property", "\n", "def", "xml", "(", "self", ")", ":", "\n", "        ", "\"\"\" Yields the sentence as an XML-formatted string (plain bytestring, UTF-8 encoded).\n            All the sentences in the XML are wrapped in a <text> element.\n        \"\"\"", "\n", "xml", "=", "[", "]", "\n", "xml", ".", "append", "(", "'<?xml version=\"1.0\" encoding=\"%s\"?>'", "%", "XML_ENCODING", ".", "get", "(", "self", ".", "encoding", ",", "self", ".", "encoding", ")", ")", "\n", "xml", ".", "append", "(", "\"<%s>\"", "%", "XML_TEXT", ")", "\n", "xml", ".", "extend", "(", "[", "sentence", ".", "xml", "for", "sentence", "in", "self", "]", ")", "\n", "xml", ".", "append", "(", "\"</%s>\"", "%", "XML_TEXT", ")", "\n", "return", "\"\\n\"", ".", "join", "(", "xml", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.from_xml": [[1278, 1283], ["tree.Text", "tree.parse_string"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.parse_string"], ["", "@", "classmethod", "\n", "def", "from_xml", "(", "cls", ",", "xml", ")", ":", "\n", "        ", "\"\"\" Returns a new Text from the given XML string.\n        \"\"\"", "\n", "return", "Text", "(", "parse_string", "(", "xml", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XML.__init__": [[1490, 1493], ["cElementTree.fromstring"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "string", ")", ":", "\n", "        ", "from", "xml", ".", "etree", "import", "cElementTree", "\n", "self", ".", "root", "=", "cElementTree", ".", "fromstring", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XML.__call__": [[1494, 1498], ["tree.XMLNode", "tree.XMLNode", "tree.XML.root.findall"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "tag", ")", ":", "\n", "        ", "return", "self", ".", "root", ".", "tag", "==", "tag", "and", "[", "XMLNode", "(", "self", ".", "root", ")", "]", "or", "[", "XMLNode", "(", "e", ")", "for", "e", "in", "self", ".", "root", ".", "findall", "(", "tag", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.__init__": [[1501, 1503], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "element", ")", ":", "\n", "        ", "self", ".", "element", "=", "element", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.tag": [[1504, 1507], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tag", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "element", ".", "tag", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.value": [[1508, 1511], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "value", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "element", ".", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.__iter__": [[1512, 1514], ["iter", "tree.XMLNode"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "XMLNode", "(", "e", ")", "for", "e", "in", "self", ".", "element", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.__getitem__": [[1515, 1517], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "k", ")", ":", "\n", "        ", "return", "self", ".", "element", ".", "attrib", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get": [[1518, 1520], ["tree.XMLNode.element.attrib.get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "get", "(", "self", ",", "k", ",", "default", "=", "\"\"", ")", ":", "\n", "        ", "return", "self", ".", "element", ".", "attrib", ".", "get", "(", "k", ",", "default", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.TaggedString.__new__": [[1533, 1540], ["builtins.str.__new__", "list", "isinstance", "hasattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.TaggedString.__new__"], ["    ", "def", "__new__", "(", "cls", ",", "string", ",", "tags", "=", "[", "\"word\"", "]", ",", "language", "=", "\"en\"", ")", ":", "\n", "        ", "if", "isinstance", "(", "string", ",", "str", ")", "and", "hasattr", "(", "string", ",", "\"tags\"", ")", ":", "\n", "            ", "tags", ",", "language", "=", "string", ".", "tags", ",", "getattr", "(", "string", ",", "\"language\"", ",", "language", ")", "\n", "", "s", "=", "str", ".", "__new__", "(", "cls", ",", "string", ")", "\n", "s", ".", "tags", "=", "list", "(", "tags", ")", "\n", "s", ".", "language", "=", "language", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find": [[68, 74], ["function"], "function", ["None"], ["def", "find", "(", "function", ",", "iterable", ")", ":", "\n", "    ", "\"\"\" Returns the first item in the list for which function(item) is True, None otherwise.\n    \"\"\"", "\n", "for", "x", "in", "iterable", ":", "\n", "        ", "if", "function", "(", "x", ")", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.intersects": [[76, 80], ["tree.find"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find"], ["", "", "", "def", "intersects", "(", "iterable1", ",", "iterable2", ")", ":", "\n", "    ", "\"\"\" Returns True if the given lists have at least one item in common.\n    \"\"\"", "\n", "return", "find", "(", "lambda", "x", ":", "x", "in", "iterable1", ",", "iterable2", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unique": [[82, 87], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], ["", "def", "unique", "(", "iterable", ")", ":", "\n", "    ", "\"\"\" Returns a list copy in which each item occurs only once (in-order).\n    \"\"\"", "\n", "seen", "=", "set", "(", ")", "\n", "return", "[", "x", "for", "x", "in", "iterable", "if", "x", "not", "in", "seen", "and", "not", "seen", ".", "add", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip": [[91, 99], ["max", "kwargs.get", "list", "list", "builtins.map", "_zip", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["def", "zip", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Returns a list of tuples, where the i-th tuple contains the i-th element \n        from each of the argument sequences or iterables (or default if too short).\n    \"\"\"", "\n", "args", "=", "[", "list", "(", "iterable", ")", "for", "iterable", "in", "args", "]", "\n", "n", "=", "max", "(", "map", "(", "len", ",", "args", ")", ")", "\n", "v", "=", "kwargs", ".", "get", "(", "\"default\"", ",", "None", ")", "\n", "return", "list", "(", "_zip", "(", "*", "[", "i", "+", "[", "v", "]", "*", "(", "n", "-", "len", "(", "i", ")", ")", "for", "i", "in", "args", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip": [[101, 105], ["None"], "function", ["None"], ["", "def", "unzip", "(", "i", ",", "iterable", ")", ":", "\n", "    ", "\"\"\" Returns the item at the given index from inside each tuple in the list.\n    \"\"\"", "\n", "return", "[", "x", "[", "i", "]", "for", "x", "in", "iterable", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._uid": [[634, 638], ["None"], "function", ["None"], ["def", "_uid", "(", ")", ":", "\n", "    ", "global", "_UID", "\n", "_UID", "+=", "1", "\n", "return", "_UID", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._is_tokenstring": [[640, 645], ["isinstance", "hasattr"], "function", ["None"], ["", "def", "_is_tokenstring", "(", "string", ")", ":", "\n", "# The class mbsp.TokenString stores the format of tags for each token.", "\n", "# Since it comes directly from MBSP.parse(), this format is always correct,", "\n", "# regardless of the given token format parameter for Sentence() or Text().", "\n", "    ", "return", "isinstance", "(", "string", ",", "str", ")", "and", "hasattr", "(", "string", ",", "\"tags\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.chunked": [[1180, 1198], ["tree.Chink", "tree.Chunk.append", "chunks.append", "chunks.append", "word.copy", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.copy"], ["", "", "def", "chunked", "(", "sentence", ")", ":", "\n", "    ", "\"\"\" Returns a list of Chunk and Chink objects from the given sentence.\n        Chink is a subclass of Chunk used for words that have Word.chunk == None\n        (e.g., punctuation marks, conjunctions).\n    \"\"\"", "\n", "# For example, to construct a training vector with the head of previous chunks as a feature.", "\n", "# Doing this with Sentence.chunks would discard the punctuation marks and conjunctions", "\n", "# (Sentence.chunks only yields Chunk objects), which amy be useful features.", "\n", "chunks", "=", "[", "]", "\n", "for", "word", "in", "sentence", ":", "\n", "        ", "if", "word", ".", "chunk", "is", "not", "None", ":", "\n", "            ", "if", "len", "(", "chunks", ")", "==", "0", "or", "chunks", "[", "-", "1", "]", "!=", "word", ".", "chunk", ":", "\n", "                ", "chunks", ".", "append", "(", "word", ".", "chunk", ")", "\n", "", "", "else", ":", "\n", "            ", "ch", "=", "Chink", "(", "sentence", ")", "\n", "ch", ".", "append", "(", "word", ".", "copy", "(", "ch", ")", ")", "\n", "chunks", ".", "append", "(", "ch", ")", "\n", "", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.tree": [[1289, 1294], ["tree.Text"], "function", ["None"], ["def", "tree", "(", "string", ",", "token", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", ":", "\n", "    ", "\"\"\" Transforms the output of parse() into a Text object.\n        The token parameter lists the order of tags in each token in the input string.\n    \"\"\"", "\n", "return", "Text", "(", "string", ",", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml": [[1298, 1303], ["tree.Text"], "function", ["None"], ["def", "xml", "(", "string", ",", "token", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", ":", "\n", "    ", "\"\"\" Transforms the output of parse() into XML.\n        The token parameter lists the order of tags in each token in the input string.\n    \"\"\"", "\n", "return", "Text", "(", "string", ",", "token", ")", ".", "xml", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_encode": [[1337, 1346], ["string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace"], "function", ["None"], ["def", "xml_encode", "(", "string", ")", ":", "\n", "    ", "\"\"\" Returns the string with XML-safe special characters.\n    \"\"\"", "\n", "string", "=", "string", ".", "replace", "(", "\"&\"", ",", "\"&amp;\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"<\"", ",", "\"&lt;\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\">\"", ",", "\"&gt;\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"\\\"\"", ",", "\"&quot;\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "SLASH", ",", "\"/\"", ")", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_decode": [[1348, 1357], ["string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace", "string.replace.replace"], "function", ["None"], ["", "def", "xml_decode", "(", "string", ")", ":", "\n", "    ", "\"\"\" Returns the string with special characters decoded.\n    \"\"\"", "\n", "string", "=", "string", ".", "replace", "(", "\"&amp;\"", ",", "\"&\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"&lt;\"", ",", "\"<\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"&gt;\"", ",", "\">\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"&quot;\"", ",", "\"\\\"\"", ")", "\n", "string", "=", "string", ".", "replace", "(", "\"/\"", ",", "SLASH", ")", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.parse_xml": [[1365, 1474], ["xml.append", "xml.append", "xml.append", "xml.append", "push", "xml.append", "push", "xml.append", "push", "pop", "xml.append", "pop", "xml.append", "pop", "xml.append", "len", "tree.unzip", "tree.unzip", "len", "uid", "tree.xml_encode", "builtins.str", "builtins.str", "uid", "builtins.str", "anchors.get", "len", "len", "tree.unique", "tree.unique", "tree.xml_encode", "tree.xml_encode", "uid", "uid", "word.custom_tags.items"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unzip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_encode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unique", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.unique", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_encode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_encode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["def", "parse_xml", "(", "sentence", ",", "tab", "=", "\"\\t\"", ",", "id", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\" Returns the given Sentence object as an XML-string (plain bytestring, UTF-8 encoded).\n        The tab delimiter is used as indendation for nested elements.\n        The id can be used as a unique identifier per sentence for chunk id's and anchors.\n        For example: \"I eat pizza with a fork.\" =>\n\n        <sentence token=\"word, part-of-speech, chunk, preposition, relation, anchor, lemma\" language=\"en\">\n            <chunk type=\"NP\" relation=\"SBJ\" of=\"1\">\n                <word type=\"PRP\" lemma=\"i\">I</word>\n            </chunk>\n            <chunk type=\"VP\" relation=\"VP\" id=\"1\" anchor=\"A1\">\n                <word type=\"VBP\" lemma=\"eat\">eat</word>\n            </chunk>\n            <chunk type=\"NP\" relation=\"OBJ\" of=\"1\">\n                <word type=\"NN\" lemma=\"pizza\">pizza</word>\n            </chunk>\n            <chunk type=\"PNP\" of=\"A1\">\n                <chunk type=\"PP\">\n                    <word type=\"IN\" lemma=\"with\">with</word>\n                </chunk>\n                <chunk type=\"NP\">\n                    <word type=\"DT\" lemma=\"a\">a</word>\n                    <word type=\"NN\" lemma=\"fork\">fork</word>\n                </chunk>\n            </chunk>\n            <chink>\n                <word type=\".\" lemma=\".\">.</word>\n            </chink>\n        </sentence>\n    \"\"\"", "\n", "uid", "=", "lambda", "*", "parts", ":", "\"\"", ".", "join", "(", "[", "str", "(", "id", ")", ",", "_UID_SEPARATOR", "]", "+", "[", "str", "(", "x", ")", "for", "x", "in", "parts", "]", ")", ".", "lstrip", "(", "_UID_SEPARATOR", ")", "\n", "push", "=", "lambda", "indent", ":", "indent", "+", "tab", "# push() increases the indentation.", "\n", "pop", "=", "lambda", "indent", ":", "indent", "[", ":", "-", "len", "(", "tab", ")", "]", "# pop() decreases the indentation.", "\n", "indent", "=", "tab", "\n", "xml", "=", "[", "]", "\n", "# Start the sentence element:", "\n", "# <sentence token=\"word, part-of-speech, chunk, preposition, relation, anchor, lemma\">", "\n", "xml", ".", "append", "(", "'<%s%s %s=\"%s\" %s=\"%s\">'", "%", "(", "\n", "XML_SENTENCE", ",", "\n", "XML_ID", "and", "\" %s=\\\"%s\\\"\"", "%", "(", "XML_ID", ",", "str", "(", "id", ")", ")", "or", "\"\"", ",", "\n", "XML_TOKEN", ",", "\", \"", ".", "join", "(", "sentence", ".", "token", ")", ",", "\n", "XML_LANGUAGE", ",", "sentence", ".", "language", "\n", ")", ")", "\n", "# Collect chunks that are PNP anchors and assign id.", "\n", "anchors", "=", "{", "}", "\n", "for", "chunk", "in", "sentence", ".", "chunks", ":", "\n", "        ", "if", "chunk", ".", "attachments", ":", "\n", "            ", "anchors", "[", "chunk", ".", "start", "]", "=", "len", "(", "anchors", ")", "+", "1", "\n", "# Traverse all words in the sentence.", "\n", "", "", "for", "word", "in", "sentence", ".", "words", ":", "\n", "        ", "chunk", "=", "word", ".", "chunk", "\n", "pnp", "=", "word", ".", "chunk", "and", "word", ".", "chunk", ".", "pnp", "or", "None", "\n", "# Start the PNP element if the chunk is the first chunk in PNP:", "\n", "# <chunk type=\"PNP\" of=\"A1\">", "\n", "if", "pnp", "and", "pnp", ".", "start", "==", "chunk", ".", "start", "and", "pnp", ".", "start", "==", "word", ".", "index", ":", "\n", "            ", "a", "=", "pnp", ".", "anchor", "and", "' %s=\"%s\"'", "%", "(", "XML_OF", ",", "uid", "(", "\"A\"", ",", "anchors", ".", "get", "(", "pnp", ".", "anchor", ".", "start", ",", "\"\"", ")", ")", ")", "or", "\"\"", "\n", "xml", ".", "append", "(", "indent", "+", "'<%s %s=\"PNP\"%s>'", "%", "(", "XML_CHUNK", ",", "XML_TYPE", ",", "a", ")", ")", "\n", "indent", "=", "push", "(", "indent", ")", "\n", "# Start the chunk element if the word is the first word in the chunk:", "\n", "# <chunk type=\"VP\" relation=\"VP\" id=\"1\" anchor=\"A1\">", "\n", "", "if", "chunk", "and", "chunk", ".", "start", "==", "word", ".", "index", ":", "\n", "            ", "if", "chunk", ".", "relations", ":", "\n", "# Create the shortest possible attribute values for multiple relations,", "\n", "# e.g., [(1,\"OBJ\"),(2,\"OBJ\")]) => relation=\"OBJ\" id=\"1|2\"", "\n", "                ", "r1", "=", "unzip", "(", "0", ",", "chunk", ".", "relations", ")", "# Relation id's.", "\n", "r2", "=", "unzip", "(", "1", ",", "chunk", ".", "relations", ")", "# Relation roles.", "\n", "r1", "=", "[", "x", "is", "None", "and", "\"-\"", "or", "uid", "(", "x", ")", "for", "x", "in", "r1", "]", "\n", "r2", "=", "[", "x", "is", "None", "and", "\"-\"", "or", "x", "for", "x", "in", "r2", "]", "\n", "r1", "=", "not", "len", "(", "unique", "(", "r1", ")", ")", "==", "1", "and", "\"|\"", ".", "join", "(", "r1", ")", "or", "(", "r1", "+", "[", "None", "]", ")", "[", "0", "]", "\n", "r2", "=", "not", "len", "(", "unique", "(", "r2", ")", ")", "==", "1", "and", "\"|\"", ".", "join", "(", "r2", ")", "or", "(", "r2", "+", "[", "None", "]", ")", "[", "0", "]", "\n", "", "xml", ".", "append", "(", "indent", "+", "'<%s%s%s%s%s%s>'", "%", "(", "\n", "XML_CHUNK", ",", "\n", "chunk", ".", "type", "and", "' %s=\"%s\"'", "%", "(", "XML_TYPE", ",", "chunk", ".", "type", ")", "or", "\"\"", ",", "\n", "chunk", ".", "relations", "and", "chunk", ".", "role", "is", "not", "None", "and", "' %s=\"%s\"'", "%", "(", "XML_RELATION", ",", "r2", ")", "or", "\"\"", ",", "\n", "chunk", ".", "relation", "and", "chunk", ".", "type", "==", "\"VP\"", "and", "' %s=\"%s\"'", "%", "(", "XML_ID", ",", "uid", "(", "chunk", ".", "relation", ")", ")", "or", "\"\"", ",", "\n", "chunk", ".", "relation", "and", "chunk", ".", "type", "!=", "\"VP\"", "and", "' %s=\"%s\"'", "%", "(", "XML_OF", ",", "r1", ")", "or", "\"\"", ",", "\n", "chunk", ".", "attachments", "and", "' %s=\"%s\"'", "%", "(", "XML_ANCHOR", ",", "uid", "(", "\"A\"", ",", "anchors", "[", "chunk", ".", "start", "]", ")", ")", "or", "\"\"", "\n", ")", ")", "\n", "indent", "=", "push", "(", "indent", ")", "\n", "# Words outside of a chunk are wrapped in a <chink> tag:", "\n", "# <chink>", "\n", "", "if", "not", "chunk", ":", "\n", "            ", "xml", ".", "append", "(", "indent", "+", "'<%s>'", "%", "XML_CHINK", ")", "\n", "indent", "=", "push", "(", "indent", ")", "\n", "# Add the word element:", "\n", "# <word type=\"VBP\" lemma=\"eat\">eat</word>", "\n", "", "xml", ".", "append", "(", "indent", "+", "'<%s%s%s%s>%s</%s>'", "%", "(", "\n", "XML_WORD", ",", "\n", "word", ".", "type", "and", "' %s=\"%s\"'", "%", "(", "XML_TYPE", ",", "xml_encode", "(", "word", ".", "type", ")", ")", "or", "''", ",", "\n", "word", ".", "lemma", "and", "' %s=\"%s\"'", "%", "(", "XML_LEMMA", ",", "xml_encode", "(", "word", ".", "lemma", ")", ")", "or", "''", ",", "\n", "(", "\" \"", "+", "\" \"", ".", "join", "(", "[", "'%s=\"%s\"'", "%", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "word", ".", "custom_tags", ".", "items", "(", ")", "if", "v", "is", "not", "None", "]", ")", ")", ".", "rstrip", "(", ")", ",", "\n", "xml_encode", "(", "word", ".", "string", ")", ",", "\n", "XML_WORD", "\n", ")", ")", "\n", "if", "not", "chunk", ":", "\n", "# Close the <chink> element if outside of a chunk.", "\n", "            ", "indent", "=", "pop", "(", "indent", ")", "\n", "xml", ".", "append", "(", "indent", "+", "\"</%s>\"", "%", "XML_CHINK", ")", "\n", "", "if", "chunk", "and", "chunk", ".", "stop", "-", "1", "==", "word", ".", "index", ":", "\n", "# Close the <chunk> element if this is the last word in the chunk.", "\n", "            ", "indent", "=", "pop", "(", "indent", ")", "\n", "xml", ".", "append", "(", "indent", "+", "\"</%s>\"", "%", "XML_CHUNK", ")", "\n", "", "if", "pnp", "and", "pnp", ".", "stop", "-", "1", "==", "word", ".", "index", ":", "\n", "# Close the PNP element if this is the last word in the PNP.", "\n", "            ", "indent", "=", "pop", "(", "indent", ")", "\n", "xml", ".", "append", "(", "indent", "+", "\"</%s>\"", "%", "XML_CHUNK", ")", "\n", "", "", "xml", ".", "append", "(", "\"</%s>\"", "%", "XML_SENTENCE", ")", "\n", "# Return as a plain str.", "\n", "return", "\"\\n\"", ".", "join", "(", "xml", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.parse_string": [[1542, 1591], ["tree.XML", "XML.", "_anchors.clear", "_attachments.clear", "sentence.get", "sentence.get", "TokenString", "sentence.get.replace().split", "tokens.extend", "sorted", "string.strip", "tree.TaggedString", "tree._parse_tokens", "sentence.get.index", "A.keys", "enumerate", "len", "string.strip", "isinstance", "sentence.get.replace", "token[].strip", "token[].strip", "builtins.str", "builtins.str", "builtins.range", "len", "tree.xml", "tree.xml"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_tokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml"], ["", "", "def", "parse_string", "(", "xml", ")", ":", "\n", "    ", "\"\"\" Returns a slash-formatted string from the given XML representation.\n        The return value is a TokenString (for MBSP) or TaggedString (for Pattern).\n    \"\"\"", "\n", "string", "=", "\"\"", "\n", "# Traverse all the <sentence> elements in the XML.", "\n", "dom", "=", "XML", "(", "xml", ")", "\n", "for", "sentence", "in", "dom", "(", "XML_SENTENCE", ")", ":", "\n", "        ", "_anchors", ".", "clear", "(", ")", "# Populated by calling _parse_tokens().", "\n", "_attachments", ".", "clear", "(", ")", "# Populated by calling _parse_tokens().", "\n", "# Parse the language from <sentence language=\"\">.", "\n", "language", "=", "sentence", ".", "get", "(", "XML_LANGUAGE", ",", "\"en\"", ")", "\n", "# Parse the token tag format from <sentence token=\"\">.", "\n", "# This information is returned in TokenString.tags,", "\n", "# so the format and order of the token tags is retained when exporting/importing as XML.", "\n", "format", "=", "sentence", ".", "get", "(", "XML_TOKEN", ",", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", "\n", "format", "=", "not", "isinstance", "(", "format", ",", "str", ")", "and", "format", "or", "format", ".", "replace", "(", "\" \"", ",", "\"\"", ")", ".", "split", "(", "\",\"", ")", "\n", "# Traverse all <chunk> and <chink> elements in the sentence.", "\n", "# Find the <word> elements inside and create tokens.", "\n", "tokens", "=", "[", "]", "\n", "for", "chunk", "in", "sentence", ":", "\n", "            ", "tokens", ".", "extend", "(", "_parse_tokens", "(", "chunk", ",", "format", ")", ")", "\n", "# Attach PNP's to their anchors.", "\n", "# Keys in _anchors have linked anchor chunks (each chunk is a list of tokens).", "\n", "# The keys correspond to the keys in _attachments, which have linked PNP chunks.", "\n", "", "if", "ANCHOR", "in", "format", ":", "\n", "            ", "A", ",", "P", ",", "a", ",", "i", "=", "_anchors", ",", "_attachments", ",", "1", ",", "format", ".", "index", "(", "ANCHOR", ")", "\n", "for", "id", "in", "sorted", "(", "A", ".", "keys", "(", ")", ")", ":", "\n", "                ", "for", "token", "in", "A", "[", "id", "]", ":", "\n", "                    ", "token", "[", "i", "]", "+=", "\"-\"", "+", "\"-\"", ".", "join", "(", "[", "\"A\"", "+", "str", "(", "a", "+", "p", ")", "for", "p", "in", "range", "(", "len", "(", "P", "[", "id", "]", ")", ")", "]", ")", "\n", "token", "[", "i", "]", "=", "token", "[", "i", "]", ".", "strip", "(", "\"O-\"", ")", "\n", "", "for", "p", ",", "pnp", "in", "enumerate", "(", "P", "[", "id", "]", ")", ":", "\n", "                    ", "for", "token", "in", "pnp", ":", "\n", "                        ", "token", "[", "i", "]", "+=", "\"-\"", "+", "\"P\"", "+", "str", "(", "a", "+", "p", ")", "\n", "token", "[", "i", "]", "=", "token", "[", "i", "]", ".", "strip", "(", "\"O-\"", ")", "\n", "", "", "a", "+=", "len", "(", "P", "[", "id", "]", ")", "\n", "# Collapse the tokens to string.", "\n", "# Separate multiple sentences with a new line.", "\n", "", "", "tokens", "=", "[", "\"/\"", ".", "join", "(", "[", "tag", "for", "tag", "in", "token", "]", ")", "for", "token", "in", "tokens", "]", "\n", "tokens", "=", "\" \"", ".", "join", "(", "tokens", ")", "\n", "string", "+=", "tokens", "+", "\"\\n\"", "\n", "# Return a TokenString, which is a unicode string that transforms easily", "\n", "# into a plain str, a list of tokens, or a Sentence.", "\n", "", "try", ":", "\n", "        ", "if", "MBSP", ":", "\n", "            ", "from", "mbsp", "import", "TokenString", "\n", "", "return", "TokenString", "(", "string", ".", "strip", "(", ")", ",", "tags", "=", "format", ",", "language", "=", "language", ")", "\n", "", "except", ":", "\n", "        ", "return", "TaggedString", "(", "string", ".", "strip", "(", ")", ",", "tags", "=", "format", ",", "language", "=", "language", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_tokens": [[1593, 1639], ["chunk.get", "tree._parse_relation", "builtins.filter", "tokens.append", "format.index", "enumerate", "chunk.get().split", "tokens.extend", "format.index", "enumerate", "chunk.get().split", "_attachments.setdefault", "_attachments[].append", "tree._parse_token", "tree._parse_tokens", "chunk.get", "chunk.get"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_relation", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_token", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_tokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "", "def", "_parse_tokens", "(", "chunk", ",", "format", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", ":", "\n", "    ", "\"\"\" Parses tokens from <word> elements in the given XML <chunk> element.\n        Returns a flat list of tokens, in which each token is [WORD, POS, CHUNK, PNP, RELATION, ANCHOR, LEMMA].\n        If a <chunk type=\"PNP\"> is encountered, traverses all of the chunks in the PNP.\n    \"\"\"", "\n", "tokens", "=", "[", "]", "\n", "# Only process <chunk> and <chink> elements,", "\n", "# text nodes in between return an empty list.", "\n", "if", "not", "(", "chunk", ".", "tag", "==", "XML_CHUNK", "or", "chunk", ".", "tag", "==", "XML_CHINK", ")", ":", "\n", "        ", "return", "[", "]", "\n", "", "type", "=", "chunk", ".", "get", "(", "XML_TYPE", ",", "\"O\"", ")", "\n", "if", "type", "==", "\"PNP\"", ":", "\n", "# For, <chunk type=\"PNP\">, recurse all the child chunks inside the PNP.", "\n", "        ", "for", "ch", "in", "chunk", ":", "\n", "            ", "tokens", ".", "extend", "(", "_parse_tokens", "(", "ch", ",", "format", ")", ")", "\n", "# Tag each of them as part of the PNP.", "\n", "", "if", "PNP", "in", "format", ":", "\n", "            ", "i", "=", "format", ".", "index", "(", "PNP", ")", "\n", "for", "j", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                ", "token", "[", "i", "]", "=", "(", "j", "==", "0", "and", "\"B-\"", "or", "\"I-\"", ")", "+", "\"PNP\"", "\n", "# Store attachments so we can construct anchor id's in parse_string().", "\n", "# This has to be done at the end, when all the chunks have been found.", "\n", "", "", "a", "=", "chunk", ".", "get", "(", "XML_OF", ")", ".", "split", "(", "_UID_SEPARATOR", ")", "[", "-", "1", "]", "\n", "if", "a", ":", "\n", "            ", "_attachments", ".", "setdefault", "(", "a", ",", "[", "]", ")", "\n", "_attachments", "[", "a", "]", ".", "append", "(", "tokens", ")", "\n", "", "return", "tokens", "\n", "# For <chunk type-\"VP\" id=\"1\">, the relation is VP-1.", "\n", "# For <chunk type=\"NP\" relation=\"OBJ\" of=\"1\">, the relation is NP-OBJ-1.", "\n", "", "relation", "=", "_parse_relation", "(", "chunk", ",", "type", ")", "\n", "# Process all of the <word> elements in the chunk, for example:", "\n", "# <word type=\"NN\" lemma=\"pizza\">pizza</word> => [pizza, NN, I-NP, O, NP-OBJ-1, O, pizza]", "\n", "for", "word", "in", "filter", "(", "lambda", "n", ":", "n", ".", "tag", "==", "XML_WORD", ",", "chunk", ")", ":", "\n", "        ", "tokens", ".", "append", "(", "_parse_token", "(", "word", ",", "chunk", "=", "type", ",", "relation", "=", "relation", ",", "format", "=", "format", ")", ")", "\n", "# Add the IOB chunk tags:", "\n", "# words at the start of a chunk are marked with B-, words inside with I-.", "\n", "", "if", "CHUNK", "in", "format", ":", "\n", "        ", "i", "=", "format", ".", "index", "(", "CHUNK", ")", "\n", "for", "j", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "token", "[", "i", "]", "=", "token", "[", "i", "]", "!=", "\"O\"", "and", "(", "(", "j", "==", "0", "and", "\"B-\"", "or", "\"I-\"", ")", "+", "token", "[", "i", "]", ")", "or", "\"O\"", "\n", "# The chunk can be the anchor of one or more PNP chunks.", "\n", "# Store anchors so we can construct anchor id's in parse_string().", "\n", "", "", "a", "=", "chunk", ".", "get", "(", "XML_ANCHOR", ",", "\"\"", ")", ".", "split", "(", "_UID_SEPARATOR", ")", "[", "-", "1", "]", "\n", "if", "a", ":", "\n", "        ", "_anchors", "[", "a", "]", "=", "tokens", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_relation": [[1641, 1655], ["chunk.get", "chunk.get", "chunk.get", "len", "len", "len", "len", "chunk.get.split", "chunk.get.split", "builtins.zip", "x.split", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "def", "_parse_relation", "(", "chunk", ",", "type", "=", "\"O\"", ")", ":", "\n", "    ", "\"\"\" Returns a string of the roles and relations parsed from the given <chunk> element.\n        The chunk type (which is part of the relation string) can be given as parameter.\n    \"\"\"", "\n", "r1", "=", "chunk", ".", "get", "(", "XML_RELATION", ")", "\n", "r2", "=", "chunk", ".", "get", "(", "XML_ID", ",", "chunk", ".", "get", "(", "XML_OF", ")", ")", "\n", "r1", "=", "[", "x", "!=", "\"-\"", "and", "x", "or", "None", "for", "x", "in", "r1", ".", "split", "(", "\"|\"", ")", "]", "or", "[", "None", "]", "\n", "r2", "=", "[", "x", "!=", "\"-\"", "and", "x", "or", "None", "for", "x", "in", "r2", ".", "split", "(", "\"|\"", ")", "]", "or", "[", "None", "]", "\n", "r2", "=", "[", "x", "is", "not", "None", "and", "x", ".", "split", "(", "_UID_SEPARATOR", ")", "[", "-", "1", "]", "or", "x", "for", "x", "in", "r2", "]", "\n", "if", "len", "(", "r1", ")", "<", "len", "(", "r2", ")", ":", "\n", "        ", "r1", "=", "r1", "+", "r1", "*", "(", "len", "(", "r2", ")", "-", "len", "(", "r1", ")", ")", "# [1] [\"SBJ\", \"OBJ\"] => \"SBJ-1;OBJ-1\"", "\n", "", "if", "len", "(", "r2", ")", "<", "len", "(", "r1", ")", ":", "\n", "        ", "r2", "=", "r2", "+", "r2", "*", "(", "len", "(", "r1", ")", "-", "len", "(", "r2", ")", ")", "# [2,4] [\"OBJ\"] => \"OBJ-2;OBJ-4\"", "\n", "", "return", "\";\"", ".", "join", "(", "[", "\"-\"", ".", "join", "(", "[", "x", "for", "x", "in", "(", "type", ",", "r1", ",", "r2", ")", "if", "x", "]", ")", "for", "r1", ",", "r2", "in", "zip", "(", "r1", ",", "r2", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._parse_token": [[1657, 1682], ["tags.append", "tree.xml_decode", "tags.append", "tree.xml_decode", "tags.append", "word.get", "tags.append", "tags.append", "tags.append", "tags.append", "tags.append", "tree.xml_decode", "tree.xml_decode", "word.get", "word.get"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_decode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_decode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_decode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.xml_decode", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "_parse_token", "(", "word", ",", "chunk", "=", "\"O\"", ",", "pnp", "=", "\"O\"", ",", "relation", "=", "\"O\"", ",", "anchor", "=", "\"O\"", ",", "\n", "format", "=", "[", "WORD", ",", "POS", ",", "CHUNK", ",", "PNP", ",", "REL", ",", "ANCHOR", ",", "LEMMA", "]", ")", ":", "\n", "    ", "\"\"\" Returns a list of token tags parsed from the given <word> element.\n        Tags that are not attributes in a <word> (e.g., relation) can be given as parameters.\n    \"\"\"", "\n", "tags", "=", "[", "]", "\n", "for", "tag", "in", "format", ":", "\n", "        ", "if", "tag", "==", "WORD", ":", "\n", "            ", "tags", ".", "append", "(", "xml_decode", "(", "word", ".", "value", ")", ")", "\n", "", "elif", "tag", "==", "POS", ":", "\n", "            ", "tags", ".", "append", "(", "xml_decode", "(", "word", ".", "get", "(", "XML_TYPE", ",", "\"O\"", ")", ")", ")", "\n", "", "elif", "tag", "==", "CHUNK", ":", "\n", "            ", "tags", ".", "append", "(", "chunk", ")", "\n", "", "elif", "tag", "==", "PNP", ":", "\n", "            ", "tags", ".", "append", "(", "pnp", ")", "\n", "", "elif", "tag", "==", "REL", ":", "\n", "            ", "tags", ".", "append", "(", "relation", ")", "\n", "", "elif", "tag", "==", "ANCHOR", ":", "\n", "            ", "tags", ".", "append", "(", "anchor", ")", "\n", "", "elif", "tag", "==", "LEMMA", ":", "\n", "            ", "tags", ".", "append", "(", "xml_decode", "(", "word", ".", "get", "(", "XML_LEMMA", ",", "\"\"", ")", ")", ")", "\n", "", "else", ":", "\n", "# Custom tags when the parser has been extended, see also Word.custom_tags{}.", "\n", "            ", "tags", ".", "append", "(", "xml_decode", "(", "word", ".", "get", "(", "tag", ",", "\"O\"", ")", ")", ")", "\n", "", "", "return", "tags", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.nltk_tree": [[1686, 1717], ["sentence.chunked", "T.append", "tree.bracket_parse", "isinstance", "T.append", "tree.nltk_tree.do_chunk"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.chunked", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "nltk_tree", "(", "sentence", ")", ":", "\n", "    ", "\"\"\" Returns an NLTK nltk.tree.Tree object from the given Sentence.\n        The NLTK module should be on the search path somewhere.\n    \"\"\"", "\n", "from", "nltk", "import", "tree", "\n", "\n", "def", "do_pnp", "(", "pnp", ")", ":", "\n", "# Returns the PNPChunk (and the contained Chunk objects) in NLTK bracket format.", "\n", "        ", "s", "=", "' '", ".", "join", "(", "[", "do_chunk", "(", "ch", ")", "for", "ch", "in", "pnp", ".", "chunks", "]", ")", "\n", "return", "'(PNP %s)'", "%", "s", "\n", "\n", "", "def", "do_chunk", "(", "ch", ")", ":", "\n", "# Returns the Chunk in NLTK bracket format. Recurse attached PNP's.", "\n", "        ", "s", "=", "' '", ".", "join", "(", "[", "'(%s %s)'", "%", "(", "w", ".", "pos", ",", "w", ".", "string", ")", "for", "w", "in", "ch", ".", "words", "]", ")", "\n", "s", "+=", "' '", ".", "join", "(", "[", "do_pnp", "(", "pnp", ")", "for", "pnp", "in", "ch", ".", "attachments", "]", ")", "\n", "return", "'(%s %s)'", "%", "(", "ch", ".", "type", ",", "s", ")", "\n", "\n", "", "T", "=", "[", "'(S'", "]", "\n", "v", "=", "[", "]", "# PNP's already visited.", "\n", "for", "ch", "in", "sentence", ".", "chunked", "(", ")", ":", "\n", "        ", "if", "not", "ch", ".", "pnp", "and", "isinstance", "(", "ch", ",", "Chink", ")", ":", "\n", "            ", "T", ".", "append", "(", "'(%s %s)'", "%", "(", "ch", ".", "words", "[", "0", "]", ".", "pos", ",", "ch", ".", "words", "[", "0", "]", ".", "string", ")", ")", "\n", "", "elif", "not", "ch", ".", "pnp", ":", "\n", "            ", "T", ".", "append", "(", "do_chunk", "(", "ch", ")", ")", "\n", "#elif ch.pnp not in v:", "\n", "", "elif", "ch", ".", "pnp", ".", "anchor", "is", "None", "and", "ch", ".", "pnp", "not", "in", "v", ":", "\n", "# The chunk is part of a PNP without an anchor.", "\n", "            ", "T", ".", "append", "(", "do_pnp", "(", "ch", ".", "pnp", ")", ")", "\n", "v", ".", "append", "(", "ch", ".", "pnp", ")", "\n", "", "", "T", ".", "append", "(", "')'", ")", "\n", "return", "tree", ".", "bracket_parse", "(", "' '", ".", "join", "(", "T", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._colorize": [[1728, 1738], ["isinstance", "isinstance", "colors.get", "colors.get", "colors.get"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["def", "_colorize", "(", "x", ",", "colors", ")", ":", "\n", "    ", "s", "=", "''", "\n", "if", "isinstance", "(", "x", ",", "Word", ")", ":", "\n", "        ", "x", "=", "x", ".", "chunk", "\n", "", "if", "isinstance", "(", "x", ",", "Chunk", ")", ":", "\n", "        ", "s", "=", "',style=filled, fillcolor=\"%s\", fontcolor=\"%s\"'", "%", "(", "colors", ".", "get", "(", "x", ".", "role", ")", "or", "colors", ".", "get", "(", "x", ".", "type", ")", "or", "colors", ".", "get", "(", "''", ")", "or", "(", "\"none\"", ",", "\"black\"", ")", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.graphviz_dot": [[1740, 1792], ["enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "tree._colorize", "tree._colorize", "tree._colorize", "sentence.pnp.index", "sentence.chunks.index", "sentence.chunks.index", "builtins.str"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._colorize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._colorize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree._colorize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index"], ["", "def", "graphviz_dot", "(", "sentence", ",", "font", "=", "\"Arial\"", ",", "colors", "=", "BLUE", ")", ":", "\n", "    ", "\"\"\" Returns a dot-formatted string that can be visualized as a graph in GraphViz.\n    \"\"\"", "\n", "s", "=", "'digraph sentence {\\n'", "\n", "s", "+=", "'\\tranksep=0.75;\\n'", "\n", "s", "+=", "'\\tnodesep=0.15;\\n'", "\n", "s", "+=", "'\\tnode [penwidth=1, fontname=\"%s\", shape=record, margin=0.1, height=0.35];\\n'", "%", "font", "\n", "s", "+=", "'\\tedge [penwidth=1];\\n'", "\n", "s", "+=", "'\\t{ rank=same;\\n'", "\n", "# Create node groups for words, chunks and PNP chunks.", "\n", "for", "w", "in", "sentence", ".", "words", ":", "\n", "        ", "s", "+=", "'\\t\\tword%s [label=\"<f0>%s|<f1>%s\"%s];\\n'", "%", "(", "w", ".", "index", ",", "w", ".", "string", ",", "w", ".", "type", ",", "_colorize", "(", "w", ",", "colors", ")", ")", "\n", "", "for", "w", "in", "sentence", ".", "words", "[", ":", "-", "1", "]", ":", "\n", "# Invisible edges forces the words into the right order:", "\n", "        ", "s", "+=", "'\\t\\tword%s -> word%s [color=none];\\n'", "%", "(", "w", ".", "index", ",", "w", ".", "index", "+", "1", ")", "\n", "", "s", "+=", "'\\t}\\n'", "\n", "s", "+=", "'\\t{ rank=same;\\n'", "\n", "for", "i", ",", "ch", "in", "enumerate", "(", "sentence", ".", "chunks", ")", ":", "\n", "        ", "s", "+=", "'\\t\\tchunk%s [label=\"<f0>%s\"%s];\\n'", "%", "(", "i", "+", "1", ",", "\"-\"", ".", "join", "(", "[", "x", "for", "x", "in", "(", "\n", "ch", ".", "type", ",", "ch", ".", "role", ",", "str", "(", "ch", ".", "relation", "or", "''", ")", ")", "if", "x", "]", ")", "or", "'-'", ",", "_colorize", "(", "ch", ",", "colors", ")", ")", "\n", "", "for", "i", ",", "ch", "in", "enumerate", "(", "sentence", ".", "chunks", "[", ":", "-", "1", "]", ")", ":", "\n", "# Invisible edges forces the chunks into the right order:", "\n", "        ", "s", "+=", "'\\t\\tchunk%s -> chunk%s [color=none];\\n'", "%", "(", "i", "+", "1", ",", "i", "+", "2", ")", "\n", "", "s", "+=", "'}\\n'", "\n", "s", "+=", "'\\t{ rank=same;\\n'", "\n", "for", "i", ",", "ch", "in", "enumerate", "(", "sentence", ".", "pnp", ")", ":", "\n", "        ", "s", "+=", "'\\t\\tpnp%s [label=\"<f0>PNP\"%s];\\n'", "%", "(", "i", "+", "1", ",", "_colorize", "(", "ch", ",", "colors", ")", ")", "\n", "", "s", "+=", "'\\t}\\n'", "\n", "s", "+=", "'\\t{ rank=same;\\n S [shape=circle, margin=0.25, penwidth=2]; }\\n'", "\n", "# Connect words to chunks.", "\n", "# Connect chunks to PNP or S.", "\n", "for", "i", ",", "ch", "in", "enumerate", "(", "sentence", ".", "chunks", ")", ":", "\n", "        ", "for", "w", "in", "ch", ":", "\n", "            ", "s", "+=", "'\\tword%s -> chunk%s;\\n'", "%", "(", "w", ".", "index", ",", "i", "+", "1", ")", "\n", "", "if", "ch", ".", "pnp", ":", "\n", "            ", "s", "+=", "'\\tchunk%s -> pnp%s;\\n'", "%", "(", "i", "+", "1", ",", "sentence", ".", "pnp", ".", "index", "(", "ch", ".", "pnp", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "s", "+=", "'\\tchunk%s -> S;\\n'", "%", "(", "i", "+", "1", ")", "\n", "", "if", "ch", ".", "type", "==", "'VP'", ":", "\n", "# Indicate related chunks with a dotted", "\n", "            ", "for", "r", "in", "ch", ".", "related", ":", "\n", "                ", "s", "+=", "'\\tchunk%s -> chunk%s [style=dotted, arrowhead=none];\\n'", "%", "(", "\n", "i", "+", "1", ",", "sentence", ".", "chunks", ".", "index", "(", "r", ")", "+", "1", ")", "\n", "# Connect PNP to anchor chunk or S.", "\n", "", "", "", "for", "i", ",", "ch", "in", "enumerate", "(", "sentence", ".", "pnp", ")", ":", "\n", "        ", "if", "ch", ".", "anchor", ":", "\n", "            ", "s", "+=", "'\\tpnp%s -> chunk%s;\\n'", "%", "(", "i", "+", "1", ",", "sentence", ".", "chunks", ".", "index", "(", "ch", ".", "anchor", ")", "+", "1", ")", "\n", "s", "+=", "'\\tpnp%s -> S [color=none];\\n'", "%", "(", "i", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "s", "+=", "'\\tpnp%s -> S;\\n'", "%", "(", "i", "+", "1", ")", "\n", "", "", "s", "+=", "\"}\"", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.table": [[1796, 1852], ["enumerate", "enumerate", "max", "columns[].insert", "columns[].insert", "tree.table.outline"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert"], ["", "def", "table", "(", "sentence", ",", "fill", "=", "1", ",", "placeholder", "=", "\"-\"", ")", ":", "\n", "    ", "\"\"\" Returns a string where the tags of tokens in the sentence are organized in outlined columns.\n    \"\"\"", "\n", "tags", "=", "[", "WORD", ",", "POS", ",", "IOB", ",", "CHUNK", ",", "ROLE", ",", "REL", ",", "PNP", ",", "ANCHOR", ",", "LEMMA", "]", "\n", "tags", "+=", "[", "tag", "for", "tag", "in", "sentence", ".", "token", "if", "tag", "not", "in", "tags", "]", "\n", "\n", "def", "format", "(", "token", ",", "tag", ")", ":", "\n", "# Returns the token tag as a string.", "\n", "        ", "if", "tag", "==", "WORD", ":", "\n", "            ", "s", "=", "token", ".", "string", "\n", "", "elif", "tag", "==", "POS", ":", "\n", "            ", "s", "=", "token", ".", "type", "\n", "", "elif", "tag", "==", "IOB", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "(", "token", ".", "index", "==", "token", ".", "chunk", ".", "start", "and", "\"B\"", "or", "\"I\"", ")", "\n", "", "elif", "tag", "==", "CHUNK", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "token", ".", "chunk", ".", "type", "\n", "", "elif", "tag", "==", "ROLE", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "token", ".", "chunk", ".", "role", "\n", "", "elif", "tag", "==", "REL", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "token", ".", "chunk", ".", "relation", "and", "str", "(", "token", ".", "chunk", ".", "relation", ")", "\n", "", "elif", "tag", "==", "PNP", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "token", ".", "chunk", ".", "pnp", "and", "token", ".", "chunk", ".", "pnp", ".", "type", "\n", "", "elif", "tag", "==", "ANCHOR", ":", "\n", "            ", "s", "=", "token", ".", "chunk", "and", "token", ".", "chunk", ".", "anchor_id", "\n", "", "elif", "tag", "==", "LEMMA", ":", "\n", "            ", "s", "=", "token", ".", "lemma", "\n", "", "else", ":", "\n", "            ", "s", "=", "token", ".", "custom_tags", ".", "get", "(", "tag", ")", "\n", "", "return", "s", "or", "placeholder", "\n", "\n", "", "def", "outline", "(", "column", ",", "fill", "=", "1", ",", "padding", "=", "3", ",", "align", "=", "\"left\"", ")", ":", "\n", "# Add spaces to each string in the column so they line out to the highest width.", "\n", "        ", "n", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "column", "]", "+", "[", "fill", "]", ")", "\n", "if", "align", "==", "\"left\"", ":", "\n", "            ", "return", "[", "x", "+", "\" \"", "*", "(", "n", "-", "len", "(", "x", ")", ")", "+", "\" \"", "*", "padding", "for", "x", "in", "column", "]", "\n", "", "if", "align", "==", "\"right\"", ":", "\n", "            ", "return", "[", "\" \"", "*", "(", "n", "-", "len", "(", "x", ")", ")", "+", "x", "+", "\" \"", "*", "padding", "for", "x", "in", "column", "]", "\n", "\n", "# Gather the tags of the tokens in the sentece per column.", "\n", "# If the IOB-tag is I-, mark the chunk tag with \"^\".", "\n", "# Add the tag names as headers in each column.", "\n", "", "", "columns", "=", "[", "[", "format", "(", "token", ",", "tag", ")", "for", "token", "in", "sentence", "]", "for", "tag", "in", "tags", "]", "\n", "columns", "[", "3", "]", "=", "[", "columns", "[", "3", "]", "[", "i", "]", "+", "(", "iob", "==", "\"I\"", "and", "\" ^\"", "or", "\"\"", ")", "for", "i", ",", "iob", "in", "enumerate", "(", "columns", "[", "2", "]", ")", "]", "\n", "del", "columns", "[", "2", "]", "\n", "for", "i", ",", "header", "in", "enumerate", "(", "[", "'word'", ",", "'tag'", ",", "'chunk'", ",", "'role'", ",", "'id'", ",", "'pnp'", ",", "'anchor'", ",", "'lemma'", "]", "+", "tags", "[", "9", ":", "]", ")", ":", "\n", "        ", "columns", "[", "i", "]", ".", "insert", "(", "0", ",", "\"\"", ")", "\n", "columns", "[", "i", "]", ".", "insert", "(", "0", ",", "header", ".", "upper", "(", ")", ")", "\n", "# The left column (the word itself) is outlined to the right,", "\n", "# and has extra spacing so that words across sentences line out nicely below each other.", "\n", "", "for", "i", ",", "column", "in", "enumerate", "(", "columns", ")", ":", "\n", "        ", "columns", "[", "i", "]", "=", "outline", "(", "column", ",", "fill", "+", "10", "*", "(", "i", "==", "0", ")", ",", "align", "=", "(", "\"left\"", ",", "\"right\"", ")", "[", "i", "==", "0", "]", ")", "\n", "# Anchor column is useful in MBSP but not in pattern.en.", "\n", "", "if", "not", "MBSP", ":", "\n", "        ", "del", "columns", "[", "6", "]", "\n", "# Create a string with one row (i.e., one token) per line.", "\n", "", "return", "\"\\n\"", ".", "join", "(", "[", "\"\"", ".", "join", "(", "[", "x", "[", "i", "]", "for", "x", "in", "columns", "]", ")", "for", "i", "in", "range", "(", "len", "(", "columns", "[", "0", "]", ")", ")", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.generate.generate": [[4, 26], ["os.system", "os.system"], "function", ["None"], ["def", "generate", "(", "data_path", ",", "ckpt", ",", "system_out", ",", "ori_path", "=", "None", ",", "gen_subset", "=", "None", ",", "beam", "=", "12", ",", "max_tokens", "=", "6000", ",", "buffer_size", "=", "6000", ")", ":", "\n", "    ", "\"\"\"\n    :param data_path: data-bin path\n    :param ckpt: checkpoint path\n    :param system_out: system out path to be created\n    :param ori_path: bpe-tokenized ori file path (for fairseq-interactive)\n    :param gen_subset: subset of the data-bin path (for fairseq-generate)\n    :param beam: beam size\n    :param max_tokens: max tokens\n    :param buffer_size: buffer size\n    :return:\n    \"\"\"", "\n", "\n", "if", "ori_path", "is", "not", "None", ":", "\n", "        ", "generate", "=", "f\"fairseq-interactive {data_path} --path {ckpt} --input {ori_path} \"", "f\"--beam {beam} --max-tokens {max_tokens} --buffer-size {buffer_size} > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "\n", "", "elif", "gen_subset", "is", "not", "None", ":", "\n", "        ", "generate", "=", "f\"fairseq-generate {data_path} --path {ckpt} --gen-subset {gen_subset} \"", "f\"--beam {beam} --max-tokens {max_tokens} --print-alignment > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.load_lm": [[19, 35], ["argparse.Namespace", "lm_scorer.LMScorer"], "function", ["None"], ["def", "load_lm", "(", "lm_path", ",", "data_bin", ")", ":", "\n", "    ", "args", "=", "argparse", ".", "Namespace", "(", "\n", "path", "=", "lm_path", ",", "\n", "data", "=", "data_bin", ",", "\n", "fp16", "=", "False", ",", "fp16_init_scale", "=", "128", ",", "fp16_scale_tolerance", "=", "0.0", ",", "\n", "fp16_scale_window", "=", "None", ",", "fpath", "=", "None", ",", "future_target", "=", "False", ",", "\n", "gen_subset", "=", "'test'", ",", "lazy_load", "=", "False", ",", "log_format", "=", "None", ",", "log_interval", "=", "1000", ",", "\n", "max_sentences", "=", "None", ",", "max_tokens", "=", "None", ",", "memory_efficient_fp16", "=", "False", ",", "\n", "min_loss_scale", "=", "0.0001", ",", "model_overrides", "=", "'{}'", ",", "no_progress_bar", "=", "True", ",", "\n", "num_shards", "=", "1", ",", "num_workers", "=", "4", ",", "output_dictionary_size", "=", "-", "1", ",", "\n", "output_sent", "=", "False", ",", "past_target", "=", "False", ",", "\n", "quiet", "=", "True", ",", "raw_text", "=", "False", ",", "remove_bpe", "=", "None", ",", "sample_break_mode", "=", "None", ",", "\n", "seed", "=", "1", ",", "self_target", "=", "False", ",", "shard_id", "=", "0", ",", "skip_invalid_size_inputs_valid_test", "=", "False", ",", "\n", "task", "=", "'language_modeling'", ",", "tensorboard_logdir", "=", "''", ",", "threshold_loss_scale", "=", "None", ",", "\n", "tokens_per_sample", "=", "1024", ",", "user_dir", "=", "None", ",", "cpu", "=", "False", ")", "\n", "return", "LMScorer", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.load_capital_dictionary": [[36, 42], ["open", "f.readlines", "line.strip.strip", "line.strip.split"], "function", ["None"], ["", "def", "load_capital_dictionary", "(", "cap_word_dic", ")", ":", "\n", "    ", "with", "open", "(", "cap_word_dic", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "keyword", ",", "correction", "=", "line", ".", "split", "(", "\",\"", ")", "\n", "CAPITAL_WORDS_DIC", "[", "keyword", "]", "=", "correction", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.suggest": [[44, 99], ["text[].isupper", "speller.spell", "text.isalpha", "text.lower", "len", "len", "speller.suggest", "speller.suggest", "returns.append", "text.lower", "suggestions.append", "suggestion[].isupper", "suggestions.append", "len", "suggestion[].upper"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.suggest", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.suggest", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "", "def", "suggest", "(", "text", ",", "is_first", ",", "speller", ",", "n", "=", "NUM_OF_SUGGESTIONS", ")", ":", "\n", "    ", "if", "(", "text", ",", "is_first", ")", "in", "SUGGESTION_CACHE", ":", "\n", "        ", "return", "SUGGESTION_CACHE", "[", "(", "text", ",", "is_first", ")", "]", "\n", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "CAPITAL_WORDS_DIC", ":", "\n", "        ", "suggestions", "=", "[", "CAPITAL_WORDS_DIC", "[", "text", ".", "lower", "(", ")", "]", "]", "\n", "\n", "", "else", ":", "\n", "        ", "suggestions", "=", "[", "]", "\n", "\n", "", "is_upper", "=", "text", "[", "0", "]", ".", "isupper", "(", ")", "\n", "\n", "if", "speller", ".", "spell", "(", "text", ")", ":", "\n", "        ", "SUGGESTION_CACHE", "[", "(", "text", ",", "is_first", ")", "]", "=", "suggestions", "\n", "return", "suggestions", "\n", "\n", "", "if", "text", ".", "isalpha", "(", ")", ":", "\n", "        ", "if", "is_upper", ":", "\n", "            ", "pass", "\n", "\n", "", "elif", "is_first", ":", "\n", "            ", "raw_suggestions", "=", "speller", ".", "suggest", "(", "text", ")", "\n", "for", "suggestion", "in", "raw_suggestions", ":", "\n", "                ", "suggestions", ".", "append", "(", "suggestion", "[", "0", "]", ".", "upper", "(", ")", "+", "suggestion", "[", "1", ":", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "raw_suggestions", "=", "speller", ".", "suggest", "(", "text", ")", "\n", "\n", "for", "suggestion", "in", "raw_suggestions", ":", "\n", "                ", "if", "suggestion", "[", "0", "]", ".", "isupper", "(", ")", ":", "\n", "                    ", "continue", "\n", "", "suggestions", ".", "append", "(", "suggestion", ")", "\n", "\n", "", "if", "len", "(", "suggestions", ")", "==", "0", ":", "\n", "                ", "suggestions", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "suggestions", "=", "suggestions", "[", ":", "n", "]", "\n", "\n", "", "", "", "if", "len", "(", "suggestions", ")", "==", "0", ":", "\n", "        ", "SUGGESTION_CACHE", "[", "(", "text", ",", "is_first", ")", "]", "=", "[", "]", "\n", "return", "[", "]", "\n", "", "else", ":", "\n", "        ", "returns", "=", "[", "]", "\n", "for", "suggestion", "in", "suggestions", ":", "\n", "            ", "if", "'-'", "in", "suggestion", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "returns", ".", "append", "(", "suggestion", ")", "\n", "\n", "", "", "if", "len", "(", "returns", ")", "==", "0", ":", "\n", "            ", "SUGGESTION_CACHE", "[", "(", "text", ",", "is_first", ")", "]", "=", "[", "text", "]", "\n", "return", "[", "text", "]", "\n", "", "else", ":", "\n", "            ", "SUGGESTION_CACHE", "[", "(", "text", ",", "is_first", ")", "]", "=", "returns", "\n", "return", "returns", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.spellcheck": [[100, 154], ["open", "open().read().splitlines", "tqdm.tqdm", "nltk.tokenize.sent_tokenize", "fout.write", "open().read", "fix_tokenization_errors.fix", "copy.deepcopy().split", "enumerate", "corrected_sents.append", "spell.suggest", "model.score", "zip", "winner.replace", "open", "copy.deepcopy", "len", "len", "candidates.append", "range", "print", "print", "print", "zip", "print", "input", "len", "range", "print", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.fix", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.suggest", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "", "def", "spellcheck", "(", "model", ",", "fin", ",", "fout", ",", "speller", ")", ":", "\n", "\n", "    ", "with", "open", "(", "fout", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "lines", "=", "open", "(", "fin", ",", "'r'", ")", ".", "read", "(", ")", ".", "splitlines", "(", ")", "\n", "for", "line", "in", "tqdm", "(", "lines", ")", ":", "\n", "            ", "corrected_sents", "=", "[", "]", "\n", "sents", "=", "sent_tokenize", "(", "line", ")", "\n", "for", "sent", "in", "sents", ":", "\n", "                ", "sent", "=", "fix", "(", "sent", ")", "\n", "tokens", "=", "copy", ".", "deepcopy", "(", "sent", ")", ".", "split", "(", ")", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                    ", "if", "i", "==", "0", ":", "\n", "                        ", "is_first", "=", "True", "\n", "", "else", ":", "\n", "                        ", "is_first", "=", "False", "\n", "\n", "", "suggestions", "=", "suggest", "(", "word", ",", "is_first", ",", "speller", ")", "\n", "if", "len", "(", "suggestions", ")", "==", "0", ":", "continue", "\n", "if", "len", "(", "suggestions", ")", "==", "1", ":", "\n", "                        ", "if", "suggestions", "[", "0", "]", "==", "word", ":", "continue", "\n", "\n", "", "candidates", "=", "[", "]", "\n", "for", "suggestion", "in", "suggestions", ":", "\n", "                        ", "tokens", "[", "i", "]", "=", "suggestion", "\n", "candidates", ".", "append", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", "\n", "\n", "# inspect", "\n", "", "winner", "=", "\"\"", "\n", "\n", "# For Neural Net", "\n", "min_score", "=", "1000", "\n", "\n", "scores", "=", "model", ".", "score", "(", "candidates", ")", "\n", "# scores = {idx: idx for idx in range(len(candidates))}", "\n", "\n", "for", "idx", ",", "s", "in", "zip", "(", "range", "(", "len", "(", "candidates", ")", ")", ",", "suggestions", ")", ":", "\n", "# print(s, score)", "\n", "                        ", "score", "=", "scores", "[", "idx", "]", "\n", "if", "score", "<", "min_score", ":", "\n", "                            ", "min_score", "=", "score", "\n", "winner", "=", "s", "\n", "\n", "", "", "if", "verbose", ":", "\n", "                        ", "print", "(", "\"ORIGINAL: \"", "+", "sent", ")", "\n", "print", "(", "\"TARGET: \"", "+", "word", ".", "text", ")", "\n", "print", "(", "\"SUGGEST: \"", "+", "','", ".", "join", "(", "suggestions", ")", ")", "\n", "for", "idx", ",", "candidates", "in", "zip", "(", "range", "(", "len", "(", "scores", ")", ")", ",", "candidates", ")", ":", "\n", "                            ", "print", "(", "\"{}: \"", ".", "format", "(", "scores", "[", "idx", "]", ")", "+", "candidates", ")", "\n", "", "print", "(", "\"WINNER: \"", "+", "winner", ")", "\n", "input", "(", ")", "\n", "\n", "", "tokens", "[", "i", "]", "=", "winner", ".", "replace", "(", "\" \"", ",", "\"_\"", ")", "\n", "", "corrected_sents", ".", "append", "(", "\" \"", ".", "join", "(", "tokens", ")", ".", "replace", "(", "\"_\"", ",", "\" \"", ")", ")", "\n", "", "fout", ".", "write", "(", "\" \"", ".", "join", "(", "corrected_sents", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.check": [[155, 169], ["spell.load_lm", "spell.load_capital_dictionary", "hunspell.HunSpell", "spell.spellcheck"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.postprocess.load_lm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.load_capital_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.spell.spellcheck"], ["", "", "", "def", "check", "(", "fin", ",", "fout", ",", "\n", "aff", "=", "Path", ".", "aff", ",", "\n", "dic", "=", "Path", ".", "dic", ",", "\n", "lm_path", "=", "Path", ".", "lm_path", ",", "\n", "data_bin", "=", "Path", ".", "lm_databin", ",", "\n", "cap_word_dic", "=", "Path", ".", "cap_word_dic", "\n", ")", ":", "\n", "\n", "    ", "model", "=", "load_lm", "(", "lm_path", ",", "data_bin", ")", "\n", "load_capital_dictionary", "(", "cap_word_dic", ")", "\n", "\n", "speller", "=", "hunspell", ".", "HunSpell", "(", "dic", ",", "aff", ")", "\n", "\n", "spellcheck", "(", "model", ",", "fin", ",", "fout", ",", "speller", "=", "speller", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.space_puncts": [[3, 17], ["sent.strip().split", "re.search", "_sent.append", "sent.strip", "re.sub", "re.sub.count"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["def", "space_puncts", "(", "sent", ")", ":", "\n", "    ", "'''\n    e.g., noise!He -> noise ! He\n    '''", "\n", "symbols", "=", "'[:.,!?\"]'", "\n", "_sent", "=", "[", "]", "\n", "for", "word", "in", "sent", ".", "strip", "(", ")", ".", "split", "(", ")", ":", "\n", "        ", "detect", "=", "re", ".", "search", "(", "f\"{symbols}[A-Za-z]\"", ",", "word", ")", "\n", "if", "detect", "is", "not", "None", ":", "\n", "            ", "if", "not", "word", ".", "count", "(", "\".\"", ")", ">", "1", ":", "# e.g., `U.S.` is correct.", "\n", "                ", "word", "=", "re", ".", "sub", "(", "f\"({symbols})\"", ",", "r\" \\1 \"", ",", "word", ")", "\n", "", "", "_sent", ".", "append", "(", "word", ")", "\n", "\n", "", "return", "\" \"", ".", "join", "(", "_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.space_contracts": [[18, 29], ["sent.split", "_sent.append", "re.sub"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "space_contracts", "(", "sent", ")", ":", "\n", "    ", "'''\n    e.g., haven't -> have n't\n    '''", "\n", "clitics", "=", "(", "\"n't\"", ",", "\"'ll\"", ",", "\"'s\"", ",", "\"'m\"", ",", "\"'ve\"", ")", "\n", "_sent", "=", "[", "]", "\n", "for", "w", "in", "sent", ".", "split", "(", ")", ":", "\n", "        ", "for", "clitic", "in", "clitics", ":", "\n", "            ", "w", "=", "re", ".", "sub", "(", "f\"([A-Za-z])({clitic})\"", ",", "r\"\\1 \\2\"", ",", "w", ")", "\n", "", "_sent", ".", "append", "(", "w", ")", "\n", "", "return", "\" \"", ".", "join", "(", "_sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.collapse_spaces": [[30, 33], ["re.sub"], "function", ["None"], ["", "def", "collapse_spaces", "(", "sent", ")", ":", "\n", "    ", "sent", "=", "re", ".", "sub", "(", "\" +\"", ",", "\" \"", ",", "sent", ")", "\n", "return", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.fix": [[34, 39], ["fix_tokenization_errors.space_puncts", "fix_tokenization_errors.space_contracts", "fix_tokenization_errors.collapse_spaces"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.space_puncts", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.space_contracts", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.fix_tokenization_errors.collapse_spaces"], ["", "def", "fix", "(", "sent", ")", ":", "\n", "    ", "sent", "=", "space_puncts", "(", "sent", ")", "\n", "sent", "=", "space_contracts", "(", "sent", ")", "\n", "sent", "=", "collapse_spaces", "(", "sent", ")", "\n", "return", "sent", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_all_coder_ids": [[9, 17], ["set", "sorted", "edit.split.split", "int", "sorted.add", "list"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], ["def", "get_all_coder_ids", "(", "edits", ")", ":", "\n", "    ", "coder_ids", "=", "set", "(", ")", "\n", "for", "edit", "in", "edits", ":", "\n", "        ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "coder_id", "=", "int", "(", "edit", "[", "-", "1", "]", ")", "\n", "coder_ids", ".", "add", "(", "coder_id", ")", "\n", "", "coder_ids", "=", "sorted", "(", "list", "(", "coder_ids", ")", ")", "\n", "return", "coder_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2_to_parallel": [[19, 58], ["open", "open", "open().read().strip().split", "entry.split", "open().read().strip", "lines[].split", "m2.get_all_coder_ids", "open.write", "edit.split.split", "int", "int", "int", "edit[].split", "open.write", "open().read", "edit[].split", "len", "open"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_all_coder_ids"], ["", "def", "m2_to_parallel", "(", "m2_files", ",", "ori", ",", "cor", ",", "drop_unchanged_samples", ",", "all", ")", ":", "\n", "\n", "    ", "ori_fout", "=", "None", "\n", "if", "ori", "is", "not", "None", ":", "\n", "        ", "ori_fout", "=", "open", "(", "ori", ",", "'w'", ")", "\n", "", "cor_fout", "=", "open", "(", "cor", ",", "'w'", ")", "\n", "\n", "# Do not apply edits with these error types", "\n", "skip", "=", "{", "\"noop\"", ",", "\"UNK\"", ",", "\"Um\"", "}", "\n", "for", "m2_file", "in", "m2_files", ":", "\n", "        ", "entries", "=", "open", "(", "m2_file", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "entry", "in", "entries", ":", "\n", "            ", "lines", "=", "entry", ".", "split", "(", "\"\\n\"", ")", "\n", "ori_sent", "=", "lines", "[", "0", "]", "[", "2", ":", "]", "# Ignore \"S \"", "\n", "cor_tokens", "=", "lines", "[", "0", "]", ".", "split", "(", ")", "[", "1", ":", "]", "# Ignore \"S \"", "\n", "edits", "=", "lines", "[", "1", ":", "]", "\n", "offset", "=", "0", "\n", "\n", "coders", "=", "get_all_coder_ids", "(", "edits", ")", "if", "all", "==", "True", "else", "[", "0", "]", "\n", "for", "coder", "in", "coders", ":", "\n", "                ", "for", "edit", "in", "edits", ":", "\n", "                    ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "if", "edit", "[", "1", "]", "in", "skip", ":", "continue", "# Ignore certain edits", "\n", "coder_id", "=", "int", "(", "edit", "[", "-", "1", "]", ")", "\n", "if", "coder_id", "!=", "coder", ":", "continue", "# Ignore other coders", "\n", "span", "=", "edit", "[", "0", "]", ".", "split", "(", ")", "[", "1", ":", "]", "# Ignore \"A \"", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cor", "=", "edit", "[", "2", "]", ".", "split", "(", ")", "\n", "cor_tokens", "[", "start", "+", "offset", ":", "end", "+", "offset", "]", "=", "cor", "\n", "offset", "=", "offset", "-", "(", "end", "-", "start", ")", "+", "len", "(", "cor", ")", "\n", "\n", "", "cor_sent", "=", "\" \"", ".", "join", "(", "cor_tokens", ")", "\n", "if", "drop_unchanged_samples", "and", "ori_sent", "==", "cor_sent", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "ori", "is", "not", "None", ":", "\n", "                    ", "ori_fout", ".", "write", "(", "ori_sent", "+", "\"\\n\"", ")", "\n", "", "cor_fout", ".", "write", "(", "cor_sent", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2_to_cor": [[60, 62], ["m2.m2_to_parallel"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2_to_parallel"], ["", "", "", "", "def", "m2_to_cor", "(", "m2", ",", "cor", ")", ":", "\n", "    ", "m2_to_parallel", "(", "m2_files", "=", "[", "m2", "]", ",", "ori", "=", "None", ",", "cor", "=", "cor", ",", "drop_unchanged_samples", "=", "False", ",", "all", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.parallel_to_m2": [[64, 67], ["os.system"], "function", ["None"], ["", "def", "parallel_to_m2", "(", "ori", ",", "cor", ",", "m2", ")", ":", "\n", "    ", "p2m", "=", "f\"python {Path.parallel_to_m2} -orig {ori} -cor {cor} -out {m2}\"", "\n", "os", ".", "system", "(", "p2m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.sys_to_cor": [[69, 93], ["open().read().strip().splitlines", "m2.sys_to_cor._sorted"], "function", ["None"], ["", "def", "sys_to_cor", "(", "system_out", ",", "cor_path", ")", ":", "\n", "\n", "    ", "def", "detokenize", "(", "sent", ")", ":", "\n", "        ", "'''restores a bpe-segmented sentence\n            '''", "\n", "return", "sent", ".", "replace", "(", "\" \"", ",", "\"\"", ")", ".", "replace", "(", "\"\u2581\"", ",", "\" \"", ")", ".", "strip", "(", ")", "\n", "\n", "", "lines", "=", "open", "(", "system_out", ",", "'r'", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "splitlines", "(", ")", "\n", "hypo_lst", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "if", "line", ".", "startswith", "(", "\"H\"", ")", ":", "\n", "            ", "cols", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "num", "=", "int", "(", "cols", "[", "0", "]", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ")", "\n", "hypo", "=", "(", "num", ",", "cols", "[", "-", "1", "]", ")", "\n", "hypo_lst", ".", "append", "(", "hypo", ")", "\n", "\n", "", "", "def", "_sorted", "(", "x", ")", ":", "\n", "        ", "x", "=", "sorted", "(", "x", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "return", "[", "elem", "[", "1", "]", "for", "elem", "in", "x", "]", "\n", "\n", "", "hypo_lst", "=", "_sorted", "(", "hypo_lst", ")", "\n", "with", "open", "(", "cor_path", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "fout", ".", "write", "(", "\"\\n\"", ".", "join", "(", "detokenize", "(", "sent", ")", "for", "sent", "in", "hypo_lst", ")", ")", "\n", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.line_to_edit": [[95, 104], ["m2_line.split", "features[].split", "m2_line.startswith", "int", "int"], "function", ["None"], ["", "", "def", "line_to_edit", "(", "m2_line", ")", ":", "\n", "    ", "if", "not", "m2_line", ".", "startswith", "(", "\"A\"", ")", ":", "\n", "        ", "return", "None", "\n", "", "features", "=", "m2_line", ".", "split", "(", "\"|||\"", ")", "\n", "span", "=", "features", "[", "0", "]", ".", "split", "(", ")", "\n", "start", ",", "end", "=", "int", "(", "span", "[", "1", "]", ")", ",", "int", "(", "span", "[", "2", "]", ")", "\n", "error_type", "=", "features", "[", "1", "]", "\n", "replace_token", "=", "features", "[", "2", "]", "\n", "return", "start", ",", "end", ",", "error_type", ",", "replace_token", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.remove_m2": [[106, 130], ["m2_entry.splitlines", "output_entries.append", "m2.line_to_edit", "len", "preserve_m2.append", "preserve_m2.append", "preserve_m2.append"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.line_to_edit", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "remove_m2", "(", "m2_entries", ",", "filter_type_lst", ",", "filter_token", ")", ":", "\n", "    ", "noop", "=", "\"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\"", "\n", "\n", "output_entries", "=", "[", "]", "\n", "for", "m2_entry", "in", "m2_entries", ":", "\n", "        ", "lines", "=", "m2_entry", ".", "splitlines", "(", ")", "\n", "sent", "=", "lines", "[", "0", "]", "\n", "edits", "=", "lines", "[", "1", ":", "]", "\n", "preserve_m2", "=", "[", "sent", "]", "\n", "\n", "for", "line", "in", "edits", ":", "\n", "            ", "start", ",", "end", ",", "error_type", ",", "replace_token", "=", "line_to_edit", "(", "line", ")", "\n", "if", "filter_type_lst", "is", "not", "None", ":", "\n", "                ", "if", "error_type", "not", "in", "filter_type_lst", ":", "\n", "                    ", "preserve_m2", ".", "append", "(", "line", ")", "\n", "", "", "if", "filter_token", "is", "not", "None", ":", "\n", "                ", "if", "filter_token", "not", "in", "replace_token", ":", "\n", "                    ", "preserve_m2", ".", "append", "(", "line", ")", "\n", "\n", "", "", "", "if", "len", "(", "preserve_m2", ")", "==", "1", ":", "\n", "            ", "preserve_m2", ".", "append", "(", "noop", ")", "\n", "\n", "", "output_entries", ".", "append", "(", "\"\\n\"", ".", "join", "(", "preserve_m2", ")", ")", "\n", "", "return", "output_entries", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.sort_m2_lines": [[132, 138], ["dict", "m2.line_to_edit", "sorted", "dict.items"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.line_to_edit", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "sort_m2_lines", "(", "m2_lines", ")", ":", "\n", "    ", "m2_dict", "=", "dict", "(", ")", "\n", "for", "line", "in", "m2_lines", ":", "\n", "        ", "s", ",", "_", ",", "_", ",", "_", "=", "line_to_edit", "(", "line", ")", "\n", "m2_dict", "[", "s", "]", "=", "line", "\n", "", "return", "[", "i", "[", "1", "]", "for", "i", "in", "sorted", "(", "m2_dict", ".", "items", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2edits_to_cor": [[140, 157], ["m2.sort_m2_lines", "ori.split", "edit.split.split", "int", "int", "edit[].split", "edit[].split", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.sort_m2_lines"], ["", "def", "m2edits_to_cor", "(", "ori", ",", "m2_lines", ")", ":", "\n", "    ", "_m2_lines", "=", "sort_m2_lines", "(", "m2_lines", ")", "\n", "\n", "skip", "=", "{", "\"noop\"", ",", "\"UNK\"", ",", "\"Um\"", "}", "\n", "cor_sent", "=", "ori", ".", "split", "(", ")", "\n", "\n", "offset", "=", "0", "\n", "for", "edit", "in", "_m2_lines", ":", "\n", "        ", "edit", "=", "edit", ".", "split", "(", "\"|||\"", ")", "\n", "if", "edit", "[", "1", "]", "in", "skip", ":", "continue", "# Ignore certain edits", "\n", "span", "=", "edit", "[", "0", "]", ".", "split", "(", ")", "[", "1", ":", "]", "# Ignore \"A \"", "\n", "start", "=", "int", "(", "span", "[", "0", "]", ")", "\n", "end", "=", "int", "(", "span", "[", "1", "]", ")", "\n", "cor", "=", "edit", "[", "2", "]", ".", "split", "(", ")", "\n", "cor_sent", "[", "start", "+", "offset", ":", "end", "+", "offset", "]", "=", "cor", "\n", "offset", "=", "offset", "-", "(", "end", "-", "start", ")", "+", "len", "(", "cor", ")", "\n", "", "return", "\" \"", ".", "join", "(", "cor_sent", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_edit_combinations": [[159, 165], ["range", "edit_combinations.extend", "len", "itertools.combinations"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend"], ["", "def", "get_edit_combinations", "(", "edits", ")", ":", "\n", "    ", "edit_combinations", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "edits", ")", "+", "1", ")", ":", "\n", "        ", "edit_combinations", ".", "extend", "(", "combinations", "(", "edits", ",", "i", ")", ")", "\n", "\n", "", "return", "edit_combinations", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.apply_lm_rerank": [[167, 207], ["tqdm.tqdm", "m2_entry.splitlines", "m2.line_to_edit", "m2.get_edit_combinations", "lm_scorer.score", "m2.sort_m2_lines", "output_entries.append", "preserve_m2.append", "len", "m2.m2edits_to_cor", "cor_sents.append", "sorted", "len", "output_entries.append", "output_entries.append", "preserve_m2.append", "rerank_m2.append", "list", "list"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.line_to_edit", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_edit_combinations", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.sort_m2_lines", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2edits_to_cor", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "apply_lm_rerank", "(", "m2_entries", ",", "preserve_spell", ",", "max_edits", ",", "lm_scorer", ")", ":", "\n", "    ", "noop", "=", "\"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\"", "\n", "\n", "output_entries", "=", "[", "]", "\n", "for", "m2_entry", "in", "tqdm", "(", "m2_entries", ")", ":", "\n", "        ", "lines", "=", "m2_entry", ".", "splitlines", "(", ")", "\n", "sent", "=", "lines", "[", "0", "]", "[", "2", ":", "]", "\n", "edits", "=", "lines", "[", "1", ":", "]", "\n", "preserve_m2", "=", "[", "]", "\n", "rerank_m2", "=", "[", "]", "\n", "\n", "for", "line", "in", "edits", ":", "\n", "            ", "start", ",", "end", ",", "error_type", ",", "replace_token", "=", "line_to_edit", "(", "line", ")", "\n", "if", "error_type", "==", "'noop'", ":", "\n", "                ", "preserve_m2", ".", "append", "(", "line", ")", "\n", "", "elif", "preserve_spell", "and", "\"SPELL\"", "in", "error_type", ":", "\n", "                ", "preserve_m2", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                ", "rerank_m2", ".", "append", "(", "line", ")", "\n", "\n", "", "", "if", "max_edits", "is", "None", "or", "0", "<", "len", "(", "rerank_m2", ")", "<", "max_edits", ":", "\n", "            ", "edit_comb", "=", "get_edit_combinations", "(", "rerank_m2", ")", "\n", "\n", "cor_sents", "=", "[", "]", "\n", "for", "e", "in", "edit_comb", ":", "\n", "                ", "cor", "=", "m2edits_to_cor", "(", "sent", ",", "preserve_m2", "+", "list", "(", "e", ")", ")", "\n", "cor_sents", ".", "append", "(", "cor", ")", "\n", "\n", "", "score_dict", "=", "lm_scorer", ".", "score", "(", "cor_sents", ")", "\n", "\n", "min_idx", "=", "sorted", "(", "score_dict", ",", "key", "=", "score_dict", ".", "get", ",", "reverse", "=", "False", ")", "[", "0", "]", "\n", "sorted_m2_lines", "=", "sort_m2_lines", "(", "preserve_m2", "+", "list", "(", "edit_comb", "[", "min_idx", "]", ")", ")", "\n", "if", "len", "(", "sorted_m2_lines", ")", "==", "0", ":", "\n", "                ", "output_entries", ".", "append", "(", "\"\\n\"", ".", "join", "(", "[", "lines", "[", "0", "]", ",", "noop", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "output_entries", ".", "append", "(", "\"\\n\"", ".", "join", "(", "[", "lines", "[", "0", "]", "]", "+", "sorted_m2_lines", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "output_entries", ".", "append", "(", "m2_entry", ")", "\n", "", "", "return", "output_entries", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_m2_entries": [[209, 212], ["open().read().strip().split", "open().read().strip", "open().read", "open"], "function", ["None"], ["", "def", "get_m2_entries", "(", "m2_file", ")", ":", "\n", "    ", "m2_entries", "=", "open", "(", "m2_file", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "'\\n\\n'", ")", "\n", "return", "m2_entries", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.write_m2_entries": [[214, 217], ["open", "fout.write"], "function", ["None"], ["", "def", "write_m2_entries", "(", "m2_entries", ",", "m2_file", ")", ":", "\n", "    ", "with", "open", "(", "m2_file", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "fout", ".", "write", "(", "\"\\n\\n\"", ".", "join", "(", "m2_entries", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.split_m2": [[218, 232], ["open", "open", "open().read().split", "line.startswith", "open().read", "random.random", "finetune_out.write", "valid_out.write", "open"], "function", ["None"], ["", "", "def", "split_m2", "(", "fpaths", ",", "finetune_m2", ",", "valid_m2", ",", "finetune_split_ratio", ")", ":", "\n", "    ", "\"\"\"\n    split m2 files into finetune and valid files\n    \"\"\"", "\n", "with", "open", "(", "finetune_m2", ",", "'w'", ")", "as", "finetune_out", ",", "open", "(", "valid_m2", ",", "'w'", ")", "as", "valid_out", ":", "\n", "        ", "for", "fpath", "in", "fpaths", ":", "\n", "            ", "if", "'ABCN'", "in", "fpath", ":", "continue", "\n", "lines", "=", "open", "(", "fpath", ",", "'r'", ")", ".", "read", "(", ")", ".", "split", "(", "'\\n\\n'", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "if", "line", ".", "startswith", "(", "'S'", ")", ":", "\n", "                    ", "if", "random", ".", "random", "(", ")", "<", "finetune_split_ratio", ":", "\n", "                        ", "finetune_out", ".", "write", "(", "line", "+", "'\\n\\n'", ")", "\n", "", "else", ":", "\n", "                        ", "valid_out", ".", "write", "(", "line", "+", "'\\n\\n'", ")", "\n", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.bpe.train": [[3, 10], ["sentencepiece.SentencePieceTrainer.Train", "bpe.adjust_vocab"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.bpe.adjust_vocab"], ["def", "train", "(", "inp", ",", "model_prefix", ",", "vocab_size", ",", "character_coverage", ",", "model_type", ")", ":", "\n", "    ", "train", "=", "f'--input={inp} --model_prefix={model_prefix} \\\n            --vocab_size={vocab_size} --character_coverage={character_coverage} \\\n            --model_type={model_type}'", "\n", "spm", ".", "SentencePieceTrainer", ".", "Train", "(", "train", ")", "\n", "\n", "adjust_vocab", "(", "model_prefix", "+", "\".vocab\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.bpe.adjust_vocab": [[11, 21], ["open().read().splitlines", "line.split", "open", "fout.write", "len", "adjusted.append", "open().read", "open"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "adjust_vocab", "(", "vocab_fpath", ")", ":", "\n", "    ", "'''Adjust bpe vocab file such that it fits the fairseq dict format.'''", "\n", "adjusted", "=", "[", "]", "\n", "for", "line", "in", "open", "(", "vocab_fpath", ",", "'r'", ")", ".", "read", "(", ")", ".", "splitlines", "(", ")", "[", "3", ":", "]", ":", "\n", "        ", "tok", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "tok", ")", ">", "1", ":", "\n", "            ", "adjusted", ".", "append", "(", "\" \"", ".", "join", "(", "tok", ")", ")", "\n", "\n", "", "", "with", "open", "(", "vocab_fpath", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "fout", ".", "write", "(", "\"\\n\"", ".", "join", "(", "adjusted", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.bpe.bpe_tokenize": [[22, 35], ["sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "open", "open", "spm.SentencePieceProcessor.EncodeAsPieces", "fout.write", "line.strip"], "function", ["None"], ["", "", "def", "bpe_tokenize", "(", "model", ",", "fin", ",", "fout", ")", ":", "\n", "    ", "'''\n    model: str. model fp\n    fin: input fp\n    fout: output fp\n    '''", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "model", ")", "\n", "\n", "with", "open", "(", "fout", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "line", "in", "open", "(", "fin", ",", "'r'", ")", ":", "\n", "            ", "tokens", "=", "sp", ".", "EncodeAsPieces", "(", "line", ".", "strip", "(", ")", ")", "\n", "fout", ".", "write", "(", "\" \"", ".", "join", "(", "tokens", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.postprocess.load_lm": [[15, 30], ["argparse.Namespace", "lm_scorer.LMScorer"], "function", ["None"], ["def", "load_lm", "(", "lm_path", "=", "Path", ".", "lm_path", ",", "lm_databin", "=", "Path", ".", "lm_databin", ")", ":", "\n", "    ", "args", "=", "argparse", ".", "Namespace", "(", "\n", "path", "=", "lm_path", ",", "data", "=", "lm_databin", ",", "\n", "fp16", "=", "False", ",", "fp16_init_scale", "=", "128", ",", "fp16_scale_tolerance", "=", "0.0", ",", "\n", "fp16_scale_window", "=", "None", ",", "fpath", "=", "None", ",", "future_target", "=", "False", ",", "\n", "gen_subset", "=", "'test'", ",", "lazy_load", "=", "False", ",", "log_format", "=", "None", ",", "log_interval", "=", "1000", ",", "\n", "max_sentences", "=", "None", ",", "max_tokens", "=", "None", ",", "memory_efficient_fp16", "=", "False", ",", "\n", "min_loss_scale", "=", "0.0001", ",", "model_overrides", "=", "'{}'", ",", "no_progress_bar", "=", "True", ",", "\n", "num_shards", "=", "1", ",", "num_workers", "=", "0", ",", "output_dictionary_size", "=", "-", "1", ",", "\n", "output_sent", "=", "False", ",", "past_target", "=", "False", ",", "\n", "quiet", "=", "True", ",", "raw_text", "=", "False", ",", "remove_bpe", "=", "None", ",", "sample_break_mode", "=", "None", ",", "\n", "seed", "=", "1", ",", "self_target", "=", "False", ",", "shard_id", "=", "0", ",", "skip_invalid_size_inputs_valid_test", "=", "False", ",", "\n", "task", "=", "'language_modeling'", ",", "tensorboard_logdir", "=", "''", ",", "threshold_loss_scale", "=", "None", ",", "\n", "tokens_per_sample", "=", "1024", ",", "user_dir", "=", "None", ",", "cpu", "=", "False", ")", "\n", "return", "LMScorer", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.postprocess.postprocess": [[32, 73], ["logging.info", "m2.sys_to_cor", "logging.info", "m2.parallel_to_m2", "logging.info", "m2.get_m2_entries", "logging.info", "logging.info", "util.get_basename", "m2.write_m2_entries", "logging.info", "m2.m2_to_parallel", "logging.info", "m2.remove_m2", "len", "logging.info", "m2.remove_m2", "logging.info", "postprocess.load_lm", "m2.apply_lm_rerank", "logging.error"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.sys_to_cor", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.parallel_to_m2", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.get_m2_entries", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.write_m2_entries", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.m2_to_parallel", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.remove_m2", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.remove_m2", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.postprocess.load_lm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.m2.apply_lm_rerank"], ["", "def", "postprocess", "(", "ori_path", ",", "system_out", ",", "cor_path", ",", "remove_unk_edits", "=", "True", ",", "remove_error_type_lst", "=", "[", "]", ",", "\n", "apply_rerank", "=", "False", ",", "preserve_spell", "=", "False", ",", "max_edits", "=", "None", ")", ":", "\n", "\n", "    ", "pred", "=", "f\"{system_out}.pred\"", "\n", "m2_file_tmp", "=", "f\"{system_out}._m2\"", "\n", "\n", "try", ":", "\n", "        ", "logging", ".", "info", "(", "\"[Postprocess] 1. get pred file\"", ")", "\n", "m2", ".", "sys_to_cor", "(", "system_out", ",", "pred", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Postprocess] 2. convert pred into m2\"", ")", "\n", "m2", ".", "parallel_to_m2", "(", "ori_path", ",", "pred", ",", "m2_file_tmp", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Postprocess] 3. adjust m2\"", ")", "\n", "m2_entries", "=", "m2", ".", "get_m2_entries", "(", "m2_file_tmp", ")", "\n", "\n", "if", "remove_unk_edits", ":", "\n", "            ", "logging", ".", "info", "(", "\"[Postprocess] 3-1. removing <unk> edits\"", ")", "\n", "m2_entries", "=", "m2", ".", "remove_m2", "(", "m2_entries", ",", "None", ",", "'<unk>'", ")", "\n", "\n", "", "if", "len", "(", "remove_error_type_lst", ")", ">", "0", ":", "\n", "            ", "logging", ".", "info", "(", "\"[Postprocess] 3-2. remove error types\"", ")", "\n", "m2_entries", "=", "m2", ".", "remove_m2", "(", "m2_entries", ",", "remove_error_type_lst", ",", "None", ")", "\n", "\n", "", "if", "apply_rerank", ":", "\n", "            ", "logging", ".", "info", "(", "\"[Postprocess] 3-3. apply rerank\"", ")", "\n", "lm_scorer", "=", "load_lm", "(", ")", "\n", "m2_entries", "=", "m2", ".", "apply_lm_rerank", "(", "m2_entries", ",", "preserve_spell", ",", "max_edits", ",", "lm_scorer", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"[Postprocess] 4. get pred again\"", ")", "\n", "logging", ".", "info", "(", "\"[Postprocess] 4-1. write m2 file\"", ")", "\n", "\n", "cor_basename", "=", "util", ".", "get_basename", "(", "cor_path", ",", "include_extension", "=", "False", ")", "\n", "m2_file", "=", "f\"{cor_basename}.m2\"", "\n", "m2", ".", "write_m2_entries", "(", "m2_entries", ",", "m2_file", ")", "\n", "\n", "logging", ".", "info", "(", "\"[Postprocess] 4-2. write cor file\"", ")", "\n", "m2", ".", "m2_to_parallel", "(", "[", "m2_file", "]", ",", "None", ",", "cor_path", ",", "False", ",", "True", ")", "\n", "\n", "", "except", ":", "\n", "        ", "logging", ".", "error", "(", "\"[Postprocess] error occurred\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.filepath.FilePath.__init__": [[32, 192], ["os.path.dirname", "filepath.FilePath.make_dirs", "os.path.dirname", "os.path.abspath"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.filepath.FilePath.make_dirs"], ["def", "__init__", "(", "self", ")", ":", "\n", "\n", "# set your root path here", "\n", "        ", "self", ".", "root", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", ")", "\n", "\n", "self", ".", "gutenberg", "=", "f\"{self.root}/data/gutenberg\"", "\n", "self", ".", "tatoeba", "=", "f\"{self.root}/data/tatoeba\"", "\n", "self", ".", "wiki103", "=", "f\"{self.root}/data/wiki103\"", "\n", "self", ".", "bea19", "=", "f'{self.root}/data/bea19'", "\n", "self", ".", "conll2013", "=", "f'{self.root}/data/conll2013'", "\n", "self", ".", "conll2014", "=", "f'{self.root}/data/conll2014'", "\n", "self", ".", "jfleg", "=", "f'{self.root}/data/jfleg'", "\n", "self", ".", "bpe_model", "=", "f'{self.root}/data/bpe-model'", "\n", "self", ".", "parallel", "=", "f'{self.root}/data/parallel'", "\n", "self", ".", "fce", "=", "f'{self.bea19}/fce'", "\n", "self", ".", "wi", "=", "f'{self.bea19}/wi+locness'", "\n", "\n", "self", ".", "fce_m2", "=", "f'{self.fce}/m2'", "\n", "self", ".", "lang8_m2", "=", "f'{self.bea19}/lang8.bea19'", "\n", "self", ".", "nucle_m2", "=", "f'{self.bea19}/nucle3.3/bea2019'", "\n", "self", ".", "wi_m2", "=", "f'{self.wi}/m2'", "\n", "\n", "self", ".", "conll2013_m2", "=", "f'{self.conll2013}/release2.3.1/revised/data'", "\n", "self", ".", "conll2014_m2", "=", "f'{self.conll2014}/conll14st-test-data/noalt'", "\n", "\n", "# for dae", "\n", "self", ".", "GUTENBERG_TXT", "=", "f\"{self.gutenberg}/gutenberg.txt\"", "\n", "self", ".", "TATOEBA_TXT", "=", "f\"{self.tatoeba}/tatoeba.txt\"", "\n", "self", ".", "WIKI103_TXT", "=", "f\"{self.wiki103}/wiki103.txt\"", "\n", "\n", "# bpe", "\n", "self", ".", "BPE_MODEL", "=", "f\"{self.bpe_model}/gutenberg.model\"", "\n", "self", ".", "BPE_VOCAB", "=", "f\"{self.bpe_model}/gutenberg.vocab\"", "\n", "\n", "# dae tok", "\n", "self", ".", "GUTENBERG_ORI1", "=", "f\"{self.parallel}/tok/gutenberg.tok.wi.train.ori\"", "\n", "self", ".", "GUTENBERG_COR1", "=", "f\"{self.parallel}/tok/gutenberg.tok.wi.train.cor\"", "\n", "self", ".", "TATOEBA_ORI1", "=", "f\"{self.parallel}/tok//tatoeba.tok.wi.train.ori\"", "\n", "self", ".", "TATOEBA_COR1", "=", "f\"{self.parallel}/tok/tatoeba.tok.wi.train.cor\"", "\n", "self", ".", "WIKI103_ORI1", "=", "f\"{self.parallel}/tok/wiki103.tok.wi.train.ori\"", "\n", "self", ".", "WIKI103_COR1", "=", "f\"{self.parallel}/tok/wiki103.tok.wi.train.cor\"", "\n", "\n", "self", ".", "GUTENBERG_ORI3", "=", "f\"{self.parallel}/tok/gutenberg.tok.wi.dev.3k.ori\"", "\n", "self", ".", "GUTENBERG_COR3", "=", "f\"{self.parallel}/tok/gutenberg.tok.wi.dev.3k.cor\"", "\n", "self", ".", "TATOEBA_ORI3", "=", "f\"{self.parallel}/tok/tatoeba.tok.wi.dev.3k.ori\"", "\n", "self", ".", "TATOEBA_COR3", "=", "f\"{self.parallel}/tok/tatoeba.tok.wi.dev.3k.cor\"", "\n", "self", ".", "WIKI103_ORI3", "=", "f\"{self.parallel}/tok/wiki103.tok.wi.dev.3k.ori\"", "\n", "self", ".", "WIKI103_COR3", "=", "f\"{self.parallel}/tok/wiki103.tok.wi.dev.3k.cor\"", "\n", "\n", "self", ".", "GUTENBERG_ORI0", "=", "f\"{self.parallel}/tok/gutenberg.tok.nucle.ori\"", "\n", "self", ".", "GUTENBERG_COR0", "=", "f\"{self.parallel}/tok/gutenberg.tok.nucle.cor\"", "\n", "self", ".", "TATOEBA_ORI0", "=", "f\"{self.parallel}/tok/tatoeba.tok.nucle.ori\"", "\n", "self", ".", "TATOEBA_COR0", "=", "f\"{self.parallel}/tok/tatoeba.tok.nucle.cor\"", "\n", "self", ".", "WIKI103_ORI0", "=", "f\"{self.parallel}/tok/wiki103.tok.nucle.ori\"", "\n", "self", ".", "WIKI103_COR0", "=", "f\"{self.parallel}/tok/wiki103.tok.nucle.cor\"", "\n", "\n", "# raw", "\n", "self", ".", "FCE_ORI", "=", "f\"{self.parallel}/raw/fce.ori\"", "\n", "self", ".", "FCE_COR", "=", "f\"{self.parallel}/raw/fce.cor\"", "\n", "self", ".", "LANG8_ORI", "=", "f\"{self.parallel}/raw/lang8.ori\"", "\n", "self", ".", "LANG8_COR", "=", "f\"{self.parallel}/raw/lang8.cor\"", "\n", "self", ".", "NUCLE_ORI", "=", "f\"{self.parallel}/raw/nucle.ori\"", "\n", "self", ".", "NUCLE_COR", "=", "f\"{self.parallel}/raw/nucle.cor\"", "\n", "self", ".", "WI_TRAIN_ORI", "=", "f\"{self.parallel}/raw/wi.train.ori\"", "\n", "self", ".", "WI_TRAIN_COR", "=", "f\"{self.parallel}/raw/wi.train.cor\"", "\n", "self", ".", "WI_DEV_ORI", "=", "f\"{self.parallel}/raw/wi.dev.ori\"", "\n", "self", ".", "WI_DEV_COR", "=", "f\"{self.parallel}/raw/wi.dev.cor\"", "\n", "self", ".", "WI_TEST_ORI", "=", "f\"{self.parallel}/raw/ABCN.test.bea19.orig\"", "\n", "# self.WI_TEST_COR = f\"{self.parallel}/raw/wi.test.cor\"", "\n", "\n", "self", ".", "WI_DEV_3K_ORI", "=", "f\"{self.parallel}/raw/wi.dev.3k.ori\"", "\n", "self", ".", "WI_DEV_3K_COR", "=", "f\"{self.parallel}/raw/wi.dev.3k.cor\"", "\n", "self", ".", "WI_DEV_1K_ORI", "=", "f\"{self.parallel}/raw/wi.dev.1k.ori\"", "\n", "self", ".", "WI_DEV_1K_COR", "=", "f\"{self.parallel}/raw/wi.dev.1k.cor\"", "\n", "\n", "self", ".", "CONLL2013_ORI", "=", "f\"{self.parallel}/raw/conll2013.ori\"", "\n", "self", ".", "CONLL2013_COR", "=", "f\"{self.parallel}/raw/conll2013.cor\"", "\n", "self", ".", "CONLL2014_ORI", "=", "f\"{self.parallel}/raw/conll2014.ori\"", "\n", "self", ".", "CONLL2014_COR", "=", "f\"{self.parallel}/raw/conll2014.cor\"", "\n", "self", ".", "JFLEG_ORI", "=", "f\"{self.jfleg}/test/test.src\"", "\n", "\n", "# sp", "\n", "self", ".", "FCE_SP_ORI", "=", "f\"{self.parallel}/sp/fce.sp.ori\"", "\n", "self", ".", "LANG8_SP_ORI", "=", "f\"{self.parallel}/sp/lang8.sp.ori\"", "\n", "self", ".", "NUCLE_SP_ORI", "=", "f\"{self.parallel}/sp/nucle.sp.ori\"", "\n", "self", ".", "WI_TRAIN_SP_ORI", "=", "f\"{self.parallel}/sp/wi.train.sp.ori\"", "\n", "self", ".", "WI_DEV_SP_ORI", "=", "f\"{self.parallel}/sp/wi.dev.sp.ori\"", "\n", "self", ".", "WI_TEST_SP_ORI", "=", "f\"{self.parallel}/sp/wi.test.sp.ori\"", "\n", "\n", "self", ".", "WI_DEV_3K_SP_ORI", "=", "f\"{self.parallel}/sp/wi.dev.3k.sp.ori\"", "\n", "self", ".", "WI_DEV_1K_SP_ORI", "=", "f\"{self.parallel}/sp/wi.dev.1k.sp.ori\"", "\n", "\n", "self", ".", "CONLL2013_SP_ORI", "=", "f\"{self.parallel}/sp/conll2013.sp.ori\"", "\n", "self", ".", "CONLL2014_SP_ORI", "=", "f\"{self.parallel}/sp/conll2014.sp.ori\"", "\n", "self", ".", "JFLEG_SP_ORI", "=", "f\"{self.parallel}/sp/jfleg.sp.ori\"", "\n", "\n", "# tok", "\n", "self", ".", "FCE_TOK_ORI", "=", "f\"{self.parallel}/tok/fce.tok.ori\"", "\n", "self", ".", "FCE_TOK_COR", "=", "f\"{self.parallel}/tok/fce.tok.cor\"", "\n", "self", ".", "LANG8_TOK_ORI", "=", "f\"{self.parallel}/tok/lang8.tok.ori\"", "\n", "self", ".", "LANG8_TOK_COR", "=", "f\"{self.parallel}/tok/lang8.tok.cor\"", "\n", "self", ".", "NUCLE_TOK_ORI", "=", "f\"{self.parallel}/tok/nucle.tok.ori\"", "\n", "self", ".", "NUCLE_TOK_COR", "=", "f\"{self.parallel}/tok/nucle.tok.cor\"", "\n", "self", ".", "WI_TRAIN_TOK_ORI", "=", "f\"{self.parallel}/tok/wi.train.tok.ori\"", "\n", "self", ".", "WI_TRAIN_TOK_COR", "=", "f\"{self.parallel}/tok/wi.train.tok.cor\"", "\n", "self", ".", "WI_DEV_TOK_ORI", "=", "f\"{self.parallel}/tok/wi.dev.tok.ori\"", "\n", "self", ".", "WI_DEV_TOK_COR", "=", "f\"{self.parallel}/tok/wi.dev.tok.cor\"", "\n", "self", ".", "WI_TEST_TOK_ORI", "=", "f\"{self.parallel}/tok/wi.test.tok.ori\"", "\n", "# self.WI_TEST_TOK_COR = f\"{self.parallel}/tok/wi.test.tok.cor\"", "\n", "\n", "self", ".", "WI_DEV_3K_TOK_ORI", "=", "f\"{self.parallel}/tok/wi.dev.3k.tok.ori\"", "\n", "self", ".", "WI_DEV_3K_TOK_COR", "=", "f\"{self.parallel}/tok/wi.dev.3k.tok.cor\"", "\n", "self", ".", "WI_DEV_1K_TOK_ORI", "=", "f\"{self.parallel}/tok/wi.dev.1k.tok.ori\"", "\n", "self", ".", "WI_DEV_1K_TOK_COR", "=", "f\"{self.parallel}/tok/wi.dev.1k.tok.cor\"", "\n", "\n", "self", ".", "CONLL2013_TOK_ORI", "=", "f\"{self.parallel}/tok/conll2013.tok.ori\"", "\n", "self", ".", "CONLL2013_TOK_COR", "=", "f\"{self.parallel}/tok/conll2013.tok.cor\"", "\n", "self", ".", "CONLL2014_TOK_ORI", "=", "f\"{self.parallel}/tok/conll2014.tok.ori\"", "\n", "self", ".", "CONLL2014_TOK_COR", "=", "f\"{self.parallel}/tok/conll2014.tok.cor\"", "\n", "self", ".", "JFLEG_TOK_ORI", "=", "f\"{self.parallel}/tok/jfleg.tok.ori\"", "\n", "# self.JFLEG_TOK_COR = f\"{self.parallel}/tok/jfleg.tok.cor\"", "\n", "\n", "# track1", "\n", "self", ".", "DAE_ORI1", "=", "f\"{self.root}/track1/parallel/dae.ori\"", "\n", "self", ".", "DAE_COR1", "=", "f\"{self.root}/track1/parallel/dae.cor\"", "\n", "self", ".", "TRAIN_ORI1", "=", "f\"{self.root}/track1/parallel/train.ori\"", "\n", "self", ".", "TRAIN_COR1", "=", "f\"{self.root}/track1/parallel/train.cor\"", "\n", "self", ".", "FINETUNE_ORI1", "=", "f\"{self.root}/track1/parallel/finetune.ori\"", "\n", "self", ".", "FINETUNE_COR1", "=", "f\"{self.root}/track1/parallel/finetune.cor\"", "\n", "self", ".", "VALID_ORI1", "=", "f\"{self.root}/track1/parallel/valid.ori\"", "\n", "self", ".", "VALID_COR1", "=", "f\"{self.root}/track1/parallel/valid.cor\"", "\n", "self", ".", "TEST_ORI1", "=", "f\"{self.root}/track1/parallel/test.ori\"", "\n", "self", ".", "TEST_COR1", "=", "f\"{self.root}/track1/parallel/test.cor\"", "\n", "\n", "# track3", "\n", "self", ".", "DAE_ORI3", "=", "f\"{self.root}/track3/parallel/dae.ori\"", "\n", "self", ".", "DAE_COR3", "=", "f\"{self.root}/track3/parallel/dae.cor\"", "\n", "self", ".", "FINETUNE_ORI3", "=", "f\"{self.root}/track3/parallel/finetune.ori\"", "\n", "self", ".", "FINETUNE_COR3", "=", "f\"{self.root}/track3/parallel/finetune.cor\"", "\n", "self", ".", "VALID_ORI3", "=", "f\"{self.root}/track3/parallel/valid.ori\"", "\n", "self", ".", "VALID_COR3", "=", "f\"{self.root}/track3/parallel/valid.cor\"", "\n", "self", ".", "TEST_ORI3", "=", "f\"{self.root}/track3/parallel/test.ori\"", "\n", "self", ".", "TEST_COR3", "=", "f\"{self.root}/track3/parallel/test.cor\"", "\n", "\n", "# track0", "\n", "self", ".", "DAE_ORI0", "=", "f\"{self.root}/track0/parallel/dae.ori\"", "\n", "self", ".", "DAE_COR0", "=", "f\"{self.root}/track0/parallel/dae.cor\"", "\n", "self", ".", "TRAIN_ORI0", "=", "f\"{self.root}/track0/parallel/train.ori\"", "\n", "self", ".", "TRAIN_COR0", "=", "f\"{self.root}/track0/parallel/train.cor\"", "\n", "self", ".", "FINETUNE_ORI0", "=", "f\"{self.root}/track0/parallel/finetune.ori\"", "\n", "self", ".", "FINETUNE_COR0", "=", "f\"{self.root}/track0/parallel/finetune.cor\"", "\n", "self", ".", "VALID_ORI0", "=", "f\"{self.root}/track0/parallel/valid.ori\"", "\n", "self", ".", "VALID_COR0", "=", "f\"{self.root}/track0/parallel/valid.cor\"", "\n", "self", ".", "TEST_ORI0", "=", "f\"{self.root}/track0/parallel/test.ori\"", "\n", "self", ".", "TEST_COR0", "=", "f\"{self.root}/track0/parallel/test.cor\"", "\n", "self", ".", "TEST_ORI_JFLEG0", "=", "f\"{self.root}/track0/parallel/test_jfleg.ori\"", "\n", "self", ".", "TEST_COR_JFLEG0", "=", "f\"{self.root}/track0/parallel/test_jfleg.cor\"", "\n", "\n", "# make_dirs", "\n", "self", ".", "make_dirs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.filepath.FilePath.make_dirs": [[193, 197], ["getattr", "os.makedirs", "dir", "attr[].isupper", "os.path.dirname"], "methods", ["None"], ["", "def", "make_dirs", "(", "self", ")", ":", "\n", "        ", "fnames", "=", "[", "getattr", "(", "self", ",", "attr", ")", "for", "attr", "in", "dir", "(", "self", ")", "if", "attr", "[", "0", "]", ".", "isupper", "(", ")", "]", "\n", "for", "fname", "in", "fnames", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "fname", ")", ",", "exist_ok", "=", "True", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.change_type": [[20, 36], ["random.random", "random.choice", "random.random", "pattern3.en.pluralize", "random.random", "pattern3.en.singularize", "random.random", "random.choice", "pattern3.en.conjugate"], "function", ["None"], ["def", "change_type", "(", "word", ",", "tag", ",", "change_prob", ")", ":", "\n", "    ", "global", "PREPOSITIONS", ",", "VERB_TYPES", "\n", "if", "tag", "==", "\"IN\"", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "change_prob", ":", "\n", "            ", "word", "=", "random", ".", "choice", "(", "PREPOSITIONS", ")", "\n", "", "", "elif", "tag", "==", "\"NN\"", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "change_prob", ":", "\n", "            ", "word", "=", "pluralize", "(", "word", ")", "\n", "", "", "elif", "tag", "==", "\"NNS\"", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "change_prob", ":", "\n", "            ", "word", "=", "singularize", "(", "word", ")", "\n", "", "", "elif", "\"VB\"", "in", "tag", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "change_prob", ":", "\n", "            ", "verb_type", "=", "random", ".", "choice", "(", "VERB_TYPES", ")", "\n", "word", "=", "conjugate", "(", "word", ",", "verb_type", ")", "\n", "", "", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.make_word2ptbs": [[37, 111], ["dict", "dict", "dict.items", "open().read().strip().split", "collections.Counter", "collections.Counter.most_common", "S.split", "enumerate", "len", "ptb_cnt_li.append", "len", "_ptbs.extend", "open().read().strip", "entry.splitlines", "edit[].split", "features[].split", "len", "int", "int", "word2ptbs[].append", "open().read", "entry.splitlines", "word2ptbs[].append", "skip_indices.append", "skip_indices.append", "word2ptbs[].append", "skip_indices.append", "open", "len", "word2ptbs[].append", "word.split"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "make_word2ptbs", "(", "m2_files", ",", "min_cnt", ")", ":", "\n", "    ", "'''Error Simulation\n    m2: string. m2 file path.\n    min_cnt: int. minimum count\n    '''", "\n", "word2ptbs", "=", "dict", "(", ")", "# ptb: pertubation", "\n", "for", "m2_file", "in", "m2_files", ":", "\n", "        ", "entries", "=", "open", "(", "m2_file", ",", "'r'", ")", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "entry", "in", "entries", ":", "\n", "            ", "skip", "=", "(", "\"noop\"", ",", "\"UNK\"", ",", "\"Um\"", ")", "\n", "S", "=", "entry", ".", "splitlines", "(", ")", "[", "0", "]", "[", "2", ":", "]", "+", "\" </s>\"", "\n", "words", "=", "S", ".", "split", "(", ")", "\n", "edits", "=", "entry", ".", "splitlines", "(", ")", "[", "1", ":", "]", "\n", "\n", "skip_indices", "=", "[", "]", "\n", "for", "edit", "in", "edits", ":", "\n", "                ", "features", "=", "edit", "[", "2", ":", "]", ".", "split", "(", "\"|||\"", ")", "\n", "if", "features", "[", "1", "]", "in", "skip", ":", "continue", "\n", "start", ",", "end", "=", "features", "[", "0", "]", ".", "split", "(", ")", "\n", "start", ",", "end", "=", "int", "(", "start", ")", ",", "int", "(", "end", ")", "\n", "word", "=", "features", "[", "2", "]", "\n", "\n", "if", "start", "==", "end", ":", "# insertion -> deletion", "\n", "                    ", "ptb", "=", "\"\"", "\n", "if", "word", "in", "word2ptbs", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", ".", "append", "(", "ptb", ")", "\n", "", "else", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", "=", "[", "ptb", "]", "\n", "", "", "elif", "start", "+", "1", "==", "end", "and", "word", "==", "\"\"", ":", "# deletion -> substitution", "\n", "                    ", "ptb", "=", "words", "[", "start", "]", "+", "\" \"", "+", "words", "[", "start", "+", "1", "]", "\n", "word", "=", "words", "[", "start", "+", "1", "]", "\n", "if", "word", "in", "word2ptbs", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", ".", "append", "(", "ptb", ")", "\n", "", "else", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", "=", "[", "ptb", "]", "\n", "", "skip_indices", ".", "append", "(", "start", ")", "\n", "skip_indices", ".", "append", "(", "start", "+", "1", ")", "\n", "", "elif", "start", "+", "1", "==", "end", "and", "word", "!=", "\"\"", "and", "len", "(", "word", ".", "split", "(", ")", ")", "==", "1", ":", "# substitution", "\n", "                    ", "ptb", "=", "words", "[", "start", "]", "\n", "if", "word", "in", "word2ptbs", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", ".", "append", "(", "ptb", ")", "\n", "", "else", ":", "\n", "                        ", "word2ptbs", "[", "word", "]", "=", "[", "ptb", "]", "\n", "", "skip_indices", ".", "append", "(", "start", ")", "\n", "", "else", ":", "\n", "                    ", "continue", "\n", "\n", "", "", "for", "idx", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "                ", "if", "idx", "in", "skip_indices", ":", "continue", "\n", "if", "word", "in", "word2ptbs", ":", "\n", "                    ", "word2ptbs", "[", "word", "]", ".", "append", "(", "word", ")", "\n", "", "else", ":", "\n", "                    ", "word2ptbs", "[", "word", "]", "=", "[", "word", "]", "\n", "\n", "# pruning", "\n", "", "", "", "", "_word2ptbs", "=", "dict", "(", ")", "\n", "for", "word", ",", "ptbs", "in", "word2ptbs", ".", "items", "(", ")", ":", "\n", "        ", "ptb2cnt", "=", "Counter", "(", "ptbs", ")", "\n", "\n", "ptb_cnt_li", "=", "[", "]", "\n", "for", "ptb", ",", "cnt", "in", "ptb2cnt", ".", "most_common", "(", "len", "(", "ptb2cnt", ")", ")", ":", "\n", "            ", "if", "cnt", "<", "min_cnt", ":", "break", "\n", "ptb_cnt_li", ".", "append", "(", "(", "ptb", ",", "cnt", ")", ")", "\n", "\n", "", "if", "len", "(", "ptb_cnt_li", ")", "==", "0", ":", "continue", "\n", "if", "len", "(", "ptb_cnt_li", ")", "==", "1", "and", "ptb_cnt_li", "[", "0", "]", "[", "0", "]", "==", "word", ":", "continue", "\n", "\n", "_ptbs", "=", "[", "]", "\n", "for", "ptb", ",", "cnt", "in", "ptb_cnt_li", ":", "\n", "            ", "_ptbs", ".", "extend", "(", "[", "ptb", "]", "*", "cnt", ")", "\n", "\n", "", "_word2ptbs", "[", "word", "]", "=", "_ptbs", "\n", "\n", "", "return", "_word2ptbs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.apply_perturbation": [[112, 131], ["nltk.pos_tag", "zip", "re.sub.append", "re.sub", "random.choice", "perturb.change_type", "random.random"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.change_type"], ["", "def", "apply_perturbation", "(", "words", ",", "word2ptbs", ",", "word_change_prob", ",", "type_change_prob", ")", ":", "\n", "    ", "word_tags", "=", "pos_tag", "(", "words", ")", "\n", "\n", "sent", "=", "[", "]", "\n", "for", "(", "_", ",", "t", ")", ",", "w", "in", "zip", "(", "word_tags", ",", "words", ")", ":", "\n", "        ", "if", "w", "in", "word2ptbs", "and", "random", ".", "random", "(", ")", ">", "1", "-", "word_change_prob", ":", "\n", "            ", "oris", "=", "word2ptbs", "[", "w", "]", "\n", "w", "=", "random", ".", "choice", "(", "oris", ")", "\n", "", "else", ":", "\n", "            ", "w", "=", "change_type", "(", "w", ",", "t", ",", "type_change_prob", ")", "\n", "", "sent", ".", "append", "(", "w", ")", "\n", "\n", "", "try", ":", "\n", "        ", "sent", "=", "\" \"", ".", "join", "(", "sent", ")", "\n", "sent", "=", "re", ".", "sub", "(", "\"[ ]+\"", ",", "\" \"", ",", "sent", ")", "\n", "", "except", ":", "\n", "        ", "return", "None", "\n", "\n", "", "return", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.blocks": [[132, 137], ["files.read"], "function", ["None"], ["", "def", "blocks", "(", "files", ",", "size", "=", "65536", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "b", "=", "files", ".", "read", "(", "size", ")", "\n", "if", "not", "b", ":", "break", "\n", "yield", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.count_lines": [[138, 141], ["open", "sum", "bl.count", "perturb.blocks"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.blocks"], ["", "", "def", "count_lines", "(", "f", ")", ":", "\n", "    ", "with", "open", "(", "f", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "'ignore'", ")", "as", "f", ":", "\n", "        ", "return", "sum", "(", "bl", ".", "count", "(", "\"\\n\"", ")", "for", "bl", "in", "blocks", "(", "f", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.make_parallel": [[142, 168], ["logging.info", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "os.path.join", "os.path.join", "os.makedirs", "os.makedirs", "os.path.dirname", "os.path.dirname", "open", "open", "tqdm.tqdm", "range", "open", "line.strip().split", "perturb.apply_perturbation", "spm.SentencePieceProcessor.EncodeAsPieces", "spm.SentencePieceProcessor.EncodeAsPieces", "ori.write", "cor.write", "apply_perturbation.strip", "line.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.apply_perturbation"], ["", "", "def", "make_parallel", "(", "inputs", ")", ":", "\n", "    ", "word2ptbs", ",", "bpe_model", ",", "txt", ",", "ori", ",", "cor", ",", "n_epochs", ",", "word_change_prob", ",", "type_change_prob", ",", "start", ",", "end", "=", "inputs", "\n", "logging", ".", "info", "(", "\"Load sentencepiece model\"", ")", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "bpe_model", ")", "\n", "\n", "ori_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "ori", ")", ",", "f\"working/ori\"", ")", "\n", "cor_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "ori", ")", ",", "f\"working/cor\"", ")", "\n", "os", ".", "makedirs", "(", "ori_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "cor_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "with", "open", "(", "f'{ori_dir}/{start}'", ",", "'w'", ")", "as", "ori", ",", "open", "(", "f'{cor_dir}/{start}'", ",", "'w'", ")", "as", "cor", ":", "\n", "        ", "for", "_", "in", "tqdm", "(", "range", "(", "n_epochs", ")", ")", ":", "\n", "            ", "i", "=", "0", "\n", "for", "line", "in", "open", "(", "txt", ",", "'r'", ")", ":", "\n", "                ", "i", "+=", "1", "\n", "if", "start", "<=", "i", "<", "end", ":", "\n", "                    ", "words", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "perturbation", "=", "apply_perturbation", "(", "words", ",", "word2ptbs", ",", "word_change_prob", ",", "type_change_prob", ")", "\n", "if", "perturbation", "is", "None", ":", "continue", "\n", "ori_pieces", "=", "sp", ".", "EncodeAsPieces", "(", "perturbation", ".", "strip", "(", ")", ")", "\n", "cor_pieces", "=", "sp", ".", "EncodeAsPieces", "(", "line", ".", "strip", "(", ")", ")", "\n", "ori", ".", "write", "(", "\" \"", ".", "join", "(", "ori_pieces", ")", "+", "\"\\n\"", ")", "\n", "cor", ".", "write", "(", "\" \"", ".", "join", "(", "cor_pieces", ")", "+", "\"\\n\"", ")", "\n", "", "if", "i", ">", "end", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.do": [[169, 194], ["print", "multiprocessing.cpu_count", "multiprocessing.Pool", "print", "perturb.count_lines", "list", "print", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "print", "print", "os.system", "os.system", "os.system", "print", "range", "os.path.dirname", "os.path.dirname", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.perturb.count_lines", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "", "", "", "", "def", "do", "(", "word2ptbs", ",", "bpe_model", ",", "txt", ",", "ori", ",", "cor", ",", "n_epochs", ",", "word_change_prob", ",", "type_change_prob", ")", ":", "\n", "    ", "print", "(", "\"# multiprocessing settings\"", ")", "\n", "n_cpus", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "p", "=", "multiprocessing", ".", "Pool", "(", "n_cpus", ")", "\n", "\n", "print", "(", "\"# prepare inputs\"", ")", "\n", "n_lines", "=", "count_lines", "(", "txt", ")", "\n", "\n", "start_li", "=", "list", "(", "range", "(", "0", ",", "n_lines", ",", "n_lines", "//", "n_cpus", ")", ")", "\n", "start_end_li", "=", "[", "(", "start", ",", "start", "+", "n_lines", "//", "n_cpus", ")", "for", "start", "in", "start_li", "]", "\n", "inputs_li", "=", "[", "(", "word2ptbs", ",", "bpe_model", ",", "txt", ",", "ori", ",", "cor", ",", "n_epochs", ",", "word_change_prob", ",", "type_change_prob", ",", "start", ",", "end", ")", "for", "start", ",", "end", "in", "start_end_li", "]", "\n", "\n", "print", "(", "\"# work\"", ")", "\n", "p", ".", "map", "(", "make_parallel", ",", "inputs_li", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "print", "(", "\"# work done!\"", ")", "\n", "\n", "print", "(", "\"# concat...\"", ")", "\n", "os", ".", "system", "(", "f\"cat {os.path.dirname(ori)}/working/ori/* > {ori}\"", ")", "\n", "os", ".", "system", "(", "f\"cat {os.path.dirname(cor)}/working/cor/* > {cor}\"", ")", "\n", "os", ".", "system", "(", "f\"rm -r {os.path.dirname(ori)}/working\"", ")", "\n", "print", "(", "\"All done!\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_do": [[7, 12], ["os.path.exists", "logging.info", "func"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["def", "maybe_do", "(", "fp", ",", "func", ",", "inputs", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "fp", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"skip this step as {fp} already exists\"", ")", "\n", "", "else", ":", "\n", "        ", "func", "(", "*", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.maybe_prompt": [[14, 19], ["os.path.exists", "logging.info", "os.system"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["", "", "def", "maybe_prompt", "(", "fp", ",", "prompt", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "fp", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"skip this step as {fp} already exists\"", ")", "\n", "", "else", ":", "\n", "        ", "os", ".", "system", "(", "prompt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_scores": [[21, 50], ["open", "float", "float", "float", "report.read().strip().splitlines.split", "open.read().strip().splitlines", "logging.error", "open.read().strip().splitlines", "line[].split", "line[].split", "line[].split", "open.read().strip", "open.read().strip", "open.read", "open.read"], "function", ["None"], ["", "", "def", "get_scores", "(", "report_fname", ",", "scorer", ")", ":", "\n", "    ", "assert", "scorer", "in", "[", "\"errant\"", ",", "\"m2scorer\"", "]", "\n", "\n", "report", "=", "open", "(", "report_fname", ",", "'r'", ")", "\n", "\n", "try", ":", "\n", "        ", "if", "scorer", "==", "\"errant\"", ":", "\n", "            ", "line", "=", "report", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "splitlines", "(", ")", "[", "-", "2", "]", "\n", "tokens", "=", "line", ".", "split", "(", ")", "\n", "precision", ",", "recall", ",", "fscore", "=", "tokens", "[", "-", "3", "]", ",", "tokens", "[", "-", "2", "]", ",", "tokens", "[", "-", "1", "]", "\n", "\n", "", "else", ":", "# m2scorer", "\n", "            ", "line", "=", "report", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "splitlines", "(", ")", "\n", "precision", "=", "line", "[", "0", "]", ".", "split", "(", ")", "[", "-", "1", "]", "\n", "recall", "=", "line", "[", "1", "]", ".", "split", "(", ")", "[", "-", "1", "]", "\n", "fscore", "=", "line", "[", "2", "]", ".", "split", "(", ")", "[", "-", "1", "]", "\n", "", "", "except", ":", "\n", "        ", "logging", ".", "error", "(", "f\"[Util] cannot get scores from {report_fname}\"", ")", "\n", "precision", ",", "recall", ",", "fscore", "=", "0", ",", "0", ",", "0", "\n", "\n", "", "precision", "=", "float", "(", "precision", ")", "*", "100", "\n", "recall", "=", "float", "(", "recall", ")", "*", "100", "\n", "fscore", "=", "float", "(", "fscore", ")", "*", "100", "\n", "\n", "# precision = int(float(precision) * 100)", "\n", "# recall = int(float(recall) * 100)", "\n", "# fscore = int(float(fscore) * 100)", "\n", "\n", "return", "precision", ",", "recall", ",", "fscore", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.find_highest_score": [[52, 71], ["util.get_basename", "glob.glob", "os.path.join", "util.get_scores", "print", "exit", "get_basename().replace", "util.get_basename"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_scores", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename"], ["", "def", "find_highest_score", "(", "output_dir", ",", "ori_path", ",", "scorer_type", ")", ":", "\n", "    ", "data_basename", "=", "get_basename", "(", "ori_path", ",", "include_path", "=", "False", ",", "include_extension", "=", "False", ")", "\n", "files", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f\"*{data_basename}.report\"", ")", ")", "\n", "\n", "highest_fscore", "=", "0", "\n", "highest_basename", "=", "''", "\n", "\n", "for", "report_fname", "in", "files", ":", "\n", "        ", "precision", ",", "recall", ",", "fscore", "=", "get_scores", "(", "report_fname", ",", "scorer_type", ")", "\n", "if", "fscore", ">", "highest_fscore", ":", "\n", "            ", "highest_fscore", "=", "fscore", "\n", "highest_basename", "=", "get_basename", "(", "report_fname", ",", "include_path", "=", "True", ",", "include_extension", "=", "False", ")", ".", "replace", "(", "data_basename", ",", "''", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "", "highest_ckpt", "=", "f\"{highest_basename}.pt\"", ".", "replace", "(", "\"outputs\"", ",", "\"ckpt\"", ")", "\n", "if", "highest_basename", "==", "''", ":", "\n", "        ", "print", "(", "f'cannot find highest basename from {output_dir}'", ")", "\n", "exit", "(", ")", "\n", "\n", "", "return", "highest_fscore", ",", "highest_ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_sorted_ckpts": [[73, 88], ["glob.glob", "sorted", "os.path.join", "sorted.append", "[].replace().split", "int", "[].replace", "[].split", "f.split"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "get_sorted_ckpts", "(", "ckpt_dir", ",", "epoch_start", "=", "1", ",", "epoch_interval", "=", "1", ")", ":", "\n", "    ", "files", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "ckpt_dir", ",", "\"*pt\"", ")", ")", "\n", "epoch_files", "=", "[", "]", "\n", "for", "f", "in", "files", ":", "\n", "        ", "epoch", "=", "f", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ".", "replace", "(", "\"checkpoint\"", ",", "\"\"", ")", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "\n", "try", ":", "\n", "            ", "epoch", "=", "int", "(", "epoch", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "continue", "\n", "", "epoch_files", ".", "append", "(", "(", "epoch", ",", "f", ")", ")", "\n", "", "epoch_files", "=", "sorted", "(", "epoch_files", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "files", "=", "[", "f", "for", "epoch", ",", "f", "in", "epoch_files", "if", "epoch", ">=", "epoch_start", "]", "\n", "files", "=", "files", "[", ":", ":", "epoch_interval", "]", "# skip some", "\n", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename": [[90, 106], ["os.path.basename", "os.path.isdir", "os.path.isfile", "os.path.dirname", "os.path.basename.split"], "function", ["None"], ["", "def", "get_basename", "(", "path", ",", "include_path", "=", "True", ",", "include_extension", "=", "True", ")", ":", "\n", "    ", "if", "path", "is", "None", ":", "\n", "        ", "return", "None", "\n", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "path", ")", "and", "path", "[", "-", "1", "]", "==", "'/'", ":", "\n", "        ", "path", "=", "path", "[", ":", "-", "1", "]", "\n", "", "base", "=", "os", ".", "path", ".", "basename", "(", "path", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "path", ")", "and", "not", "include_extension", ":", "\n", "        ", "base", "=", "'.'", ".", "join", "(", "base", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", ")", "\n", "\n", "", "if", "include_path", ":", "\n", "        ", "dirpath", "=", "os", ".", "path", ".", "dirname", "(", "path", ")", "\n", "return", "f'{dirpath}/{base}'", "\n", "", "else", ":", "\n", "        ", "return", "base", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_cor_path": [[108, 122], ["util.get_basename", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename"], ["", "", "def", "get_cor_path", "(", "system_out", ",", "remove_unk_edits", ",", "remove_error_type_lst", ",", "\n", "apply_rerank", ",", "preserve_spell", ",", "max_edits", ")", ":", "\n", "    ", "cor_path", "=", "get_basename", "(", "system_out", ",", "include_extension", "=", "False", ")", "\n", "if", "remove_unk_edits", ":", "\n", "        ", "cor_path", "+=", "'-unk'", "\n", "", "if", "len", "(", "remove_error_type_lst", ")", ">", "0", ":", "\n", "        ", "cor_path", "+=", "'-'", ".", "join", "(", "remove_error_type_lst", ")", "\n", "", "if", "apply_rerank", ":", "\n", "        ", "cor_path", "+=", "'-rerank'", "\n", "", "if", "preserve_spell", ":", "\n", "        ", "cor_path", "+=", "'-spell'", "\n", "", "if", "max_edits", "is", "not", "None", ":", "\n", "        ", "cor_path", "+=", "f'-max-{max_edits}'", "\n", "", "return", "f\"{cor_path}.cor\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.change_ckpt_dir": [[124, 130], ["os.path.basename", "os.path.basename", "os.path.dirname().split", "os.path.dirname"], "function", ["None"], ["", "def", "change_ckpt_dir", "(", "ckpt_fpath", ",", "new_ckpt_dir", ")", ":", "\n", "    ", "fpath_basename", "=", "os", ".", "path", ".", "basename", "(", "ckpt_fpath", ")", "\n", "dirname", "=", "os", ".", "path", ".", "dirname", "(", "ckpt_fpath", ")", ".", "split", "(", "'/'", ")", "[", ":", "-", "1", "]", "\n", "new_ckpt_basename", "=", "os", ".", "path", ".", "basename", "(", "new_ckpt_dir", ")", "\n", "dirname", "=", "'/'", ".", "join", "(", "dirname", "+", "[", "new_ckpt_basename", "]", ")", "\n", "return", "f\"{dirname}/{fpath_basename}\"", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.__init__": [[20, 24], ["filepath.FilePath"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "track_num", ")", ":", "\n", "        ", "self", ".", "fp", "=", "FilePath", "(", ")", "\n", "self", ".", "TRACK_NUM", "=", "track_num", "\n", "self", ".", "TRACK_PATH", "=", "f\"{self.fp.root}/track{track_num}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.train_modes": [[25, 28], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "train_modes", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.subsets": [[29, 32], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "subsets", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_databin_path": [[33, 36], ["None"], "methods", ["None"], ["", "def", "get_databin_path", "(", "self", ",", "train_mode", ")", ":", "\n", "        ", "assert", "train_mode", "in", "self", ".", "train_modes", "\n", "return", "f\"{self.TRACK_PATH}/data-bin/{train_mode}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_ckpt_dir": [[37, 55], ["track.Track.get_ckpt_dir._get_ckpt_dir_basename"], "methods", ["None"], ["", "def", "get_ckpt_dir", "(", "self", ",", "train_mode", ",", "model", ",", "lr", "=", "5e-4", ",", "dropout", "=", "0.3", ",", "seed", "=", "None", ",", "prev_model_dir", "=", "None", ")", ":", "\n", "\n", "        ", "def", "_get_ckpt_dir_basename", "(", "train_mode", ",", "model", ",", "lr", ",", "dropout", ",", "seed", ",", "prev_model_dir", ")", ":", "\n", "            ", "basenames", "=", "[", "]", "\n", "if", "prev_model_dir", "is", "not", "None", ":", "\n", "                ", "prev_model_basename", "=", "util", ".", "get_basename", "(", "prev_model_dir", ",", "include_path", "=", "False", ",", "include_extension", "=", "False", ")", "\n", "basenames", ".", "append", "(", "prev_model_basename", ")", "\n", "\n", "", "basename", "=", "f\"{train_mode}-{model}-lr{lr}-dr{dropout}\"", "\n", "if", "seed", "is", "not", "None", ":", "\n", "                ", "basename", "+=", "f\"-s{seed}\"", "\n", "", "basenames", ".", "append", "(", "basename", ")", "\n", "\n", "return", "\"_\"", ".", "join", "(", "basenames", ")", "\n", "\n", "", "ckpt_basename", "=", "_get_ckpt_dir_basename", "(", "train_mode", ",", "model", ",", "lr", ",", "dropout", ",", "seed", ",", "prev_model_dir", ")", "\n", "\n", "return", "f\"{self.TRACK_PATH}/ckpt/{ckpt_basename}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_output_dir": [[56, 82], ["os.path.isdir", "util.get_basename", "ckpt_fpath.split", "track.Track.get_output_dir._get_output_dir_from_ckpt_dir"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.util.get_basename"], ["", "def", "get_output_dir", "(", "self", ",", "ckpt", ")", ":", "\n", "        ", "def", "_get_output_dir_from_ckpt_dir", "(", "ckpt_dir", ")", ":", "\n", "            ", "dir_basename", "=", "util", ".", "get_basename", "(", "ckpt_dir", ",", "include_path", "=", "False", ")", "\n", "return", "f\"{self.TRACK_PATH}/outputs/{dir_basename}\"", "\n", "\n", "", "def", "_get_output_dir_from_ckpt_fpath", "(", "ckpt_fpath", ")", ":", "\n", "            ", "ckpts", "=", "ckpt_fpath", ".", "split", "(", "':'", ")", "\n", "\n", "# not ensemble", "\n", "if", "len", "(", "ckpts", ")", "==", "1", ":", "\n", "                ", "ckpt_dir", "=", "os", ".", "path", ".", "dirname", "(", "ckpt_fpath", ")", "\n", "return", "_get_output_dir_from_ckpt_dir", "(", "ckpt_dir", ")", "\n", "\n", "# ensemble", "\n", "", "else", ":", "\n", "                ", "dirname_lst", "=", "[", "]", "\n", "for", "ckpt", "in", "ckpts", ":", "\n", "                    ", "ckpt_dir", "=", "os", ".", "path", ".", "dirname", "(", "ckpt", ")", "\n", "ckpt_dir_basename", "=", "util", ".", "get_basename", "(", "ckpt_dir", ",", "include_path", "=", "False", ")", "\n", "dirname_lst", ".", "append", "(", "ckpt_dir_basename", ")", "\n", "", "return", "f\"{self.TRACK_PATH}/outputs/\"", "+", "\":\"", ".", "join", "(", "dirname_lst", ")", "\n", "\n", "", "", "if", "os", ".", "path", ".", "isdir", "(", "ckpt", ")", ":", "\n", "            ", "return", "_get_output_dir_from_ckpt_dir", "(", "ckpt", ")", "\n", "", "else", ":", "\n", "            ", "return", "_get_output_dir_from_ckpt_fpath", "(", "ckpt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_subset_datapath": [[83, 86], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "get_subset_datapath", "(", "self", ",", "subset", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track.get_model_config": [[87, 120], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_model_config", "(", "model", ",", "lr", ",", "dropout", ",", "max_epoch", ",", "seed", ",", "reset", "=", "False", ")", ":", "\n", "        ", "assert", "model", "in", "[", "'base'", ",", "'copy'", ",", "'t2t'", "]", "\n", "if", "model", "==", "'base'", ":", "\n", "            ", "model_config", "=", "f\"--arch transformer --share-all-embeddings \"", "f\"--optimizer adam --lr {lr} --label-smoothing 0.1 --dropout {dropout} \"", "f\"--max-tokens 4000 --min-lr '1e-09' --lr-scheduler inverse_sqrt \"", "f\"--weight-decay 0.0001 --criterion label_smoothed_cross_entropy \"", "f\"--max-epoch {max_epoch} --warmup-updates 4000 --warmup-init-lr '1e-07' --max-tokens 4000 \"", "f\"--adam-betas '(0.9, 0.98)' --save-interval-updates 5000 \"", "\n", "\n", "", "elif", "model", "==", "'copy'", ":", "\n", "            ", "model_config", "=", "f\"--ddp-backend=no_c10d --arch copy_augmented_transformer \"", "f\"--update-freq 8 --alpha-warmup 10000 --optimizer adam --lr {lr} \"", "f\"--dropout {dropout} --max-tokens 4000 --min-lr '1e-09' --save-interval-updates 5000 \"", "f\"--lr-scheduler inverse_sqrt --weight-decay 0.0001 --max-epoch {max_epoch} \"", "f\"--warmup-updates 4000 --warmup-init-lr '1e-07' --adam-betas '(0.9, 0.98)' \"", "\n", "\n", "", "else", ":", "# model == 't2t':", "\n", "\n", "            ", "model_config", "=", "f\"--arch transformer_wmt_en_de_big_t2t --share-all-embeddings \"", "f\"--criterion label_smoothed_cross_entropy --label-smoothing 0.1 \"", "f\"--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \"", "f\"--lr-scheduler inverse_sqrt --warmup-init-lr '1e-07' --max-epoch {max_epoch} \"", "f\"--warmup-updates 4000 --lr {lr} --min-lr '1e-09' --dropout {dropout} \"", "f\"--weight-decay 0.0 --max-tokens 4000 --save-interval-updates 5000 \"", "\n", "\n", "", "if", "seed", "is", "not", "None", ":", "\n", "            ", "model_config", "+=", "f\"--seed {seed} \"", "\n", "", "if", "reset", ":", "\n", "            ", "model_config", "+=", "f\"--reset-optimizer --reset-lr-scheduler \"", "\n", "\n", "", "return", "model_config", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track0.__init__": [[123, 125], ["track.Track.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Track0", ",", "self", ")", ".", "__init__", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track0.get_pref": [[129, 139], ["os.path.splitext", "os.path.splitext", "os.path.splitext", "os.path.splitext"], "methods", ["None"], ["def", "get_pref", "(", "self", ",", "train_mode", ")", ":", "\n", "        ", "assert", "train_mode", "in", "self", ".", "train_modes", "\n", "if", "train_mode", "==", "'pretrain'", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "DAE_ORI0", ")", "[", "0", "]", "\n", "", "elif", "train_mode", "==", "'train'", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "TRAIN_ORI0", ")", "[", "0", "]", "\n", "", "else", ":", "# finetune", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "FINETUNE_ORI0", ")", "[", "0", "]", "\n", "", "validpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "VALID_ORI0", ")", "[", "0", "]", "\n", "return", "trainpref", ",", "validpref", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track0.get_subset_datapath": [[140, 165], ["None"], "methods", ["None"], ["", "def", "get_subset_datapath", "(", "self", ",", "subset", ")", ":", "\n", "        ", "assert", "subset", "in", "self", ".", "subsets", "\n", "\n", "if", "subset", "==", "'valid'", ":", "\n", "            ", "gold_m2", "=", "f\"{self.fp.conll2013_m2}/official-preprocessed.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "CONLL2013_ORI", "\n", "ori_bpe_path", "=", "None", "\n", "gen_subset", "=", "\"valid\"", "\n", "scorer_type", "=", "\"m2scorer\"", "\n", "\n", "", "elif", "subset", "==", "'conll2014'", ":", "\n", "            ", "gold_m2", "=", "f\"{self.fp.conll2014_m2}/official-2014.combined.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "CONLL2014_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "CONLL2014_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "\"m2scorer\"", "\n", "\n", "", "else", ":", "# 'jfleg':", "\n", "            ", "gold_m2", "=", "None", "\n", "ori_path", "=", "self", ".", "fp", ".", "JFLEG_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "JFLEG_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "\"jfleg\"", "\n", "\n", "", "return", "gold_m2", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "scorer_type", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track1.__init__": [[168, 170], ["track.Track.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Track1", ",", "self", ")", ".", "__init__", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track1.get_pref": [[174, 186], ["os.path.splitext", "os.path.splitext", "os.path.splitext", "os.path.splitext", "os.path.splitext"], "methods", ["None"], ["def", "get_pref", "(", "self", ",", "train_mode", ")", ":", "\n", "        ", "assert", "train_mode", "in", "self", ".", "train_modes", "\n", "if", "train_mode", "==", "'pretrain'", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "DAE_ORI1", ")", "[", "0", "]", "\n", "", "elif", "train_mode", "==", "'train'", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "TRAIN_ORI1", ")", "[", "0", "]", "\n", "", "elif", "train_mode", "==", "'finetune'", ":", "# finetune", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "FINETUNE_ORI1", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "VALID_ORI1", ")", "[", "0", "]", "\n", "", "validpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "VALID_ORI1", ")", "[", "0", "]", "\n", "return", "trainpref", ",", "validpref", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track1.get_subset_datapath": [[187, 212], ["None"], "methods", ["None"], ["", "def", "get_subset_datapath", "(", "self", ",", "subset", ")", ":", "\n", "        ", "assert", "subset", "in", "self", ".", "subsets", "\n", "\n", "if", "subset", "==", "'valid'", ":", "\n", "            ", "gold_m2", "=", "f\"{self.fp.wi_m2}/ABCN.dev.gold.bea19.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "WI_DEV_ORI", "\n", "ori_bpe_path", "=", "None", "\n", "gen_subset", "=", "\"valid\"", "\n", "scorer_type", "=", "'errant'", "\n", "\n", "", "elif", "subset", "==", "'test'", ":", "\n", "            ", "gold_m2", "=", "None", "\n", "ori_path", "=", "self", ".", "fp", ".", "WI_TEST_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "WI_TEST_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "None", "\n", "\n", "", "else", ":", "# 'conll2014':", "\n", "            ", "gold_m2", "=", "f\"{self.fp.conll2014_m2}/official-2014.combined.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "CONLL2014_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "CONLL2014_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "'m2scorer'", "\n", "\n", "", "return", "gold_m2", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "scorer_type", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.__init__": [[215, 217], ["track.Track.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Track3", ",", "self", ")", ".", "__init__", "(", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.get_pref": [[221, 229], ["os.path.splitext", "os.path.splitext", "os.path.splitext"], "methods", ["None"], ["def", "get_pref", "(", "self", ",", "train_mode", ")", ":", "\n", "        ", "assert", "train_mode", "in", "self", ".", "train_modes", "\n", "if", "train_mode", "==", "'pretrain'", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "DAE_ORI3", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "trainpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "FINETUNE_ORI3", ")", "[", "0", "]", "\n", "", "validpref", "=", "os", ".", "path", ".", "splitext", "(", "self", ".", "fp", ".", "VALID_ORI3", ")", "[", "0", "]", "\n", "return", "trainpref", ",", "validpref", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.Track3.get_subset_datapath": [[230, 255], ["None"], "methods", ["None"], ["", "def", "get_subset_datapath", "(", "self", ",", "subset", ")", ":", "\n", "        ", "assert", "subset", "in", "self", ".", "subsets", "\n", "\n", "if", "subset", "==", "'valid'", ":", "\n", "            ", "gold_m2", "=", "f\"{self.fp.wi_m2}/ABCN.dev.gold.bea19.1k.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "WI_DEV_1K_ORI", "\n", "ori_bpe_path", "=", "None", "\n", "gen_subset", "=", "\"valid\"", "\n", "scorer_type", "=", "'errant'", "\n", "\n", "", "elif", "subset", "==", "'test'", ":", "\n", "            ", "gold_m2", "=", "None", "\n", "ori_path", "=", "self", ".", "fp", ".", "WI_TEST_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "WI_TEST_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "None", "\n", "\n", "", "else", ":", "# 'conll2014':", "\n", "            ", "gold_m2", "=", "f\"{self.fp.conll2014_m2}/official-2014.combined.m2\"", "\n", "ori_path", "=", "self", ".", "fp", ".", "CONLL2014_ORI", "\n", "ori_bpe_path", "=", "self", ".", "fp", ".", "CONLL2014_TOK_ORI", "\n", "gen_subset", "=", "None", "\n", "scorer_type", "=", "'m2scorer'", "\n", "\n", "", "return", "gold_m2", ",", "ori_path", ",", "ori_bpe_path", ",", "gen_subset", ",", "scorer_type", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.track.choice_track": [[8, 17], ["track.Track0", "track.Track1", "track.Track3"], "function", ["None"], ["def", "choice_track", "(", "track_num", ")", ":", "\n", "    ", "if", "track_num", "==", "0", ":", "\n", "        ", "return", "Track0", "(", ")", "\n", "\n", "", "if", "track_num", "==", "1", ":", "\n", "        ", "return", "Track1", "(", ")", "\n", "\n", "", "if", "track_num", "==", "3", ":", "\n", "        ", "return", "Track3", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.word_tokenize.gutenberg": [[12, 28], ["open", "logging.info", "open().read().split", "para.replace().replace().replace.replace().replace().replace", "nltk.tokenize.sent_tokenize", "open().read", "nlp.tokenizer", "para.replace().replace().replace.replace().replace", "len", "len", "fout.write", "open", "para.replace().replace().replace.replace"], "function", ["None"], ["def", "gutenberg", "(", "fpaths", ",", "fout", ",", "max_tokens", ")", ":", "\n", "    ", "global", "nlp", "\n", "\n", "with", "open", "(", "fout", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "fpath", "in", "fpaths", ":", "\n", "            ", "logging", ".", "info", "(", "f\"Working on {fpath}\"", ")", "\n", "paras", "=", "open", "(", "fpath", ",", "'r'", ",", "errors", "=", "'ignore'", ")", ".", "read", "(", ")", ".", "split", "(", "\"\\n\\n\"", ")", "\n", "for", "para", "in", "paras", ":", "\n", "                ", "para", "=", "para", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", ".", "replace", "(", "\"_\"", ",", "\"\"", ")", ".", "replace", "(", "\"  \"", ",", "\" \"", ")", "\n", "sents", "=", "sent_tokenize", "(", "para", ")", "\n", "for", "sent", "in", "sents", ":", "\n", "                    ", "if", "len", "(", "sent", ")", "<", "2", ":", "continue", "\n", "doc", "=", "nlp", ".", "tokenizer", "(", "sent", ")", "\n", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "doc", "]", "\n", "if", "len", "(", "tokens", ")", "<=", "max_tokens", ":", "\n", "                        ", "fout", ".", "write", "(", "\" \"", ".", "join", "(", "tokens", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.word_tokenize.tatoeba": [[30, 44], ["open", "logging.info", "open", "line.strip", "line.strip.split", "nlp.tokenizer", "len", "fout.write"], "function", ["None"], ["", "", "", "", "", "", "def", "tatoeba", "(", "fpath", ",", "fout", ",", "max_tokens", ")", ":", "\n", "    ", "global", "nlp", "\n", "\n", "with", "open", "(", "fout", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "logging", ".", "info", "(", "f\"Working on {fpath}\"", ")", "\n", "for", "line", "in", "open", "(", "fpath", ",", "'r'", ")", ":", "\n", "            ", "sent", "=", "line", ".", "strip", "(", ")", "\n", "_", ",", "lang", ",", "sent", "=", "sent", ".", "split", "(", "\"\\t\"", ")", "\n", "if", "lang", "!=", "\"eng\"", ":", "continue", "\n", "\n", "doc", "=", "nlp", ".", "tokenizer", "(", "sent", ")", "\n", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "doc", "]", "\n", "if", "len", "(", "tokens", ")", "<=", "max_tokens", ":", "\n", "                ", "fout", ".", "write", "(", "\" \"", ".", "join", "(", "tokens", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.gec.word_tokenize.wiki103": [[46, 65], ["re.compile", "open", "logging.info", "open().read().splitlines", "para.strip", "nltk.tokenize.sent_tokenize", "open().read", "para.strip.startswith", "nlp.tokenizer", "len", "len", "open", "re.compile.match", "fout.write", "fout.write"], "function", ["None"], ["", "", "", "", "def", "wiki103", "(", "fpath", ",", "fout", ",", "max_tokens", ")", ":", "\n", "    ", "global", "nlp", "\n", "proc", "=", "re", ".", "compile", "(", "'^[A-Za-z0-9,.:/;\\-@\"!?()<>\\[\\]\\']*$'", ")", "\n", "with", "open", "(", "fout", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "logging", ".", "info", "(", "f\"Working on {fpath}\"", ")", "\n", "for", "para", "in", "open", "(", "fpath", ",", "'r'", ")", ".", "read", "(", ")", ".", "splitlines", "(", ")", ":", "\n", "            ", "p", "=", "para", ".", "strip", "(", ")", "\n", "if", "p", ".", "startswith", "(", "'='", ")", "or", "p", "==", "''", ":", "continue", "\n", "sents", "=", "sent_tokenize", "(", "p", ")", "\n", "for", "sent", "in", "sents", ":", "\n", "                ", "if", "len", "(", "sent", ")", "<", "2", ":", "continue", "\n", "doc", "=", "nlp", ".", "tokenizer", "(", "sent", ")", "\n", "tokens", "=", "[", "token", ".", "text", "if", "proc", ".", "match", "(", "token", ".", "text", ")", "else", "'<unk>'", "for", "token", "in", "doc", "]", "\n", "if", "len", "(", "tokens", ")", "<=", "max_tokens", ":", "\n", "                    ", "line", "=", "\" \"", ".", "join", "(", "tokens", ")", ".", "replace", "(", "'< unk >'", ",", "'<unk>'", ")", "\n", "if", "'('", "in", "line", "and", "')'", "not", "in", "line", ":", "\n", "                        ", "fout", ".", "write", "(", "line", "+", "\" \"", ")", "\n", "", "else", ":", "\n", "                        ", "fout", ".", "write", "(", "line", "+", "\"\\n\"", ")", "\n", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.generate.main": [[19, 183], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "fairseq.utils.load_ensemble_for_inference", "fairseq.utils.load_align_dict", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "tasks.setup_task.build_generator", "print", "torch.cuda.is_available", "getattr", "args.path.split", "model.make_generation_fast_", "fairseq.bleu.SacrebleuScorer", "fairseq.bleu.Scorer", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "len", "eval", "model.half", "model.cuda", "tasks.setup_task.get_batch_iterator", "tgt_dict.pad", "tgt_dict.eos", "tgt_dict.unk", "fairseq.meters.StopwatchMeter.start", "tasks.setup_task.inference_step", "sum", "fairseq.meters.StopwatchMeter.stop", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "sample[].tolist", "fairseq.utils.strip_pad", "enumerate", "bleu.Scorer.result_string", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "len", "tgt_dict.pad", "fairseq.utils.strip_pad().int().cpu", "tasks.setup_task.dataset().src.get_original_text", "tasks.setup_task.dataset().tgt.get_original_text", "fairseq.utils.post_process_prediction", "round", "tasks.setup_task.max_positions", "getattr.string", "tgt_dict.string", "print", "print", "print", "print", "hasattr", "fairseq.utils.strip_pad().int", "min", "hypo[].int().cpu", "print", "tgt_dict.encode_line", "bleu.Scorer.add_string", "bleu.Scorer.add", "model.max_positions", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "len", "hypo[].int().cpu", "fairseq.utils.strip_pad", "hypo[].int", "map", "tgt_dict.pad", "hypo[].int", "hypo[].tolist", "map", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.add_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["f\"--beam {beam} --max-tokens {max_tokens} --buffer-size {buffer_size} > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "\n", "", "elif", "gen_subset", "is", "not", "None", ":", "\n", "        ", "generate", "=", "f\"fairseq-generate {data_path} --path {ckpt} --gen-subset {gen_subset} \"", "f\"--beam {beam} --max-tokens {max_tokens} --print-alignment > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.generate.cli_main": [[185, 189], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm_fp16.WordStat.__init__": [[26, 33], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "word", ",", "is_bpe", ")", ":", "\n", "        ", "self", ".", "word", "=", "word", "\n", "self", ".", "is_bpe", "=", "is_bpe", "\n", "self", ".", "log_prob", "=", "0", "\n", "self", ".", "next_word_prob", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "missing_next_words", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm_fp16.WordStat.add": [[34, 45], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "log_prob", ",", "next_word_prob", ")", ":", "\n", "        ", "\"\"\" increments counters for the sum of log probs of current word and next\n            word (given context ending at current word). Since the next word might be at the end of the example,\n            or it might be not counted because it is not an ending subword unit,\n            also keeps track of how many of those we have seen \"\"\"", "\n", "if", "next_word_prob", "is", "not", "None", ":", "\n", "            ", "self", ".", "next_word_prob", "+=", "next_word_prob", "\n", "", "else", ":", "\n", "            ", "self", ".", "missing_next_words", "+=", "1", "\n", "", "self", ".", "log_prob", "+=", "log_prob", "\n", "self", ".", "count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm_fp16.WordStat.__str__": [[46, 49], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "'{}\\t{}\\t{}\\t{}\\t{}\\t{}'", ".", "format", "(", "self", ".", "word", ",", "self", ".", "count", ",", "self", ".", "log_prob", ",", "self", ".", "is_bpe", ",", "\n", "self", ".", "next_word_prob", ",", "self", ".", "count", "-", "self", ".", "missing_next_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm_fp16.main": [[51, 189], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "vars().keys", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "fairseq.sequence_scorer.SequenceScorer", "dict", "print", "print", "torch.cuda.is_available", "parsed_args.path.split", "model.make_generation_fast_", "len", "len", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "sorted", "eval", "vars", "setattr", "len", "model.half", "model.cuda", "sum", "tasks.setup_task.get_batch_iterator", "args.remove_bpe.rstrip", "set", "fairseq.meters.StopwatchMeter.start", "fairseq.sequence_scorer.SequenceScorer.generate", "fairseq.meters.StopwatchMeter.stop", "fairseq.meters.TimeMeter.update", "t.log", "numpy.exp", "dict.values", "print", "getattr", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "inf_scores.any", "pos_scores.sum().cpu().item", "p.numel", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "range", "pos_scores.eq", "pos_scores.eq", "print", "pos_scores.numel", "range", "round", "models[].parameters", "range", "tasks.setup_task.dictionary[].endswith", "float", "float", "tasks.setup_task.target_dictionary.string", "pos_scores.sum().cpu", "len", "[].item", "print", "len", "len", "[].item", "word_prob.append", "dict.setdefault().add", "model.max_positions", "pos_scores.sum", "len", "pos_scores[].item", "inf_scores.nonzero", "pos_scores[].item", "pos_scores[].item", "dict.setdefault", "eval_lm_fp16.WordStat"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault"], ["", "", "def", "main", "(", "parsed_args", ")", ":", "\n", "    ", "assert", "parsed_args", ".", "path", "is", "not", "None", ",", "'--path required for evaluation!'", "\n", "\n", "import_user_module", "(", "parsed_args", ")", "\n", "\n", "print", "(", "parsed_args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "parsed_args", ".", "cpu", "\n", "\n", "task", "=", "tasks", ".", "setup_task", "(", "parsed_args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "parsed_args", ".", "path", ")", ")", "\n", "models", ",", "args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "parsed_args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "parsed_args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "for", "arg", "in", "vars", "(", "parsed_args", ")", ".", "keys", "(", ")", ":", "\n", "        ", "if", "arg", "not", "in", "{", "'self_target'", ",", "'future_target'", ",", "'past_target'", ",", "'tokens_per_sample'", ",", "'output_size_dictionary'", "}", ":", "\n", "            ", "setattr", "(", "args", ",", "arg", ",", "getattr", "(", "parsed_args", ",", "arg", ")", ")", "\n", "", "", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load dataset splits", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation and set the source and dest dicts on the model (required by scorer)", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "", "assert", "len", "(", "models", ")", ">", "0", "\n", "\n", "print", "(", "'num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "models", "[", "0", "]", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", "or", "36000", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "*", "[", "\n", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "\n", "]", ")", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ")", "\n", "\n", "score_sum", "=", "0.", "\n", "count", "=", "0", "\n", "\n", "if", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "remove_bpe", "==", "'sentencepiece'", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "bpe_cont", "=", "args", ".", "remove_bpe", ".", "rstrip", "(", ")", "\n", "bpe_toks", "=", "set", "(", "i", "for", "i", "in", "range", "(", "len", "(", "task", ".", "dictionary", ")", ")", "if", "task", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont", ")", ")", "\n", "", "bpe_len", "=", "len", "(", "bpe_cont", ")", "\n", "", "else", ":", "\n", "        ", "bpe_toks", "=", "None", "\n", "bpe_len", "=", "0", "\n", "\n", "", "word_stats", "=", "dict", "(", ")", "\n", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "scorer", ".", "generate", "(", "models", ",", "sample", ")", "\n", "gen_timer", ".", "stop", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "\n", "for", "hypos_i", "in", "hypos", ":", "\n", "                ", "hypo", "=", "hypos_i", "[", "0", "]", "\n", "pos_scores", "=", "hypo", "[", "'positional_scores'", "]", "\n", "\n", "skipped_toks", "=", "0", "\n", "if", "bpe_toks", "is", "not", "None", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", "-", "1", ")", ":", "\n", "                        ", "if", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "in", "bpe_toks", ":", "\n", "                            ", "skipped_toks", "+=", "1", "\n", "pos_scores", "[", "i", "+", "1", "]", "+=", "pos_scores", "[", "i", "]", "\n", "pos_scores", "[", "i", "]", "=", "0", "\n", "\n", "", "", "", "inf_scores", "=", "pos_scores", ".", "eq", "(", "float", "(", "'inf'", ")", ")", "|", "pos_scores", ".", "eq", "(", "float", "(", "'-inf'", ")", ")", "\n", "if", "inf_scores", ".", "any", "(", ")", ":", "\n", "                    ", "print", "(", "'| Skipping tokens with inf scores:'", ",", "\n", "task", ".", "target_dictionary", ".", "string", "(", "hypo", "[", "'tokens'", "]", "[", "inf_scores", ".", "nonzero", "(", ")", "]", ")", ")", "\n", "pos_scores", "=", "pos_scores", "[", "(", "~", "inf_scores", ")", ".", "nonzero", "(", ")", "]", "\n", "", "score_sum", "+=", "pos_scores", ".", "sum", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "# custom fix to work with fp16", "\n", "count", "+=", "pos_scores", ".", "numel", "(", ")", "-", "skipped_toks", "\n", "\n", "if", "args", ".", "output_word_probs", "or", "args", ".", "output_word_stats", ":", "\n", "                    ", "w", "=", "''", "\n", "word_prob", "=", "[", "]", "\n", "is_bpe", "=", "False", "\n", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", ")", ":", "\n", "                        ", "w_ind", "=", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "w", "+=", "task", ".", "dictionary", "[", "w_ind", "]", "\n", "if", "bpe_toks", "is", "not", "None", "and", "w_ind", "in", "bpe_toks", ":", "\n", "                            ", "w", "=", "w", "[", ":", "-", "bpe_len", "]", "\n", "is_bpe", "=", "True", "\n", "", "else", ":", "\n", "                            ", "word_prob", ".", "append", "(", "(", "w", ",", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "next_prob", "=", "None", "\n", "ind", "=", "i", "+", "1", "\n", "while", "ind", "<", "len", "(", "hypo", "[", "'tokens'", "]", ")", ":", "\n", "                                ", "if", "pos_scores", "[", "ind", "]", ".", "item", "(", ")", "!=", "0", ":", "\n", "                                    ", "next_prob", "=", "pos_scores", "[", "ind", "]", "\n", "break", "\n", "", "ind", "+=", "1", "\n", "\n", "", "word_stats", ".", "setdefault", "(", "w", ",", "WordStat", "(", "w", ",", "is_bpe", ")", ")", ".", "add", "(", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ",", "next_prob", ")", "\n", "is_bpe", "=", "False", "\n", "w", "=", "''", "\n", "", "", "if", "args", ".", "output_word_probs", ":", "\n", "                        ", "print", "(", "'\\t'", ".", "join", "(", "'{} [{:2f}]'", ".", "format", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "for", "x", "in", "word_prob", ")", ")", "\n", "\n", "", "", "", "wps_meter", ".", "update", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "\n", "", "", "avg_nll_loss", "=", "-", "score_sum", "/", "count", "\n", "print", "(", "'| Evaluated {} tokens in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "print", "(", "'| Loss: {:.4f}, Perplexity: {:.2f}'", ".", "format", "(", "avg_nll_loss", ",", "np", ".", "exp", "(", "avg_nll_loss", ")", ")", ")", "\n", "\n", "if", "args", ".", "output_word_stats", ":", "\n", "        ", "for", "ws", "in", "sorted", "(", "word_stats", ".", "values", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", ".", "count", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "ws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm_fp16.cli_main": [[191, 195], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm_fp16.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.interactive.buffered_read": [[26, 37], ["fileinput.input", "len", "buffer.append", "fileinput.hook_encoded", "src_str.strip", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["def", "buffered_read", "(", "input", ",", "buffer_size", ")", ":", "\n", "    ", "buffer", "=", "[", "]", "\n", "with", "fileinput", ".", "input", "(", "files", "=", "[", "input", "]", ",", "openhook", "=", "fileinput", ".", "hook_encoded", "(", "\"utf-8\"", ")", ")", "as", "h", ":", "\n", "        ", "for", "src_str", "in", "h", ":", "\n", "            ", "buffer", ".", "append", "(", "src_str", ".", "strip", "(", ")", ")", "\n", "if", "len", "(", "buffer", ")", ">=", "buffer_size", ":", "\n", "                ", "yield", "buffer", "\n", "buffer", "=", "[", "]", "\n", "\n", "", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "yield", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.interactive.make_batches": [[39, 55], ["torch.LongTensor", "task.get_batch_iterator().next_epoch_itr", "task.source_dictionary.encode_line().long", "t.numel", "task.get_batch_iterator", "Batch", "task.source_dictionary.encode_line", "task.build_dataset_for_inference"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_dataset_for_inference"], ["", "", "def", "make_batches", "(", "lines", ",", "args", ",", "task", ",", "max_positions", ")", ":", "\n", "    ", "tokens", "=", "[", "\n", "task", ".", "source_dictionary", ".", "encode_line", "(", "src_str", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "for", "src_str", "in", "lines", "\n", "]", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "[", "t", ".", "numel", "(", ")", "for", "t", "in", "tokens", "]", ")", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "build_dataset_for_inference", "(", "tokens", ",", "lengths", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "itr", ":", "\n", "        ", "yield", "Batch", "(", "\n", "ids", "=", "batch", "[", "'id'", "]", ",", "\n", "src_tokens", "=", "batch", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ",", "src_lengths", "=", "batch", "[", "'net_input'", "]", "[", "'src_lengths'", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.interactive.main": [[58, 164], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "tasks.setup_task.build_generator", "fairseq.utils.load_align_dict", "fairseq.utils.resolve_max_positions", "print", "interactive.buffered_read", "torch.cuda.is_available", "args.path.split", "model.make_generation_fast_", "tasks.setup_task.max_positions", "print", "interactive.make_batches", "sorted", "len", "eval", "model.half", "model.cuda", "tasks.setup_task.inference_step", "enumerate", "model.max_positions", "src_tokens.cuda.cuda", "src_lengths.cuda.cuda", "zip", "fairseq.utils.strip_pad", "results.append", "src_dict.string", "print", "fairseq.utils.post_process_prediction", "print", "print", "batch.ids.tolist", "tgt_dict.pad", "min", "print", "len", "hypo[].int().cpu", "hypo[].int().cpu", "map", "hypo[].int", "hypo[].tolist", "map", "hypo[].int", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.buffered_read", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.make_batches", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "import_user_module", "(", "args", ")", "\n", "\n", "if", "args", ".", "buffer_size", "<", "1", ":", "\n", "        ", "args", ".", "buffer_size", "=", "1", "\n", "", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences", "=", "1", "\n", "\n", "", "assert", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", ",", "'--sampling requires --nbest to be equal to --beam'", "\n", "assert", "not", "args", ".", "max_sentences", "or", "args", ".", "max_sentences", "<=", "args", ".", "buffer_size", ",", "'--max-sentences/--batch-size cannot be larger than --buffer-size'", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Setup task, e.g., translation", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "# Set dictionaries", "\n", "src_dict", "=", "task", ".", "source_dictionary", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "# Initialize generator", "\n", "", "", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "\n", "*", "[", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "]", "\n", ")", "\n", "\n", "if", "args", ".", "buffer_size", ">", "1", ":", "\n", "        ", "print", "(", "'| Sentence buffer size:'", ",", "args", ".", "buffer_size", ")", "\n", "", "print", "(", "'| Type the input sentence and press return:'", ")", "\n", "start_id", "=", "0", "\n", "for", "inputs", "in", "buffered_read", "(", "args", ".", "input", ",", "args", ".", "buffer_size", ")", ":", "\n", "        ", "results", "=", "[", "]", "\n", "for", "batch", "in", "make_batches", "(", "inputs", ",", "args", ",", "task", ",", "max_positions", ")", ":", "\n", "            ", "src_tokens", "=", "batch", ".", "src_tokens", "\n", "src_lengths", "=", "batch", ".", "src_lengths", "\n", "if", "use_cuda", ":", "\n", "                ", "src_tokens", "=", "src_tokens", ".", "cuda", "(", ")", "\n", "src_lengths", "=", "src_lengths", ".", "cuda", "(", ")", "\n", "\n", "", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n", "}", "\n", "translations", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ")", "\n", "for", "i", ",", "(", "id", ",", "hypos", ")", "in", "enumerate", "(", "zip", "(", "batch", ".", "ids", ".", "tolist", "(", ")", ",", "translations", ")", ")", ":", "\n", "                ", "src_tokens_i", "=", "utils", ".", "strip_pad", "(", "src_tokens", "[", "i", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", "\n", "results", ".", "append", "(", "(", "start_id", "+", "id", ",", "src_tokens_i", ",", "hypos", ")", ")", "\n", "\n", "# sort output to match input order", "\n", "", "", "for", "id", ",", "src_tokens", ",", "hypos", "in", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "            ", "if", "src_dict", "is", "not", "None", ":", "\n", "                ", "src_str", "=", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "id", ",", "src_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "                ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "else", "None", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "tgt_dict", "=", "tgt_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "id", ",", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", ")", "\n", "if", "args", ".", "print_alignment", ":", "\n", "                    ", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# update running id counter", "\n", "", "", "", "start_id", "+=", "len", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.interactive.cli_main": [[166, 170], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "interactive.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", "interactive", "=", "True", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.main": [[27, 119], ["fairseq.utils.import_user_module", "print", "torch.manual_seed", "fairseq.tasks.setup_task", "train.load_dataset_splits", "tasks.setup_task.build_model", "tasks.setup_task.build_criterion", "print", "print", "print", "fairseq.utils.resolve_max_positions", "tasks.setup_task.dataset().get_dummy_batch", "tasks.setup_task.dataset().get_dummy_batch", "fairseq.trainer.Trainer", "print", "print", "tasks.setup_task.get_batch_iterator", "fairseq.trainer.Trainer.get_lr", "fairseq.meters.StopwatchMeter", "fairseq.meters.StopwatchMeter.start", "args.valid_subset.split", "fairseq.meters.StopwatchMeter.stop", "print", "torch.cuda.is_available", "torch.cuda.set_device", "fairseq.distributed_utils.distributed_init", "print", "tasks.setup_task.max_positions", "task.build_model.max_positions", "train.load_checkpoint", "fairseq.trainer.Trainer.dummy_train_step", "train.train", "fairseq.trainer.Trainer.lr_step", "sum", "sum", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "fairseq.trainer.Trainer.get_num_updates", "train.validate", "train.save_checkpoint", "socket.gethostname", "p.numel", "p.numel", "task.build_model.parameters", "task.build_model.parameters"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_dataset_splits", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.dummy_train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.validate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint"], ["\n", "\n", "", "def", "train", "(", "databin_path", ",", "model_config", ",", "ckpt_dir", ",", "restore_ckpt", ",", "ngpu", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "ckpt_dir", ",", "exist_ok", "=", "True", ")", "\n", "logging", ".", "info", "(", "f\"[Train] working on {ckpt_dir}\"", ")", "\n", "\n", "if", "ngpu", ">", "1", ":", "\n", "        ", "prompt", "=", "f\"python -m torch.distributed.launch --nproc_per_node {ngpu} $(which fairseq-train) {databin_path} \"", "f\"{model_config} --save-dir {ckpt_dir} \"", "\n", "", "else", ":", "\n", "        ", "prompt", "=", "f\"fairseq-train {databin_path} {model_config} --save-dir {ckpt_dir} \"", "\n", "", "logging", ".", "info", "(", "f\"[Train] {prompt}\"", ")", "\n", "\n", "if", "restore_ckpt", "is", "not", "None", ":", "\n", "        ", "finetune_ckpt", "=", "os", ".", "path", ".", "basename", "(", "util", ".", "change_ckpt_dir", "(", "restore_ckpt", ",", "ckpt_dir", ")", ")", "\n", "logging", ".", "info", "(", "f\"[Train] copy the ckpt {restore_ckpt} into {finetune_ckpt}\"", ")", "\n", "os", ".", "system", "(", "f\"cp {restore_ckpt} {finetune_ckpt}\"", ")", "\n", "\n", "prompt", "+=", "f\"--restore-file {finetune_ckpt} \"", "\n", "", "os", ".", "system", "(", "prompt", ")", "\n", "\n", "\n", "", "def", "find_restore", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", ":", "\n", "    ", "if", "prev_model_output_dir", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "highest_fscore", ",", "highest_ckpt", "=", "util", ".", "find_highest_score", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "logging", ".", "info", "(", "f\"[Train] highest fscore: {highest_fscore}, ckpt: {highest_ckpt}\"", ")", "\n", "if", "highest_fscore", "==", "0", "and", "highest_ckpt", "==", "'.pt'", ":", "\n", "        ", "logging", ".", "error", "(", "f\"[Train] cannot find the highest ckpt\"", ")", "\n", "exit", "(", ")", "\n", "", "return", "highest_fscore", ",", "highest_ckpt", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--track\"", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--train-mode\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--ngpu\"", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "5e-4", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.3", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epoch\"", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "\"--reset\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--prev-model-output-dir\"", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "main", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.train": [[121, 182], ["epoch_itr.next_epoch_itr", "fairseq.data.iterators.GroupedIterator", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "enumerate", "train.get_training_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "args.valid_subset.split", "trainer.train_step", "train.get_training_stats", "trainer.train_step.items", "progress_bar.build_progress_bar.log", "trainer.get_num_updates", "trainer.get_meter", "len", "fairseq.meters.AverageMeter", "trainer.get_meter().reset", "train.validate", "train.save_checkpoint", "trainer.get_meter.reset", "extra_meters[].update", "extra_meters[].update", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_training_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_training_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.validate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.get_training_stats": [[184, 207], ["collections.OrderedDict", "trainer.get_meter", "train.get_perplexity", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_num_updates", "trainer.get_lr", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "round", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_perplexity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.validate": [[209, 258], ["task.get_batch_iterator().next_epoch_itr", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "train.get_valid_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "valid_losses.append", "trainer.get_meter", "trainer.valid_step", "trainer.valid_step.items", "task.get_batch_iterator", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_num_updates", "task.dataset", "fairseq.utils.resolve_max_positions", "task.max_positions", "trainer.get_model().max_positions", "trainer.get_model"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_valid_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.valid_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_model"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.get_valid_stats": [[260, 273], ["collections.OrderedDict", "trainer.get_meter", "train.get_perplexity", "trainer.get_num_updates", "hasattr", "trainer.get_meter", "min", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_perplexity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.get_perplexity": [[275, 280], ["math.pow", "float"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.save_checkpoint": [[282, 332], ["epoch_itr.end_of_epoch", "trainer.get_num_updates", "collections.OrderedDict", "getattr", "hasattr", "min", "epoch_itr.state_dict", "extra_state.update", "os.path.join", "len", "fairseq.utils.checkpoint_paths", "fairseq.utils.checkpoint_paths", "fairseq.distributed_utils.is_master", "collections.OrderedDict.items", "trainer.save_checkpoint", "os.path.lexists", "os.path.lexists", "hasattr", "os.remove", "os.remove"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.end_of_epoch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.checkpoint_paths", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.checkpoint_paths", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.load_checkpoint": [[334, 359], ["os.makedirs", "os.path.isabs", "os.path.isfile", "os.path.join", "trainer.load_checkpoint", "print", "eval", "epoch_itr.load_state_dict", "print", "trainer.lr_step", "trainer.lr_step_update", "trainer.get_num_updates", "trainer.get_num_updates"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step_update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.load_dataset_splits": [[361, 374], ["task.load_dataset", "itertools.count", "task.load_dataset", "str"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.distributed_main": [[376, 381], ["train.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.train.cli_main": [[383, 408], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "fairseq.distributed_utils.infer_init_method", "train.distributed_main", "random.randint", "torch.multiprocessing.spawn", "train.main", "print", "max"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.infer_init_method", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.distributed_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.generate_or_copy.main": [[32, 257], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "fairseq.utils.load_ensemble_for_inference", "fairseq.utils.load_align_dict", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "tasks.setup_task.build_generator", "print", "torch.cuda.is_available", "getattr", "args.path.split", "model.make_generation_fast_", "fairseq.bleu.SacrebleuScorer", "fairseq.bleu.Scorer", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "len", "eval", "model.half", "model.cuda", "tasks.setup_task.get_batch_iterator", "tgt_dict.pad", "tgt_dict.eos", "tgt_dict.unk", "fairseq.meters.StopwatchMeter.start", "tasks.setup_task.inference_step", "sum", "fairseq.meters.StopwatchMeter.stop", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "[].get", "print", "sample[].tolist", "fairseq.utils.strip_pad", "tasks.setup_task.dataset", "task.dataset.src.get_original_text", "task.dataset.src_dict.string", "task.dataset.tgt.get_original_text", "enumerate", "bleu.Scorer.result_string", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "len", "tgt_dict.pad", "fairseq.utils.strip_pad().int().cpu", "fairseq.utils.post_process_prediction", "rawtext_dataset.src.get_original_text.split", "enumerate", "round", "tasks.setup_task.max_positions", "print", "print", "print", "print", "print", "hasattr", "fairseq.utils.strip_pad().int", "min", "hypo[].int().cpu", "[].cpu", "torch.argmax().item", "print", "final_hypo_tokens_str.append", "print", "tgt_dict.encode_line", "bleu.Scorer.add_string", "bleu.Scorer.add", "model.max_positions", "len", "hypo[].int().cpu", "tgt_dict.unk", "len", "len", "print", "final_hypo_tokens_str.append", "fairseq.utils.strip_pad", "hypo[].int", "len", "torch.argmax", "map", "tgt_dict.pad", "hypo[].int", "map", "hypo[].tolist", "map", "[].cpu.tolist", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.add_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "path", "is", "not", "None", ",", "'--path required for generation!'", "\n", "assert", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", ",", "'--sampling requires --nbest to be equal to --beam'", "\n", "assert", "args", ".", "replace_unk", "is", "None", "or", "args", ".", "raw_text", ",", "'--replace-unk requires a raw text dataset (--raw-text)'", "\n", "\n", "import_user_module", "(", "args", ")", "\n", "\n", "\"\"\"\n    MODIFIED: The GEC task uses token-labeled raw text datasets, which \n    require raw text to be used.\n    \"\"\"", "\n", "assert", "args", ".", "raw_text", ",", "f\"--raw-text option is required for copy-based generation.\"", "\n", "\n", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_tokens", "=", "12000", "\n", "", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load dataset splits", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", ")", ")", "\n", "\n", "# Set dictionaries", "\n", "try", ":", "\n", "        ", "src_dict", "=", "getattr", "(", "task", ",", "'source_dictionary'", ",", "None", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "        ", "src_dict", "=", "None", "\n", "", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "\n", "*", "[", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "]", "\n", ")", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "8", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "# Initialize generator", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "# Generate and compute BLEU score", "\n", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "scorer", "=", "bleu", ".", "SacrebleuScorer", "(", ")", "\n", "", "else", ":", "\n", "        ", "scorer", "=", "bleu", ".", "Scorer", "(", "tgt_dict", ".", "pad", "(", ")", ",", "tgt_dict", ".", "eos", "(", ")", ",", "tgt_dict", ".", "unk", "(", ")", ")", "\n", "", "num_sentences", "=", "0", "\n", "has_target", "=", "True", "\n", "has_copy_scores", "=", "True", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "prefix_tokens", "=", "None", "\n", "if", "args", ".", "prefix_size", ">", "0", ":", "\n", "                ", "prefix_tokens", "=", "sample", "[", "'target'", "]", "[", ":", ",", ":", "args", ".", "prefix_size", "]", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "\"\"\"\n            MODIFIED: Use copy scores to replace <unk>'s with raw source words.\n            \n            use_copy_scores may be False with non-copy-based transformers that\n            only use edit labels (e.g., transformer_aux_el and transformer_el).\n            \"\"\"", "\n", "hypos", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", ")", "\n", "use_copy_scores", "=", "hypos", "[", "0", "]", "[", "0", "]", ".", "get", "(", "'copy_scores'", ",", "None", ")", "is", "not", "None", "\n", "if", "has_copy_scores", "and", "not", "use_copy_scores", ":", "\n", "                ", "print", "(", "\"| generate_or_copy.py | INFO | \"", "\n", "\"Model does not include copy scores. \"", "\n", "\"Generating hypotheses without replacing UNKs.\"", ")", "\n", "has_copy_scores", "=", "False", "\n", "", "num_generated_tokens", "=", "sum", "(", "len", "(", "h", "[", "0", "]", "[", "'tokens'", "]", ")", "for", "h", "in", "hypos", ")", "\n", "gen_timer", ".", "stop", "(", "num_generated_tokens", ")", "\n", "\n", "for", "i", ",", "sample_id", "in", "enumerate", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "has_target", "=", "sample", "[", "'target'", "]", "is", "not", "None", "\n", "\n", "# Remove padding", "\n", "src_tokens", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "i", ",", ":", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", "\n", "target_tokens", "=", "None", "\n", "if", "has_target", ":", "\n", "                    ", "target_tokens", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'target'", "]", "[", "i", ",", ":", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", ".", "int", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "", "\"\"\"\n                MODIFIED: Replace <unk>s with raw source tokens. \n                This is analogous to the case where align_dict is provided\n                in the original generate.py.\n                \"\"\"", "\n", "rawtext_dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", "\n", "src_str", "=", "rawtext_dataset", ".", "src", ".", "get_original_text", "(", "sample_id", ")", "\n", "tokenized_src_str", "=", "rawtext_dataset", ".", "src_dict", ".", "string", "(", "\n", "src_tokens", ",", "bpe_symbol", "=", "args", ".", "remove_bpe", "\n", ")", "\n", "target_str", "=", "rawtext_dataset", ".", "tgt", ".", "get_original_text", "(", "sample_id", ")", "\n", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "                    ", "if", "src_dict", "is", "not", "None", ":", "\n", "# Raw source text", "\n", "                        ", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "sample_id", ",", "src_str", ")", ")", "\n", "# Tokenized source text", "\n", "print", "(", "'K-{}\\t{}'", ".", "format", "(", "sample_id", ",", "tokenized_src_str", ")", ")", "\n", "", "if", "has_target", ":", "\n", "                        ", "print", "(", "'T-{}\\t{}'", ".", "format", "(", "sample_id", ",", "target_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "", "for", "k", ",", "hypo", "in", "enumerate", "(", "hypos", "[", "i", "]", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ")", ":", "\n", "                    ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "else", "None", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "tgt_dict", "=", "tgt_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "\n", "\"\"\"\n                    MODIFIED: Replace predicted <unk>s with the source token\n                    that received the highest score.\n                    \"\"\"", "\n", "raw_src_tokens", "=", "src_str", ".", "split", "(", ")", "\n", "final_hypo_tokens_str", "=", "[", "]", "\n", "for", "tgt_position", ",", "hypo_token", "in", "enumerate", "(", "hypo_tokens", ")", ":", "\n", "                        ", "if", "use_copy_scores", "and", "hypo_token", "==", "tgt_dict", ".", "unk", "(", ")", ":", "\n", "# See sequence_copygenerator.py#L292 for details.", "\n", "                            ", "copy_scores", "=", "hypo", "[", "'copy_scores'", "]", "[", ":", ",", "tgt_position", "]", ".", "cpu", "(", ")", "\n", "assert", "len", "(", "copy_scores", ")", "-", "1", "==", "len", "(", "raw_src_tokens", ")", ",", "f\"length of copy scores do not match input source tokens \"", "f\"(copy_scores: {copy_scores}, raw_src_tokens: {raw_src_tokens})\"", "\n", "src_position", "=", "torch", ".", "argmax", "(", "copy_scores", ")", ".", "item", "(", ")", "\n", "# Don't copy if attending to an EOS (not ideal).", "\n", "if", "src_position", "==", "len", "(", "raw_src_tokens", ")", ":", "\n", "                                ", "print", "(", "\"WARNING: copy score highest at EOS.\"", ")", "\n", "", "else", ":", "\n", "                                ", "final_hypo_tokens_str", ".", "append", "(", "\n", "raw_src_tokens", "[", "src_position", "]", "\n", ")", "\n", "", "print", "(", "'U-{}\\t{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "tgt_position", ",", "\n", "' '", ".", "join", "(", "map", "(", "\n", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "\n", "copy_scores", ".", "tolist", "(", ")", ",", "\n", ")", ")", ",", "\n", ")", ")", "\n", "", "else", ":", "\n", "                            ", "final_hypo_tokens_str", ".", "append", "(", "tgt_dict", "[", "hypo_token", "]", ")", "\n", "\n", "# Note: raw input tokens could be included here.", "\n", "", "", "final_hypo_str", "=", "' '", ".", "join", "(", "\n", "[", "token", "for", "token", "in", "final_hypo_tokens_str", "\n", "if", "token", "!=", "tgt_dict", ".", "eos_word", "]", "\n", ")", "\n", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "                        ", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "sample_id", ",", "hypo", "[", "'score'", "]", ",", "final_hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "\n", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "\n", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ",", "\n", ")", ")", "\n", ")", ")", "\n", "\n", "if", "args", ".", "print_alignment", ":", "\n", "                            ", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# Score only the top hypothesis", "\n", "", "", "if", "has_target", "and", "k", "==", "0", ":", "\n", "                        ", "if", "align_dict", "is", "not", "None", "or", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "# Convert back to tokens for evaluation with unk replacement and/or without BPE", "\n", "                            ", "target_tokens", "=", "tgt_dict", ".", "encode_line", "(", "target_str", ",", "add_if_not_exist", "=", "True", ")", "\n", "", "if", "hasattr", "(", "scorer", ",", "'add_string'", ")", ":", "\n", "                            ", "scorer", ".", "add_string", "(", "target_str", ",", "hypo_str", ")", "\n", "", "else", ":", "\n", "                            ", "scorer", ".", "add", "(", "target_tokens", ",", "hypo_tokens", ")", "\n", "\n", "", "", "", "", "wps_meter", ".", "update", "(", "num_generated_tokens", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "sample", "[", "'nsentences'", "]", "\n", "\n", "", "", "print", "(", "'| Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)'", ".", "format", "(", "\n", "num_sentences", ",", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "num_sentences", "/", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "if", "has_target", ":", "\n", "        ", "print", "(", "'| Generate {} with beam={}: {}'", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ",", "scorer", ".", "result_string", "(", ")", ")", ")", "\n", "", "return", "scorer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.generate_or_copy.cli_main": [[259, 266], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate_or_copy.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "\"\"\"\n    MODIFIED: task defaults to gec\n    \"\"\"", "\n", "parser", "=", "options", ".", "get_generation_parser", "(", "default_task", "=", "'gec'", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.main": [[25, 234], ["fairseq.utils.import_user_module", "print", "os.makedirs", "fairseq.tasks.get_task", "build_dictionary.save", "preprocess.main.make_all"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.get_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save"], ["\n", "\n", "", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# 1. word-tokenize", "\n", "parser", ".", "add_argument", "(", "\"--max_tokens\"", ",", "type", "=", "int", ",", "default", "=", "150", ",", "\n", "help", "=", "\"Maximum number of tokens in a sample\"", ")", "\n", "\n", "# 2. train bpe model", "\n", "parser", ".", "add_argument", "(", "\"--vocab_size\"", ",", "type", "=", "int", ",", "default", "=", "32000", ",", "\n", "help", "=", "\"vocabulary size\"", ")", "\n", "\n", "# 3. perturbation -> bpe-tokenize", "\n", "parser", ".", "add_argument", "(", "\"--min_cnt\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_change_prob\"", ",", "type", "=", "float", ",", "default", "=", ".9", ")", "\n", "parser", ".", "add_argument", "(", "\"--type_change_prob\"", ",", "type", "=", "float", ",", "default", "=", ".1", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "1", ",", "12", ",", "5", "]", ",", "\n", "help", "=", "\"list of n_epochs of gutenberg, tatoeba, and wiki103\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "fp", "=", "filepath", ".", "FilePath", "(", ")", "\n", "fp", ".", "make_dirs", "(", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0. Download data\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 0-1. Download Gutenberg Text\"", ")", "\n", "maybe_download", "(", "fp", ".", "gutenberg", ",", "\n", "f\"gdown https://drive.google.com/uc?id=0B2Mzhc7popBga2RkcWZNcjlRTGM & \"", "\n", "f\"unzip Gutenberg.zip -d {fp.gutenberg} & \"", "\n", "f\"rm Gutenberg.zip\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-2. Download Tatoeba\"", ")", "\n", "maybe_download", "(", "fp", ".", "tatoeba", ",", "\n", "f\"wget http://downloads.tatoeba.org/exports/sentences.tar.bz2 & \"", "\n", "f\"tar -C {fp.tatoeba} -xjf sentences.tar.bz2 &\"", "\n", "f\"rm sentences.tar.bz2\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-3. Download Wiki-103\"", ")", "\n", "maybe_download", "(", "fp", ".", "wiki103", ",", "\n", "f\"wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip & \"", "\n", "f\"sleep 10s & \"", "\n", "f\"unzip wikitext-103-v1.zip -d {fp.wiki103} & \"", "\n", "f\"mv {fp.wiki103}/wikitext-103/wiki.train.tokens {fp.wiki103}/wiki.train.tokens &\"", "\n", "f\"rm wikitext-103-v1.zip\"", "\n", ")", "\n", "\n", "\n", "# TODO: make these directories at filepath.py", "\n", "# make directories", "\n", "for", "dir_name", "in", "[", "f\"{fp.bea19}\"", ",", "f\"{fp.wi}\"", ",", "f\"{fp.fce}\"", ",", "f\"{fp.conll2013}\"", ",", "f\"{fp.conll2014}\"", "]", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "mkdir", "(", "dir_name", ")", "\n", "", "except", ":", "pass", "\n", "\n", "", "logging", ".", "info", "(", "\"STEP 0-4. Download FCE\"", ")", "\n", "maybe_download", "(", "f\"{fp.bea19}/fce\"", ",", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/fce_v2.1.bea19.tar.gz & \"", "\n", "f\"tar -C {fp.bea19} -xvzf fce_v2.1.bea19.tar.gz $\"", "\n", "f\"rm fce_v2.1.bea19.tar.gz\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-5. Download WI+LOCNESS\"", ")", "\n", "maybe_download", "(", "f\"{fp.bea19}/wi+locness\"", ",", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz & \"", "\n", "f\"tar -C {fp.bea19} -xvzf wi+locness_v2.1.bea19.tar.gz &\"", "\n", "f\"rm wi+locness_v2.1.bea19.tar.gz & \"", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.test.orig & \"", "\n", "f\"mv ABCN.bea19.test.orig {fp.WI_TEST_ORI}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-6. Download LANG8\"", ")", "\n", "logging", ".", "info", "(", "f\"NO PUBLIC DATA AVAILABLE.\\n \"", "\n", "f\"Please visit 'https://www.cl.cam.ac.uk/research/nl/bea2019st/' to obtain data and extract file to {fp.nucle_m2}/*m2\"", ")", "\n", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-7. Download Conll 2013, 2014\"", ")", "\n", "maybe_download", "(", "fp", ".", "conll2013", ",", "\n", "f\"wget https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz & \"", "\n", "f\"tar -C {fp.conll2013} -xvzf &\"", "\n", "f\"rm release2.3.1.tar.gz\"", ")", "\n", "\n", "maybe_download", "(", "fp", ".", "conll2014", ",", "\n", "f\"wget https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz & \"", "\n", "f\"tar -C {fp.conll2014} -xvzf conll14st-test-data.tar.gz &\"", "\n", "f\"rm conll14st-test-data.tar.gz\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-8. Download language model\"", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filepath", ".", "Path", ".", "lm_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filepath", ".", "Path", ".", "lm_dict", ")", ",", "exist_ok", "=", "True", ")", "\n", "maybe_download", "(", "filepath", ".", "Path", ".", "lm_databin", ",", "\n", "f\"wget https://dl.fbaipublicfiles.com/fairseq/models/wiki103_fconv_lm.tar.bz2 & \"", "\n", "f\"tar -xvf wiki103_fconv_lm.tar.bz2 & \"", "\n", "f\"mv wiki103.pt {filepath.Path.lm_path} & \"", "\n", "f\"mv dict.txt {filepath.Path.lm_dict} & \"", "\n", "f\"rm wiki103_fconv_lm.tar.bz2\"", ")", "\n", "\n", "# logging.info(\"STEP 0-8. Download M2 Scorer\")", "\n", "# maybe_download(fp.m2scorer, \"wget https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\")", "\n", "# maybe_download(fp.errant, \"git clone https://github.com/chrisjbryant/errant.git\")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1. Word-tokenize the original files and merge them\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 1-1. gutenberg\"", ")", "\n", "fpaths", "=", "sorted", "(", "glob", "(", "f'{fp.gutenberg}/Gutenberg/txt/*.txt'", ")", ")", "\n", "maybe_do", "(", "fp", ".", "GUTENBERG_TXT", ",", "word_tokenize", ".", "gutenberg", ",", "\n", "(", "fpaths", ",", "fp", ".", "GUTENBERG_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1-2. tatoeba\"", ")", "\n", "fpath", "=", "f'{fp.tatoeba}/sentences.csv'", "\n", "maybe_do", "(", "fp", ".", "TATOEBA_TXT", ",", "word_tokenize", ".", "tatoeba", ",", "\n", "(", "fpath", ",", "fp", ".", "TATOEBA_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1-3. wiki103\"", ")", "\n", "fpath", "=", "f'{fp.wiki103}/wiki.train.tokens'", "\n", "maybe_do", "(", "fp", ".", "WIKI103_TXT", ",", "word_tokenize", ".", "wiki103", ",", "\n", "(", "fpath", ",", "fp", ".", "WIKI103_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 2. Train bpe model\"", ")", "\n", "maybe_do", "(", "fp", ".", "BPE_MODEL", ",", "bpe", ".", "train", ",", "\n", "(", "fp", ".", "GUTENBERG_TXT", ",", "fp", ".", "BPE_MODEL", ".", "replace", "(", "\".model\"", ",", "\"\"", ")", ",", "args", ".", "vocab_size", ",", "1.0", ",", "'bpe'", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 3. Split wi.dev into wi.dev.3k and wi.dev.1k\"", ")", "\n", "fpaths", "=", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/*.dev.gold.bea19.m2'", ")", ")", "\n", "wi_dev_3k_m2", "=", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.3k.m2'", "\n", "wi_dev_1k_m2", "=", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.1k.m2'", "\n", "maybe_do", "(", "wi_dev_3k_m2", ",", "m2", ".", "split_m2", ",", "\n", "(", "fpaths", ",", "wi_dev_3k_m2", ",", "wi_dev_1k_m2", ",", "0.75", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4. Perturb and make parallel files\"", ")", "\n", "for", "track_no", "in", "(", "\"1\"", ",", "\"3\"", ",", "\"0\"", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"Track {track_no}\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 4-1. writing perturbation scenario\"", ")", "\n", "if", "track_no", "==", "\"1\"", ":", "\n", "            ", "dir", "=", "f\"{fp.wi_m2}/*train*.m2\"", "\n", "", "elif", "track_no", "==", "\"3\"", ":", "\n", "            ", "dir", "=", "f\"{fp.wi_m2}/*dev.*3k*.m2\"", "\n", "", "else", ":", "\n", "            ", "dir", "=", "f\"{fp.nucle_m2}/*nucle*.m2\"", "\n", "", "word2ptbs", "=", "perturb", ".", "make_word2ptbs", "(", "sorted", "(", "glob", "(", "dir", ")", ")", ",", "args", ".", "min_cnt", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-2. gutenberg\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.GUTENBERG_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "GUTENBERG_TXT", ",", "\n", "eval", "(", "f\"fp.GUTENBERG_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.GUTENBERG_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "0", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-3. tatoeba\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.TATOEBA_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "TATOEBA_TXT", ",", "\n", "eval", "(", "f\"fp.TATOEBA_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.TATOEBA_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "1", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-4. wiki103\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.WIKI103_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WIKI103_TXT", ",", "\n", "eval", "(", "f\"fp.WIKI103_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.WIKI103_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "2", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"STEP 5. m2 to parallel\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 5-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.fce_m2}/*m2'", ")", ")", ",", "fp", ".", "FCE_ORI", ",", "fp", ".", "FCE_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.lang8_m2}/*m2'", ")", ")", ",", "fp", ".", "LANG8_ORI", ",", "fp", ".", "LANG8_COR", ",", "True", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.nucle_m2}/*m2'", ")", ")", ",", "fp", ".", "NUCLE_ORI", ",", "fp", ".", "NUCLE_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/*train*m2'", ")", ")", ",", "fp", ".", "WI_TRAIN_ORI", ",", "fp", ".", "WI_TRAIN_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-5. wi dev\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.m2'", ")", ")", ",", "fp", ".", "WI_DEV_ORI", ",", "fp", ".", "WI_DEV_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "# logging.info(\"STEP 5-6. wi test\")", "\n", "# if os.path.exists(WI_TEST_ORI): logging.info(f\"skip this step as {WI_TEST_ORI} already exists.\")", "\n", "# else: m2.m2_to_parallel(glob(f'{wi_m2}/*test*m2'), WI_TEST_ORI, WI_TEST_COR, False, True)", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-7. wi dev 3k. For track 3 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.3k.m2'", ")", ")", ",", "fp", ".", "WI_DEV_3K_ORI", ",", "fp", ".", "WI_DEV_3K_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-8. wi dev 1k. For track 3 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_1K_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.1k.m2'", ")", ")", ",", "fp", ".", "WI_DEV_1K_ORI", ",", "fp", ".", "WI_DEV_1K_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-9. conll2013. For track 0 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2013_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.conll2013_m2}/official-preprocessed.m2'", ")", ")", ",", "fp", ".", "CONLL2013_ORI", ",", "fp", ".", "CONLL2013_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-10. conll2014. For track 0 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2014_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.conll2014_m2}/official-2014.combined.m2'", ")", ")", ",", "fp", ".", "CONLL2014_ORI", ",", "fp", ".", "CONLL2014_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6. spell-check\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 6-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "FCE_ORI", ",", "fp", ".", "FCE_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "LANG8_ORI", ",", "fp", ".", "LANG8_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "NUCLE_ORI", ",", "fp", ".", "NUCLE_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_TRAIN_ORI", ",", "fp", ".", "WI_TRAIN_SP_ORI", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.binarize": [[236, 248], ["fairseq.data.indexed_dataset.IndexedDatasetBuilder", "fairseq.binarizer.Binarizer.binarize", "indexed_dataset.IndexedDatasetBuilder.finalize", "preprocess.dataset_dest_file", "indexed_dataset.IndexedDatasetBuilder.add_item", "preprocess.dataset_dest_file"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.binarize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_file"], ["maybe_do", "(", "fp", ".", "WI_DEV_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_ORI", ",", "fp", ".", "WI_DEV_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-6. wi test\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TEST_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_TEST_ORI", ",", "fp", ".", "WI_TEST_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-7. wi dev 3k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_3K_ORI", ",", "fp", ".", "WI_DEV_3K_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-8. wi dev 1k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_1K_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_1K_ORI", ",", "fp", ".", "WI_DEV_1K_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-9. conll 2013\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2013_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "CONLL2013_ORI", ",", "fp", ".", "CONLL2013_SP_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.dataset_dest_prefix": [[250, 256], ["None"], "function", ["None"], ["logging", ".", "info", "(", "\"STEP 6-10. conll 2014\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2014_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "CONLL2014_ORI", ",", "fp", ".", "CONLL2014_SP_ORI", ")", ")", "\n", "\n", "#", "\n", "logging", ".", "info", "(", "\"STEP 7. bpe-tokenize\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 7-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "FCE_SP_ORI", ",", "fp", ".", "FCE_TOK_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.dataset_dest_file": [[258, 261], ["preprocess.dataset_dest_prefix"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_prefix"], ["\n", "logging", ".", "info", "(", "\"STEP 7-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "LANG8_SP_ORI", ",", "fp", ".", "LANG8_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "LANG8_COR", ",", "fp", ".", "LANG8_TOK_COR", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.get_offsets": [[263, 265], ["fairseq.binarizer.Binarizer.find_offsets"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.Binarizer.find_offsets"], ["logging", ".", "info", "(", "\"STEP 7-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "NUCLE_SP_ORI", ",", "fp", ".", "NUCLE_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "NUCLE_COR", ",", "fp", ".", "NUCLE_TOK_COR", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.merge_files": [[267, 274], ["fairseq.data.indexed_dataset.IndexedDatasetBuilder", "indexed_dataset.IndexedDatasetBuilder.finalize", "indexed_dataset.IndexedDatasetBuilder.merge_file_", "os.remove", "os.remove", "fairseq.data.indexed_dataset.data_file_path", "fairseq.data.indexed_dataset.index_file_path"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.merge_file_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.index_file_path"], ["logging", ".", "info", "(", "\"STEP 7-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TRAIN_SP_ORI", ",", "fp", ".", "WI_TRAIN_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TRAIN_COR", ",", "fp", ".", "WI_TRAIN_TOK_COR", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 7-5. wi dev\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_SP_ORI", ",", "fp", ".", "WI_DEV_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_COR", ",", "fp", ".", "WI_DEV_TOK_COR", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.preprocess.cli_main": [[276, 280], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["maybe_do", "(", "fp", ".", "WI_TEST_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TEST_SP_ORI", ",", "fp", ".", "WI_TEST_TOK_ORI", ")", ")", "\n", "# maybe_do(fp.WI_TEST_TOK_COR, bpe.bpe_tokenize, (fp.BPE_MODEL, fp.WI_TEST_COR, fp.WI_TEST_TOK_COR))", "\n", "\n", "logging", ".", "info", "(", "\"STEP 7-7. wi dev 3k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_3K_SP_ORI", ",", "fp", ".", "WI_DEV_3K_TOK_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.score.get_parser": [[20, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "parser", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.score.main": [[35, 75], ["score.get_parser", "get_parser.parse_args", "print", "os.path.exists", "fairseq.data.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "line", "\n", "\n", "", "", "", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "import", "sacrebleu", "\n", "\n", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "print", "(", "sacrebleu", ".", "corpus_bleu", "(", "fdsys", ",", "[", "fdref", "]", ")", ")", "\n", "", "", "", "else", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                    ", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm.WordStat.__init__": [[23, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "word", ",", "is_bpe", ")", ":", "\n", "        ", "self", ".", "word", "=", "word", "\n", "self", ".", "is_bpe", "=", "is_bpe", "\n", "self", ".", "log_prob", "=", "0", "\n", "self", ".", "next_word_prob", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "missing_next_words", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm.WordStat.add": [[31, 42], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "log_prob", ",", "next_word_prob", ")", ":", "\n", "        ", "\"\"\" increments counters for the sum of log probs of current word and next\n            word (given context ending at current word). Since the next word might be at the end of the example,\n            or it might be not counted because it is not an ending subword unit,\n            also keeps track of how many of those we have seen \"\"\"", "\n", "if", "next_word_prob", "is", "not", "None", ":", "\n", "            ", "self", ".", "next_word_prob", "+=", "next_word_prob", "\n", "", "else", ":", "\n", "            ", "self", ".", "missing_next_words", "+=", "1", "\n", "", "self", ".", "log_prob", "+=", "log_prob", "\n", "self", ".", "count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm.WordStat.__str__": [[43, 46], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "'{}\\t{}\\t{}\\t{}\\t{}\\t{}'", ".", "format", "(", "self", ".", "word", ",", "self", ".", "count", ",", "self", ".", "log_prob", ",", "self", ".", "is_bpe", ",", "\n", "self", ".", "next_word_prob", ",", "self", ".", "count", "-", "self", ".", "missing_next_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm.main": [[48, 186], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "vars().keys", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "fairseq.sequence_scorer.SequenceScorer", "dict", "print", "print", "torch.cuda.is_available", "parsed_args.path.split", "model.make_generation_fast_", "len", "len", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "sorted", "eval", "vars", "setattr", "len", "model.half", "model.cuda", "sum", "tasks.setup_task.get_batch_iterator", "args.remove_bpe.rstrip", "set", "fairseq.meters.StopwatchMeter.start", "fairseq.sequence_scorer.SequenceScorer.generate", "fairseq.meters.StopwatchMeter.stop", "fairseq.meters.TimeMeter.update", "t.log", "numpy.exp", "dict.values", "print", "getattr", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "inf_scores.any", "pos_scores.sum().cpu", "p.numel", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "range", "pos_scores.eq", "pos_scores.eq", "print", "pos_scores.numel", "range", "round", "models[].parameters", "range", "tasks.setup_task.dictionary[].endswith", "float", "float", "tasks.setup_task.target_dictionary.string", "pos_scores.sum", "len", "[].item", "print", "len", "len", "[].item", "word_prob.append", "dict.setdefault().add", "model.max_positions", "len", "pos_scores[].item", "inf_scores.nonzero", "pos_scores[].item", "pos_scores[].item", "dict.setdefault", "eval_lm.WordStat"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault"], ["", "", "def", "main", "(", "parsed_args", ")", ":", "\n", "    ", "assert", "parsed_args", ".", "path", "is", "not", "None", ",", "'--path required for evaluation!'", "\n", "\n", "import_user_module", "(", "parsed_args", ")", "\n", "\n", "print", "(", "parsed_args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "parsed_args", ".", "cpu", "\n", "\n", "task", "=", "tasks", ".", "setup_task", "(", "parsed_args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "parsed_args", ".", "path", ")", ")", "\n", "models", ",", "args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "parsed_args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "parsed_args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "for", "arg", "in", "vars", "(", "parsed_args", ")", ".", "keys", "(", ")", ":", "\n", "        ", "if", "arg", "not", "in", "{", "'self_target'", ",", "'future_target'", ",", "'past_target'", ",", "'tokens_per_sample'", ",", "'output_size_dictionary'", "}", ":", "\n", "            ", "setattr", "(", "args", ",", "arg", ",", "getattr", "(", "parsed_args", ",", "arg", ")", ")", "\n", "", "", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load dataset splits", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation and set the source and dest dicts on the model (required by scorer)", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "", "assert", "len", "(", "models", ")", ">", "0", "\n", "\n", "print", "(", "'num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "models", "[", "0", "]", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", "or", "36000", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "*", "[", "\n", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "\n", "]", ")", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ")", "\n", "\n", "score_sum", "=", "0.", "\n", "count", "=", "0", "\n", "\n", "if", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "remove_bpe", "==", "'sentencepiece'", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "bpe_cont", "=", "args", ".", "remove_bpe", ".", "rstrip", "(", ")", "\n", "bpe_toks", "=", "set", "(", "i", "for", "i", "in", "range", "(", "len", "(", "task", ".", "dictionary", ")", ")", "if", "task", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont", ")", ")", "\n", "", "bpe_len", "=", "len", "(", "bpe_cont", ")", "\n", "", "else", ":", "\n", "        ", "bpe_toks", "=", "None", "\n", "bpe_len", "=", "0", "\n", "\n", "", "word_stats", "=", "dict", "(", ")", "\n", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "scorer", ".", "generate", "(", "models", ",", "sample", ")", "\n", "gen_timer", ".", "stop", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "\n", "for", "hypos_i", "in", "hypos", ":", "\n", "                ", "hypo", "=", "hypos_i", "[", "0", "]", "\n", "pos_scores", "=", "hypo", "[", "'positional_scores'", "]", "\n", "\n", "skipped_toks", "=", "0", "\n", "if", "bpe_toks", "is", "not", "None", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", "-", "1", ")", ":", "\n", "                        ", "if", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "in", "bpe_toks", ":", "\n", "                            ", "skipped_toks", "+=", "1", "\n", "pos_scores", "[", "i", "+", "1", "]", "+=", "pos_scores", "[", "i", "]", "\n", "pos_scores", "[", "i", "]", "=", "0", "\n", "\n", "", "", "", "inf_scores", "=", "pos_scores", ".", "eq", "(", "float", "(", "'inf'", ")", ")", "|", "pos_scores", ".", "eq", "(", "float", "(", "'-inf'", ")", ")", "\n", "if", "inf_scores", ".", "any", "(", ")", ":", "\n", "                    ", "print", "(", "'| Skipping tokens with inf scores:'", ",", "\n", "task", ".", "target_dictionary", ".", "string", "(", "hypo", "[", "'tokens'", "]", "[", "inf_scores", ".", "nonzero", "(", ")", "]", ")", ")", "\n", "pos_scores", "=", "pos_scores", "[", "(", "~", "inf_scores", ")", ".", "nonzero", "(", ")", "]", "\n", "", "score_sum", "+=", "pos_scores", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "count", "+=", "pos_scores", ".", "numel", "(", ")", "-", "skipped_toks", "\n", "\n", "if", "args", ".", "output_word_probs", "or", "args", ".", "output_word_stats", ":", "\n", "                    ", "w", "=", "''", "\n", "word_prob", "=", "[", "]", "\n", "is_bpe", "=", "False", "\n", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", ")", ":", "\n", "                        ", "w_ind", "=", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "w", "+=", "task", ".", "dictionary", "[", "w_ind", "]", "\n", "if", "bpe_toks", "is", "not", "None", "and", "w_ind", "in", "bpe_toks", ":", "\n", "                            ", "w", "=", "w", "[", ":", "-", "bpe_len", "]", "\n", "is_bpe", "=", "True", "\n", "", "else", ":", "\n", "                            ", "word_prob", ".", "append", "(", "(", "w", ",", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "next_prob", "=", "None", "\n", "ind", "=", "i", "+", "1", "\n", "while", "ind", "<", "len", "(", "hypo", "[", "'tokens'", "]", ")", ":", "\n", "                                ", "if", "pos_scores", "[", "ind", "]", ".", "item", "(", ")", "!=", "0", ":", "\n", "                                    ", "next_prob", "=", "pos_scores", "[", "ind", "]", "\n", "break", "\n", "", "ind", "+=", "1", "\n", "\n", "", "word_stats", ".", "setdefault", "(", "w", ",", "WordStat", "(", "w", ",", "is_bpe", ")", ")", ".", "add", "(", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ",", "next_prob", ")", "\n", "is_bpe", "=", "False", "\n", "w", "=", "''", "\n", "", "", "if", "args", ".", "output_word_probs", ":", "\n", "                        ", "print", "(", "'\\t'", ".", "join", "(", "'{} [{:2f}]'", ".", "format", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "for", "x", "in", "word_prob", ")", ")", "\n", "\n", "", "", "", "wps_meter", ".", "update", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "\n", "", "", "avg_nll_loss", "=", "-", "score_sum", "/", "count", "\n", "print", "(", "'| Evaluated {} tokens in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "print", "(", "'| Loss: {:.4f}, Perplexity: {:.2f}'", ".", "format", "(", "avg_nll_loss", ",", "np", ".", "exp", "(", "avg_nll_loss", ")", ")", ")", "\n", "\n", "if", "args", ".", "output_word_stats", ":", "\n", "        ", "for", "ws", "in", "sorted", "(", "word_stats", ".", "values", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", ".", "count", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "ws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.eval_lm.cli_main": [[188, 192], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.LMScorer.__init__": [[46, 76], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "vars().keys", "fairseq.tasks.setup_task", "fairseq.meters.StopwatchMeter", "fairseq.sequence_scorer.SequenceScorer", "torch.cuda.is_available", "parsed_args.path.split", "model.make_generation_fast_", "eval", "model.cuda", "vars", "setattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["    ", "def", "__init__", "(", "self", ",", "parsed_args", ")", ":", "\n", "        ", "self", ".", "args", "=", "parsed_args", "\n", "import_user_module", "(", "parsed_args", ")", "\n", "assert", "parsed_args", ".", "path", "is", "not", "None", ",", "'--path required for evaluation'", "\n", "\n", "print", "(", "parsed_args", ")", "\n", "\n", "self", ".", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "parsed_args", ".", "cpu", "\n", "\n", "self", ".", "task", "=", "tasks", ".", "setup_task", "(", "parsed_args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "parsed_args", ".", "path", ")", ")", "\n", "self", ".", "models", ",", "args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "parsed_args", ".", "path", ".", "split", "(", "':'", ")", ",", "self", ".", "task", ",", "model_arg_overrides", "=", "eval", "(", "parsed_args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "make_generation_fast_", "(", ")", "\n", "if", "self", ".", "use_cuda", ":", "\n", "                ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "", "for", "arg", "in", "vars", "(", "parsed_args", ")", ".", "keys", "(", ")", ":", "\n", "            ", "if", "arg", "not", "in", "{", "'self_target'", ",", "'future_target'", ",", "'past_target'", ",", "'tokens_per_sample'", ",", "\n", "'output_size_dictionary'", "}", ":", "\n", "                ", "setattr", "(", "args", ",", "arg", ",", "getattr", "(", "parsed_args", ",", "arg", ")", ")", "\n", "", "", "self", ".", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "self", ".", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "self", ".", "scorer", "=", "SequenceScorer", "(", "self", ".", "task", ".", "target_dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.LMScorer.score_sent": [[78, 81], ["lm_scorer.LMScorer.score"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score"], ["", "def", "score_sent", "(", "self", ",", "line", ")", ":", "\n", "        ", "score_dict", "=", "self", ".", "score", "(", "[", "line", "]", ")", "\n", "return", "score_dict", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.LMScorer.make_batches": [[82, 106], ["torch.LongTensor", "fairseq.data.TokenBlockDataset", "lm_scorer.LMScorer.task.get_batch_iterator().next_epoch_itr", "lm_scorer.LMScorer.task.source_dictionary.encode_line().long", "tokens.numel", "lm_scorer.LMScorer.task.dictionary.pad", "lm_scorer.LMScorer.task.dictionary.eos", "lm_scorer.LMScorer.task.get_batch_iterator", "lm_scorer.LMScorer.task.source_dictionary.encode_line", "fairseq.data.MonolingualDataset", "fairseq.utils.resolve_max_positions", "model.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "make_batches", "(", "self", ",", "lines", ")", ":", "\n", "        ", "token_lst", "=", "[", "self", ".", "task", ".", "source_dictionary", ".", "encode_line", "(", "line", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "for", "line", "in", "lines", "]", "\n", "length_lst", "=", "torch", ".", "LongTensor", "(", "[", "tokens", ".", "numel", "(", ")", "for", "tokens", "in", "token_lst", "]", ")", "\n", "\n", "ds", "=", "data", ".", "TokenBlockDataset", "(", "token_lst", ",", "length_lst", ",", "self", ".", "args", ".", "tokens_per_sample", ",", "pad", "=", "self", ".", "task", ".", "dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "task", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "'eos'", ",", "include_targets", "=", "True", ")", "\n", "add_eos_for_other_targets", "=", "self", ".", "args", ".", "sample_break_mode", "is", "not", "None", "and", "self", ".", "args", ".", "sample_break_mode", "!=", "'none'", "\n", "itr", "=", "self", ".", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "data", ".", "MonolingualDataset", "(", "ds", ",", "ds", ".", "sizes", ",", "self", ".", "task", ".", "dictionary", ",", "self", ".", "task", ".", "target_dictionary", ",", "\n", "add_eos_for_other_targets", ",", "shuffle", "=", "False", ",", "targets", "=", "self", ".", "task", ".", "targets", ")", ",", "\n", "max_tokens", "=", "self", ".", "args", ".", "max_tokens", "or", "3000", ",", "\n", "max_sentences", "=", "self", ".", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "*", "[", "\n", "model", ".", "max_positions", "(", ")", "for", "model", "in", "self", ".", "models", "\n", "]", ")", ",", "\n", "num_shards", "=", "self", ".", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "self", ".", "args", ".", "shard_id", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "num_workers", "=", "self", ".", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "return", "itr", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.LMScorer.score": [[109, 162], ["lm_scorer.LMScorer.make_batches", "lm_scorer.LMScorer.scorer.generate", "zip", "fairseq.utils.move_to_cuda", "inf_scores.any", "pos_scores.sum().cpu", "pos_scores.numel", "range", "pos_scores.sum().cpu.item", "pos_scores.eq", "pos_scores.eq", "print", "len", "[].item", "word_prob.append", "w_lst.append", "float", "float", "lm_scorer.LMScorer.task.target_dictionary.string", "pos_scores.sum", "print", "print", "sample_id.item", "pos_scores[].item", "inf_scores.nonzero"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.make_batches", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "score", "(", "self", ",", "lines", ")", ":", "\n", "\n", "\n", "        ", "batch", "=", "self", ".", "make_batches", "(", "lines", ")", "\n", "\n", "sample_score_dict", "=", "{", "}", "\n", "\n", "\n", "\n", "# with progress_bar.build_progress_bar(self.args, itr) as t:", "\n", "for", "sample", "in", "batch", ":", "\n", "            ", "sample_id_lst", "=", "sample", "[", "'id'", "]", "\n", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "self", ".", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "hypos", "=", "self", ".", "scorer", ".", "generate", "(", "self", ".", "models", ",", "sample", ")", "\n", "\n", "# print(hypos)", "\n", "\n", "\n", "for", "sample_id", ",", "hypos_i", "in", "zip", "(", "sample_id_lst", ",", "hypos", ")", ":", "\n", "                ", "hypo", "=", "hypos_i", "[", "0", "]", "\n", "pos_scores", "=", "hypo", "[", "'positional_scores'", "]", "\n", "\n", "inf_scores", "=", "pos_scores", ".", "eq", "(", "float", "(", "'inf'", ")", ")", "|", "pos_scores", ".", "eq", "(", "float", "(", "'-inf'", ")", ")", "\n", "if", "inf_scores", ".", "any", "(", ")", ":", "\n", "                    ", "print", "(", "'| Skipping tokens with inf scores:'", ",", "\n", "self", ".", "task", ".", "target_dictionary", ".", "string", "(", "hypo", "[", "'tokens'", "]", "[", "inf_scores", ".", "nonzero", "(", ")", "]", ")", ")", "\n", "pos_scores", "=", "pos_scores", "[", "(", "~", "inf_scores", ")", ".", "nonzero", "(", ")", "]", "\n", "", "sample_score", "=", "pos_scores", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "count", "=", "pos_scores", ".", "numel", "(", ")", "\n", "\n", "w_lst", "=", "[", "]", "\n", "word_prob", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", ")", ":", "\n", "                    ", "w_ind", "=", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "w", "=", "self", ".", "task", ".", "dictionary", "[", "w_ind", "]", "\n", "word_prob", ".", "append", "(", "(", "w", ",", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "w_lst", ".", "append", "(", "w", ")", "\n", "\n", "", "sample_score", "=", "-", "sample_score", "/", "count", "\n", "\n", "if", "not", "self", ".", "args", ".", "quiet", ":", "\n", "                    ", "if", "self", ".", "args", ".", "output_sent", ":", "\n", "                        ", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "sample_id", ",", "sample_score", ",", "' '", ".", "join", "(", "w_lst", ")", ")", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'H-{}\\t{}'", ".", "format", "(", "sample_id", ",", "sample_score", ")", ")", "\n", "", "", "sample_score_dict", "[", "sample_id", ".", "item", "(", ")", "]", "=", "sample_score", ".", "item", "(", ")", "\n", "# print(sample_id, sample_score.item())", "\n", "\n", "\n", "", "", "return", "sample_score_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.main": [[164, 170], ["lm_scorer.LMScorer", "print", "open", "lm_scorer.LMScorer.score", "open.close", "open.read().splitlines", "open.read"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score"], ["", "", "def", "main", "(", "parsed_args", ")", ":", "\n", "    ", "lmscorer", "=", "LMScorer", "(", "parsed_args", ")", "\n", "print", "(", "parsed_args", ".", "fpath", ")", "\n", "fd", "=", "open", "(", "parsed_args", ".", "fpath", ",", "'r'", ")", "\n", "lmscorer", ".", "score", "(", "fd", ".", "read", "(", ")", ".", "splitlines", "(", ")", ")", "\n", "fd", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.get_lm_scorer_parser": [[172, 178], ["fairseq.options.get_parser", "fairseq.options.add_dataset_args", "fairseq.options.add_common_eval_args", "lm_scorer.add_lm_scorer_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_common_eval_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.add_lm_scorer_args"], ["", "def", "get_lm_scorer_parser", "(", "default_task", "=", "'language_modeling'", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_parser", "(", "'Evaluate Language Model'", ",", "default_task", ")", "\n", "options", ".", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n", "options", ".", "add_common_eval_args", "(", "parser", ")", "\n", "add_lm_scorer_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.add_lm_scorer_args": [[180, 184], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_lm_scorer_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'LM Scorer'", ")", "\n", "group", ".", "add_argument", "(", "'--fpath'", ",", "help", "=", "'file path'", ")", "\n", "group", ".", "add_argument", "(", "'--output-sent'", ",", "action", "=", "'store_true'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.cli_main": [[186, 192], ["lm_scorer.get_lm_scorer_parser", "fairseq.options.parse_args_and_arch", "lm_scorer.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.lm_scorer.get_lm_scorer_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "get_lm_scorer_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "\n", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_preprocessing_parser": [[20, 24], ["options.get_parser", "options.add_preprocess_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_preprocess_args"], ["def", "get_preprocessing_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Preprocessing'", ",", "default_task", ")", "\n", "add_preprocess_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_training_parser": [[26, 34], ["options.get_parser", "options.add_dataset_args", "options.add_distributed_training_args", "options.add_model_args", "options.add_optimization_args", "options.add_checkpoint_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_distributed_training_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_model_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_optimization_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_checkpoint_args"], ["", "def", "get_training_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Trainer'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "train", "=", "True", ")", "\n", "add_distributed_training_args", "(", "parser", ")", "\n", "add_model_args", "(", "parser", ")", "\n", "add_optimization_args", "(", "parser", ")", "\n", "add_checkpoint_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser": [[36, 43], ["options.get_parser", "options.add_dataset_args", "options.add_generation_args", "options.add_interactive_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_generation_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_interactive_args"], ["", "def", "get_generation_parser", "(", "interactive", "=", "False", ",", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Generation'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n", "add_generation_args", "(", "parser", ")", "\n", "if", "interactive", ":", "\n", "        ", "add_interactive_args", "(", "parser", ")", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_interactive_generation_parser": [[45, 47], ["options.get_generation_parser"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser"], ["", "def", "get_interactive_generation_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "return", "get_generation_parser", "(", "interactive", "=", "True", ",", "default_task", "=", "default_task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_eval_lm_parser": [[49, 54], ["options.get_parser", "options.add_dataset_args", "options.add_eval_lm_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_eval_lm_args"], ["", "def", "get_eval_lm_parser", "(", "default_task", "=", "'language_modeling'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Evaluate Language Model'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n", "add_eval_lm_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list": [[56, 65], ["isinstance", "eval", "list", "map", "type"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "def", "eval_str_list", "(", "x", ",", "type", "=", "float", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "x", ",", "str", ")", ":", "\n", "        ", "x", "=", "eval", "(", "x", ")", "\n", "", "try", ":", "\n", "        ", "return", "list", "(", "map", "(", "type", ",", "x", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "return", "[", "type", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool": [[67, 74], ["bool", "eval"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "", "def", "eval_bool", "(", "x", ",", "default", "=", "False", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "\n", "        ", "return", "default", "\n", "", "try", ":", "\n", "        ", "return", "bool", "(", "eval", "(", "x", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch": [[76, 124], ["parser.parse_known_args", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "getattr", "hasattr", "parser.add_argument_group", "ARCH_MODEL_REGISTRY[].add_args", "CRITERION_REGISTRY[].add_args", "OPTIMIZER_REGISTRY[].add_args", "LR_SCHEDULER_REGISTRY[].add_args", "TASK_REGISTRY[].add_args", "parser.parse_known_args", "parser.parse_args", "hasattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args"], ["", "", "def", "parse_args_and_arch", "(", "parser", ",", "input_args", "=", "None", ",", "parse_known", "=", "False", ")", ":", "\n", "# The parser doesn't know about model/criterion/optimizer-specific args, so", "\n", "# we parse twice. First we parse the model/criterion/optimizer, then we", "\n", "# parse a second time after adding the *-specific arguments.", "\n", "# If input_args is given, we will parse those args instead of sys.argv.", "\n", "    ", "args", ",", "_", "=", "parser", ".", "parse_known_args", "(", "input_args", ")", "\n", "\n", "# Add model-specific args to parser.", "\n", "if", "hasattr", "(", "args", ",", "'arch'", ")", ":", "\n", "        ", "model_specific_group", "=", "parser", ".", "add_argument_group", "(", "\n", "'Model-specific configuration'", ",", "\n", "# Only include attributes which are explicitly given as command-line", "\n", "# arguments or which have default values.", "\n", "argument_default", "=", "argparse", ".", "SUPPRESS", ",", "\n", ")", "\n", "ARCH_MODEL_REGISTRY", "[", "args", ".", "arch", "]", ".", "add_args", "(", "model_specific_group", ")", "\n", "\n", "# Add *-specific args to parser.", "\n", "", "if", "hasattr", "(", "args", ",", "'criterion'", ")", ":", "\n", "        ", "CRITERION_REGISTRY", "[", "args", ".", "criterion", "]", ".", "add_args", "(", "parser", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'optimizer'", ")", ":", "\n", "        ", "OPTIMIZER_REGISTRY", "[", "args", ".", "optimizer", "]", ".", "add_args", "(", "parser", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'lr_scheduler'", ")", ":", "\n", "        ", "LR_SCHEDULER_REGISTRY", "[", "args", ".", "lr_scheduler", "]", ".", "add_args", "(", "parser", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'task'", ")", ":", "\n", "        ", "TASK_REGISTRY", "[", "args", ".", "task", "]", ".", "add_args", "(", "parser", ")", "\n", "\n", "# Parse a second time.", "\n", "", "if", "parse_known", ":", "\n", "        ", "args", ",", "extra", "=", "parser", ".", "parse_known_args", "(", "input_args", ")", "\n", "", "else", ":", "\n", "        ", "args", "=", "parser", ".", "parse_args", "(", "input_args", ")", "\n", "extra", "=", "None", "\n", "\n", "# Post-process args.", "\n", "", "if", "hasattr", "(", "args", ",", "'max_sentences_valid'", ")", "and", "args", ".", "max_sentences_valid", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences_valid", "=", "args", ".", "max_sentences", "\n", "", "if", "getattr", "(", "args", ",", "'memory_efficient_fp16'", ",", "False", ")", ":", "\n", "        ", "args", ".", "fp16", "=", "True", "\n", "\n", "# Apply architecture configuration.", "\n", "", "if", "hasattr", "(", "args", ",", "'arch'", ")", ":", "\n", "        ", "ARCH_CONFIG_REGISTRY", "[", "args", ".", "arch", "]", "(", "args", ")", "\n", "\n", "", "if", "parse_known", ":", "\n", "        ", "return", "args", ",", "extra", "\n", "", "else", ":", "\n", "        ", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_parser": [[126, 169], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "fairseq.utils.import_user_module", "argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "fairseq.tasks.TASK_REGISTRY.keys"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module"], ["", "", "def", "get_parser", "(", "desc", ",", "default_task", "=", "'translation'", ")", ":", "\n", "# Before creating the true parser, we need to import optional user module", "\n", "# in order to eagerly import custom tasks, optimizers, architectures, etc.", "\n", "    ", "usr_parser", "=", "argparse", ".", "ArgumentParser", "(", "add_help", "=", "False", ")", "\n", "usr_parser", ".", "add_argument", "(", "'--user-dir'", ",", "default", "=", "None", ")", "\n", "usr_args", ",", "_", "=", "usr_parser", ".", "parse_known_args", "(", ")", "\n", "import_user_module", "(", "usr_args", ")", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--no-progress-bar'", ",", "action", "=", "'store_true'", ",", "help", "=", "'disable progress bar'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'log progress every N batches (when progress bar is disabled)'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-format'", ",", "default", "=", "None", ",", "help", "=", "'log format to use'", ",", "\n", "choices", "=", "[", "'json'", ",", "'none'", ",", "'simple'", ",", "'tqdm'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--tensorboard-logdir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "''", ",", "\n", "help", "=", "'path to save logs for tensorboard, should match --logdir '", "\n", "'of running tensorboard (default: no tensorboard logging)'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'pseudo random number generator seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--cpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use CPU instead of CUDA'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use FP16'", ")", "\n", "parser", ".", "add_argument", "(", "'--memory-efficient-fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use a memory-efficient version of FP16 training; implies --fp16'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-init-scale'", ",", "default", "=", "2", "**", "7", ",", "type", "=", "int", ",", "\n", "help", "=", "'default FP16 loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-scale-window'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of updates before increasing loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-scale-tolerance'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "\n", "help", "=", "'pct of updates that can overflow before decreasing the loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-loss-scale'", ",", "default", "=", "1e-4", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'minimum FP16 loss scale, after which training is stopped'", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold-loss-scale'", ",", "type", "=", "float", ",", "\n", "help", "=", "'threshold FP16 loss scale from below'", ")", "\n", "parser", ".", "add_argument", "(", "'--user-dir'", ",", "default", "=", "None", ",", "\n", "help", "=", "'path to a python module containing custom extensions (tasks and/or architectures)'", ")", "\n", "\n", "# Task definitions can be found under fairseq/tasks/", "\n", "parser", ".", "add_argument", "(", "'--task'", ",", "metavar", "=", "'TASK'", ",", "default", "=", "default_task", ",", "\n", "choices", "=", "TASK_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'task'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_preprocess_args": [[171, 213], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_preprocess_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Preprocessing'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "\"-s\"", ",", "\"--source-lang\"", ",", "default", "=", "None", ",", "metavar", "=", "\"SRC\"", ",", "\n", "help", "=", "\"source language\"", ")", "\n", "group", ".", "add_argument", "(", "\"-t\"", ",", "\"--target-lang\"", ",", "default", "=", "None", ",", "metavar", "=", "\"TARGET\"", ",", "\n", "help", "=", "\"target language\"", ")", "\n", "group", ".", "add_argument", "(", "\"--trainpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"train file prefix\"", ")", "\n", "group", ".", "add_argument", "(", "\"--validpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"comma separated, valid file prefixes\"", ")", "\n", "group", ".", "add_argument", "(", "\"--testpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"comma separated, test file prefixes\"", ")", "\n", "group", ".", "add_argument", "(", "\"--destdir\"", ",", "metavar", "=", "\"DIR\"", ",", "default", "=", "\"data-bin\"", ",", "\n", "help", "=", "\"destination dir\"", ")", "\n", "group", ".", "add_argument", "(", "\"--thresholdtgt\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"map words appearing less than threshold times to unknown\"", ")", "\n", "group", ".", "add_argument", "(", "\"--thresholdsrc\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"map words appearing less than threshold times to unknown\"", ")", "\n", "group", ".", "add_argument", "(", "\"--tgtdict\"", ",", "metavar", "=", "\"FP\"", ",", "\n", "help", "=", "\"reuse given target dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--srcdict\"", ",", "metavar", "=", "\"FP\"", ",", "\n", "help", "=", "\"reuse given source dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--nwordstgt\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of target words to retain\"", ")", "\n", "group", ".", "add_argument", "(", "\"--nwordssrc\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of source words to retain\"", ")", "\n", "group", ".", "add_argument", "(", "\"--alignfile\"", ",", "metavar", "=", "\"ALIGN\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"an alignment file (optional)\"", ")", "\n", "group", ".", "add_argument", "(", "\"--output-format\"", ",", "metavar", "=", "\"FORMAT\"", ",", "default", "=", "\"binary\"", ",", "\n", "choices", "=", "[", "\"binary\"", ",", "\"raw\"", "]", ",", "\n", "help", "=", "\"output format (optional)\"", ")", "\n", "group", ".", "add_argument", "(", "\"--joined-dictionary\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Generate joined dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--only-source\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Only process the source language\"", ")", "\n", "group", ".", "add_argument", "(", "\"--padding-factor\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Pad dictionary size to be multiple of N\"", ")", "\n", "group", ".", "add_argument", "(", "\"--workers\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of parallel workers\"", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_dataset_args": [[215, 247], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_dataset_args", "(", "parser", ",", "train", "=", "False", ",", "gen", "=", "False", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Dataset and data loading'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--num-workers'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'how many subprocesses to use for data loading'", ")", "\n", "group", ".", "add_argument", "(", "'--skip-invalid-size-inputs-valid-test'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'ignore too long or too short lines in valid and test set'", ")", "\n", "group", ".", "add_argument", "(", "'--max-tokens'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of tokens in a batch'", ")", "\n", "group", ".", "add_argument", "(", "'--max-sentences'", ",", "'--batch-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of sentences in a batch'", ")", "\n", "if", "train", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--train-subset'", ",", "default", "=", "'train'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "choices", "=", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ",", "\n", "help", "=", "'data subset to use for training (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--valid-subset'", ",", "default", "=", "'valid'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'comma separated list of data subsets to use for validation'", "\n", "' (train, valid, valid1, test, test1)'", ")", "\n", "group", ".", "add_argument", "(", "'--max-sentences-valid'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of sentences in a validation batch'", "\n", "' (defaults to --max-sentences)'", ")", "\n", "group", ".", "add_argument", "(", "'--curriculum'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'don\\'t shuffle batches for first N epochs'", ")", "\n", "", "if", "gen", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--gen-subset'", ",", "default", "=", "'test'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'data subset to generate (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--num-shards'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'shard generation over N shards'", ")", "\n", "group", ".", "add_argument", "(", "'--shard-id'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'ID'", ",", "\n", "help", "=", "'id of the shard to generate (id < num_shards)'", ")", "\n", "# fmt: on", "\n", "", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_distributed_training_args": [[249, 277], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "max", "torch.cuda.device_count"], "function", ["None"], ["", "def", "add_distributed_training_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Distributed training'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--distributed-world-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "max", "(", "1", ",", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ",", "\n", "help", "=", "'total number of GPUs across all nodes (default: all visible GPUs)'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'rank of the current worker'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-backend'", ",", "default", "=", "'nccl'", ",", "type", "=", "str", ",", "\n", "help", "=", "'distributed backend'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-init-method'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "'typically tcp://hostname:port that will be used to '", "\n", "'establish initial connetion'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-port'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'port number (not required if using --distributed-init-method)'", ")", "\n", "group", ".", "add_argument", "(", "'--device-id'", ",", "'--local_rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'which GPU to use (usually configured automatically)'", ")", "\n", "group", ".", "add_argument", "(", "'--ddp-backend'", ",", "default", "=", "'c10d'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'c10d'", ",", "'no_c10d'", "]", ",", "\n", "help", "=", "'DistributedDataParallel backend'", ")", "\n", "group", ".", "add_argument", "(", "'--bucket-cap-mb'", ",", "default", "=", "25", ",", "type", "=", "int", ",", "metavar", "=", "'MB'", ",", "\n", "help", "=", "'bucket size for reduction'", ")", "\n", "group", ".", "add_argument", "(", "'--fix-batches-to-gpus'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t shuffle batches between GPUs; this reduces overall '", "\n", "'randomness and may affect precision but avoids the cost of '", "\n", "'re-reading the data'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_optimization_args": [[279, 318], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "fairseq.optim.OPTIMIZER_REGISTRY.keys", "fairseq.optim.lr_scheduler.LR_SCHEDULER_REGISTRY.keys", "options.eval_str_list"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list"], ["", "def", "add_optimization_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Optimization'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--max-epoch'", ",", "'--me'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force stop training at specified epoch'", ")", "\n", "group", ".", "add_argument", "(", "'--max-update'", ",", "'--mu'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force stop training at specified update'", ")", "\n", "group", ".", "add_argument", "(", "'--clip-norm'", ",", "default", "=", "25", ",", "type", "=", "float", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'clip threshold of gradients'", ")", "\n", "group", ".", "add_argument", "(", "'--sentence-avg'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'normalize gradients by the number of sentences in a batch'", "\n", "' (default is to normalize by number of tokens)'", ")", "\n", "group", ".", "add_argument", "(", "'--update-freq'", ",", "default", "=", "'1'", ",", "metavar", "=", "'N1,N2,...,N_K'", ",", "\n", "type", "=", "lambda", "uf", ":", "eval_str_list", "(", "uf", ",", "type", "=", "int", ")", ",", "\n", "help", "=", "'update parameters every N_i batches, when in epoch i'", ")", "\n", "\n", "# Optimizer definitions can be found under fairseq/optim/", "\n", "group", ".", "add_argument", "(", "'--optimizer'", ",", "default", "=", "'nag'", ",", "metavar", "=", "'OPT'", ",", "\n", "choices", "=", "OPTIMIZER_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "group", ".", "add_argument", "(", "'--lr'", ",", "'--learning-rate'", ",", "default", "=", "'0.25'", ",", "type", "=", "eval_str_list", ",", "\n", "metavar", "=", "'LR_1,LR_2,...,LR_N'", ",", "\n", "help", "=", "'learning rate for the first N epochs; all epochs >N using LR_N'", "\n", "' (note: this may be interpreted differently depending on --lr-scheduler)'", ")", "\n", "group", ".", "add_argument", "(", "'--momentum'", ",", "default", "=", "0.99", ",", "type", "=", "float", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'momentum factor'", ")", "\n", "group", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "\n", "# Learning rate schedulers can be found under fairseq/optim/lr_scheduler/", "\n", "group", ".", "add_argument", "(", "'--lr-scheduler'", ",", "default", "=", "'reduce_lr_on_plateau'", ",", "\n", "choices", "=", "LR_SCHEDULER_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Learning Rate Scheduler'", ")", "\n", "group", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n", "help", "=", "'learning rate shrink factor for annealing, lr_new = (lr * lr_shrink)'", ")", "\n", "group", ".", "add_argument", "(", "'--min-lr'", ",", "default", "=", "1e-5", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'minimum learning rate'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_checkpoint_args": [[320, 349], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_checkpoint_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Checkpointing'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--save-dir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "'checkpoints'", ",", "\n", "help", "=", "'path to save checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--restore-file'", ",", "default", "=", "'checkpoint_last.pt'", ",", "\n", "help", "=", "'filename in save-dir from which to load checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-optimizer'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not load optimizer state from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-lr-scheduler'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not load lr scheduler state from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--optimizer-overrides'", ",", "default", "=", "\"{}\"", ",", "type", "=", "str", ",", "metavar", "=", "'DICT'", ",", "\n", "help", "=", "'a dictionary used to override optimizer args when loading a checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'save a checkpoint every N epochs'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval-updates'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'save a checkpoint (and validate) every N updates'", ")", "\n", "group", ".", "add_argument", "(", "'--keep-interval-updates'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'keep the last N checkpoints saved with --save-interval-updates'", ")", "\n", "group", ".", "add_argument", "(", "'--keep-last-epochs'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'keep last N epoch checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t save models or checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-epoch-checkpoints'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only store last and best checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--validate-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'validate every N epochs'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_common_eval_args": [[351, 361], ["group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument"], "function", ["None"], ["", "def", "add_common_eval_args", "(", "group", ")", ":", "\n", "# fmt: off", "\n", "    ", "group", ".", "add_argument", "(", "'--path'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'path(s) to model file(s), colon separated'", ")", "\n", "group", ".", "add_argument", "(", "'--remove-bpe'", ",", "nargs", "=", "'?'", ",", "const", "=", "'@@ '", ",", "default", "=", "None", ",", "\n", "help", "=", "'remove BPE tokens before scoring (can be set to sentencepiece)'", ")", "\n", "group", ".", "add_argument", "(", "'--quiet'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only print final scores'", ")", "\n", "group", ".", "add_argument", "(", "'--model-overrides'", ",", "default", "=", "\"{}\"", ",", "type", "=", "str", ",", "metavar", "=", "'DICT'", ",", "\n", "help", "=", "'a dictionary used to override model args at generation '", "\n", "'that were used during model training'", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_eval_lm_args": [[365, 373], ["parser.add_argument_group", "options.add_common_eval_args", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_common_eval_args"], ["", "def", "add_eval_lm_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'LM Evaluation'", ")", "\n", "add_common_eval_args", "(", "group", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--output-word-probs'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, outputs words and their predicted log probabilities to standard output'", ")", "\n", "group", ".", "add_argument", "(", "'--output-word-stats'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, outputs word statistics such as word count, average probability, etc'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_generation_args": [[376, 430], ["parser.add_argument_group", "options.add_common_eval_args", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_common_eval_args"], ["", "def", "add_generation_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Generation'", ")", "\n", "add_common_eval_args", "(", "group", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--beam'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'beam size'", ")", "\n", "group", ".", "add_argument", "(", "'--nbest'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of hypotheses to output'", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-a'", ",", "default", "=", "0", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-b'", ",", "default", "=", "200", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--min-len'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'minimum generation length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--match-source-len'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "(", "'generations should match the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--no-early-stop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "(", "'continue searching even after finalizing k=beam '", "\n", "'hypotheses; this is more correct, but increases '", "\n", "'generation time by 50%%'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--unnormalized'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'compare unnormalized hypothesis scores'", ")", "\n", "group", ".", "add_argument", "(", "'--no-beamable-mm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t use BeamableMM in attention layers'", ")", "\n", "group", ".", "add_argument", "(", "'--lenpen'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "\n", "help", "=", "'length penalty: <1.0 favors shorter, >1.0 favors longer sentences'", ")", "\n", "group", ".", "add_argument", "(", "'--unkpen'", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "'unknown word penalty: <0 produces more unks, >0 produces fewer'", ")", "\n", "group", ".", "add_argument", "(", "'--replace-unk'", ",", "nargs", "=", "'?'", ",", "const", "=", "True", ",", "default", "=", "None", ",", "\n", "help", "=", "'perform unknown replacement (optionally with alignment dictionary)'", ")", "\n", "group", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "group", ".", "add_argument", "(", "'--score-reference'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'just score the reference translation'", ")", "\n", "group", ".", "add_argument", "(", "'--prefix-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "'initialize generation by target prefix of given length'", ")", "\n", "group", ".", "add_argument", "(", "'--no-repeat-ngram-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'ngram blocking such that this size ngram cannot be repeated in the generation'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'sample hypotheses instead of using beam search'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling-topk'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "'sample from top K likely next words instead of all words'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling-temperature'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'temperature for random sampling'", ")", "\n", "group", ".", "add_argument", "(", "'--diverse-beam-groups'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of groups for Diverse Beam Search'", ")", "\n", "group", ".", "add_argument", "(", "'--diverse-beam-strength'", ",", "default", "=", "0.5", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'strength of diversity penalty for Diverse Beam Search'", ")", "\n", "group", ".", "add_argument", "(", "'--print-alignment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses attention feedback to compute and print alignment to source tokens'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_interactive_args": [[432, 439], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_interactive_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Interactive'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--buffer-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'read this many sentences into a buffer before processing them'", ")", "\n", "group", ".", "add_argument", "(", "'--input'", ",", "default", "=", "'-'", ",", "type", "=", "str", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file to read from; use - for stdin'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.add_model_args": [[442, 463], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "fairseq.models.ARCH_MODEL_REGISTRY.keys", "fairseq.criterions.CRITERION_REGISTRY.keys"], "function", ["None"], ["", "def", "add_model_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Model configuration'", ")", "\n", "# fmt: off", "\n", "\n", "# Model definitions can be found under fairseq/models/", "\n", "#", "\n", "# The model architecture can be specified in several ways.", "\n", "# In increasing order of priority:", "\n", "# 1) model defaults (lowest priority)", "\n", "# 2) --arch argument", "\n", "# 3) --encoder/decoder-* arguments (highest priority)", "\n", "group", ".", "add_argument", "(", "'--arch'", ",", "'-a'", ",", "default", "=", "'fconv'", ",", "metavar", "=", "'ARCH'", ",", "required", "=", "True", ",", "\n", "choices", "=", "ARCH_MODEL_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Model Architecture'", ")", "\n", "\n", "# Criterion definitions can be found under fairseq/criterions/", "\n", "group", ".", "add_argument", "(", "'--criterion'", ",", "default", "=", "'cross_entropy'", ",", "metavar", "=", "'CRIT'", ",", "\n", "choices", "=", "CRITERION_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Training Criterion'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master": [[20, 22], ["None"], "function", ["None"], ["def", "is_master", "(", "args", ")", ":", "\n", "    ", "return", "args", ".", "distributed_rank", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.infer_init_method": [[24, 54], ["all", "int", "int", "os.environ.get", "subprocess.check_output", "int", "int", "os.environ.get", "os.environ.get", "[].decode", "subprocess.check_output.split"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "infer_init_method", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_init_method", "is", "not", "None", ":", "\n", "        ", "return", "\n", "\n", "# support torch.distributed.launch", "\n", "", "if", "all", "(", "key", "in", "os", ".", "environ", "for", "key", "in", "[", "\n", "'MASTER_ADDR'", ",", "'MASTER_PORT'", ",", "'WORLD_SIZE'", ",", "'RANK'", "\n", "]", ")", ":", "\n", "        ", "args", ".", "distributed_init_method", "=", "'tcp://{addr}:{port}'", ".", "format", "(", "\n", "addr", "=", "os", ".", "environ", "[", "'MASTER_ADDR'", "]", ",", "\n", "port", "=", "os", ".", "environ", "[", "'MASTER_PORT'", "]", ",", "\n", ")", "\n", "args", ".", "distributed_world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", "[", "'RANK'", "]", ")", "\n", "\n", "# we can determine the init method automatically for Slurm", "\n", "", "elif", "args", ".", "distributed_port", ">", "0", ":", "\n", "        ", "node_list", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_NODELIST'", ")", "\n", "if", "node_list", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "hostnames", "=", "subprocess", ".", "check_output", "(", "[", "'scontrol'", ",", "'show'", ",", "'hostnames'", ",", "node_list", "]", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://{host}:{port}'", ".", "format", "(", "\n", "host", "=", "hostnames", ".", "split", "(", ")", "[", "0", "]", ".", "decode", "(", "'utf-8'", ")", ",", "\n", "port", "=", "args", ".", "distributed_port", ")", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_PROCID'", ")", ")", "\n", "args", ".", "device_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_LOCALID'", ")", ")", "\n", "", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# scontrol failed", "\n", "                ", "raise", "e", "\n", "", "except", "FileNotFoundError", ":", "# Slurm is not installed", "\n", "                ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.distributed_init": [[56, 73], ["distributed_utils.suppress_output.print", "torch.init_process_group", "distributed_utils.suppress_output", "ValueError", "distributed_utils.is_master"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.suppress_output", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master"], ["", "", "", "", "def", "distributed_init", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_world_size", "==", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Cannot initialize distributed with distributed_world_size=1'", ")", "\n", "\n", "", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "distributed_rank", ",", "args", ".", "distributed_init_method", ")", ",", "flush", "=", "True", ")", "\n", "\n", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "args", ".", "distributed_backend", ",", "\n", "init_method", "=", "args", ".", "distributed_init_method", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ",", "\n", "rank", "=", "args", ".", "distributed_rank", ",", "\n", ")", "\n", "\n", "suppress_output", "(", "is_master", "(", "args", ")", ")", "\n", "\n", "return", "args", ".", "distributed_rank", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.suppress_output": [[75, 86], ["kwargs.pop", "builtin_print"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop"], ["", "def", "suppress_output", "(", "is_master", ")", ":", "\n", "    ", "\"\"\"Suppress printing on the current device. Force printing with `force=True`.\"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ",", "False", ")", "\n", "if", "is_master", "or", "force", ":", "\n", "            ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_rank": [[88, 90], ["torch.get_rank"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_rank"], ["", "def", "get_rank", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_world_size": [[92, 94], ["torch.get_world_size"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_default_group": [[96, 98], ["None"], "function", ["None"], ["", "def", "get_default_group", "(", ")", ":", "\n", "    ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_reduce": [[100, 104], ["torch.all_reduce", "distributed_utils.get_default_group"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_default_group"], ["", "def", "all_reduce", "(", "tensor", ",", "group", "=", "None", ")", ":", "\n", "    ", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "get_default_group", "(", ")", "\n", "", "return", "dist", ".", "all_reduce", "(", "tensor", ",", "group", "=", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_gather_list": [[106, 154], ["distributed_utils.get_rank", "distributed_utils.get_world_size", "buffer.zero_", "pickle.dumps", "len", "torch.ByteTensor", "torch.ByteTensor", "distributed_utils.all_reduce", "torch.cuda.ByteTensor", "torch.cuda.ByteTensor", "ValueError", "list", "range", "hasattr", "all_gather_list._buffer.numel", "Exception", "fairseq.utils.item", "result.append", "fairseq.utils.item", "pickle.loads", "bytes", "out_buffer[].tolist"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.get_world_size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "all_gather_list", "(", "data", ",", "group", "=", "None", ",", "max_size", "=", "16384", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\n\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable.\n\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group (optional): group of the collective\n        max_size (int, optional): maximum size of the data to be gathered\n            across workers\n    \"\"\"", "\n", "rank", "=", "get_rank", "(", ")", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "\n", "buffer_size", "=", "max_size", "*", "world_size", "\n", "if", "not", "hasattr", "(", "all_gather_list", ",", "'_buffer'", ")", "or", "all_gather_list", ".", "_buffer", ".", "numel", "(", ")", "<", "buffer_size", ":", "\n", "        ", "all_gather_list", ".", "_buffer", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "buffer_size", ")", "\n", "", "buffer", "=", "all_gather_list", ".", "_buffer", "\n", "buffer", ".", "zero_", "(", ")", "\n", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "enc_size", "=", "len", "(", "enc", ")", "\n", "if", "enc_size", "+", "2", ">", "max_size", ":", "\n", "        ", "raise", "ValueError", "(", "'encoded data exceeds max_size: {}'", ".", "format", "(", "enc_size", "+", "2", ")", ")", "\n", "", "assert", "max_size", "<", "255", "*", "256", "\n", "\n", "buffer_rank", "=", "buffer", "[", "rank", "*", "max_size", ":", "(", "rank", "+", "1", ")", "*", "max_size", "]", "\n", "buffer_rank", "[", "0", "]", "=", "enc_size", "//", "255", "# this encoding works for max_size < 65k", "\n", "buffer_rank", "[", "1", "]", "=", "enc_size", "%", "255", "\n", "buffer_rank", "[", "2", ":", "enc_size", "+", "2", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "\n", "all_reduce", "(", "buffer", ",", "group", "=", "group", ")", "\n", "\n", "try", ":", "\n", "        ", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "out_buffer", "=", "buffer", "[", "i", "*", "max_size", ":", "(", "i", "+", "1", ")", "*", "max_size", "]", "\n", "size", "=", "(", "255", "*", "utils", ".", "item", "(", "out_buffer", "[", "0", "]", ")", ")", "+", "utils", ".", "item", "(", "out_buffer", "[", "1", "]", ")", "\n", "if", "size", ">", "0", ":", "\n", "                ", "result", ".", "append", "(", "\n", "pickle", ".", "loads", "(", "bytes", "(", "out_buffer", "[", "2", ":", "size", "+", "2", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", "\n", "", "", "return", "result", "\n", "", "except", "pickle", ".", "UnpicklingError", ":", "\n", "        ", "raise", "Exception", "(", "\n", "'Unable to unpickle data from other workers. all_gather_list requires all '", "\n", "'workers to enter the function together, so this error usually indicates '", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__init__": [[44, 68], ["torch.nn.Module.__init__", "min", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook", "sum", "p.numel", "module.parameters"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], ["def", "__init__", "(", "self", ",", "module", ",", "world_size", ",", "process_group", "=", "None", ",", "buffer_size", "=", "2", "**", "28", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "world_size", "=", "world_size", "\n", "self", ".", "process_group", "=", "process_group", "\n", "\n", "# Never use a bigger buffer than the number of model params", "\n", "self", ".", "buffer_size", "=", "min", "(", "buffer_size", ",", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "module", ".", "parameters", "(", ")", ")", ")", "\n", "self", ".", "buffer", "=", "None", "\n", "\n", "# Flag used by the NCCL backend to make sure we only reduce gradients", "\n", "# one time in the execution engine", "\n", "self", ".", "need_reduction", "=", "False", "\n", "\n", "# We can also forcibly accumulate grads locally and only do the", "\n", "# all-reduce at some later time", "\n", "self", ".", "accumulate_grads", "=", "False", "\n", "\n", "# For NCCL backend, since every single NCCL call is asynchoronous, we", "\n", "# therefore directly enqueue all the NCCL reduction calls to the", "\n", "# default CUDA stream without spawning up other reduction threads.", "\n", "# This achieves the best performance.", "\n", "self", ".", "_register_grad_hook", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__getstate__": [[69, 72], ["copy.copy"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.copy"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "attrs", "=", "copy", ".", "copy", "(", "self", ".", "__dict__", ")", "\n", "return", "attrs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__setstate__": [[73, 76], ["super().__setstate__", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__setstate__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", ")", ".", "__setstate__", "(", "state", ")", "\n", "self", ".", "_register_grad_hook", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.forward": [[77, 79], ["legacy_distributed_data_parallel.LegacyDistributedDataParallel.module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "module", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook": [[80, 173], ["legacy_distributed_data_parallel.LegacyDistributedDataParallel.module.parameters", "distributed_utils.all_reduce", "legacy_distributed_data_parallel.LegacyDistributedDataParallel.module.parameters", "len", "torch.zeros_like.div_", "p.numel", "next().new", "param.numel", "len", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook.all_reduce"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_reduce"], ["", "def", "_register_grad_hook", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This function registers the callback all-reduction function for the\n        NCCL backend. All gradients will be all reduced in one single step.\n        The NCCL reduction will directly be enqueued into the default CUDA\n        stream. Therefore, no synchronization is needed.\n        \"\"\"", "\n", "\n", "def", "all_reduce", "(", "params", ")", ":", "\n", "            ", "buffer", "=", "self", ".", "buffer", "\n", "nonzero_buffer", "=", "False", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "                ", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "                    ", "sz", "=", "p", ".", "numel", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "copy_", "(", "p", ".", "grad", ".", "data", ".", "view", "(", "-", "1", ")", ")", "\n", "nonzero_buffer", "=", "True", "\n", "", "else", ":", "\n", "                        ", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "zero_", "(", ")", "\n", "", "offset", "+=", "sz", "\n", "", "", "else", ":", "\n", "# we only have a single grad to all-reduce", "\n", "                ", "p", "=", "params", "[", "0", "]", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "buffer", "=", "p", ".", "grad", ".", "data", "\n", "nonzero_buffer", "=", "True", "\n", "", "elif", "p", ".", "numel", "(", ")", "<=", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "                    ", "buffer", "=", "buffer", "[", ":", "p", ".", "numel", "(", ")", "]", "\n", "buffer", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                    ", "buffer", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "if", "nonzero_buffer", ":", "\n", "                ", "buffer", ".", "div_", "(", "self", ".", "world_size", ")", "\n", "\n", "", "distributed_utils", ".", "all_reduce", "(", "buffer", ",", "self", ".", "process_group", ")", "\n", "\n", "# copy all-reduced grads back into their original place", "\n", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "                ", "sz", "=", "p", ".", "numel", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "p", ".", "grad", ".", "data", ".", "copy_", "(", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "view_as", "(", "p", ")", ")", "\n", "", "else", ":", "\n", "                    ", "p", ".", "grad", "=", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "view_as", "(", "p", ")", ".", "clone", "(", ")", "\n", "", "offset", "+=", "sz", "\n", "\n", "", "", "def", "reduction_fn", "(", ")", ":", "\n", "# This function only needs to be called once", "\n", "            ", "if", "not", "self", ".", "need_reduction", "or", "self", ".", "accumulate_grads", ":", "\n", "                ", "return", "\n", "", "self", ".", "need_reduction", "=", "False", "\n", "\n", "if", "self", ".", "buffer", "is", "None", ":", "\n", "                ", "self", ".", "buffer", "=", "next", "(", "self", ".", "module", ".", "parameters", "(", ")", ")", ".", "new", "(", "self", ".", "buffer_size", ")", "\n", "\n", "# All-reduce the gradients in buckets", "\n", "", "offset", "=", "0", "\n", "buffered_params", "=", "[", "]", "\n", "for", "param", "in", "self", ".", "module", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "not", "param", ".", "requires_grad", ":", "\n", "                    ", "continue", "\n", "", "if", "param", ".", "grad", "is", "None", ":", "\n", "                    ", "param", ".", "grad", "=", "torch", ".", "zeros_like", "(", "param", ")", "\n", "", "if", "param", ".", "grad", ".", "requires_grad", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"DistributedDataParallel only works \"", "\n", "\"with gradients that don't require \"", "\n", "\"grad\"", ")", "\n", "", "sz", "=", "param", ".", "numel", "(", ")", "\n", "if", "sz", ">", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "# all-reduce big params directly", "\n", "                    ", "all_reduce", "(", "[", "param", "]", ")", "\n", "", "else", ":", "\n", "                    ", "if", "offset", "+", "sz", ">", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "                        ", "all_reduce", "(", "buffered_params", ")", "\n", "offset", "=", "0", "\n", "buffered_params", ".", "clear", "(", ")", "\n", "", "buffered_params", ".", "append", "(", "param", ")", "\n", "offset", "+=", "sz", "\n", "\n", "", "", "if", "len", "(", "buffered_params", ")", ">", "0", ":", "\n", "                ", "all_reduce", "(", "buffered_params", ")", "\n", "\n", "# Now register the reduction hook on the parameters", "\n", "", "", "for", "p", "in", "self", ".", "module", ".", "parameters", "(", ")", ":", "\n", "\n", "            ", "def", "allreduce_hook", "(", "*", "unused", ")", ":", "\n", "                ", "self", ".", "need_reduction", "=", "True", "\n", "Variable", ".", "_execution_engine", ".", "queue_callback", "(", "reduction_fn", ")", "\n", "\n", "", "if", "p", ".", "requires_grad", ":", "\n", "                ", "p", ".", "register_hook", "(", "allreduce_hook", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.__init__": [[63, 71], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "prefix", "=", "''", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "'| epoch {:03d}'", ".", "format", "(", "epoch", ")", "\n", "", "if", "prefix", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "' | {}'", ".", "format", "(", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.__enter__": [[72, 74], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.__exit__": [[75, 77], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "exc", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.__iter__": [[78, 80], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.log": [[81, 84], ["None"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar.print": [[85, 88], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._str_commas": [[89, 92], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_commas", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "', '", ".", "join", "(", "key", "+", "'='", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._str_pipes": [[93, 96], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_pipes", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "' | '", ".", "join", "(", "key", "+", "' '", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._format_stats": [[97, 103], ["collections.OrderedDict", "collections.OrderedDict.keys", "str", "progress_bar.format_stat"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.format_stat"], ["", "def", "_format_stats", "(", "self", ",", "stats", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", "stats", ")", "\n", "# Preprocess stats according to datatype", "\n", "for", "key", "in", "postfix", ".", "keys", "(", ")", ":", "\n", "            ", "postfix", "[", "key", "]", "=", "str", "(", "format_stat", "(", "postfix", "[", "key", "]", ")", ")", "\n", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar.__init__": [[108, 112], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar.__iter__": [[113, 122], ["float", "enumerate", "len", "progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "float", "(", "len", "(", "self", ".", "iterable", ")", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "update", "=", "self", ".", "epoch", "-", "1", "+", "float", "(", "i", "/", "size", ")", "if", "self", ".", "epoch", "is", "not", "None", "else", "None", "\n", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ",", "update", "=", "update", ")", "\n", "print", "(", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar.log": [[123, 126], ["None"], "methods", ["None"], ["", "", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar.print": [[127, 134], ["progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "self", ".", "stats", "=", "stats", "\n", "if", "tag", "!=", "''", ":", "\n", "            ", "self", ".", "stats", "=", "OrderedDict", "(", "[", "(", "tag", "+", "'_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "self", ".", "stats", ".", "items", "(", ")", "]", ")", "\n", "", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ")", "\n", "print", "(", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats": [[135, 145], ["collections.OrderedDict", "stats.keys", "round", "progress_bar.format_stat"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.format_stat"], ["", "def", "_format_stats", "(", "self", ",", "stats", ",", "epoch", "=", "None", ",", "update", "=", "None", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", ")", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'epoch'", "]", "=", "epoch", "\n", "", "if", "update", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'update'", "]", "=", "round", "(", "update", ",", "3", ")", "\n", "# Preprocess stats according to datatype", "\n", "", "for", "key", "in", "stats", ".", "keys", "(", ")", ":", "\n", "            ", "postfix", "[", "key", "]", "=", "format_stat", "(", "stats", "[", "key", "]", ")", "\n", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.noop_progress_bar.__init__": [[150, 152], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.noop_progress_bar.__iter__": [[153, 156], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "obj", "in", "self", ".", "iterable", ":", "\n", "            ", "yield", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.noop_progress_bar.log": [[157, 160], ["None"], "methods", ["None"], ["", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.noop_progress_bar.print": [[161, 164], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.simple_progress_bar.__init__": [[169, 173], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.simple_progress_bar.__iter__": [[174, 183], ["len", "enumerate", "progress_bar.simple_progress_bar._str_commas", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._str_commas", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "len", "(", "self", ".", "iterable", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "postfix", "=", "self", ".", "_str_commas", "(", "self", ".", "stats", ")", "\n", "print", "(", "'{}:  {:5d} / {:d} {}'", ".", "format", "(", "self", ".", "prefix", ",", "i", ",", "size", ",", "postfix", ")", ",", "\n", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.simple_progress_bar.log": [[184, 187], ["progress_bar.simple_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "self", ".", "_format_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.simple_progress_bar.print": [[188, 192], ["progress_bar.simple_progress_bar._str_pipes", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "print", "(", "'{} | {}'", ".", "format", "(", "self", ".", "prefix", ",", "postfix", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tqdm_progress_bar.__init__": [[197, 200], ["progress_bar.progress_bar.__init__", "tqdm.tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "tqdm", "=", "tqdm", "(", "iterable", ",", "self", ".", "prefix", ",", "leave", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tqdm_progress_bar.__iter__": [[201, 203], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "tqdm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tqdm_progress_bar.log": [[204, 207], ["progress_bar.tqdm_progress_bar.tqdm.set_postfix", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "tqdm", ".", "set_postfix", "(", "self", ".", "_format_stats", "(", "stats", ")", ",", "refresh", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tqdm_progress_bar.print": [[208, 212], ["progress_bar.tqdm_progress_bar._str_pipes", "progress_bar.tqdm_progress_bar.tqdm.write", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "self", ".", "tqdm", ".", "write", "(", "'{} | {}'", ".", "format", "(", "self", ".", "tqdm", ".", "desc", ",", "postfix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.__init__": [[217, 229], ["progress_bar.tensorboard_log_wrapper.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "__init__", "(", "self", ",", "wrapped_bar", ",", "tensorboard_logdir", ")", ":", "\n", "        ", "self", ".", "wrapped_bar", "=", "wrapped_bar", "\n", "self", ".", "tensorboard_logdir", "=", "tensorboard_logdir", "\n", "\n", "try", ":", "\n", "            ", "from", "tensorboardX", "import", "SummaryWriter", "\n", "self", ".", "SummaryWriter", "=", "SummaryWriter", "\n", "self", ".", "_writers", "=", "{", "}", "\n", "", "except", "ImportError", ":", "\n", "            ", "print", "(", "\"tensorboard or required dependencies not found, \"", "\n", "\"please see README for using tensorboard.\"", ")", "\n", "self", ".", "SummaryWriter", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper._writer": [[230, 238], ["progress_bar.tensorboard_log_wrapper.SummaryWriter", "os.path.join"], "methods", ["None"], ["", "", "def", "_writer", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "self", ".", "SummaryWriter", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "if", "key", "not", "in", "self", ".", "_writers", ":", "\n", "            ", "self", ".", "_writers", "[", "key", "]", "=", "self", ".", "SummaryWriter", "(", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "tensorboard_logdir", ",", "key", ")", ",", "\n", ")", "\n", "", "return", "self", ".", "_writers", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.__iter__": [[239, 241], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "wrapped_bar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log": [[242, 246], ["progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "progress_bar.tensorboard_log_wrapper.wrapped_bar.log"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats to tensorboard.\"\"\"", "\n", "self", ".", "_log_to_tensorboard", "(", "stats", ",", "tag", ",", "step", ")", "\n", "self", ".", "wrapped_bar", ".", "log", "(", "stats", ",", "tag", "=", "tag", ",", "step", "=", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print": [[247, 251], ["progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "progress_bar.tensorboard_log_wrapper.wrapped_bar.print"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "self", ".", "_log_to_tensorboard", "(", "stats", ",", "tag", ",", "step", ")", "\n", "self", ".", "wrapped_bar", ".", "print", "(", "stats", ",", "tag", "=", "tag", ",", "step", "=", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard": [[252, 263], ["progress_bar.tensorboard_log_wrapper._writer", "stats.keys", "isinstance", "progress_bar.tensorboard_log_wrapper.add_scalar", "isinstance", "progress_bar.tensorboard_log_wrapper.add_scalar"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper._writer"], ["", "def", "_log_to_tensorboard", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "writer", "=", "self", ".", "_writer", "(", "tag", ")", "\n", "if", "writer", "is", "None", ":", "\n", "            ", "return", "\n", "", "if", "step", "is", "None", ":", "\n", "            ", "step", "=", "stats", "[", "'num_updates'", "]", "\n", "", "for", "key", "in", "stats", ".", "keys", "(", ")", "-", "{", "'num_updates'", "}", ":", "\n", "            ", "if", "isinstance", "(", "stats", "[", "key", "]", ",", "AverageMeter", ")", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "key", ",", "stats", "[", "key", "]", ".", "val", ",", "step", ")", "\n", "", "elif", "isinstance", "(", "stats", "[", "key", "]", ",", "Number", ")", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "key", ",", "stats", "[", "key", "]", ",", "step", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar": [[25, 47], ["progress_bar.json_progress_bar", "fairseq.distributed_utils.is_master", "progress_bar.tensorboard_log_wrapper", "sys.stderr.isatty", "progress_bar.noop_progress_bar", "progress_bar.simple_progress_bar", "progress_bar.tqdm_progress_bar", "ValueError"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master"], ["def", "build_progress_bar", "(", "args", ",", "iterator", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "default", "=", "'tqdm'", ",", "no_progress_bar", "=", "'none'", ")", ":", "\n", "    ", "if", "args", ".", "log_format", "is", "None", ":", "\n", "        ", "args", ".", "log_format", "=", "no_progress_bar", "if", "args", ".", "no_progress_bar", "else", "default", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'tqdm'", "and", "not", "sys", ".", "stderr", ".", "isatty", "(", ")", ":", "\n", "        ", "args", ".", "log_format", "=", "'simple'", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'json'", ":", "\n", "        ", "bar", "=", "json_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'none'", ":", "\n", "        ", "bar", "=", "noop_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'simple'", ":", "\n", "        ", "bar", "=", "simple_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'tqdm'", ":", "\n", "        ", "bar", "=", "tqdm_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown log format: {}'", ".", "format", "(", "args", ".", "log_format", ")", ")", "\n", "\n", "", "if", "args", ".", "tensorboard_logdir", "and", "distributed_utils", ".", "is_master", "(", "args", ")", ":", "\n", "        ", "bar", "=", "tensorboard_log_wrapper", "(", "bar", ",", "args", ".", "tensorboard_logdir", ")", "\n", "\n", "", "return", "bar", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.format_stat": [[49, 59], ["isinstance", "isinstance", "isinstance", "isinstance", "round", "round"], "function", ["None"], ["", "def", "format_stat", "(", "stat", ")", ":", "\n", "    ", "if", "isinstance", "(", "stat", ",", "Number", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "stat", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "AverageMeter", ")", ":", "\n", "        ", "stat", "=", "'{:.3f}'", ".", "format", "(", "stat", ".", "avg", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "TimeMeter", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "round", "(", "stat", ".", "avg", ")", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "StopwatchMeter", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "round", "(", "stat", ".", "sum", ")", ")", "\n", "", "return", "stat", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_scorer.SequenceScorer.__init__": [[16, 18], ["tgt_dict.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["def", "__init__", "(", "self", ",", "tgt_dict", ")", ":", "\n", "        ", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_scorer.SequenceScorer.generate": [[19, 73], ["torch.no_grad", "avg_probs.gather().squeeze.gather().squeeze.gather().squeeze", "range", "model.eval", "model.forward", "model.get_normalized_probs", "len", "avg_probs.gather().squeeze.gather().squeeze.div_", "avg_probs.gather().squeeze.gather().squeeze.log_", "avg_probs.gather().squeeze.gather().squeeze.size", "ref.numel", "hypos.append", "avg_probs.gather().squeeze.gather().squeeze.add_", "torch.is_tensor", "len", "avg_attn.div_", "avg_probs.gather().squeeze.gather().squeeze.gather", "fairseq.utils.strip_pad", "avg_probs_i.sum", "avg_attn_i.max", "avg_attn.add_", "len", "len", "sample[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.forward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "self", ",", "models", ",", "sample", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Score a batch of translations.\"\"\"", "\n", "net_input", "=", "sample", "[", "'net_input'", "]", "\n", "\n", "# compute scores for each model in the ensemble", "\n", "avg_probs", "=", "None", "\n", "avg_attn", "=", "None", "\n", "for", "model", "in", "models", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "decoder_out", "=", "model", ".", "forward", "(", "**", "net_input", ")", "\n", "attn", "=", "decoder_out", "[", "1", "]", "\n", "\n", "probs", "=", "model", ".", "get_normalized_probs", "(", "decoder_out", ",", "log_probs", "=", "len", "(", "models", ")", "==", "1", ",", "sample", "=", "sample", ")", "\n", "if", "avg_probs", "is", "None", ":", "\n", "                ", "avg_probs", "=", "probs", "\n", "", "else", ":", "\n", "                ", "avg_probs", ".", "add_", "(", "probs", ")", "\n", "", "if", "attn", "is", "not", "None", "and", "torch", ".", "is_tensor", "(", "attn", ")", ":", "\n", "                ", "attn", "=", "attn", ".", "data", "\n", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "", "if", "len", "(", "models", ")", ">", "1", ":", "\n", "            ", "avg_probs", ".", "div_", "(", "len", "(", "models", ")", ")", "\n", "avg_probs", ".", "log_", "(", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "                ", "avg_attn", ".", "div_", "(", "len", "(", "models", ")", ")", "\n", "", "", "avg_probs", "=", "avg_probs", ".", "gather", "(", "\n", "dim", "=", "2", ",", "\n", "index", "=", "sample", "[", "'target'", "]", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", ")", ".", "squeeze", "(", "2", ")", "\n", "\n", "hypos", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "avg_probs", ".", "size", "(", "0", ")", ")", ":", "\n", "# remove padding from ref", "\n", "            ", "ref", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'target'", "]", "[", "i", ",", ":", "]", ",", "self", ".", "pad", ")", "if", "sample", "[", "'target'", "]", "is", "not", "None", "else", "None", "\n", "tgt_len", "=", "ref", ".", "numel", "(", ")", "\n", "avg_probs_i", "=", "avg_probs", "[", "i", "]", "[", ":", "tgt_len", "]", "\n", "score_i", "=", "avg_probs_i", ".", "sum", "(", ")", "/", "tgt_len", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "                ", "avg_attn_i", "=", "avg_attn", "[", "i", "]", "\n", "_", ",", "alignment", "=", "avg_attn_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "avg_attn_i", "=", "alignment", "=", "None", "\n", "", "hypos", ".", "append", "(", "[", "{", "\n", "'tokens'", ":", "ref", ",", "\n", "'score'", ":", "score_i", ",", "\n", "'attention'", ":", "avg_attn_i", ",", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "avg_probs_i", ",", "\n", "}", "]", ")", "\n", "", "return", "hypos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.tokenizer.tokenize_line": [[13, 17], ["SPACE_NORMALIZER.sub", "line.strip.strip", "line.strip.split"], "function", ["None"], ["def", "tokenize_line", "(", "line", ")", ":", "\n", "    ", "line", "=", "SPACE_NORMALIZER", ".", "sub", "(", "\" \"", ",", "line", ")", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "return", "line", ".", "split", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.__init__": [[39, 43], ["bleu.SacrebleuScorer.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "import", "sacrebleu", "\n", "self", ".", "sacrebleu", "=", "sacrebleu", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.reset": [[44, 49], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "one_init", "=", "False", ")", ":", "\n", "        ", "if", "one_init", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "ref", "=", "[", "]", "\n", "self", ".", "sys", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.add_string": [[50, 53], ["bleu.SacrebleuScorer.ref.append", "bleu.SacrebleuScorer.sys.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "add_string", "(", "self", ",", "ref", ",", "pred", ")", ":", "\n", "        ", "self", ".", "ref", ".", "append", "(", "ref", ")", "\n", "self", ".", "sys", ".", "append", "(", "pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.score": [[54, 56], ["bleu.SacrebleuScorer.result_string"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.result_string"], ["", "def", "score", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "return", "self", ".", "result_string", "(", "order", ")", ".", "bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.result_string": [[57, 61], ["bleu.SacrebleuScorer.sacrebleu.corpus_bleu"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.corpus_bleu"], ["", "def", "result_string", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "if", "order", "!=", "4", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "self", ".", "sacrebleu", ".", "corpus_bleu", "(", "self", ".", "sys", ",", "[", "self", ".", "ref", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.__init__": [[64, 70], ["bleu.BleuStat", "bleu.Scorer.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["    ", "def", "__init__", "(", "self", ",", "pad", ",", "eos", ",", "unk", ")", ":", "\n", "        ", "self", ".", "stat", "=", "BleuStat", "(", ")", "\n", "self", ".", "pad", "=", "pad", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "unk", "=", "unk", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.reset": [[71, 76], ["C.bleu_one_init", "C.bleu_zero_init", "ctypes.byref", "ctypes.byref"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "one_init", "=", "False", ")", ":", "\n", "        ", "if", "one_init", ":", "\n", "            ", "C", ".", "bleu_one_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "", "else", ":", "\n", "            ", "C", ".", "bleu_zero_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.add": [[77, 101], ["ref.clone", "rref.contiguous().view.contiguous().view.contiguous().view", "pred.contiguous().view.contiguous().view.contiguous().view", "C.bleu_add", "isinstance", "TypeError", "isinstance", "TypeError", "rref.contiguous().view.contiguous().view.lt().any", "ctypes.byref", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_int", "ctypes.c_int", "rref.contiguous().view.contiguous().view.eq", "rref.contiguous().view.contiguous().view.contiguous", "pred.contiguous().view.contiguous().view.contiguous", "rref.contiguous().view.contiguous().view.size", "rref.contiguous().view.contiguous().view.data_ptr", "pred.contiguous().view.contiguous().view.size", "pred.contiguous().view.contiguous().view.data_ptr", "type", "type", "rref.contiguous().view.contiguous().view.lt"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "", "def", "add", "(", "self", ",", "ref", ",", "pred", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "ref", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'ref must be a torch.IntTensor (got {})'", "\n", ".", "format", "(", "type", "(", "ref", ")", ")", ")", "\n", "", "if", "not", "isinstance", "(", "pred", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'pred must be a torch.IntTensor(got {})'", "\n", ".", "format", "(", "type", "(", "pred", ")", ")", ")", "\n", "\n", "# don't match unknown words", "\n", "", "rref", "=", "ref", ".", "clone", "(", ")", "\n", "assert", "not", "rref", ".", "lt", "(", "0", ")", ".", "any", "(", ")", "\n", "rref", "[", "rref", ".", "eq", "(", "self", ".", "unk", ")", "]", "=", "-", "999", "\n", "\n", "rref", "=", "rref", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "pred", "=", "pred", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "C", ".", "bleu_add", "(", "\n", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "rref", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "rref", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "pred", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "pred", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "pad", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "eos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score": [[102, 106], ["sum", "bleu.Scorer.brevity", "math.exp", "math.log", "float", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.precision"], ["", "def", "score", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "psum", "=", "sum", "(", "math", ".", "log", "(", "p", ")", "if", "p", ">", "0", "else", "float", "(", "'-Inf'", ")", "\n", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", ")", "\n", "return", "self", ".", "brevity", "(", ")", "*", "math", ".", "exp", "(", "psum", "/", "order", ")", "*", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.precision": [[107, 116], ["bleu.Scorer.precision.ratio"], "methods", ["None"], ["", "def", "precision", "(", "self", ")", ":", "\n", "        ", "def", "ratio", "(", "a", ",", "b", ")", ":", "\n", "            ", "return", "a", "/", "b", "if", "b", ">", "0", "else", "0", "\n", "\n", "", "return", "[", "\n", "ratio", "(", "self", ".", "stat", ".", "match1", ",", "self", ".", "stat", ".", "count1", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match2", ",", "self", ".", "stat", ".", "count2", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match3", ",", "self", ".", "stat", ".", "count3", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match4", ",", "self", ".", "stat", ".", "count4", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.brevity": [[118, 121], ["min", "math.exp"], "methods", ["None"], ["", "def", "brevity", "(", "self", ")", ":", "\n", "        ", "r", "=", "self", ".", "stat", ".", "reflen", "/", "self", ".", "stat", ".", "predlen", "\n", "return", "min", "(", "1", ",", "math", ".", "exp", "(", "1", "-", "r", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.result_string": [[122, 132], ["range", "fmt.format", "bleu.Scorer.score", "bleu.Scorer.brevity", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.precision"], ["", "def", "result_string", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "assert", "order", "<=", "4", ",", "\"BLEU scores for order > 4 aren't supported\"", "\n", "fmt", "=", "'BLEU{} = {:2.2f}, {:2.1f}'", "\n", "for", "_", "in", "range", "(", "1", ",", "order", ")", ":", "\n", "            ", "fmt", "+=", "'/{:2.1f}'", "\n", "", "fmt", "+=", "' (BP={:.3f}, ratio={:.3f}, syslen={}, reflen={})'", "\n", "bleup", "=", "[", "p", "*", "100", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", "]", "\n", "return", "fmt", ".", "format", "(", "order", ",", "self", ".", "score", "(", "order", "=", "order", ")", ",", "*", "bleup", ",", "\n", "self", ".", "brevity", "(", ")", ",", "self", ".", "stat", ".", "predlen", "/", "self", ".", "stat", ".", "reflen", ",", "\n", "self", ".", "stat", ".", "predlen", ",", "self", ".", "stat", ".", "reflen", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.__init__": [[32, 57], ["trainer.Trainer.init_meters", "torch.cuda.is_available", "trainer.Trainer._model.half", "trainer.Trainer.criterion.cuda", "trainer.Trainer._model.cuda"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.init_meters"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ",", "model", ",", "criterion", ",", "dummy_batch", ",", "oom_batch", "=", "None", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "task", "=", "task", "\n", "\n", "# copy model and criterion to current device", "\n", "self", ".", "criterion", "=", "criterion", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "_model", "=", "self", ".", "_model", ".", "half", "(", ")", "\n", "", "if", "self", ".", "cuda", ":", "\n", "            ", "self", ".", "criterion", "=", "self", ".", "criterion", ".", "cuda", "(", ")", "\n", "self", ".", "_model", "=", "self", ".", "_model", ".", "cuda", "(", ")", "\n", "\n", "", "self", ".", "_dummy_batch", "=", "dummy_batch", "\n", "self", ".", "_oom_batch", "=", "oom_batch", "\n", "\n", "self", ".", "_lr_scheduler", "=", "None", "\n", "self", ".", "_num_updates", "=", "0", "\n", "self", ".", "_optim_history", "=", "None", "\n", "self", ".", "_optimizer", "=", "None", "\n", "self", ".", "_prev_grad_norm", "=", "None", "\n", "self", ".", "_wrapped_model", "=", "None", "\n", "\n", "self", ".", "init_meters", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.init_meters": [[58, 75], ["collections.OrderedDict", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.TimeMeter", "fairseq.meters.TimeMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.TimeMeter", "fairseq.meters.StopwatchMeter", "fairseq.meters.AverageMeter"], "methods", ["None"], ["", "def", "init_meters", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "meters", "=", "OrderedDict", "(", ")", "\n", "self", ".", "meters", "[", "'train_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'train_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'wps'", "]", "=", "TimeMeter", "(", ")", "# words per second", "\n", "self", ".", "meters", "[", "'ups'", "]", "=", "TimeMeter", "(", ")", "# updates per second", "\n", "self", ".", "meters", "[", "'wpb'", "]", "=", "AverageMeter", "(", ")", "# words per batch", "\n", "self", ".", "meters", "[", "'bsz'", "]", "=", "AverageMeter", "(", ")", "# sentences per batch", "\n", "self", ".", "meters", "[", "'gnorm'", "]", "=", "AverageMeter", "(", ")", "# gradient norm", "\n", "self", ".", "meters", "[", "'clip'", "]", "=", "AverageMeter", "(", ")", "# % of updates clipped", "\n", "self", ".", "meters", "[", "'oom'", "]", "=", "AverageMeter", "(", ")", "# out of memory", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "meters", "[", "'loss_scale'", "]", "=", "AverageMeter", "(", ")", "# dynamic loss scale", "\n", "", "self", ".", "meters", "[", "'wall'", "]", "=", "TimeMeter", "(", ")", "# wall time in seconds", "\n", "self", ".", "meters", "[", "'train_wall'", "]", "=", "StopwatchMeter", "(", ")", "# train wall time in seconds", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model": [[76, 86], ["fairseq.models.DistributedFairseqModel"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.distributed_fairseq_model.DistributedFairseqModel"], ["", "@", "property", "\n", "def", "model", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_wrapped_model", "is", "None", ":", "\n", "            ", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "                ", "self", ".", "_wrapped_model", "=", "models", ".", "DistributedFairseqModel", "(", "\n", "self", ".", "args", ",", "self", ".", "_model", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_wrapped_model", "=", "self", ".", "_model", "\n", "", "", "return", "self", ".", "_wrapped_model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.optimizer": [[87, 92], ["trainer.Trainer._build_optimizer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._build_optimizer"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_optimizer", "is", "None", ":", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "\n", "", "return", "self", ".", "_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_scheduler": [[93, 98], ["trainer.Trainer._build_optimizer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._build_optimizer"], ["", "@", "property", "\n", "def", "lr_scheduler", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_lr_scheduler", "is", "None", ":", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "# this will initialize self._lr_scheduler", "\n", "", "return", "self", ".", "_lr_scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._build_optimizer": [[99, 117], ["list", "fairseq.optim.lr_scheduler.build_lr_scheduler", "filter", "fairseq.optim.build_optimizer", "trainer.Trainer.model.parameters", "print", "fairseq.optim.MemoryEfficientFP16Optimizer.build_optimizer", "fairseq.optim.FP16Optimizer.build_optimizer", "print", "torch.cuda.get_device_capability", "torch.cuda.get_device_capability"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.__init__.build_lr_scheduler", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "_build_optimizer", "(", "self", ")", ":", "\n", "        ", "params", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", ")", "\n", "if", "self", ".", "args", ".", "fp16", ":", "\n", "            ", "if", "self", ".", "cuda", "and", "torch", ".", "cuda", ".", "get_device_capability", "(", "0", ")", "[", "0", "]", "<", "7", ":", "\n", "                ", "print", "(", "'| WARNING: your device does NOT support faster training with --fp16, '", "\n", "'please switch to FP32 which is likely to be faster'", ")", "\n", "", "if", "self", ".", "args", ".", "memory_efficient_fp16", ":", "\n", "                ", "self", ".", "_optimizer", "=", "optim", ".", "MemoryEfficientFP16Optimizer", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_optimizer", "=", "optim", ".", "FP16Optimizer", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "cuda", "and", "torch", ".", "cuda", ".", "get_device_capability", "(", "0", ")", "[", "0", "]", ">=", "7", ":", "\n", "                ", "print", "(", "'| NOTICE: your device may support faster training with --fp16'", ")", "\n", "", "self", ".", "_optimizer", "=", "optim", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "\n", "# We should initialize the learning rate scheduler immediately after", "\n", "# building the optimizer, so that the initial learning rate is set.", "\n", "", "self", ".", "_lr_scheduler", "=", "lr_scheduler", ".", "build_lr_scheduler", "(", "self", ".", "args", ",", "self", ".", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.save_checkpoint": [[118, 125], ["fairseq.distributed_utils.is_master", "fairseq.utils.save_state", "trainer.Trainer.get_model().state_dict", "trainer.Trainer.get_model"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.save_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_model"], ["", "def", "save_checkpoint", "(", "self", ",", "filename", ",", "extra_state", ")", ":", "\n", "        ", "\"\"\"Save all training state in a checkpoint file.\"\"\"", "\n", "if", "distributed_utils", ".", "is_master", "(", "self", ".", "args", ")", ":", "# only save one checkpoint", "\n", "            ", "extra_state", "[", "'train_meters'", "]", "=", "self", ".", "meters", "\n", "utils", ".", "save_state", "(", "\n", "filename", ",", "self", ".", "args", ",", "self", ".", "get_model", "(", ")", ".", "state_dict", "(", ")", ",", "self", ".", "criterion", ",", "self", ".", "optimizer", ",", "\n", "self", ".", "lr_scheduler", ",", "self", ".", "_num_updates", ",", "self", ".", "_optim_history", ",", "extra_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.load_checkpoint": [[127, 159], ["fairseq.utils.load_model_state", "trainer.Trainer.get_model", "trainer.Trainer._build_optimizer", "trainer.Trainer.optimizer.load_state_dict", "trainer.Trainer.meters.update", "trainer.Trainer.meters.values", "trainer.Trainer.lr_scheduler.load_state_dict", "isinstance", "meter.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_model_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._build_optimizer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["", "", "def", "load_checkpoint", "(", "self", ",", "filename", ",", "reset_optimizer", "=", "False", ",", "reset_lr_scheduler", "=", "False", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load all training state from a checkpoint file.\"\"\"", "\n", "extra_state", ",", "self", ".", "_optim_history", ",", "last_optim_state", "=", "utils", ".", "load_model_state", "(", "\n", "filename", ",", "self", ".", "get_model", "(", ")", ",", "\n", ")", "\n", "if", "last_optim_state", "is", "not", "None", "and", "not", "reset_optimizer", ":", "\n", "# rebuild optimizer after loading model, since params may have changed", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "\n", "\n", "# only reload optimizer and lr_scheduler if they match", "\n", "last_optim", "=", "self", ".", "_optim_history", "[", "-", "1", "]", "\n", "assert", "last_optim", "[", "'criterion_name'", "]", "==", "self", ".", "criterion", ".", "__class__", ".", "__name__", ",", "'criterion does not match; please reset the optimizer (--reset-optimizer)'", "\n", "assert", "last_optim", "[", "'optimizer_name'", "]", "==", "self", ".", "optimizer", ".", "__class__", ".", "__name__", ",", "'optimizer does not match; please reset the optimizer (--reset-optimizer)'", "\n", "\n", "if", "not", "reset_lr_scheduler", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "last_optim", "[", "'lr_scheduler_state'", "]", ")", "\n", "", "self", ".", "optimizer", ".", "load_state_dict", "(", "last_optim_state", ",", "optimizer_overrides", ")", "\n", "\n", "self", ".", "_num_updates", "=", "last_optim", "[", "'num_updates'", "]", "\n", "\n", "", "if", "extra_state", "is", "not", "None", "and", "'train_meters'", "in", "extra_state", ":", "\n", "            ", "self", ".", "meters", ".", "update", "(", "extra_state", "[", "'train_meters'", "]", ")", "\n", "del", "extra_state", "[", "'train_meters'", "]", "\n", "\n", "# reset TimeMeters, since their start times don't make sense anymore", "\n", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "meter", ",", "TimeMeter", ")", ":", "\n", "                    ", "meter", ".", "reset", "(", ")", "\n", "\n", "", "", "", "return", "extra_state", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.train_step": [[160, 288], ["trainer.Trainer._set_seed", "trainer.Trainer.model.train", "trainer.Trainer.criterion.train", "trainer.Trainer.zero_grad", "enumerate", "trainer.Trainer.meters[].update", "trainer.Trainer.task.aggregate_logging_outputs", "trainer.Trainer.task.grad_denom", "trainer.Trainer.meters[].stop", "trainer.Trainer.meters[].start", "trainer.Trainer._prepare_sample", "trainer.Trainer.handle_ooms", "zip", "list", "list", "sum", "all", "len", "print", "trainer.Trainer.zero_grad", "all", "Exception", "trainer.Trainer.optimizer.multiply_grads", "trainer.Trainer.optimizer.clip_grad_norm", "trainer.Trainer.optimizer.step", "trainer.Trainer.lr_scheduler.step_update", "trainer.Trainer.get", "trainer.Trainer.get", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].reset", "trainer.Trainer.meters[].update", "trainer.Trainer._prepare_sample", "trainer.Trainer.task.train_step", "itertools.chain.from_iterable", "itertools.chain.from_iterable", "len", "trainer.Trainer.get", "trainer.Trainer.meters[].update", "print", "trainer.Trainer.zero_grad", "list.append", "list.append", "fairseq.distributed_utils.all_gather_list", "float", "trainer.Trainer.get", "str", "print", "trainer.Trainer.zero_grad", "str", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._set_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.handle_ooms", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step_update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_gather_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "train_step", "(", "self", ",", "samples", ",", "dummy_batch", "=", "False", ")", ":", "\n", "        ", "\"\"\"Do forward, backward and parameter update.\"\"\"", "\n", "self", ".", "_set_seed", "(", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "criterion", ".", "train", "(", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n", "if", "not", "dummy_batch", ":", "\n", "            ", "self", ".", "meters", "[", "'train_wall'", "]", ".", "start", "(", ")", "\n", "\n", "# forward and backward pass", "\n", "", "logging_outputs", ",", "sample_sizes", ",", "ooms", "=", "[", "]", ",", "[", "]", ",", "0", "\n", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "            ", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ")", "\n", "if", "sample", "is", "None", ":", "\n", "# when sample is None, run forward/backward on a dummy batch", "\n", "# and ignore the resulting gradients", "\n", "                ", "sample", "=", "self", ".", "_prepare_sample", "(", "self", ".", "_dummy_batch", ")", "\n", "ignore_grad", "=", "True", "\n", "", "else", ":", "\n", "                ", "ignore_grad", "=", "False", "\n", "\n", "", "try", ":", "\n", "                ", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "# Whenever *samples* contains more than one mini-batch, we", "\n", "# want to accumulate gradients locally and only call", "\n", "# all-reduce in the last backwards pass. Currently the", "\n", "# *need_reduction* flag is only supported by", "\n", "# LegacyDistributedDataParallel.", "\n", "                    ", "if", "i", "<", "len", "(", "samples", ")", "-", "1", ":", "\n", "                        ", "self", ".", "model", ".", "accumulate_grads", "=", "True", "\n", "", "else", ":", "\n", "                        ", "self", ".", "model", ".", "accumulate_grads", "=", "False", "\n", "\n", "# forward and backward", "\n", "", "", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "task", ".", "train_step", "(", "\n", "sample", ",", "self", ".", "model", ",", "self", ".", "criterion", ",", "self", ".", "optimizer", ",", "\n", "ignore_grad", "\n", ")", "\n", "\n", "if", "not", "ignore_grad", ":", "\n", "                    ", "logging_outputs", ".", "append", "(", "logging_output", ")", "\n", "sample_sizes", ".", "append", "(", "sample_size", ")", "\n", "", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                    ", "print", "(", "'| WARNING: ran out of memory, skipping batch'", ")", "\n", "ooms", "+=", "1", "\n", "self", ".", "zero_grad", "(", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "", "", "", "if", "ooms", ">", "0", "and", "self", ".", "_oom_batch", "is", "not", "None", ":", "\n", "            ", "self", ".", "handle_ooms", "(", "ooms", ")", "\n", "\n", "", "if", "dummy_batch", ":", "\n", "            ", "return", "None", "\n", "\n", "# gather logging outputs from all replicas", "\n", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "            ", "logging_outputs", ",", "sample_sizes", ",", "ooms", ",", "prev_norms", "=", "zip", "(", "*", "distributed_utils", ".", "all_gather_list", "(", "\n", "[", "logging_outputs", ",", "sample_sizes", ",", "ooms", ",", "self", ".", "_prev_grad_norm", "]", ",", "\n", ")", ")", "\n", "logging_outputs", "=", "list", "(", "chain", ".", "from_iterable", "(", "logging_outputs", ")", ")", "\n", "sample_sizes", "=", "list", "(", "chain", ".", "from_iterable", "(", "sample_sizes", ")", ")", "\n", "ooms", "=", "sum", "(", "ooms", ")", "\n", "assert", "all", "(", "norm", "==", "prev_norms", "[", "0", "]", "for", "norm", "in", "prev_norms", ")", ",", "'Fatal error: gradients are inconsistent between workers'", "\n", "\n", "", "self", ".", "meters", "[", "'oom'", "]", ".", "update", "(", "ooms", ",", "len", "(", "samples", ")", ")", "\n", "if", "ooms", "==", "self", ".", "args", ".", "distributed_world_size", "*", "len", "(", "samples", ")", ":", "\n", "            ", "print", "(", "'| WARNING: OOM in all workers, skipping update'", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "return", "None", "\n", "\n", "# aggregate logging outputs and sample sizes", "\n", "", "logging_output", "=", "self", ".", "task", ".", "aggregate_logging_outputs", "(", "\n", "logging_outputs", ",", "self", ".", "criterion", "\n", ")", "\n", "sample_size", "=", "self", ".", "task", ".", "grad_denom", "(", "sample_sizes", ",", "self", ".", "criterion", ")", "\n", "\n", "if", "not", "all", "(", "k", "in", "logging_output", "for", "k", "in", "[", "'ntokens'", ",", "'nsentences'", "]", ")", ":", "\n", "            ", "raise", "Exception", "(", "(", "\n", "'Please update the {}.aggregate_logging_outputs() method to '", "\n", "'return ntokens and nsentences'", "\n", ")", ".", "format", "(", "self", ".", "task", ".", "__class__", ".", "__name__", ")", ")", "\n", "\n", "", "try", ":", "\n", "# normalize grads by sample size", "\n", "            ", "self", ".", "optimizer", ".", "multiply_grads", "(", "self", ".", "args", ".", "distributed_world_size", "/", "float", "(", "sample_size", ")", ")", "\n", "\n", "# clip grads", "\n", "grad_norm", "=", "self", ".", "optimizer", ".", "clip_grad_norm", "(", "self", ".", "args", ".", "clip_norm", ")", "\n", "self", ".", "_prev_grad_norm", "=", "grad_norm", "\n", "\n", "# take an optimization step", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "_num_updates", "+=", "1", "\n", "\n", "# update learning rate", "\n", "self", ".", "lr_scheduler", ".", "step_update", "(", "self", ".", "_num_updates", ")", "\n", "\n", "# update meters", "\n", "ntokens", "=", "logging_output", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "nsentences", "=", "logging_output", ".", "get", "(", "'nsentences'", ",", "0", ")", "\n", "self", ".", "meters", "[", "'wps'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'ups'", "]", ".", "update", "(", "1.", ")", "\n", "self", ".", "meters", "[", "'wpb'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'bsz'", "]", ".", "update", "(", "nsentences", ")", "\n", "self", ".", "meters", "[", "'gnorm'", "]", ".", "update", "(", "grad_norm", ")", "\n", "self", ".", "meters", "[", "'clip'", "]", ".", "update", "(", "\n", "1.", "if", "grad_norm", ">", "self", ".", "args", ".", "clip_norm", "and", "self", ".", "args", ".", "clip_norm", ">", "0", "else", "0.", "\n", ")", "\n", "self", ".", "meters", "[", "'train_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'loss'", ",", "0", ")", ",", "sample_size", ")", "\n", "if", "'nll_loss'", "in", "logging_output", ":", "\n", "                ", "self", ".", "meters", "[", "'train_nll_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'nll_loss'", ",", "0", ")", ",", "ntokens", ")", "\n", "", "", "except", "OverflowError", "as", "e", ":", "\n", "            ", "print", "(", "'| WARNING: overflow detected, '", "+", "str", "(", "e", ")", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "logging_output", "=", "None", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "meters", "[", "'loss_scale'", "]", ".", "reset", "(", ")", "\n", "self", ".", "meters", "[", "'loss_scale'", "]", ".", "update", "(", "self", ".", "optimizer", ".", "scaler", ".", "loss_scale", ")", "\n", "\n", "", "self", ".", "meters", "[", "'train_wall'", "]", ".", "stop", "(", ")", "\n", "\n", "return", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.valid_step": [[289, 347], ["trainer.Trainer.task.aggregate_logging_outputs", "trainer.Trainer.task.grad_denom", "list.get", "trainer.Trainer.meters[].update", "torch.no_grad", "trainer.Trainer.model.eval", "trainer.Trainer.criterion.eval", "trainer.Trainer._prepare_sample", "zip", "list", "list", "list.get", "trainer.Trainer.meters[].update", "trainer.Trainer._prepare_sample", "trainer.Trainer.task.valid_step", "list.get", "fairseq.distributed_utils.all_gather_list", "print", "trainer.Trainer.model.parameters", "trainer.Trainer.valid_step", "str", "torch.cuda.empty_cache"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.valid_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.all_gather_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.valid_step"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "raise_oom", "=", "False", ")", ":", "\n", "        ", "\"\"\"Do forward pass in evaluation mode.\"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "criterion", ".", "eval", "(", ")", "\n", "\n", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ")", "\n", "if", "sample", "is", "None", ":", "\n", "                ", "sample", "=", "self", ".", "_prepare_sample", "(", "self", ".", "_dummy_batch", ")", "\n", "ignore_results", "=", "True", "\n", "", "else", ":", "\n", "                ", "ignore_results", "=", "False", "\n", "\n", "", "try", ":", "\n", "                ", "_loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "task", ".", "valid_step", "(", "\n", "sample", ",", "self", ".", "model", ",", "self", ".", "criterion", "\n", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", "and", "not", "raise_oom", ":", "\n", "                    ", "print", "(", "'| WARNING: ran out of memory, retrying batch'", ")", "\n", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "                        ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                            ", "del", "p", ".", "grad", "# free some memory", "\n", "", "", "if", "self", ".", "cuda", ":", "\n", "                        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "return", "self", ".", "valid_step", "(", "sample", ",", "raise_oom", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "", "", "if", "ignore_results", ":", "\n", "                ", "logging_output", ",", "sample_size", "=", "{", "}", ",", "0", "\n", "\n", "# gather logging outputs from all replicas", "\n", "", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "            ", "logging_output", ",", "sample_size", "=", "zip", "(", "*", "distributed_utils", ".", "all_gather_list", "(", "\n", "[", "logging_output", ",", "sample_size", "]", ",", "\n", ")", ")", "\n", "logging_output", "=", "list", "(", "logging_output", ")", "\n", "sample_size", "=", "list", "(", "sample_size", ")", "\n", "", "else", ":", "\n", "            ", "logging_output", "=", "[", "logging_output", "]", "\n", "sample_size", "=", "[", "sample_size", "]", "\n", "\n", "# aggregate logging outputs and sample sizes", "\n", "", "logging_output", "=", "self", ".", "task", ".", "aggregate_logging_outputs", "(", "\n", "logging_output", ",", "self", ".", "criterion", "\n", ")", "\n", "sample_size", "=", "self", ".", "task", ".", "grad_denom", "(", "\n", "sample_size", ",", "self", ".", "criterion", "\n", ")", "\n", "\n", "# update meters for validation", "\n", "ntokens", "=", "logging_output", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "self", ".", "meters", "[", "'valid_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'loss'", ",", "0", ")", ",", "sample_size", ")", "\n", "if", "'nll_loss'", "in", "logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'valid_nll_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'nll_loss'", ",", "0", ")", ",", "ntokens", ")", "\n", "\n", "", "return", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.dummy_train_step": [[348, 352], ["trainer.Trainer.train_step", "trainer.Trainer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "dummy_train_step", "(", "self", ",", "dummy_batch", ")", ":", "\n", "        ", "\"\"\"Dummy training step for warming caching allocator.\"\"\"", "\n", "self", ".", "train_step", "(", "dummy_batch", ",", "dummy_batch", "=", "True", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.handle_ooms": [[353, 361], ["range", "trainer.Trainer.train_step"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step"], ["", "def", "handle_ooms", "(", "self", ",", "number_of_ooms", ")", ":", "\n", "        ", "\"\"\"\n        c10d accumulates/syncs gradients between gpus during backward pass.\n        In case of OOMs, gpus may fail to sync, so we manually iterate\n        extra to make sure each gpu makes same number of iterations.\n        \"\"\"", "\n", "for", "_", "in", "range", "(", "number_of_ooms", ")", ":", "\n", "            ", "self", ".", "train_step", "(", "[", "self", ".", "_oom_batch", "]", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.zero_grad": [[362, 364], ["trainer.Trainer.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step": [[365, 368], ["trainer.Trainer.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step"], ["", "def", "lr_step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Adjust the learning rate based on the validation loss.\"\"\"", "\n", "return", "self", ".", "lr_scheduler", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step_update": [[369, 372], ["trainer.Trainer.lr_scheduler.step_update"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step_update"], ["", "def", "lr_step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "return", "self", ".", "lr_scheduler", ".", "step_update", "(", "num_updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_lr": [[373, 376], ["trainer.Trainer.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the current learning rate.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_model": [[377, 380], ["None"], "methods", ["None"], ["", "def", "get_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the (non-wrapped) model instance.\"\"\"", "\n", "return", "self", ".", "_model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter": [[381, 386], ["None"], "methods", ["None"], ["", "def", "get_meter", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Get a specific meter by name.\"\"\"", "\n", "if", "name", "not", "in", "self", ".", "meters", ":", "\n", "            ", "return", "None", "\n", "", "return", "self", ".", "meters", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates": [[387, 390], ["None"], "methods", ["None"], ["", "def", "get_num_updates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the number of parameters updates.\"\"\"", "\n", "return", "self", ".", "_num_updates", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._prepare_sample": [[391, 397], ["fairseq.utils.move_to_cuda", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda"], ["", "def", "_prepare_sample", "(", "self", ",", "sample", ")", ":", "\n", "        ", "if", "sample", "is", "None", "or", "len", "(", "sample", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "if", "self", ".", "cuda", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer._set_seed": [[398, 405], ["torch.manual_seed", "trainer.Trainer.get_num_updates", "torch.cuda.manual_seed"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates"], ["", "def", "_set_seed", "(", "self", ")", ":", "\n", "# Set seed based on args.seed and the update number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "        ", "seed", "=", "self", ".", "args", ".", "seed", "+", "self", ".", "get_num_updates", "(", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "self", ".", "cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.pdb.MultiprocessingPdb.__init__": [[31, 33], ["pdb.Pdb.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pdb", ".", "Pdb", ".", "__init__", "(", "self", ",", "nosigint", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.pdb.MultiprocessingPdb._cmdloop": [[34, 45], ["pdb.MultiprocessingPdb.cmdloop", "os.fdopen"], "methods", ["None"], ["", "def", "_cmdloop", "(", "self", ")", ":", "\n", "        ", "stdin_bak", "=", "sys", ".", "stdin", "\n", "with", "_stdin_lock", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "_stdin_fd", "is", "not", "None", ":", "\n", "                    ", "if", "not", "_stdin", "[", "0", "]", ":", "\n", "                        ", "_stdin", "[", "0", "]", "=", "os", ".", "fdopen", "(", "_stdin_fd", ")", "\n", "", "sys", ".", "stdin", "=", "_stdin", "[", "0", "]", "\n", "", "self", ".", "cmdloop", "(", ")", "\n", "", "finally", ":", "\n", "                ", "sys", ".", "stdin", "=", "stdin_bak", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.pdb.set_trace": [[47, 50], ["pdb.MultiprocessingPdb", "MultiprocessingPdb.set_trace", "sys._getframe"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.pdb.set_trace"], ["", "", "", "", "def", "set_trace", "(", ")", ":", "\n", "    ", "pdb", "=", "MultiprocessingPdb", "(", ")", "\n", "pdb", ".", "set_trace", "(", "sys", ".", "_getframe", "(", ")", ".", "f_back", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.Binarizer.binarize": [[26, 56], ["collections.Counter", "open", "f.seek", "binarizer.safe_readline", "sum", "collections.Counter.update", "dict.encode_line", "len", "consumer", "f.readline", "collections.Counter.values", "f.tell"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.safe_readline", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line"], ["    ", "@", "staticmethod", "\n", "def", "binarize", "(", "filename", ",", "dict", ",", "consumer", ",", "tokenize", "=", "tokenize_line", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ",", "\n", "offset", "=", "0", ",", "end", "=", "-", "1", ")", ":", "\n", "        ", "nseq", ",", "ntok", "=", "0", ",", "0", "\n", "replaced", "=", "Counter", "(", ")", "\n", "\n", "def", "replaced_consumer", "(", "word", ",", "idx", ")", ":", "\n", "            ", "if", "idx", "==", "dict", ".", "unk_index", "and", "word", "!=", "dict", ".", "unk_word", ":", "\n", "                ", "replaced", ".", "update", "(", "[", "word", "]", ")", "\n", "\n", "", "", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "seek", "(", "offset", ")", "\n", "# next(f) breaks f.tell(), hence readline() must be used", "\n", "line", "=", "safe_readline", "(", "f", ")", "\n", "while", "line", ":", "\n", "                ", "if", "end", ">", "0", "and", "f", ".", "tell", "(", ")", ">", "end", ":", "\n", "                    ", "break", "\n", "", "ids", "=", "dict", ".", "encode_line", "(", "\n", "line", "=", "line", ",", "\n", "line_tokenizer", "=", "tokenize", ",", "\n", "add_if_not_exist", "=", "False", ",", "\n", "consumer", "=", "replaced_consumer", ",", "\n", "append_eos", "=", "append_eos", ",", "\n", "reverse_order", "=", "reverse_order", ",", "\n", ")", "\n", "nseq", "+=", "1", "\n", "ntok", "+=", "len", "(", "ids", ")", "\n", "consumer", "(", "ids", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "{", "'nseq'", ":", "nseq", ",", "'nunk'", ":", "sum", "(", "replaced", ".", "values", "(", ")", ")", ",", "'ntok'", ":", "ntok", ",", "'replaced'", ":", "replaced", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.Binarizer.find_offsets": [[57, 68], ["open", "range", "os.fstat", "f.seek", "binarizer.safe_readline", "f.tell", "f.fileno", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.safe_readline", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "@", "staticmethod", "\n", "def", "find_offsets", "(", "filename", ",", "num_chunks", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "size", "=", "os", ".", "fstat", "(", "f", ".", "fileno", "(", ")", ")", ".", "st_size", "\n", "chunk_size", "=", "size", "//", "num_chunks", "\n", "offsets", "=", "[", "0", "for", "_", "in", "range", "(", "num_chunks", "+", "1", ")", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "num_chunks", ")", ":", "\n", "                ", "f", ".", "seek", "(", "chunk_size", "*", "i", ")", "\n", "safe_readline", "(", "f", ")", "\n", "offsets", "[", "i", "]", "=", "f", ".", "tell", "(", ")", "\n", "", "return", "offsets", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.safe_readline": [[14, 22], ["f.tell", "f.readline", "f.seek"], "function", ["None"], ["def", "safe_readline", "(", "f", ")", ":", "\n", "    ", "pos", "=", "f", ".", "tell", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "f", ".", "readline", "(", ")", "\n", "", "except", "UnicodeDecodeError", ":", "\n", "            ", "pos", "-=", "1", "\n", "f", ".", "seek", "(", "pos", ")", "# search where this character begins", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.AverageMeter.__init__": [[13, 15], ["meters.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.AverageMeter.reset": [[16, 21], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.AverageMeter.update": [[22, 27], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.TimeMeter.__init__": [[31, 33], ["meters.TimeMeter.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ",", "init", "=", "0", ")", ":", "\n", "        ", "self", ".", "reset", "(", "init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.TimeMeter.reset": [[34, 38], ["time.time"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "init", "=", "0", ")", ":", "\n", "        ", "self", ".", "init", "=", "init", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "n", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.TimeMeter.update": [[39, 41], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", "=", "1", ")", ":", "\n", "        ", "self", ".", "n", "+=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.TimeMeter.avg": [[42, 45], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n", "/", "self", ".", "elapsed_time", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.TimeMeter.elapsed_time": [[46, 49], ["time.time"], "methods", ["None"], ["", "@", "property", "\n", "def", "elapsed_time", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "init", "+", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.__init__": [[53, 55], ["meters.StopwatchMeter.reset"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start": [[56, 58], ["time.time"], "methods", ["None"], ["", "def", "start", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop": [[59, 65], ["time.time"], "methods", ["None"], ["", "def", "stop", "(", "self", ",", "n", "=", "1", ")", ":", "\n", "        ", "if", "self", ".", "start_time", "is", "not", "None", ":", "\n", "            ", "delta", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "self", ".", "sum", "+=", "delta", "\n", "self", ".", "n", "+=", "n", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset": [[66, 70], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "sum", "=", "0", "\n", "self", ".", "n", "=", "0", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.avg": [[71, 74], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sum", "/", "self", ".", "n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search.__init__": [[15, 23], ["tgt_dict.pad", "tgt_dict.unk", "tgt_dict.eos", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ")", ":", "\n", "        ", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "tgt_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "tgt_dict", ".", "eos", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "tgt_dict", ")", "\n", "self", ".", "scores_buf", "=", "None", "\n", "self", ".", "indices_buf", "=", "None", "\n", "self", ".", "beams_buf", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search._init_buffers": [[24, 29], ["t.new", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor"], "methods", ["None"], ["", "def", "_init_buffers", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "self", ".", "scores_buf", "is", "None", ":", "\n", "            ", "self", ".", "scores_buf", "=", "t", ".", "new", "(", ")", "\n", "self", ".", "indices_buf", "=", "torch", ".", "LongTensor", "(", ")", ".", "to", "(", "device", "=", "t", ".", "device", ")", "\n", "self", ".", "beams_buf", "=", "torch", ".", "LongTensor", "(", ")", ".", "to", "(", "device", "=", "t", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search.step": [[30, 51], ["None"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\"Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search.set_src_lengths": [[52, 54], ["None"], "methods", ["None"], ["", "def", "set_src_lengths", "(", "self", ",", "src_lengths", ")", ":", "\n", "        ", "self", ".", "src_lengths", "=", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.BeamSearch.__init__": [[58, 60], ["search.Search.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.BeamSearch.step": [[61, 86], ["search.Search._init_buffers", "lprobs[].contiguous.size", "torch.topk", "torch.div", "search.BeamSearch.indices_buf.fmod_", "lprobs[].contiguous", "lprobs[].contiguous.add_", "lprobs[].contiguous.view", "scores[].unsqueeze", "min", "lprobs[].contiguous.view().size", "lprobs[].contiguous.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "# at the first step all hypotheses are equally likely, so use", "\n", "# only the first beam", "\n", "            ", "lprobs", "=", "lprobs", "[", ":", ",", ":", ":", "beam_size", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "# make probs contain cumulative scores for each hypothesis", "\n", "            ", "lprobs", ".", "add_", "(", "scores", "[", ":", ",", ":", ",", "step", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "\n", "", "torch", ".", "topk", "(", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "\n", "k", "=", "min", "(", "\n", "# Take the best 2 x beam_size predictions. We'll choose the first", "\n", "# beam_size of these which don't predict eos to continue with.", "\n", "beam_size", "*", "2", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ")", ".", "size", "(", "1", ")", "-", "1", ",", "# -1 so we never select pad", "\n", ")", ",", "\n", "out", "=", "(", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ")", ",", "\n", ")", "\n", "torch", ".", "div", "(", "self", ".", "indices_buf", ",", "vocab_size", ",", "out", "=", "self", ".", "beams_buf", ")", "\n", "self", ".", "indices_buf", ".", "fmod_", "(", "vocab_size", ")", "\n", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.LengthConstrainedBeamSearch.__init__": [[90, 97], ["search.Search.__init__", "search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ",", "min_len_a", ",", "min_len_b", ",", "max_len_a", ",", "max_len_b", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "min_len_a", "=", "min_len_a", "\n", "self", ".", "min_len_b", "=", "min_len_b", "\n", "self", ".", "max_len_a", "=", "max_len_a", "\n", "self", ".", "max_len_b", "=", "max_len_b", "\n", "self", ".", "beam", "=", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.LengthConstrainedBeamSearch.step": [[98, 105], ["search.LengthConstrainedBeamSearch.beam.step"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "min_lens", "=", "self", ".", "min_len_a", "*", "self", ".", "src_lengths", "+", "self", ".", "min_len_b", "\n", "max_lens", "=", "self", ".", "max_len_a", "*", "self", ".", "src_lengths", "+", "self", ".", "max_len_b", "\n", "lprobs", "[", "step", "<", "min_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "lprobs", "[", "step", "==", "max_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "0", "\n", "lprobs", "[", "step", ">", "max_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "return", "self", ".", "beam", ".", "step", "(", "step", ",", "lprobs", ",", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.DiverseBeamSearch.__init__": [[117, 123], ["search.Search.__init__", "search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "tgt_dict", ",", "num_groups", ",", "diversity_strength", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "num_groups", "=", "num_groups", "\n", "self", ".", "diversity_strength", "=", "-", "diversity_strength", "\n", "self", ".", "diversity_buf", "=", "None", "\n", "self", ".", "beam", "=", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.DiverseBeamSearch.step": [[124, 167], ["search.Search._init_buffers", "lprobs.size", "torch.zeros", "range", "torch.stack().view", "torch.stack().view", "torch.stack().view", "ValueError", "lprobs.new", "lprobs[].size", "search.DiverseBeamSearch.beam.step", "beams_buf.mul_().add_", "scores_G.append", "indices_G.append", "beams_G.append", "search.DiverseBeamSearch.diversity_buf.scatter_add_", "torch.add", "lprobs_g.contiguous.contiguous.contiguous", "scores_buf.clone", "indices_buf.clone", "beams_buf.clone", "search.DiverseBeamSearch.diversity_buf.new_ones", "torch.stack", "torch.stack", "torch.stack", "search.DiverseBeamSearch.diversity_buf.unsqueeze", "beams_buf.mul_", "indices_buf.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "if", "beam_size", "%", "self", ".", "num_groups", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'DiverseBeamSearch requires --beam to be divisible by the number of groups'", "\n", ")", "\n", "\n", "# initialize diversity penalty", "\n", "", "if", "self", ".", "diversity_buf", "is", "None", ":", "\n", "            ", "self", ".", "diversity_buf", "=", "lprobs", ".", "new", "(", ")", "\n", "", "torch", ".", "zeros", "(", "lprobs", "[", ":", ",", "0", ",", ":", "]", ".", "size", "(", ")", ",", "out", "=", "self", ".", "diversity_buf", ")", "\n", "\n", "scores_G", ",", "indices_G", ",", "beams_G", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "g", "in", "range", "(", "self", ".", "num_groups", ")", ":", "\n", "            ", "lprobs_g", "=", "lprobs", "[", ":", ",", "g", ":", ":", "self", ".", "num_groups", ",", ":", "]", "\n", "scores_g", "=", "scores", "[", ":", ",", "g", ":", ":", "self", ".", "num_groups", ",", ":", "]", "if", "step", ">", "0", "else", "None", "\n", "\n", "# apply diversity penalty", "\n", "if", "g", ">", "0", ":", "\n", "                ", "lprobs_g", "=", "torch", ".", "add", "(", "lprobs_g", ",", "self", ".", "diversity_strength", ",", "self", ".", "diversity_buf", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "lprobs_g", "=", "lprobs_g", ".", "contiguous", "(", ")", "\n", "\n", "", "scores_buf", ",", "indices_buf", ",", "beams_buf", "=", "self", ".", "beam", ".", "step", "(", "step", ",", "lprobs_g", ",", "scores_g", ")", "\n", "beams_buf", ".", "mul_", "(", "self", ".", "num_groups", ")", ".", "add_", "(", "g", ")", "\n", "\n", "scores_G", ".", "append", "(", "scores_buf", ".", "clone", "(", ")", ")", "\n", "indices_G", ".", "append", "(", "indices_buf", ".", "clone", "(", ")", ")", "\n", "beams_G", ".", "append", "(", "beams_buf", ".", "clone", "(", ")", ")", "\n", "\n", "# update diversity penalty", "\n", "self", ".", "diversity_buf", ".", "scatter_add_", "(", "\n", "1", ",", "\n", "indices_buf", ",", "\n", "self", ".", "diversity_buf", ".", "new_ones", "(", "indices_buf", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "# interleave results from different groups", "\n", "", "self", ".", "scores_buf", "=", "torch", ".", "stack", "(", "scores_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "scores_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "self", ".", "indices_buf", "=", "torch", ".", "stack", "(", "indices_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "indices_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "self", ".", "beams_buf", "=", "torch", ".", "stack", "(", "beams_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "beams_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Sampling.__init__": [[171, 175], ["search.Search.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ",", "sampling_topk", "=", "-", "1", ",", "sampling_temperature", "=", "1.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "sampling_topk", "=", "sampling_topk", "\n", "self", ".", "sampling_temperature", "=", "sampling_temperature", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Sampling.step": [[176, 252], ["search.Search._init_buffers", "lprobs[].contiguous.size", "lprobs_nopad.div_.div_.exp_", "torch.gather", "search.Sampling.scores_buf.log_().view", "search.Sampling.indices_buf.add_", "lprobs[].contiguous", "lprobs_nopad.div_.div_.topk", "lprobs_nopad.div_.div_.div_", "torch.multinomial().view", "torch.multinomial().view", "probs_nopad.expand.expand.expand", "torch.gather().squeeze", "search.Sampling.indices_buf.new_zeros", "torch.arange().repeat", "search.Sampling.scores_buf.add_", "search.Sampling.indices_buf.unsqueeze", "search.Sampling.scores_buf.log_", "torch.gather", "torch.multinomial", "torch.multinomial", "torch.gather", "torch.arange", "probs_nopad.expand.expand.view", "probs_nopad.expand.expand.view", "topk_indices.expand", "search.Sampling.indices_buf.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "# at the first step all hypotheses are equally likely, so use", "\n", "# only the first beam", "\n", "            ", "lprobs", "=", "lprobs", "[", ":", ",", ":", ":", "beam_size", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "# we exclude the first two vocab items, one of which is pad", "\n", "", "assert", "self", ".", "pad", "==", "1", ",", "'sampling assumes the first two symbols can be ignored'", "\n", "lprobs_nopad", "=", "lprobs", "[", ":", ",", ":", ",", "2", ":", "]", "\n", "\n", "# only sample from top-k candidates", "\n", "if", "self", ".", "sampling_topk", ">", "0", ":", "\n", "            ", "lprobs_nopad", ",", "topk_indices", "=", "lprobs_nopad", ".", "topk", "(", "self", ".", "sampling_topk", ")", "\n", "\n", "# sampling temperature", "\n", "", "if", "self", ".", "sampling_temperature", "!=", "1.", ":", "\n", "            ", "lprobs_nopad", "=", "lprobs_nopad", ".", "div_", "(", "self", ".", "sampling_temperature", ")", "\n", "\n", "# sample", "\n", "", "probs_nopad", "=", "lprobs_nopad", ".", "exp_", "(", ")", "\n", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "multinomial", "(", "\n", "probs_nopad", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "\n", "beam_size", ",", "\n", "replacement", "=", "True", ",", "\n", "out", "=", "self", ".", "indices_buf", ",", "\n", ")", ".", "view", "(", "bsz", ",", "beam_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "multinomial", "(", "\n", "probs_nopad", ".", "view", "(", "bsz", "*", "beam_size", ",", "-", "1", ")", ",", "\n", "1", ",", "\n", "replacement", "=", "True", ",", "\n", "out", "=", "self", ".", "indices_buf", ",", "\n", ")", ".", "view", "(", "bsz", ",", "beam_size", ")", "\n", "\n", "", "if", "step", "==", "0", ":", "\n", "# expand to beam size", "\n", "            ", "probs_nopad", "=", "probs_nopad", ".", "expand", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "\n", "\n", "# gather scores", "\n", "", "torch", ".", "gather", "(", "\n", "probs_nopad", ",", "\n", "dim", "=", "2", ",", "\n", "index", "=", "self", ".", "indices_buf", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "out", "=", "self", ".", "scores_buf", ",", "\n", ")", "\n", "self", ".", "scores_buf", "=", "self", ".", "scores_buf", ".", "log_", "(", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "\n", "# remap indices if using top-k sampling", "\n", "if", "self", ".", "sampling_topk", ">", "0", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "gather", "(", "\n", "topk_indices", ".", "expand", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", ",", "\n", "dim", "=", "2", ",", "\n", "index", "=", "self", ".", "indices_buf", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", ")", ".", "squeeze", "(", "2", ")", "\n", "\n", "# remap indices since we excluded the first two vocab items", "\n", "", "self", ".", "indices_buf", ".", "add_", "(", "2", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "beams_buf", "=", "self", ".", "indices_buf", ".", "new_zeros", "(", "bsz", ",", "beam_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "beams_buf", "=", "torch", ".", "arange", "(", "0", ",", "beam_size", ",", "out", "=", "self", ".", "beams_buf", ")", ".", "repeat", "(", "bsz", ",", "1", ")", "\n", "# make scores cumulative", "\n", "self", ".", "scores_buf", ".", "add_", "(", "\n", "torch", ".", "gather", "(", "\n", "scores", "[", ":", ",", ":", ",", "step", "-", "1", "]", ",", "\n", "dim", "=", "1", ",", "\n", "index", "=", "self", ".", "beams_buf", ",", "\n", ")", "\n", ")", "\n", "\n", "", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.SequenceCopyGenerator.__init__": [[27, 109], ["tgt_dict.pad", "tgt_dict.unk", "tgt_dict.eos", "len", "min", "fairseq.search.Sampling", "fairseq.search.DiverseBeamSearch", "fairseq.search.LengthConstrainedBeamSearch", "fairseq.search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "tgt_dict", ",", "\n", "beam_size", "=", "1", ",", "\n", "max_len_a", "=", "0", ",", "\n", "max_len_b", "=", "200", ",", "\n", "min_len", "=", "1", ",", "\n", "stop_early", "=", "True", ",", "\n", "normalize_scores", "=", "True", ",", "\n", "len_penalty", "=", "1.", ",", "\n", "unk_penalty", "=", "0.", ",", "\n", "retain_dropout", "=", "False", ",", "\n", "sampling", "=", "False", ",", "\n", "sampling_topk", "=", "-", "1", ",", "\n", "sampling_temperature", "=", "1.", ",", "\n", "diverse_beam_groups", "=", "-", "1", ",", "\n", "diverse_beam_strength", "=", "0.5", ",", "\n", "match_source_len", "=", "False", ",", "\n", "no_repeat_ngram_size", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generates translations of a given source sentence.\n\n        Args:\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            stop_early (bool, optional): stop generation immediately after we\n                finalize beam_size hypotheses, even though longer hypotheses\n                might have better normalized scores (default: True)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            sampling (bool, optional): sample outputs instead of beam search\n                (default: False)\n            sampling_topk (int, optional): only sample among the top-k choices\n                at each step (default: -1)\n            sampling_temperature (float, optional): temperature for sampling,\n                where values >1.0 produces more uniform sampling and values\n                <1.0 produces sharper sampling (default: 1.0)\n            diverse_beam_groups/strength (float, optional): parameters for\n                Diverse Beam Search sampling\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"", "\n", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "tgt_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "tgt_dict", ".", "eos", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "tgt_dict", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "# the max beam size is the dictionary size - 1, since we never select pad", "\n", "self", ".", "beam_size", "=", "min", "(", "beam_size", ",", "self", ".", "vocab_size", "-", "1", ")", "\n", "self", ".", "max_len_a", "=", "max_len_a", "\n", "self", ".", "max_len_b", "=", "max_len_b", "\n", "self", ".", "min_len", "=", "min_len", "\n", "self", ".", "stop_early", "=", "stop_early", "\n", "self", ".", "normalize_scores", "=", "normalize_scores", "\n", "self", ".", "len_penalty", "=", "len_penalty", "\n", "self", ".", "unk_penalty", "=", "unk_penalty", "\n", "self", ".", "retain_dropout", "=", "retain_dropout", "\n", "self", ".", "match_source_len", "=", "match_source_len", "\n", "self", ".", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", "\n", "\n", "assert", "sampling_topk", "<", "0", "or", "sampling", ",", "'--sampling-topk requires --sampling'", "\n", "\n", "if", "sampling", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "Sampling", "(", "tgt_dict", ",", "sampling_topk", ",", "sampling_temperature", ")", "\n", "", "elif", "diverse_beam_groups", ">", "0", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "DiverseBeamSearch", "(", "tgt_dict", ",", "diverse_beam_groups", ",", "diverse_beam_strength", ")", "\n", "", "elif", "match_source_len", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "LengthConstrainedBeamSearch", "(", "\n", "tgt_dict", ",", "min_len_a", "=", "1", ",", "min_len_b", "=", "0", ",", "max_len_a", "=", "1", ",", "max_len_b", "=", "0", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.SequenceCopyGenerator.generate": [[110, 584], ["torch.no_grad", "sequence_copygenerator.CopyEnsembleModel", "src_tokens.size", "sequence_copygenerator.CopyEnsembleModel.forward_encoder", "torch.arange().view().repeat().view", "new_order.to().long.to().long.to().long", "sequence_copygenerator.CopyEnsembleModel.reorder_encoder_out", "src_tokens.new().float().fill_", "[].view.clone", "src_tokens.new().fill_", "[].view.clone", "torch.arange().type_as", "range", "range", "CopyEnsembleModel.eval", "src_lengths.max().item", "min", "[].view.index_select", "set", "enumerate", "sequence_copygenerator.CopyEnsembleModel.forward_decoder", "[].view.type_as", "scores_buf.type_as.type_as.type_as", "sequence_copygenerator.SequenceCopyGenerator.generate.buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_decoder"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "None", ",", "\n", "bos_token", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"Generate a batch of translations.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n        \"\"\"", "\n", "model", "=", "CopyEnsembleModel", "(", "models", ")", "\n", "if", "not", "self", ".", "retain_dropout", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "\n", "# model.forward normally channels prev_output_tokens into the decoder", "\n", "# separately, but SequenceGenerator directly calls model.encoder", "\n", "", "encoder_input", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "sample", "[", "'net_input'", "]", ".", "items", "(", ")", "\n", "if", "k", "!=", "'prev_output_tokens'", "\n", "}", "\n", "\n", "src_tokens", "=", "encoder_input", "[", "'src_tokens'", "]", "\n", "src_lengths", "=", "(", "src_tokens", ".", "ne", "(", "self", ".", "eos", ")", "&", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "bsz", ",", "src_len", "=", "src_tokens", ".", "size", "(", ")", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "\n", "if", "self", ".", "match_source_len", ":", "\n", "            ", "max_len", "=", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "max_len", "=", "min", "(", "\n", "int", "(", "self", ".", "max_len_a", "*", "src_len", "+", "self", ".", "max_len_b", ")", ",", "\n", "# exclude the EOS marker", "\n", "model", ".", "max_decoder_positions", "(", ")", "-", "1", ",", "\n", ")", "\n", "\n", "# compute the encoder output for each beam", "\n", "", "encoder_outs", "=", "model", ".", "forward_encoder", "(", "encoder_input", ")", "\n", "new_order", "=", "torch", ".", "arange", "(", "bsz", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "beam_size", ")", ".", "view", "(", "-", "1", ")", "\n", "new_order", "=", "new_order", ".", "to", "(", "src_tokens", ".", "device", ")", ".", "long", "(", ")", "\n", "encoder_outs", "=", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "new_order", ")", "\n", "\n", "# initialize buffers", "\n", "scores", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "1", ")", ".", "float", "(", ")", ".", "fill_", "(", "0", ")", "\n", "scores_buf", "=", "scores", ".", "clone", "(", ")", "\n", "tokens", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "2", ")", ".", "fill_", "(", "self", ".", "pad", ")", "\n", "tokens_buf", "=", "tokens", ".", "clone", "(", ")", "\n", "tokens", "[", ":", ",", "0", "]", "=", "bos_token", "or", "self", ".", "eos", "\n", "attn", ",", "attn_buf", "=", "None", ",", "None", "\n", "\"\"\"\n        MODIFIED: Also record copy scores.\n        \"\"\"", "\n", "copy_scores", ",", "copy_scores_buf", "=", "None", ",", "None", "\n", "nonpad_idxs", "=", "None", "\n", "\n", "# list of completed sentences", "\n", "finalized", "=", "[", "[", "]", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "finished", "=", "[", "False", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "worst_finalized", "=", "[", "{", "'idx'", ":", "None", ",", "'score'", ":", "-", "math", ".", "inf", "}", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "num_remaining_sent", "=", "bsz", "\n", "\n", "# number of candidate hypos per step", "\n", "cand_size", "=", "2", "*", "beam_size", "# 2 x beam size in case half are EOS", "\n", "\n", "# offset arrays for converting between different indexing schemes", "\n", "bbsz_offsets", "=", "(", "torch", ".", "arange", "(", "0", ",", "bsz", ")", "*", "beam_size", ")", ".", "unsqueeze", "(", "1", ")", ".", "type_as", "(", "tokens", ")", "\n", "cand_offsets", "=", "torch", ".", "arange", "(", "0", ",", "cand_size", ")", ".", "type_as", "(", "tokens", ")", "\n", "\n", "# helper function for allocating buffers on the fly", "\n", "buffers", "=", "{", "}", "\n", "\n", "def", "buffer", "(", "name", ",", "type_of", "=", "tokens", ")", ":", "# noqa", "\n", "            ", "if", "name", "not", "in", "buffers", ":", "\n", "                ", "buffers", "[", "name", "]", "=", "type_of", ".", "new", "(", ")", "\n", "", "return", "buffers", "[", "name", "]", "\n", "\n", "", "def", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"", "\n", "assert", "len", "(", "finalized", "[", "sent", "]", ")", "<=", "beam_size", "\n", "if", "len", "(", "finalized", "[", "sent", "]", ")", "==", "beam_size", ":", "\n", "                ", "if", "self", ".", "stop_early", "or", "step", "==", "max_len", "or", "unfinalized_scores", "is", "None", ":", "\n", "                    ", "return", "True", "\n", "# stop if the best unfinalized score is worse than the worst", "\n", "# finalized one", "\n", "", "best_unfinalized_score", "=", "unfinalized_scores", "[", "sent", "]", ".", "max", "(", ")", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                    ", "best_unfinalized_score", "/=", "max_len", "**", "self", ".", "len_penalty", "\n", "", "if", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ">=", "best_unfinalized_score", ":", "\n", "                    ", "return", "True", "\n", "", "", "return", "False", "\n", "\n", "", "def", "finalize_hypos", "(", "step", ",", "bbsz_idx", ",", "eos_scores", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n                unfinalized_scores: A vector containing scores for all\n                    unfinalized hypotheses\n            \"\"\"", "\n", "assert", "bbsz_idx", ".", "numel", "(", ")", "==", "eos_scores", ".", "numel", "(", ")", "\n", "\n", "# clone relevant token and attention tensors", "\n", "tokens_clone", "=", "tokens", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "\n", "tokens_clone", "=", "tokens_clone", "[", ":", ",", "1", ":", "step", "+", "2", "]", "# skip the first index, which is EOS", "\n", "tokens_clone", "[", ":", ",", "step", "]", "=", "self", ".", "eos", "\n", "attn_clone", "=", "attn", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", ",", "1", ":", "step", "+", "2", "]", "if", "attn", "is", "not", "None", "else", "None", "\n", "\"\"\"\n            MODIFIED: record copy scores analogously to attention scores\n            \"\"\"", "\n", "copy_scores_clone", "=", "copy_scores", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", ",", "1", ":", "step", "+", "2", "]", "if", "copy_scores", "is", "not", "None", "else", "None", "\n", "if", "attn_clone", "is", "not", "None", "and", "copy_scores_clone", "is", "not", "None", ":", "\n", "                ", "assert", "copy_scores_clone", ".", "size", "(", ")", "==", "attn_clone", ".", "size", "(", ")", ",", "f\"attn & copy scores shape mismatch \"", "f\"({copy_scores_clone.size()}, {attn_clone.size()})\"", "\n", "\n", "# compute scores per token position", "\n", "", "pos_scores", "=", "scores", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", "step", "+", "1", "]", "\n", "pos_scores", "[", ":", ",", "step", "]", "=", "eos_scores", "\n", "# convert from cumulative to per-position scores", "\n", "pos_scores", "[", ":", ",", "1", ":", "]", "=", "pos_scores", "[", ":", ",", "1", ":", "]", "-", "pos_scores", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "# normalize sentence-level scores", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                ", "eos_scores", "/=", "(", "step", "+", "1", ")", "**", "self", ".", "len_penalty", "\n", "\n", "", "cum_unfin", "=", "[", "]", "\n", "prev", "=", "0", "\n", "for", "f", "in", "finished", ":", "\n", "                ", "if", "f", ":", "\n", "                    ", "prev", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cum_unfin", ".", "append", "(", "prev", ")", "\n", "\n", "", "", "sents_seen", "=", "set", "(", ")", "\n", "for", "i", ",", "(", "idx", ",", "score", ")", "in", "enumerate", "(", "zip", "(", "bbsz_idx", ".", "tolist", "(", ")", ",", "eos_scores", ".", "tolist", "(", ")", ")", ")", ":", "\n", "                ", "unfin_idx", "=", "idx", "//", "beam_size", "\n", "sent", "=", "unfin_idx", "+", "cum_unfin", "[", "unfin_idx", "]", "\n", "\n", "sents_seen", ".", "add", "(", "(", "sent", ",", "unfin_idx", ")", ")", "\n", "\n", "if", "self", ".", "match_source_len", "and", "step", ">", "src_lengths", "[", "unfin_idx", "]", ":", "\n", "                    ", "score", "=", "-", "math", ".", "inf", "\n", "\n", "", "def", "get_hypo", "(", ")", ":", "\n", "\n", "                    ", "if", "attn_clone", "is", "not", "None", ":", "\n", "# remove padding tokens from attn scores", "\n", "                        ", "hypo_attn", "=", "attn_clone", "[", "i", "]", "[", "nonpad_idxs", "[", "sent", "]", "]", "\n", "_", ",", "alignment", "=", "hypo_attn", ".", "max", "(", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "hypo_attn", "=", "None", "\n", "alignment", "=", "None", "\n", "\n", "", "\"\"\"\n                    MODIFIED: also get copy scores for this hypothesis.\n                    \"\"\"", "\n", "if", "copy_scores_clone", "is", "not", "None", ":", "\n", "# remove padding tokens from copy scores", "\n", "                        ", "hypo_copy_scores", "=", "copy_scores_clone", "[", "i", "]", "[", "nonpad_idxs", "[", "sent", "]", "]", "\n", "", "else", ":", "\n", "                        ", "hypo_copy_scores", "=", "None", "\n", "\n", "", "return", "{", "\n", "'tokens'", ":", "tokens_clone", "[", "i", "]", ",", "\n", "'score'", ":", "score", ",", "\n", "'attention'", ":", "hypo_attn", ",", "# src_len x tgt_len", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "pos_scores", "[", "i", "]", ",", "\n", "'copy_scores'", ":", "hypo_copy_scores", ",", "# src_len x tgt_len", "\n", "}", "\n", "\n", "", "if", "len", "(", "finalized", "[", "sent", "]", ")", "<", "beam_size", ":", "\n", "                    ", "finalized", "[", "sent", "]", ".", "append", "(", "get_hypo", "(", ")", ")", "\n", "", "elif", "not", "self", ".", "stop_early", "and", "score", ">", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ":", "\n", "# replace worst hypo for this sentence with new/better one", "\n", "                    ", "worst_idx", "=", "worst_finalized", "[", "sent", "]", "[", "'idx'", "]", "\n", "if", "worst_idx", "is", "not", "None", ":", "\n", "                        ", "finalized", "[", "sent", "]", "[", "worst_idx", "]", "=", "get_hypo", "(", ")", "\n", "\n", "# find new worst finalized hypo for this sentence", "\n", "", "idx", ",", "s", "=", "min", "(", "enumerate", "(", "finalized", "[", "sent", "]", ")", ",", "key", "=", "lambda", "r", ":", "r", "[", "1", "]", "[", "'score'", "]", ")", "\n", "worst_finalized", "[", "sent", "]", "=", "{", "\n", "'score'", ":", "s", "[", "'score'", "]", ",", "\n", "'idx'", ":", "idx", ",", "\n", "}", "\n", "\n", "", "", "newly_finished", "=", "[", "]", "\n", "for", "sent", ",", "unfin_idx", "in", "sents_seen", ":", "\n", "# check termination conditions for this sentence", "\n", "                ", "if", "not", "finished", "[", "sent", "]", "and", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", ")", ":", "\n", "                    ", "finished", "[", "sent", "]", "=", "True", "\n", "newly_finished", ".", "append", "(", "unfin_idx", ")", "\n", "", "", "return", "newly_finished", "\n", "\n", "", "reorder_state", "=", "None", "\n", "batch_idxs", "=", "None", "\n", "for", "step", "in", "range", "(", "max_len", "+", "1", ")", ":", "# one extra step for EOS marker", "\n", "# reorder decoder internal states based on the prev choice of beams", "\n", "            ", "if", "reorder_state", "is", "not", "None", ":", "\n", "                ", "if", "batch_idxs", "is", "not", "None", ":", "\n", "# update beam indices to take into account removed sentences", "\n", "                    ", "corr", "=", "batch_idxs", "-", "torch", ".", "arange", "(", "batch_idxs", ".", "numel", "(", ")", ")", ".", "type_as", "(", "batch_idxs", ")", "\n", "reorder_state", ".", "view", "(", "-", "1", ",", "beam_size", ")", ".", "add_", "(", "corr", ".", "unsqueeze", "(", "-", "1", ")", "*", "beam_size", ")", "\n", "", "model", ".", "reorder_incremental_state", "(", "reorder_state", ")", "\n", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "reorder_state", ")", "\n", "\n", "", "\"\"\"\n            MODIFIED: Decoder also returns copy scores, which are also returned.\n            \"\"\"", "\n", "lprobs", ",", "avg_attn_scores", ",", "avg_copy_scores", "=", "model", ".", "forward_decoder", "(", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "encoder_outs", ")", "\n", "\n", "lprobs", "[", ":", ",", "self", ".", "pad", "]", "=", "-", "math", ".", "inf", "# never select pad", "\n", "lprobs", "[", ":", ",", "self", ".", "unk", "]", "-=", "self", ".", "unk_penalty", "# apply unk penalty", "\n", "\n", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "# for each beam and batch sentence, generate a list of previous ngrams", "\n", "                ", "gen_ngrams", "=", "[", "{", "}", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                    ", "gen_tokens", "=", "tokens", "[", "bbsz_idx", "]", ".", "tolist", "(", ")", "\n", "for", "ngram", "in", "zip", "(", "*", "[", "gen_tokens", "[", "i", ":", "]", "for", "i", "in", "range", "(", "self", ".", "no_repeat_ngram_size", ")", "]", ")", ":", "\n", "                        ", "gen_ngrams", "[", "bbsz_idx", "]", "[", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", "]", "=", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", ",", "[", "]", ")", "+", "[", "ngram", "[", "-", "1", "]", "]", "\n", "\n", "# Record attention scores", "\n", "", "", "", "if", "avg_attn_scores", "is", "not", "None", ":", "\n", "                ", "if", "attn", "is", "None", ":", "\n", "                    ", "attn", "=", "scores", ".", "new", "(", "bsz", "*", "beam_size", ",", "src_tokens", ".", "size", "(", "1", ")", ",", "max_len", "+", "2", ")", "\n", "attn_buf", "=", "attn", ".", "clone", "(", ")", "\n", "nonpad_idxs", "=", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", "\n", "", "attn", "[", ":", ",", ":", ",", "step", "+", "1", "]", ".", "copy_", "(", "avg_attn_scores", ")", "\n", "\n", "# Record copy scores", "\n", "", "if", "avg_copy_scores", "is", "not", "None", ":", "\n", "                ", "if", "copy_scores", "is", "None", ":", "\n", "                    ", "copy_scores", "=", "scores", ".", "new", "(", "bsz", "*", "beam_size", ",", "src_tokens", ".", "size", "(", "1", ")", ",", "max_len", "+", "2", ")", "\n", "copy_scores_buf", "=", "copy_scores", ".", "clone", "(", ")", "\n", "nonpad_idxs", "=", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", "\n", "", "copy_scores", "[", ":", ",", ":", ",", "step", "+", "1", "]", ".", "copy_", "(", "avg_copy_scores", ")", "\n", "\n", "", "scores", "=", "scores", ".", "type_as", "(", "lprobs", ")", "\n", "scores_buf", "=", "scores_buf", ".", "type_as", "(", "lprobs", ")", "\n", "eos_bbsz_idx", "=", "buffer", "(", "'eos_bbsz_idx'", ")", "\n", "eos_scores", "=", "buffer", "(", "'eos_scores'", ",", "type_of", "=", "scores", ")", "\n", "if", "step", "<", "max_len", ":", "\n", "                ", "self", ".", "search", ".", "set_src_lengths", "(", "src_lengths", ")", "\n", "\n", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "                    ", "def", "calculate_banned_tokens", "(", "bbsz_idx", ")", ":", "\n", "# before decoding the next token, prevent decoding of ngrams that have already appeared", "\n", "                        ", "ngram_index", "=", "tuple", "(", "tokens", "[", "bbsz_idx", ",", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ":", "step", "+", "1", "]", ".", "tolist", "(", ")", ")", "\n", "return", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "ngram_index", ",", "[", "]", ")", "\n", "\n", "", "if", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ">=", "0", ":", "\n", "# no banned tokens if we haven't generated no_repeat_ngram_size tokens yet", "\n", "                        ", "banned_tokens", "=", "[", "calculate_banned_tokens", "(", "bbsz_idx", ")", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "", "else", ":", "\n", "                        ", "banned_tokens", "=", "[", "[", "]", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "\n", "", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                        ", "lprobs", "[", "bbsz_idx", ",", "banned_tokens", "[", "bbsz_idx", "]", "]", "=", "-", "math", ".", "inf", "\n", "\n", "", "", "if", "prefix_tokens", "is", "not", "None", "and", "step", "<", "prefix_tokens", ".", "size", "(", "1", ")", ":", "\n", "                    ", "probs_slice", "=", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "cand_scores", "=", "torch", ".", "gather", "(", "\n", "probs_slice", ",", "dim", "=", "1", ",", "\n", "index", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "cand_size", ")", "\n", "if", "step", ">", "0", ":", "\n", "# save cumulative scores for each hypothesis", "\n", "                        ", "cand_scores", ".", "add_", "(", "scores", "[", ":", ",", "step", "-", "1", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ".", "repeat", "(", "1", ",", "2", ")", ")", "\n", "", "cand_indices", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "cand_size", ")", "\n", "cand_beams", "=", "torch", ".", "zeros_like", "(", "cand_indices", ")", "\n", "\n", "# handle prefixes of different lengths", "\n", "partial_prefix_mask", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "eq", "(", "self", ".", "pad", ")", "\n", "if", "partial_prefix_mask", ".", "any", "(", ")", ":", "\n", "                        ", "partial_scores", ",", "partial_indices", ",", "partial_beams", "=", "self", ".", "search", ".", "step", "(", "\n", "step", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "self", ".", "vocab_size", ")", ",", "\n", "scores", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "cand_scores", "[", "partial_prefix_mask", "]", "=", "partial_scores", "[", "partial_prefix_mask", "]", "\n", "cand_indices", "[", "partial_prefix_mask", "]", "=", "partial_indices", "[", "partial_prefix_mask", "]", "\n", "cand_beams", "[", "partial_prefix_mask", "]", "=", "partial_beams", "[", "partial_prefix_mask", "]", "\n", "", "", "else", ":", "\n", "                    ", "cand_scores", ",", "cand_indices", ",", "cand_beams", "=", "self", ".", "search", ".", "step", "(", "\n", "step", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "self", ".", "vocab_size", ")", ",", "\n", "scores", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "# make probs contain cumulative scores for each hypothesis", "\n", "                ", "lprobs", ".", "add_", "(", "scores", "[", ":", ",", "step", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "\n", "# finalize all active hypotheses once we hit max_len", "\n", "# pick the hypothesis with the highest prob of EOS right now", "\n", "torch", ".", "sort", "(", "\n", "lprobs", "[", ":", ",", "self", ".", "eos", "]", ",", "\n", "descending", "=", "True", ",", "\n", "out", "=", "(", "eos_scores", ",", "eos_bbsz_idx", ")", ",", "\n", ")", "\n", "num_remaining_sent", "-=", "len", "(", "finalize_hypos", "(", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ")", ")", "\n", "assert", "num_remaining_sent", "==", "0", "\n", "break", "\n", "\n", "# cand_bbsz_idx contains beam indices for the top candidate", "\n", "# hypotheses, with a range of values: [0, bsz*beam_size),", "\n", "# and dimensions: [bsz, cand_size]", "\n", "", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "\n", "# finalize hypotheses that end in eos", "\n", "eos_mask", "=", "cand_indices", ".", "eq", "(", "self", ".", "eos", ")", "\n", "\n", "finalized_sents", "=", "set", "(", ")", "\n", "if", "step", ">=", "self", ".", "min_len", ":", "\n", "# only consider eos when it's among the top beam_size indices", "\n", "                ", "torch", ".", "masked_select", "(", "\n", "cand_bbsz_idx", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_bbsz_idx", ",", "\n", ")", "\n", "if", "eos_bbsz_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                    ", "torch", ".", "masked_select", "(", "\n", "cand_scores", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_scores", ",", "\n", ")", "\n", "finalized_sents", "=", "finalize_hypos", "(", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ",", "cand_scores", ")", "\n", "num_remaining_sent", "-=", "len", "(", "finalized_sents", ")", "\n", "\n", "", "", "assert", "num_remaining_sent", ">=", "0", "\n", "if", "num_remaining_sent", "==", "0", ":", "\n", "                ", "break", "\n", "", "assert", "step", "<", "max_len", "\n", "\n", "if", "len", "(", "finalized_sents", ")", ">", "0", ":", "\n", "                ", "new_bsz", "=", "bsz", "-", "len", "(", "finalized_sents", ")", "\n", "\n", "# construct batch_idxs which holds indices of batches to keep for the next pass", "\n", "batch_mask", "=", "cand_indices", ".", "new_ones", "(", "bsz", ")", "\n", "batch_mask", "[", "cand_indices", ".", "new", "(", "finalized_sents", ")", "]", "=", "0", "\n", "batch_idxs", "=", "batch_mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "eos_mask", "=", "eos_mask", "[", "batch_idxs", "]", "\n", "cand_beams", "=", "cand_beams", "[", "batch_idxs", "]", "\n", "bbsz_offsets", ".", "resize_", "(", "new_bsz", ",", "1", ")", "\n", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "cand_scores", "=", "cand_scores", "[", "batch_idxs", "]", "\n", "cand_indices", "=", "cand_indices", "[", "batch_idxs", "]", "\n", "if", "prefix_tokens", "is", "not", "None", ":", "\n", "                    ", "prefix_tokens", "=", "prefix_tokens", "[", "batch_idxs", "]", "\n", "", "src_lengths", "=", "src_lengths", "[", "batch_idxs", "]", "\n", "\n", "scores", "=", "scores", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "scores_buf", ".", "resize_as_", "(", "scores", ")", "\n", "tokens", "=", "tokens", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "tokens_buf", ".", "resize_as_", "(", "tokens", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                    ", "attn", "=", "attn", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "attn", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "attn_buf", ".", "resize_as_", "(", "attn", ")", "\n", "", "\"\"\"\n                MODIFIED: analogously resize to new batch size\n                \"\"\"", "\n", "if", "copy_scores", "is", "not", "None", ":", "\n", "                    ", "copy_scores", "=", "copy_scores", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "copy_scores", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "copy_scores_buf", ".", "resize_as_", "(", "copy_scores", ")", "\n", "", "bsz", "=", "new_bsz", "\n", "", "else", ":", "\n", "                ", "batch_idxs", "=", "None", "\n", "\n", "# set active_mask so that values > cand_size indicate eos hypos", "\n", "# and values < cand_size indicate candidate active hypos.", "\n", "# After, the min values per row are the top candidate active hypos", "\n", "", "active_mask", "=", "buffer", "(", "'active_mask'", ")", "\n", "torch", ".", "add", "(", "\n", "eos_mask", ".", "type_as", "(", "cand_offsets", ")", "*", "cand_size", ",", "\n", "cand_offsets", "[", ":", "eos_mask", ".", "size", "(", "1", ")", "]", ",", "\n", "out", "=", "active_mask", ",", "\n", ")", "\n", "\n", "# get the top beam_size active hypotheses, which are just the hypos", "\n", "# with the smallest values in active_mask", "\n", "active_hypos", ",", "_ignore", "=", "buffer", "(", "'active_hypos'", ")", ",", "buffer", "(", "'_ignore'", ")", "\n", "torch", ".", "topk", "(", "\n", "active_mask", ",", "k", "=", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "False", ",", "\n", "out", "=", "(", "_ignore", ",", "active_hypos", ")", "\n", ")", "\n", "\n", "active_bbsz_idx", "=", "buffer", "(", "'active_bbsz_idx'", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_bbsz_idx", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "active_bbsz_idx", ",", "\n", ")", "\n", "active_scores", "=", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores", "[", ":", ",", "step", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ",", "\n", ")", "\n", "\n", "active_bbsz_idx", "=", "active_bbsz_idx", ".", "view", "(", "-", "1", ")", "\n", "active_scores", "=", "active_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# copy tokens and scores for active hypotheses", "\n", "torch", ".", "index_select", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "tokens_buf", "[", ":", ",", ":", "step", "+", "1", "]", ",", "\n", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_indices", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "tokens_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "+", "1", "]", ",", "\n", ")", "\n", "if", "step", ">", "0", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "scores", "[", ":", ",", ":", "step", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "scores_buf", "[", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "]", ",", "\n", ")", "\n", "\n", "# copy attention for active hypotheses", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "attn", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "attn_buf", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "\n", ")", "\n", "", "\"\"\"\n            MODIFIED: copy (to buffer) copy scores for active hypotheses\n            \"\"\"", "\n", "if", "copy_scores", "is", "not", "None", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "copy_scores", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "copy_scores_buf", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", "\n", ")", "\n", "\n", "# swap buffers", "\n", "", "tokens", ",", "tokens_buf", "=", "tokens_buf", ",", "tokens", "\n", "scores", ",", "scores_buf", "=", "scores_buf", ",", "scores", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "attn", ",", "attn_buf", "=", "attn_buf", ",", "attn", "\n", "", "\"\"\"\n            MODIFIED: swap buffers for copy scores as well\n            \"\"\"", "\n", "if", "copy_scores", "is", "not", "None", ":", "\n", "                ", "copy_scores", ",", "copy_scores_buf", "=", "copy_scores_buf", ",", "copy_scores", "\n", "\n", "# reorder incremental state in decoder", "\n", "", "reorder_state", "=", "active_bbsz_idx", "\n", "\n", "# sort by score descending", "\n", "", "for", "sent", "in", "range", "(", "len", "(", "finalized", ")", ")", ":", "\n", "            ", "finalized", "[", "sent", "]", "=", "sorted", "(", "finalized", "[", "sent", "]", ",", "key", "=", "lambda", "r", ":", "r", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "", "return", "finalized", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.__init__": [[589, 595], ["super().__init__", "torch.nn.ModuleList", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "models", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "models", "=", "torch", ".", "nn", ".", "ModuleList", "(", "models", ")", "\n", "self", ".", "incremental_states", "=", "None", "\n", "if", "all", "(", "isinstance", "(", "m", ".", "decoder", ",", "FairseqIncrementalDecoder", ")", "for", "m", "in", "models", ")", ":", "\n", "            ", "self", ".", "incremental_states", "=", "{", "m", ":", "{", "}", "for", "m", "in", "models", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.has_encoder": [[596, 598], ["hasattr"], "methods", ["None"], ["", "", "def", "has_encoder", "(", "self", ")", ":", "\n", "        ", "return", "hasattr", "(", "self", ".", "models", "[", "0", "]", ",", "'encoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.max_decoder_positions": [[599, 601], ["min", "m.max_decoder_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.max_decoder_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "return", "min", "(", "m", ".", "max_decoder_positions", "(", ")", "for", "m", "in", "self", ".", "models", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.forward_encoder": [[602, 607], ["torch.no_grad", "sequence_copygenerator.CopyEnsembleModel.has_encoder", "model.encoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_encoder", "(", "self", ",", "encoder_input", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "model", ".", "encoder", "(", "**", "encoder_input", ")", "for", "model", "in", "self", ".", "models", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.forward_decoder": [[608, 641], ["torch.no_grad", "zip", "len", "sequence_copygenerator.CopyEnsembleModel._decode_one", "sequence_copygenerator.CopyEnsembleModel._decode_one", "log_probs.append", "torch.logsumexp", "math.log", "avg_attn.div_", "avg_copy_score.div", "torch.stack", "len", "len", "len", "sequence_copygenerator.CopyEnsembleModel.has_encoder", "avg_attn.add_", "avg_copy_score.add_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_decoder", "(", "self", ",", "tokens", ",", "encoder_outs", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "models", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "_decode_one", "(", "\n", "tokens", ",", "\n", "self", ".", "models", "[", "0", "]", ",", "\n", "encoder_outs", "[", "0", "]", "if", "self", ".", "has_encoder", "(", ")", "else", "None", ",", "\n", "self", ".", "incremental_states", ",", "\n", "log_probs", "=", "True", ",", "\n", ")", "\n", "\n", "", "log_probs", "=", "[", "]", "\n", "avg_attn", "=", "None", "\n", "avg_copy_score", "=", "None", "# across multiple models", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", ":", "\n", "            ", "probs", ",", "attn", ",", "copy_score", "=", "self", ".", "_decode_one", "(", "tokens", ",", "model", ",", "encoder_out", ",", "self", ".", "incremental_states", ",", "log_probs", "=", "True", ")", "\n", "log_probs", ".", "append", "(", "probs", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "if", "copy_score", "is", "not", "None", ":", "\n", "                ", "if", "avg_copy_score", "is", "None", ":", "\n", "                    ", "avg_copy_score", "=", "copy_score", "\n", "", "else", ":", "\n", "                    ", "avg_copy_score", ".", "add_", "(", "copy_score", ")", "\n", "", "", "", "avg_probs", "=", "torch", ".", "logsumexp", "(", "torch", ".", "stack", "(", "log_probs", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "-", "math", ".", "log", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "            ", "avg_attn", ".", "div_", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "", "if", "avg_copy_score", "is", "not", "None", ":", "\n", "            ", "avg_copy_score", ".", "div", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "", "return", "avg_probs", ",", "avg_attn", ",", "avg_copy_score", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel._decode_one": [[642, 665], ["model.get_normalized_probs", "decoder_out[].get", "list", "list", "type", "model.decoder", "model.decoder", "type"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "_decode_one", "(", "self", ",", "tokens", ",", "model", ",", "encoder_out", ",", "incremental_states", ",", "log_probs", ")", ":", "\n", "        ", "\"\"\"Take one decoding step and return values.\n\n        MODIFIED: Additionally returns copy attention scores. (B x T_enc)\n        \"\"\"", "\n", "if", "self", ".", "incremental_states", "is", "not", "None", ":", "\n", "            ", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "tokens", ",", "encoder_out", ",", "incremental_state", "=", "self", ".", "incremental_states", "[", "model", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "tokens", ",", "encoder_out", ")", ")", "\n", "", "decoder_out", "[", "0", "]", "=", "decoder_out", "[", "0", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "attn", "=", "decoder_out", "[", "1", "]", "\n", "if", "type", "(", "attn", ")", "is", "dict", ":", "\n", "            ", "attn", "=", "attn", "[", "'attn'", "]", "\n", "", "if", "attn", "is", "not", "None", ":", "\n", "            ", "if", "type", "(", "attn", ")", "is", "dict", ":", "\n", "                ", "attn", "=", "attn", "[", "'attn'", "]", "\n", "", "attn", "=", "attn", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "probs", "=", "model", ".", "get_normalized_probs", "(", "decoder_out", ",", "log_probs", "=", "log_probs", ")", "\n", "probs", "=", "probs", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "copy_score", "=", "decoder_out", "[", "1", "]", ".", "get", "(", "'copy_scores'", ",", "None", ")", "\n", "if", "copy_score", "is", "not", "None", ":", "\n", "            ", "copy_score", "=", "copy_score", "[", ":", ",", "-", "1", ",", ":", "]", "# only the last decoding position", "\n", "", "return", "probs", ",", "attn", ",", "copy_score", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.reorder_encoder_out": [[666, 672], ["sequence_copygenerator.CopyEnsembleModel.has_encoder", "model.encoder.reorder_encoder_out", "zip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_outs", ",", "new_order", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "\n", "", "return", "[", "\n", "model", ".", "encoder", ".", "reorder_encoder_out", "(", "encoder_out", ",", "new_order", ")", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_copygenerator.CopyEnsembleModel.reorder_incremental_state": [[674, 679], ["model.decoder.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "new_order", ")", ":", "\n", "        ", "if", "self", ".", "incremental_states", "is", "None", ":", "\n", "            ", "return", "\n", "", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "decoder", ".", "reorder_incremental_state", "(", "self", ".", "incremental_states", "[", "model", "]", ",", "new_order", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.__init__": [[17, 99], ["tgt_dict.pad", "tgt_dict.unk", "tgt_dict.eos", "len", "min", "fairseq.search.Sampling", "fairseq.search.DiverseBeamSearch", "fairseq.search.LengthConstrainedBeamSearch", "fairseq.search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "tgt_dict", ",", "\n", "beam_size", "=", "1", ",", "\n", "max_len_a", "=", "0", ",", "\n", "max_len_b", "=", "200", ",", "\n", "min_len", "=", "1", ",", "\n", "stop_early", "=", "True", ",", "\n", "normalize_scores", "=", "True", ",", "\n", "len_penalty", "=", "1.", ",", "\n", "unk_penalty", "=", "0.", ",", "\n", "retain_dropout", "=", "False", ",", "\n", "sampling", "=", "False", ",", "\n", "sampling_topk", "=", "-", "1", ",", "\n", "sampling_temperature", "=", "1.", ",", "\n", "diverse_beam_groups", "=", "-", "1", ",", "\n", "diverse_beam_strength", "=", "0.5", ",", "\n", "match_source_len", "=", "False", ",", "\n", "no_repeat_ngram_size", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Generates translations of a given source sentence.\n\n        Args:\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            stop_early (bool, optional): stop generation immediately after we\n                finalize beam_size hypotheses, even though longer hypotheses\n                might have better normalized scores (default: True)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            sampling (bool, optional): sample outputs instead of beam search\n                (default: False)\n            sampling_topk (int, optional): only sample among the top-k choices\n                at each step (default: -1)\n            sampling_temperature (float, optional): temperature for sampling,\n                where values >1.0 produces more uniform sampling and values\n                <1.0 produces sharper sampling (default: 1.0)\n            diverse_beam_groups/strength (float, optional): parameters for\n                Diverse Beam Search sampling\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"", "\n", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "tgt_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "tgt_dict", ".", "eos", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "tgt_dict", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "# the max beam size is the dictionary size - 1, since we never select pad", "\n", "self", ".", "beam_size", "=", "min", "(", "beam_size", ",", "self", ".", "vocab_size", "-", "1", ")", "\n", "self", ".", "max_len_a", "=", "max_len_a", "\n", "self", ".", "max_len_b", "=", "max_len_b", "\n", "self", ".", "min_len", "=", "min_len", "\n", "self", ".", "stop_early", "=", "stop_early", "\n", "self", ".", "normalize_scores", "=", "normalize_scores", "\n", "self", ".", "len_penalty", "=", "len_penalty", "\n", "self", ".", "unk_penalty", "=", "unk_penalty", "\n", "self", ".", "retain_dropout", "=", "retain_dropout", "\n", "self", ".", "match_source_len", "=", "match_source_len", "\n", "self", ".", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", "\n", "\n", "assert", "sampling_topk", "<", "0", "or", "sampling", ",", "'--sampling-topk requires --sampling'", "\n", "\n", "if", "sampling", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "Sampling", "(", "tgt_dict", ",", "sampling_topk", ",", "sampling_temperature", ")", "\n", "", "elif", "diverse_beam_groups", ">", "0", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "DiverseBeamSearch", "(", "tgt_dict", ",", "diverse_beam_groups", ",", "diverse_beam_strength", ")", "\n", "", "elif", "match_source_len", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "LengthConstrainedBeamSearch", "(", "\n", "tgt_dict", ",", "min_len_a", "=", "1", ",", "min_len_b", "=", "0", ",", "max_len_a", "=", "1", ",", "max_len_b", "=", "0", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate": [[100, 522], ["torch.no_grad", "sequence_generator.EnsembleModel", "src_tokens.size", "sequence_generator.EnsembleModel.forward_encoder", "torch.arange().view().repeat().view", "new_order.to().long.to().long.to().long", "sequence_generator.EnsembleModel.reorder_encoder_out", "src_tokens.new().float().fill_", "[].view.clone", "src_tokens.new().fill_", "[].view.clone", "torch.arange().type_as", "range", "range", "EnsembleModel.eval", "src_lengths.max().item", "min", "[].view.index_select", "set", "enumerate", "sequence_generator.EnsembleModel.forward_decoder", "[].view.type_as", "scores_buf.type_as.type_as.type_as", "sequence_generator.SequenceGenerator.generate.buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_decoder"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "None", ",", "\n", "bos_token", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"Generate a batch of translations.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n        \"\"\"", "\n", "model", "=", "EnsembleModel", "(", "models", ")", "\n", "if", "not", "self", ".", "retain_dropout", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "\n", "# model.forward normally channels prev_output_tokens into the decoder", "\n", "# separately, but SequenceGenerator directly calls model.encoder", "\n", "", "encoder_input", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "sample", "[", "'net_input'", "]", ".", "items", "(", ")", "\n", "if", "k", "!=", "'prev_output_tokens'", "\n", "}", "\n", "\n", "src_tokens", "=", "encoder_input", "[", "'src_tokens'", "]", "\n", "src_lengths", "=", "(", "src_tokens", ".", "ne", "(", "self", ".", "eos", ")", "&", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "bsz", ",", "src_len", "=", "src_tokens", ".", "size", "(", ")", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "\n", "if", "self", ".", "match_source_len", ":", "\n", "            ", "max_len", "=", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "max_len", "=", "min", "(", "\n", "int", "(", "self", ".", "max_len_a", "*", "src_len", "+", "self", ".", "max_len_b", ")", ",", "\n", "# exclude the EOS marker", "\n", "model", ".", "max_decoder_positions", "(", ")", "-", "1", ",", "\n", ")", "\n", "\n", "# compute the encoder output for each beam", "\n", "", "encoder_outs", "=", "model", ".", "forward_encoder", "(", "encoder_input", ")", "\n", "new_order", "=", "torch", ".", "arange", "(", "bsz", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "beam_size", ")", ".", "view", "(", "-", "1", ")", "\n", "new_order", "=", "new_order", ".", "to", "(", "src_tokens", ".", "device", ")", ".", "long", "(", ")", "\n", "encoder_outs", "=", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "new_order", ")", "\n", "\n", "# initialize buffers", "\n", "scores", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "1", ")", ".", "float", "(", ")", ".", "fill_", "(", "0", ")", "\n", "scores_buf", "=", "scores", ".", "clone", "(", ")", "\n", "tokens", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "2", ")", ".", "fill_", "(", "self", ".", "pad", ")", "\n", "tokens_buf", "=", "tokens", ".", "clone", "(", ")", "\n", "tokens", "[", ":", ",", "0", "]", "=", "bos_token", "or", "self", ".", "eos", "\n", "attn", ",", "attn_buf", "=", "None", ",", "None", "\n", "nonpad_idxs", "=", "None", "\n", "\n", "# list of completed sentences", "\n", "finalized", "=", "[", "[", "]", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "finished", "=", "[", "False", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "worst_finalized", "=", "[", "{", "'idx'", ":", "None", ",", "'score'", ":", "-", "math", ".", "inf", "}", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "num_remaining_sent", "=", "bsz", "\n", "\n", "# number of candidate hypos per step", "\n", "cand_size", "=", "2", "*", "beam_size", "# 2 x beam size in case half are EOS", "\n", "\n", "# offset arrays for converting between different indexing schemes", "\n", "bbsz_offsets", "=", "(", "torch", ".", "arange", "(", "0", ",", "bsz", ")", "*", "beam_size", ")", ".", "unsqueeze", "(", "1", ")", ".", "type_as", "(", "tokens", ")", "\n", "cand_offsets", "=", "torch", ".", "arange", "(", "0", ",", "cand_size", ")", ".", "type_as", "(", "tokens", ")", "\n", "\n", "# helper function for allocating buffers on the fly", "\n", "buffers", "=", "{", "}", "\n", "\n", "def", "buffer", "(", "name", ",", "type_of", "=", "tokens", ")", ":", "# noqa", "\n", "            ", "if", "name", "not", "in", "buffers", ":", "\n", "                ", "buffers", "[", "name", "]", "=", "type_of", ".", "new", "(", ")", "\n", "", "return", "buffers", "[", "name", "]", "\n", "\n", "", "def", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"", "\n", "assert", "len", "(", "finalized", "[", "sent", "]", ")", "<=", "beam_size", "\n", "if", "len", "(", "finalized", "[", "sent", "]", ")", "==", "beam_size", ":", "\n", "                ", "if", "self", ".", "stop_early", "or", "step", "==", "max_len", "or", "unfinalized_scores", "is", "None", ":", "\n", "                    ", "return", "True", "\n", "# stop if the best unfinalized score is worse than the worst", "\n", "# finalized one", "\n", "", "best_unfinalized_score", "=", "unfinalized_scores", "[", "sent", "]", ".", "max", "(", ")", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                    ", "best_unfinalized_score", "/=", "max_len", "**", "self", ".", "len_penalty", "\n", "", "if", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ">=", "best_unfinalized_score", ":", "\n", "                    ", "return", "True", "\n", "", "", "return", "False", "\n", "\n", "", "def", "finalize_hypos", "(", "step", ",", "bbsz_idx", ",", "eos_scores", ",", "unfinalized_scores", "=", "None", ")", ":", "\n", "            ", "\"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n                unfinalized_scores: A vector containing scores for all\n                    unfinalized hypotheses\n            \"\"\"", "\n", "assert", "bbsz_idx", ".", "numel", "(", ")", "==", "eos_scores", ".", "numel", "(", ")", "\n", "\n", "# clone relevant token and attention tensors", "\n", "tokens_clone", "=", "tokens", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "\n", "tokens_clone", "=", "tokens_clone", "[", ":", ",", "1", ":", "step", "+", "2", "]", "# skip the first index, which is EOS", "\n", "tokens_clone", "[", ":", ",", "step", "]", "=", "self", ".", "eos", "\n", "attn_clone", "=", "attn", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", ",", "1", ":", "step", "+", "2", "]", "if", "attn", "is", "not", "None", "else", "None", "\n", "\n", "# compute scores per token position", "\n", "pos_scores", "=", "scores", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", "step", "+", "1", "]", "\n", "pos_scores", "[", ":", ",", "step", "]", "=", "eos_scores", "\n", "# convert from cumulative to per-position scores", "\n", "pos_scores", "[", ":", ",", "1", ":", "]", "=", "pos_scores", "[", ":", ",", "1", ":", "]", "-", "pos_scores", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "# normalize sentence-level scores", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                ", "eos_scores", "/=", "(", "step", "+", "1", ")", "**", "self", ".", "len_penalty", "\n", "\n", "", "cum_unfin", "=", "[", "]", "\n", "prev", "=", "0", "\n", "for", "f", "in", "finished", ":", "\n", "                ", "if", "f", ":", "\n", "                    ", "prev", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cum_unfin", ".", "append", "(", "prev", ")", "\n", "\n", "", "", "sents_seen", "=", "set", "(", ")", "\n", "for", "i", ",", "(", "idx", ",", "score", ")", "in", "enumerate", "(", "zip", "(", "bbsz_idx", ".", "tolist", "(", ")", ",", "eos_scores", ".", "tolist", "(", ")", ")", ")", ":", "\n", "                ", "unfin_idx", "=", "idx", "//", "beam_size", "\n", "sent", "=", "unfin_idx", "+", "cum_unfin", "[", "unfin_idx", "]", "\n", "\n", "sents_seen", ".", "add", "(", "(", "sent", ",", "unfin_idx", ")", ")", "\n", "\n", "if", "self", ".", "match_source_len", "and", "step", ">", "src_lengths", "[", "unfin_idx", "]", ":", "\n", "                    ", "score", "=", "-", "math", ".", "inf", "\n", "\n", "", "def", "get_hypo", "(", ")", ":", "\n", "\n", "                    ", "if", "attn_clone", "is", "not", "None", ":", "\n", "# remove padding tokens from attn scores", "\n", "                        ", "hypo_attn", "=", "attn_clone", "[", "i", "]", "[", "nonpad_idxs", "[", "sent", "]", "]", "\n", "_", ",", "alignment", "=", "hypo_attn", ".", "max", "(", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "hypo_attn", "=", "None", "\n", "alignment", "=", "None", "\n", "\n", "", "return", "{", "\n", "'tokens'", ":", "tokens_clone", "[", "i", "]", ",", "\n", "'score'", ":", "score", ",", "\n", "'attention'", ":", "hypo_attn", ",", "# src_len x tgt_len", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "pos_scores", "[", "i", "]", ",", "\n", "}", "\n", "\n", "", "if", "len", "(", "finalized", "[", "sent", "]", ")", "<", "beam_size", ":", "\n", "                    ", "finalized", "[", "sent", "]", ".", "append", "(", "get_hypo", "(", ")", ")", "\n", "", "elif", "not", "self", ".", "stop_early", "and", "score", ">", "worst_finalized", "[", "sent", "]", "[", "'score'", "]", ":", "\n", "# replace worst hypo for this sentence with new/better one", "\n", "                    ", "worst_idx", "=", "worst_finalized", "[", "sent", "]", "[", "'idx'", "]", "\n", "if", "worst_idx", "is", "not", "None", ":", "\n", "                        ", "finalized", "[", "sent", "]", "[", "worst_idx", "]", "=", "get_hypo", "(", ")", "\n", "\n", "# find new worst finalized hypo for this sentence", "\n", "", "idx", ",", "s", "=", "min", "(", "enumerate", "(", "finalized", "[", "sent", "]", ")", ",", "key", "=", "lambda", "r", ":", "r", "[", "1", "]", "[", "'score'", "]", ")", "\n", "worst_finalized", "[", "sent", "]", "=", "{", "\n", "'score'", ":", "s", "[", "'score'", "]", ",", "\n", "'idx'", ":", "idx", ",", "\n", "}", "\n", "\n", "", "", "newly_finished", "=", "[", "]", "\n", "for", "sent", ",", "unfin_idx", "in", "sents_seen", ":", "\n", "# check termination conditions for this sentence", "\n", "                ", "if", "not", "finished", "[", "sent", "]", "and", "is_finished", "(", "sent", ",", "step", ",", "unfinalized_scores", ")", ":", "\n", "                    ", "finished", "[", "sent", "]", "=", "True", "\n", "newly_finished", ".", "append", "(", "unfin_idx", ")", "\n", "", "", "return", "newly_finished", "\n", "\n", "", "reorder_state", "=", "None", "\n", "batch_idxs", "=", "None", "\n", "for", "step", "in", "range", "(", "max_len", "+", "1", ")", ":", "# one extra step for EOS marker", "\n", "# reorder decoder internal states based on the prev choice of beams", "\n", "            ", "if", "reorder_state", "is", "not", "None", ":", "\n", "                ", "if", "batch_idxs", "is", "not", "None", ":", "\n", "# update beam indices to take into account removed sentences", "\n", "                    ", "corr", "=", "batch_idxs", "-", "torch", ".", "arange", "(", "batch_idxs", ".", "numel", "(", ")", ")", ".", "type_as", "(", "batch_idxs", ")", "\n", "reorder_state", ".", "view", "(", "-", "1", ",", "beam_size", ")", ".", "add_", "(", "corr", ".", "unsqueeze", "(", "-", "1", ")", "*", "beam_size", ")", "\n", "", "model", ".", "reorder_incremental_state", "(", "reorder_state", ")", "\n", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "reorder_state", ")", "\n", "\n", "", "lprobs", ",", "avg_attn_scores", "=", "model", ".", "forward_decoder", "(", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "encoder_outs", ")", "\n", "\n", "lprobs", "[", ":", ",", "self", ".", "pad", "]", "=", "-", "math", ".", "inf", "# never select pad", "\n", "lprobs", "[", ":", ",", "self", ".", "unk", "]", "-=", "self", ".", "unk_penalty", "# apply unk penalty", "\n", "\n", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "# for each beam and batch sentence, generate a list of previous ngrams", "\n", "                ", "gen_ngrams", "=", "[", "{", "}", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                    ", "gen_tokens", "=", "tokens", "[", "bbsz_idx", "]", ".", "tolist", "(", ")", "\n", "for", "ngram", "in", "zip", "(", "*", "[", "gen_tokens", "[", "i", ":", "]", "for", "i", "in", "range", "(", "self", ".", "no_repeat_ngram_size", ")", "]", ")", ":", "\n", "                        ", "gen_ngrams", "[", "bbsz_idx", "]", "[", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", "]", "=", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", ",", "[", "]", ")", "+", "[", "ngram", "[", "-", "1", "]", "]", "\n", "\n", "# Record attention scores", "\n", "", "", "", "if", "avg_attn_scores", "is", "not", "None", ":", "\n", "                ", "if", "attn", "is", "None", ":", "\n", "                    ", "attn", "=", "scores", ".", "new", "(", "bsz", "*", "beam_size", ",", "src_tokens", ".", "size", "(", "1", ")", ",", "max_len", "+", "2", ")", "\n", "attn_buf", "=", "attn", ".", "clone", "(", ")", "\n", "nonpad_idxs", "=", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", "\n", "", "attn", "[", ":", ",", ":", ",", "step", "+", "1", "]", ".", "copy_", "(", "avg_attn_scores", ")", "\n", "\n", "", "scores", "=", "scores", ".", "type_as", "(", "lprobs", ")", "\n", "scores_buf", "=", "scores_buf", ".", "type_as", "(", "lprobs", ")", "\n", "eos_bbsz_idx", "=", "buffer", "(", "'eos_bbsz_idx'", ")", "\n", "eos_scores", "=", "buffer", "(", "'eos_scores'", ",", "type_of", "=", "scores", ")", "\n", "if", "step", "<", "max_len", ":", "\n", "                ", "self", ".", "search", ".", "set_src_lengths", "(", "src_lengths", ")", "\n", "\n", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "                    ", "def", "calculate_banned_tokens", "(", "bbsz_idx", ")", ":", "\n", "# before decoding the next token, prevent decoding of ngrams that have already appeared", "\n", "                        ", "ngram_index", "=", "tuple", "(", "tokens", "[", "bbsz_idx", ",", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ":", "step", "+", "1", "]", ".", "tolist", "(", ")", ")", "\n", "return", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "ngram_index", ",", "[", "]", ")", "\n", "\n", "", "if", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ">=", "0", ":", "\n", "# no banned tokens if we haven't generated no_repeat_ngram_size tokens yet", "\n", "                        ", "banned_tokens", "=", "[", "calculate_banned_tokens", "(", "bbsz_idx", ")", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "", "else", ":", "\n", "                        ", "banned_tokens", "=", "[", "[", "]", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "\n", "", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                        ", "lprobs", "[", "bbsz_idx", ",", "banned_tokens", "[", "bbsz_idx", "]", "]", "=", "-", "math", ".", "inf", "\n", "\n", "", "", "if", "prefix_tokens", "is", "not", "None", "and", "step", "<", "prefix_tokens", ".", "size", "(", "1", ")", ":", "\n", "                    ", "probs_slice", "=", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "cand_scores", "=", "torch", ".", "gather", "(", "\n", "probs_slice", ",", "dim", "=", "1", ",", "\n", "index", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "cand_size", ")", "\n", "if", "step", ">", "0", ":", "\n", "# save cumulative scores for each hypothesis", "\n", "                        ", "cand_scores", ".", "add_", "(", "scores", "[", ":", ",", "step", "-", "1", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ".", "repeat", "(", "1", ",", "2", ")", ")", "\n", "", "cand_indices", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "cand_size", ")", "\n", "cand_beams", "=", "torch", ".", "zeros_like", "(", "cand_indices", ")", "\n", "\n", "# handle prefixes of different lengths", "\n", "partial_prefix_mask", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "eq", "(", "self", ".", "pad", ")", "\n", "if", "partial_prefix_mask", ".", "any", "(", ")", ":", "\n", "                        ", "partial_scores", ",", "partial_indices", ",", "partial_beams", "=", "self", ".", "search", ".", "step", "(", "\n", "step", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "self", ".", "vocab_size", ")", ",", "\n", "scores", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "cand_scores", "[", "partial_prefix_mask", "]", "=", "partial_scores", "[", "partial_prefix_mask", "]", "\n", "cand_indices", "[", "partial_prefix_mask", "]", "=", "partial_indices", "[", "partial_prefix_mask", "]", "\n", "cand_beams", "[", "partial_prefix_mask", "]", "=", "partial_beams", "[", "partial_prefix_mask", "]", "\n", "", "", "else", ":", "\n", "                    ", "cand_scores", ",", "cand_indices", ",", "cand_beams", "=", "self", ".", "search", ".", "step", "(", "\n", "step", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "self", ".", "vocab_size", ")", ",", "\n", "scores", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "# make probs contain cumulative scores for each hypothesis", "\n", "                ", "lprobs", ".", "add_", "(", "scores", "[", ":", ",", "step", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "\n", "# finalize all active hypotheses once we hit max_len", "\n", "# pick the hypothesis with the highest prob of EOS right now", "\n", "torch", ".", "sort", "(", "\n", "lprobs", "[", ":", ",", "self", ".", "eos", "]", ",", "\n", "descending", "=", "True", ",", "\n", "out", "=", "(", "eos_scores", ",", "eos_bbsz_idx", ")", ",", "\n", ")", "\n", "num_remaining_sent", "-=", "len", "(", "finalize_hypos", "(", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ")", ")", "\n", "assert", "num_remaining_sent", "==", "0", "\n", "break", "\n", "\n", "# cand_bbsz_idx contains beam indices for the top candidate", "\n", "# hypotheses, with a range of values: [0, bsz*beam_size),", "\n", "# and dimensions: [bsz, cand_size]", "\n", "", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "\n", "# finalize hypotheses that end in eos", "\n", "eos_mask", "=", "cand_indices", ".", "eq", "(", "self", ".", "eos", ")", "\n", "\n", "finalized_sents", "=", "set", "(", ")", "\n", "if", "step", ">=", "self", ".", "min_len", ":", "\n", "# only consider eos when it's among the top beam_size indices", "\n", "                ", "torch", ".", "masked_select", "(", "\n", "cand_bbsz_idx", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_bbsz_idx", ",", "\n", ")", "\n", "if", "eos_bbsz_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                    ", "torch", ".", "masked_select", "(", "\n", "cand_scores", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_scores", ",", "\n", ")", "\n", "finalized_sents", "=", "finalize_hypos", "(", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ",", "cand_scores", ")", "\n", "num_remaining_sent", "-=", "len", "(", "finalized_sents", ")", "\n", "\n", "", "", "assert", "num_remaining_sent", ">=", "0", "\n", "if", "num_remaining_sent", "==", "0", ":", "\n", "                ", "break", "\n", "", "assert", "step", "<", "max_len", "\n", "\n", "if", "len", "(", "finalized_sents", ")", ">", "0", ":", "\n", "                ", "new_bsz", "=", "bsz", "-", "len", "(", "finalized_sents", ")", "\n", "\n", "# construct batch_idxs which holds indices of batches to keep for the next pass", "\n", "batch_mask", "=", "cand_indices", ".", "new_ones", "(", "bsz", ")", "\n", "batch_mask", "[", "cand_indices", ".", "new", "(", "finalized_sents", ")", "]", "=", "0", "\n", "batch_idxs", "=", "batch_mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "eos_mask", "=", "eos_mask", "[", "batch_idxs", "]", "\n", "cand_beams", "=", "cand_beams", "[", "batch_idxs", "]", "\n", "bbsz_offsets", ".", "resize_", "(", "new_bsz", ",", "1", ")", "\n", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "cand_scores", "=", "cand_scores", "[", "batch_idxs", "]", "\n", "cand_indices", "=", "cand_indices", "[", "batch_idxs", "]", "\n", "if", "prefix_tokens", "is", "not", "None", ":", "\n", "                    ", "prefix_tokens", "=", "prefix_tokens", "[", "batch_idxs", "]", "\n", "", "src_lengths", "=", "src_lengths", "[", "batch_idxs", "]", "\n", "\n", "scores", "=", "scores", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "scores_buf", ".", "resize_as_", "(", "scores", ")", "\n", "tokens", "=", "tokens", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "tokens_buf", ".", "resize_as_", "(", "tokens", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                    ", "attn", "=", "attn", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "attn", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "attn_buf", ".", "resize_as_", "(", "attn", ")", "\n", "", "bsz", "=", "new_bsz", "\n", "", "else", ":", "\n", "                ", "batch_idxs", "=", "None", "\n", "\n", "# set active_mask so that values > cand_size indicate eos hypos", "\n", "# and values < cand_size indicate candidate active hypos.", "\n", "# After, the min values per row are the top candidate active hypos", "\n", "", "active_mask", "=", "buffer", "(", "'active_mask'", ")", "\n", "torch", ".", "add", "(", "\n", "eos_mask", ".", "type_as", "(", "cand_offsets", ")", "*", "cand_size", ",", "\n", "cand_offsets", "[", ":", "eos_mask", ".", "size", "(", "1", ")", "]", ",", "\n", "out", "=", "active_mask", ",", "\n", ")", "\n", "\n", "# get the top beam_size active hypotheses, which are just the hypos", "\n", "# with the smallest values in active_mask", "\n", "active_hypos", ",", "_ignore", "=", "buffer", "(", "'active_hypos'", ")", ",", "buffer", "(", "'_ignore'", ")", "\n", "torch", ".", "topk", "(", "\n", "active_mask", ",", "k", "=", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "False", ",", "\n", "out", "=", "(", "_ignore", ",", "active_hypos", ")", "\n", ")", "\n", "\n", "active_bbsz_idx", "=", "buffer", "(", "'active_bbsz_idx'", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_bbsz_idx", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "active_bbsz_idx", ",", "\n", ")", "\n", "active_scores", "=", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores", "[", ":", ",", "step", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ",", "\n", ")", "\n", "\n", "active_bbsz_idx", "=", "active_bbsz_idx", ".", "view", "(", "-", "1", ")", "\n", "active_scores", "=", "active_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# copy tokens and scores for active hypotheses", "\n", "torch", ".", "index_select", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "tokens_buf", "[", ":", ",", ":", "step", "+", "1", "]", ",", "\n", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_indices", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "tokens_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "+", "1", "]", ",", "\n", ")", "\n", "if", "step", ">", "0", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "scores", "[", ":", ",", ":", "step", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "scores_buf", "[", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "]", ",", "\n", ")", "\n", "\n", "# copy attention for active hypotheses", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "attn", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "attn_buf", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "\n", ")", "\n", "\n", "# swap buffers", "\n", "", "tokens", ",", "tokens_buf", "=", "tokens_buf", ",", "tokens", "\n", "scores", ",", "scores_buf", "=", "scores_buf", ",", "scores", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "attn", ",", "attn_buf", "=", "attn_buf", ",", "attn", "\n", "\n", "# reorder incremental state in decoder", "\n", "", "reorder_state", "=", "active_bbsz_idx", "\n", "\n", "# sort by score descending", "\n", "", "for", "sent", "in", "range", "(", "len", "(", "finalized", ")", ")", ":", "\n", "            ", "finalized", "[", "sent", "]", "=", "sorted", "(", "finalized", "[", "sent", "]", ",", "key", "=", "lambda", "r", ":", "r", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "", "return", "finalized", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.__init__": [[527, 533], ["super().__init__", "torch.nn.ModuleList", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "models", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "models", "=", "torch", ".", "nn", ".", "ModuleList", "(", "models", ")", "\n", "self", ".", "incremental_states", "=", "None", "\n", "if", "all", "(", "isinstance", "(", "m", ".", "decoder", ",", "FairseqIncrementalDecoder", ")", "for", "m", "in", "models", ")", ":", "\n", "            ", "self", ".", "incremental_states", "=", "{", "m", ":", "{", "}", "for", "m", "in", "models", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder": [[534, 536], ["hasattr"], "methods", ["None"], ["", "", "def", "has_encoder", "(", "self", ")", ":", "\n", "        ", "return", "hasattr", "(", "self", ".", "models", "[", "0", "]", ",", "'encoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.max_decoder_positions": [[537, 539], ["min", "m.max_decoder_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.max_decoder_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "return", "min", "(", "m", ".", "max_decoder_positions", "(", ")", "for", "m", "in", "self", ".", "models", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_encoder": [[540, 545], ["torch.no_grad", "sequence_generator.EnsembleModel.has_encoder", "model.encoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_encoder", "(", "self", ",", "encoder_input", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "model", ".", "encoder", "(", "**", "encoder_input", ")", "for", "model", "in", "self", ".", "models", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.forward_decoder": [[546, 571], ["torch.no_grad", "zip", "len", "sequence_generator.EnsembleModel._decode_one", "sequence_generator.EnsembleModel._decode_one", "log_probs.append", "torch.logsumexp", "math.log", "avg_attn.div_", "torch.stack", "len", "len", "sequence_generator.EnsembleModel.has_encoder", "avg_attn.add_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_decoder", "(", "self", ",", "tokens", ",", "encoder_outs", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "models", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "_decode_one", "(", "\n", "tokens", ",", "\n", "self", ".", "models", "[", "0", "]", ",", "\n", "encoder_outs", "[", "0", "]", "if", "self", ".", "has_encoder", "(", ")", "else", "None", ",", "\n", "self", ".", "incremental_states", ",", "\n", "log_probs", "=", "True", ",", "\n", ")", "\n", "\n", "", "log_probs", "=", "[", "]", "\n", "avg_attn", "=", "None", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", ":", "\n", "            ", "probs", ",", "attn", "=", "self", ".", "_decode_one", "(", "tokens", ",", "model", ",", "encoder_out", ",", "self", ".", "incremental_states", ",", "log_probs", "=", "True", ")", "\n", "log_probs", ".", "append", "(", "probs", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "", "avg_probs", "=", "torch", ".", "logsumexp", "(", "torch", ".", "stack", "(", "log_probs", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "-", "math", ".", "log", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "            ", "avg_attn", ".", "div_", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "", "return", "avg_probs", ",", "avg_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel._decode_one": [[572, 588], ["model.get_normalized_probs", "list", "list", "type", "model.decoder", "model.decoder", "type"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "_decode_one", "(", "self", ",", "tokens", ",", "model", ",", "encoder_out", ",", "incremental_states", ",", "log_probs", ")", ":", "\n", "        ", "if", "self", ".", "incremental_states", "is", "not", "None", ":", "\n", "            ", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "tokens", ",", "encoder_out", ",", "incremental_state", "=", "self", ".", "incremental_states", "[", "model", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "tokens", ",", "encoder_out", ")", ")", "\n", "", "decoder_out", "[", "0", "]", "=", "decoder_out", "[", "0", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "attn", "=", "decoder_out", "[", "1", "]", "\n", "if", "type", "(", "attn", ")", "is", "dict", ":", "\n", "            ", "attn", "=", "attn", "[", "'attn'", "]", "\n", "", "if", "attn", "is", "not", "None", ":", "\n", "            ", "if", "type", "(", "attn", ")", "is", "dict", ":", "\n", "                ", "attn", "=", "attn", "[", "'attn'", "]", "\n", "", "attn", "=", "attn", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "probs", "=", "model", ".", "get_normalized_probs", "(", "decoder_out", ",", "log_probs", "=", "log_probs", ")", "\n", "probs", "=", "probs", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "return", "probs", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.reorder_encoder_out": [[589, 595], ["sequence_generator.EnsembleModel.has_encoder", "model.encoder.reorder_encoder_out", "zip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_outs", ",", "new_order", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "\n", "", "return", "[", "\n", "model", ".", "encoder", ".", "reorder_encoder_out", "(", "encoder_out", ",", "new_order", ")", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.EnsembleModel.reorder_incremental_state": [[597, 602], ["model.decoder.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "new_order", ")", ":", "\n", "        ", "if", "self", ".", "incremental_states", "is", "None", ":", "\n", "            ", "return", "\n", "", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "decoder", ".", "reorder_incremental_state", "(", "self", ".", "incremental_states", "[", "model", "]", ",", "new_order", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.torch_persistent_save": [[19, 26], ["range", "torch.save", "logging.error", "traceback.format_exc"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save"], ["from", "fairseq", ".", "tasks", "import", "FairseqTask", "\n", "\n", "\n", "def", "dummy_dictionary", "(", "vocab_size", ",", "prefix", "=", "'token_'", ")", ":", "\n", "    ", "d", "=", "Dictionary", "(", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "token", "=", "prefix", "+", "str", "(", "i", ")", "\n", "d", ".", "add_symbol", "(", "token", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_state_dict_type": [[28, 40], ["isinstance", "collections.OrderedDict", "state_dict.items", "isinstance", "utils.convert_state_dict_type", "torch.is_tensor", "utils.convert_state_dict_type", "state_dict.type"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_state_dict_type", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_state_dict_type"], ["return", "d", "\n", "\n", "\n", "", "def", "dummy_dataloader", "(", "\n", "samples", ",", "\n", "padding_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n", "batch_size", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "batch_size", "is", "None", ":", "\n", "        ", "batch_size", "=", "len", "(", "samples", ")", "\n", "\n", "# add any missing data to samples", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.save_state": [[42, 63], ["utils.torch_persistent_save", "utils.convert_state_dict_type", "optimizer.state_dict", "lr_scheduler.state_dict"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.torch_persistent_save", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_state_dict_type", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict"], ["        ", "if", "'id'", "not", "in", "sample", ":", "\n", "            ", "sample", "[", "'id'", "]", "=", "i", "\n", "\n", "# create dataloader", "\n", "", "", "dataset", "=", "TestDataset", "(", "samples", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "collate_fn", "=", "(", "lambda", "samples", ":", "collate", "(", "samples", ",", "padding_idx", ",", "eos_idx", ")", ")", ",", "\n", ")", "\n", "return", "iter", "(", "dataloader", ")", "\n", "\n", "\n", "", "def", "sequence_generator_setup", "(", ")", ":", "\n", "# construct dummy dictionary", "\n", "    ", "d", "=", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct source data", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_model_state": [[65, 80], ["torch.load", "utils._upgrade_state_dict", "model.upgrade_state_dict", "os.path.exists", "model.load_state_dict", "Exception", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._upgrade_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict"], ["src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._upgrade_state_dict": [[82, 133], ["hasattr", "hasattr", "state[].get"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1: 0.9  (emit: w1 <eos>: 0.9*1.0)", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# w2: 0.1", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "# w1: 0.7  (don't emit: w1 <eos>: 0.7*0.25)", "\n", "[", "0.00", ",", "unk", ",", "0.10", ",", "0.9", "]", ",", "# w2: 0.3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.1", ",", "0.9", "]", ",", "# w2 w1: 0.1*0.9", "\n", "[", "0.6", ",", "unk", ",", "0.2", ",", "0.2", "]", ",", "# w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)", "\n", "# sentence 2:", "\n", "[", "0.60", ",", "unk", ",", "0.4", ",", "0.00", "]", ",", "# w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)", "\n", "[", "0.01", ",", "unk", ",", "0.0", ",", "0.99", "]", ",", "# w2 w2: 0.3*0.9", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)", "\n", "# sentence 2:", "\n", "[", "0.1", ",", "unk", ",", "0.5", ",", "0.4", "]", ",", "# w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "return", "tgt_dict", ",", "w1", ",", "w2", ",", "src_tokens", ",", "src_lengths", ",", "model", "\n", "\n", "\n", "", "class", "TestDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "sizes", "=", "None", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n", "\n", "", "", "class", "TestTranslationTask", "(", "FairseqTask", ")", ":", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_checkpoint_to_cpu": [[135, 139], ["torch.load", "utils._upgrade_state_dict", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._upgrade_state_dict"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "model", "=", "model", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference": [[141, 174], ["utils.load_checkpoint_to_cpu", "states.append", "task.build_model", "task.build_model.upgrade_state_dict", "task.build_model.load_state_dict", "ensemble.append", "os.path.exists", "IOError", "utils.override_model_args", "utils.override_model_args"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.override_model_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.override_model_args"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "src_dict", "=", "None", ",", "tgt_dict", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", "\n", "\n", "", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TestModel", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", "\n", "\n", "", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dict", "\n", "\n", "\n", "", "", "class", "TestModel", "(", "FairseqModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "encoder", "=", "TestEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "decoder", "=", "TestIncrementalDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n", "\n", "", "", "class", "TestEncoder", "(", "FairseqEncoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "src_tokens", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.override_model_args": [[176, 181], ["model_arg_overrides.items", "setattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "return", "encoder_out", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "\n", "", "", "class", "TestIncrementalDecoder", "(", "FairseqIncrementalDecoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda": [[183, 201], ["utils.move_to_cuda._move_to_cuda"], "function", ["None"], ["assert", "hasattr", "(", "args", ",", "'beam_probs'", ")", "or", "hasattr", "(", "args", ",", "'probs'", ")", "\n", "args", ".", "max_decoder_positions", "=", "getattr", "(", "args", ",", "'max_decoder_positions'", ",", "100", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bbsz", "=", "prev_output_tokens", ".", "size", "(", "0", ")", "\n", "vocab", "=", "len", "(", "self", ".", "dictionary", ")", "\n", "src_len", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# determine number of steps", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# cache step number", "\n", "            ", "step", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ")", "\n", "if", "step", "is", "None", ":", "\n", "                ", "step", "=", "0", "\n", "", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ",", "step", "+", "1", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._get_full_incremental_state_key": [[206, 216], ["hasattr"], "function", ["None"], ["# define output in terms of raw probs", "\n", "", "if", "hasattr", "(", "self", ".", "args", ",", "'probs'", ")", ":", "\n", "            ", "assert", "self", ".", "args", ".", "probs", ".", "dim", "(", ")", "==", "3", ",", "'expected probs to have size bsz*steps*vocab'", "\n", "probs", "=", "self", ".", "args", ".", "probs", ".", "index_select", "(", "1", ",", "torch", ".", "LongTensor", "(", "steps", ")", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "torch", ".", "FloatTensor", "(", "bbsz", ",", "len", "(", "steps", ")", ",", "vocab", ")", ".", "zero_", "(", ")", "\n", "for", "i", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "# args.beam_probs gives the probability for every vocab element,", "\n", "# starting with eos, then unknown, and then the rest of the vocab", "\n", "                ", "if", "step", "<", "len", "(", "self", ".", "args", ".", "beam_probs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state": [[218, 224], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._get_full_incremental_state_key"], ["", "else", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", "]", "=", "1.0", "\n", "\n", "# random attention", "\n", "", "", "", "attn", "=", "torch", ".", "rand", "(", "bbsz", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "dev", "=", "prev_output_tokens", ".", "device", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state": [[226, 231], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils._get_full_incremental_state_key"], ["\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "_", ")", ":", "\n", "# the decoder returns probabilities directly", "\n", "        ", "probs", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "probs", ".", "log", "(", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict": [[233, 248], ["isinstance", "open", "line.split"], "function", ["None"], ["            ", "return", "probs", "\n", "\n", "", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "max_decoder_positions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.print_embed_overlap": [[250, 255], ["set", "set", "len", "print", "embed_dict.keys", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.parse_embedding": [[257, 275], ["open", "next", "line.rstrip().split", "torch.Tensor", "line.rstrip", "float"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_embedding": [[277, 283], ["range", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.replace_unk": [[285, 297], ["tokenizer.tokenize_line", "enumerate", "tokenizer.tokenize_line", "align_dict.get"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction": [[299, 309], ["tgt_dict.string", "utils.replace_unk", "tgt_dict.encode_line", "tgt_dict.unk_string"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.replace_unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk_string"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.make_positions": [[311, 338], ["make_positions.range_buf.type_as", "tensor.ne", "make_positions.range_buf[].expand_as", "tensor.clone().masked_scatter_", "tensor.ne", "range_buf.expand_as", "tensor.size", "hasattr", "tensor.new", "make_positions.range_buf.numel", "torch.arange", "tensor.ne.long().sum().unsqueeze", "tensor.clone", "torch._dim_arange", "tensor.ne.long().sum().unsqueeze", "tensor.ne.long", "tensor.ne.size", "tensor.ne.size", "tensor.ne.long", "tensor.size", "tensor.ne.long().sum", "tensor.ne.long().sum", "tensor.ne.long", "tensor.ne.long"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad": [[340, 342], ["tensor.ne"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.buffered_arange": [[344, 350], ["hasattr", "torch.LongTensor", "buffered_arange.buf.numel", "torch.arange"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_padding_direction": [[352, 372], ["src_tokens.eq", "src_tokens.size", "buffered_arange().type_as().expand_as", "src_tokens.eq.long().sum", "src_tokens.gather", "src_tokens.eq.any", "torch.remainder", "torch.remainder", "pad_mask[].any", "pad_mask[].any", "buffered_arange().type_as", "src_tokens.eq.long", "utils.buffered_arange"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.buffered_arange"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item": [[374, 380], ["hasattr", "hasattr", "tensor.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.clip_grad_norm_": [[382, 388], ["utils.item", "torch.norm", "tensor.mul_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf": [[390, 393], ["t.float().fill_().type_as", "t.float().fill_", "float", "t.float"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.checkpoint_paths": [[395, 412], ["re.compile", "os.listdir", "enumerate", "re.compile.fullmatch", "os.path.join", "entries.append", "sorted", "int", "len", "pt_regexp.fullmatch.group", "pt_regexp.fullmatch.group", "pt_regexp.fullmatch.groups"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions": [[414, 438], ["isinstance", "isinstance", "min", "tuple", "map", "zip"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module": [[440, 450], ["getattr", "os.path.abspath", "os.path.split", "sys.path.insert", "importlib.import_module", "sys.path.pop"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.pop"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.generate.main": [[19, 183], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "fairseq.utils.load_ensemble_for_inference", "fairseq.utils.load_align_dict", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "tasks.setup_task.build_generator", "print", "torch.cuda.is_available", "getattr", "args.path.split", "model.make_generation_fast_", "fairseq.bleu.SacrebleuScorer", "fairseq.bleu.Scorer", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "len", "eval", "model.half", "model.cuda", "tasks.setup_task.get_batch_iterator", "tgt_dict.pad", "tgt_dict.eos", "tgt_dict.unk", "fairseq.meters.StopwatchMeter.start", "tasks.setup_task.inference_step", "sum", "fairseq.meters.StopwatchMeter.stop", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "sample[].tolist", "fairseq.utils.strip_pad", "enumerate", "bleu.Scorer.result_string", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "len", "tgt_dict.pad", "fairseq.utils.strip_pad().int().cpu", "tasks.setup_task.dataset().src.get_original_text", "tasks.setup_task.dataset().tgt.get_original_text", "fairseq.utils.post_process_prediction", "round", "tasks.setup_task.max_positions", "getattr.string", "tgt_dict.string", "print", "print", "print", "print", "hasattr", "fairseq.utils.strip_pad().int", "min", "hypo[].int().cpu", "print", "tgt_dict.encode_line", "bleu.Scorer.add_string", "bleu.Scorer.add", "model.max_positions", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "len", "hypo[].int().cpu", "fairseq.utils.strip_pad", "hypo[].int", "map", "tgt_dict.pad", "hypo[].int", "hypo[].tolist", "map", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.SacrebleuScorer.add_string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["f\"--beam {beam} --max-tokens {max_tokens} --buffer-size {buffer_size} > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "\n", "", "elif", "gen_subset", "is", "not", "None", ":", "\n", "        ", "generate", "=", "f\"fairseq-generate {data_path} --path {ckpt} --gen-subset {gen_subset} \"", "f\"--beam {beam} --max-tokens {max_tokens} --print-alignment > {system_out}\"", "\n", "os", ".", "system", "(", "generate", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.generate.cli_main": [[185, 189], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.buffered_read": [[26, 37], ["fileinput.input", "len", "buffer.append", "fileinput.hook_encoded", "src_str.strip", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["def", "buffered_read", "(", "input", ",", "buffer_size", ")", ":", "\n", "    ", "buffer", "=", "[", "]", "\n", "with", "fileinput", ".", "input", "(", "files", "=", "[", "input", "]", ",", "openhook", "=", "fileinput", ".", "hook_encoded", "(", "\"utf-8\"", ")", ")", "as", "h", ":", "\n", "        ", "for", "src_str", "in", "h", ":", "\n", "            ", "buffer", ".", "append", "(", "src_str", ".", "strip", "(", ")", ")", "\n", "if", "len", "(", "buffer", ")", ">=", "buffer_size", ":", "\n", "                ", "yield", "buffer", "\n", "buffer", "=", "[", "]", "\n", "\n", "", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "yield", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.make_batches": [[39, 55], ["torch.LongTensor", "task.get_batch_iterator().next_epoch_itr", "task.source_dictionary.encode_line().long", "t.numel", "task.get_batch_iterator", "Batch", "task.source_dictionary.encode_line", "task.build_dataset_for_inference"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_dataset_for_inference"], ["", "", "def", "make_batches", "(", "lines", ",", "args", ",", "task", ",", "max_positions", ")", ":", "\n", "    ", "tokens", "=", "[", "\n", "task", ".", "source_dictionary", ".", "encode_line", "(", "src_str", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "for", "src_str", "in", "lines", "\n", "]", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "[", "t", ".", "numel", "(", ")", "for", "t", "in", "tokens", "]", ")", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "build_dataset_for_inference", "(", "tokens", ",", "lengths", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "itr", ":", "\n", "        ", "yield", "Batch", "(", "\n", "ids", "=", "batch", "[", "'id'", "]", ",", "\n", "src_tokens", "=", "batch", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ",", "src_lengths", "=", "batch", "[", "'net_input'", "]", "[", "'src_lengths'", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.main": [[58, 164], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "tasks.setup_task.build_generator", "fairseq.utils.load_align_dict", "fairseq.utils.resolve_max_positions", "print", "interactive.buffered_read", "torch.cuda.is_available", "args.path.split", "model.make_generation_fast_", "tasks.setup_task.max_positions", "print", "interactive.make_batches", "sorted", "len", "eval", "model.half", "model.cuda", "tasks.setup_task.inference_step", "enumerate", "model.max_positions", "src_tokens.cuda.cuda", "src_lengths.cuda.cuda", "zip", "fairseq.utils.strip_pad", "results.append", "src_dict.string", "print", "fairseq.utils.post_process_prediction", "print", "print", "batch.ids.tolist", "tgt_dict.pad", "min", "print", "len", "hypo[].int().cpu", "hypo[].int().cpu", "map", "hypo[].int", "hypo[].tolist", "map", "hypo[].int", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.buffered_read", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.make_batches", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "import_user_module", "(", "args", ")", "\n", "\n", "if", "args", ".", "buffer_size", "<", "1", ":", "\n", "        ", "args", ".", "buffer_size", "=", "1", "\n", "", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences", "=", "1", "\n", "\n", "", "assert", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", ",", "'--sampling requires --nbest to be equal to --beam'", "\n", "assert", "not", "args", ".", "max_sentences", "or", "args", ".", "max_sentences", "<=", "args", ".", "buffer_size", ",", "'--max-sentences/--batch-size cannot be larger than --buffer-size'", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Setup task, e.g., translation", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "# Set dictionaries", "\n", "src_dict", "=", "task", ".", "source_dictionary", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "# Initialize generator", "\n", "", "", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "\n", "*", "[", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "]", "\n", ")", "\n", "\n", "if", "args", ".", "buffer_size", ">", "1", ":", "\n", "        ", "print", "(", "'| Sentence buffer size:'", ",", "args", ".", "buffer_size", ")", "\n", "", "print", "(", "'| Type the input sentence and press return:'", ")", "\n", "start_id", "=", "0", "\n", "for", "inputs", "in", "buffered_read", "(", "args", ".", "input", ",", "args", ".", "buffer_size", ")", ":", "\n", "        ", "results", "=", "[", "]", "\n", "for", "batch", "in", "make_batches", "(", "inputs", ",", "args", ",", "task", ",", "max_positions", ")", ":", "\n", "            ", "src_tokens", "=", "batch", ".", "src_tokens", "\n", "src_lengths", "=", "batch", ".", "src_lengths", "\n", "if", "use_cuda", ":", "\n", "                ", "src_tokens", "=", "src_tokens", ".", "cuda", "(", ")", "\n", "src_lengths", "=", "src_lengths", ".", "cuda", "(", ")", "\n", "\n", "", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n", "}", "\n", "translations", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ")", "\n", "for", "i", ",", "(", "id", ",", "hypos", ")", "in", "enumerate", "(", "zip", "(", "batch", ".", "ids", ".", "tolist", "(", ")", ",", "translations", ")", ")", ":", "\n", "                ", "src_tokens_i", "=", "utils", ".", "strip_pad", "(", "src_tokens", "[", "i", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", "\n", "results", ".", "append", "(", "(", "start_id", "+", "id", ",", "src_tokens_i", ",", "hypos", ")", ")", "\n", "\n", "# sort output to match input order", "\n", "", "", "for", "id", ",", "src_tokens", ",", "hypos", "in", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "            ", "if", "src_dict", "is", "not", "None", ":", "\n", "                ", "src_str", "=", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "id", ",", "src_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "                ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "else", "None", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "tgt_dict", "=", "tgt_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "id", ",", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", ")", "\n", "if", "args", ".", "print_alignment", ":", "\n", "                    ", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# update running id counter", "\n", "", "", "", "start_id", "+=", "len", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.interactive.cli_main": [[166, 170], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "interactive.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", "interactive", "=", "True", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.main": [[27, 119], ["fairseq.utils.import_user_module", "print", "torch.manual_seed", "fairseq.tasks.setup_task", "train.load_dataset_splits", "tasks.setup_task.build_model", "tasks.setup_task.build_criterion", "print", "print", "print", "fairseq.utils.resolve_max_positions", "tasks.setup_task.dataset().get_dummy_batch", "tasks.setup_task.dataset().get_dummy_batch", "fairseq.trainer.Trainer", "print", "print", "tasks.setup_task.get_batch_iterator", "fairseq.trainer.Trainer.get_lr", "fairseq.meters.StopwatchMeter", "fairseq.meters.StopwatchMeter.start", "args.valid_subset.split", "fairseq.meters.StopwatchMeter.stop", "print", "torch.cuda.is_available", "torch.cuda.set_device", "fairseq.distributed_utils.distributed_init", "print", "tasks.setup_task.max_positions", "task.build_model.max_positions", "train.load_checkpoint", "fairseq.trainer.Trainer.dummy_train_step", "train.train", "fairseq.trainer.Trainer.lr_step", "sum", "sum", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "fairseq.trainer.Trainer.get_num_updates", "train.validate", "train.save_checkpoint", "socket.gethostname", "p.numel", "p.numel", "task.build_model.parameters", "task.build_model.parameters"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_dataset_splits", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.dummy_train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.validate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint"], ["\n", "\n", "", "def", "train", "(", "databin_path", ",", "model_config", ",", "ckpt_dir", ",", "restore_ckpt", ",", "ngpu", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "ckpt_dir", ",", "exist_ok", "=", "True", ")", "\n", "logging", ".", "info", "(", "f\"[Train] working on {ckpt_dir}\"", ")", "\n", "\n", "if", "ngpu", ">", "1", ":", "\n", "        ", "prompt", "=", "f\"python -m torch.distributed.launch --nproc_per_node {ngpu} $(which fairseq-train) {databin_path} \"", "f\"{model_config} --save-dir {ckpt_dir} \"", "\n", "", "else", ":", "\n", "        ", "prompt", "=", "f\"fairseq-train {databin_path} {model_config} --save-dir {ckpt_dir} \"", "\n", "", "logging", ".", "info", "(", "f\"[Train] {prompt}\"", ")", "\n", "\n", "if", "restore_ckpt", "is", "not", "None", ":", "\n", "        ", "finetune_ckpt", "=", "os", ".", "path", ".", "basename", "(", "util", ".", "change_ckpt_dir", "(", "restore_ckpt", ",", "ckpt_dir", ")", ")", "\n", "logging", ".", "info", "(", "f\"[Train] copy the ckpt {restore_ckpt} into {finetune_ckpt}\"", ")", "\n", "os", ".", "system", "(", "f\"cp {restore_ckpt} {finetune_ckpt}\"", ")", "\n", "\n", "prompt", "+=", "f\"--restore-file {finetune_ckpt} \"", "\n", "", "os", ".", "system", "(", "prompt", ")", "\n", "\n", "\n", "", "def", "find_restore", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", ":", "\n", "    ", "if", "prev_model_output_dir", "is", "None", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "highest_fscore", ",", "highest_ckpt", "=", "util", ".", "find_highest_score", "(", "prev_model_output_dir", ",", "ori_path", ",", "scorer_type", ")", "\n", "logging", ".", "info", "(", "f\"[Train] highest fscore: {highest_fscore}, ckpt: {highest_ckpt}\"", ")", "\n", "if", "highest_fscore", "==", "0", "and", "highest_ckpt", "==", "'.pt'", ":", "\n", "        ", "logging", ".", "error", "(", "f\"[Train] cannot find the highest ckpt\"", ")", "\n", "exit", "(", ")", "\n", "", "return", "highest_fscore", ",", "highest_ckpt", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--track\"", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--train-mode\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--ngpu\"", ",", "type", "=", "int", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "5e-4", ")", "\n", "parser", ".", "add_argument", "(", "\"--dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.3", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-epoch\"", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "\"--reset\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--prev-model-output-dir\"", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "main", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train": [[121, 182], ["epoch_itr.next_epoch_itr", "fairseq.data.iterators.GroupedIterator", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "enumerate", "train.get_training_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "args.valid_subset.split", "trainer.train_step", "train.get_training_stats", "trainer.train_step.items", "progress_bar.build_progress_bar.log", "trainer.get_num_updates", "trainer.get_meter", "len", "fairseq.meters.AverageMeter", "trainer.get_meter().reset", "train.validate", "train.save_checkpoint", "trainer.get_meter.reset", "extra_meters[].update", "extra_meters[].update", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_training_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_training_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.validate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_training_stats": [[184, 207], ["collections.OrderedDict", "trainer.get_meter", "train.get_perplexity", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_num_updates", "trainer.get_lr", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "round", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_perplexity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.validate": [[209, 258], ["task.get_batch_iterator().next_epoch_itr", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "train.get_valid_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "valid_losses.append", "trainer.get_meter", "trainer.valid_step", "trainer.valid_step.items", "task.get_batch_iterator", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_num_updates", "task.dataset", "fairseq.utils.resolve_max_positions", "task.max_positions", "trainer.get_model().max_positions", "trainer.get_model"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_valid_stats", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.valid_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_model"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_valid_stats": [[260, 273], ["collections.OrderedDict", "trainer.get_meter", "train.get_perplexity", "trainer.get_num_updates", "hasattr", "trainer.get_meter", "min", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_perplexity", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_meter"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.get_perplexity": [[275, 280], ["math.pow", "float"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint": [[282, 332], ["epoch_itr.end_of_epoch", "trainer.get_num_updates", "collections.OrderedDict", "getattr", "hasattr", "min", "epoch_itr.state_dict", "extra_state.update", "os.path.join", "len", "fairseq.utils.checkpoint_paths", "fairseq.utils.checkpoint_paths", "fairseq.distributed_utils.is_master", "collections.OrderedDict.items", "trainer.save_checkpoint", "os.path.lexists", "os.path.lexists", "hasattr", "os.remove", "os.remove"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.end_of_epoch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.checkpoint_paths", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.checkpoint_paths", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.save_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint": [[334, 359], ["os.makedirs", "os.path.isabs", "os.path.isfile", "os.path.join", "trainer.load_checkpoint", "print", "eval", "epoch_itr.load_state_dict", "print", "trainer.lr_step", "trainer.lr_step_update", "trainer.get_num_updates", "trainer.get_num_updates"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.lr_step_update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.get_num_updates"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_dataset_splits": [[361, 374], ["task.load_dataset", "itertools.count", "task.load_dataset", "str"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.distributed_main": [[376, 381], ["train.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.cli_main": [[383, 408], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "fairseq.distributed_utils.infer_init_method", "train.distributed_main", "random.randint", "torch.multiprocessing.spawn", "train.main", "print", "max"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.distributed_utils.infer_init_method", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.distributed_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.main": [[25, 234], ["fairseq.utils.import_user_module", "print", "os.makedirs", "fairseq.tasks.get_task", "build_dictionary.save", "preprocess.main.make_all"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.get_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save"], ["\n", "\n", "", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# 1. word-tokenize", "\n", "parser", ".", "add_argument", "(", "\"--max_tokens\"", ",", "type", "=", "int", ",", "default", "=", "150", ",", "\n", "help", "=", "\"Maximum number of tokens in a sample\"", ")", "\n", "\n", "# 2. train bpe model", "\n", "parser", ".", "add_argument", "(", "\"--vocab_size\"", ",", "type", "=", "int", ",", "default", "=", "32000", ",", "\n", "help", "=", "\"vocabulary size\"", ")", "\n", "\n", "# 3. perturbation -> bpe-tokenize", "\n", "parser", ".", "add_argument", "(", "\"--min_cnt\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_change_prob\"", ",", "type", "=", "float", ",", "default", "=", ".9", ")", "\n", "parser", ".", "add_argument", "(", "\"--type_change_prob\"", ",", "type", "=", "float", ",", "default", "=", ".1", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "1", ",", "12", ",", "5", "]", ",", "\n", "help", "=", "\"list of n_epochs of gutenberg, tatoeba, and wiki103\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "fp", "=", "filepath", ".", "FilePath", "(", ")", "\n", "fp", ".", "make_dirs", "(", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0. Download data\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 0-1. Download Gutenberg Text\"", ")", "\n", "maybe_download", "(", "fp", ".", "gutenberg", ",", "\n", "f\"gdown https://drive.google.com/uc?id=0B2Mzhc7popBga2RkcWZNcjlRTGM & \"", "\n", "f\"unzip Gutenberg.zip -d {fp.gutenberg} & \"", "\n", "f\"rm Gutenberg.zip\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-2. Download Tatoeba\"", ")", "\n", "maybe_download", "(", "fp", ".", "tatoeba", ",", "\n", "f\"wget http://downloads.tatoeba.org/exports/sentences.tar.bz2 & \"", "\n", "f\"tar -C {fp.tatoeba} -xjf sentences.tar.bz2 &\"", "\n", "f\"rm sentences.tar.bz2\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-3. Download Wiki-103\"", ")", "\n", "maybe_download", "(", "fp", ".", "wiki103", ",", "\n", "f\"wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip & \"", "\n", "f\"sleep 10s & \"", "\n", "f\"unzip wikitext-103-v1.zip -d {fp.wiki103} & \"", "\n", "f\"mv {fp.wiki103}/wikitext-103/wiki.train.tokens {fp.wiki103}/wiki.train.tokens &\"", "\n", "f\"rm wikitext-103-v1.zip\"", "\n", ")", "\n", "\n", "\n", "# TODO: make these directories at filepath.py", "\n", "# make directories", "\n", "for", "dir_name", "in", "[", "f\"{fp.bea19}\"", ",", "f\"{fp.wi}\"", ",", "f\"{fp.fce}\"", ",", "f\"{fp.conll2013}\"", ",", "f\"{fp.conll2014}\"", "]", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "mkdir", "(", "dir_name", ")", "\n", "", "except", ":", "pass", "\n", "\n", "", "logging", ".", "info", "(", "\"STEP 0-4. Download FCE\"", ")", "\n", "maybe_download", "(", "f\"{fp.bea19}/fce\"", ",", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/fce_v2.1.bea19.tar.gz & \"", "\n", "f\"tar -C {fp.bea19} -xvzf fce_v2.1.bea19.tar.gz $\"", "\n", "f\"rm fce_v2.1.bea19.tar.gz\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-5. Download WI+LOCNESS\"", ")", "\n", "maybe_download", "(", "f\"{fp.bea19}/wi+locness\"", ",", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz & \"", "\n", "f\"tar -C {fp.bea19} -xvzf wi+locness_v2.1.bea19.tar.gz &\"", "\n", "f\"rm wi+locness_v2.1.bea19.tar.gz & \"", "\n", "f\"wget https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.test.orig & \"", "\n", "f\"mv ABCN.bea19.test.orig {fp.WI_TEST_ORI}\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-6. Download LANG8\"", ")", "\n", "logging", ".", "info", "(", "f\"NO PUBLIC DATA AVAILABLE.\\n \"", "\n", "f\"Please visit 'https://www.cl.cam.ac.uk/research/nl/bea2019st/' to obtain data and extract file to {fp.nucle_m2}/*m2\"", ")", "\n", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-7. Download Conll 2013, 2014\"", ")", "\n", "maybe_download", "(", "fp", ".", "conll2013", ",", "\n", "f\"wget https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz & \"", "\n", "f\"tar -C {fp.conll2013} -xvzf &\"", "\n", "f\"rm release2.3.1.tar.gz\"", ")", "\n", "\n", "maybe_download", "(", "fp", ".", "conll2014", ",", "\n", "f\"wget https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz & \"", "\n", "f\"tar -C {fp.conll2014} -xvzf conll14st-test-data.tar.gz &\"", "\n", "f\"rm conll14st-test-data.tar.gz\"", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 0-8. Download language model\"", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filepath", ".", "Path", ".", "lm_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filepath", ".", "Path", ".", "lm_dict", ")", ",", "exist_ok", "=", "True", ")", "\n", "maybe_download", "(", "filepath", ".", "Path", ".", "lm_databin", ",", "\n", "f\"wget https://dl.fbaipublicfiles.com/fairseq/models/wiki103_fconv_lm.tar.bz2 & \"", "\n", "f\"tar -xvf wiki103_fconv_lm.tar.bz2 & \"", "\n", "f\"mv wiki103.pt {filepath.Path.lm_path} & \"", "\n", "f\"mv dict.txt {filepath.Path.lm_dict} & \"", "\n", "f\"rm wiki103_fconv_lm.tar.bz2\"", ")", "\n", "\n", "# logging.info(\"STEP 0-8. Download M2 Scorer\")", "\n", "# maybe_download(fp.m2scorer, \"wget https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\")", "\n", "# maybe_download(fp.errant, \"git clone https://github.com/chrisjbryant/errant.git\")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1. Word-tokenize the original files and merge them\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 1-1. gutenberg\"", ")", "\n", "fpaths", "=", "sorted", "(", "glob", "(", "f'{fp.gutenberg}/Gutenberg/txt/*.txt'", ")", ")", "\n", "maybe_do", "(", "fp", ".", "GUTENBERG_TXT", ",", "word_tokenize", ".", "gutenberg", ",", "\n", "(", "fpaths", ",", "fp", ".", "GUTENBERG_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1-2. tatoeba\"", ")", "\n", "fpath", "=", "f'{fp.tatoeba}/sentences.csv'", "\n", "maybe_do", "(", "fp", ".", "TATOEBA_TXT", ",", "word_tokenize", ".", "tatoeba", ",", "\n", "(", "fpath", ",", "fp", ".", "TATOEBA_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 1-3. wiki103\"", ")", "\n", "fpath", "=", "f'{fp.wiki103}/wiki.train.tokens'", "\n", "maybe_do", "(", "fp", ".", "WIKI103_TXT", ",", "word_tokenize", ".", "wiki103", ",", "\n", "(", "fpath", ",", "fp", ".", "WIKI103_TXT", ",", "args", ".", "max_tokens", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 2. Train bpe model\"", ")", "\n", "maybe_do", "(", "fp", ".", "BPE_MODEL", ",", "bpe", ".", "train", ",", "\n", "(", "fp", ".", "GUTENBERG_TXT", ",", "fp", ".", "BPE_MODEL", ".", "replace", "(", "\".model\"", ",", "\"\"", ")", ",", "args", ".", "vocab_size", ",", "1.0", ",", "'bpe'", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 3. Split wi.dev into wi.dev.3k and wi.dev.1k\"", ")", "\n", "fpaths", "=", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/*.dev.gold.bea19.m2'", ")", ")", "\n", "wi_dev_3k_m2", "=", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.3k.m2'", "\n", "wi_dev_1k_m2", "=", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.1k.m2'", "\n", "maybe_do", "(", "wi_dev_3k_m2", ",", "m2", ".", "split_m2", ",", "\n", "(", "fpaths", ",", "wi_dev_3k_m2", ",", "wi_dev_1k_m2", ",", "0.75", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4. Perturb and make parallel files\"", ")", "\n", "for", "track_no", "in", "(", "\"1\"", ",", "\"3\"", ",", "\"0\"", ")", ":", "\n", "        ", "logging", ".", "info", "(", "f\"Track {track_no}\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 4-1. writing perturbation scenario\"", ")", "\n", "if", "track_no", "==", "\"1\"", ":", "\n", "            ", "dir", "=", "f\"{fp.wi_m2}/*train*.m2\"", "\n", "", "elif", "track_no", "==", "\"3\"", ":", "\n", "            ", "dir", "=", "f\"{fp.wi_m2}/*dev.*3k*.m2\"", "\n", "", "else", ":", "\n", "            ", "dir", "=", "f\"{fp.nucle_m2}/*nucle*.m2\"", "\n", "", "word2ptbs", "=", "perturb", ".", "make_word2ptbs", "(", "sorted", "(", "glob", "(", "dir", ")", ")", ",", "args", ".", "min_cnt", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-2. gutenberg\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.GUTENBERG_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "GUTENBERG_TXT", ",", "\n", "eval", "(", "f\"fp.GUTENBERG_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.GUTENBERG_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "0", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-3. tatoeba\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.TATOEBA_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "TATOEBA_TXT", ",", "\n", "eval", "(", "f\"fp.TATOEBA_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.TATOEBA_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "1", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 4-4. wiki103\"", ")", "\n", "maybe_do", "(", "eval", "(", "f\"fp.WIKI103_ORI{track_no}\"", ")", ",", "perturb", ".", "do", ",", "\n", "(", "word2ptbs", ",", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WIKI103_TXT", ",", "\n", "eval", "(", "f\"fp.WIKI103_ORI{track_no}\"", ")", ",", "eval", "(", "f\"fp.WIKI103_COR{track_no}\"", ")", ",", "args", ".", "n_epochs", "[", "2", "]", ",", "\n", "args", ".", "word_change_prob", ",", "args", ".", "type_change_prob", ")", ")", "\n", "\n", "", "logging", ".", "info", "(", "\"STEP 5. m2 to parallel\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 5-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.fce_m2}/*m2'", ")", ")", ",", "fp", ".", "FCE_ORI", ",", "fp", ".", "FCE_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.lang8_m2}/*m2'", ")", ")", ",", "fp", ".", "LANG8_ORI", ",", "fp", ".", "LANG8_COR", ",", "True", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.nucle_m2}/*m2'", ")", ")", ",", "fp", ".", "NUCLE_ORI", ",", "fp", ".", "NUCLE_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/*train*m2'", ")", ")", ",", "fp", ".", "WI_TRAIN_ORI", ",", "fp", ".", "WI_TRAIN_COR", ",", "False", ",", "True", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-5. wi dev\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.m2'", ")", ")", ",", "fp", ".", "WI_DEV_ORI", ",", "fp", ".", "WI_DEV_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "# logging.info(\"STEP 5-6. wi test\")", "\n", "# if os.path.exists(WI_TEST_ORI): logging.info(f\"skip this step as {WI_TEST_ORI} already exists.\")", "\n", "# else: m2.m2_to_parallel(glob(f'{wi_m2}/*test*m2'), WI_TEST_ORI, WI_TEST_COR, False, True)", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-7. wi dev 3k. For track 3 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.3k.m2'", ")", ")", ",", "fp", ".", "WI_DEV_3K_ORI", ",", "fp", ".", "WI_DEV_3K_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-8. wi dev 1k. For track 3 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_1K_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.wi_m2}/ABCN.dev.gold.bea19.1k.m2'", ")", ")", ",", "fp", ".", "WI_DEV_1K_ORI", ",", "fp", ".", "WI_DEV_1K_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-9. conll2013. For track 0 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2013_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.conll2013_m2}/official-preprocessed.m2'", ")", ")", ",", "fp", ".", "CONLL2013_ORI", ",", "fp", ".", "CONLL2013_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 5-10. conll2014. For track 0 only.\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2014_ORI", ",", "m2", ".", "m2_to_parallel", ",", "\n", "(", "sorted", "(", "glob", "(", "f'{fp.conll2014_m2}/official-2014.combined.m2'", ")", ")", ",", "fp", ".", "CONLL2014_ORI", ",", "fp", ".", "CONLL2014_COR", ",", "False", ",", "False", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6. spell-check\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 6-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "FCE_ORI", ",", "fp", ".", "FCE_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "LANG8_ORI", ",", "fp", ".", "LANG8_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "NUCLE_ORI", ",", "fp", ".", "NUCLE_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_TRAIN_ORI", ",", "fp", ".", "WI_TRAIN_SP_ORI", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.binarize": [[236, 248], ["fairseq.data.indexed_dataset.IndexedDatasetBuilder", "fairseq.binarizer.Binarizer.binarize", "indexed_dataset.IndexedDatasetBuilder.finalize", "preprocess.dataset_dest_file", "indexed_dataset.IndexedDatasetBuilder.add_item", "preprocess.dataset_dest_file"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.binarize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_file"], ["maybe_do", "(", "fp", ".", "WI_DEV_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_ORI", ",", "fp", ".", "WI_DEV_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-6. wi test\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TEST_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_TEST_ORI", ",", "fp", ".", "WI_TEST_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-7. wi dev 3k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_3K_ORI", ",", "fp", ".", "WI_DEV_3K_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-8. wi dev 1k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_1K_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "WI_DEV_1K_ORI", ",", "fp", ".", "WI_DEV_1K_SP_ORI", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 6-9. conll 2013\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2013_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "CONLL2013_ORI", ",", "fp", ".", "CONLL2013_SP_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_prefix": [[250, 256], ["None"], "function", ["None"], ["logging", ".", "info", "(", "\"STEP 6-10. conll 2014\"", ")", "\n", "maybe_do", "(", "fp", ".", "CONLL2014_SP_ORI", ",", "spell", ".", "check", ",", "(", "fp", ".", "CONLL2014_ORI", ",", "fp", ".", "CONLL2014_SP_ORI", ")", ")", "\n", "\n", "#", "\n", "logging", ".", "info", "(", "\"STEP 7. bpe-tokenize\"", ")", "\n", "logging", ".", "info", "(", "\"STEP 7-1. fce\"", ")", "\n", "maybe_do", "(", "fp", ".", "FCE_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "FCE_SP_ORI", ",", "fp", ".", "FCE_TOK_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_file": [[258, 261], ["preprocess.dataset_dest_prefix"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.dataset_dest_prefix"], ["\n", "logging", ".", "info", "(", "\"STEP 7-2. lang8\"", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "LANG8_SP_ORI", ",", "fp", ".", "LANG8_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "LANG8_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "LANG8_COR", ",", "fp", ".", "LANG8_TOK_COR", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.get_offsets": [[263, 265], ["fairseq.binarizer.Binarizer.find_offsets"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.Binarizer.find_offsets"], ["logging", ".", "info", "(", "\"STEP 7-3. nucle\"", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "NUCLE_SP_ORI", ",", "fp", ".", "NUCLE_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "NUCLE_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "NUCLE_COR", ",", "fp", ".", "NUCLE_TOK_COR", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.merge_files": [[267, 274], ["fairseq.data.indexed_dataset.IndexedDatasetBuilder", "indexed_dataset.IndexedDatasetBuilder.finalize", "indexed_dataset.IndexedDatasetBuilder.merge_file_", "os.remove", "os.remove", "fairseq.data.indexed_dataset.data_file_path", "fairseq.data.indexed_dataset.index_file_path"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.merge_file_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.index_file_path"], ["logging", ".", "info", "(", "\"STEP 7-4. wi train\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TRAIN_SP_ORI", ",", "fp", ".", "WI_TRAIN_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "WI_TRAIN_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TRAIN_COR", ",", "fp", ".", "WI_TRAIN_TOK_COR", ")", ")", "\n", "\n", "logging", ".", "info", "(", "\"STEP 7-5. wi dev\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_SP_ORI", ",", "fp", ".", "WI_DEV_TOK_ORI", ")", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_TOK_COR", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_COR", ",", "fp", ".", "WI_DEV_TOK_COR", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.preprocess.cli_main": [[276, 280], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["maybe_do", "(", "fp", ".", "WI_TEST_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_TEST_SP_ORI", ",", "fp", ".", "WI_TEST_TOK_ORI", ")", ")", "\n", "# maybe_do(fp.WI_TEST_TOK_COR, bpe.bpe_tokenize, (fp.BPE_MODEL, fp.WI_TEST_COR, fp.WI_TEST_TOK_COR))", "\n", "\n", "logging", ".", "info", "(", "\"STEP 7-7. wi dev 3k\"", ")", "\n", "maybe_do", "(", "fp", ".", "WI_DEV_3K_TOK_ORI", ",", "bpe", ".", "bpe_tokenize", ",", "(", "fp", ".", "BPE_MODEL", ",", "fp", ".", "WI_DEV_3K_SP_ORI", ",", "fp", ".", "WI_DEV_3K_TOK_ORI", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.score.get_parser": [[20, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "parser", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.score.main": [[35, 75], ["score.get_parser", "get_parser.parse_args", "print", "os.path.exists", "fairseq.data.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "line", "\n", "\n", "", "", "", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "import", "sacrebleu", "\n", "\n", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "print", "(", "sacrebleu", ".", "corpus_bleu", "(", "fdsys", ",", "[", "fdref", "]", ")", ")", "\n", "", "", "", "else", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                    ", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.__init__": [[23, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "word", ",", "is_bpe", ")", ":", "\n", "        ", "self", ".", "word", "=", "word", "\n", "self", ".", "is_bpe", "=", "is_bpe", "\n", "self", ".", "log_prob", "=", "0", "\n", "self", ".", "next_word_prob", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "missing_next_words", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add": [[31, 42], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "log_prob", ",", "next_word_prob", ")", ":", "\n", "        ", "\"\"\" increments counters for the sum of log probs of current word and next\n            word (given context ending at current word). Since the next word might be at the end of the example,\n            or it might be not counted because it is not an ending subword unit,\n            also keeps track of how many of those we have seen \"\"\"", "\n", "if", "next_word_prob", "is", "not", "None", ":", "\n", "            ", "self", ".", "next_word_prob", "+=", "next_word_prob", "\n", "", "else", ":", "\n", "            ", "self", ".", "missing_next_words", "+=", "1", "\n", "", "self", ".", "log_prob", "+=", "log_prob", "\n", "self", ".", "count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.__str__": [[43, 46], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "'{}\\t{}\\t{}\\t{}\\t{}\\t{}'", ".", "format", "(", "self", ".", "word", ",", "self", ".", "count", ",", "self", ".", "log_prob", ",", "self", ".", "is_bpe", ",", "\n", "self", ".", "next_word_prob", ",", "self", ".", "count", "-", "self", ".", "missing_next_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.main": [[48, 186], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.utils.load_ensemble_for_inference", "vars().keys", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "print", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "fairseq.sequence_scorer.SequenceScorer", "dict", "print", "print", "torch.cuda.is_available", "parsed_args.path.split", "model.make_generation_fast_", "len", "len", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "sorted", "eval", "vars", "setattr", "len", "model.half", "model.cuda", "sum", "tasks.setup_task.get_batch_iterator", "args.remove_bpe.rstrip", "set", "fairseq.meters.StopwatchMeter.start", "fairseq.sequence_scorer.SequenceScorer.generate", "fairseq.meters.StopwatchMeter.stop", "fairseq.meters.TimeMeter.update", "t.log", "numpy.exp", "dict.values", "print", "getattr", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "inf_scores.any", "pos_scores.sum().cpu", "p.numel", "tasks.setup_task.dataset", "fairseq.utils.resolve_max_positions", "range", "pos_scores.eq", "pos_scores.eq", "print", "pos_scores.numel", "range", "round", "models[].parameters", "range", "tasks.setup_task.dictionary[].endswith", "float", "float", "tasks.setup_task.target_dictionary.string", "pos_scores.sum", "len", "[].item", "print", "len", "len", "[].item", "word_prob.append", "dict.setdefault().add", "model.max_positions", "len", "pos_scores[].item", "inf_scores.nonzero", "pos_scores[].item", "pos_scores[].item", "dict.setdefault", "eval_lm.WordStat"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault"], ["", "", "def", "main", "(", "parsed_args", ")", ":", "\n", "    ", "assert", "parsed_args", ".", "path", "is", "not", "None", ",", "'--path required for evaluation!'", "\n", "\n", "import_user_module", "(", "parsed_args", ")", "\n", "\n", "print", "(", "parsed_args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "parsed_args", ".", "cpu", "\n", "\n", "task", "=", "tasks", ".", "setup_task", "(", "parsed_args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "parsed_args", ".", "path", ")", ")", "\n", "models", ",", "args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "parsed_args", ".", "path", ".", "split", "(", "':'", ")", ",", "task", ",", "model_arg_overrides", "=", "eval", "(", "parsed_args", ".", "model_overrides", ")", ",", "\n", ")", "\n", "\n", "for", "arg", "in", "vars", "(", "parsed_args", ")", ".", "keys", "(", ")", ":", "\n", "        ", "if", "arg", "not", "in", "{", "'self_target'", ",", "'future_target'", ",", "'past_target'", ",", "'tokens_per_sample'", ",", "'output_size_dictionary'", "}", ":", "\n", "            ", "setattr", "(", "args", ",", "arg", ",", "getattr", "(", "parsed_args", ",", "arg", ")", ")", "\n", "", "", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load dataset splits", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation and set the source and dest dicts on the model (required by scorer)", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "", "assert", "len", "(", "models", ")", ">", "0", "\n", "\n", "print", "(", "'num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "models", "[", "0", "]", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", "or", "36000", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "*", "[", "\n", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "\n", "]", ")", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ")", "\n", "\n", "score_sum", "=", "0.", "\n", "count", "=", "0", "\n", "\n", "if", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "remove_bpe", "==", "'sentencepiece'", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "bpe_cont", "=", "args", ".", "remove_bpe", ".", "rstrip", "(", ")", "\n", "bpe_toks", "=", "set", "(", "i", "for", "i", "in", "range", "(", "len", "(", "task", ".", "dictionary", ")", ")", "if", "task", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont", ")", ")", "\n", "", "bpe_len", "=", "len", "(", "bpe_cont", ")", "\n", "", "else", ":", "\n", "        ", "bpe_toks", "=", "None", "\n", "bpe_len", "=", "0", "\n", "\n", "", "word_stats", "=", "dict", "(", ")", "\n", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "scorer", ".", "generate", "(", "models", ",", "sample", ")", "\n", "gen_timer", ".", "stop", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "\n", "for", "hypos_i", "in", "hypos", ":", "\n", "                ", "hypo", "=", "hypos_i", "[", "0", "]", "\n", "pos_scores", "=", "hypo", "[", "'positional_scores'", "]", "\n", "\n", "skipped_toks", "=", "0", "\n", "if", "bpe_toks", "is", "not", "None", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", "-", "1", ")", ":", "\n", "                        ", "if", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "in", "bpe_toks", ":", "\n", "                            ", "skipped_toks", "+=", "1", "\n", "pos_scores", "[", "i", "+", "1", "]", "+=", "pos_scores", "[", "i", "]", "\n", "pos_scores", "[", "i", "]", "=", "0", "\n", "\n", "", "", "", "inf_scores", "=", "pos_scores", ".", "eq", "(", "float", "(", "'inf'", ")", ")", "|", "pos_scores", ".", "eq", "(", "float", "(", "'-inf'", ")", ")", "\n", "if", "inf_scores", ".", "any", "(", ")", ":", "\n", "                    ", "print", "(", "'| Skipping tokens with inf scores:'", ",", "\n", "task", ".", "target_dictionary", ".", "string", "(", "hypo", "[", "'tokens'", "]", "[", "inf_scores", ".", "nonzero", "(", ")", "]", ")", ")", "\n", "pos_scores", "=", "pos_scores", "[", "(", "~", "inf_scores", ")", ".", "nonzero", "(", ")", "]", "\n", "", "score_sum", "+=", "pos_scores", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "count", "+=", "pos_scores", ".", "numel", "(", ")", "-", "skipped_toks", "\n", "\n", "if", "args", ".", "output_word_probs", "or", "args", ".", "output_word_stats", ":", "\n", "                    ", "w", "=", "''", "\n", "word_prob", "=", "[", "]", "\n", "is_bpe", "=", "False", "\n", "for", "i", "in", "range", "(", "len", "(", "hypo", "[", "'tokens'", "]", ")", ")", ":", "\n", "                        ", "w_ind", "=", "hypo", "[", "'tokens'", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "w", "+=", "task", ".", "dictionary", "[", "w_ind", "]", "\n", "if", "bpe_toks", "is", "not", "None", "and", "w_ind", "in", "bpe_toks", ":", "\n", "                            ", "w", "=", "w", "[", ":", "-", "bpe_len", "]", "\n", "is_bpe", "=", "True", "\n", "", "else", ":", "\n", "                            ", "word_prob", ".", "append", "(", "(", "w", ",", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "next_prob", "=", "None", "\n", "ind", "=", "i", "+", "1", "\n", "while", "ind", "<", "len", "(", "hypo", "[", "'tokens'", "]", ")", ":", "\n", "                                ", "if", "pos_scores", "[", "ind", "]", ".", "item", "(", ")", "!=", "0", ":", "\n", "                                    ", "next_prob", "=", "pos_scores", "[", "ind", "]", "\n", "break", "\n", "", "ind", "+=", "1", "\n", "\n", "", "word_stats", ".", "setdefault", "(", "w", ",", "WordStat", "(", "w", ",", "is_bpe", ")", ")", ".", "add", "(", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ",", "next_prob", ")", "\n", "is_bpe", "=", "False", "\n", "w", "=", "''", "\n", "", "", "if", "args", ".", "output_word_probs", ":", "\n", "                        ", "print", "(", "'\\t'", ".", "join", "(", "'{} [{:2f}]'", ".", "format", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "for", "x", "in", "word_prob", ")", ")", "\n", "\n", "", "", "", "wps_meter", ".", "update", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "\n", "", "", "avg_nll_loss", "=", "-", "score_sum", "/", "count", "\n", "print", "(", "'| Evaluated {} tokens in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "print", "(", "'| Loss: {:.4f}, Perplexity: {:.2f}'", ".", "format", "(", "avg_nll_loss", ",", "np", ".", "exp", "(", "avg_nll_loss", ")", ")", ")", "\n", "\n", "if", "args", ".", "output_word_stats", ":", "\n", "        ", "for", "ws", "in", "sorted", "(", "word_stats", ".", "values", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", ".", "count", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "ws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.cli_main": [[188, 192], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_encode.main": [[17, 96], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "len", "len", "contextlib.ExitStack", "enumerate", "print", "print", "spm.SentencePieceProcessor.EncodeAsPieces", "encode.strip", "zip", "list", "list", "stack.enter_context", "stack.enter_context", "len", "spm_encode.main.encode"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"sentencepiece model to use for encoding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inputs\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"input files to filter/encode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputs\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"path to save encoded outputs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_format\"", ",", "choices", "=", "[", "\"piece\"", ",", "\"id\"", "]", ",", "default", "=", "\"piece\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min-len\"", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"filter sentence pairs with fewer than N tokens\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-len\"", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"filter sentence pairs with more than N tokens\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "len", "(", "args", ".", "inputs", ")", "==", "len", "(", "args", ".", "outputs", ")", ",", "\"number of input and output paths should match\"", "\n", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "args", ".", "model", ")", "\n", "\n", "if", "args", ".", "output_format", "==", "\"piece\"", ":", "\n", "        ", "def", "encode", "(", "l", ")", ":", "\n", "            ", "return", "sp", ".", "EncodeAsPieces", "(", "l", ")", "\n", "", "", "elif", "args", ".", "output_format", "==", "\"id\"", ":", "\n", "        ", "def", "encode", "(", "l", ")", ":", "\n", "            ", "return", "list", "(", "map", "(", "str", ",", "sp", ".", "EncodeAsIds", "(", "l", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "args", ".", "min_len", "is", "not", "None", "or", "args", ".", "max_len", "is", "not", "None", ":", "\n", "        ", "def", "valid", "(", "line", ")", ":", "\n", "            ", "return", "(", "\n", "(", "args", ".", "min_len", "is", "None", "or", "len", "(", "line", ")", ">=", "args", ".", "min_len", ")", "\n", "and", "(", "args", ".", "max_len", "is", "None", "or", "len", "(", "line", ")", "<=", "args", ".", "max_len", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "def", "valid", "(", "lines", ")", ":", "\n", "            ", "return", "True", "\n", "\n", "", "", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "\n", "        ", "inputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "input", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "if", "input", "!=", "\"-\"", "else", "sys", ".", "stdin", "\n", "for", "input", "in", "args", ".", "inputs", "\n", "]", "\n", "outputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "output", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "if", "output", "!=", "\"-\"", "else", "sys", ".", "stdout", "\n", "for", "output", "in", "args", ".", "outputs", "\n", "]", "\n", "\n", "stats", "=", "{", "\n", "\"num_empty\"", ":", "0", ",", "\n", "\"num_filtered\"", ":", "0", ",", "\n", "}", "\n", "\n", "def", "encode_line", "(", "line", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", ">", "0", ":", "\n", "                ", "line", "=", "encode", "(", "line", ")", "\n", "if", "valid", "(", "line", ")", ":", "\n", "                    ", "return", "line", "\n", "", "else", ":", "\n", "                    ", "stats", "[", "\"num_filtered\"", "]", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "stats", "[", "\"num_empty\"", "]", "+=", "1", "\n", "", "return", "None", "\n", "\n", "", "for", "i", ",", "lines", "in", "enumerate", "(", "zip", "(", "*", "inputs", ")", ",", "start", "=", "1", ")", ":", "\n", "            ", "enc_lines", "=", "list", "(", "map", "(", "encode_line", ",", "lines", ")", ")", "\n", "if", "not", "any", "(", "enc_line", "is", "None", "for", "enc_line", "in", "enc_lines", ")", ":", "\n", "                ", "for", "enc_line", ",", "output_h", "in", "zip", "(", "enc_lines", ",", "outputs", ")", ":", "\n", "                    ", "print", "(", "\" \"", ".", "join", "(", "enc_line", ")", ",", "file", "=", "output_h", ")", "\n", "", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "                ", "print", "(", "\"processed {} lines\"", ".", "format", "(", "i", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "", "print", "(", "\"skipped {} empty lines\"", ".", "format", "(", "stats", "[", "\"num_empty\"", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "\"filtered {} lines\"", ".", "format", "(", "stats", "[", "\"num_filtered\"", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.get_parser": [[16, 25], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'writes text from binarized file to stdout'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dict'", ",", "metavar", "=", "'FP'", ",", "required", "=", "True", ",", "help", "=", "'dictionary containing known words'", ")", "\n", "parser", ".", "add_argument", "(", "'--input'", ",", "metavar", "=", "'FP'", ",", "required", "=", "True", ",", "help", "=", "'binarized file to read'", ")", "\n", "# fmt: on", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.read_binarized.main": [[27, 32], ["fairseq.data.dictionary.Dictionary.load", "fairseq.data.IndexedDataset", "print", "dictionary.Dictionary.load.string"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "dict", "=", "dictionary", ".", "Dictionary", ".", "load", "(", "args", ".", "dict", ")", "\n", "ds", "=", "IndexedDataset", "(", "args", ".", "input", ",", "fix_lua_indexing", "=", "True", ")", "\n", "for", "tensor_line", "in", "ds", ":", "\n", "        ", "print", "(", "dict", ".", "string", "(", "tensor_line", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.test_gec_modules.test_copy_augmented_transformer_el": [[17, 27], ["None"], "function", ["None"], ["def", "test_copy_augmented_transformer_el", "(", "args", ",", "device", ")", ":", "\n", "    ", "\"\"\"Build a copy-augmented transformer model.\"\"\"", "\n", "from", "fairseq", ".", "models", ".", "copy_augmented_transformer_el", "import", "(", "\n", "TransformerELEncoder", ",", "CopyAugmentedTransformerELDecoder", ",", "\n", "CopyAugmentedTransformerELModel", "\n", ")", "\n", "# encoder = TransformerELEncoder(args, ...)", "\n", "# decoder = CopyAugmentedTransformerELDecoder(args, ...)", "\n", "# model = CopyAugmentedTransformerELModel(encoder, decoder)", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.test_gec_modules.test_compute_copy_probs": [[29, 62], ["torch.Tensor().to", "torch.Tensor().requires_grad_().to", "torch.Tensor().to", "compute_copy_probs", "torch.allclose", "print", "print", "print", "RuntimeError", "torch.Tensor", "torch.Tensor().requires_grad_", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.compute_copy_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "test_compute_copy_probs", "(", "device", ")", ":", "\n", "    ", "\"\"\"Sanity check tester for compute_copy_logits.\"\"\"", "\n", "from", "fairseq", ".", "models", ".", "copy_augmented_transformer_el", "import", "compute_copy_probs", "\n", "batch_size", ",", "tgt_len", ",", "src_len", ",", "vocab_size", "=", "2", ",", "3", ",", "4", ",", "10", "\n", "src_tokens", "=", "torch", ".", "Tensor", "(", "[", "\n", "[", "7", ",", "3", ",", "1", ",", "9", "]", ",", "\n", "[", "5", ",", "4", ",", "2", ",", "5", "]", "\n", "]", ")", ".", "to", "(", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "copy_scores", "=", "torch", ".", "Tensor", "(", "[", "\n", "[", "[", "0.9", ",", "0.1", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "0.1", ",", "0.7", ",", "0.2", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "0.2", ",", "0.8", "]", "]", ",", "\n", "[", "[", "0.9", ",", "0.0", ",", "0.0", ",", "0.1", "]", ",", "\n", "[", "0.0", ",", "0.9", ",", "0.1", ",", "0.0", "]", ",", "\n", "[", "0.1", ",", "0.0", ",", "0.2", ",", "0.7", "]", "]", ",", "\n", "]", ")", ".", "requires_grad_", "(", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "copy_probs", "=", "torch", ".", "Tensor", "(", "[", "\n", "[", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.1", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.9", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.2", ",", "0.0", ",", "0.7", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.1", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.2", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.8", "]", "]", ",", "\n", "[", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "1.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "0.1", ",", "0.0", ",", "0.9", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "0.0", ",", "0.2", ",", "0.0", ",", "0.0", ",", "0.8", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", "]", "\n", "]", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "probs", "=", "compute_copy_probs", "(", "copy_scores", ",", "src_tokens", ",", "vocab_size", ")", "\n", "assert", "probs", ".", "requires_grad", ",", "\"compute_copy_probs: output probs must require gradients in this test\"", "\n", "if", "torch", ".", "allclose", "(", "probs", ",", "copy_probs", ")", ":", "\n", "        ", "print", "(", "\"compute_copy_probs: test passed!\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"expected:\"", ",", "copy_probs", ")", "\n", "print", "(", "\"got:\"", ",", "probs", ")", "\n", "raise", "RuntimeError", "(", "\"compute_copy_probs: test failed\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.test_gec_modules.test_token_labeled_language_pair_dataset": [[69, 76], ["None"], "function", ["None"], ["def", "test_token_labeled_language_pair_dataset", "(", "args", ",", "device", ")", ":", "\n", "    ", "from", "fairseq", ".", "data", ".", "token_labeled_language_pair_dataset", "import", "(", "\n", "TokenLabeledIndexedRawTextDataset", ",", "TokenLabeledLanguagePairDataset", "\n", ")", "\n", "# raw_text_dataset = TokenLabeledIndexedRawTextDataset(args)", "\n", "# dataset = TokenLabeledLanguagePairDataset(raw_text_dataset, args)", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.build_sym_alignment.main": [[33, 98], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "open", "open", "os.system", "os.system", "os.system", "open", "itertools.zip_longest", "print", "s.strip", "t.strip"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'symmetric alignment builer'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--fast_align_dir'", ",", "\n", "help", "=", "'path to fast_align build directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--mosesdecoder_dir'", ",", "\n", "help", "=", "'path to mosesdecoder root directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--sym_heuristic'", ",", "\n", "help", "=", "'heuristic to use for symmetrization'", ",", "\n", "default", "=", "'grow-diag-final-and'", ")", "\n", "parser", ".", "add_argument", "(", "'--source_file'", ",", "\n", "help", "=", "'path to a file with sentences '", "\n", "'in the source language'", ")", "\n", "parser", ".", "add_argument", "(", "'--target_file'", ",", "\n", "help", "=", "'path to a file with sentences '", "\n", "'in the target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "\n", "help", "=", "'output directory'", ")", "\n", "# fmt: on", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "fast_align_bin", "=", "os", ".", "path", ".", "join", "(", "args", ".", "fast_align_dir", ",", "'fast_align'", ")", "\n", "symal_bin", "=", "os", ".", "path", ".", "join", "(", "args", ".", "mosesdecoder_dir", ",", "'bin'", ",", "'symal'", ")", "\n", "sym_fast_align_bin", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "mosesdecoder_dir", ",", "'scripts'", ",", "'ems'", ",", "\n", "'support'", ",", "'symmetrize-fast-align.perl'", ")", "\n", "\n", "# create joined file", "\n", "joined_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'text.joined'", ")", "\n", "with", "open", "(", "args", ".", "source_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "src", ",", "open", "(", "args", ".", "target_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "tgt", ":", "\n", "        ", "with", "open", "(", "joined_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "joined", ":", "\n", "            ", "for", "s", ",", "t", "in", "zip_longest", "(", "src", ",", "tgt", ")", ":", "\n", "                ", "print", "(", "'{} ||| {}'", ".", "format", "(", "s", ".", "strip", "(", ")", ",", "t", ".", "strip", "(", ")", ")", ",", "file", "=", "joined", ")", "\n", "\n", "", "", "", "bwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.backward'", ")", "\n", "\n", "# run forward alignment", "\n", "fwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.forward'", ")", "\n", "fwd_fast_align_cmd", "=", "'{FASTALIGN} -i {JOINED} -d -o -v > {FWD}'", ".", "format", "(", "\n", "FASTALIGN", "=", "fast_align_bin", ",", "\n", "JOINED", "=", "joined_file", ",", "\n", "FWD", "=", "fwd_align_file", ")", "\n", "assert", "os", ".", "system", "(", "fwd_fast_align_cmd", ")", "==", "0", "\n", "\n", "# run backward alignment", "\n", "bwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.backward'", ")", "\n", "bwd_fast_align_cmd", "=", "'{FASTALIGN} -i {JOINED} -d -o -v -r > {BWD}'", ".", "format", "(", "\n", "FASTALIGN", "=", "fast_align_bin", ",", "\n", "JOINED", "=", "joined_file", ",", "\n", "BWD", "=", "bwd_align_file", ")", "\n", "assert", "os", ".", "system", "(", "bwd_fast_align_cmd", ")", "==", "0", "\n", "\n", "# run symmetrization", "\n", "sym_out_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'aligned'", ")", "\n", "sym_cmd", "=", "'{SYMFASTALIGN} {FWD} {BWD} {SRC} {TGT} {OUT} {HEURISTIC} {SYMAL}'", ".", "format", "(", "\n", "SYMFASTALIGN", "=", "sym_fast_align_bin", ",", "\n", "FWD", "=", "fwd_align_file", ",", "\n", "BWD", "=", "bwd_align_file", ",", "\n", "SRC", "=", "args", ".", "source_file", ",", "\n", "TGT", "=", "args", ".", "target_file", ",", "\n", "OUT", "=", "sym_out_file", ",", "\n", "HEURISTIC", "=", "args", ".", "sym_heuristic", ",", "\n", "SYMAL", "=", "symal_bin", "\n", ")", "\n", "assert", "os", ".", "system", "(", "sym_cmd", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.dictolist": [[36, 39], ["sorted", "d.items"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["def", "dictolist", "(", "d", ")", ":", "\n", "    ", "a", "=", "sorted", "(", "d", ".", "items", "(", ")", ",", "key", "=", "lambda", "i", ":", "i", "[", "0", "]", ")", "\n", "return", "[", "i", "[", "1", "]", "for", "i", "in", "a", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.load_sys": [[40, 58], ["score_moe.dictolist", "score_moe.dictolist", "score_moe.dictolist", "score_moe.dictolist", "open", "line.startswith", "int", "line.startswith", "line.startswith", "line.startswith", "hypos[].append", "log_probs[].append", "line.split", "line.split", "float", "line.find", "line.split", "line.find", "line.split"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.dictolist", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.dictolist", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.dictolist", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.dictolist", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.find"], ["", "def", "load_sys", "(", "paths", ")", ":", "\n", "    ", "src", ",", "tgt", ",", "hypos", ",", "log_probs", "=", "{", "}", ",", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "for", "path", "in", "paths", ":", "\n", "        ", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "if", "line", ".", "startswith", "(", "(", "'S-'", ",", "'T-'", ",", "'H-'", ")", ")", ":", "\n", "                    ", "i", "=", "int", "(", "line", "[", "line", ".", "find", "(", "'-'", ")", "+", "1", ":", "line", ".", "find", "(", "'\\t'", ")", "]", ")", "\n", "if", "line", ".", "startswith", "(", "'S-'", ")", ":", "\n", "                        ", "src", "[", "i", "]", "=", "line", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "", "if", "line", ".", "startswith", "(", "'T-'", ")", ":", "\n", "                        ", "tgt", "[", "i", "]", "=", "line", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "", "if", "line", ".", "startswith", "(", "'H-'", ")", ":", "\n", "                        ", "if", "i", "not", "in", "hypos", ":", "\n", "                            ", "hypos", "[", "i", "]", "=", "[", "]", "\n", "log_probs", "[", "i", "]", "=", "[", "]", "\n", "", "hypos", "[", "i", "]", ".", "append", "(", "line", ".", "split", "(", "'\\t'", ")", "[", "2", "]", ")", "\n", "log_probs", "[", "i", "]", ".", "append", "(", "float", "(", "line", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", ")", "\n", "", "", "", "", "", "return", "dictolist", "(", "src", ")", ",", "dictolist", "(", "tgt", ")", ",", "dictolist", "(", "hypos", ")", ",", "dictolist", "(", "log_probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.load_ref": [[59, 78], ["open", "f.readlines", "len", "lines[].startswith", "src.append", "lines[].startswith", "tgt.append", "refs.append", "lines[].split", "lines[].startswith", "a.append", "lines[].split", "len", "lines[].split"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "load_ref", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "src", ",", "tgt", ",", "refs", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "lines", ")", ":", "\n", "        ", "if", "lines", "[", "i", "]", ".", "startswith", "(", "'S-'", ")", ":", "\n", "            ", "src", ".", "append", "(", "lines", "[", "i", "]", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", "\n", "i", "+=", "1", "\n", "", "elif", "lines", "[", "i", "]", ".", "startswith", "(", "'T-'", ")", ":", "\n", "            ", "tgt", ".", "append", "(", "lines", "[", "i", "]", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "            ", "a", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "lines", ")", "and", "lines", "[", "i", "]", ".", "startswith", "(", "'R'", ")", ":", "\n", "                ", "a", ".", "append", "(", "lines", "[", "i", "]", ".", "split", "(", "'\\t'", ")", "[", "1", "]", ")", "\n", "i", "+=", "1", "\n", "", "refs", ".", "append", "(", "a", ")", "\n", "", "", "return", "src", ",", "tgt", ",", "refs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.merge": [[79, 88], ["open", "zip", "f.write", "f.write", "f.write", "zip", "f.write", "f.write"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "def", "merge", "(", "src", ",", "tgt", ",", "hypos", ",", "log_probs", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "s", ",", "t", ",", "hs", ",", "lps", "in", "zip", "(", "src", ",", "tgt", ",", "hypos", ",", "log_probs", ")", ":", "\n", "            ", "f", ".", "write", "(", "s", ")", "\n", "f", ".", "write", "(", "t", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "for", "h", ",", "lp", "in", "zip", "(", "hs", ",", "lps", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%f\\t'", "%", "lp", "+", "h", ")", "\n", "", "f", ".", "write", "(", "'------------------------------------------------------\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.corpus_bleu": [[89, 96], ["scorer.reset", "zip", "scorer.score", "fairseq.tokenizer.Tokenizer.tokenize", "fairseq.tokenizer.Tokenizer.tokenize", "scorer.add"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], ["", "", "", "def", "corpus_bleu", "(", "ref", ",", "hypo", ")", ":", "\n", "    ", "scorer", ".", "reset", "(", ")", "\n", "for", "r", ",", "h", "in", "zip", "(", "ref", ",", "hypo", ")", ":", "\n", "        ", "r_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "r", ",", "dict", ")", "\n", "h_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "h", ",", "dict", ")", "\n", "scorer", ".", "add", "(", "r_tok", ",", "h_tok", ")", "\n", "", "return", "scorer", ".", "score", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.sentence_bleu": [[97, 103], ["scorer.reset", "fairseq.tokenizer.Tokenizer.tokenize", "fairseq.tokenizer.Tokenizer.tokenize", "scorer.add", "scorer.score"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.reset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.bleu.Scorer.score"], ["", "def", "sentence_bleu", "(", "ref", ",", "hypo", ")", ":", "\n", "    ", "scorer", ".", "reset", "(", "one_init", "=", "True", ")", "\n", "r_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "ref", ",", "dict", ")", "\n", "h_tok", "=", "tokenizer", ".", "Tokenizer", ".", "tokenize", "(", "hypo", ",", "dict", ")", "\n", "scorer", ".", "add", "(", "r_tok", ",", "h_tok", ")", "\n", "return", "scorer", ".", "score", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.pairwise": [[104, 113], ["score_moe.corpus_bleu", "range", "len", "range", "len", "_ref.append", "_hypo.append"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.corpus_bleu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "pairwise", "(", "sents", ")", ":", "\n", "    ", "_ref", ",", "_hypo", "=", "[", "]", ",", "[", "]", "\n", "for", "s", "in", "sents", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "s", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "s", ")", ")", ":", "\n", "                ", "if", "i", "!=", "j", ":", "\n", "                    ", "_ref", ".", "append", "(", "s", "[", "i", "]", ")", "\n", "_hypo", ".", "append", "(", "s", "[", "j", "]", ")", "\n", "", "", "", "", "return", "corpus_bleu", "(", "_ref", ",", "_hypo", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.multi_ref": [[114, 129], ["zip", "print", "print", "set", "len", "numpy.argmax", "_ref.append", "_hypo.append", "set.add", "score_moe.corpus_bleu", "score_moe.sentence_bleu", "random.choice", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.corpus_bleu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.sentence_bleu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "multi_ref", "(", "refs", ",", "hypos", ")", ":", "\n", "    ", "_ref", ",", "_hypo", "=", "[", "]", ",", "[", "]", "\n", "ref_cnt", "=", "0", "\n", "for", "rs", ",", "hs", "in", "zip", "(", "refs", ",", "hypos", ")", ":", "\n", "        ", "a", "=", "set", "(", ")", "\n", "for", "h", "in", "hs", ":", "\n", "            ", "s", "=", "[", "sentence_bleu", "(", "r", ",", "h", ")", "for", "r", "in", "rs", "]", "\n", "j", "=", "np", ".", "argmax", "(", "s", ")", "\n", "_ref", ".", "append", "(", "rs", "[", "j", "]", ")", "\n", "_hypo", ".", "append", "(", "h", ")", "\n", "best", "=", "[", "k", "for", "k", "in", "range", "(", "len", "(", "rs", ")", ")", "if", "s", "[", "k", "]", "==", "s", "[", "j", "]", "]", "\n", "a", ".", "add", "(", "random", ".", "choice", "(", "best", ")", ")", "\n", "", "ref_cnt", "+=", "len", "(", "a", ")", "\n", "", "print", "(", "'avg oracle BLEU: %.2f'", "%", "corpus_bleu", "(", "_ref", ",", "_hypo", ")", ")", "\n", "print", "(", "'#refs covered: %.2f'", "%", "(", "ref_cnt", "/", "len", "(", "refs", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.intra_ref": [[130, 141], ["print", "print", "enumerate", "score_moe.pairwise", "numpy.argmax", "_ref.append", "_hypo.append", "score_moe.corpus_bleu", "score_moe.sentence_bleu"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.pairwise", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.corpus_bleu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.sentence_bleu"], ["", "def", "intra_ref", "(", "refs", ")", ":", "\n", "    ", "print", "(", "'ref pairwise BLEU: %.2f'", "%", "pairwise", "(", "refs", ")", ")", "\n", "_ref", ",", "_hypo", "=", "[", "]", ",", "[", "]", "\n", "for", "rs", "in", "refs", ":", "\n", "        ", "for", "i", ",", "h", "in", "enumerate", "(", "rs", ")", ":", "\n", "            ", "rest", "=", "rs", "[", ":", "i", "]", "+", "rs", "[", "i", "+", "1", ":", "]", "\n", "s", "=", "[", "sentence_bleu", "(", "r", ",", "h", ")", "for", "r", "in", "rest", "]", "\n", "j", "=", "np", ".", "argmax", "(", "s", ")", "\n", "_ref", ".", "append", "(", "rest", "[", "j", "]", ")", "\n", "_hypo", ".", "append", "(", "h", ")", "\n", "", "", "print", "(", "'ref avg oracle BLEU (leave-one-out): %.2f'", "%", "corpus_bleu", "(", "_ref", ",", "_hypo", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.average_checkpoints": [[10, 63], ["collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict.items", "torch.load", "list", "model_params.keys", "isinstance", "params_dict[].append", "len", "KeyError", "p.float.float", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["def", "average_checkpoints", "(", "inputs", ")", ":", "\n", "    ", "\"\"\"Loads checkpoints from inputs and returns a model with averaged weights.\n\n    Args:\n      inputs: An iterable of string paths of checkpoints to load from.\n\n    Returns:\n      A dict of string keys mapping to various values. The 'model' key\n      from the returned dict should correspond to an OrderedDict mapping\n      string parameter names to torch Tensors.\n    \"\"\"", "\n", "params_dict", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "params_keys", "=", "None", "\n", "new_state", "=", "None", "\n", "for", "f", "in", "inputs", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "\n", "f", ",", "\n", "map_location", "=", "(", "\n", "lambda", "s", ",", "_", ":", "torch", ".", "serialization", ".", "default_restore_location", "(", "s", ",", "'cpu'", ")", "\n", ")", ",", "\n", ")", "\n", "# Copies over the settings from the first checkpoint", "\n", "if", "new_state", "is", "None", ":", "\n", "            ", "new_state", "=", "state", "\n", "\n", "", "model_params", "=", "state", "[", "'model'", "]", "\n", "\n", "model_params_keys", "=", "list", "(", "model_params", ".", "keys", "(", ")", ")", "\n", "if", "params_keys", "is", "None", ":", "\n", "            ", "params_keys", "=", "model_params_keys", "\n", "", "elif", "params_keys", "!=", "model_params_keys", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "'For checkpoint {}, expected list of params: {}, '", "\n", "'but found: {}'", ".", "format", "(", "f", ",", "params_keys", ",", "model_params_keys", ")", "\n", ")", "\n", "\n", "", "for", "k", "in", "params_keys", ":", "\n", "            ", "if", "k", "not", "in", "params_dict", ":", "\n", "                ", "params_dict", "[", "k", "]", "=", "[", "]", "\n", "", "p", "=", "model_params", "[", "k", "]", "\n", "if", "isinstance", "(", "p", ",", "torch", ".", "HalfTensor", ")", ":", "\n", "                ", "p", "=", "p", ".", "float", "(", ")", "\n", "", "params_dict", "[", "k", "]", ".", "append", "(", "p", ")", "\n", "\n", "", "", "averaged_params", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "# v should be a list of torch Tensor.", "\n", "for", "k", ",", "v", "in", "params_dict", ".", "items", "(", ")", ":", "\n", "        ", "summed_v", "=", "None", "\n", "for", "x", "in", "v", ":", "\n", "            ", "summed_v", "=", "summed_v", "+", "x", "if", "summed_v", "is", "not", "None", "else", "x", "\n", "", "averaged_params", "[", "k", "]", "=", "summed_v", "/", "len", "(", "v", ")", "\n", "", "new_state", "[", "'model'", "]", "=", "averaged_params", "\n", "return", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.last_n_checkpoints": [[65, 84], ["os.listdir", "len", "re.compile", "re.compile", "re.compile.fullmatch", "len", "Exception", "os.path.join", "int", "len", "pt_regexp.fullmatch.group", "entries.append", "sorted", "pt_regexp.fullmatch.group"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "last_n_checkpoints", "(", "paths", ",", "n", ",", "update_based", ",", "upper_bound", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "paths", ")", "==", "1", "\n", "path", "=", "paths", "[", "0", "]", "\n", "if", "update_based", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint_\\d+_(\\d+)\\.pt'", ")", "\n", "", "else", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint(\\d+)\\.pt'", ")", "\n", "", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "\n", "entries", "=", "[", "]", "\n", "for", "f", "in", "files", ":", "\n", "        ", "m", "=", "pt_regexp", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "sort_key", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "if", "upper_bound", "is", "None", "or", "sort_key", "<=", "upper_bound", ":", "\n", "                ", "entries", ".", "append", "(", "(", "sort_key", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "", "", "if", "len", "(", "entries", ")", "<", "n", ":", "\n", "        ", "raise", "Exception", "(", "'Found {} checkpoint files but need at least {}'", ",", "len", "(", "entries", ")", ",", "n", ")", "\n", "", "return", "[", "os", ".", "path", ".", "join", "(", "path", ",", "x", "[", "1", "]", ")", "for", "x", "in", "sorted", "(", "entries", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.main": [[86, 132], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "average_checkpoints.average_checkpoints", "torch.save", "print", "average_checkpoints.last_n_checkpoints", "print"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.average_checkpoints", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.last_n_checkpoints", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Tool to average the params of input checkpoints to '", "\n", "'produce a new checkpoint'", ",", "\n", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--inputs'", ",", "required", "=", "True", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'Input checkpoint file paths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "required", "=", "True", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'Write the new checkpoint containing the averaged weights to this path.'", ")", "\n", "num_group", "=", "parser", ".", "add_mutually_exclusive_group", "(", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-epoch-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-update-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint-upper-bound'", ",", "type", "=", "int", ",", "\n", "help", "=", "'when using --num-epoch-checkpoints, this will set an upper bound on which checkpoint to use, '", "\n", "'e.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.'", ")", "\n", "# fmt: on", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "num", "=", "None", "\n", "is_update_based", "=", "False", "\n", "if", "args", ".", "num_update_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_update_checkpoints", "\n", "is_update_based", "=", "True", "\n", "", "elif", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_epoch_checkpoints", "\n", "\n", "", "assert", "args", ".", "checkpoint_upper_bound", "is", "None", "or", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ",", "'--checkpoint-upper-bound requires --num-epoch-checkpoints'", "\n", "assert", "args", ".", "num_epoch_checkpoints", "is", "None", "or", "args", ".", "num_update_checkpoints", "is", "None", ",", "'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'", "\n", "\n", "if", "num", "is", "not", "None", ":", "\n", "        ", "args", ".", "inputs", "=", "last_n_checkpoints", "(", "\n", "args", ".", "inputs", ",", "num", ",", "is_update_based", ",", "upper_bound", "=", "args", ".", "checkpoint_upper_bound", ",", "\n", ")", "\n", "print", "(", "'averaging checkpoints: '", ",", "args", ".", "inputs", ")", "\n", "\n", "", "new_state", "=", "average_checkpoints", "(", "args", ".", "inputs", ")", "\n", "torch", ".", "save", "(", "new_state", ",", "args", ".", "output", ")", "\n", "print", "(", "'Finished writing averaged checkpoint to {}.'", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main": [[15, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "open", "int", "print", "spm.SentencePieceProcessor.DecodePieces", "spm_decode.main.decode"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"sentencepiece model to use for decoding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "required", "=", "True", ",", "help", "=", "\"input file to decode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_format\"", ",", "choices", "=", "[", "\"piece\"", ",", "\"id\"", "]", ",", "default", "=", "\"piece\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "args", ".", "model", ")", "\n", "\n", "if", "args", ".", "input_format", "==", "\"piece\"", ":", "\n", "        ", "def", "decode", "(", "l", ")", ":", "\n", "            ", "return", "\"\"", ".", "join", "(", "sp", ".", "DecodePieces", "(", "l", ")", ")", "\n", "", "", "elif", "args", ".", "input_format", "==", "\"id\"", ":", "\n", "        ", "def", "decode", "(", "l", ")", ":", "\n", "            ", "return", "\"\"", ".", "join", "(", "sp", ".", "DecodeIds", "(", "l", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "def", "tok2int", "(", "tok", ")", ":", "\n", "# remap reference-side <unk> (represented as <<unk>>) to 0", "\n", "        ", "return", "int", "(", "tok", ")", "if", "tok", "!=", "\"<<unk>>\"", "else", "0", "\n", "\n", "", "with", "open", "(", "args", ".", "input", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "h", ":", "\n", "        ", "for", "line", "in", "h", ":", "\n", "            ", "print", "(", "decode", "(", "list", "(", "map", "(", "tok2int", ",", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility._test_reproducibility": [[20, 68], ["tempfile.TemporaryDirectory", "io.StringIO", "stdout.getvalue.getvalue.getvalue", "map", "os.rename", "io.StringIO", "stdout.getvalue.getvalue.getvalue", "map", "contextlib.redirect_stdout", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "contextlib.redirect_stdout", "test_binaries.train_translation_model", "os.path.join", "os.path.join", "contextlib.redirect_stdout", "test_binaries.train_translation_model", "round", "test_reproducibility.TestReproducibility.assertEqual", "test_reproducibility.TestReproducibility.assertEqual", "io.StringIO", "stdout.getvalue.getvalue.split", "stdout.getvalue.getvalue.split", "float", "test_reproducibility.TestReproducibility._test_reproducibility.cast"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model"], ["    ", "def", "_test_reproducibility", "(", "self", ",", "name", ",", "extra_flags", "=", "None", ")", ":", "\n", "        ", "if", "extra_flags", "is", "None", ":", "\n", "            ", "extra_flags", "=", "[", "]", "\n", "\n", "", "with", "tempfile", ".", "TemporaryDirectory", "(", "name", ")", "as", "data_dir", ":", "\n", "            ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "                ", "test_binaries", ".", "create_dummy_data", "(", "data_dir", ")", "\n", "test_binaries", ".", "preprocess_translation_data", "(", "data_dir", ")", "\n", "\n", "# train epochs 1 and 2 together", "\n", "", "stdout", "=", "StringIO", "(", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "stdout", ")", ":", "\n", "                ", "test_binaries", ".", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "\n", "'--dropout'", ",", "'0.0'", ",", "\n", "'--log-format'", ",", "'json'", ",", "\n", "'--log-interval'", ",", "'1'", ",", "\n", "'--max-epoch'", ",", "'3'", ",", "\n", "]", "+", "extra_flags", ",", "\n", ")", "\n", "", "stdout", "=", "stdout", ".", "getvalue", "(", ")", "\n", "train_log", ",", "valid_log", "=", "map", "(", "json", ".", "loads", ",", "stdout", ".", "split", "(", "'\\n'", ")", "[", "-", "4", ":", "-", "2", "]", ")", "\n", "\n", "# train epoch 2, resuming from previous checkpoint 1", "\n", "os", ".", "rename", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint1.pt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", ")", "\n", "stdout", "=", "StringIO", "(", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "stdout", ")", ":", "\n", "                ", "test_binaries", ".", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "\n", "'--dropout'", ",", "'0.0'", ",", "\n", "'--log-format'", ",", "'json'", ",", "\n", "'--log-interval'", ",", "'1'", ",", "\n", "'--max-epoch'", ",", "'3'", ",", "\n", "]", "+", "extra_flags", ",", "\n", ")", "\n", "", "stdout", "=", "stdout", ".", "getvalue", "(", ")", "\n", "train_res_log", ",", "valid_res_log", "=", "map", "(", "json", ".", "loads", ",", "stdout", ".", "split", "(", "'\\n'", ")", "[", "-", "4", ":", "-", "2", "]", ")", "\n", "\n", "def", "cast", "(", "s", ")", ":", "\n", "                ", "return", "round", "(", "float", "(", "s", ")", ",", "3", ")", "\n", "\n", "", "for", "k", "in", "[", "'train_loss'", ",", "'train_ppl'", ",", "'train_num_updates'", ",", "'train_gnorm'", "]", ":", "\n", "                ", "self", ".", "assertEqual", "(", "cast", "(", "train_log", "[", "k", "]", ")", ",", "cast", "(", "train_res_log", "[", "k", "]", ")", ")", "\n", "", "for", "k", "in", "[", "'valid_loss'", ",", "'valid_ppl'", ",", "'valid_num_updates'", ",", "'valid_best_loss'", "]", ":", "\n", "                ", "self", ".", "assertEqual", "(", "cast", "(", "valid_log", "[", "k", "]", ")", ",", "cast", "(", "valid_res_log", "[", "k", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility.test_reproducibility": [[69, 71], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "", "", "def", "test_reproducibility", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility.test_reproducibility_fp16": [[72, 76], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "def", "test_reproducibility_fp16", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility_fp16'", ",", "[", "\n", "'--fp16'", ",", "\n", "'--fp16-init-scale'", ",", "'4096'", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility.test_reproducibility_memory_efficient_fp16": [[78, 82], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "def", "test_reproducibility_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility_memory_efficient_fp16'", ",", "[", "\n", "'--memory-efficient-fp16'", ",", "\n", "'--fp16-init-scale'", ",", "'4096'", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_character_token_embedder.TestCharacterTokenEmbedder.test_character_token_embedder": [[16, 41], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.modules.CharacterTokenEmbedder", "max", "torch.LongTensor().fill_", "range", "fairseq.modules.CharacterTokenEmbedder.", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "fairseq.modules.CharacterTokenEmbedder.sum().backward", "fairseq.data.Dictionary.pad", "len", "fairseq.data.Dictionary.eos", "range", "fairseq.data.Dictionary.eos", "fairseq.modules.CharacterTokenEmbedder.size", "len", "torch.LongTensor", "len", "fairseq.data.Dictionary.index", "len", "fairseq.modules.CharacterTokenEmbedder.sum", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index"], ["    ", "def", "test_character_token_embedder", "(", "self", ")", ":", "\n", "        ", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "'hello'", ")", "\n", "vocab", ".", "add_symbol", "(", "'there'", ")", "\n", "\n", "embedder", "=", "CharacterTokenEmbedder", "(", "vocab", ",", "[", "(", "2", ",", "16", ")", ",", "(", "4", ",", "32", ")", ",", "(", "8", ",", "64", ")", ",", "(", "16", ",", "2", ")", "]", ",", "64", ",", "5", ",", "2", ")", "\n", "\n", "test_sents", "=", "[", "[", "'hello'", ",", "'unk'", ",", "'there'", "]", ",", "[", "'there'", "]", ",", "[", "'hello'", ",", "'there'", "]", "]", "\n", "max_len", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "test_sents", ")", "\n", "input", "=", "torch", ".", "LongTensor", "(", "len", "(", "test_sents", ")", ",", "max_len", "+", "2", ")", ".", "fill_", "(", "vocab", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "test_sents", ")", ")", ":", "\n", "            ", "input", "[", "i", "]", "[", "0", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "test_sents", "[", "i", "]", ")", ")", ":", "\n", "                ", "input", "[", "i", "]", "[", "j", "+", "1", "]", "=", "vocab", ".", "index", "(", "test_sents", "[", "i", "]", "[", "j", "]", ")", "\n", "", "input", "[", "i", "]", "[", "j", "+", "2", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "", "embs", "=", "embedder", "(", "input", ")", "\n", "\n", "assert", "embs", ".", "size", "(", ")", "==", "(", "len", "(", "test_sents", ")", ",", "max_len", "+", "2", ",", "5", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "0", "]", ",", "embs", "[", "1", "]", "[", "0", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "0", "]", ",", "embs", "[", "0", "]", "[", "-", "1", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "1", "]", ",", "embs", "[", "2", "]", "[", "1", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "3", "]", ",", "embs", "[", "1", "]", "[", "1", "]", ")", "\n", "\n", "embs", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "assert", "embedder", ".", "char_embeddings", ".", "weight", ".", "grad", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual": [[42, 45], ["test_character_token_embedder.TestCharacterTokenEmbedder.assertEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.setUp": [[22, 51], ["tests.dummy_dictionary", "len", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "next", "argparse.Namespace", "torch.FloatTensor().unsqueeze().expand", "tests.TestTranslationTask.setup_task", "test_label_smoothing.TestLabelSmoothing.task.build_model", "test_label_smoothing.TestLabelSmoothing.d.pad", "test_label_smoothing.TestLabelSmoothing.d.eos", "test_label_smoothing.TestLabelSmoothing.d.unk", "tests.dummy_dataloader", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor().unsqueeze", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dataloader"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "# build dictionary", "\n", "        ", "self", ".", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "3", ")", "\n", "vocab", "=", "len", "(", "self", ".", "d", ")", "\n", "self", ".", "assertEqual", "(", "vocab", ",", "4", "+", "3", ")", "# 4 special + 3 tokens", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "pad", ",", "eos", ",", "unk", ",", "w1", ",", "w2", ",", "w3", "=", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "# noqa: F841", "\n", "\n", "# build dataset", "\n", "self", ".", "data", "=", "[", "\n", "# the first batch item has padding", "\n", "{", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", ",", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", "}", ",", "\n", "{", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", ",", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w1", ",", "eos", "]", ")", "}", ",", "\n", "]", "\n", "self", ".", "sample", "=", "next", "(", "test_utils", ".", "dummy_dataloader", "(", "self", ".", "data", ")", ")", "\n", "\n", "# build model", "\n", "self", ".", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "self", ".", "args", ".", "sentence_avg", "=", "False", "\n", "self", ".", "args", ".", "probs", "=", "torch", ".", "FloatTensor", "(", "[", "\n", "#      pad   eos  unk   w1   w2   w3", "\n", "[", "0.05", ",", "0.05", ",", "0.1", ",", "0.05", ",", "0.3", ",", "0.4", ",", "0.05", "]", ",", "\n", "[", "0.05", ",", "0.10", ",", "0.2", ",", "0.05", ",", "0.2", ",", "0.3", ",", "0.10", "]", ",", "\n", "[", "0.05", ",", "0.15", ",", "0.3", ",", "0.05", ",", "0.1", ",", "0.2", ",", "0.15", "]", ",", "\n", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "2", ",", "3", ",", "7", ")", "# add batch dimension", "\n", "self", ".", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "self", ".", "args", ",", "self", ".", "d", ",", "self", ".", "d", ")", "\n", "self", ".", "model", "=", "self", ".", "task", ".", "build_model", "(", "self", ".", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.test_nll_loss": [[52, 60], ["fairseq.criterions.cross_entropy.CrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.cross_entropy.CrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertLess", "test_label_smoothing.TestLabelSmoothing.assertLess", "abs", "abs"], "methods", ["None"], ["", "def", "test_nll_loss", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "nll_crit", "=", "CrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "smooth_crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "nll_loss", ",", "nll_sample_size", ",", "nll_logging_output", "=", "nll_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "smooth_loss", ",", "smooth_sample_size", ",", "smooth_logging_output", "=", "smooth_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "self", ".", "assertLess", "(", "abs", "(", "nll_loss", "-", "nll_logging_output", "[", "'loss'", "]", ")", ",", "1e-6", ")", "\n", "self", ".", "assertLess", "(", "abs", "(", "nll_loss", "-", "smooth_logging_output", "[", "'nll_loss'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.test_padding": [[61, 79], ["fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.test_padding.get_one_no_padding"], "methods", ["None"], ["", "def", "test_padding", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "loss", ",", "_", ",", "logging_output", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "\n", "def", "get_one_no_padding", "(", "idx", ")", ":", "\n", "# create a new sample with just a single batch item so that there's", "\n", "# no padding", "\n", "            ", "sample1", "=", "next", "(", "test_utils", ".", "dummy_dataloader", "(", "[", "self", ".", "data", "[", "idx", "]", "]", ")", ")", "\n", "args1", "=", "copy", ".", "copy", "(", "self", ".", "args", ")", "\n", "args1", ".", "probs", "=", "args1", ".", "probs", "[", "idx", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", "\n", "model1", "=", "self", ".", "task", ".", "build_model", "(", "args1", ")", "\n", "loss1", ",", "_", ",", "_", "=", "crit", "(", "model1", ",", "sample1", ")", "\n", "return", "loss1", "\n", "\n", "", "loss1", "=", "get_one_no_padding", "(", "0", ")", "\n", "loss2", "=", "get_one_no_padding", "(", "1", ")", "\n", "self", ".", "assertAlmostEqual", "(", "loss", ",", "loss1", "+", "loss2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.test_reduction": [[80, 86], ["fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertAlmostEqual", "unreduced_loss.sum"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual"], ["", "def", "test_reduction", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "loss", ",", "_", ",", "logging_output", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ",", "reduce", "=", "True", ")", "\n", "unreduced_loss", ",", "_", ",", "_", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ",", "reduce", "=", "False", ")", "\n", "self", ".", "assertAlmostEqual", "(", "loss", ",", "unreduced_loss", ".", "sum", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.test_zero_eps": [[87, 94], ["fairseq.criterions.cross_entropy.CrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.cross_entropy.CrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertAlmostEqual"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual"], ["", "def", "test_zero_eps", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.0", "\n", "nll_crit", "=", "CrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "smooth_crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "nll_loss", ",", "nll_sample_size", ",", "nll_logging_output", "=", "nll_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "smooth_loss", ",", "smooth_sample_size", ",", "smooth_logging_output", "=", "smooth_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "self", ".", "assertAlmostEqual", "(", "nll_loss", ",", "smooth_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_label_smoothing.TestLabelSmoothing.assertAlmostEqual": [[95, 98], ["test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_iterators.TestIterators.test_counting_iterator": [[15, 26], ["list", "fairseq.data.iterators.CountingIterator", "test_iterators.TestIterators.assertTrue", "test_iterators.TestIterators.assertEqual", "test_iterators.TestIterators.assertEqual", "fairseq.data.iterators.CountingIterator.skip", "test_iterators.TestIterators.assertEqual", "fairseq.data.iterators.CountingIterator.skip", "test_iterators.TestIterators.assertEqual", "test_iterators.TestIterators.assertFalse", "range", "fairseq.data.iterators.CountingIterator.has_next", "next", "next", "next", "next", "fairseq.data.iterators.CountingIterator.has_next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.skip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.skip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.has_next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.has_next"], ["    ", "def", "test_counting_iterator", "(", "self", ")", ":", "\n", "        ", "x", "=", "list", "(", "range", "(", "10", ")", ")", "\n", "itr", "=", "iterators", ".", "CountingIterator", "(", "x", ")", "\n", "self", ".", "assertTrue", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "1", ")", "\n", "itr", ".", "skip", "(", "3", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "5", ")", "\n", "itr", ".", "skip", "(", "3", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "9", ")", "\n", "self", ".", "assertFalse", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_dictionary.TestDictionary.test_finalize": [[18, 69], ["list", "list", "fairseq.data.Dictionary", "test_dictionary.TestDictionary.test_finalize.get_ids"], "methods", ["None"], ["    ", "def", "test_finalize", "(", "self", ")", ":", "\n", "        ", "txt", "=", "[", "\n", "'A B C D'", ",", "\n", "'B C D'", ",", "\n", "'C D'", ",", "\n", "'D'", ",", "\n", "]", "\n", "ref_ids1", "=", "list", "(", "map", "(", "torch", ".", "IntTensor", ",", "[", "\n", "[", "4", ",", "5", ",", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "5", ",", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "7", ",", "2", "]", ",", "\n", "]", ")", ")", "\n", "ref_ids2", "=", "list", "(", "map", "(", "torch", ".", "IntTensor", ",", "[", "\n", "[", "7", ",", "6", ",", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "6", ",", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "4", ",", "2", "]", ",", "\n", "]", ")", ")", "\n", "\n", "# build dictionary", "\n", "d", "=", "Dictionary", "(", ")", "\n", "for", "line", "in", "txt", ":", "\n", "            ", "d", ".", "encode_line", "(", "line", ",", "add_if_not_exist", "=", "True", ")", "\n", "\n", "", "def", "get_ids", "(", "dictionary", ")", ":", "\n", "            ", "ids", "=", "[", "]", "\n", "for", "line", "in", "txt", ":", "\n", "                ", "ids", ".", "append", "(", "dictionary", ".", "encode_line", "(", "line", ",", "add_if_not_exist", "=", "False", ")", ")", "\n", "", "return", "ids", "\n", "\n", "", "def", "assertMatch", "(", "ids", ",", "ref_ids", ")", ":", "\n", "            ", "for", "toks", ",", "ref_toks", "in", "zip", "(", "ids", ",", "ref_ids", ")", ":", "\n", "                ", "self", ".", "assertEqual", "(", "toks", ".", "size", "(", ")", ",", "ref_toks", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "0", ",", "(", "toks", "!=", "ref_toks", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "", "ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "ids", ",", "ref_ids1", ")", "\n", "\n", "# check finalized dictionary", "\n", "d", ".", "finalize", "(", ")", "\n", "finalized_ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "finalized_ids", ",", "ref_ids2", ")", "\n", "\n", "# write to disk and reload", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", "mode", "=", "'w'", ")", "as", "tmp_dict", ":", "\n", "            ", "d", ".", "save", "(", "tmp_dict", ".", "name", ")", "\n", "d", "=", "Dictionary", ".", "load", "(", "tmp_dict", ".", "name", ")", "\n", "reload_ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "reload_ids", ",", "ref_ids2", ")", "\n", "assertMatch", "(", "finalized_ids", ",", "reload_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker": [[24, 58], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["    ", "def", "_get_test_data_with_bpe_cont_marker", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with continuation markers as suffixes to denote\n                non-end of word tokens. This is the standard BPE format used in\n                fairseq's preprocessing.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "\"he@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"llo\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"y@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ou\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"n@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ew\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"or@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"k\"", ")", "\n", "\n", "src_tokens", "=", "[", "\n", "[", "\"he@@\"", ",", "\"llo\"", ",", "\"n@@\"", ",", "\"ew\"", ",", "\"y@@\"", ",", "\"or@@\"", ",", "\"k\"", "]", ",", "\n", "[", "\"how\"", ",", "\"are\"", ",", "\"y@@\"", ",", "\"ou\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_end_marker": [[59, 94], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["", "def", "_get_test_data_with_bpe_end_marker", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with end-of-word markers as suffixes to denote\n                tokens at the end of a word. This is an alternative to fairseq's\n                standard preprocessing framework and is not generally supported\n                within fairseq.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "\"he\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"llo_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"y\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ou_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"n\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ew_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"or\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"k_EOW\"", ")", "\n", "\n", "src_tokens", "=", "[", "\n", "[", "\"he\"", ",", "\"llo_EOW\"", ",", "\"n\"", ",", "\"ew_EOW\"", ",", "\"y\"", ",", "\"or\"", ",", "\"k_EOW\"", "]", ",", "\n", "[", "\"how_EOW\"", ",", "\"are_EOW\"", ",", "\"y\"", ",", "\"ou_EOW\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_word_vocab": [[95, 123], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["", "def", "_get_test_data_with_word_vocab", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: word vocab\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "\n", "vocab", ".", "add_symbol", "(", "\"hello\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"you\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"new\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"york\"", ")", "\n", "src_tokens", "=", "[", "\n", "[", "\"hello\"", ",", "\"new\"", ",", "\"york\"", ",", "\"you\"", "]", ",", "\n", "[", "\"how\"", ",", "\"are\"", ",", "\"you\"", ",", "\"new\"", ",", "\"york\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor": [[124, 141], ["torch.LongTensor().fill_", "range", "x.transpose.transpose.transpose", "len", "vocab.pad", "len", "range", "torch.LongTensor", "torch.LongTensor", "len", "vocab.index", "vocab.eos", "len", "max"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "_convert_src_tokens_to_tensor", "(", "\n", "self", ",", "vocab", ":", "Dictionary", ",", "src_tokens", ":", "List", "[", "List", "[", "str", "]", "]", ",", "append_eos", ":", "bool", "\n", ")", ":", "\n", "        ", "src_len", "=", "[", "len", "(", "x", ")", "for", "x", "in", "src_tokens", "]", "\n", "# If we have to append EOS, we include EOS in counting src length", "\n", "if", "append_eos", ":", "\n", "            ", "src_len", "=", "[", "length", "+", "1", "for", "length", "in", "src_len", "]", "\n", "\n", "", "x", "=", "torch", ".", "LongTensor", "(", "len", "(", "src_tokens", ")", ",", "max", "(", "src_len", ")", ")", ".", "fill_", "(", "vocab", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "src_tokens", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "src_tokens", "[", "i", "]", ")", ")", ":", "\n", "                ", "x", "[", "i", "]", "[", "j", "]", "=", "vocab", ".", "index", "(", "src_tokens", "[", "i", "]", "[", "j", "]", ")", "\n", "", "if", "append_eos", ":", "\n", "                ", "x", "[", "i", "]", "[", "j", "+", "1", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "x", ",", "torch", ".", "LongTensor", "(", "src_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_eos_at_end": [[142, 152], ["range", "len", "test_noising.TestDataNoising.assertEqual"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "assert_eos_at_end", "(", "self", ",", "x", ",", "x_len", ",", "eos", ")", ":", "\n", "        ", "\"\"\"Asserts last token of every sentence in x is EOS \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "x_len", ")", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "\n", "x", "[", "x_len", "[", "i", "]", "-", "1", "]", "[", "i", "]", ",", "\n", "eos", ",", "\n", "(", "\n", "\"Expected eos (token id {eos}) at the end of sentence {i} \"", "\n", "\"but got {other} instead\"", "\n", ")", ".", "format", "(", "i", "=", "i", ",", "eos", "=", "eos", ",", "other", "=", "x", "[", "i", "]", "[", "-", "1", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_dropout_correct": [[154, 160], ["test_noising.TestDataNoising.assertEqual", "range", "test_noising.TestDataNoising.assertEqual"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "", "def", "assert_word_dropout_correct", "(", "self", ",", "x", ",", "x_noised", ",", "x_len", ",", "l_noised", ")", ":", "\n", "# Expect only the first word (2 bpe tokens) of the first example", "\n", "# was dropped out", "\n", "        ", "self", ".", "assertEqual", "(", "x_len", "[", "0", "]", "-", "2", ",", "l_noised", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "l_noised", "[", "0", "]", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "x", "[", "i", "+", "2", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_dropout_with_eos": [[161, 171], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_dropout_correct", "test_noising.TestDataNoising.assert_eos_at_end", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_dropout_correct", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_dropout_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ")", "\n", "self", ".", "assert_word_dropout_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", "\n", ")", "\n", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_blanking_correct": [[172, 181], ["test_noising.TestDataNoising.assertEqual", "range", "test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assertEqual"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "", "def", "assert_word_blanking_correct", "(", "self", ",", "x", ",", "x_noised", ",", "x_len", ",", "l_noised", ",", "unk", ")", ":", "\n", "# Expect only the first word (2 bpe tokens) of the first example", "\n", "# was blanked out", "\n", "        ", "self", ".", "assertEqual", "(", "x_len", "[", "0", "]", ",", "l_noised", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "l_noised", "[", "0", "]", ")", ":", "\n", "            ", "if", "i", "<", "2", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "unk", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "x", "[", "i", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_blank_with_eos": [[182, 192], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_blanking_correct", "test_noising.TestDataNoising.assert_eos_at_end", "vocab.unk", "vocab.unk", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_blanking_correct", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "", "", "def", "test_word_blank_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ",", "vocab", ".", "unk", "(", ")", ")", "\n", "self", ".", "assert_word_blanking_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", ",", "unk", "=", "vocab", ".", "unk", "(", ")", "\n", ")", "\n", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map": [[193, 195], ["range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "", "def", "generate_unchanged_shuffle_map", "(", "self", ",", "length", ")", ":", "\n", "        ", "return", "{", "i", ":", "i", "for", "i", "in", "range", "(", "length", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected": [[196, 247], ["range", "zip", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordShuffle", "fairseq.data.noising.WordShuffle.noising", "len", "shuffle_map.items", "test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assert_eos_at_end", "test_noising.TestDataNoising.assertEqual", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "assert_word_shuffle_matches_expected", "(", "\n", "self", ",", "\n", "x", ",", "\n", "x_len", ",", "\n", "max_shuffle_distance", ":", "int", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "expected_shufle_maps", ":", "List", "[", "Dict", "[", "int", ",", "int", "]", "]", ",", "\n", "expect_eos_at_end", ":", "bool", ",", "\n", "bpe_end_marker", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        This verifies that with a given x, x_len, max_shuffle_distance, and\n        vocab, we get the expected shuffle result.\n\n        Args:\n            x: Tensor of shape (T x B) = (sequence_length, batch_size)\n            x_len: Tensor of length B = batch_size\n            max_shuffle_distance: arg to pass to noising\n            expected_shuffle_maps: List[mapping] where mapping is a\n                Dict[old_index, new_index], mapping x's elements from their\n                old positions in x to their new positions in x.\n            expect_eos_at_end: if True, check the output to make sure there is\n                an EOS at the end.\n            bpe_end_marker: str denoting the BPE end token. If this is not None, we\n                set the BPE cont token to None in the noising classes.\n        \"\"\"", "\n", "bpe_cont_marker", "=", "None", "\n", "if", "bpe_end_marker", "is", "None", ":", "\n", "            ", "bpe_cont_marker", "=", "\"@@\"", "\n", "\n", "", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "word_shuffle", "=", "noising", ".", "WordShuffle", "(", "\n", "vocab", ",", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "bpe_end_marker", "=", "bpe_end_marker", "\n", ")", "\n", "x_noised", ",", "l_noised", "=", "word_shuffle", ".", "noising", "(", "\n", "x", ",", "x_len", ",", "max_shuffle_distance", "=", "max_shuffle_distance", "\n", ")", "\n", "\n", "# For every example, we have a different expected shuffle map. We check", "\n", "# that each example is shuffled as expected according to each", "\n", "# corresponding shuffle map.", "\n", "", "for", "i", "in", "range", "(", "len", "(", "expected_shufle_maps", ")", ")", ":", "\n", "            ", "shuffle_map", "=", "expected_shufle_maps", "[", "i", "]", "\n", "for", "k", ",", "v", "in", "shuffle_map", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x", "[", "k", "]", "[", "i", "]", ",", "x_noised", "[", "v", "]", "[", "i", "]", ")", "\n", "\n", "# Shuffling should not affect the length of each example", "\n", "", "", "for", "pre_shuffle_length", ",", "post_shuffle_length", "in", "zip", "(", "x_len", ",", "l_noised", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "pre_shuffle_length", ",", "post_shuffle_length", ")", "\n", "", "if", "expect_eos_at_end", ":", "\n", "            ", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_shuffle_with_eos": [[248, 277], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "", "def", "test_word_shuffle_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_shuffle_with_eos_nonbpe": [[279, 309], ["test_noising.TestDataNoising._get_test_data_with_word_vocab", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_word_vocab", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_with_eos_nonbpe", "(", "self", ")", ":", "\n", "        ", "\"\"\"The purpose of this is to test shuffling logic with word vocabs\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_word_vocab", "(", "append_eos", "=", "True", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "{", "0", ":", "0", ",", "1", ":", "1", ",", "2", ":", "3", ",", "3", ":", "2", "}", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "2", ",", "2", ":", "1", ",", "3", ":", "3", ",", "4", ":", "4", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_shuffle_without_eos": [[311, 341], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word shuffle with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_shuffle_without_eos_with_bpe_end_marker": [[343, 375], ["test_noising.TestDataNoising._get_test_data_with_bpe_end_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_end_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_without_eos_with_bpe_end_marker", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word shuffle without eos except using BPE end token\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_end_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", "bpe_end_marker", "=", "\"_EOW\"", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", "bpe_end_marker", "=", "\"_EOW\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_no_eos_at_end": [[377, 385], ["range", "len", "test_noising.TestDataNoising.assertNotEqual"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "assert_no_eos_at_end", "(", "self", ",", "x", ",", "x_len", ",", "eos", ")", ":", "\n", "        ", "\"\"\"Asserts that the last token of each sentence in x is not EOS \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "x_len", ")", ")", ":", "\n", "            ", "self", ".", "assertNotEqual", "(", "\n", "x", "[", "x_len", "[", "i", "]", "-", "1", "]", "[", "i", "]", ",", "\n", "eos", ",", "\n", "\"Expected no eos (token id {eos}) at the end of sentence {i}.\"", ".", "format", "(", "\n", "eos", "=", "eos", ",", "i", "=", "i", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_dropout_without_eos": [[388, 399], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_dropout_correct", "test_noising.TestDataNoising.assert_no_eos_at_end", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_dropout_correct", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_no_eos_at_end", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_dropout_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word dropout with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ")", "\n", "self", ".", "assert_word_dropout_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", "\n", ")", "\n", "self", ".", "assert_no_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_word_blank_without_eos": [[400, 411], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_blanking_correct", "test_noising.TestDataNoising.assert_no_eos_at_end", "vocab.unk", "vocab.unk", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_word_blanking_correct", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assert_no_eos_at_end", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_blank_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word blank with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ",", "vocab", ".", "unk", "(", ")", ")", "\n", "self", ".", "assert_word_blanking_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", ",", "unk", "=", "vocab", ".", "unk", "(", ")", "\n", ")", "\n", "self", ".", "assert_no_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_noising_dataset_batch": [[412, 449], ["tests.TestDataset", "fairseq.data.noising.NoisingDataset", "fairseq.data.LanguagePairDataset", "fairseq.data.TransformEosDataset", "torch.utils.data.DataLoader", "next", "src_dict.eos", "iter"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "", "def", "_get_noising_dataset_batch", "(", "\n", "self", ",", "src_tokens_no_pad", ",", "src_dict", ",", "append_eos_to_tgt", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Constructs a NoisingDataset and the corresponding\n        ``LanguagePairDataset(NoisingDataset(src), src)``. If\n        *append_eos_to_tgt* is True, wrap the source dataset in\n        :class:`TransformEosDataset` to append EOS to the clean source when\n        using it as the target.\n        \"\"\"", "\n", "src_dataset", "=", "test_utils", ".", "TestDataset", "(", "data", "=", "src_tokens_no_pad", ")", "\n", "\n", "noising_dataset", "=", "noising", ".", "NoisingDataset", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "src_dict", "=", "src_dict", ",", "\n", "seed", "=", "1234", ",", "\n", "max_word_shuffle_distance", "=", "3", ",", "\n", "word_dropout_prob", "=", "0.2", ",", "\n", "word_blanking_prob", "=", "0.2", ",", "\n", "noising_class", "=", "noising", ".", "UnsupervisedMTNoising", ",", "\n", ")", "\n", "tgt", "=", "src_dataset", "\n", "language_pair_dataset", "=", "LanguagePairDataset", "(", "\n", "src", "=", "noising_dataset", ",", "tgt", "=", "tgt", ",", "src_sizes", "=", "None", ",", "src_dict", "=", "src_dict", "\n", ")", "\n", "language_pair_dataset", "=", "TransformEosDataset", "(", "\n", "language_pair_dataset", ",", "src_dict", ".", "eos", "(", ")", ",", "\n", "append_eos_to_tgt", "=", "append_eos_to_tgt", ",", "\n", ")", "\n", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "language_pair_dataset", ",", "\n", "batch_size", "=", "2", ",", "\n", "collate_fn", "=", "language_pair_dataset", ".", "collater", ",", "\n", ")", "\n", "denoising_batch_result", "=", "next", "(", "iter", "(", "dataloader", ")", ")", "\n", "return", "denoising_batch_result", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_noising_dataset_with_eos": [[450, 481], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "torch.t", "test_noising.TestDataNoising._get_noising_dataset_batch", "torch.LongTensor", "torch.LongTensor", "test_noising.TestDataNoising.assertTensorEqual", "test_noising.TestDataNoising.assertTensorEqual", "src_tokens_no_pad.append", "src_dict.eos", "src_dict.pad", "fairseq.utils.strip_pad", "src_dict.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_noising_dataset_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "def", "test_noising_dataset_with_eos", "(", "self", ")", ":", "\n", "        ", "src_dict", ",", "src_tokens", ",", "_", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "\n", "append_eos", "=", "True", "\n", ")", "\n", "\n", "# Format data for src_dataset", "\n", "src_tokens", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "src_tokens_no_pad", "=", "[", "]", "\n", "for", "src_sentence", "in", "src_tokens", ":", "\n", "            ", "src_tokens_no_pad", ".", "append", "(", "\n", "utils", ".", "strip_pad", "(", "tensor", "=", "src_sentence", ",", "pad", "=", "src_dict", ".", "pad", "(", ")", ")", "\n", ")", "\n", "", "denoising_batch_result", "=", "self", ".", "_get_noising_dataset_batch", "(", "\n", "src_tokens_no_pad", "=", "src_tokens_no_pad", ",", "src_dict", "=", "src_dict", "\n", ")", "\n", "\n", "eos", ",", "pad", "=", "src_dict", ".", "eos", "(", ")", ",", "src_dict", ".", "pad", "(", ")", "\n", "\n", "# Generated noisy source as source", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "pad", ",", "pad", ",", "pad", ",", "6", ",", "8", ",", "9", ",", "7", ",", "eos", "]", "]", "\n", ")", "\n", "# Original clean source as target (right-padded)", "\n", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "6", ",", "7", ",", "8", ",", "9", ",", "eos", ",", "pad", ",", "pad", ",", "pad", "]", "]", "\n", ")", "\n", "generated_src", "=", "denoising_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "denoising_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.test_noising_dataset_without_eos": [[482, 521], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "torch.t", "test_noising.TestDataNoising._get_noising_dataset_batch", "torch.LongTensor", "torch.LongTensor", "test_noising.TestDataNoising.assertTensorEqual", "test_noising.TestDataNoising.assertTensorEqual", "src_tokens_no_pad.append", "src_dict.eos", "src_dict.pad", "fairseq.utils.strip_pad", "src_dict.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising._get_noising_dataset_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "def", "test_noising_dataset_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Similar to test noising dataset with eos except that we have to set\n        *append_eos_to_tgt* to ``True``.\n        \"\"\"", "\n", "\n", "src_dict", ",", "src_tokens", ",", "_", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "\n", "append_eos", "=", "False", "\n", ")", "\n", "\n", "# Format data for src_dataset", "\n", "src_tokens", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "src_tokens_no_pad", "=", "[", "]", "\n", "for", "src_sentence", "in", "src_tokens", ":", "\n", "            ", "src_tokens_no_pad", ".", "append", "(", "\n", "utils", ".", "strip_pad", "(", "tensor", "=", "src_sentence", ",", "pad", "=", "src_dict", ".", "pad", "(", ")", ")", "\n", ")", "\n", "", "denoising_batch_result", "=", "self", ".", "_get_noising_dataset_batch", "(", "\n", "src_tokens_no_pad", "=", "src_tokens_no_pad", ",", "\n", "src_dict", "=", "src_dict", ",", "\n", "append_eos_to_tgt", "=", "True", ",", "\n", ")", "\n", "\n", "eos", ",", "pad", "=", "src_dict", ".", "eos", "(", ")", ",", "src_dict", ".", "pad", "(", ")", "\n", "\n", "# Generated noisy source as source", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", "]", ",", "[", "pad", ",", "pad", ",", "pad", ",", "6", ",", "8", ",", "9", ",", "7", "]", "]", "\n", ")", "\n", "# Original clean source as target (right-padded)", "\n", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "6", ",", "7", ",", "8", ",", "9", ",", "eos", ",", "pad", ",", "pad", ",", "pad", "]", "]", "\n", ")", "\n", "\n", "generated_src", "=", "denoising_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "denoising_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_noising.TestDataNoising.assertTensorEqual": [[522, 525], ["test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_convtbc.TestConvTBC.test_convtbc": [[16, 43], ["fairseq.modules.ConvTBC", "torch.Conv1d", "torch.Conv1d", "fairseq.modules.ConvTBC.weight.data.copy_", "fairseq.modules.ConvTBC.bias.data.copy_", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn.data.transpose().transpose", "torch.randn.data.transpose().transpose", "fairseq.modules.ConvTBC.", "torch.Conv1d.", "test_convtbc.TestConvTBC.assertAlmostEqual", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn.transpose().transpose().contiguous", "torch.randn.transpose().transpose().contiguous", "fairseq.modules.ConvTBC.backward", "nn.Conv1d.backward", "test_convtbc.TestConvTBC.assertAlmostEqual", "test_convtbc.TestConvTBC.assertAlmostEqual", "test_convtbc.TestConvTBC.assertAlmostEqual", "torch.Conv1d.weight.data.transpose", "fairseq.modules.ConvTBC.data.transpose().transpose", "fairseq.modules.ConvTBC.size", "fairseq.modules.ConvTBC.weight.grad.data.transpose", "torch.randn.grad.data.transpose().transpose", "torch.randn.grad.data.transpose().transpose", "torch.randn.data.transpose", "torch.randn.data.transpose", "torch.randn.transpose().transpose", "torch.randn.transpose().transpose", "fairseq.modules.ConvTBC.data.transpose", "torch.randn.grad.data.transpose", "torch.randn.grad.data.transpose", "torch.randn.transpose", "torch.randn.transpose"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.ConvTBC", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["    ", "def", "test_convtbc", "(", "self", ")", ":", "\n", "# ksz, in_channels, out_channels", "\n", "        ", "conv_tbc", "=", "ConvTBC", "(", "4", ",", "5", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "# out_channels, in_channels, ksz", "\n", "conv1d", "=", "nn", ".", "Conv1d", "(", "4", ",", "5", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "\n", "conv_tbc", ".", "weight", ".", "data", ".", "copy_", "(", "conv1d", ".", "weight", ".", "data", ".", "transpose", "(", "0", ",", "2", ")", ")", "\n", "conv_tbc", ".", "bias", ".", "data", ".", "copy_", "(", "conv1d", ".", "bias", ".", "data", ")", "\n", "\n", "input_tbc", "=", "torch", ".", "randn", "(", "7", ",", "2", ",", "4", ",", "requires_grad", "=", "True", ")", "\n", "input1d", "=", "input_tbc", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "input1d", ".", "requires_grad", "=", "True", "\n", "\n", "output_tbc", "=", "conv_tbc", "(", "input_tbc", ")", "\n", "output1d", "=", "conv1d", "(", "input1d", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "output_tbc", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ",", "output1d", ".", "data", ")", "\n", "\n", "grad_tbc", "=", "torch", ".", "randn", "(", "output_tbc", ".", "size", "(", ")", ")", "\n", "grad1d", "=", "grad_tbc", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "output_tbc", ".", "backward", "(", "grad_tbc", ")", "\n", "output1d", ".", "backward", "(", "grad1d", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "conv_tbc", ".", "weight", ".", "grad", ".", "data", ".", "transpose", "(", "0", ",", "2", ")", ",", "conv1d", ".", "weight", ".", "grad", ".", "data", ")", "\n", "self", ".", "assertAlmostEqual", "(", "conv_tbc", ".", "bias", ".", "grad", ".", "data", ",", "conv1d", ".", "bias", ".", "grad", ".", "data", ")", "\n", "self", ".", "assertAlmostEqual", "(", "input_tbc", ".", "grad", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ",", "input1d", ".", "grad", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_convtbc.TestConvTBC.assertAlmostEqual": [[44, 47], ["test_convtbc.TestConvTBC.assertEqual", "test_convtbc.TestConvTBC.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_scorer.TestSequenceScorer.test_sequence_scorer": [[20, 94], ["tests.dummy_dictionary", "test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "tests.dummy_dictionary.eos", "tests.dummy_dataloader", "argparse.Namespace", "tests.TestTranslationTask.setup_task", "tests.TestTranslationTask.setup_task.build_model", "fairseq.sequence_scorer.SequenceScorer", "tests.dummy_dictionary.pad", "tests.dummy_dictionary.eos", "tests.dummy_dictionary.unk", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "tests.TestTranslationTask.setup_task.inference_step", "zip", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sample[].tolist", "test_sequence_scorer.TestSequenceScorer.assertHypoTokens", "test_sequence_scorer.TestSequenceScorer.assertHypoScore"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dataloader", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore"], ["    ", "def", "test_sequence_scorer", "(", "self", ")", ":", "\n", "# construct dummy dictionary", "\n", "        ", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct dataloader", "\n", "data", "=", "[", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "w1", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "]", "\n", "data_itr", "=", "test_utils", ".", "dummy_dataloader", "(", "data", ")", "\n", "\n", "# specify expected output probabilities", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.4", ",", "0.6", "]", ",", "# sentence 2", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "0.0", ",", "unk", ",", "0.2", ",", "0.7", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.8", ",", "0.2", "]", ",", "# sentence 2", "\n", "[", "0.7", ",", "unk", ",", "0.1", ",", "0.2", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos       w1    w2", "\n", "[", "0.10", ",", "unk", ",", "0.50", ",", "0.4", "]", ",", "# sentence 1", "\n", "[", "0.15", ",", "unk", ",", "0.15", ",", "0.7", "]", ",", "# sentence 2", "\n", "[", "0.00", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1    w2", "\n", "[", "0.9", ",", "unk", ",", "0.05", ",", "0.05", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 2", "\n", "[", "0.0", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "]", "\n", "expected_scores", "=", "[", "\n", "[", "0.6", ",", "0.7", ",", "0.5", ",", "0.9", "]", ",", "# sentence 1", "\n", "[", "0.6", ",", "0.8", ",", "0.15", "]", ",", "# sentence 2", "\n", "[", "0.3", ",", "0.7", "]", ",", "# sentence 3", "\n", "]", "\n", "\n", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ")", "\n", "for", "sample", "in", "data_itr", ":", "\n", "            ", "hypos", "=", "task", ".", "inference_step", "(", "scorer", ",", "[", "model", "]", ",", "sample", ")", "\n", "for", "id", ",", "hypos_id", "in", "zip", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ",", "hypos", ")", ":", "\n", "                ", "self", ".", "assertHypoTokens", "(", "hypos_id", "[", "0", "]", ",", "data", "[", "id", "]", "[", "'target'", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos_id", "[", "0", "]", ",", "expected_scores", "[", "id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_scorer.TestSequenceScorer.assertHypoTokens": [[95, 97], ["test_sequence_scorer.TestSequenceScorer.assertTensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual"], ["", "", "", "def", "assertHypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "assertTensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_scorer.TestSequenceScorer.assertHypoScore": [[98, 106], ["torch.FloatTensor().log", "test_sequence_scorer.TestSequenceScorer.assertAlmostEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "torch.FloatTensor().log.sum", "test_sequence_scorer.TestSequenceScorer.assertLess", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual"], ["", "def", "assertHypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", "\n", "self", ".", "assertEqual", "(", "pos_scores", ".", "numel", "(", ")", ",", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ")", "\n", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "self", ".", "assertLess", "(", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_scorer.TestSequenceScorer.assertAlmostEqual": [[107, 110], ["test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_scorer.TestSequenceScorer.assertTensorEqual": [[111, 114], ["test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset.setUp": [[24, 33], ["tests.sequence_generator_setup", "tests.TestDataset", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.sequence_generator_setup"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "tgt_dict", ",", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "src_tokens", ",", "self", ".", "src_lengths", ",", "self", ".", "model", "=", "(", "\n", "test_utils", ".", "sequence_generator_setup", "(", ")", "\n", ")", "\n", "\n", "dummy_src_samples", "=", "self", ".", "src_tokens", "\n", "\n", "self", ".", "tgt_dataset", "=", "test_utils", ".", "TestDataset", "(", "data", "=", "dummy_src_samples", ")", "\n", "self", ".", "cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper": [[34, 95], ["fairseq.data.LanguagePairDataset", "fairseq.sequence_generator.SequenceGenerator", "fairseq.data.BacktranslationDataset", "torch.utils.data.DataLoader", "next", "torch.LongTensor", "torch.LongTensor", "test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual", "test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual", "iter", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.pad", "fairseq.data.TransformEosDataset", "fairseq.sequence_generator.SequenceGenerator.generate", "fairseq.data.TransformEosDataset", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "_backtranslation_dataset_helper", "(", "\n", "self", ",", "remove_eos_from_input_src", ",", "remove_eos_from_output_src", ",", "\n", ")", ":", "\n", "        ", "tgt_dataset", "=", "LanguagePairDataset", "(", "\n", "src", "=", "self", ".", "tgt_dataset", ",", "\n", "src_sizes", "=", "self", ".", "tgt_dataset", ".", "sizes", ",", "\n", "src_dict", "=", "self", ".", "tgt_dict", ",", "\n", "tgt", "=", "None", ",", "\n", "tgt_sizes", "=", "None", ",", "\n", "tgt_dict", "=", "None", ",", "\n", ")", "\n", "\n", "generator", "=", "SequenceGenerator", "(", "\n", "tgt_dict", "=", "self", ".", "tgt_dict", ",", "\n", "max_len_a", "=", "0", ",", "\n", "max_len_b", "=", "200", ",", "\n", "beam_size", "=", "2", ",", "\n", "unk_penalty", "=", "0", ",", "\n", "sampling", "=", "False", ",", "\n", ")", "\n", "\n", "backtranslation_dataset", "=", "BacktranslationDataset", "(", "\n", "tgt_dataset", "=", "TransformEosDataset", "(", "\n", "dataset", "=", "tgt_dataset", ",", "\n", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "\n", "# remove eos from the input src", "\n", "remove_eos_from_src", "=", "remove_eos_from_input_src", ",", "\n", ")", ",", "\n", "backtranslation_fn", "=", "(", "\n", "lambda", "net_input", ":", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "{", "'net_input'", ":", "net_input", "}", ")", "\n", ")", ",", "\n", "output_collater", "=", "TransformEosDataset", "(", "\n", "dataset", "=", "tgt_dataset", ",", "\n", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "\n", "# if we remove eos from the input src, then we need to add it", "\n", "# back to the output tgt", "\n", "append_eos_to_tgt", "=", "remove_eos_from_input_src", ",", "\n", "remove_eos_from_src", "=", "remove_eos_from_output_src", ",", "\n", ")", ".", "collater", ",", "\n", "cuda", "=", "self", ".", "cuda", ",", "\n", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "backtranslation_dataset", ",", "\n", "batch_size", "=", "2", ",", "\n", "collate_fn", "=", "backtranslation_dataset", ".", "collater", ",", "\n", ")", "\n", "backtranslation_batch_result", "=", "next", "(", "iter", "(", "dataloader", ")", ")", "\n", "\n", "eos", ",", "pad", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "tgt_dict", ".", "pad", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "\n", "# Note that we sort by src_lengths and add left padding, so actually", "\n", "# ids will look like: [1, 0]", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ",", "[", "pad", ",", "pad", ",", "w1", ",", "eos", "]", "]", ")", "\n", "if", "remove_eos_from_output_src", ":", "\n", "            ", "expected_src", "=", "expected_src", "[", ":", ",", ":", "-", "1", "]", "\n", "", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "eos", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", "]", ")", "\n", "generated_src", "=", "backtranslation_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "backtranslation_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_no_eos_in_output_src": [[96, 99], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_no_eos_in_output_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "False", ",", "remove_eos_from_output_src", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_with_eos_in_output_src": [[101, 104], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_with_eos_in_output_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "False", ",", "remove_eos_from_output_src", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_no_eos_in_input_src": [[106, 109], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_no_eos_in_input_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "True", ",", "remove_eos_from_output_src", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual": [[111, 114], ["test_backtranslation_dataset.TestBacktranslationDataset.assertEqual", "test_backtranslation_dataset.TestBacktranslationDataset.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_utils.TestUtils.test_convert_padding_direction": [[17, 44], ["torch.LongTensor", "torch.LongTensor", "test_utils.TestUtils.assertAlmostEqual", "test_utils.TestUtils.assertAlmostEqual", "fairseq.utils.convert_padding_direction", "fairseq.utils.convert_padding_direction"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_padding_direction", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_padding_direction"], ["    ", "def", "test_convert_padding_direction", "(", "self", ")", ":", "\n", "        ", "pad", "=", "1", "\n", "left_pad", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "1", ",", "7", ",", "8", ",", "9", ",", "10", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "11", ",", "12", "]", ",", "\n", "]", ")", "\n", "right_pad", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "7", ",", "8", ",", "9", ",", "10", ",", "1", "]", ",", "\n", "[", "11", ",", "12", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "right_pad", ",", "\n", "utils", ".", "convert_padding_direction", "(", "\n", "left_pad", ",", "\n", "pad", ",", "\n", "left_to_right", "=", "True", ",", "\n", ")", ",", "\n", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "left_pad", ",", "\n", "utils", ".", "convert_padding_direction", "(", "\n", "right_pad", ",", "\n", "pad", ",", "\n", "right_to_left", "=", "True", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_utils.TestUtils.test_make_positions": [[47, 77], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "test_utils.TestUtils.assertAlmostEqual", "test_utils.TestUtils.assertAlmostEqual", "fairseq.utils.make_positions", "fairseq.utils.make_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.make_positions"], ["", "def", "test_make_positions", "(", "self", ")", ":", "\n", "        ", "pad", "=", "1", "\n", "left_pad_input", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "1", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "9", ",", "9", "]", ",", "\n", "]", ")", "\n", "left_pad_output", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "2", ",", "3", "]", ",", "\n", "]", ")", "\n", "right_pad_input", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "1", "]", ",", "\n", "[", "9", ",", "9", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "right_pad_output", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "1", "]", ",", "\n", "[", "2", ",", "3", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "left_pad_output", ",", "\n", "utils", ".", "make_positions", "(", "left_pad_input", ",", "pad", ",", "left_pad", "=", "True", ")", ",", "\n", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "right_pad_output", ",", "\n", "utils", ".", "make_positions", "(", "right_pad_input", ",", "pad", ",", "left_pad", "=", "False", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_utils.TestUtils.assertAlmostEqual": [[79, 82], ["test_utils.TestUtils.assertEqual", "test_utils.TestUtils.assertLess", "t1.size", "t2.size", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "utils", ".", "item", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.TestLoadCheckpoint.setUp": [[58, 69], ["unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.patch", "p.start", "test_train.TestLoadCheckpoint.patches.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "args_mock", "=", "MagicMock", "(", ")", "\n", "self", ".", "args_mock", ".", "optimizer_overrides", "=", "'{}'", "\n", "self", ".", "patches", "=", "{", "\n", "'os.makedirs'", ":", "MagicMock", "(", ")", ",", "\n", "'os.path.join'", ":", "MagicMock", "(", ")", ",", "\n", "'os.path.isfile'", ":", "MagicMock", "(", "return_value", "=", "True", ")", ",", "\n", "'os.path.isabs'", ":", "MagicMock", "(", "return_value", "=", "False", ")", ",", "\n", "}", "\n", "self", ".", "applied_patches", "=", "[", "patch", "(", "p", ",", "d", ")", "for", "p", ",", "d", "in", "self", ".", "patches", ".", "items", "(", ")", "]", "\n", "[", "p", ".", "start", "(", ")", "for", "p", "in", "self", ".", "applied_patches", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.TestLoadCheckpoint.test_load_partial_checkpoint": [[70, 84], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "train.load_checkpoint", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "def", "test_load_partial_checkpoint", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "2", ",", "150", ",", "200", ",", "50", ")", "\n", "\n", "train", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ",", "epoch_itr", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "50", ")", "\n", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "50", ")", "\n", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "50", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "51", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.TestLoadCheckpoint.test_load_full_checkpoint": [[85, 95], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "train.load_checkpoint", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "", "def", "test_load_full_checkpoint", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "2", ",", "150", ",", "300", ",", "150", ")", "\n", "\n", "train", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ",", "epoch_itr", ")", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.TestLoadCheckpoint.test_load_no_checkpoint": [[96, 107], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "train.load_checkpoint", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.load_checkpoint", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "", "def", "test_load_no_checkpoint", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "0", ",", "150", ",", "0", ",", "0", ")", "\n", "self", ".", "patches", "[", "'os.path.isfile'", "]", ".", "return_value", "=", "False", "\n", "\n", "train", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ",", "epoch_itr", ")", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.TestLoadCheckpoint.tearDown": [[108, 110], ["unittest.mock.patch.stopall"], "methods", ["None"], ["", "", "def", "tearDown", "(", "self", ")", ":", "\n", "        ", "patch", ".", "stopall", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.mock_trainer": [[20, 31], ["unittest.mock.MagicMock"], "function", ["None"], ["def", "mock_trainer", "(", "epoch", ",", "num_updates", ",", "iterations_in_epoch", ")", ":", "\n", "    ", "trainer", "=", "MagicMock", "(", ")", "\n", "trainer", ".", "load_checkpoint", ".", "return_value", "=", "{", "\n", "'train_iterator'", ":", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'iterations_in_epoch'", ":", "iterations_in_epoch", ",", "\n", "'shuffle'", ":", "False", ",", "\n", "}", ",", "\n", "}", "\n", "trainer", ".", "get_num_updates", ".", "return_value", "=", "num_updates", "\n", "return", "trainer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.mock_dict": [[33, 39], ["unittest.mock.MagicMock"], "function", ["None"], ["", "def", "mock_dict", "(", ")", ":", "\n", "    ", "d", "=", "MagicMock", "(", ")", "\n", "d", ".", "pad", ".", "return_value", "=", "1", "\n", "d", ".", "eos", ".", "return_value", "=", "2", "\n", "d", ".", "unk", ".", "return_value", "=", "3", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.get_trainer_and_epoch_itr": [[41, 54], ["torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "test_train.mock_trainer", "fairseq.data.LanguagePairDataset", "fairseq.data.EpochBatchIterator", "test_train.mock_dict", "torch.LongTensor", "list", "torch.LongTensor().view.size", "range", "range"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.mock_trainer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_train.mock_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "get_trainer_and_epoch_itr", "(", "epoch", ",", "epoch_size", ",", "num_updates", ",", "iterations_in_epoch", ")", ":", "\n", "    ", "tokens", "=", "torch", ".", "LongTensor", "(", "list", "(", "range", "(", "epoch_size", ")", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds", "=", "data", ".", "TokenBlockDataset", "(", "\n", "tokens", ",", "sizes", "=", "[", "tokens", ".", "size", "(", "-", "1", ")", "]", ",", "block_size", "=", "1", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "include_targets", "=", "False", ",", "\n", ")", "\n", "trainer", "=", "mock_trainer", "(", "epoch", ",", "num_updates", ",", "iterations_in_epoch", ")", "\n", "dataset", "=", "data", ".", "LanguagePairDataset", "(", "tokens_ds", ",", "tokens_ds", ".", "sizes", ",", "mock_dict", "(", ")", ",", "shuffle", "=", "False", ")", "\n", "epoch_itr", "=", "data", ".", "EpochBatchIterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "collate_fn", "=", "dataset", ".", "collater", ",", "\n", "batch_sampler", "=", "[", "[", "i", "]", "for", "i", "in", "range", "(", "epoch_size", ")", "]", ",", "\n", ")", "\n", "return", "trainer", ",", "epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_fconv": [[29, 36], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["    ", "def", "test_fconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_raw": [[37, 44], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_raw", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_raw'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ",", "[", "'--output-format'", ",", "'raw'", "]", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--raw-text'", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "'--raw-text'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_fp16": [[45, 52], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_fp16", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fp16'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--fp16'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_memory_efficient_fp16": [[53, 60], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_memory_efficient_fp16'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--memory-efficient-fp16'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_update_freq": [[61, 68], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_update_freq", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_update_freq'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--update-freq'", ",", "'3'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_max_positions": [[69, 88], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.TestTranslation.assertTrue", "test_binaries.train_translation_model", "test_binaries.generate_main", "test_binaries.TestTranslation.assertRaises", "test_binaries.train_translation_model", "test_binaries.TestTranslation.assertRaises", "test_binaries.generate_main", "str"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_max_positions", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_max_positions'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", "as", "context", ":", "\n", "                    ", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--max-target-positions'", ",", "'5'", "]", ",", "\n", ")", "\n", "", "self", ".", "assertTrue", "(", "\n", "'skip this example with --skip-invalid-size-inputs-valid-test'", "in", "str", "(", "context", ".", "exception", ")", "\n", ")", "\n", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "\n", "[", "'--max-target-positions'", ",", "'5'", ",", "'--skip-invalid-size-inputs-valid-test'", "]", ",", "\n", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", "as", "context", ":", "\n", "                    ", "generate_main", "(", "data_dir", ")", "\n", "", "generate_main", "(", "data_dir", ",", "[", "'--skip-invalid-size-inputs-valid-test'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_generation": [[89, 108], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main", "test_binaries.generate_main", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_generation", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_sampling'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--sampling'", ",", "\n", "'--sampling-temperature'", ",", "'2'", ",", "\n", "'--beam'", ",", "'2'", ",", "\n", "'--nbest'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--sampling'", ",", "\n", "'--sampling-topk'", ",", "'3'", ",", "\n", "'--beam'", ",", "'2'", ",", "\n", "'--nbest'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "'--prefix-size'", ",", "'2'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_lstm": [[109, 119], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lstm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lstm'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lstm_wiseman_iwslt_de_en'", ",", "[", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_lstm_bidirectional": [[120, 132], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lstm_bidirectional", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lstm_bidirectional'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lstm'", ",", "[", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--encoder-bidirectional'", ",", "\n", "'--encoder-hidden-size'", ",", "'256'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_transformer": [[133, 140], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_transformer", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_transformer'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'transformer_iwslt_de_en'", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_lightconv": [[141, 151], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lightconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lightconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lightconv_iwslt_de_en'", ",", "[", "\n", "'--encoder-conv-type'", ",", "'lightweight'", ",", "\n", "'--decoder-conv-type'", ",", "'lightweight'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_dynamicconv": [[152, 162], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_dynamicconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_dynamicconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lightconv_iwslt_de_en'", ",", "[", "\n", "'--encoder-conv-type'", ",", "'dynamic'", ",", "\n", "'--decoder-conv-type'", ",", "'dynamic'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestTranslation.test_mixture_of_experts": [[163, 180], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main"], ["", "", "", "def", "test_mixture_of_experts", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_moe'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'transformer_iwslt_de_en'", ",", "[", "\n", "'--task'", ",", "'translation_moe'", ",", "\n", "'--method'", ",", "'hMoElp'", ",", "\n", "'--mean-pool-gating-network'", ",", "\n", "'--num-experts'", ",", "'3'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--task'", ",", "'translation_moe'", ",", "\n", "'--method'", ",", "'hMoElp'", ",", "\n", "'--mean-pool-gating-network'", ",", "\n", "'--num-experts'", ",", "'3'", ",", "\n", "'--gen-expert'", ",", "'0'", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestStories.test_fconv_self_att_wp": [[185, 210], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main", "os.rename", "config.extend", "test_binaries.train_translation_model", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model"], ["    ", "def", "test_fconv_self_att_wp", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_self_att_wp'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "config", "=", "[", "\n", "'--encoder-layers'", ",", "'[(512, 3)] * 2'", ",", "\n", "'--decoder-layers'", ",", "'[(512, 3)] * 2'", ",", "\n", "'--decoder-attention'", ",", "'True'", ",", "\n", "'--encoder-attention'", ",", "'False'", ",", "\n", "'--gated-attention'", ",", "'True'", ",", "\n", "'--self-attention'", ",", "'True'", ",", "\n", "'--project-input'", ",", "'True'", ",", "\n", "]", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_self_att_wp'", ",", "config", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n", "# fusion model", "\n", "os", ".", "rename", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'pretrained.pt'", ")", ")", "\n", "config", ".", "extend", "(", "[", "\n", "'--pretrained'", ",", "'True'", ",", "\n", "'--pretrained-checkpoint'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'pretrained.pt'", ")", ",", "\n", "'--save-dir'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'fusion_model'", ")", ",", "\n", "]", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_self_att_wp'", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.TestLanguageModeling.test_fconv_lm": [[214, 221], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_lm_data", "test_binaries.train_language_model", "test_binaries.eval_lm_main"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_lm_data", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_language_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.eval_lm_main"], ["    ", "def", "test_fconv_lm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_lm'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_lm_data", "(", "data_dir", ")", "\n", "train_language_model", "(", "data_dir", ",", "'fconv_lm'", ")", "\n", "eval_lm_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.create_dummy_data": [[223, 242], ["test_binaries.create_dummy_data._create_dummy_data"], "function", ["None"], ["", "", "", "", "def", "create_dummy_data", "(", "data_dir", ",", "num_examples", "=", "1000", ",", "maxlen", "=", "20", ")", ":", "\n", "\n", "    ", "def", "_create_dummy_data", "(", "filename", ")", ":", "\n", "        ", "data", "=", "torch", ".", "rand", "(", "num_examples", "*", "maxlen", ")", "\n", "data", "=", "97", "+", "torch", ".", "floor", "(", "26", "*", "data", ")", ".", "int", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "filename", ")", ",", "'w'", ")", "as", "h", ":", "\n", "            ", "offset", "=", "0", "\n", "for", "_", "in", "range", "(", "num_examples", ")", ":", "\n", "                ", "ex_len", "=", "random", ".", "randint", "(", "1", ",", "maxlen", ")", "\n", "ex_str", "=", "' '", ".", "join", "(", "map", "(", "chr", ",", "data", "[", "offset", ":", "offset", "+", "ex_len", "]", ")", ")", "\n", "print", "(", "ex_str", ",", "file", "=", "h", ")", "\n", "offset", "+=", "ex_len", "\n", "\n", "", "", "", "_create_dummy_data", "(", "'train.in'", ")", "\n", "_create_dummy_data", "(", "'train.out'", ")", "\n", "_create_dummy_data", "(", "'valid.in'", ")", "\n", "_create_dummy_data", "(", "'valid.out'", ")", "\n", "_create_dummy_data", "(", "'test.in'", ")", "\n", "_create_dummy_data", "(", "'test.out'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_translation_data": [[244, 259], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "preprocess_translation_data", "(", "data_dir", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "preprocess_parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "preprocess_args", "=", "preprocess_parser", ".", "parse_args", "(", "\n", "[", "\n", "'--source-lang'", ",", "'in'", ",", "\n", "'--target-lang'", ",", "'out'", ",", "\n", "'--trainpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train'", ")", ",", "\n", "'--validpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid'", ")", ",", "\n", "'--testpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test'", ")", ",", "\n", "'--thresholdtgt'", ",", "'0'", ",", "\n", "'--thresholdsrc'", ",", "'0'", ",", "\n", "'--destdir'", ",", "data_dir", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "preprocess", ".", "main", "(", "preprocess_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_translation_model": [[261, 281], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "train.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "train_translation_model", "(", "data_dir", ",", "arch", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "train_parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "train_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "train_parser", ",", "\n", "[", "\n", "'--task'", ",", "'translation'", ",", "\n", "data_dir", ",", "\n", "'--save-dir'", ",", "data_dir", ",", "\n", "'--arch'", ",", "arch", ",", "\n", "'--optimizer'", ",", "'nag'", ",", "\n", "'--lr'", ",", "'0.05'", ",", "\n", "'--max-tokens'", ",", "'500'", ",", "\n", "'--max-epoch'", ",", "'1'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--distributed-world-size'", ",", "'1'", ",", "\n", "'--source-lang'", ",", "'in'", ",", "\n", "'--target-lang'", ",", "'out'", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "train", ".", "main", "(", "train_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.generate_main": [[283, 310], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate.main", "io.StringIO", "interactive.main", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "generate_main", "(", "data_dir", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "generate_parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "generate_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "generate_parser", ",", "\n", "[", "\n", "data_dir", ",", "\n", "'--path'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", "'--beam'", ",", "'3'", ",", "\n", "'--batch-size'", ",", "'64'", ",", "\n", "'--max-len-b'", ",", "'5'", ",", "\n", "'--gen-subset'", ",", "'valid'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--print-alignment'", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "\n", "# evaluate model in batch mode", "\n", "generate", ".", "main", "(", "generate_args", ")", "\n", "\n", "# evaluate model interactively", "\n", "generate_args", ".", "buffer_size", "=", "0", "\n", "generate_args", ".", "input", "=", "'-'", "\n", "generate_args", ".", "max_sentences", "=", "None", "\n", "orig_stdin", "=", "sys", ".", "stdin", "\n", "sys", ".", "stdin", "=", "StringIO", "(", "'h e l l o\\n'", ")", "\n", "interactive", ".", "main", "(", "generate_args", ")", "\n", "sys", ".", "stdin", "=", "orig_stdin", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.preprocess_lm_data": [[312, 322], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "preprocess_lm_data", "(", "data_dir", ")", ":", "\n", "    ", "preprocess_parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "preprocess_args", "=", "preprocess_parser", ".", "parse_args", "(", "[", "\n", "'--only-source'", ",", "\n", "'--trainpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train.out'", ")", ",", "\n", "'--validpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid.out'", ")", ",", "\n", "'--testpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test.out'", ")", ",", "\n", "'--destdir'", ",", "data_dir", ",", "\n", "]", ")", "\n", "preprocess", ".", "main", "(", "preprocess_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.train_language_model": [[324, 348], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "train.main"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "train_language_model", "(", "data_dir", ",", "arch", ")", ":", "\n", "    ", "train_parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "train_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "train_parser", ",", "\n", "[", "\n", "'--task'", ",", "'language_modeling'", ",", "\n", "data_dir", ",", "\n", "'--arch'", ",", "arch", ",", "\n", "'--optimizer'", ",", "'nag'", ",", "\n", "'--lr'", ",", "'0.1'", ",", "\n", "'--criterion'", ",", "'adaptive_loss'", ",", "\n", "'--adaptive-softmax-cutoff'", ",", "'5,10,15'", ",", "\n", "'--decoder-layers'", ",", "'[(850, 3)] * 2 + [(1024,4)]'", ",", "\n", "'--decoder-embed-dim'", ",", "'280'", ",", "\n", "'--max-tokens'", ",", "'500'", ",", "\n", "'--tokens-per-sample'", ",", "'500'", ",", "\n", "'--save-dir'", ",", "data_dir", ",", "\n", "'--max-epoch'", ",", "'1'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--distributed-world-size'", ",", "'1'", ",", "\n", "'--ddp-backend'", ",", "'no_c10d'", ",", "\n", "]", ",", "\n", ")", "\n", "train", ".", "main", "(", "train_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_binaries.eval_lm_main": [[350, 361], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm.main", "os.path.join"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.spm_decode.main"], ["", "def", "eval_lm_main", "(", "data_dir", ")", ":", "\n", "    ", "eval_lm_parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "eval_lm_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "eval_lm_parser", ",", "\n", "[", "\n", "data_dir", ",", "\n", "'--path'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", "'--no-progress-bar'", ",", "\n", "]", ",", "\n", ")", "\n", "eval_lm", ".", "main", "(", "eval_lm_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.setUp": [[20, 27], ["tests.sequence_generator_setup"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.sequence_generator_setup"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "tgt_dict", ",", "self", ".", "w1", ",", "self", ".", "w2", ",", "src_tokens", ",", "src_lengths", ",", "self", ".", "model", "=", "(", "\n", "test_utils", ".", "sequence_generator_setup", "(", ")", "\n", ")", "\n", "self", ".", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_with_normalization": [[30, 46], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_with_normalization", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_without_normalization": [[47, 65], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_without_normalization", "(", "self", ")", ":", "\n", "# Sentence 1: unchanged from the normalized case", "\n", "# Sentence 2: beams swap order", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "normalize_scores", "=", "False", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_with_lenpen_favoring_short_hypos": [[66, 83], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_with_lenpen_favoring_short_hypos", "(", "self", ")", ":", "\n", "        ", "lenpen", "=", "0.6", "\n", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "len_penalty", "=", "lenpen", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_with_lenpen_favoring_long_hypos": [[84, 101], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_with_lenpen_favoring_long_hypos", "(", "self", ")", ":", "\n", "        ", "lenpen", "=", "5.0", "\n", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "len_penalty", "=", "lenpen", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_maxlen": [[102, 118], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_maxlen", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "max_len_b", "=", "2", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.1", ",", "0.6", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w2", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.3", ",", "0.9", ",", "0.01", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.test_no_stop_early": [[119, 135], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "test_no_stop_early", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "stop_early", "=", "False", ",", "beam_size", "=", "2", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w2", ",", "w2", ",", "w2", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.3", ",", "0.9", ",", "0.99", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.assertHypoTokens": [[136, 138], ["test_sequence_generator.TestSequenceGenerator.assertTensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual"], ["", "def", "assertHypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "assertTensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.assertHypoScore": [[139, 147], ["torch.FloatTensor().log", "test_sequence_generator.TestSequenceGenerator.assertAlmostEqual", "test_sequence_generator.TestSequenceGenerator.assertEqual", "torch.FloatTensor().log.sum", "test_sequence_generator.TestSequenceGenerator.assertLess", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual"], ["", "def", "assertHypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", "\n", "self", ".", "assertEqual", "(", "pos_scores", ".", "numel", "(", ")", ",", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ")", "\n", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "self", ".", "assertLess", "(", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.assertAlmostEqual": [[148, 151], ["test_sequence_generator.TestSequenceGenerator.assertEqual", "test_sequence_generator.TestSequenceGenerator.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestSequenceGenerator.assertTensorEqual": [[152, 155], ["test_sequence_generator.TestSequenceGenerator.assertEqual", "test_sequence_generator.TestSequenceGenerator.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.setUp": [[159, 214], ["tests.dummy_dictionary", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "tests.dummy_dictionary.eos", "torch.LongTensor", "torch.LongTensor", "argparse.Namespace", "tests.TestTranslationTask.setup_task", "tests.TestTranslationTask.setup_task.build_model", "tests.dummy_dictionary.pad", "tests.dummy_dictionary.eos", "tests.dummy_dictionary.unk", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "# construct dummy dictionary", "\n", "        ", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "self", ".", "eos", "=", "d", ".", "eos", "(", ")", "\n", "self", ".", "w1", "=", "4", "\n", "self", ".", "w2", "=", "5", "\n", "\n", "# construct source data", "\n", "self", ".", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "]", ")", "\n", "self", ".", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "# sentence 2:", "\n", "[", "0.9", ",", "unk", ",", "0.1", ",", "0.0", "]", ",", "\n", "[", "0.9", ",", "unk", ",", "0.1", ",", "0.0", "]", ",", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "self", ".", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "self", ".", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.test_diverse_beam_search": [[215, 234], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore"], ["", "def", "test_diverse_beam_search", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "\n", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "diverse_beam_groups", "=", "2", ",", "diverse_beam_strength", "=", "0.", ",", "\n", ")", "\n", "sample", "=", "{", "'net_input'", ":", "{", "'src_tokens'", ":", "self", ".", "src_tokens", ",", "'src_lengths'", ":", "self", ".", "src_lengths", "}", "}", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "eos", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "0.6", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.9", ",", "0.6", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.9", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens": [[235, 237], ["test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual"], ["", "def", "assertHypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "assertTensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertHypoScore": [[238, 246], ["torch.FloatTensor().log", "test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "torch.FloatTensor().log.sum", "test_sequence_generator.TestDiverseBeamSearch.assertLess", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual"], ["", "def", "assertHypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", "\n", "self", ".", "assertEqual", "(", "pos_scores", ".", "numel", "(", ")", ",", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ")", "\n", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "self", ".", "assertLess", "(", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertAlmostEqual": [[247, 250], ["test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_sequence_generator.TestDiverseBeamSearch.assertTensorEqual": [[251, 254], ["test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_average_checkpoints.TestAverageCheckpoints.test_average_checkpoints": [[20, 68], ["collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "tempfile.mkstemp", "tempfile.mkstemp", "torch.save", "torch.save", "os.close", "os.remove", "os.close", "os.remove", "zip", "collections.OrderedDict", "collections.OrderedDict", "scripts.average_checkpoints.average_checkpoints", "collections.OrderedDict.items", "output.items", "test_average_checkpoints.TestAverageCheckpoints.assertEqual", "numpy.testing.assert_allclose", "v_expected.numpy", "v_out.numpy", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "collections.OrderedDict.keys", "output.keys"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.remove", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.average_checkpoints.average_checkpoints", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["    ", "def", "test_average_checkpoints", "(", "self", ")", ":", "\n", "        ", "params_0", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "100.0", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "2.0", ",", "3.0", "]", ",", "[", "4.0", ",", "5.0", ",", "6.0", "]", "]", ")", ")", ",", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "7", ",", "8", ",", "9", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "params_1", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "1.0", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "1.0", ",", "1.0", "]", ",", "[", "1.0", ",", "1.0", ",", "1.0", "]", "]", ")", ")", ",", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "2", ",", "2", ",", "2", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "params_avg", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "50.5", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "1.5", ",", "2.0", "]", ",", "[", "2.5", ",", "3.0", ",", "3.5", "]", "]", ")", ")", ",", "\n", "# We expect truncation for integer division", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "4", ",", "5", ",", "5", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "\n", "fd_0", ",", "path_0", "=", "tempfile", ".", "mkstemp", "(", ")", "\n", "fd_1", ",", "path_1", "=", "tempfile", ".", "mkstemp", "(", ")", "\n", "torch", ".", "save", "(", "collections", ".", "OrderedDict", "(", "[", "(", "'model'", ",", "params_0", ")", "]", ")", ",", "path_0", ")", "\n", "torch", ".", "save", "(", "collections", ".", "OrderedDict", "(", "[", "(", "'model'", ",", "params_1", ")", "]", ")", ",", "path_1", ")", "\n", "\n", "output", "=", "average_checkpoints", "(", "[", "path_0", ",", "path_1", "]", ")", "[", "'model'", "]", "\n", "\n", "os", ".", "close", "(", "fd_0", ")", "\n", "os", ".", "remove", "(", "path_0", ")", "\n", "os", ".", "close", "(", "fd_1", ")", "\n", "os", ".", "remove", "(", "path_1", ")", "\n", "\n", "for", "(", "k_expected", ",", "v_expected", ")", ",", "(", "k_out", ",", "v_out", ")", "in", "zip", "(", "\n", "params_avg", ".", "items", "(", ")", ",", "output", ".", "items", "(", ")", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "\n", "k_expected", ",", "k_out", ",", "'Key mismatch - expected {} but found {}. '", "\n", "'(Expected list of keys: {} vs actual list of keys: {})'", ".", "format", "(", "\n", "k_expected", ",", "k_out", ",", "params_avg", ".", "keys", "(", ")", ",", "output", ".", "keys", "(", ")", "\n", ")", "\n", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "\n", "v_expected", ".", "numpy", "(", ")", ",", "\n", "v_out", ".", "numpy", "(", ")", ",", "\n", "err_msg", "=", "'Tensor value mismatch for key {}'", ".", "format", "(", "k_expected", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset": [[19, 23], ["tests.TestDataset", "fairseq.data.TokenBlockDataset", "len"], "methods", ["None"], ["    ", "def", "_build_dataset", "(", "self", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "        ", "sizes", "=", "[", "len", "(", "x", ")", "for", "x", "in", "data", "]", "\n", "underlying_ds", "=", "test_utils", ".", "TestDataset", "(", "data", ")", "\n", "return", "TokenBlockDataset", "(", "underlying_ds", ",", "sizes", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset.test_eos_break_mode": [[24, 44], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "ds[].tolist", "ds[].tolist", "ds[].tolist", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_eos_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ",", "# this should be filtered", "\n", "torch", ".", "LongTensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "None", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'eos'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", "\n", "\n", "data", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ",", "# this should be filtered", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "None", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'eos'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset.test_block_break_mode": [[45, 56], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "ds[].tolist", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_block_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "9", ",", "1", "]", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "3", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'none'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "2", ",", "1", ",", "8", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "7", ",", "6", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "3", "]", ".", "tolist", "(", ")", ",", "[", "9", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset.test_complete_break_mode": [[57, 77], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "ds[].tolist", "ds[].tolist", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_complete_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "9", ",", "1", "]", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "6", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'complete'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", ",", "9", ",", "1", "]", ")", "\n", "\n", "data", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "4", ",", "3", ",", "2", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "5", ",", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "6", ",", "1", "]", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "3", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'complete'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "6", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestDataset.__init__": [[121, 125], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "sizes", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestDataset.__getitem__": [[126, 128], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestDataset.__len__": [[129, 131], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestTranslationTask.__init__": [[135, 140], ["fairseq.tasks.FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestTranslationTask.setup_task": [[141, 144], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "src_dict", "=", "None", ",", "tgt_dict", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestTranslationTask.build_model": [[145, 147], ["fairseq.utils.TestModel.build_model"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TestModel", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestTranslationTask.source_dictionary": [[148, 151], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestTranslationTask.target_dictionary": [[152, 155], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestModel.__init__": [[158, 160], ["fairseq.models.FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestModel.build_model": [[161, 166], ["fairseq.utils.TestEncoder", "fairseq.utils.TestIncrementalDecoder", "cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "encoder", "=", "TestEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "decoder", "=", "TestIncrementalDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestEncoder.__init__": [[169, 172], ["fairseq.models.FairseqEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestEncoder.forward": [[173, 175], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "src_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestEncoder.reorder_encoder_out": [[176, 178], ["encoder_out.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "return", "encoder_out", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestIncrementalDecoder.__init__": [[181, 186], ["fairseq.models.FairseqIncrementalDecoder.__init__", "getattr", "hasattr", "hasattr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "assert", "hasattr", "(", "args", ",", "'beam_probs'", ")", "or", "hasattr", "(", "args", ",", "'probs'", ")", "\n", "args", ".", "max_decoder_positions", "=", "getattr", "(", "args", ",", "'max_decoder_positions'", ",", "100", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestIncrementalDecoder.forward": [[187, 226], ["prev_output_tokens.size", "len", "encoder_out.size", "prev_output_tokens.size", "hasattr", "torch.rand", "fairseq.utils.get_incremental_state", "fairseq.utils.set_incremental_state", "list", "fairseq.utils.TestIncrementalDecoder.args.probs.index_select", "torch.FloatTensor().zero_", "enumerate", "torch.FloatTensor().zero_.to", "torch.rand.to", "range", "fairseq.utils.TestIncrementalDecoder.args.probs.dim", "torch.LongTensor", "torch.FloatTensor", "len", "len", "fairseq.utils.TestIncrementalDecoder.dictionary.eos", "fairseq.utils.TestIncrementalDecoder.dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bbsz", "=", "prev_output_tokens", ".", "size", "(", "0", ")", "\n", "vocab", "=", "len", "(", "self", ".", "dictionary", ")", "\n", "src_len", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# determine number of steps", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# cache step number", "\n", "            ", "step", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ")", "\n", "if", "step", "is", "None", ":", "\n", "                ", "step", "=", "0", "\n", "", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ",", "step", "+", "1", ")", "\n", "steps", "=", "[", "step", "]", "\n", "", "else", ":", "\n", "            ", "steps", "=", "list", "(", "range", "(", "tgt_len", ")", ")", "\n", "\n", "# define output in terms of raw probs", "\n", "", "if", "hasattr", "(", "self", ".", "args", ",", "'probs'", ")", ":", "\n", "            ", "assert", "self", ".", "args", ".", "probs", ".", "dim", "(", ")", "==", "3", ",", "'expected probs to have size bsz*steps*vocab'", "\n", "probs", "=", "self", ".", "args", ".", "probs", ".", "index_select", "(", "1", ",", "torch", ".", "LongTensor", "(", "steps", ")", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "torch", ".", "FloatTensor", "(", "bbsz", ",", "len", "(", "steps", ")", ",", "vocab", ")", ".", "zero_", "(", ")", "\n", "for", "i", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "# args.beam_probs gives the probability for every vocab element,", "\n", "# starting with eos, then unknown, and then the rest of the vocab", "\n", "                ", "if", "step", "<", "len", "(", "self", ".", "args", ".", "beam_probs", ")", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "]", "=", "self", ".", "args", ".", "beam_probs", "[", "step", "]", "\n", "", "else", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", "]", "=", "1.0", "\n", "\n", "# random attention", "\n", "", "", "", "attn", "=", "torch", ".", "rand", "(", "bbsz", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "dev", "=", "prev_output_tokens", ".", "device", "\n", "return", "probs", ".", "to", "(", "dev", ")", ",", "attn", ".", "to", "(", "dev", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestIncrementalDecoder.get_normalized_probs": [[227, 234], ["probs.log"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "_", ")", ":", "\n", "# the decoder returns probabilities directly", "\n", "        ", "probs", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "probs", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.TestIncrementalDecoder.max_positions": [[235, 237], ["None"], "methods", ["None"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "max_decoder_positions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dictionary": [[22, 29], ["fairseq.data.Dictionary", "range", "fairseq.data.Dictionary.finalize", "fairseq.data.Dictionary.add_symbol", "str"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol"], ["def", "dummy_dictionary", "(", "vocab_size", ",", "prefix", "=", "'token_'", ")", ":", "\n", "    ", "d", "=", "Dictionary", "(", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "token", "=", "prefix", "+", "str", "(", "i", ")", "\n", "d", ".", "add_symbol", "(", "token", ")", "\n", "", "d", ".", "finalize", "(", "padding_factor", "=", "1", ")", "# don't add extra padding symbols", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dataloader": [[31, 53], ["enumerate", "utils.TestDataset", "torch.utils.data.DataLoader", "iter", "len", "fairseq.data.language_pair_dataset.collate"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.collate"], ["", "def", "dummy_dataloader", "(", "\n", "samples", ",", "\n", "padding_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n", "batch_size", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "batch_size", "is", "None", ":", "\n", "        ", "batch_size", "=", "len", "(", "samples", ")", "\n", "\n", "# add any missing data to samples", "\n", "", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "        ", "if", "'id'", "not", "in", "sample", ":", "\n", "            ", "sample", "[", "'id'", "]", "=", "i", "\n", "\n", "# create dataloader", "\n", "", "", "dataset", "=", "TestDataset", "(", "samples", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "collate_fn", "=", "(", "lambda", "samples", ":", "collate", "(", "samples", ",", "padding_idx", ",", "eos_idx", ")", ")", ",", "\n", ")", "\n", "return", "iter", "(", "dataloader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.sequence_generator_setup": [[55, 117], ["utils.dummy_dictionary", "dummy_dictionary.eos", "torch.LongTensor", "torch.LongTensor", "argparse.Namespace", "utils.TestTranslationTask.setup_task", "TestTranslationTask.setup_task.build_model", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "sequence_generator_setup", "(", ")", ":", "\n", "# construct dummy dictionary", "\n", "    ", "d", "=", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct source data", "\n", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "eos", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", "]", ")", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1: 0.9  (emit: w1 <eos>: 0.9*1.0)", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# w2: 0.1", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "# w1: 0.7  (don't emit: w1 <eos>: 0.7*0.25)", "\n", "[", "0.00", ",", "unk", ",", "0.10", ",", "0.9", "]", ",", "# w2: 0.3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.1", ",", "0.9", "]", ",", "# w2 w1: 0.1*0.9", "\n", "[", "0.6", ",", "unk", ",", "0.2", ",", "0.2", "]", ",", "# w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)", "\n", "# sentence 2:", "\n", "[", "0.60", ",", "unk", ",", "0.4", ",", "0.00", "]", ",", "# w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)", "\n", "[", "0.01", ",", "unk", ",", "0.0", ",", "0.99", "]", ",", "# w2 w2: 0.3*0.9", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)", "\n", "# sentence 2:", "\n", "[", "0.1", ",", "unk", ",", "0.5", ",", "0.4", "]", ",", "# w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "return", "tgt_dict", ",", "w1", ",", "w2", ",", "src_tokens", ",", "src_lengths", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adagrad.Adagrad.__init__": [[15, 18], ["FairseqOptimizer.__init__", "torch.optim.Adagrad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "Adagrad", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adagrad.Adagrad.optimizer_config": [[19, 30], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.__init__": [[15, 19], ["object.__init__", "list"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "params", "=", "list", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.add_args": [[20, 24], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.optimizer": [[25, 33], ["hasattr", "isinstance", "ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return a torch.optim.optimizer.Optimizer instance.\"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'_optimizer'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "if", "not", "isinstance", "(", "self", ".", "_optimizer", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'_optimizer must be an instance of torch.optim.Optimizer'", ")", "\n", "", "return", "self", ".", "_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.optimizer_config": [[34, 43], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.get_lr": [[44, 47], ["None"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the current learning rate.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.set_lr": [[48, 52], ["None"], "methods", ["None"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "\"\"\"Set the learning rate.\"\"\"", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.state_dict": [[53, 56], ["fairseq_optimizer.FairseqOptimizer.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict"], ["", "", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.load_state_dict": [[57, 71], ["fairseq_optimizer.FairseqOptimizer.optimizer.load_state_dict", "len", "group.update"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "if", "optimizer_overrides", "is", "not", "None", "and", "len", "(", "optimizer_overrides", ")", ">", "0", ":", "\n", "# override learning rate, momentum, etc. with latest values", "\n", "            ", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "group", ".", "update", "(", "optimizer_overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.backward": [[72, 75], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward"], ["", "", "", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\"\"\"", "\n", "loss", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.multiply_grads": [[76, 81], ["p.grad.data.mul_"], "methods", ["None"], ["", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant *c*.\"\"\"", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                ", "p", ".", "grad", ".", "data", ".", "mul_", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.clip_grad_norm": [[82, 88], ["torch.nn.utils.clip_grad_norm_", "math.sqrt", "sum", "p.grad.data.norm"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.clip_grad_norm_"], ["", "", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm.\"\"\"", "\n", "if", "max_norm", ">", "0", ":", "\n", "            ", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "max_norm", ")", "\n", "", "else", ":", "\n", "            ", "return", "math", ".", "sqrt", "(", "sum", "(", "p", ".", "grad", ".", "data", ".", "norm", "(", ")", "**", "2", "for", "p", "in", "self", ".", "params", "if", "p", ".", "grad", "is", "not", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.step": [[89, 92], ["fairseq_optimizer.FairseqOptimizer.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "optimizer", ".", "step", "(", "closure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fairseq_optimizer.FairseqOptimizer.zero_grad": [[93, 96], ["fairseq_optimizer.FairseqOptimizer.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.__init__": [[15, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "init_scale", "=", "2.", "**", "15", ",", "scale_factor", "=", "2.", ",", "scale_window", "=", "2000", ",", "\n", "tolerance", "=", "0.05", ",", "threshold", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "loss_scale", "=", "init_scale", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "scale_window", "=", "scale_window", "\n", "self", ".", "tolerance", "=", "tolerance", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "_iter", "=", "0", "\n", "self", ".", "_last_overflow_iter", "=", "-", "1", "\n", "self", ".", "_last_rescale_iter", "=", "-", "1", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.update_scale": [[29, 43], ["float", "fp16_optimizer.DynamicLossScaler._decrease_loss_scale"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler._decrease_loss_scale"], ["", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "iter_since_rescale", "=", "self", ".", "_iter", "-", "self", ".", "_last_rescale_iter", "\n", "if", "overflow", ":", "\n", "            ", "self", ".", "_last_overflow_iter", "=", "self", ".", "_iter", "\n", "self", ".", "_overflows_since_rescale", "+=", "1", "\n", "pct_overflow", "=", "self", ".", "_overflows_since_rescale", "/", "float", "(", "iter_since_rescale", ")", "\n", "if", "pct_overflow", ">=", "self", ".", "tolerance", ":", "\n", "                ", "self", ".", "_decrease_loss_scale", "(", ")", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_iter", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "", "", "elif", "(", "self", ".", "_iter", "-", "self", ".", "_last_overflow_iter", ")", "%", "self", ".", "scale_window", "==", "0", ":", "\n", "            ", "self", ".", "loss_scale", "*=", "self", ".", "scale_factor", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_iter", "\n", "", "self", ".", "_iter", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler._decrease_loss_scale": [[44, 48], ["max"], "methods", ["None"], ["", "def", "_decrease_loss_scale", "(", "self", ")", ":", "\n", "        ", "self", ".", "loss_scale", "/=", "self", ".", "scale_factor", "\n", "if", "self", ".", "threshold", "is", "not", "None", ":", "\n", "            ", "self", ".", "loss_scale", "=", "max", "(", "self", ".", "loss_scale", ",", "self", ".", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.has_overflow": [[49, 55], ["float"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "has_overflow", "(", "grad_norm", ")", ":", "\n", "# detect inf and nan", "\n", "        ", "if", "grad_norm", "==", "float", "(", "'inf'", ")", "or", "grad_norm", "!=", "grad_norm", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.__init__": [[62, 82], ["fairseq.optim.FairseqOptimizer.__init__", "fp16_optimizer.DynamicLossScaler", "getattr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "params", ",", "fp32_optimizer", ",", "fp32_params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "fp32_optimizer", "=", "fp32_optimizer", "\n", "self", ".", "fp32_params", "=", "fp32_params", "\n", "\n", "if", "getattr", "(", "args", ",", "'fp16_scale_window'", ",", "None", ")", "is", "None", ":", "\n", "            ", "if", "len", "(", "args", ".", "update_freq", ")", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--fp16-scale-window must be given explicitly when using a '", "\n", "'custom --update-freq schedule'", "\n", ")", "\n", "", "scale_window", "=", "2", "**", "14", "/", "args", ".", "distributed_world_size", "/", "args", ".", "update_freq", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scale_window", "=", "args", ".", "fp16_scale_window", "\n", "\n", "", "self", ".", "scaler", "=", "DynamicLossScaler", "(", "\n", "init_scale", "=", "args", ".", "fp16_init_scale", ",", "\n", "scale_window", "=", "scale_window", ",", "\n", "tolerance", "=", "args", ".", "fp16_scale_tolerance", ",", "\n", "threshold", "=", "args", ".", "threshold_loss_scale", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.build_optimizer": [[84, 104], ["sum", "params[].new().float().new", "torch.nn.Parameter", "torch.nn.Parameter.data.new", "fairseq.optim.build_optimizer", "cls", "p.data.numel", "fp32_params[].copy_", "p.data.numel", "params[].new().float", "p.data.view", "params[].new"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer"], ["", "@", "classmethod", "\n", "def", "build_optimizer", "(", "cls", ",", "args", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"", "\n", "# create FP32 copy of parameters and grads", "\n", "total_param_size", "=", "sum", "(", "p", ".", "data", ".", "numel", "(", ")", "for", "p", "in", "params", ")", "\n", "fp32_params", "=", "params", "[", "0", "]", ".", "new", "(", "0", ")", ".", "float", "(", ")", ".", "new", "(", "total_param_size", ")", "\n", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "            ", "numel", "=", "p", ".", "data", ".", "numel", "(", ")", "\n", "fp32_params", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "p", ".", "data", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "", "fp32_params", "=", "torch", ".", "nn", ".", "Parameter", "(", "fp32_params", ")", "\n", "fp32_params", ".", "grad", "=", "fp32_params", ".", "data", ".", "new", "(", "total_param_size", ")", "\n", "\n", "fp32_optimizer", "=", "optim", ".", "build_optimizer", "(", "args", ",", "[", "fp32_params", "]", ")", "\n", "return", "cls", "(", "args", ",", "params", ",", "fp32_optimizer", ",", "fp32_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.optimizer": [[105, 108], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.optimizer_config": [[109, 112], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "optimizer_config", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.get_lr": [[113, 115], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.set_lr": [[116, 118], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "fp32_optimizer", ".", "set_lr", "(", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.state_dict": [[119, 124], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "state_dict", "=", "self", ".", "fp32_optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'loss_scale'", "]", "=", "self", ".", "scaler", ".", "loss_scale", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.load_state_dict": [[125, 136], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "if", "'loss_scale'", "in", "state_dict", ":", "\n", "            ", "self", ".", "scaler", ".", "loss_scale", "=", "state_dict", "[", "'loss_scale'", "]", "\n", "", "self", ".", "fp32_optimizer", ".", "load_state_dict", "(", "state_dict", ",", "optimizer_overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.backward": [[137, 147], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward"], ["", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"", "\n", "loss", "=", "loss", "*", "self", ".", "scaler", ".", "loss_scale", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "_needs_sync", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32": [[148, 164], ["fp16_optimizer.FP16Optimizer.fp32_params.grad.data.mul_", "grad_data.numel", "fp16_optimizer.FP16Optimizer.fp32_params.grad.data[].copy_", "p.data.new_zeros", "grad_data.view"], "methods", ["None"], ["", "def", "_sync_fp16_grads_to_fp32", "(", "self", ",", "multiply_grads", "=", "1.", ")", ":", "\n", "        ", "if", "self", ".", "_needs_sync", ":", "\n", "# copy FP16 grads to FP32", "\n", "            ", "offset", "=", "0", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "                ", "if", "not", "p", ".", "requires_grad", ":", "\n", "                    ", "continue", "\n", "", "grad_data", "=", "p", ".", "grad", ".", "data", "if", "p", ".", "grad", "is", "not", "None", "else", "p", ".", "data", ".", "new_zeros", "(", "p", ".", "data", ".", "shape", ")", "\n", "numel", "=", "grad_data", ".", "numel", "(", ")", "\n", "self", ".", "fp32_params", ".", "grad", ".", "data", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "grad_data", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# correct for dynamic loss scaler", "\n", "", "self", ".", "fp32_params", ".", "grad", ".", "data", ".", "mul_", "(", "multiply_grads", "/", "self", ".", "scaler", ".", "loss_scale", ")", "\n", "\n", "self", ".", "_needs_sync", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.multiply_grads": [[165, 171], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fp16_optimizer.FP16Optimizer.fp32_params.grad.data.mul_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32"], ["", "", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant ``c``.\"\"\"", "\n", "if", "self", ".", "_needs_sync", ":", "\n", "            ", "self", ".", "_sync_fp16_grads_to_fp32", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_params", ".", "grad", ".", "data", ".", "mul_", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.clip_grad_norm": [[172, 191], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fairseq.utils.clip_grad_norm_", "fp16_optimizer.DynamicLossScaler.has_overflow", "fp16_optimizer.FP16Optimizer.scaler.update_scale", "OverflowError", "FloatingPointError", "str"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.clip_grad_norm_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.has_overflow", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.update_scale"], ["", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"", "\n", "self", ".", "_sync_fp16_grads_to_fp32", "(", ")", "\n", "grad_norm", "=", "utils", ".", "clip_grad_norm_", "(", "self", ".", "fp32_params", ".", "grad", ".", "data", ",", "max_norm", ")", "\n", "\n", "# detect overflow and adjust loss scale", "\n", "overflow", "=", "DynamicLossScaler", ".", "has_overflow", "(", "grad_norm", ")", "\n", "self", ".", "scaler", ".", "update_scale", "(", "overflow", ")", "\n", "if", "overflow", ":", "\n", "            ", "if", "self", ".", "scaler", ".", "loss_scale", "<=", "self", ".", "args", ".", "min_loss_scale", ":", "\n", "# Use FloatingPointError as an uncommon error that parent", "\n", "# functions can safely catch to stop training.", "\n", "                ", "raise", "FloatingPointError", "(", "(", "\n", "'Minimum loss scale reached ({}). Your loss is probably exploding. '", "\n", "'Try lowering the learning rate, using gradient clipping or '", "\n", "'increasing the batch size.'", "\n", ")", ".", "format", "(", "self", ".", "args", ".", "min_loss_scale", ")", ")", "\n", "", "raise", "OverflowError", "(", "'setting loss scale to: '", "+", "str", "(", "self", ".", "scaler", ".", "loss_scale", ")", ")", "\n", "", "return", "grad_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.step": [[192, 205], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fp16_optimizer.FP16Optimizer.fp32_optimizer.step", "p.data.numel", "p.data.copy_", "fp16_optimizer.FP16Optimizer.fp32_params.data[].view_as"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "_sync_fp16_grads_to_fp32", "(", ")", "\n", "self", ".", "fp32_optimizer", ".", "step", "(", "closure", ")", "\n", "\n", "# copy FP32 params back into FP16 model", "\n", "offset", "=", "0", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "if", "not", "p", ".", "requires_grad", ":", "\n", "                ", "continue", "\n", "", "numel", "=", "p", ".", "data", ".", "numel", "(", ")", "\n", "p", ".", "data", ".", "copy_", "(", "self", ".", "fp32_params", ".", "data", "[", "offset", ":", "offset", "+", "numel", "]", ".", "view_as", "(", "p", ".", "data", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.FP16Optimizer.zero_grad": [[206, 214], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.zero_grad", "p.grad.detach_", "p.grad.zero_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "self", ".", "fp32_optimizer", ".", "zero_grad", "(", ")", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                ", "p", ".", "grad", ".", "detach_", "(", ")", "\n", "p", ".", "grad", ".", "zero_", "(", ")", "\n", "", "", "self", ".", "_needs_sync", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.__init__": [[222, 233], ["isinstance", "map", "p.data.float", "p.grad.data.float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "params", ")", ":", "\n", "\n", "        ", "def", "convert_to_fp32", "(", "p", ")", ":", "\n", "            ", "p", ".", "data", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                ", "p", ".", "grad", ".", "data", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "", "return", "p", "\n", "\n", "", "assert", "isinstance", "(", "params", ",", "list", ")", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "itr", "=", "map", "(", "convert_to_fp32", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.wrap_optimizer_": [[234, 238], ["fp16_optimizer.ConvertToFP32"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "wrap_optimizer_", "(", "optimizer", ")", ":", "\n", "        ", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "group", "[", "'params'", "]", "=", "ConvertToFP32", "(", "group", "[", "'params'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.unwrap_optimizer_": [[239, 247], ["p.data.half", "p.grad.data.half"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "unwrap_optimizer_", "(", "optimizer", ")", ":", "\n", "        ", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "group", "[", "'params'", "]", "=", "group", "[", "'params'", "]", ".", "params", "# unwrap from ConvertToFP32", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "p", ".", "data", "=", "p", ".", "data", ".", "half", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "p", ".", "grad", ".", "data", "=", "p", ".", "grad", ".", "data", ".", "half", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.__len__": [[248, 250], ["len"], "methods", ["None"], ["", "", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.__iter__": [[251, 256], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "itr", "is", "not", "None", ":", "\n", "            ", "return", "self", "\n", "", "else", ":", "\n", "            ", "return", "iter", "(", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.__next__": [[257, 263], ["next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "next", "(", "self", ".", "itr", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "self", ".", "itr", "=", "None", "\n", "raise", "StopIteration", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.__init__": [[274, 293], ["fairseq.optim.FairseqOptimizer.__init__", "fp16_optimizer.DynamicLossScaler", "getattr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "params", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "wrapped_optimizer", "=", "optimizer", "\n", "\n", "if", "getattr", "(", "args", ",", "'fp16_scale_window'", ",", "None", ")", "is", "None", ":", "\n", "            ", "if", "len", "(", "args", ".", "update_freq", ")", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--fp16-scale-window must be given explicitly when using a '", "\n", "'custom --update-freq schedule'", "\n", ")", "\n", "", "scale_window", "=", "2", "**", "14", "/", "args", ".", "distributed_world_size", "/", "args", ".", "update_freq", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scale_window", "=", "args", ".", "fp16_scale_window", "\n", "\n", "", "self", ".", "scaler", "=", "DynamicLossScaler", "(", "\n", "init_scale", "=", "args", ".", "fp16_init_scale", ",", "\n", "scale_window", "=", "scale_window", ",", "\n", "tolerance", "=", "args", ".", "fp16_scale_tolerance", ",", "\n", "threshold", "=", "args", ".", "threshold_loss_scale", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.build_optimizer": [[295, 304], ["fairseq.optim.build_optimizer", "cls"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer"], ["", "@", "classmethod", "\n", "def", "build_optimizer", "(", "cls", ",", "args", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"", "\n", "fp16_optimizer", "=", "optim", ".", "build_optimizer", "(", "args", ",", "params", ")", "\n", "return", "cls", "(", "args", ",", "params", ",", "fp16_optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.optimizer": [[305, 308], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.optimizer_config": [[309, 312], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "optimizer_config", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr": [[313, 315], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr": [[316, 318], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "wrapped_optimizer", ".", "set_lr", "(", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.state_dict": [[319, 324], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "state_dict", "=", "self", ".", "wrapped_optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'loss_scale'", "]", "=", "self", ".", "scaler", ".", "loss_scale", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.load_state_dict": [[325, 338], ["fp16_optimizer.ConvertToFP32.wrap_optimizer_", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.load_state_dict", "fp16_optimizer.ConvertToFP32.unwrap_optimizer_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.wrap_optimizer_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.unwrap_optimizer_"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "if", "'loss_scale'", "in", "state_dict", ":", "\n", "            ", "self", ".", "scaler", ".", "loss_scale", "=", "state_dict", "[", "'loss_scale'", "]", "\n", "", "ConvertToFP32", ".", "wrap_optimizer_", "(", "self", ".", "wrapped_optimizer", ".", "optimizer", ")", "\n", "self", ".", "wrapped_optimizer", ".", "load_state_dict", "(", "state_dict", ",", "optimizer_overrides", ")", "\n", "ConvertToFP32", ".", "unwrap_optimizer_", "(", "self", ".", "wrapped_optimizer", ".", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.backward": [[339, 349], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward"], ["", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"", "\n", "loss", "=", "loss", "*", "self", ".", "scaler", ".", "loss_scale", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "_grads_are_scaled", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads": [[350, 358], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.multiply_grads"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads"], ["", "def", "_unscale_grads", "(", "self", ",", "multiply_grads", "=", "1.", ")", ":", "\n", "        ", "if", "self", ".", "_grads_are_scaled", ":", "\n", "            ", "self", ".", "_grads_are_scaled", "=", "False", "\n", "\n", "# correct for dynamic loss scaler", "\n", "self", ".", "wrapped_optimizer", ".", "multiply_grads", "(", "multiply_grads", "/", "self", ".", "scaler", ".", "loss_scale", ")", "\n", "", "else", ":", "\n", "            ", "assert", "multiply_grads", "==", "1.", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads": [[359, 365], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.multiply_grads"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads"], ["", "", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant *c*.\"\"\"", "\n", "if", "self", ".", "_grads_are_scaled", ":", "\n", "            ", "self", ".", "_unscale_grads", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "wrapped_optimizer", ".", "multiply_grads", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm": [[366, 386], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.clip_grad_norm", "fp16_optimizer.DynamicLossScaler.has_overflow", "fp16_optimizer.MemoryEfficientFP16Optimizer.scaler.update_scale", "OverflowError", "FloatingPointError", "str"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.has_overflow", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.DynamicLossScaler.update_scale"], ["", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"", "\n", "self", ".", "_unscale_grads", "(", ")", "\n", "grad_norm", "=", "self", ".", "wrapped_optimizer", ".", "clip_grad_norm", "(", "max_norm", ")", "\n", "\n", "# detect overflow and adjust loss scale", "\n", "overflow", "=", "DynamicLossScaler", ".", "has_overflow", "(", "grad_norm", ")", "\n", "self", ".", "scaler", ".", "update_scale", "(", "overflow", ")", "\n", "if", "overflow", ":", "\n", "            ", "if", "self", ".", "scaler", ".", "loss_scale", "<=", "self", ".", "args", ".", "min_loss_scale", ":", "\n", "# Use FloatingPointError as an uncommon error that parent", "\n", "# functions can safely catch to stop training.", "\n", "                ", "raise", "FloatingPointError", "(", "(", "\n", "'Minimum loss scale reached ({}). Your loss is probably exploding. '", "\n", "'Try lowering the learning rate, using gradient clipping or '", "\n", "'increasing the batch size.'", "\n", ")", ".", "format", "(", "self", ".", "args", ".", "min_loss_scale", ")", ")", "\n", "", "raise", "OverflowError", "(", "'setting loss scale to: '", "+", "str", "(", "self", ".", "scaler", ".", "loss_scale", ")", ")", "\n", "\n", "", "return", "grad_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.step": [[387, 398], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.ConvertToFP32.wrap_optimizer_", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.step", "fp16_optimizer.ConvertToFP32.unwrap_optimizer_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.wrap_optimizer_", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.ConvertToFP32.unwrap_optimizer_"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "_unscale_grads", "(", ")", "\n", "\n", "# convert params and grads to FP32 (lazily)", "\n", "ConvertToFP32", ".", "wrap_optimizer_", "(", "self", ".", "wrapped_optimizer", ".", "optimizer", ")", "\n", "\n", "self", ".", "wrapped_optimizer", ".", "step", "(", "closure", ")", "\n", "\n", "# convert params back to FP16", "\n", "ConvertToFP32", ".", "unwrap_optimizer_", "(", "self", ".", "wrapped_optimizer", ".", "optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad": [[399, 403], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "self", ".", "wrapped_optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "_grads_are_scaled", "=", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adam.FairseqAdam.__init__": [[17, 20], ["FairseqOptimizer.__init__", "adam.Adam"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "Adam", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adam.FairseqAdam.add_args": [[21, 29], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--adam-betas'", ",", "default", "=", "'(0.9, 0.999)'", ",", "metavar", "=", "'B'", ",", "\n", "help", "=", "'betas for Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for Adam optimizer'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adam.FairseqAdam.optimizer_config": [[31, 44], ["eval"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'betas'", ":", "eval", "(", "self", ".", "args", ".", "adam_betas", ")", ",", "\n", "'eps'", ":", "self", ".", "args", ".", "adam_eps", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adam.Adam.__init__": [[74, 79], ["dict", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "Adam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adam.Adam.step": [[80, 141], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.max", "torch.max", "torch.max", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "p.data.add_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "math.sqrt", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.build_optimizer": [[19, 22], ["list", "filter"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.__init__.register_optimizer": [[24, 41], ["OPTIMIZER_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.nag.FairseqNAG.__init__": [[15, 18], ["FairseqOptimizer.__init__", "nag.NAG"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "NAG", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.nag.FairseqNAG.optimizer_config": [[19, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.nag.NAG.__init__": [[35, 38], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "momentum", "=", "0", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "lr_old", "=", "lr", ",", "momentum", "=", "momentum", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", "NAG", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.nag.NAG.step": [[39, 78], ["closure", "group.get", "p.data.add_", "p.data.add_", "buf.mul_().add_", "d_p.clone().zero_", "p.data.mul_", "buf.mul_", "d_p.clone"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "lr", "=", "group", "[", "'lr'", "]", "\n", "lr_old", "=", "group", ".", "get", "(", "'lr_old'", ",", "lr", ")", "\n", "lr_correct", "=", "lr", "/", "lr_old", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "d_p", "=", "p", ".", "grad", ".", "data", "\n", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                    ", "param_state", "[", "'momentum_buffer'", "]", "=", "d_p", ".", "clone", "(", ")", ".", "zero_", "(", ")", "\n", "\n", "", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1", "-", "lr", "*", "weight_decay", ")", "\n", "", "p", ".", "data", ".", "add_", "(", "momentum", "*", "momentum", "*", "lr_correct", ",", "buf", ")", "\n", "p", ".", "data", ".", "add_", "(", "-", "(", "1", "+", "momentum", ")", "*", "lr", ",", "d_p", ")", "\n", "\n", "buf", ".", "mul_", "(", "momentum", "*", "lr_correct", ")", ".", "add_", "(", "-", "lr", ",", "d_p", ")", "\n", "\n", "", "group", "[", "'lr_old'", "]", "=", "lr", "\n", "\n", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.sgd.SGD.__init__": [[15, 18], ["FairseqOptimizer.__init__", "torch.optim.SGD"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.sgd.SGD.optimizer_config": [[19, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.FairseqAdafactor.__init__": [[17, 20], ["FairseqOptimizer.__init__", "adafactor.Adafactor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "Adafactor", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.FairseqAdafactor.add_args": [[21, 38], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--adafactor-eps'", ",", "default", "=", "'(1e-30, 1e-3)'", ",", "metavar", "=", "\"E\"", ",", "\n", "help", "=", "'epsilons for Adafactor optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-threshold'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "\"C\"", ",", "\n", "help", "=", "'threshold for clipping update root mean square'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "type", "=", "float", ",", "default", "=", "-", "0.8", ",", "metavar", "=", "\"D\"", ",", "\n", "help", "=", "'decay rate of the second moment estimator'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "\"B\"", ",", "\n", "help", "=", "'beta for first moment estimator. Optional'", ")", "\n", "parser", ".", "add_argument", "(", "'--scale-parameter'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'scale learning rate by root mean square of parameter.'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use relative step for warm-up learning rate schedule'", ")", "\n", "parser", ".", "add_argument", "(", "'--relative-step'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'set learning rate to inverse square root of timestep.'", "\n", "'If false, external learning rate applied'", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.FairseqAdafactor.optimizer_config": [[40, 60], ["eval"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        Note : Convergence issues empirically observed with fp16 on.\n               Might require search for appropriate configuration.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'eps'", ":", "eval", "(", "self", ".", "args", ".", "adafactor_eps", ")", ",", "\n", "'clip_threshold'", ":", "self", ".", "args", ".", "clip_threshold", ",", "\n", "'beta1'", ":", "self", ".", "args", ".", "beta1", ",", "\n", "'decay_rate'", ":", "self", ".", "args", ".", "decay_rate", ",", "\n", "'scale_parameter'", ":", "self", ".", "args", ".", "scale_parameter", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "'relative_step'", ":", "self", ".", "args", ".", "relative_step", ",", "\n", "'warmup_init'", ":", "self", ".", "args", ".", "warmup_init", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor.__init__": [[91, 98], ["dict", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "None", ",", "eps", "=", "(", "1e-30", ",", "1e-3", ")", ",", "clip_threshold", "=", "1.0", ",", "\n", "decay_rate", "=", "-", "0.8", ",", "beta1", "=", "None", ",", "weight_decay", "=", "0.0", ",", "scale_parameter", "=", "True", ",", "\n", "relative_step", "=", "True", ",", "warmup_init", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "clip_threshold", "=", "clip_threshold", ",", "decay_rate", "=", "decay_rate", ",", "\n", "beta1", "=", "beta1", ",", "weight_decay", "=", "weight_decay", ",", "scale_parameter", "=", "scale_parameter", ",", "\n", "relative_step", "=", "relative_step", ",", "warmup_init", "=", "warmup_init", ")", "\n", "super", "(", "Adafactor", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._get_lr": [[99, 108], ["min", "max", "math.sqrt"], "methods", ["None"], ["", "def", "_get_lr", "(", "self", ",", "param_group", ",", "param_state", ")", ":", "\n", "        ", "rel_step_sz", "=", "param_group", "[", "'lr'", "]", "\n", "if", "param_group", "[", "'relative_step'", "]", ":", "\n", "            ", "min_step", "=", "1e-6", "*", "param_state", "[", "'step'", "]", "if", "param_group", "[", "'warmup_init'", "]", "else", "1e-2", "\n", "rel_step_sz", "=", "min", "(", "min_step", ",", "1.0", "/", "math", ".", "sqrt", "(", "param_state", "[", "'step'", "]", ")", ")", "\n", "", "param_scale", "=", "1.0", "\n", "if", "param_group", "[", "'scale_parameter'", "]", ":", "\n", "            ", "param_scale", "=", "max", "(", "param_group", "[", "'eps'", "]", "[", "1", "]", ",", "param_state", "[", "'RMS'", "]", ")", "\n", "", "return", "param_scale", "*", "rel_step_sz", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._get_options": [[109, 113], ["len"], "methods", ["None"], ["", "def", "_get_options", "(", "self", ",", "param_group", ",", "param_shape", ")", ":", "\n", "        ", "factored", "=", "len", "(", "param_shape", ")", ">=", "2", "\n", "use_first_moment", "=", "param_group", "[", "'beta1'", "]", "is", "not", "None", "\n", "return", "factored", ",", "use_first_moment", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._rms": [[114, 116], ["tensor.norm", "tensor.numel"], "methods", ["None"], ["", "def", "_rms", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "norm", "(", "2", ")", "/", "(", "tensor", ".", "numel", "(", ")", "**", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._approx_sq_grad": [[117, 121], ["exp_avg_sq_col.unsqueeze().rsqrt", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "exp_avg_sq_col.unsqueeze", "exp_avg_sq_row.mean"], "methods", ["None"], ["", "def", "_approx_sq_grad", "(", "self", ",", "exp_avg_sq_row", ",", "exp_avg_sq_col", ",", "output", ")", ":", "\n", "        ", "r_factor", "=", "(", "exp_avg_sq_row", "/", "exp_avg_sq_row", ".", "mean", "(", "dim", "=", "-", "1", ")", ")", ".", "rsqrt_", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "c_factor", "=", "exp_avg_sq_col", ".", "unsqueeze", "(", "-", "2", ")", ".", "rsqrt", "(", ")", "\n", "torch", ".", "mul", "(", "r_factor", ",", "c_factor", ",", "out", "=", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor.step": [[122, 196], ["closure", "adafactor.Adafactor._get_options", "adafactor.Adafactor._rms", "adafactor.Adafactor._get_lr", "update.div_", "update.mul_", "p.data.add_", "RuntimeError", "len", "math.pow", "exp_avg_sq_row.mul_().add_", "exp_avg_sq_col.mul_().add_", "adafactor.Adafactor._approx_sq_grad", "update.mul_", "exp_avg_sq.mul_().add_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "max", "exp_avg.mul_().add_", "p.data.add_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "update.mean", "update.mean", "exp_avg_sq_row.mul_", "exp_avg_sq_col.mul_", "exp_avg_sq.mul_", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "adafactor.Adafactor._rms", "exp_avg.mul_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._get_options", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._rms", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._approx_sq_grad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.adafactor.Adafactor._rms"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adafactor does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "grad_shape", "=", "grad", ".", "shape", "\n", "\n", "factored", ",", "use_first_moment", "=", "self", ".", "_get_options", "(", "group", ",", "grad_shape", ")", "\n", "# State Initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "\n", "if", "use_first_moment", ":", "\n", "# Exponential moving average of gradient values", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "1", "]", ")", ".", "type_as", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "2", "]", "+", "grad_shape", "[", "-", "1", ":", "]", ")", ".", "type_as", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "\n", "", "state", "[", "'RMS'", "]", "=", "0", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "state", "[", "'RMS'", "]", "=", "self", ".", "_rms", "(", "p", ".", "data", ")", "\n", "group", "[", "'lr'", "]", "=", "self", ".", "_get_lr", "(", "group", ",", "state", ")", "\n", "\n", "beta2t", "=", "1.0", "-", "math", ".", "pow", "(", "state", "[", "'step'", "]", ",", "group", "[", "'decay_rate'", "]", ")", "\n", "update", "=", "(", "grad", "**", "2", ")", "+", "group", "[", "'eps'", "]", "[", "0", "]", "\n", "if", "factored", ":", "\n", "                    ", "exp_avg_sq_row", "=", "state", "[", "'exp_avg_sq_row'", "]", "\n", "exp_avg_sq_col", "=", "state", "[", "'exp_avg_sq_col'", "]", "\n", "\n", "exp_avg_sq_row", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "1", ")", ")", "\n", "exp_avg_sq_col", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "2", ")", ")", "\n", "\n", "# Approximation of exponential moving average of square of gradient", "\n", "self", ".", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ",", "update", ")", "\n", "update", ".", "mul_", "(", "grad", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", "=", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ")", "\n", "torch", ".", "rsqrt", "(", "exp_avg_sq", ",", "out", "=", "update", ")", ".", "mul_", "(", "grad", ")", "\n", "\n", "", "update", ".", "div_", "(", "max", "(", "1.0", ",", "self", ".", "_rms", "(", "update", ")", "/", "group", "[", "'clip_threshold'", "]", ")", ")", "\n", "update", ".", "mul_", "(", "group", "[", "'lr'", "]", ")", "\n", "\n", "if", "use_first_moment", ":", "\n", "                    ", "exp_avg", "=", "state", "[", "'exp_avg'", "]", "\n", "exp_avg", ".", "mul_", "(", "group", "[", "'beta1'", "]", ")", ".", "add_", "(", "1", "-", "group", "[", "'beta1'", "]", ",", "update", ")", "\n", "update", "=", "exp_avg", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "p", ".", "data", ".", "add_", "(", "-", "update", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.__init__": [[17, 26], ["FairseqLRScheduler.__init__", "torch.optim.lr_scheduler.ReduceLROnPlateau", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with reduce_lr_on_plateau.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "self", ".", "optimizer", ".", "optimizer", ",", "patience", "=", "0", ",", "factor", "=", "args", ".", "lr_shrink", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.state_dict": [[27, 32], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "\n", "'best'", ":", "self", ".", "lr_scheduler", ".", "best", ",", "\n", "'last_epoch'", ":", "self", ".", "lr_scheduler", ".", "last_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.load_state_dict": [[34, 39], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "lr_scheduler", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "if", "'last_epoch'", "in", "state_dict", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "state_dict", "[", "'last_epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.step": [[40, 47], ["reduce_lr_on_plateau.ReduceLROnPlateau.optimizer.get_lr", "reduce_lr_on_plateau.ReduceLROnPlateau.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step"], ["", "", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "step", "(", "val_loss", ",", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "epoch", "\n", "", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.__init__": [[31, 51], ["FairseqLRScheduler.__init__", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with inverse_sqrt.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "warmup_end_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_init_lr", "<", "0", ":", "\n", "            ", "args", ".", "warmup_init_lr", "=", "warmup_end_lr", "\n", "\n", "# linearly warmup for the first args.warmup_updates", "\n", "", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "args", ".", "warmup_init_lr", ")", "/", "args", ".", "warmup_updates", "\n", "\n", "# then, decay prop. to the inverse square root of the update number", "\n", "self", ".", "decay_factor", "=", "warmup_end_lr", "*", "args", ".", "warmup_updates", "**", "0.5", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "args", ".", "warmup_init_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.add_args": [[52, 60], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "4000", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial learning rate during warmup phase; default is args.lr'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step": [[62, 67], ["super().step", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step_update": [[68, 76], ["inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "num_updates", "<", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "args", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "decay_factor", "*", "num_updates", "**", "-", "0.5", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.__init__": [[13, 20], ["object.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "optimizer", ",", "FairseqOptimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'optimizer must be an instance of FairseqOptimizer'", ")", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "best", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.add_args": [[21, 25], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict": [[26, 29], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "'best'", ":", "self", ".", "best", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict": [[30, 33], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step": [[34, 41], ["min"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "best", "is", "None", ":", "\n", "                ", "self", ".", "best", "=", "val_loss", "\n", "", "else", ":", "\n", "                ", "self", ".", "best", "=", "min", "(", "self", ".", "best", ",", "val_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step_update": [[42, 45], ["fairseq_lr_scheduler.FairseqLRScheduler.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "", "", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.__init__": [[15, 26], ["FairseqLRScheduler.__init__", "getattr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "\n", "# set defaults", "\n", "args", ".", "warmup_updates", "=", "getattr", "(", "args", ",", "'warmup_updates'", ",", "0", ")", "or", "0", "\n", "\n", "self", ".", "lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_updates", ">", "0", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1.", "/", "args", ".", "warmup_updates", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.add_args": [[27, 35], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--force-anneal'", ",", "'--fa'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force annealing at specified epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.get_next_lr": [[37, 46], ["min", "len"], "methods", ["None"], ["", "def", "get_next_lr", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "lrs", "=", "self", ".", "args", ".", "lr", "\n", "if", "self", ".", "args", ".", "force_anneal", "is", "None", "or", "epoch", "<", "self", ".", "args", ".", "force_anneal", ":", "\n", "# use fixed LR schedule", "\n", "            ", "next_lr", "=", "lrs", "[", "min", "(", "epoch", ",", "len", "(", "lrs", ")", "-", "1", ")", "]", "\n", "", "else", ":", "\n", "# annneal based on lr_shrink", "\n", "            ", "next_lr", "=", "lrs", "[", "-", "1", "]", "*", "self", ".", "args", ".", "lr_shrink", "**", "(", "epoch", "+", "1", "-", "self", ".", "args", ".", "force_anneal", ")", "\n", "", "return", "next_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.step": [[47, 53], ["super().step", "fixed_schedule.FixedSchedule.get_next_lr", "fixed_schedule.FixedSchedule.optimizer.set_lr", "fixed_schedule.FixedSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.get_next_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "self", ".", "lr", "=", "self", ".", "get_next_lr", "(", "epoch", ")", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.fixed_schedule.FixedSchedule.step_update": [[54, 60], ["fixed_schedule.FixedSchedule.optimizer.get_lr", "fixed_schedule.FixedSchedule.optimizer.set_lr", "float"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "self", ".", "args", ".", "warmup_updates", ">", "0", "and", "num_updates", "<=", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "num_updates", "/", "float", "(", "self", ".", "args", ".", "warmup_updates", ")", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.__init__.build_lr_scheduler": [[17, 19], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.__init__.register_lr_scheduler": [[21, 33], ["ValueError", "issubclass", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.__init__": [[20, 40], ["FairseqLRScheduler.__init__", "triangular_lr_scheduler.TriangularSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with triangular.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "\n", "", "lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "\n", "assert", "args", ".", "max_lr", ">", "lr", ",", "'max_lr must be more than lr'", "\n", "self", ".", "min_lr", "=", "lr", "\n", "self", ".", "max_lr", "=", "args", ".", "max_lr", "\n", "self", ".", "stepsize", "=", "args", ".", "lr_period_updates", "//", "2", "\n", "self", ".", "lr_shrink", "=", "args", ".", "lr_shrink", "\n", "self", ".", "shrink_min", "=", "args", ".", "shrink_min", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "self", ".", "min_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.add_args": [[41, 51], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--max-lr'", ",", "required", "=", "True", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'max learning rate, must be more than args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-period-updates'", ",", "default", "=", "5000", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial number of updates per period (cycle length)'", ")", "\n", "parser", ".", "add_argument", "(", "'--shrink-min'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, also shrinks min lr'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.step": [[53, 58], ["super().step", "triangular_lr_scheduler.TriangularSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.step_update": [[59, 75], ["math.floor", "abs", "triangular_lr_scheduler.TriangularSchedule.optimizer.set_lr", "max"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "cycle", "=", "math", ".", "floor", "(", "num_updates", "/", "(", "2", "*", "self", ".", "stepsize", ")", ")", "\n", "\n", "lr_shrink", "=", "self", ".", "lr_shrink", "**", "cycle", "\n", "max_lr", "=", "self", ".", "max_lr", "*", "lr_shrink", "\n", "if", "self", ".", "shrink_min", ":", "\n", "            ", "min_lr", "=", "self", ".", "min_lr", "*", "lr_shrink", "\n", "", "else", ":", "\n", "            ", "min_lr", "=", "self", ".", "min_lr", "\n", "\n", "", "x", "=", "abs", "(", "num_updates", "/", "self", ".", "stepsize", "-", "2", "*", "(", "cycle", "+", "1", ")", "+", "1", ")", "\n", "self", ".", "lr", "=", "min_lr", "+", "(", "max_lr", "-", "min_lr", ")", "*", "max", "(", "0", ",", "(", "1", "-", "x", ")", ")", "\n", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.__init__": [[37, 69], ["FairseqLRScheduler.__init__", "cosine_lr_scheduler.CosineSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with cosine.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "\n", "", "warmup_end_lr", "=", "args", ".", "max_lr", "\n", "if", "args", ".", "warmup_init_lr", "<", "0", ":", "\n", "            ", "args", ".", "warmup_init_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "\n", "", "self", ".", "min_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "self", ".", "max_lr", "=", "args", ".", "max_lr", "\n", "\n", "assert", "self", ".", "max_lr", ">", "self", ".", "min_lr", ",", "'max_lr must be more than lr'", "\n", "\n", "self", ".", "t_mult", "=", "args", ".", "t_mult", "\n", "self", ".", "period", "=", "args", ".", "lr_period_updates", "\n", "\n", "if", "args", ".", "warmup_updates", ">", "0", ":", "\n", "# linearly warmup for the first args.warmup_updates", "\n", "            ", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "args", ".", "warmup_init_lr", ")", "/", "args", ".", "warmup_updates", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr_step", "=", "1", "\n", "\n", "", "self", ".", "warmup_updates", "=", "args", ".", "warmup_updates", "\n", "self", ".", "lr_shrink", "=", "args", ".", "lr_shrink", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "args", ".", "warmup_init_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.add_args": [[70, 84], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial learning rate during warmup phase; default is args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-lr'", ",", "required", "=", "True", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'max learning rate, must be more than args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--t-mult'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'factor to grow the length of each period'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-period-updates'", ",", "default", "=", "5000", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial number of updates per period'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step": [[86, 91], ["super().step", "cosine_lr_scheduler.CosineSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step", "home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step_update": [[92, 115], ["cosine_lr_scheduler.CosineSchedule.optimizer.set_lr", "math.floor", "math.floor", "math.log", "math.cos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "num_updates", "<", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "args", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "curr_updates", "=", "num_updates", "-", "self", ".", "args", ".", "warmup_updates", "\n", "if", "self", ".", "t_mult", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "curr_updates", "/", "self", ".", "period", "*", "(", "1", "-", "self", ".", "t_mult", ")", ",", "self", ".", "t_mult", ")", ")", "\n", "t_i", "=", "self", ".", "t_mult", "**", "i", "*", "self", ".", "period", "\n", "t_curr", "=", "curr_updates", "-", "(", "1", "-", "self", ".", "t_mult", "**", "i", ")", "/", "(", "1", "-", "self", ".", "t_mult", ")", "*", "self", ".", "period", "\n", "", "else", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "curr_updates", "/", "self", ".", "period", ")", "\n", "t_i", "=", "self", ".", "period", "\n", "t_curr", "=", "curr_updates", "-", "(", "self", ".", "period", "*", "i", ")", "\n", "\n", "", "lr_shrink", "=", "self", ".", "lr_shrink", "**", "i", "\n", "min_lr", "=", "self", ".", "min_lr", "*", "lr_shrink", "\n", "max_lr", "=", "self", ".", "max_lr", "*", "lr_shrink", "\n", "\n", "self", ".", "lr", "=", "min_lr", "+", "0.5", "*", "(", "max_lr", "-", "min_lr", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "t_curr", "/", "t_i", ")", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_input.AdaptiveInput.__init__": [[17, 59], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "range", "adaptive_input.AdaptiveInput.apply", "adaptive_input.AdaptiveInput.register_buffer", "len", "int", "torch.nn.Sequential", "adaptive_input.AdaptiveInput.embeddings.append", "isinstance", "torch.FloatTensor", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.constant_", "hasattr", "torch.nn.init.xavier_uniform_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", "initial_dim", ":", "int", ",", "\n", "factor", ":", "float", ",", "\n", "output_dim", ":", "int", ",", "\n", "cutoff", ":", "List", "[", "int", "]", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "vocab_size", ">", "cutoff", "[", "-", "1", "]", ":", "\n", "            ", "cutoff", "=", "cutoff", "+", "[", "vocab_size", "]", "\n", "", "else", ":", "\n", "            ", "assert", "vocab_size", "==", "cutoff", "[", "\n", "-", "1", "]", ",", "'cannot specify cutoff larger than vocab size'", "\n", "\n", "", "self", ".", "cutoff", "=", "cutoff", "\n", "self", ".", "embedding_dim", "=", "output_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "\n", "self", ".", "embeddings", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", ")", ":", "\n", "            ", "prev", "=", "self", ".", "cutoff", "[", "i", "-", "1", "]", "if", "i", ">", "0", "else", "0", "\n", "size", "=", "self", ".", "cutoff", "[", "i", "]", "-", "prev", "\n", "dim", "=", "int", "(", "initial_dim", "//", "(", "factor", "**", "i", ")", ")", "\n", "seq", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Embedding", "(", "size", ",", "dim", ",", "padding_idx", ")", ",", "\n", "nn", ".", "Linear", "(", "dim", ",", "output_dim", ",", "bias", "=", "False", ")", "\n", ")", "\n", "self", ".", "embeddings", ".", "append", "(", "seq", ")", "\n", "\n", "", "def", "init_weights", "(", "m", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Embedding", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "m", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "elif", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_input.AdaptiveInput.weights_for_band": [[60, 62], ["None"], "methods", ["None"], ["", "def", "weights_for_band", "(", "self", ",", "band", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", "[", "band", "]", "[", "0", "]", ".", "weight", ",", "self", ".", "embeddings", "[", "band", "]", "[", "1", "]", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_input.AdaptiveInput.forward": [[63, 75], ["adaptive_input.AdaptiveInput._float_tensor.new", "range", "len", "input.lt", "input.lt.any", "input.lt.mul_", "input.ge"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "result", "=", "self", ".", "_float_tensor", ".", "new", "(", "input", ".", "shape", "+", "(", "self", ".", "embedding_dim", ",", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", ")", ":", "\n", "            ", "mask", "=", "input", ".", "lt", "(", "self", ".", "cutoff", "[", "i", "]", ")", "\n", "if", "i", ">", "0", ":", "\n", "                ", "mask", ".", "mul_", "(", "input", ".", "ge", "(", "self", ".", "cutoff", "[", "i", "-", "1", "]", ")", ")", "\n", "chunk_input", "=", "input", "[", "mask", "]", "-", "self", ".", "cutoff", "[", "i", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "chunk_input", "=", "input", "[", "mask", "]", "\n", "", "if", "mask", ".", "any", "(", ")", ":", "\n", "                ", "result", "[", "mask", "]", "=", "self", ".", "embeddings", "[", "i", "]", "(", "chunk_input", ")", "\n", "", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.grad_multiply.GradMultiply.forward": [[12, 17], ["x.new"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "scale", ")", ":", "\n", "        ", "ctx", ".", "scale", "=", "scale", "\n", "res", "=", "x", ".", "new", "(", "x", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.grad_multiply.GradMultiply.backward": [[18, 21], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "grad", "*", "ctx", ".", "scale", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.learned_positional_embedding.LearnedPositionalEmbedding.__init__": [[20, 24], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.learned_positional_embedding.LearnedPositionalEmbedding.forward": [[25, 33], ["super().forward", "input.data.new().fill_", "fairseq.utils.make_positions", "input.data.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.forward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "            ", "positions", "=", "input", ".", "data", ".", "new", "(", "1", ",", "1", ")", ".", "fill_", "(", "self", ".", "padding_idx", "+", "input", ".", "size", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "positions", "=", "utils", ".", "make_positions", "(", "input", ".", "data", ",", "self", ".", "padding_idx", ",", "self", ".", "left_pad", ",", "self", ".", "onnx_trace", ")", "\n", "", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.learned_positional_embedding.LearnedPositionalEmbedding.max_positions": [[34, 37], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "return", "self", ".", "num_embeddings", "-", "self", ".", "padding_idx", "-", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.__init__": [[22, 49], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "multihead_attention.MultiheadAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.nn.Parameter", "torch.nn.Parameter", "multihead_attention.MultiheadAttention.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.prepare_for_onnx_export_": [[50, 52], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.reset_parameters": [[53, 63], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.forward": [[64, 208], ["query.size", "multihead_attention.MultiheadAttention.contiguous().view().transpose", "torch.cat.size", "torch.cat.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax().type_as", "torch.softmax().type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "multihead_attention.MultiheadAttention.out_proj", "query.data_ptr", "key.data_ptr", "value.data_ptr", "key.data_ptr", "value.data_ptr", "list", "key.size", "value.size", "multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.in_proj_qkv", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "multihead_attention.MultiheadAttention._set_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.transpose", "torch.cat.transpose", "list", "attn_mask.repeat.repeat.unsqueeze", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "list", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.view", "query.size", "multihead_attention.MultiheadAttention.in_proj_q", "multihead_attention.MultiheadAttention.in_proj_q", "multihead_attention.MultiheadAttention.in_proj_k", "multihead_attention.MultiheadAttention.in_proj_v", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multihead_attention.MultiheadAttention.contiguous().view", "saved_state[].view", "saved_state[].view", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.size", "attn_mask.repeat.repeat.repeat", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "torch.softmax", "torch.softmax", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.sum", "multihead_attention.MultiheadAttention.in_proj_kv", "multihead_attention.MultiheadAttention.bias_k.repeat", "multihead_attention.MultiheadAttention.bias_v.repeat", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.size", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "multihead_attention.MultiheadAttention.contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.where", "torch.where", "torch.where", "torch.where", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "attn_mask.repeat.repeat.size", "torch.cat.size", "torch.cat.size", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "attn_mask.repeat.repeat.size", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "float", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "attn_weights.float().masked_fill().type_as.float().masked_fill().type_as.float", "torch.cat.size", "torch.cat.size", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "float", "torch.cat.unsqueeze", "torch.cat.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_qkv", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_kv", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "incremental_state", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "static_kv", "=", "False", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Self-attention can be implemented by passing in the same arguments for\n        query, key and value. Timesteps can be masked by supplying a T x T mask in the\n        `attn_mask` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "\n", "qkv_same", "=", "query", ".", "data_ptr", "(", ")", "==", "key", ".", "data_ptr", "(", ")", "==", "value", ".", "data_ptr", "(", ")", "\n", "kv_same", "=", "key", ".", "data_ptr", "(", ")", "==", "value", ".", "data_ptr", "(", ")", "\n", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "'prev_key'", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "kv_same", "and", "not", "qkv_same", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "", "if", "qkv_same", ":", "\n", "# self-attention", "\n", "            ", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_qkv", "(", "query", ")", "\n", "", "elif", "kv_same", ":", "\n", "# encoder-decoder attention", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "if", "key", "is", "None", ":", "\n", "                ", "assert", "value", "is", "None", "\n", "k", "=", "v", "=", "None", "\n", "", "else", ":", "\n", "                ", "k", ",", "v", "=", "self", ".", "in_proj_kv", "(", "key", ")", "\n", "", "", "else", ":", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "k", "=", "self", ".", "in_proj_k", "(", "key", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "value", ")", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "key_padding_mask", ".", "new_zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "'prev_key'", "in", "saved_state", ":", "\n", "                ", "prev_key", "=", "saved_state", "[", "'prev_key'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "k", "=", "torch", ".", "cat", "(", "(", "prev_key", ",", "k", ")", ",", "dim", "=", "1", ")", "\n", "", "", "if", "'prev_value'", "in", "saved_state", ":", "\n", "                ", "prev_value", "=", "saved_state", "[", "'prev_value'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "v", "=", "torch", ".", "cat", "(", "(", "prev_value", ",", "v", ")", ",", "dim", "=", "1", ")", "\n", "", "", "saved_state", "[", "'prev_key'", "]", "=", "k", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "'prev_value'", "]", "=", "v", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "src_len", "=", "k", ".", "size", "(", "1", ")", "\n", "\n", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "add_zero_attn", ":", "\n", "            ", "src_len", "+=", "1", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "k", ".", "new_zeros", "(", "(", "k", ".", "size", "(", "0", ")", ",", "1", ")", "+", "k", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "v", ".", "new_zeros", "(", "(", "v", ".", "size", "(", "0", ")", ",", "1", ")", "+", "v", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "torch", ".", "zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ".", "type_as", "(", "key_padding_mask", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "assert", "list", "(", "attn_weights", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", "]", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_mask", "=", "attn_mask", ".", "repeat", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "", "attn_weights", "+=", "attn_mask", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_weights", "=", "torch", ".", "where", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "float", "(", "\"-Inf\"", ")", "]", ")", ",", "\n", "attn_weights", ".", "float", "(", ")", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "", "else", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "float", "(", "'-inf'", ")", ",", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "# FP16 support: cast to float and back", "\n", "", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "attn_weights", "=", "F", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn", "=", "torch", ".", "bmm", "(", "attn_weights", ",", "v", ")", "\n", "assert", "list", "(", "attn", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", "]", "\n", "if", "(", "self", ".", "onnx_trace", "and", "attn", ".", "size", "(", "1", ")", "==", "1", ")", ":", "\n", "# when ONNX tracing a single decoder step (sequence length == 1)", "\n", "# the transpose is a no-op copy before view, thus unnecessary", "\n", "            ", "attn", "=", "attn", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "\n", "if", "need_weights", ":", "\n", "# average attention weights over heads", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "self", ".", "num_heads", "\n", "", "else", ":", "\n", "            ", "attn_weights", "=", "None", "\n", "\n", "", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_qkv": [[209, 211], ["multihead_attention.MultiheadAttention._in_proj().chunk", "multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_qkv", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_kv": [[212, 214], ["multihead_attention.MultiheadAttention._in_proj().chunk", "multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_kv", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ")", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_q": [[215, 217], ["multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_q", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ",", "end", "=", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_k": [[218, 220], ["multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_k", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ",", "end", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_v": [[221, 223], ["multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_v", "(", "self", ",", "value", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "value", ",", "start", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._in_proj": [[224, 231], ["torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "_in_proj", "(", "self", ",", "input", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "weight", "=", "self", ".", "in_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "weight", "=", "weight", "[", "start", ":", "end", ",", ":", "]", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "", "return", "F", ".", "linear", "(", "input", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.reorder_incremental_state": [[232, 239], ["multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.keys", "multihead_attention.MultiheadAttention._set_input_buffer", "input_buffer[].index_select"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer", "[", "k", "]", "=", "input_buffer", "[", "k", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._get_input_buffer": [[240, 246], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", ")", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention._set_input_buffer": [[247, 253], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "buffer", ")", ":", "\n", "        ", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", "buffer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.__init__": [[48, 71], ["torch.Module.__init__", "dynamic_convolution.DynamicConv1dTBC.reset_parameters", "dynamic_convolution.Linear", "dynamic_convolution.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding_l", "=", "None", ",", "num_heads", "=", "1", ",", "\n", "weight_dropout", "=", "0.", ",", "weight_softmax", "=", "False", ",", "\n", "renorm_padding", "=", "False", ",", "bias", "=", "False", ",", "conv_bias", "=", "False", ",", "\n", "query_size", "=", "None", ",", "in_proj", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "query_size", "=", "input_size", "if", "query_size", "is", "None", "else", "query_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "padding_l", "=", "padding_l", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "self", ".", "renorm_padding", "=", "renorm_padding", "\n", "\n", "if", "in_proj", ":", "\n", "            ", "self", ".", "weight_linear", "=", "Linear", "(", "self", ".", "input_size", ",", "self", ".", "input_size", "+", "num_heads", "*", "kernel_size", "*", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weight_linear", "=", "Linear", "(", "self", ".", "query_size", ",", "num_heads", "*", "kernel_size", "*", "1", ",", "bias", "=", "bias", ")", "\n", "", "if", "conv_bias", ":", "\n", "            ", "self", ".", "conv_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_bias", "=", "None", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.in_proj": [[72, 75], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_proj", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "weight_linear", ".", "out_features", "==", "self", ".", "input_size", "+", "self", ".", "num_heads", "*", "self", ".", "kernel_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.reset_parameters": [[76, 80], ["dynamic_convolution.DynamicConv1dTBC.weight_linear.reset_parameters", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "weight_linear", ".", "reset_parameters", "(", ")", "\n", "if", "self", ".", "conv_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "conv_bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.forward": [[81, 104], ["dynamic_convolution.DynamicConv1dTBC._forward_unfolded", "dynamic_convolution.DynamicConv1dTBC._forward_expanded", "x.size", "dynamic_convolution.DynamicConv1dTBC.conv_bias.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "incremental_state", "=", "None", ",", "query", "=", "None", ",", "unfold", "=", "None", ")", ":", "\n", "        ", "'''Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n            query: use the specified query to predict the conv filters\n        '''", "\n", "unfold", "=", "x", ".", "size", "(", "0", ")", ">", "512", "if", "unfold", "is", "None", "else", "unfold", "# use unfold mode as default for long sequence to save memory", "\n", "unfold", "=", "unfold", "or", "(", "incremental_state", "is", "not", "None", ")", "\n", "assert", "query", "is", "None", "or", "not", "self", ".", "in_proj", "\n", "\n", "if", "query", "is", "None", ":", "\n", "            ", "query", "=", "x", "\n", "\n", "", "if", "unfold", ":", "\n", "            ", "output", "=", "self", ".", "_forward_unfolded", "(", "x", ",", "incremental_state", ",", "query", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_forward_expanded", "(", "x", ",", "incremental_state", ",", "query", ")", "\n", "\n", "", "if", "self", ".", "conv_bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "conv_bias", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC._forward_unfolded": [[105, 156], ["dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.size", "weight.narrow.narrow.narrow", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.view.view.view", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear().view", "dynamic_convolution.DynamicConv1dTBC._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x_unfold.view.view.view", "fairseq.modules.unfold1d", "x_unfold.view.view.view", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.size", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.unsqueeze", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.new", "dynamic_convolution.DynamicConv1dTBC._set_input_buffer", "weight.narrow.narrow.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.unsqueeze", "dynamic_convolution.DynamicConv1dTBC.narrow", "x_unfold.view.view.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.unfold1d.unfold1d", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_forward_unfolded", "(", "self", ",", "x", ",", "incremental_state", ",", "query", ")", ":", "\n", "        ", "'''The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.'''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "if", "self", ".", "in_proj", ":", "\n", "            ", "proj", "=", "self", ".", "weight_linear", "(", "x", ")", "\n", "x", "=", "proj", ".", "narrow", "(", "2", ",", "0", ",", "self", ".", "input_size", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "proj", ".", "narrow", "(", "2", ",", "self", ".", "input_size", ",", "H", "*", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "weight_linear", "(", "query", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "\n", "# renorm_padding is only implemented in _forward_expanded", "\n", "", "assert", "not", "self", ".", "renorm_padding", "or", "incremental_state", "is", "not", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "x", ".", "new", "(", ")", "\n", "", "x_unfold", "=", "torch", ".", "cat", "(", "[", "input_buffer", ",", "x", ".", "unsqueeze", "(", "3", ")", "]", ",", "dim", "=", "3", ")", "\n", "if", "self", ".", "kernel_size", ">", "1", ":", "\n", "                ", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "x_unfold", "[", ":", ",", ":", ",", ":", ",", "-", "self", ".", "kernel_size", "+", "1", ":", "]", ")", "\n", "", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "padding_l", "=", "self", ".", "padding_l", "\n", "if", "K", ">", "T", "and", "padding_l", "==", "K", "-", "1", ":", "\n", "                ", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "padding_l", "=", "T", ",", "T", "-", "1", "\n", "# unfold the input: T x B x C --> T' x B x C x K", "\n", "", "x_unfold", "=", "unfold1d", "(", "x", ",", "K", ",", "padding_l", ",", "0", ")", "\n", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "K", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", "and", "not", "self", ".", "renorm_padding", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "0", ",", "K", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "weight", "=", "weight", "[", ":", ",", "-", "x_unfold", ".", "size", "(", "2", ")", ":", "]", "\n", "K", "=", "weight", ".", "size", "(", "1", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", "and", "self", ".", "renorm_padding", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "\n", "output", "=", "torch", ".", "bmm", "(", "x_unfold", ",", "weight", ".", "unsqueeze", "(", "2", ")", ")", "# T*B*H x R x 1", "\n", "output", "=", "output", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC._forward_expanded": [[157, 203], ["dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.size", "weight.narrow.narrow.narrow().contiguous", "weight.narrow.narrow.view().transpose", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.view().transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear().view", "torch.dropout", "torch.dropout", "torch.dropout", "weight.narrow.narrow.new().fill_", "weight_expanded.narrow.narrow.as_strided().copy_", "weight_expanded.narrow.narrow.narrow", "torch.softmax", "torch.softmax", "torch.softmax", "torch.dropout", "torch.dropout", "torch.dropout", "weight.narrow.narrow.new_zeros", "weight_expanded.narrow.narrow.as_strided().copy_", "weight_expanded.narrow.narrow.narrow", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.narrow", "weight.narrow.narrow.view", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.view", "float", "weight.narrow.narrow.narrow", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "weight.narrow.narrow.new", "weight_expanded.narrow.narrow.as_strided", "weight_expanded.narrow.narrow.as_strided", "output.transpose().contiguous().view.transpose().contiguous().view.transpose", "dynamic_convolution.DynamicConv1dTBC.narrow"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_forward_expanded", "(", "self", ",", "x", ",", "incremental_stat", ",", "query", ")", ":", "\n", "        ", "'''Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        '''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "if", "self", ".", "in_proj", ":", "\n", "            ", "proj", "=", "self", ".", "weight_linear", "(", "x", ")", "\n", "x", "=", "proj", ".", "narrow", "(", "2", ",", "0", ",", "self", ".", "input_size", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "proj", ".", "narrow", "(", "2", ",", "self", ".", "input_size", ",", "H", "*", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "weight_linear", "(", "query", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "\n", "", "if", "not", "self", ".", "renorm_padding", ":", "\n", "            ", "if", "self", ".", "weight_softmax", ":", "\n", "                ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "0", ",", "K", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "weight", ".", "view", "(", "T", ",", "B", "*", "H", ",", "K", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "T", ",", "B", "*", "H", ",", "R", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "self", ".", "weight_softmax", "and", "self", ".", "renorm_padding", ":", "\n", "# turn the convolution filters into band matrices", "\n", "            ", "weight_expanded", "=", "weight", ".", "new", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ")", ".", "fill_", "(", "float", "(", "'-inf'", ")", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "self", ".", "padding_l", ",", "T", ")", "\n", "# normalize the weight over valid positions like self-attention", "\n", "weight_expanded", "=", "F", ".", "softmax", "(", "weight_expanded", ",", "dim", "=", "2", ")", "\n", "weight_expanded", "=", "F", ".", "dropout", "(", "weight_expanded", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "P", "=", "self", ".", "padding_l", "\n", "# For efficieny, we cut the kernel size and reduce the padding when the kernel is larger than the length", "\n", "if", "K", ">", "T", "and", "P", "==", "K", "-", "1", ":", "\n", "                ", "weight", "=", "weight", ".", "narrow", "(", "2", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "P", "=", "T", ",", "T", "-", "1", "\n", "# turn the convolution filters into band matrices", "\n", "", "weight_expanded", "=", "weight", ".", "new_zeros", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ",", "requires_grad", "=", "False", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "P", ",", "T", ")", "# B*H x T x T", "\n", "\n", "", "output", "=", "torch", ".", "bmm", "(", "weight_expanded", ",", "x", ")", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.reorder_incremental_state": [[204, 209], ["dynamic_convolution.DynamicConv1dTBC._get_input_buffer", "input_buffer.index_select.index_select.index_select", "dynamic_convolution.DynamicConv1dTBC._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC._get_input_buffer": [[210, 212], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC._set_input_buffer": [[213, 215], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.DynamicConv1dTBC.extra_repr": [[216, 228], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, conv_bias={}, renorm_padding={}, in_proj={}'", ".", "format", "(", "\n", "self", ".", "input_size", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "\n", "self", ".", "num_heads", ",", "self", ".", "weight_softmax", ",", "self", ".", "conv_bias", "is", "not", "None", ",", "self", ".", "renorm_padding", ",", "\n", "self", ".", "in_proj", ",", "\n", ")", "\n", "\n", "if", "self", ".", "query_size", "!=", "self", ".", "input_size", ":", "\n", "            ", "s", "+=", "', query_size={}'", ".", "format", "(", "self", ".", "query_size", ")", "\n", "", "if", "self", ".", "weight_dropout", ">", "0.", ":", "\n", "            ", "s", "+=", "', weight_dropout={}'", ".", "format", "(", "self", ".", "weight_dropout", ")", "\n", "", "return", "s", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.dynamic_convolution.Linear": [[16, 22], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.TiedLinear.__init__": [[17, 21], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "weight", ",", "transpose", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "transpose", "=", "transpose", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.TiedLinear.forward": [[22, 24], ["torch.linear", "torch.linear", "adaptive_softmax.TiedLinear.weight.t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ".", "t", "(", ")", "if", "self", ".", "transpose", "else", "self", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.TiedHeadModule.__init__": [[27, 43], ["torch.nn.Module.__init__", "tied_emb.size", "adaptive_softmax.TiedLinear", "torch.nn.Linear", "torch.nn.Linear", "adaptive_softmax.TiedHeadModule.register_buffer", "torch.nn.Sequential", "torch.nn.Sequential", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "weights", ",", "input_dim", ",", "num_classes", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "tied_emb", ",", "_", "=", "weights", "\n", "self", ".", "num_words", ",", "emb_dim", "=", "tied_emb", ".", "size", "(", ")", "\n", "\n", "self", ".", "word_proj", "=", "TiedLinear", "(", "tied_emb", ",", "transpose", "=", "False", ")", "\n", "if", "input_dim", "!=", "emb_dim", ":", "\n", "            ", "self", ".", "word_proj", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "emb_dim", ",", "bias", "=", "False", ")", ",", "\n", "self", ".", "word_proj", ",", "\n", ")", "\n", "\n", "", "self", ".", "class_proj", "=", "nn", ".", "Linear", "(", "input_dim", ",", "num_classes", ",", "bias", "=", "False", ")", "\n", "self", ".", "out_dim", "=", "self", ".", "num_words", "+", "num_classes", "\n", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.TiedHeadModule.forward": [[44, 50], ["functools.reduce", "adaptive_softmax.TiedHeadModule._float_tensor.new", "adaptive_softmax.TiedHeadModule.word_proj", "adaptive_softmax.TiedHeadModule.class_proj", "input.view", "input.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "inp_sz", "=", "functools", ".", "reduce", "(", "operator", ".", "mul", ",", "input", ".", "shape", "[", ":", "-", "1", "]", ",", "1", ")", "\n", "out", "=", "self", ".", "_float_tensor", ".", "new", "(", "inp_sz", ",", "self", ".", "out_dim", ")", "\n", "out", "[", ":", ",", ":", "self", ".", "num_words", "]", "=", "self", ".", "word_proj", "(", "input", ".", "view", "(", "inp_sz", ",", "-", "1", ")", ")", "\n", "out", "[", ":", ",", "self", ".", "num_words", ":", "]", "=", "self", ".", "class_proj", "(", "input", ".", "view", "(", "inp_sz", ",", "-", "1", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.__init__": [[59, 94], ["torch.nn.Module.__init__", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "adaptive_softmax.AdaptiveSoftmax._make_tail", "adaptive_softmax.AdaptiveSoftmax.apply", "adaptive_softmax.AdaptiveSoftmax.register_buffer", "adaptive_softmax.TiedHeadModule", "torch.nn.Linear", "torch.nn.Linear", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "adaptive_inputs.weights_for_band", "hasattr", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "len", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax._make_tail", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_input.AdaptiveInput.weights_for_band"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "input_dim", ",", "cutoff", ",", "dropout", ",", "factor", "=", "4.", ",", "adaptive_inputs", "=", "None", ",", "tie_proj", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "vocab_size", ">", "cutoff", "[", "-", "1", "]", ":", "\n", "            ", "cutoff", "=", "cutoff", "+", "[", "vocab_size", "]", "\n", "", "else", ":", "\n", "            ", "assert", "vocab_size", "==", "cutoff", "[", "\n", "-", "1", "]", ",", "'cannot specify cutoff larger than vocab size'", "\n", "\n", "", "output_dim", "=", "cutoff", "[", "0", "]", "+", "len", "(", "cutoff", ")", "-", "1", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "cutoff", "=", "cutoff", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "factor", "=", "factor", "\n", "\n", "self", ".", "lsm", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n", "if", "adaptive_inputs", "is", "not", "None", ":", "\n", "            ", "self", ".", "head", "=", "TiedHeadModule", "(", "adaptive_inputs", ".", "weights_for_band", "(", "0", ")", ",", "input_dim", ",", "len", "(", "cutoff", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "head", "=", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "_make_tail", "(", "True", ",", "adaptive_inputs", ",", "tie_proj", ")", "\n", "\n", "def", "init_weights", "(", "m", ")", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "not", "isinstance", "(", "m", ",", "TiedLinear", ")", "and", "not", "isinstance", "(", "m", ",", "TiedHeadModule", ")", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ")", "\n", "# versions prior to 1 had a bug that offset indices on the head by 1", "\n", "self", ".", "buggy_offset", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax._make_tail": [[95, 122], ["torch.nn.ModuleList", "torch.nn.ModuleList", "range", "int", "torch.nn.Sequential", "torch.nn.Sequential", "adaptive_softmax.AdaptiveSoftmax.tail.append", "len", "adaptive_inputs.weights_for_band", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "adaptive_softmax.TiedLinear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "adaptive_softmax.TiedLinear", "tied_proj.size", "tied_proj.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_input.AdaptiveInput.weights_for_band", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_make_tail", "(", "self", ",", "fix_exponent", ",", "adaptive_inputs", "=", "None", ",", "tie_proj", "=", "False", ")", ":", "\n", "        ", "extra_denom", "=", "1", "if", "fix_exponent", "else", "0", "\n", "\n", "self", ".", "tail", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", "-", "1", ")", ":", "\n", "            ", "dim", "=", "int", "(", "self", ".", "input_dim", "//", "self", ".", "factor", "**", "(", "i", "+", "extra_denom", ")", ")", "\n", "\n", "tied_emb", ",", "tied_proj", "=", "adaptive_inputs", ".", "weights_for_band", "(", "i", "+", "1", ")", "if", "adaptive_inputs", "is", "not", "None", "else", "(", "None", ",", "None", ")", "\n", "\n", "if", "tied_proj", "is", "not", "None", ":", "\n", "                ", "if", "tie_proj", ":", "\n", "                    ", "proj", "=", "TiedLinear", "(", "tied_proj", ",", "transpose", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "proj", "=", "nn", ".", "Linear", "(", "tied_proj", ".", "size", "(", "0", ")", ",", "tied_proj", ".", "size", "(", "1", ")", ",", "bias", "=", "False", ")", "\n", "", "", "else", ":", "\n", "                ", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "\n", "", "m", "=", "nn", ".", "Sequential", "(", "\n", "proj", ",", "\n", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "\n", "dim", ",", "self", ".", "cutoff", "[", "i", "+", "1", "]", "-", "self", ".", "cutoff", "[", "i", "]", ",", "bias", "=", "False", ",", "\n", ")", "if", "tied_emb", "is", "None", "else", "TiedLinear", "(", "tied_emb", ",", "transpose", "=", "False", ")", ",", "\n", ")", "\n", "\n", "self", ".", "tail", ".", "append", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.upgrade_state_dict_named": [[123, 129], ["adaptive_softmax.AdaptiveSoftmax._make_tail", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax._make_tail"], ["", "", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "version_name", "=", "name", "+", "'.version'", "\n", "if", "version_name", "not", "in", "state_dict", ":", "\n", "            ", "self", ".", "buggy_offset", "=", "1", "\n", "self", ".", "_make_tail", "(", "False", ")", "\n", "state_dict", "[", "version_name", "]", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target": [[130, 154], ["target.view.view.view", "range", "target.view.view.clone", "target.view.view.ge().mul", "target.view.ge().mul.any", "len", "target.view.view.lt", "target_idxs.append", "new_target.append", "target_idxs.append", "new_target.append", "target.view.view.ge", "target.view.ge().mul.nonzero().squeeze", "target[].add", "target.view.ge().mul.nonzero"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], ["", "", "def", "adapt_target", "(", "self", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        In order to be efficient, the AdaptiveSoftMax does not compute the\n        scores for all the word of the vocabulary for all the examples. It is\n        thus necessary to call the method adapt_target of the AdaptiveSoftMax\n        layer inside each forward pass.\n        \"\"\"", "\n", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "new_target", "=", "[", "target", ".", "clone", "(", ")", "]", "\n", "target_idxs", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", "-", "1", ")", ":", "\n", "            ", "mask", "=", "target", ".", "ge", "(", "self", ".", "cutoff", "[", "i", "]", ")", ".", "mul", "(", "target", ".", "lt", "(", "self", ".", "cutoff", "[", "i", "+", "1", "]", ")", ")", "\n", "new_target", "[", "0", "]", "[", "mask", "]", "=", "self", ".", "cutoff", "[", "0", "]", "+", "i", "-", "self", ".", "buggy_offset", "\n", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "                ", "target_idxs", ".", "append", "(", "mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", ")", "\n", "new_target", ".", "append", "(", "target", "[", "mask", "]", ".", "add", "(", "-", "self", ".", "cutoff", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "target_idxs", ".", "append", "(", "None", ")", "\n", "new_target", ".", "append", "(", "None", ")", "\n", "\n", "", "", "return", "new_target", ",", "target_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.forward": [[155, 177], ["torch.dropout.contiguous().view", "torch.dropout", "torch.dropout", "adaptive_softmax.AdaptiveSoftmax.adapt_target", "range", "torch.dropout.size", "adaptive_softmax.AdaptiveSoftmax.head", "len", "torch.dropout.contiguous", "output.append", "output.append", "torch.dropout.index_select"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.head", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input: (b x t x d)\n            target: (b x t)\n        Returns:\n            2 lists: output for each cutoff section and new targets by cut off\n        \"\"\"", "\n", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "input", ".", "size", "(", "-", "1", ")", ")", "\n", "input", "=", "F", ".", "dropout", "(", "input", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "new_target", ",", "target_idxs", "=", "self", ".", "adapt_target", "(", "target", ")", "\n", "output", "=", "[", "self", ".", "head", "(", "input", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "target_idxs", ")", ")", ":", "\n", "            ", "if", "target_idxs", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "output", ".", "append", "(", "self", ".", "tail", "[", "i", "]", "(", "input", ".", "index_select", "(", "0", ",", "target_idxs", "[", "i", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "None", ")", "\n", "\n", "", "", "return", "output", ",", "new_target", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.get_log_prob": [[178, 215], ["input.contiguous().view.contiguous().view.size", "input.contiguous().view.contiguous().view.contiguous().view", "adaptive_softmax.AdaptiveSoftmax.head", "adaptive_softmax.AdaptiveSoftmax.new_zeros", "adaptive_softmax.AdaptiveSoftmax.lsm", "log_probs[].clone", "range", "log_probs.view.view.view", "adaptive_softmax.AdaptiveSoftmax.adapt_target", "input.contiguous().view.contiguous().view.size", "len", "len", "input.contiguous().view.contiguous().view.contiguous", "tail_out.copy_", "adaptive_softmax.AdaptiveSoftmax.lsm().add_", "tail_out.copy_", "adaptive_softmax.AdaptiveSoftmax.lsm().add_", "adaptive_softmax.AdaptiveSoftmax.lsm", "adaptive_softmax.AdaptiveSoftmax.lsm"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.head", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "get_log_prob", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Computes the log probabilities for all the words of the vocabulary,\n        given a 2D tensor of hidden vectors.\n        \"\"\"", "\n", "\n", "bsz", ",", "length", ",", "dim", "=", "input", ".", "size", "(", ")", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "_", ",", "target_idxs", "=", "self", ".", "adapt_target", "(", "target", ")", "\n", "", "else", ":", "\n", "            ", "target_idxs", "=", "None", "\n", "\n", "", "head_y", "=", "self", ".", "head", "(", "input", ")", "\n", "log_probs", "=", "head_y", ".", "new_zeros", "(", "input", ".", "size", "(", "0", ")", ",", "self", ".", "vocab_size", ")", "\n", "\n", "head_sz", "=", "self", ".", "cutoff", "[", "0", "]", "+", "len", "(", "self", ".", "tail", ")", "\n", "log_probs", "[", ":", ",", ":", "head_sz", "]", "=", "self", ".", "lsm", "(", "head_y", ")", "\n", "tail_priors", "=", "log_probs", "[", ":", ",", "self", ".", "cutoff", "[", "0", "]", "-", "self", ".", "buggy_offset", ":", "head_sz", "-", "self", ".", "buggy_offset", "]", ".", "clone", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "tail", ")", ")", ":", "\n", "            ", "start", "=", "self", ".", "cutoff", "[", "i", "]", "\n", "end", "=", "self", ".", "cutoff", "[", "i", "+", "1", "]", "\n", "\n", "if", "target_idxs", "is", "None", ":", "\n", "                ", "tail_out", "=", "log_probs", "[", ":", ",", "start", ":", "end", "]", "\n", "tail_out", ".", "copy_", "(", "self", ".", "tail", "[", "i", "]", "(", "input", ")", ")", "\n", "log_probs", "[", ":", ",", "start", ":", "end", "]", "=", "self", ".", "lsm", "(", "tail_out", ")", ".", "add_", "(", "tail_priors", "[", ":", ",", "i", ",", "None", "]", ")", "\n", "", "elif", "target_idxs", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "idxs", "=", "target_idxs", "[", "i", "]", "\n", "tail_out", "=", "log_probs", "[", "idxs", ",", "start", ":", "end", "]", "\n", "tail_out", ".", "copy_", "(", "self", ".", "tail", "[", "i", "]", "(", "input", "[", "idxs", "]", ")", ")", "\n", "log_probs", "[", "idxs", ",", "start", ":", "end", "]", "=", "self", ".", "lsm", "(", "tail_out", ")", ".", "add_", "(", "tail_priors", "[", "idxs", ",", "i", ",", "None", "]", ")", "\n", "\n", "", "", "log_probs", "=", "log_probs", ".", "view", "(", "bsz", ",", "length", ",", "-", "1", ")", "\n", "return", "log_probs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.beamable_mm.BeamableMM.__init__": [[20, 23], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "beam_size", "=", "None", ")", ":", "\n", "        ", "super", "(", "BeamableMM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.beamable_mm.BeamableMM.forward": [[24, 47], ["input1[].unfold().transpose", "input1[].unfold().transpose.bmm.view", "input1[].unfold().transpose.bmm", "input1[].unfold().transpose.dim", "input1[].unfold().transpose.size", "input1[].unfold().transpose.size", "input2.unfold", "input1[].unfold().transpose.size", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "input1[].unfold().transpose.bmm", "input1[].unfold"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "input1", ",", "input2", ")", ":", "\n", "        ", "if", "(", "\n", "not", "self", ".", "training", "and", "# test mode", "\n", "self", ".", "beam_size", "is", "not", "None", "and", "# beam size is set", "\n", "input1", ".", "dim", "(", ")", "==", "3", "and", "# only support batched input", "\n", "input1", ".", "size", "(", "1", ")", "==", "1", "# single time step update", "\n", ")", ":", "\n", "            ", "bsz", ",", "beam", "=", "input1", ".", "size", "(", "0", ")", ",", "self", ".", "beam_size", "\n", "\n", "# bsz x 1 x nhu --> bsz/beam x beam x nhu", "\n", "input1", "=", "input1", "[", ":", ",", "0", ",", ":", "]", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "# bsz x sz2 x nhu --> bsz/beam x sz2 x nhu", "\n", "input2", "=", "input2", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", "\n", "# use non batched operation if bsz = beam", "\n", "if", "input1", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "                ", "output", "=", "torch", ".", "mm", "(", "input1", "[", "0", ",", ":", ",", ":", "]", ",", "input2", "[", "0", ",", ":", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "input1", ".", "bmm", "(", "input2", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "input1", ".", "bmm", "(", "input2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.beamable_mm.BeamableMM.set_beam_size": [[48, 50], ["None"], "methods", ["None"], ["", "", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "self", ".", "beam_size", "=", "beam_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.conv_tbc.ConvTBC.__init__": [[18, 28], ["super().__init__", "torch.nn.modules.utils._single", "torch.nn.modules.utils._single", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "ConvTBC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "_single", "(", "kernel_size", ")", "\n", "self", ".", "padding", "=", "_single", "(", "padding", ")", "\n", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "kernel_size", "[", "0", "]", ",", "in_channels", ",", "out_channels", ")", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.conv_tbc.ConvTBC.forward": [[29, 31], ["torch.conv_tbc", "input.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "torch", ".", "conv_tbc", "(", "input", ".", "contiguous", "(", ")", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "padding", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.conv_tbc.ConvTBC.__repr__": [[32, 39], ["s.format"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "(", "'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'", "\n", "', padding={padding}'", ")", "\n", "if", "self", ".", "bias", "is", "None", ":", "\n", "            ", "s", "+=", "', bias=False'", "\n", "", "s", "+=", "')'", "\n", "return", "s", ".", "format", "(", "name", "=", "self", ".", "__class__", ".", "__name__", ",", "**", "self", ".", "__dict__", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.__init__": [[23, 61], ["super().__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.ModuleList", "torch.nn.ModuleList", "sum", "torch.nn.Linear", "torch.nn.Linear", "character_token_embedder.CharacterTokenEmbedder.reset_parameters", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "character_token_embedder.CharacterTokenEmbedder.convolutions.append", "highway.Highway", "character_token_embedder.CharacterTokenEmbedder.set_vocab", "torch.nn.Conv1d", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.set_vocab"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "filters", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "char_embed_dim", ":", "int", ",", "\n", "word_embed_dim", ":", "int", ",", "\n", "highway_layers", ":", "int", ",", "\n", "max_char_len", ":", "int", "=", "50", ",", "\n", "char_inputs", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "CharacterTokenEmbedder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "self", ".", "embedding_dim", "=", "word_embed_dim", "\n", "self", ".", "max_char_len", "=", "max_char_len", "\n", "self", ".", "char_embeddings", "=", "nn", ".", "Embedding", "(", "257", ",", "char_embed_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "symbol_embeddings", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "2", ",", "word_embed_dim", ")", ")", "\n", "self", ".", "eos_idx", ",", "self", ".", "unk_idx", "=", "0", ",", "1", "\n", "self", ".", "char_inputs", "=", "char_inputs", "\n", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "width", ",", "out_c", "in", "filters", ":", "\n", "            ", "self", ".", "convolutions", ".", "append", "(", "\n", "nn", ".", "Conv1d", "(", "char_embed_dim", ",", "out_c", ",", "kernel_size", "=", "width", ")", "\n", ")", "\n", "\n", "", "last_dim", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "\n", "self", ".", "highway", "=", "Highway", "(", "last_dim", ",", "highway_layers", ")", "if", "highway_layers", ">", "0", "else", "None", "\n", "\n", "self", ".", "projection", "=", "nn", ".", "Linear", "(", "last_dim", ",", "word_embed_dim", ")", "\n", "\n", "assert", "vocab", "is", "not", "None", "or", "char_inputs", ",", "\"vocab must be set if not using char inputs\"", "\n", "self", ".", "vocab", "=", "None", "\n", "if", "vocab", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_vocab", "(", "vocab", ",", "max_char_len", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.prepare_for_onnx_export_": [[62, 64], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.set_vocab": [[65, 86], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "len", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "print", "vocab[].encode", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "set_vocab", "(", "self", ",", "vocab", ",", "max_char_len", ")", ":", "\n", "        ", "word_to_char", "=", "torch", ".", "LongTensor", "(", "len", "(", "vocab", ")", ",", "max_char_len", ")", "\n", "\n", "truncated", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "vocab", ")", ")", ":", "\n", "            ", "if", "i", "<", "vocab", ".", "nspecial", ":", "\n", "                ", "char_idxs", "=", "[", "0", "]", "*", "max_char_len", "\n", "", "else", ":", "\n", "                ", "chars", "=", "vocab", "[", "i", "]", ".", "encode", "(", ")", "\n", "# +1 for padding", "\n", "char_idxs", "=", "[", "c", "+", "1", "for", "c", "in", "chars", "]", "+", "[", "0", "]", "*", "(", "max_char_len", "-", "len", "(", "chars", ")", ")", "\n", "", "if", "len", "(", "char_idxs", ")", ">", "max_char_len", ":", "\n", "                ", "truncated", "+=", "1", "\n", "char_idxs", "=", "char_idxs", "[", ":", "max_char_len", "]", "\n", "", "word_to_char", "[", "i", "]", "=", "torch", ".", "LongTensor", "(", "char_idxs", ")", "\n", "\n", "", "if", "truncated", ">", "0", ":", "\n", "            ", "print", "(", "'Truncated {} words longer than {} characters'", ".", "format", "(", "truncated", ",", "max_char_len", ")", ")", "\n", "\n", "", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word_to_char", "=", "word_to_char", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.padding_idx": [[87, 90], ["fairseq.data.Dictionary().pad", "character_token_embedder.CharacterTokenEmbedder.vocab.pad", "fairseq.data.Dictionary"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "@", "property", "\n", "def", "padding_idx", "(", "self", ")", ":", "\n", "        ", "return", "Dictionary", "(", ")", ".", "pad", "(", ")", "if", "self", ".", "vocab", "is", "None", "else", "self", ".", "vocab", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.reset_parameters": [[91, 98], ["torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "char_embeddings", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "symbol_embeddings", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "projection", ".", "weight", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "char_embeddings", ".", "weight", "[", "self", ".", "char_embeddings", ".", "padding_idx", "]", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "projection", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder.forward": [[99, 138], ["character_token_embedder.CharacterTokenEmbedder._convolve", "torch.where.view", "torch.where.view", "input.view", "chars[].eq", "chars[].eq", "input.view.eq.any", "input.view", "character_token_embedder.CharacterTokenEmbedder.word_to_char[].type_as", "input.view.eq", "input.view.eq", "input.view.eq", "input.view.eq.any", "input.view.eq.any", "input.view.eq.any", "input.view.eq.any", "character_token_embedder.CharacterTokenEmbedder.vocab.pad", "character_token_embedder.CharacterTokenEmbedder.vocab.eos", "character_token_embedder.CharacterTokenEmbedder.vocab.unk", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.any", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.any", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.unsqueeze", "torch.where.new_zeros", "torch.where.new_zeros", "input.view.eq.unsqueeze", "input.view.eq.unsqueeze", "input.size", "input.view.eq.unsqueeze", "torch.where.new_zeros", "torch.where.new_zeros", "input.view.type_as"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder._convolve", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "        ", "if", "self", ".", "char_inputs", ":", "\n", "            ", "chars", "=", "input", ".", "view", "(", "-", "1", ",", "self", ".", "max_char_len", ")", "\n", "pads", "=", "chars", "[", ":", ",", "0", "]", ".", "eq", "(", "CHAR_PAD_IDX", ")", "\n", "eos", "=", "chars", "[", ":", ",", "0", "]", ".", "eq", "(", "CHAR_EOS_IDX", ")", "\n", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "if", "self", ".", "onnx_trace", ":", "\n", "                    ", "chars", "=", "torch", ".", "where", "(", "eos", ".", "unsqueeze", "(", "1", ")", ",", "chars", ".", "new_zeros", "(", "1", ")", ",", "chars", ")", "\n", "", "else", ":", "\n", "                    ", "chars", "[", "eos", "]", "=", "0", "\n", "\n", "", "", "unk", "=", "None", "\n", "", "else", ":", "\n", "            ", "flat_words", "=", "input", ".", "view", "(", "-", "1", ")", "\n", "chars", "=", "self", ".", "word_to_char", "[", "flat_words", ".", "type_as", "(", "self", ".", "word_to_char", ")", "]", ".", "type_as", "(", "input", ")", "\n", "pads", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "pad", "(", ")", ")", "\n", "eos", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "eos", "(", ")", ")", "\n", "unk", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "unk", "(", ")", ")", "\n", "\n", "", "word_embs", "=", "self", ".", "_convolve", "(", "chars", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "if", "pads", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "pads", ".", "unsqueeze", "(", "1", ")", ",", "word_embs", ".", "new_zeros", "(", "1", ")", ",", "word_embs", ")", "\n", "", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "eos", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "symbol_embeddings", "[", "self", ".", "eos_idx", "]", ",", "word_embs", ")", "\n", "", "if", "unk", "is", "not", "None", "and", "unk", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "unk", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "symbol_embeddings", "[", "self", ".", "unk_idx", "]", ",", "word_embs", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "pads", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "pads", "]", "=", "0", "\n", "", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "eos", "]", "=", "self", ".", "symbol_embeddings", "[", "self", ".", "eos_idx", "]", "\n", "", "if", "unk", "is", "not", "None", "and", "unk", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "unk", "]", "=", "self", ".", "symbol_embeddings", "[", "self", ".", "unk_idx", "]", "\n", "\n", "", "", "return", "word_embs", ".", "view", "(", "input", ".", "size", "(", ")", "[", ":", "2", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.character_token_embedder.CharacterTokenEmbedder._convolve": [[139, 161], ["character_token_embedder.CharacterTokenEmbedder.char_embeddings", "char_embs.transpose.transpose.transpose", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "character_token_embedder.CharacterTokenEmbedder.projection", "conv", "torch.max", "torch.max", "torch.max", "torch.max", "torch.relu", "torch.relu", "conv_result.append", "character_token_embedder.CharacterTokenEmbedder.highway"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "_convolve", "(", "\n", "self", ",", "\n", "char_idxs", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "        ", "char_embs", "=", "self", ".", "char_embeddings", "(", "char_idxs", ")", "\n", "char_embs", "=", "char_embs", ".", "transpose", "(", "1", ",", "2", ")", "# BTC -> BCT", "\n", "\n", "conv_result", "=", "[", "]", "\n", "\n", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "convolutions", ")", ":", "\n", "            ", "x", "=", "conv", "(", "char_embs", ")", "\n", "x", ",", "_", "=", "torch", ".", "max", "(", "x", ",", "-", "1", ")", "\n", "x", "=", "F", ".", "relu", "(", "x", ")", "\n", "conv_result", ".", "append", "(", "x", ")", "\n", "\n", "", "x", "=", "torch", ".", "cat", "(", "conv_result", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "highway", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "highway", "(", "x", ")", "\n", "", "x", "=", "self", ".", "projection", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.__init__": [[24, 36], ["torch.Module.__init__", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.register_buffer", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding"], ["def", "__init__", "(", "self", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ",", "init_size", "=", "1024", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "weights", "=", "SinusoidalPositionalEmbedding", ".", "get_embedding", "(", "\n", "init_size", ",", "\n", "embedding_dim", ",", "\n", "padding_idx", ",", "\n", ")", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.prepare_for_onnx_export_": [[37, 39], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding": [[40, 58], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "math.log", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "get_embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "None", ")", ":", "\n", "        ", "\"\"\"Build sinusoidal embeddings.\n\n        This matches the implementation in tensor2tensor, but differs slightly\n        from the description in Section 3.5 of \"Attention Is All You Need\".\n        \"\"\"", "\n", "half_dim", "=", "embedding_dim", "//", "2", "\n", "emb", "=", "math", ".", "log", "(", "10000", ")", "/", "(", "half_dim", "-", "1", ")", "\n", "emb", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "half_dim", ",", "dtype", "=", "torch", ".", "float", ")", "*", "-", "emb", ")", "\n", "emb", "=", "torch", ".", "arange", "(", "num_embeddings", ",", "dtype", "=", "torch", ".", "float", ")", ".", "unsqueeze", "(", "1", ")", "*", "emb", ".", "unsqueeze", "(", "0", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "emb", ")", ",", "torch", ".", "cos", "(", "emb", ")", "]", ",", "dim", "=", "1", ")", ".", "view", "(", "num_embeddings", ",", "-", "1", ")", "\n", "if", "embedding_dim", "%", "2", "==", "1", ":", "\n", "# zero pad", "\n", "            ", "emb", "=", "torch", ".", "cat", "(", "[", "emb", ",", "torch", ".", "zeros", "(", "num_embeddings", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "padding_idx", "is", "not", "None", ":", "\n", "            ", "emb", "[", "padding_idx", ",", ":", "]", "=", "0", "\n", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.forward": [[59, 86], ["torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.type_as", "fairseq.utils.make_positions", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().view().detach", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights[].expand", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.detach().index_select", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.size", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights[].unsqueeze().repeat", "fairseq.utils.make_positions.view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.detach", "bsz.view", "seq_len.view", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights[].unsqueeze", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select", "timestep.int", "fairseq.utils.make_positions.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ",", "timestep", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "bsz", ",", "seq_len", "=", "torch", ".", "onnx", ".", "operators", ".", "shape_as_tensor", "(", "input", ")", "\n", "max_pos", "=", "self", ".", "padding_idx", "+", "1", "+", "seq_len", "\n", "if", "self", ".", "weights", "is", "None", "or", "max_pos", ">", "self", ".", "weights", ".", "size", "(", "0", ")", ":", "\n", "# recompute/expand embeddings if needed", "\n", "            ", "self", ".", "weights", "=", "SinusoidalPositionalEmbedding", ".", "get_embedding", "(", "\n", "max_pos", ",", "\n", "self", ".", "embedding_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "", "self", ".", "weights", "=", "self", ".", "weights", ".", "type_as", "(", "self", ".", "_float_tensor", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "            ", "pos", "=", "(", "timestep", ".", "int", "(", ")", "+", "1", ")", ".", "long", "(", ")", "if", "timestep", "is", "not", "None", "else", "seq_len", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "return", "self", ".", "weights", "[", "self", ".", "padding_idx", "+", "pos", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "bsz", ",", "1", ",", "1", ")", "\n", "", "return", "self", ".", "weights", "[", "self", ".", "padding_idx", "+", "pos", ",", ":", "]", ".", "expand", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "\n", "", "positions", "=", "utils", ".", "make_positions", "(", "input", ",", "self", ".", "padding_idx", ",", "self", ".", "left_pad", ",", "self", ".", "onnx_trace", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "flat_embeddings", "=", "self", ".", "weights", ".", "detach", "(", ")", ".", "index_select", "(", "0", ",", "positions", ".", "view", "(", "-", "1", ")", ")", "\n", "embedding_shape", "=", "torch", ".", "cat", "(", "(", "bsz", ".", "view", "(", "1", ")", ",", "seq_len", ".", "view", "(", "1", ")", ",", "torch", ".", "LongTensor", "(", "[", "-", "1", "]", ")", ")", ")", "\n", "embeddings", "=", "torch", ".", "onnx", ".", "operators", ".", "reshape_from_tensor_shape", "(", "flat_embeddings", ",", "embedding_shape", ")", "\n", "return", "embeddings", "\n", "", "return", "self", ".", "weights", ".", "index_select", "(", "0", ",", "positions", ".", "view", "(", "-", "1", ")", ")", ".", "view", "(", "bsz", ",", "seq_len", ",", "-", "1", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions": [[87, 90], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1d.__init__": [[38, 54], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lightweight_convolution.LightweightConv1d.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "num_heads", "=", "1", ",", "\n", "weight_softmax", "=", "False", ",", "bias", "=", "False", ",", "weight_dropout", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_heads", ",", "1", ",", "kernel_size", ")", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1d.reset_parameters": [[55, 59], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1d.forward": [[60, 84], ["input.view.view.size", "torch.dropout", "torch.dropout", "torch.dropout", "input.view.view.view", "torch.conv1d", "torch.conv1d", "torch.conv1d", "output.view.view.view", "torch.softmax", "torch.softmax", "torch.softmax", "lightweight_convolution.LightweightConv1d.bias.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "'''\n        input size: B x C x T\n        output size: B x C x T\n        '''", "\n", "B", ",", "C", ",", "T", "=", "input", ".", "size", "(", ")", "\n", "H", "=", "self", ".", "num_heads", "\n", "\n", "weight", "=", "self", ".", "weight", "\n", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "# Merge every C/H entries into the batch dimension (C = self.input_size)", "\n", "# B x C x T -> (B * C/H) x H x T", "\n", "# One can also expand the weight to C x 1 x K by a factor of C/H", "\n", "# and do not reshape the input instead, which is slow though", "\n", "input", "=", "input", ".", "view", "(", "-", "1", ",", "H", ",", "T", ")", "\n", "output", "=", "F", ".", "conv1d", "(", "input", ",", "weight", ",", "padding", "=", "self", ".", "padding", ",", "groups", "=", "self", ".", "num_heads", ")", "\n", "output", "=", "output", ".", "view", "(", "B", ",", "C", ",", "T", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "bias", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC.__init__": [[106, 123], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lightweight_convolution.LightweightConv1dTBC.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding_l", "=", "None", ",", "num_heads", "=", "1", ",", "\n", "weight_dropout", "=", "0.", ",", "weight_softmax", "=", "False", ",", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "padding_l", "=", "padding_l", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_heads", ",", "1", ",", "kernel_size", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC.reset_parameters": [[124, 128], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC.forward": [[129, 146], ["lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "lightweight_convolution.LightweightConv1dTBC._forward_expanded", "lightweight_convolution.LightweightConv1dTBC.bias.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "incremental_state", "=", "None", ",", "unfold", "=", "False", ")", ":", "\n", "        ", "'''Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n        '''", "\n", "unfold", "=", "unfold", "or", "(", "incremental_state", "is", "not", "None", ")", "\n", "\n", "if", "unfold", ":", "\n", "            ", "output", "=", "self", ".", "_forward_unfolded", "(", "x", ",", "incremental_state", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_forward_expanded", "(", "x", ",", "incremental_state", ")", "\n", "\n", "", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "bias", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded": [[147, 182], ["x.size", "lightweight_convolution.LightweightConv1dTBC.weight.view", "torch.softmax().type_as.view().expand().contiguous().view", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.view.view.view", "lightweight_convolution.LightweightConv1dTBC._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x_unfold.view.view.view", "fairseq.modules.unfold1d", "x_unfold.view.view.view", "torch.softmax().type_as", "torch.softmax().type_as", "torch.softmax().type_as", "torch.softmax().type_as.size", "x.new", "lightweight_convolution.LightweightConv1dTBC._set_input_buffer", "torch.softmax().type_as.view().expand().contiguous", "x.unsqueeze", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax().type_as.float", "torch.softmax().type_as.view().expand", "x_unfold.view.view.size", "torch.softmax().type_as.view"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.unfold1d.unfold1d", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_forward_unfolded", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "'''The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.'''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "weight", "=", "self", ".", "weight", ".", "view", "(", "H", ",", "K", ")", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "x", ".", "new", "(", ")", "\n", "", "x_unfold", "=", "torch", ".", "cat", "(", "[", "input_buffer", ",", "x", ".", "unsqueeze", "(", "3", ")", "]", ",", "dim", "=", "3", ")", "\n", "if", "self", ".", "kernel_size", ">", "1", ":", "\n", "                ", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "x_unfold", "[", ":", ",", ":", ",", ":", ",", "-", "self", ".", "kernel_size", "+", "1", ":", "]", ")", "\n", "", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "# unfold the input: T x B x C --> T' x B x C x K", "\n", "            ", "x_unfold", "=", "unfold1d", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "0", ")", "\n", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "K", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ".", "float", "(", ")", ",", "dim", "=", "1", ")", ".", "type_as", "(", "weight", ")", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "weight", "=", "weight", "[", ":", ",", "-", "x_unfold", ".", "size", "(", "2", ")", ":", "]", "\n", "K", "=", "weight", ".", "size", "(", "1", ")", "\n", "\n", "", "weight", "=", "weight", ".", "view", "(", "1", ",", "H", ",", "K", ")", ".", "expand", "(", "T", "*", "B", ",", "H", ",", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "K", ",", "1", ")", "\n", "\n", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "output", "=", "torch", ".", "bmm", "(", "x_unfold", ",", "weight", ")", "# T*B*H x R x 1", "\n", "output", "=", "output", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded": [[183, 213], ["x.view().transpose.view().transpose.size", "lightweight_convolution.LightweightConv1dTBC.weight.view", "weight.narrow.narrow.view().expand().contiguous", "weight.narrow.narrow.view().transpose", "x.view().transpose.view().transpose.view().transpose", "weight.narrow.narrow.new_zeros", "torch.dropout.as_strided().copy_", "torch.dropout.narrow", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "torch.softmax().type_as", "torch.softmax().type_as", "torch.softmax().type_as", "weight.narrow.narrow.narrow", "weight.narrow.narrow.view().expand", "weight.narrow.narrow.view", "x.view().transpose.view().transpose.view", "torch.dropout.as_strided", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.float", "weight.narrow.narrow.view", "output.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_forward_expanded", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "'''Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        '''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "weight", "=", "self", ".", "weight", ".", "view", "(", "H", ",", "K", ")", "\n", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ".", "float", "(", ")", ",", "dim", "=", "1", ")", ".", "type_as", "(", "weight", ")", "\n", "", "weight", "=", "weight", ".", "view", "(", "1", ",", "H", ",", "K", ")", ".", "expand", "(", "T", "*", "B", ",", "H", ",", "K", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "weight", ".", "view", "(", "T", ",", "B", "*", "H", ",", "K", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "T", ",", "B", "*", "H", ",", "R", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "P", "=", "self", ".", "padding_l", "\n", "if", "K", ">", "T", "and", "P", "==", "K", "-", "1", ":", "\n", "            ", "weight", "=", "weight", ".", "narrow", "(", "2", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "P", "=", "T", ",", "T", "-", "1", "\n", "# turn the convolution filters into band matrices", "\n", "", "weight_expanded", "=", "weight", ".", "new_zeros", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ",", "requires_grad", "=", "False", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "P", ",", "T", ")", "\n", "weight_expanded", "=", "F", ".", "dropout", "(", "weight_expanded", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "output", "=", "torch", ".", "bmm", "(", "weight_expanded", ",", "x", ")", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC.reorder_incremental_state": [[214, 219], ["lightweight_convolution.LightweightConv1dTBC._get_input_buffer", "input_buffer.index_select.index_select.index_select", "lightweight_convolution.LightweightConv1dTBC._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._get_input_buffer": [[220, 222], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC._set_input_buffer": [[223, 225], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.lightweight_convolution.LightweightConv1dTBC.extra_repr": [[226, 234], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, bias={}'", ".", "format", "(", "\n", "self", ".", "input_size", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "\n", "self", ".", "num_heads", ",", "self", ".", "weight_softmax", ",", "self", ".", "bias", "is", "not", "None", "\n", ")", "\n", "if", "self", ".", "weight_dropout", ">", "0.", ":", "\n", "            ", "s", "+=", "', weight_dropout={}'", ".", "format", "(", "self", ".", "weight_dropout", ")", "\n", "", "return", "s", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.unfold1d.unfold1d": [[11, 20], ["x.unsqueeze.size", "torch.pad", "x.unsqueeze.as_strided", "x.unsqueeze.unsqueeze"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["def", "unfold1d", "(", "x", ",", "kernel_size", ",", "padding_l", ",", "pad_value", "=", "0", ")", ":", "\n", "    ", "'''unfold T x B x C to T x B x C x K'''", "\n", "if", "kernel_size", ">", "1", ":", "\n", "        ", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "kernel_size", "-", "1", "-", "padding_l", ")", ",", "value", "=", "pad_value", ")", "\n", "x", "=", "x", ".", "as_strided", "(", "(", "T", ",", "B", ",", "C", ",", "kernel_size", ")", ",", "(", "B", "*", "C", ",", "C", ",", "1", ",", "B", "*", "C", ")", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "3", ")", "\n", "", "return", "x", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.logsumexp_moe.LogSumExpMoE.forward": [[18, 23], ["ctx.save_for_backward", "torch.logsumexp"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "logp", ",", "posterior", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "posterior", ")", "\n", "ctx", ".", "dim", "=", "dim", "\n", "return", "torch", ".", "logsumexp", "(", "logp", ",", "dim", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.logsumexp_moe.LogSumExpMoE.backward": [[24, 29], ["grad_output.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "posterior", ",", "=", "ctx", ".", "saved_tensors", "\n", "grad_logp", "=", "grad_output", ".", "unsqueeze", "(", "ctx", ".", "dim", ")", "*", "posterior", "\n", "return", "grad_logp", ",", "None", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.__init__": [[20, 32], ["super().__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ReLU", "torch.nn.ReLU", "highway.Highway.reset_parameters", "torch.nn.Linear", "torch.nn.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "num_layers", ":", "int", "=", "1", "\n", ")", ":", "\n", "        ", "super", "(", "Highway", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", "*", "2", ")", "\n", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "self", ".", "activation", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.reset_parameters": [[33, 44], ["torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "# As per comment in AllenNLP:", "\n", "# We should bias the highway layer to just carry its input forward.  We do that by", "\n", "# setting the bias on `B(x)` to be positive, because that means `g` will be biased to", "\n", "# be high, so we will carry the input forward.  The bias on `B(x)` is the second half", "\n", "# of the bias vector in each Linear layer.", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", "[", "self", ".", "input_dim", ":", "]", ",", "1", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", "[", ":", "self", ".", "input_dim", "]", ",", "0", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "layer", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.highway.Highway.forward": [[45, 56], ["layer", "layer.chunk", "highway.Highway.activation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.new_tensor"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", "\n", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "projection", "=", "layer", "(", "x", ")", "\n", "proj_x", ",", "gate", "=", "projection", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "proj_x", "=", "self", ".", "activation", "(", "proj_x", ")", "\n", "gate", "=", "F", ".", "sigmoid", "(", "gate", ")", "\n", "x", "=", "gate", "*", "x", "+", "(", "gate", ".", "new_tensor", "(", "[", "1", "]", ")", "-", "gate", ")", "*", "proj_x", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.mean_pool_gating_network.MeanPoolGatingNetwork.__init__": [[20, 28], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_experts", ",", "dropout", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_experts", "=", "num_experts", "\n", "\n", "self", ".", "fc1", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "if", "dropout", "is", "not", "None", "else", "None", "\n", "self", ".", "fc2", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "num_experts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.mean_pool_gating_network.MeanPoolGatingNetwork.forward": [[29, 54], ["encoder_out[].transpose", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "mean_pool_gating_network.MeanPoolGatingNetwork.fc2", "torch.log_softmax().type_as", "torch.log_softmax().type_as", "ValueError", "encoder_out.clone.clone.clone", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "mean_pool_gating_network.MeanPoolGatingNetwork.fc1", "mean_pool_gating_network.MeanPoolGatingNetwork.dropout", "isinstance", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.type_as", "torch.sum.type_as", "torch.log_softmax", "torch.log_softmax", "encoder_out[].size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "if", "not", "(", "\n", "isinstance", "(", "encoder_out", ",", "dict", ")", "\n", "and", "'encoder_out'", "in", "encoder_out", "\n", "and", "'encoder_padding_mask'", "in", "encoder_out", "\n", "and", "encoder_out", "[", "'encoder_out'", "]", ".", "size", "(", "2", ")", "==", "self", ".", "embed_dim", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Unexpected format for encoder_out'", ")", "\n", "\n", "# mean pooling over time", "\n", "", "encoder_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", "# B x T", "\n", "encoder_out", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "transpose", "(", "0", ",", "1", ")", "# B x T x C", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_out", "=", "encoder_out", ".", "clone", "(", ")", "# required because of transpose above", "\n", "encoder_out", "[", "encoder_padding_mask", "]", "=", "0", "\n", "ntokens", "=", "torch", ".", "sum", "(", "1", "-", "encoder_padding_mask", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "torch", ".", "sum", "(", "encoder_out", ",", "dim", "=", "1", ")", "/", "ntokens", ".", "type_as", "(", "encoder_out", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "mean", "(", "encoder_out", ",", "dim", "=", "1", ")", "\n", "\n", "", "x", "=", "torch", ".", "tanh", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "type_as", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.forward": [[18, 26], ["list", "input.new().fill_", "input.new().fill_.narrow().copy_", "input.size", "input.new", "input.new().fill_.narrow"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "dim", ",", "bias_init", ")", ":", "\n", "        ", "size", "=", "list", "(", "input", ".", "size", "(", ")", ")", "\n", "size", "[", "dim", "]", "+=", "1", "\n", "output", "=", "input", ".", "new", "(", "*", "size", ")", ".", "fill_", "(", "bias_init", ")", "\n", "output", ".", "narrow", "(", "dim", ",", "1", ",", "size", "[", "dim", "]", "-", "1", ")", ".", "copy_", "(", "input", ")", "\n", "ctx", ".", "dim", "=", "dim", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward": [[27, 30], ["grad.narrow", "grad.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "grad", ".", "narrow", "(", "ctx", ".", "dim", ",", "1", ",", "grad", ".", "size", "(", "ctx", ".", "dim", ")", "-", "1", ")", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.scalar_bias": [[32, 34], ["ScalarBias.apply"], "function", ["None"], ["", "", "def", "scalar_bias", "(", "input", ",", "dim", ",", "bias_init", "=", "0", ")", ":", "\n", "    ", "return", "ScalarBias", ".", "apply", "(", "input", ",", "dim", ",", "bias_init", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution.__init__": [[25, 29], ["conv_tbc.ConvTBC.__init__", "linearized_convolution.LinearizedConvolution.register_backward_hook"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "self", ".", "_linearized_weight", "=", "None", "\n", "self", ".", "register_backward_hook", "(", "self", ".", "_clear_linearized_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution.forward": [[30, 67], ["linearized_convolution.LinearizedConvolution._get_linearized_weight", "input.size", "torch.linear.view", "super().forward", "linearized_convolution.LinearizedConvolution._get_input_buffer", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.linear", "torch.linear", "input.new().zero_", "linearized_convolution.LinearizedConvolution._set_input_buffer", "input_buffer[].clone", "input.view", "input.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.forward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            incremental_state: Used to buffer signal; if not None, then input is\n                expected to contain a single frame. If the input order changes\n                between time steps, call reorder_incremental_state.\n        Input:\n            Time x Batch x Channel during training\n            Batch x Time x Channel during inference\n        \"\"\"", "\n", "if", "incremental_state", "is", "None", ":", "\n", "            ", "output", "=", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "if", "self", ".", "kernel_size", "[", "0", "]", ">", "1", "and", "self", ".", "padding", "[", "0", "]", ">", "0", ":", "\n", "# remove future timesteps added by padding", "\n", "                ", "output", "=", "output", "[", ":", "-", "self", ".", "padding", "[", "0", "]", ",", ":", ",", ":", "]", "\n", "", "return", "output", "\n", "\n", "# reshape weight", "\n", "", "weight", "=", "self", ".", "_get_linearized_weight", "(", ")", "\n", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "\n", "bsz", "=", "input", ".", "size", "(", "0", ")", "# input: bsz x len x dim", "\n", "if", "kw", ">", "1", ":", "\n", "            ", "input", "=", "input", ".", "data", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "input", ".", "new", "(", "bsz", ",", "kw", ",", "input", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "", "else", ":", "\n", "# shift buffer", "\n", "                ", "input_buffer", "[", ":", ",", ":", "-", "1", ",", ":", "]", "=", "input_buffer", "[", ":", ",", "1", ":", ",", ":", "]", ".", "clone", "(", ")", "\n", "# append next input", "\n", "", "input_buffer", "[", ":", ",", "-", "1", ",", ":", "]", "=", "input", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "input", "=", "input_buffer", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output", "=", "F", ".", "linear", "(", "input", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "weight", ",", "self", ".", "bias", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state": [[68, 73], ["linearized_convolution.LinearizedConvolution._get_input_buffer", "input_buffer.index_select.index_select.index_select", "linearized_convolution.LinearizedConvolution._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer": [[74, 76], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer": [[77, 79], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight": [[80, 87], ["linearized_convolution.LinearizedConvolution.weight.transpose().transpose().contiguous", "linearized_convolution.LinearizedConvolution.view", "linearized_convolution.LinearizedConvolution.size", "linearized_convolution.LinearizedConvolution.weight.transpose().transpose", "linearized_convolution.LinearizedConvolution.weight.transpose"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_get_linearized_weight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_linearized_weight", "is", "None", ":", "\n", "            ", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "weight", "=", "self", ".", "weight", ".", "transpose", "(", "2", ",", "1", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "assert", "weight", ".", "size", "(", ")", "==", "(", "self", ".", "out_channels", ",", "kw", ",", "self", ".", "in_channels", ")", "\n", "self", ".", "_linearized_weight", "=", "weight", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", "\n", "", "return", "self", ".", "_linearized_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._clear_linearized_weight": [[88, 90], ["None"], "methods", ["None"], ["", "def", "_clear_linearized_weight", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "_linearized_weight", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.SingleHeadAttention.__init__": [[21, 63], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "k_layers.append", "v_layers.append", "k_layers.append", "downsampled_multihead_attention.GatedLinear", "v_layers.append", "k_layers.append", "downsampled_multihead_attention.Linear", "v_layers.append", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Downsample", "downsampled_multihead_attention.Downsample", "downsampled_multihead_attention.GatedLinear", "downsampled_multihead_attention.GatedLinear", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "out_channels", ",", "embed_dim", ",", "head_dim", ",", "head_index", ",", "dropout", "=", "0.", ",", "\n", "bias", "=", "True", ",", "project_input", "=", "True", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", "num_heads", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_index", "=", "head_index", "\n", "self", ".", "head_dim", "=", "head_dim", "\n", "self", ".", "project_input", "=", "project_input", "\n", "self", ".", "gated", "=", "gated", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "projection", "=", "None", "\n", "\n", "k_layers", "=", "[", "]", "\n", "v_layers", "=", "[", "]", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "k_layers", ".", "append", "(", "Downsample", "(", "self", ".", "head_index", ")", ")", "\n", "v_layers", ".", "append", "(", "Downsample", "(", "self", ".", "head_index", ")", ")", "\n", "out_proj_size", "=", "self", ".", "head_dim", "\n", "", "else", ":", "\n", "            ", "out_proj_size", "=", "self", ".", "head_dim", "*", "self", ".", "num_heads", "\n", "", "if", "self", ".", "gated", ":", "\n", "            ", "k_layers", ".", "append", "(", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "self", ".", "in_proj_q", "=", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", "\n", "v_layers", ".", "append", "(", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "k_layers", ".", "append", "(", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "self", ".", "in_proj_q", "=", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", "\n", "v_layers", ".", "append", "(", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "\n", "", "self", ".", "in_proj_k", "=", "nn", ".", "Sequential", "(", "*", "k_layers", ")", "\n", "self", ".", "in_proj_v", "=", "nn", ".", "Sequential", "(", "*", "v_layers", ")", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "self", ".", "out_proj", "=", "Linear", "(", "out_proj_size", ",", "self", ".", "head_dim", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out_proj", "=", "Linear", "(", "out_proj_size", ",", "out_channels", ",", "bias", "=", "bias", ")", "\n", "\n", "", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.SingleHeadAttention.forward": [[64, 150], ["key.size", "query.size", "q.view.view.transpose", "k.view.view.transpose", "fairseq.modules.scalar_bias.scalar_bias.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax", "torch.softmax", "torch.softmax", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "downsampled_multihead_attention.SingleHeadAttention.out_proj", "list", "key.size", "value.size", "downsampled_multihead_attention.SingleHeadAttention.in_proj_q", "downsampled_multihead_attention.SingleHeadAttention.in_proj_k", "downsampled_multihead_attention.SingleHeadAttention.in_proj_v", "q.view.view.view", "k.view.view.view", "fairseq.modules.scalar_bias.scalar_bias.view", "k.view.view.transpose", "[].unsqueeze", "[].unsqueeze", "fairseq.modules.scalar_bias.scalar_bias", "fairseq.modules.scalar_bias.scalar_bias", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "query.size", "key_padding_mask.size", "key_padding_mask.size", "k.view.view.size", "query.size", "key.size", "key_padding_mask.max", "attn_weights.view.view.masked_fill", "attn_weights.view.view.view", "attn_weights.view.view.view", "attn_weights.view.view.view", "key_padding_mask.unsqueeze().unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "attn_weights.view.view.data.new().expand().clone", "attn_weights.view.view.data.new().expand().clone", "key_padding_mask.unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "attn_weights.view.view.data.new().expand", "attn_weights.view.view.data.new().expand", "attn_weights.view.view.data.new", "attn_weights.view.view.data.new"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.scalar_bias", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.scalar_bias", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "\n", "self", ",", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "False", ",", "\n", "key_padding_mask", "=", "None", ",", "use_scalar_bias", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n        Self-attention can be implemented by passing in the same arguments for\n        query, key and value. Future timesteps can be masked with the\n        `mask_future_timesteps` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "src_len", ",", "bsz", ",", "out_channels", "=", "key", ".", "size", "(", ")", "\n", "tgt_len", "=", "query", ".", "size", "(", "0", ")", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "out_channels", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "downsample", ":", "\n", "            ", "size", "=", "bsz", "\n", "", "else", ":", "\n", "            ", "size", "=", "bsz", "*", "self", ".", "num_heads", "\n", "\n", "", "k", "=", "key", "\n", "v", "=", "value", "\n", "q", "=", "query", "\n", "if", "self", ".", "project_input", ":", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "q", ")", "\n", "k", "=", "self", ".", "in_proj_k", "(", "k", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "v", ")", "\n", "src_len", "=", "k", ".", "size", "(", ")", "[", "0", "]", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "not", "self", ".", "downsample", ":", "\n", "            ", "q", "=", "q", ".", "view", "(", "tgt_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "k", "=", "k", ".", "view", "(", "src_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "v", "=", "v", ".", "view", "(", "src_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "\n", "", "q", "=", "q", ".", "transpose", "(", "0", ",", "1", ")", "\n", "k", "=", "k", ".", "transpose", "(", "0", ",", "1", ")", "\n", "v", "=", "v", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "if", "mask_future_timesteps", ":", "\n", "            ", "assert", "query", ".", "size", "(", ")", "==", "key", ".", "size", "(", ")", ",", "'mask_future_timesteps only applies to self-attention'", "\n", "attn_weights", "*=", "torch", ".", "tril", "(", "\n", "attn_weights", ".", "data", ".", "new", "(", "[", "1", "]", ")", ".", "expand", "(", "tgt_len", ",", "tgt_len", ")", ".", "clone", "(", ")", ",", "\n", "diagonal", "=", "-", "1", ",", "\n", ")", "[", ":", ",", ":", ":", "self", ".", "head_index", "+", "1", "if", "self", ".", "downsample", "else", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "attn_weights", "+=", "torch", ".", "triu", "(", "\n", "attn_weights", ".", "data", ".", "new", "(", "[", "-", "math", ".", "inf", "]", ")", ".", "expand", "(", "tgt_len", ",", "tgt_len", ")", ".", "clone", "(", ")", ",", "\n", "diagonal", "=", "0", "\n", ")", "[", ":", ",", ":", ":", "self", ".", "head_index", "+", "1", "if", "self", ".", "downsample", "else", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "", "tgt_size", "=", "tgt_len", "\n", "if", "use_scalar_bias", ":", "\n", "            ", "attn_weights", "=", "scalar_bias", "(", "attn_weights", ",", "2", ")", "\n", "v", "=", "scalar_bias", "(", "v", ",", "1", ")", "\n", "tgt_size", "+=", "1", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "if", "key_padding_mask", ".", "max", "(", ")", ">", "0", ":", "\n", "                ", "if", "self", ".", "downsample", ":", "\n", "                    ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "1", ",", "tgt_len", ",", "src_len", ")", "\n", "", "else", ":", "\n", "                    ", "attn_weights", "=", "attn_weights", ".", "view", "(", "size", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "-", "math", ".", "inf", ",", "\n", ")", "\n", "attn_weights", "=", "attn_weights", ".", "view", "(", "size", ",", "tgt_len", ",", "src_len", ")", "\n", "", "", "attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ",", "dim", "=", "-", "1", ")", "\n", "attn_weights", "=", "F", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn", "=", "torch", ".", "bmm", "(", "attn_weights", ",", "v", ")", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "self", ".", "head_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "self", ".", "embed_dim", ")", "\n", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "\n", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.DownsampledMultiHeadAttention.__init__": [[156, 188], ["range", "torch.ModuleList.__init__", "downsampled_multihead_attention.Linear", "torch.ModuleList.__init__", "downsampled_multihead_attention.SingleHeadAttention", "attention_heads.append", "downsampled_multihead_attention.SingleHeadAttention"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["def", "__init__", "(", "\n", "self", ",", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "project_input", "=", "True", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "gated", "=", "gated", "\n", "self", ".", "project_input", "=", "project_input", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "embed_dim", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "attention_heads", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "self", ".", "num_heads", ")", ":", "\n", "                ", "attention_heads", ".", "append", "(", "\n", "SingleHeadAttention", "(", "\n", "out_channels", ",", "self", ".", "embed_dim", ",", "self", ".", "head_dim", ",", "index", ",", "\n", "self", ".", "dropout", ",", "bias", ",", "self", ".", "project_input", ",", "self", ".", "gated", ",", "\n", "self", ".", "downsample", ",", "self", ".", "num_heads", ",", "\n", ")", "\n", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "modules", "=", "attention_heads", ")", "\n", "self", ".", "out_proj", "=", "Linear", "(", "embed_dim", ",", "out_channels", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "# either we have a list of attention heads, or just one attention head", "\n", "# if not being downsampled, we can do the heads with one linear layer instead of separate ones", "\n", "            ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention_module", "=", "SingleHeadAttention", "(", "\n", "out_channels", ",", "self", ".", "embed_dim", ",", "self", ".", "head_dim", ",", "1", ",", "self", ".", "dropout", ",", "\n", "bias", ",", "self", ".", "project_input", ",", "self", ".", "gated", ",", "self", ".", "downsample", ",", "self", ".", "num_heads", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.DownsampledMultiHeadAttention.forward": [[190, 228], ["key.size", "query.size", "list", "key.size", "value.size", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "downsampled_multihead_attention.DownsampledMultiHeadAttention.out_proj", "downsampled_multihead_attention.DownsampledMultiHeadAttention.attention_module", "attn.append", "attn_weights.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "full_attn_weights.view.view.view", "query.size", "attn.append", "attn_weights.append", "attn_weights[].clone", "full_attn_weights.view.view.sum"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "forward", "(", "\n", "self", ",", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "False", ",", "\n", "key_padding_mask", "=", "None", ",", "use_scalar_bias", "=", "False", ",", "\n", ")", ":", "\n", "        ", "src_len", ",", "bsz", ",", "embed_dim", "=", "key", ".", "size", "(", ")", "\n", "tgt_len", "=", "query", ".", "size", "(", "0", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "tgt_size", "=", "tgt_len", "\n", "if", "use_scalar_bias", ":", "\n", "            ", "tgt_size", "+=", "1", "\n", "\n", "", "attn", "=", "[", "]", "\n", "attn_weights", "=", "[", "]", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "for", "attention_head_number", "in", "range", "(", "self", ".", "num_heads", ")", ":", "\n", "# call the forward of each attention head", "\n", "                ", "_attn", ",", "_attn_weight", "=", "self", "[", "attention_head_number", "]", "(", "\n", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", ",", "key_padding_mask", ",", "use_scalar_bias", ",", "\n", ")", "\n", "attn", ".", "append", "(", "_attn", ")", "\n", "attn_weights", ".", "append", "(", "_attn_weight", ")", "\n", "", "full_attn", "=", "torch", ".", "cat", "(", "attn", ",", "dim", "=", "2", ")", "\n", "full_attn", "=", "self", ".", "out_proj", "(", "full_attn", ")", "\n", "return", "full_attn", ",", "attn_weights", "[", "0", "]", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "            ", "_attn", ",", "_attn_weight", "=", "self", ".", "attention_module", "(", "\n", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", ",", "key_padding_mask", ",", "use_scalar_bias", ",", "\n", ")", "\n", "attn", ".", "append", "(", "_attn", ")", "\n", "attn_weights", ".", "append", "(", "_attn_weight", ")", "\n", "full_attn", "=", "torch", ".", "cat", "(", "attn", ",", "dim", "=", "2", ")", "\n", "full_attn_weights", "=", "torch", ".", "cat", "(", "attn_weights", ")", "\n", "full_attn_weights", "=", "full_attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_size", ",", "src_len", ")", "\n", "full_attn_weights", "=", "full_attn_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "self", ".", "num_heads", "\n", "return", "full_attn", ",", "full_attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.Downsample.__init__": [[234, 237], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "index", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.Downsample.forward": [[238, 240], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "[", ":", ":", "self", ".", "index", "+", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.Linear": [[242, 248], ["torch.Linear", "nn.Linear.weight.data.normal_", "nn.Linear.bias.data.zero_", "torch.utils.weight_norm", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: B x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.downsampled_multihead_attention.GatedLinear": [[250, 258], ["torch.Sequential", "downsampled_multihead_attention.Linear", "torch.GLU", "downsampled_multihead_attention.Linear", "torch.GLU", "downsampled_multihead_attention.Linear"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "GatedLinear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: B x T x C) with interspersed GLU units\"\"\"", "\n", "return", "nn", ".", "Sequential", "(", "\n", "Linear", "(", "in_features", ",", "out_features", "*", "4", ",", "dropout", ",", "bias", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_features", "*", "2", ",", "out_features", "*", "2", ",", "dropout", ",", "bias", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_features", ",", "out_features", ",", "dropout", ",", "bias", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.add_args": [[51, 74], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "help", "=", "'path to data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--lang-pairs'", ",", "default", "=", "None", ",", "metavar", "=", "'PAIRS'", ",", "\n", "help", "=", "'comma-separated list of language pairs (in training order): en-de,en-fr,de-fr'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language (only needed for inference)'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language (only needed for inference)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-source'", ",", "default", "=", "'True'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the source on the left (default: True)'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-target'", ",", "default", "=", "'False'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the target on the left (default: False)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-source-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the source sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-target-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.__init__": [[76, 82], ["FairseqTask.__init__", "list", "dicts.keys"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dicts", ",", "training", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dicts", "=", "dicts", "\n", "self", ".", "lang_pairs", "=", "args", ".", "lang_pairs", "\n", "self", ".", "langs", "=", "list", "(", "dicts", ".", "keys", "(", ")", ")", "\n", "self", ".", "training", "=", "training", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.setup_task": [[83, 114], ["fairseq.options.eval_bool", "fairseq.options.eval_bool", "list", "collections.OrderedDict", "cls", "args.lang_pairs.split", "args.lang_pairs[].split", "fairseq.data.Dictionary.load", "print", "ValueError", "os.path.join", "len", "lang_pair.split", "dicts[].pad", "dicts[].pad", "dicts[].eos", "dicts[].eos", "dicts[].unk", "dicts[].unk", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "args", ".", "left_pad_source", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_source", ")", "\n", "args", ".", "left_pad_target", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_target", ")", "\n", "\n", "if", "args", ".", "source_lang", "is", "not", "None", "or", "args", ".", "target_lang", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "lang_pairs", "is", "not", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--source-lang/--target-lang implies generation, which is '", "\n", "'incompatible with --lang-pairs'", "\n", ")", "\n", "", "training", "=", "False", "\n", "args", ".", "lang_pairs", "=", "[", "'{}-{}'", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", "]", "\n", "", "else", ":", "\n", "            ", "training", "=", "True", "\n", "args", ".", "lang_pairs", "=", "args", ".", "lang_pairs", ".", "split", "(", "','", ")", "\n", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "args", ".", "lang_pairs", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "\n", "", "langs", "=", "list", "(", "{", "x", "for", "lang_pair", "in", "args", ".", "lang_pairs", "for", "x", "in", "lang_pair", ".", "split", "(", "'-'", ")", "}", ")", "\n", "\n", "# load dictionaries", "\n", "dicts", "=", "OrderedDict", "(", ")", "\n", "for", "lang", "in", "langs", ":", "\n", "            ", "dicts", "[", "lang", "]", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'dict.{}.txt'", ".", "format", "(", "lang", ")", ")", ")", "\n", "if", "len", "(", "dicts", ")", ">", "0", ":", "\n", "                ", "assert", "dicts", "[", "lang", "]", ".", "pad", "(", ")", "==", "dicts", "[", "langs", "[", "0", "]", "]", ".", "pad", "(", ")", "\n", "assert", "dicts", "[", "lang", "]", ".", "eos", "(", ")", "==", "dicts", "[", "langs", "[", "0", "]", "]", ".", "eos", "(", ")", "\n", "assert", "dicts", "[", "lang", "]", ".", "unk", "(", ")", "==", "dicts", "[", "langs", "[", "0", "]", "]", ".", "unk", "(", ")", "\n", "", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "lang", ",", "len", "(", "dicts", "[", "lang", "]", ")", ")", ")", "\n", "\n", "", "return", "cls", "(", "args", ",", "dicts", ",", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.load_dataset": [[115, 170], ["fairseq.data.RoundRobinZipDatasets", "os.path.join", "lang_pair.split", "multilingual_translation.MultilingualTranslationTask.load_dataset.split_exists"], "methods", ["None"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a dataset split.\"\"\"", "\n", "\n", "def", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "'{}.{}-{}.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ")", "\n", "if", "self", ".", "args", ".", "raw_text", "and", "IndexedRawTextDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "elif", "not", "self", ".", "args", ".", "raw_text", "and", "IndexedDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "indexed_dataset", "(", "path", ",", "dictionary", ")", ":", "\n", "            ", "if", "self", ".", "args", ".", "raw_text", ":", "\n", "                ", "return", "IndexedRawTextDataset", "(", "path", ",", "dictionary", ")", "\n", "", "elif", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "if", "self", ".", "args", ".", "lazy_load", ":", "\n", "                    ", "return", "IndexedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "return", "IndexedCachedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "", "return", "None", "\n", "\n", "", "src_datasets", ",", "tgt_datasets", "=", "{", "}", ",", "{", "}", "\n", "for", "lang_pair", "in", "self", ".", "args", ".", "lang_pairs", ":", "\n", "            ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "if", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "src", ")", ":", "\n", "                ", "prefix", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "'{}.{}-{}.'", ".", "format", "(", "split", ",", "src", ",", "tgt", ")", ")", "\n", "", "elif", "split_exists", "(", "split", ",", "tgt", ",", "src", ",", "src", ")", ":", "\n", "                ", "prefix", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "'{}.{}-{}.'", ".", "format", "(", "split", ",", "tgt", ",", "src", ")", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "", "src_datasets", "[", "lang_pair", "]", "=", "indexed_dataset", "(", "prefix", "+", "src", ",", "self", ".", "dicts", "[", "src", "]", ")", "\n", "tgt_datasets", "[", "lang_pair", "]", "=", "indexed_dataset", "(", "prefix", "+", "tgt", ",", "self", ".", "dicts", "[", "tgt", "]", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "self", ".", "args", ".", "data", ",", "split", ",", "len", "(", "src_datasets", "[", "lang_pair", "]", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "src_datasets", ")", "==", "0", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "self", ".", "args", ".", "data", ")", ")", "\n", "\n", "", "def", "language_pair_dataset", "(", "lang_pair", ")", ":", "\n", "            ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "src_dataset", ",", "tgt_dataset", "=", "src_datasets", "[", "lang_pair", "]", ",", "tgt_datasets", "[", "lang_pair", "]", "\n", "return", "LanguagePairDataset", "(", "\n", "src_dataset", ",", "src_dataset", ".", "sizes", ",", "self", ".", "dicts", "[", "src", "]", ",", "\n", "tgt_dataset", ",", "tgt_dataset", ".", "sizes", ",", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "RoundRobinZipDatasets", "(", "\n", "OrderedDict", "(", "[", "\n", "(", "lang_pair", ",", "language_pair_dataset", "(", "lang_pair", ")", ")", "\n", "for", "lang_pair", "in", "self", ".", "args", ".", "lang_pairs", "\n", "]", ")", ",", "\n", "eval_key", "=", "None", "if", "self", ".", "training", "else", "\"%s-%s\"", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.build_dataset_for_inference": [[172, 179], ["fairseq.data.RoundRobinZipDatasets", "collections.OrderedDict", "fairseq.data.LanguagePairDataset"], "methods", ["None"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "lang_pair", "=", "\"%s-%s\"", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", "\n", "return", "RoundRobinZipDatasets", "(", "\n", "OrderedDict", "(", "[", "\n", "(", "lang_pair", ",", "LanguagePairDataset", "(", "src_tokens", ",", "src_lengths", ",", "self", ".", "source_dictionary", ")", ")", "\n", "]", ")", ",", "\n", "eval_key", "=", "lang_pair", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.build_model": [[181, 187], ["models.build_model", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "if", "not", "isinstance", "(", "model", ",", "FairseqMultiModel", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'MultilingualTranslationTask requires a FairseqMultiModel architecture'", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.train_step": [[188, 203], ["model.train", "criterion", "optimizer.backward", "loss.detach().item", "len", "loss.detach"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "=", "0.", ",", "0.", ",", "{", "}", "\n", "for", "lang_pair", "in", "self", ".", "args", ".", "lang_pairs", ":", "\n", "            ", "if", "sample", "[", "lang_pair", "]", "is", "None", "or", "len", "(", "sample", "[", "lang_pair", "]", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "lang_pair", "]", ")", "\n", "if", "ignore_grad", ":", "\n", "                ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "agg_loss", "+=", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", "\n", "# TODO make summing of the sample sizes configurable", "\n", "agg_sample_size", "+=", "sample_size", "\n", "agg_logging_output", "[", "lang_pair", "]", "=", "logging_output", "\n", "", "return", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.valid_step": [[204, 217], ["model.eval", "torch.no_grad", "criterion", "loss.data.item", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "=", "0.", ",", "0.", ",", "{", "}", "\n", "for", "lang_pair", "in", "self", ".", "args", ".", "lang_pairs", ":", "\n", "                ", "if", "sample", "[", "lang_pair", "]", "is", "None", "or", "len", "(", "sample", "[", "lang_pair", "]", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "lang_pair", "]", ")", "\n", "agg_loss", "+=", "loss", ".", "data", ".", "item", "(", ")", "\n", "# TODO make summing of the sample sizes configurable", "\n", "agg_sample_size", "+=", "sample_size", "\n", "agg_logging_output", "[", "lang_pair", "]", "=", "logging_output", "\n", "", "", "return", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.init_logging_output": [[218, 228], ["sum", "sum", "sample_lang.get", "sample.values", "sample_lang[].size", "sample.values"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "init_logging_output", "(", "self", ",", "sample", ")", ":", "\n", "        ", "return", "{", "\n", "'ntokens'", ":", "sum", "(", "\n", "sample_lang", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "for", "sample_lang", "in", "sample", ".", "values", "(", ")", "\n", ")", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "'nsentences'", ":", "sum", "(", "\n", "sample_lang", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "'target'", "in", "sample_lang", "else", "0", "\n", "for", "sample_lang", "in", "sample", ".", "values", "(", ")", "\n", ")", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.grad_denom": [[230, 232], ["criterion.__class__.grad_denom"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.grad_denom"], ["", "def", "grad_denom", "(", "self", ",", "sample_sizes", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.aggregate_logging_outputs": [[233, 258], ["multilingual_translation.MultilingualTranslationTask.aggregate_logging_outputs.sum_over_languages"], "methods", ["None"], ["", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "# aggregate logging outputs for each language pair", "\n", "        ", "agg_logging_outputs", "=", "{", "\n", "lang_pair", ":", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "[", "\n", "logging_output", ".", "get", "(", "lang_pair", ",", "{", "}", ")", "for", "logging_output", "in", "logging_outputs", "\n", "]", ")", "\n", "for", "lang_pair", "in", "self", ".", "args", ".", "lang_pairs", "\n", "}", "\n", "\n", "def", "sum_over_languages", "(", "key", ")", ":", "\n", "            ", "return", "sum", "(", "logging_output", "[", "key", "]", "for", "logging_output", "in", "agg_logging_outputs", ".", "values", "(", ")", ")", "\n", "\n", "# flatten logging outputs", "\n", "", "flat_logging_output", "=", "{", "\n", "'{}:{}'", ".", "format", "(", "lang_pair", ",", "k", ")", ":", "v", "\n", "for", "lang_pair", ",", "agg_logging_output", "in", "agg_logging_outputs", ".", "items", "(", ")", "\n", "for", "k", ",", "v", "in", "agg_logging_output", ".", "items", "(", ")", "\n", "}", "\n", "flat_logging_output", "[", "'loss'", "]", "=", "sum_over_languages", "(", "'loss'", ")", "\n", "if", "any", "(", "'nll_loss'", "in", "logging_output", "for", "logging_output", "in", "agg_logging_outputs", ".", "values", "(", ")", ")", ":", "\n", "            ", "flat_logging_output", "[", "'nll_loss'", "]", "=", "sum_over_languages", "(", "'nll_loss'", ")", "\n", "", "flat_logging_output", "[", "'sample_size'", "]", "=", "sum_over_languages", "(", "'sample_size'", ")", "\n", "flat_logging_output", "[", "'nsentences'", "]", "=", "sum_over_languages", "(", "'nsentences'", ")", "\n", "flat_logging_output", "[", "'ntokens'", "]", "=", "sum_over_languages", "(", "'ntokens'", ")", "\n", "return", "flat_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.source_dictionary": [[259, 262], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dicts", "[", "self", ".", "args", ".", "source_lang", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.multilingual_translation.MultilingualTranslationTask.target_dictionary": [[263, 266], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dicts", "[", "self", ".", "args", ".", "target_lang", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.add_args": [[47, 70], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "nargs", "=", "'+'", ",", "help", "=", "'path(s) to data directorie(s)'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-source'", ",", "default", "=", "'True'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the source on the left'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-target'", ",", "default", "=", "'False'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the target on the left'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-source-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the source sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-target-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--upsample-primary'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'amount to upsample primary dataset'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.load_pretrained_model": [[72, 89], ["fairseq.utils.load_checkpoint_to_cpu", "fairseq.utils.override_model_args", "fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load", "translation.TranslationTask", "TranslationTask.build_model", "TranslationTask.build_model.upgrade_state_dict", "TranslationTask.build_model.load_state_dict", "fairseq.data.Dictionary.load.pad", "fairseq.data.Dictionary.load.pad", "fairseq.data.Dictionary.load.eos", "fairseq.data.Dictionary.load.eos", "fairseq.data.Dictionary.load.unk", "fairseq.data.Dictionary.load.unk"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.override_model_args", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "@", "staticmethod", "\n", "def", "load_pretrained_model", "(", "path", ",", "src_dict_path", ",", "tgt_dict_path", ",", "arg_overrides", "=", "None", ")", ":", "\n", "        ", "model", "=", "utils", ".", "load_checkpoint_to_cpu", "(", "path", ")", "\n", "args", "=", "model", "[", "'args'", "]", "\n", "state_dict", "=", "model", "[", "'model'", "]", "\n", "args", "=", "utils", ".", "override_model_args", "(", "args", ",", "arg_overrides", ")", "\n", "src_dict", "=", "Dictionary", ".", "load", "(", "src_dict_path", ")", "\n", "tgt_dict", "=", "Dictionary", ".", "load", "(", "tgt_dict_path", ")", "\n", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "\n", "task", "=", "TranslationTask", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "model", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "True", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.__init__": [[90, 94], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.setup_task": [[95, 121], ["fairseq.options.eval_bool", "fairseq.options.eval_bool", "cls.load_dictionary", "cls.load_dictionary", "print", "print", "cls", "fairseq.data.data_utils.infer_language_pair", "Exception", "os.path.join", "os.path.join", "cls.load_dictionary.pad", "cls.load_dictionary.pad", "cls.load_dictionary.eos", "cls.load_dictionary.eos", "cls.load_dictionary.unk", "cls.load_dictionary.unk", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.infer_language_pair", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "args", ".", "left_pad_source", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_source", ")", "\n", "args", ".", "left_pad_target", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_target", ")", "\n", "\n", "# find language pair automatically", "\n", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "data_utils", ".", "infer_language_pair", "(", "args", ".", "data", "[", "0", "]", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Could not infer language pair, please provide it explicitly'", ")", "\n", "\n", "# load dictionaries", "\n", "", "src_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ")", "\n", "tgt_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ")", "\n", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "source_lang", ",", "len", "(", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "target_lang", ",", "len", "(", "tgt_dict", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.load_dataset": [[122, 193], ["enumerate", "fairseq.data.LanguagePairDataset", "os.path.join", "itertools.count", "len", "len", "len", "fairseq.data.ConcatDataset", "fairseq.data.ConcatDataset", "fairseq.data.IndexedRawTextDataset.exists", "fairseq.data.IndexedRawTextDataset", "fairseq.data.IndexedDataset.exists", "translation.TranslationTask.load_dataset.split_exists"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "def", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "lang", ",", "data_path", ")", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ")", "\n", "if", "self", ".", "args", ".", "raw_text", "and", "IndexedRawTextDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "elif", "not", "self", ".", "args", ".", "raw_text", "and", "IndexedDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "indexed_dataset", "(", "path", ",", "dictionary", ")", ":", "\n", "            ", "if", "self", ".", "args", ".", "raw_text", ":", "\n", "                ", "return", "IndexedRawTextDataset", "(", "path", ",", "dictionary", ")", "\n", "", "elif", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "if", "self", ".", "args", ".", "lazy_load", ":", "\n", "                    ", "return", "IndexedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "return", "IndexedCachedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "", "return", "None", "\n", "\n", "", "src_datasets", "=", "[", "]", "\n", "tgt_datasets", "=", "[", "]", "\n", "\n", "data_paths", "=", "self", ".", "args", ".", "data", "\n", "\n", "for", "dk", ",", "data_path", "in", "enumerate", "(", "data_paths", ")", ":", "\n", "            ", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "                ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "\n", "# infer langcode", "\n", "src", ",", "tgt", "=", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", "\n", "if", "split_exists", "(", "split_k", ",", "src", ",", "tgt", ",", "src", ",", "data_path", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "src", ",", "tgt", ")", ")", "\n", "", "elif", "split_exists", "(", "split_k", ",", "tgt", ",", "src", ",", "src", ",", "data_path", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "tgt", ",", "src", ")", ")", "\n", "", "else", ":", "\n", "                    ", "if", "k", ">", "0", "or", "dk", ">", "0", ":", "\n", "                        ", "break", "\n", "", "else", ":", "\n", "                        ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "", "", "src_datasets", ".", "append", "(", "indexed_dataset", "(", "prefix", "+", "src", ",", "self", ".", "src_dict", ")", ")", "\n", "tgt_datasets", ".", "append", "(", "indexed_dataset", "(", "prefix", "+", "tgt", ",", "self", ".", "tgt_dict", ")", ")", "\n", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "len", "(", "src_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "assert", "len", "(", "src_datasets", ")", "==", "len", "(", "tgt_datasets", ")", "\n", "\n", "if", "len", "(", "src_datasets", ")", "==", "1", ":", "\n", "            ", "src_dataset", ",", "tgt_dataset", "=", "src_datasets", "[", "0", "]", ",", "tgt_datasets", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "sample_ratios", "=", "[", "1", "]", "*", "len", "(", "src_datasets", ")", "\n", "sample_ratios", "[", "0", "]", "=", "self", ".", "args", ".", "upsample_primary", "\n", "src_dataset", "=", "ConcatDataset", "(", "src_datasets", ",", "sample_ratios", ")", "\n", "tgt_dataset", "=", "ConcatDataset", "(", "tgt_datasets", ",", "sample_ratios", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "LanguagePairDataset", "(", "\n", "src_dataset", ",", "src_dataset", ".", "sizes", ",", "self", ".", "src_dict", ",", "\n", "tgt_dataset", ",", "tgt_dataset", ".", "sizes", ",", "self", ".", "tgt_dict", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.build_dataset_for_inference": [[195, 197], ["fairseq.data.LanguagePairDataset"], "methods", ["None"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "LanguagePairDataset", "(", "src_tokens", ",", "src_lengths", ",", "self", ".", "source_dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.max_positions": [[198, 201], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max sentence length allowed by the task.\"\"\"", "\n", "return", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.source_dictionary": [[202, 206], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the source :class:`~fairseq.data.Dictionary`.\"\"\"", "\n", "return", "self", ".", "src_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation.TranslationTask.target_dictionary": [[207, 211], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the target :class:`~fairseq.data.Dictionary`.\"\"\"", "\n", "return", "self", ".", "tgt_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.add_args": [[20, 24], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.__init__": [[25, 28], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "datasets", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dictionary": [[29, 37], ["fairseq.data.Dictionary.load"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "return", "Dictionary", ".", "load", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.build_dictionary": [[38, 57], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.finalize", "fairseq.data.Dictionary.add_file_to_dictionary"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_file_to_dictionary"], ["", "@", "classmethod", "\n", "def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Build the dictionary\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"", "\n", "d", "=", "Dictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "            ", "Dictionary", ".", "add_file_to_dictionary", "(", "filename", ",", "d", ",", "tokenizer", ".", "tokenize_line", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.setup_task": [[58, 66], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "return", "cls", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dataset": [[67, 74], ["None"], "methods", ["None"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.dataset": [[75, 91], ["KeyError", "isinstance", "TypeError"], "methods", ["None"], ["", "def", "dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "\"\"\"\n        Return a loaded dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n\n        Returns:\n            a :class:`~fairseq.data.FairseqDataset` corresponding to *split*\n        \"\"\"", "\n", "from", "fairseq", ".", "data", "import", "FairseqDataset", "\n", "if", "split", "not", "in", "self", ".", "datasets", ":", "\n", "            ", "raise", "KeyError", "(", "'Dataset not loaded: '", "+", "split", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "datasets", "[", "split", "]", ",", "FairseqDataset", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'Datasets are expected to be of type FairseqDataset'", ")", "\n", "", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.get_batch_iterator": [[92, 152], ["isinstance", "fairseq.data.data_utils.filter_by_size", "fairseq.data.data_utils.batch_by_size", "fairseq.data.iterators.EpochBatchIterator", "fairseq.data.data_utils.numpy_seed", "dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.filter_by_size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.batch_by_size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices"], ["", "def", "get_batch_iterator", "(", "\n", "self", ",", "dataset", ",", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "max_positions", "=", "None", ",", "\n", "ignore_invalid_inputs", "=", "False", ",", "required_batch_size_multiple", "=", "1", ",", "\n", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "num_workers", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Get an iterator that yields batches of data from the given dataset.\n\n        Args:\n            dataset (~fairseq.data.FairseqDataset): dataset to batch\n            max_tokens (int, optional): max number of tokens in each batch\n                (default: None).\n            max_sentences (int, optional): max number of sentences in each\n                batch (default: None).\n            max_positions (optional): max sentence length supported by the\n                model (default: None).\n            ignore_invalid_inputs (bool, optional): don't raise Exception for\n                sentences that are too long (default: False).\n            required_batch_size_multiple (int, optional): require batch size to\n                be a multiple of N (default: 1).\n            seed (int, optional): seed for random number generator for\n                reproducibility (default: 1).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n\n        Returns:\n            ~fairseq.iterators.EpochBatchIterator: a batched iterator over the\n                given dataset split\n        \"\"\"", "\n", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "\n", "# get indices ordered by example size", "\n", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "            ", "indices", "=", "dataset", ".", "ordered_indices", "(", ")", "\n", "\n", "# filter examples that are too large", "\n", "", "indices", "=", "data_utils", ".", "filter_by_size", "(", "\n", "indices", ",", "dataset", ".", "size", ",", "max_positions", ",", "raise_exception", "=", "(", "not", "ignore_invalid_inputs", ")", ",", "\n", ")", "\n", "\n", "# create mini-batches with given size constraints", "\n", "batch_sampler", "=", "data_utils", ".", "batch_by_size", "(", "\n", "indices", ",", "dataset", ".", "num_tokens", ",", "max_tokens", "=", "max_tokens", ",", "max_sentences", "=", "max_sentences", ",", "\n", "required_batch_size_multiple", "=", "required_batch_size_multiple", ",", "\n", ")", "\n", "\n", "# return a reusable, sharded iterator", "\n", "return", "iterators", ".", "EpochBatchIterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "collate_fn", "=", "dataset", ".", "collater", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "seed", "=", "seed", ",", "\n", "num_shards", "=", "num_shards", ",", "\n", "shard_id", "=", "shard_id", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.build_model": [[154, 167], ["models.build_model"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        Build the :class:`~fairseq.models.BaseFairseqModel` instance for this\n        task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.models.BaseFairseqModel` instance\n        \"\"\"", "\n", "from", "fairseq", "import", "models", "\n", "return", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.build_criterion": [[168, 181], ["criterions.build_criterion"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion"], ["", "def", "build_criterion", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        Build the :class:`~fairseq.criterions.FairseqCriterion` instance for\n        this task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.criterions.FairseqCriterion` instance\n        \"\"\"", "\n", "from", "fairseq", "import", "criterions", "\n", "return", "criterions", ".", "build_criterion", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.build_generator": [[182, 205], ["SequenceScorer", "SequenceGenerator"], "methods", ["None"], ["", "def", "build_generator", "(", "self", ",", "args", ")", ":", "\n", "        ", "if", "args", ".", "score_reference", ":", "\n", "            ", "from", "fairseq", ".", "sequence_scorer", "import", "SequenceScorer", "\n", "return", "SequenceScorer", "(", "self", ".", "target_dictionary", ")", "\n", "", "else", ":", "\n", "            ", "from", "fairseq", ".", "sequence_generator", "import", "SequenceGenerator", "\n", "return", "SequenceGenerator", "(", "\n", "self", ".", "target_dictionary", ",", "\n", "beam_size", "=", "args", ".", "beam", ",", "\n", "max_len_a", "=", "args", ".", "max_len_a", ",", "\n", "max_len_b", "=", "args", ".", "max_len_b", ",", "\n", "min_len", "=", "args", ".", "min_len", ",", "\n", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "\n", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ",", "\n", "sampling", "=", "args", ".", "sampling", ",", "\n", "sampling_topk", "=", "args", ".", "sampling_topk", ",", "\n", "sampling_temperature", "=", "args", ".", "sampling_temperature", ",", "\n", "diverse_beam_groups", "=", "args", ".", "diverse_beam_groups", ",", "\n", "diverse_beam_strength", "=", "args", ".", "diverse_beam_strength", ",", "\n", "match_source_len", "=", "args", ".", "match_source_len", ",", "\n", "no_repeat_ngram_size", "=", "args", ".", "no_repeat_ngram_size", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.train_step": [[207, 233], ["model.train", "criterion", "optimizer.backward"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward"], ["", "", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Do forward and backward, and return the loss as computed by *criterion*\n        for the given *model* and *sample*.\n\n        Args:\n            sample (dict): the mini-batch. The format is defined by the\n                :class:`~fairseq.data.FairseqDataset`.\n            model (~fairseq.models.BaseFairseqModel): the model\n            criterion (~fairseq.criterions.FairseqCriterion): the criterion\n            optimizer (~fairseq.optim.FairseqOptimizer): the optimizer\n            ignore_grad (bool): multiply loss by 0 if this is set to True\n\n        Returns:\n            tuple:\n                - the loss\n                - the sample size, which is used as the denominator for the\n                  gradient\n                - logging outputs to display while training\n        \"\"\"", "\n", "model", ".", "train", "(", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ",", "sample", ")", "\n", "if", "ignore_grad", ":", "\n", "            ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.valid_step": [[234, 239], ["model.eval", "torch.no_grad", "criterion"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ",", "sample", ")", "\n", "", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.inference_step": [[240, 243], ["torch.no_grad", "generator.generate"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "generator", ".", "generate", "(", "models", ",", "sample", ",", "prefix_tokens", "=", "prefix_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.grad_denom": [[244, 246], ["criterion.__class__.grad_denom"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.grad_denom"], ["", "", "def", "grad_denom", "(", "self", ",", "sample_sizes", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.aggregate_logging_outputs": [[247, 249], ["criterion.__class__.aggregate_logging_outputs"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.aggregate_logging_outputs"], ["", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.max_positions": [[250, 253], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max input length allowed by the task.\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.source_dictionary": [[254, 259], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the source :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.target_dictionary": [[260, 265], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the target :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).\"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.add_args": [[59, 84], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'path to data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-break-mode'", ",", "\n", "choices", "=", "[", "'none'", ",", "'complete'", ",", "'eos'", "]", ",", "\n", "help", "=", "'If omitted or \"none\", fills each sample with tokens-per-sample '", "\n", "'tokens. If set to \"complete\", splits samples only at the end '", "\n", "'of sentence, but may include multiple sentences per sample. '", "\n", "'If set to \"eos\", includes only one sentence per sample.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of tokens per sample for LM dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dictionary-size'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'limit the size of output dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--self-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include self target'", ")", "\n", "parser", ".", "add_argument", "(", "'--future-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include future target'", ")", "\n", "parser", ".", "add_argument", "(", "'--past-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include past target'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.__init__": [[86, 94], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "output_dictionary", ",", "targets", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "output_dictionary", "=", "output_dictionary", "\n", "\n", "if", "targets", "is", "None", ":", "\n", "            ", "targets", "=", "[", "'future'", "]", "\n", "", "self", ".", "targets", "=", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.setup_task": [[95, 127], ["hasattr", "getattr", "getattr", "getattr", "cls", "fairseq.data.Dictionary.load", "print", "targets.append", "targets.append", "targets.append", "len", "os.path.join", "fairseq.data.TruncatedDictionary", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "dictionary", "=", "None", "\n", "output_dictionary", "=", "None", "\n", "if", "args", ".", "data", ":", "\n", "            ", "dictionary", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "output_dictionary", "=", "dictionary", "\n", "if", "args", ".", "output_dictionary_size", ">=", "0", ":", "\n", "                ", "output_dictionary", "=", "TruncatedDictionary", "(", "dictionary", ",", "args", ".", "output_dictionary_size", ")", "\n", "\n", "# upgrade old checkpoints", "\n", "", "", "if", "hasattr", "(", "args", ",", "'exclude_self_target'", ")", ":", "\n", "            ", "args", ".", "self_target", "=", "not", "args", ".", "exclude_self_target", "\n", "\n", "", "targets", "=", "[", "]", "\n", "if", "getattr", "(", "args", ",", "'self_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'self'", ")", "\n", "", "if", "getattr", "(", "args", ",", "'future_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'future'", ")", "\n", "", "if", "getattr", "(", "args", ",", "'past_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'past'", ")", "\n", "", "if", "len", "(", "targets", ")", "==", "0", ":", "\n", "# standard language modeling", "\n", "            ", "targets", "=", "[", "'future'", "]", "\n", "\n", "", "return", "cls", "(", "args", ",", "dictionary", ",", "output_dictionary", ",", "targets", "=", "targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.build_model": [[128, 136], ["super().build_model", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "model", "=", "super", "(", ")", ".", "build_model", "(", "args", ")", "\n", "\n", "for", "target", "in", "self", ".", "targets", ":", "\n", "            ", "if", "target", "not", "in", "model", ".", "supported_targets", ":", "\n", "                ", "raise", "ValueError", "(", "'Unsupported language modeling target: {}'", ".", "format", "(", "target", ")", ")", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.load_dataset": [[137, 189], ["itertools.count", "fairseq.data.MonolingualDataset", "os.path.join", "loaded_datasets.append", "print", "len", "fairseq.data.ConcatDataset", "numpy.concatenate", "fairseq.data.IndexedRawTextDataset.exists", "fairseq.data.IndexedRawTextDataset", "fairseq.data.TokenBlockDataset", "str", "fairseq.data.IndexedDataset.exists", "len", "fairseq.data.IndexedDataset", "fairseq.data.IndexedCachedDataset", "FileNotFoundError", "language_modeling.LanguageModelingTask.dictionary.pad", "language_modeling.LanguageModelingTask.dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "loaded_datasets", "=", "[", "]", "\n", "\n", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "            ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "split_k", ")", "\n", "\n", "if", "self", ".", "args", ".", "raw_text", "and", "IndexedRawTextDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "ds", "=", "IndexedRawTextDataset", "(", "path", ",", "self", ".", "dictionary", ")", "\n", "", "elif", "not", "self", ".", "args", ".", "raw_text", "and", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "if", "self", ".", "args", ".", "lazy_load", ":", "\n", "                    ", "ds", "=", "IndexedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "ds", "=", "IndexedCachedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "k", ">", "0", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "self", ".", "args", ".", "data", ")", ")", "\n", "\n", "", "", "loaded_datasets", ".", "append", "(", "\n", "TokenBlockDataset", "(", "\n", "ds", ",", "ds", ".", "sizes", ",", "self", ".", "args", ".", "tokens_per_sample", ",", "\n", "pad", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "eos", "=", "self", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "sample_break_mode", ",", "include_targets", "=", "True", ",", "\n", ")", "\n", ")", "\n", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "self", ".", "args", ".", "data", ",", "split_k", ",", "len", "(", "loaded_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "len", "(", "loaded_datasets", ")", "==", "1", ":", "\n", "            ", "dataset", "=", "loaded_datasets", "[", "0", "]", "\n", "sizes", "=", "dataset", ".", "sizes", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "ConcatDataset", "(", "loaded_datasets", ")", "\n", "sizes", "=", "np", ".", "concatenate", "(", "[", "ds", ".", "sizes", "for", "ds", "in", "loaded_datasets", "]", ")", "\n", "\n", "", "add_eos_for_other_targets", "=", "self", ".", "args", ".", "sample_break_mode", "is", "not", "None", "and", "self", ".", "args", ".", "sample_break_mode", "!=", "'none'", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "MonolingualDataset", "(", "\n", "dataset", ",", "sizes", ",", "self", ".", "dictionary", ",", "self", ".", "output_dictionary", ",", "\n", "add_eos_for_other_targets", "=", "add_eos_for_other_targets", ",", "shuffle", "=", "True", ",", "\n", "targets", "=", "self", ".", "targets", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.build_dataset_for_inference": [[191, 213], ["fairseq.data.TransformEosDataset", "fairseq.data.MonolingualDataset", "fairseq.data.TokenBlockDataset", "language_modeling.LanguageModelingTask.source_dictionary.eos", "language_modeling.LanguageModelingTask.source_dictionary.pad", "language_modeling.LanguageModelingTask.source_dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "TransformEosDataset", "(", "\n", "MonolingualDataset", "(", "\n", "TokenBlockDataset", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "block_size", "=", "None", ",", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "'eos'", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", ",", "\n", "src_lengths", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "target_dictionary", ",", "\n", "add_eos_for_other_targets", "=", "False", ",", "\n", "shuffle", "=", "False", ",", "\n", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "# remove EOS since this will be used as a prefix for generation", "\n", "remove_eos_from_src", "=", "True", ",", "\n", "has_target", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.inference_step": [[215, 221], ["torch.no_grad", "generator.generate"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "prefix_tokens", "is", "None", ":", "\n", "# note: EOS has already been removed in build_dataset_for_inference", "\n", "                ", "prefix_tokens", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "\n", "", "return", "generator", ".", "generate", "(", "models", ",", "sample", ",", "prefix_tokens", "=", "prefix_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.source_dictionary": [[222, 227], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "self", ".", "output_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.language_modeling.LanguageModelingTask.target_dictionary": [[228, 233], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "self", ".", "output_dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_dictionary": [[75, 113], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.finalize", "fairseq.data.Dictionary.add_file_to_dictionary", "open", "open", "f_out.writelines", "enumerate"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_file_to_dictionary"], ["@", "classmethod", "\n", "def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "\n", "padding_factor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Build the dictionary from edit-labeled raw text inputs.\n\n        Each file contains tokenized sentences along with their token labels:\n        ```text\n        My teacher is going to move to change his job .\n        0 0 0 0 0 0 0 0 0 0 0\n        And he took in my favorite subject like soccer .\n        0 0 0 0 0 0 1 0 0 0\n        ...\n        ```\n        A dictionary is built using only the tokens and not token labels.\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"", "\n", "d", "=", "Dictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "# Write only tokens to a separate file.", "\n", "            ", "with", "open", "(", "filename", ")", "as", "f_in", ",", "open", "(", "f\"{filename}.tokens\"", ",", "\"w\"", ")", "as", "f_out", ":", "\n", "                ", "f_out", ".", "writelines", "(", "line", "for", "i", ",", "line", "in", "enumerate", "(", "f_in", ")", "\n", "if", "i", "%", "2", "==", "0", ")", "\n", "# Add tokens to dictionary with multiprocessing.", "\n", "", "Dictionary", ".", "add_file_to_dictionary", "(", "f\"{filename}.tokens\"", ",", "d", ",", "\n", "tokenizer", ".", "tokenize_line", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "\n", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.setup_task": [[114, 140], ["fairseq.options.eval_bool", "fairseq.options.eval_bool", "cls.load_dictionary", "cls.load_dictionary", "print", "print", "cls", "fairseq.data.data_utils.infer_language_pair", "Exception", "os.path.join", "os.path.join", "cls.load_dictionary.pad", "cls.load_dictionary.pad", "cls.load_dictionary.eos", "cls.load_dictionary.eos", "cls.load_dictionary.unk", "cls.load_dictionary.unk", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.fairseq_task.FairseqTask.load_dictionary", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.infer_language_pair", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup GEC task, including dictionary & model building.\"\"\"", "\n", "\n", "\"\"\"\n        Similar to the translation task, but also load labels dictionaries\n        \"\"\"", "\n", "args", ".", "left_pad_source", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_source", ")", "\n", "args", ".", "left_pad_target", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_target", ")", "\n", "\n", "# find language pair automatically", "\n", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "data_utils", ".", "infer_language_pair", "(", "args", ".", "data", "[", "0", "]", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Could not infer language pair, please provide it explicitly'", ")", "\n", "\n", "# load dictionaries", "\n", "", "src_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ")", "\n", "tgt_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ")", "\n", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "source_lang", ",", "len", "(", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "target_lang", ",", "len", "(", "tgt_dict", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.load_dataset": [[141, 206], ["enumerate", "fairseq.data.TokenLabeledLanguagePairDataset", "os.path.join", "fairseq.data.TokenLabeledIndexedRawTextDataset.exists", "fairseq.data.TokenLabeledIndexedRawTextDataset.exists", "itertools.count", "len", "len", "len", "fairseq.data.ConcatDataset", "fairseq.data.ConcatDataset", "print", "fairseq.data.TokenLabeledIndexedRawTextDataset", "gec.GECTask.load_dataset.split_exists"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "def", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "lang", ",", "data_path", ")", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ")", "\n", "return", "TokenLabeledIndexedRawTextDataset", ".", "exists", "(", "filename", ")", "\n", "\n", "", "def", "indexed_dataset", "(", "path", ",", "dictionary", ")", ":", "\n", "            ", "if", "TokenLabeledIndexedRawTextDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "print", "(", "f\"| {split} | loading token-labeled dataset from \"", "\n", "f\"raw text at {path}\"", ")", "\n", "return", "TokenLabeledIndexedRawTextDataset", "(", "path", ",", "dictionary", ")", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "\n", "", "", "src_datasets", "=", "[", "]", "\n", "tgt_datasets", "=", "[", "]", "\n", "\n", "data_paths", "=", "self", ".", "args", ".", "data", "\n", "\n", "for", "dk", ",", "data_path", "in", "enumerate", "(", "data_paths", ")", ":", "\n", "            ", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "                ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "\n", "# infer langcode", "\n", "src", ",", "tgt", "=", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", "\n", "if", "split_exists", "(", "split_k", ",", "src", ",", "tgt", ",", "src", ",", "data_path", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "src", ",", "tgt", ")", ")", "\n", "", "elif", "split_exists", "(", "split_k", ",", "tgt", ",", "src", ",", "src", ",", "data_path", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "tgt", ",", "src", ")", ")", "\n", "", "else", ":", "\n", "                    ", "if", "k", ">", "0", "or", "dk", ">", "0", ":", "\n", "                        ", "break", "\n", "", "else", ":", "\n", "                        ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "", "", "src_datasets", ".", "append", "(", "indexed_dataset", "(", "prefix", "+", "src", ",", "self", ".", "src_dict", ")", ")", "\n", "tgt_datasets", ".", "append", "(", "indexed_dataset", "(", "prefix", "+", "tgt", ",", "self", ".", "tgt_dict", ")", ")", "\n", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "len", "(", "src_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "assert", "len", "(", "src_datasets", ")", "==", "len", "(", "tgt_datasets", ")", "\n", "\n", "if", "len", "(", "src_datasets", ")", "==", "1", ":", "\n", "            ", "src_dataset", ",", "tgt_dataset", "=", "src_datasets", "[", "0", "]", ",", "tgt_datasets", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "sample_ratios", "=", "[", "1", "]", "*", "len", "(", "src_datasets", ")", "\n", "sample_ratios", "[", "0", "]", "=", "self", ".", "args", ".", "upsample_primary", "\n", "src_dataset", "=", "ConcatDataset", "(", "src_datasets", ",", "sample_ratios", ")", "\n", "tgt_dataset", "=", "ConcatDataset", "(", "tgt_datasets", ",", "sample_ratios", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "TokenLabeledLanguagePairDataset", "(", "\n", "src_dataset", ",", "src_dataset", ".", "sizes", ",", "self", ".", "src_dict", ",", "\n", "tgt_dataset", ",", "tgt_dataset", ".", "sizes", ",", "self", ".", "tgt_dict", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_dataset_for_inference": [[208, 211], ["fairseq.data.TokenLabeledLanguagePairDataset"], "methods", ["None"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "TokenLabeledLanguagePairDataset", "(", "\n", "src_tokens", ",", "src_lengths", ",", "self", ".", "source_dictionary", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.gec.GECTask.build_generator": [[213, 237], ["SequenceScorer", "SequenceCopyGenerator"], "methods", ["None"], ["", "def", "build_generator", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"Replace SequenceGenerator with SequenceCopyGenerator.\"\"\"", "\n", "if", "args", ".", "score_reference", ":", "\n", "            ", "from", "fairseq", ".", "sequence_scorer", "import", "SequenceScorer", "\n", "return", "SequenceScorer", "(", "self", ".", "target_dictionary", ")", "\n", "", "else", ":", "\n", "            ", "from", "fairseq", ".", "sequence_copygenerator", "import", "SequenceCopyGenerator", "\n", "return", "SequenceCopyGenerator", "(", "\n", "self", ".", "target_dictionary", ",", "\n", "beam_size", "=", "args", ".", "beam", ",", "\n", "max_len_a", "=", "args", ".", "max_len_a", ",", "\n", "max_len_b", "=", "args", ".", "max_len_b", ",", "\n", "min_len", "=", "args", ".", "min_len", ",", "\n", "stop_early", "=", "(", "not", "args", ".", "no_early_stop", ")", ",", "\n", "normalize_scores", "=", "(", "not", "args", ".", "unnormalized", ")", ",", "\n", "len_penalty", "=", "args", ".", "lenpen", ",", "\n", "unk_penalty", "=", "args", ".", "unkpen", ",", "\n", "sampling", "=", "args", ".", "sampling", ",", "\n", "sampling_topk", "=", "args", ".", "sampling_topk", ",", "\n", "sampling_temperature", "=", "args", ".", "sampling_temperature", ",", "\n", "diverse_beam_groups", "=", "args", ".", "diverse_beam_groups", ",", "\n", "diverse_beam_strength", "=", "args", ".", "diverse_beam_strength", ",", "\n", "match_source_len", "=", "args", ".", "match_source_len", ",", "\n", "no_repeat_ngram_size", "=", "args", ".", "no_repeat_ngram_size", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task": [[18, 20], ["TASK_REGISTRY[].setup_task"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.setup_task"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.register_task": [[22, 56], ["TASK_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.__init__.get_task": [[77, 79], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.add_args": [[59, 76], ["translation.TranslationTask.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "TranslationTask", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "'--method'", ",", "required", "=", "True", ",", "\n", "choices", "=", "[", "'sMoElp'", ",", "'sMoEup'", ",", "'hMoElp'", ",", "'hMoEup'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--num-experts'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "required", "=", "True", ",", "\n", "help", "=", "'number of experts'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use a simple mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network-dropout'", ",", "type", "=", "float", ",", "\n", "help", "=", "'dropout for mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network-encoder-dim'", ",", "type", "=", "float", ",", "\n", "help", "=", "'encoder output dim for mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--gen-expert'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'which expert to use for generation'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.__init__": [[78, 103], ["range", "translation.TranslationTask.__init__", "src_dict.add_symbol", "tgt_dict.add_symbol"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol"], ["", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "if", "args", ".", "method", "==", "'sMoElp'", ":", "\n", "# soft MoE with learned prior", "\n", "            ", "self", ".", "uniform_prior", "=", "False", "\n", "self", ".", "hard_selection", "=", "False", "\n", "", "elif", "args", ".", "method", "==", "'sMoEup'", ":", "\n", "# soft MoE with uniform prior", "\n", "            ", "self", ".", "uniform_prior", "=", "True", "\n", "self", ".", "hard_selection", "=", "False", "\n", "", "elif", "args", ".", "method", "==", "'hMoElp'", ":", "\n", "# hard MoE with learned prior", "\n", "            ", "self", ".", "uniform_prior", "=", "False", "\n", "self", ".", "hard_selection", "=", "True", "\n", "", "elif", "args", ".", "method", "==", "'hMoEup'", ":", "\n", "# hard MoE with uniform prior", "\n", "            ", "self", ".", "uniform_prior", "=", "True", "\n", "self", ".", "hard_selection", "=", "True", "\n", "\n", "# add indicator tokens for each expert", "\n", "", "for", "i", "in", "range", "(", "args", ".", "num_experts", ")", ":", "\n", "# add to both dictionaries in case we're sharing embeddings", "\n", "            ", "src_dict", ".", "add_symbol", "(", "'<expert_{}>'", ".", "format", "(", "i", ")", ")", "\n", "tgt_dict", ".", "add_symbol", "(", "'<expert_{}>'", ".", "format", "(", "i", ")", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.build_model": [[104, 133], ["models.build_model", "hasattr", "getattr", "getattr", "fairseq.modules.MeanPoolGatingNetwork", "ValueError", "getattr", "getattr", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "if", "not", "self", ".", "uniform_prior", "and", "not", "hasattr", "(", "model", ",", "'gating_network'", ")", ":", "\n", "            ", "if", "self", ".", "args", ".", "mean_pool_gating_network", ":", "\n", "                ", "if", "getattr", "(", "args", ",", "'mean_pool_gating_network_encoder_dim'", ",", "None", ")", ":", "\n", "                    ", "encoder_dim", "=", "args", ".", "mean_pool_gating_network_encoder_dim", "\n", "", "elif", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "None", ")", ":", "\n", "# assume that encoder_embed_dim is the encoder's output dimension", "\n", "                    ", "encoder_dim", "=", "args", ".", "encoder_embed_dim", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Must specify --mean-pool-gating-network-encoder-dim'", ")", "\n", "\n", "", "if", "getattr", "(", "args", ",", "'mean_pool_gating_network_dropout'", ",", "None", ")", ":", "\n", "                    ", "dropout", "=", "args", ".", "mean_pool_gating_network_dropout", "\n", "", "elif", "getattr", "(", "args", ",", "'dropout'", ",", "None", ")", ":", "\n", "                    ", "dropout", "=", "args", ".", "dropout", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Must specify --mean-pool-gating-network-dropout'", ")", "\n", "\n", "", "model", ".", "gating_network", "=", "modules", ".", "MeanPoolGatingNetwork", "(", "\n", "encoder_dim", ",", "args", ".", "num_experts", ",", "dropout", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'translation_moe task with learned prior requires the model to '", "\n", "'have a gating network; try using --mean-pool-gating-network'", "\n", ")", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.expert_index": [[134, 136], ["translation_moe.TranslationMoETask.tgt_dict.index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index"], ["", "def", "expert_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "i", "+", "self", ".", "tgt_dict", ".", "index", "(", "'<expert_0>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask._get_loss": [[137, 200], ["hasattr", "sample[].size", "loss.view.view.sum", "model.decoder", "criterion.compute_loss", "loss.view.view.view", "model.encoder", "translation_moe.eval", "translation_moe.TranslationMoETask._get_loss.get_lprob_yz"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.compute_loss", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "def", "_get_loss", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "assert", "hasattr", "(", "criterion", ",", "'compute_loss'", ")", ",", "'translation_moe task requires the criterion to implement the compute_loss() method'", "\n", "\n", "k", "=", "self", ".", "args", ".", "num_experts", "\n", "bsz", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "\n", "\n", "def", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", ":", "\n", "            ", "net_output", "=", "model", ".", "decoder", "(", "prev_output_tokens_k", ",", "encoder_out", ")", "\n", "loss", ",", "_", "=", "criterion", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "False", ")", "\n", "loss", "=", "loss", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "return", "-", "loss", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# -> B x 1", "\n", "\n", "", "def", "get_lprob_yz", "(", "winners", "=", "None", ")", ":", "\n", "            ", "encoder_out", "=", "model", ".", "encoder", "(", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ",", "sample", "[", "'net_input'", "]", "[", "'src_lengths'", "]", ")", "\n", "\n", "if", "winners", "is", "None", ":", "\n", "                ", "lprob_y", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "k", ")", ":", "\n", "                    ", "prev_output_tokens_k", "=", "sample", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", ".", "clone", "(", ")", "\n", "assert", "not", "prev_output_tokens_k", ".", "requires_grad", "\n", "prev_output_tokens_k", "[", ":", ",", "0", "]", "=", "self", ".", "expert_index", "(", "i", ")", "\n", "lprob_y", ".", "append", "(", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", ")", "\n", "", "lprob_y", "=", "torch", ".", "cat", "(", "lprob_y", ",", "dim", "=", "1", ")", "# -> B x K", "\n", "", "else", ":", "\n", "                ", "prev_output_tokens_k", "=", "sample", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", ".", "clone", "(", ")", "\n", "prev_output_tokens_k", "[", ":", ",", "0", "]", "=", "self", ".", "expert_index", "(", "winners", ")", "\n", "lprob_y", "=", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", "# -> B", "\n", "\n", "", "if", "self", ".", "uniform_prior", ":", "\n", "                ", "lprob_yz", "=", "lprob_y", "\n", "", "else", ":", "\n", "                ", "lprob_z", "=", "model", ".", "gating_network", "(", "encoder_out", ")", "# B x K", "\n", "if", "winners", "is", "not", "None", ":", "\n", "                    ", "lprob_z", "=", "lprob_z", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "winners", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "", "lprob_yz", "=", "lprob_y", "+", "lprob_z", ".", "type_as", "(", "lprob_y", ")", "# B x K", "\n", "\n", "", "return", "lprob_yz", "\n", "\n", "# compute responsibilities without dropout", "\n", "", "with", "eval", "(", "model", ")", ":", "# disable dropout", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "# disable autograd", "\n", "                ", "lprob_yz", "=", "get_lprob_yz", "(", ")", "# B x K", "\n", "prob_z_xy", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "lprob_yz", ",", "dim", "=", "1", ")", "\n", "", "", "assert", "not", "prob_z_xy", ".", "requires_grad", "\n", "\n", "# compute loss with dropout", "\n", "if", "self", ".", "hard_selection", ":", "\n", "            ", "winners", "=", "prob_z_xy", ".", "max", "(", "dim", "=", "1", ")", "[", "1", "]", "\n", "loss", "=", "-", "get_lprob_yz", "(", "winners", ")", "\n", "", "else", ":", "\n", "            ", "lprob_yz", "=", "get_lprob_yz", "(", ")", "# B x K", "\n", "loss", "=", "-", "modules", ".", "LogSumExpMoE", ".", "apply", "(", "lprob_yz", ",", "prob_z_xy", ",", "1", ")", "\n", "\n", "", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "'posterior'", ":", "prob_z_xy", ".", "float", "(", ")", ".", "sum", "(", "dim", "=", "0", ")", ".", "cpu", "(", ")", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.train_step": [[201, 208], ["model.train", "translation_moe.TranslationMoETask._get_loss", "optimizer.backward"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask._get_loss", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.scalar_bias.ScalarBias.backward"], ["", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "_get_loss", "(", "sample", ",", "model", ",", "criterion", ")", "\n", "if", "ignore_grad", ":", "\n", "            ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.valid_step": [[209, 214], ["model.eval", "torch.no_grad", "translation_moe.TranslationMoETask._get_loss"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask._get_loss"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "_get_loss", "(", "sample", ",", "model", ",", "criterion", ")", "\n", "", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.inference_step": [[215, 222], ["torch.no_grad", "generator.generate", "translation_moe.TranslationMoETask.expert_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.expert_index"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "generator", ".", "generate", "(", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "prefix_tokens", ",", "\n", "bos_token", "=", "self", ".", "expert_index", "(", "self", ".", "args", ".", "gen_expert", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.TranslationMoETask.aggregate_logging_outputs": [[224, 230], ["criterion.__class__.aggregate_logging_outputs", "sum"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.aggregate_logging_outputs"], ["", "", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "        ", "agg_logging_outputs", "=", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "agg_logging_outputs", "[", "'posterior'", "]", "=", "sum", "(", "\n", "log", "[", "'posterior'", "]", "for", "log", "in", "logging_outputs", "if", "'posterior'", "in", "log", "\n", ")", "\n", "return", "agg_logging_outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval": [[26, 32], ["model.eval", "model.train"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.train.train"], ["@", "contextlib", ".", "contextmanager", "\n", "def", "eval", "(", "model", ")", ":", "\n", "    ", "is_training", "=", "model", ".", "training", "\n", "model", ".", "eval", "(", ")", "\n", "yield", "\n", "model", ".", "train", "(", "is_training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvModelSelfAtt.__init__": [[29, 40], ["FairseqModel.__init__", "sum", "CompositeEncoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "pretrained_encoder", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "self", ".", "encoder", ".", "num_attention_layers", "=", "sum", "(", "layer", "is", "not", "None", "for", "layer", "in", "decoder", ".", "attention", ")", "\n", "self", ".", "pretrained_encoder", "=", "pretrained_encoder", "\n", "if", "self", ".", "pretrained_encoder", "is", "None", ":", "\n", "            ", "encoders", "=", "{", "'encoder'", ":", "encoder", "}", "\n", "", "else", ":", "\n", "            ", "encoders", "=", "{", "'encoder'", ":", "encoder", ",", "'pretrained'", ":", "self", ".", "pretrained_encoder", "}", "\n", "# for fusion model, CompositeEncoder contains both pretrained and training encoders", "\n", "# these are forwarded and then combined in the decoder", "\n", "", "self", ".", "encoder", "=", "CompositeEncoder", "(", "encoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvModelSelfAtt.add_args": [[41, 79], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--self-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder self-attention layers, ex: [True] + [False]*5'", ")", "\n", "parser", ".", "add_argument", "(", "'--multihead-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--multihead-self-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in self-attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in encoder attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--project-input'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use projections in self-attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--gated-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use GLU layers in self-attention projections [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--downsample'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use downsampling in self-attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-checkpoint'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to load checkpoint from pretrained model'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'use pretrained model when training [True, ...]'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvModelSelfAtt.build_model": [[81, 132], ["eval", "fconv_self_att.FConvModelSelfAtt.FConvEncoder", "fconv_self_att.FConvModelSelfAtt.FConvDecoder", "fconv_self_att.FConvModelSelfAtt.FConvModelSelfAtt", "print", "trained_decoder.parameters", "trained_encoder.parameters", "list", "list", "eval", "eval", "eval", "eval", "eval", "eval", "eval", "eval", "fairseq.utils.load_ensemble_for_inference", "trained_model.children", "trained_model.children"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_ensemble_for_inference"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "trained_encoder", ",", "trained_decoder", "=", "None", ",", "None", "\n", "pretrained", "=", "eval", "(", "args", ".", "pretrained", ")", "\n", "if", "pretrained", ":", "\n", "            ", "print", "(", "\"| loading pretrained model\"", ")", "\n", "trained_model", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "# not actually for inference, but loads pretrained model parameters", "\n", "filenames", "=", "[", "args", ".", "pretrained_checkpoint", "]", ",", "\n", "task", "=", "task", ",", "\n", ")", "[", "0", "]", "[", "0", "]", "\n", "trained_decoder", "=", "list", "(", "trained_model", ".", "children", "(", ")", ")", "[", "1", "]", "\n", "trained_encoder", "=", "list", "(", "trained_model", ".", "children", "(", ")", ")", "[", "0", "]", "\n", "\n", "# freeze pretrained model", "\n", "for", "param", "in", "trained_decoder", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "param", "in", "trained_encoder", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "\"\"\"Build a new model instance.\"\"\"", "\n", "encoder", "=", "FConvEncoder", "(", "\n", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "encoder_layers", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_source_positions", ",", "\n", "attention", "=", "eval", "(", "args", ".", "encoder_attention", ")", ",", "\n", "attention_nheads", "=", "args", ".", "encoder_attention_nheads", "\n", ")", "\n", "\n", "decoder", "=", "FConvDecoder", "(", "\n", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_target_positions", ",", "\n", "selfattention", "=", "eval", "(", "args", ".", "self_attention", ")", ",", "\n", "attention_nheads", "=", "args", ".", "multihead_attention_nheads", ",", "\n", "selfattention_nheads", "=", "args", ".", "multihead_self_attention_nheads", ",", "\n", "project_input", "=", "eval", "(", "args", ".", "project_input", ")", ",", "\n", "gated_attention", "=", "eval", "(", "args", ".", "gated_attention", ")", ",", "\n", "downsample", "=", "eval", "(", "args", ".", "downsample", ")", ",", "\n", "pretrained", "=", "pretrained", ",", "\n", "trained_decoder", "=", "trained_decoder", "\n", ")", "\n", "model", "=", "FConvModelSelfAtt", "(", "encoder", ",", "decoder", ",", "trained_encoder", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvModelSelfAtt.pretrained": [[133, 136], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "pretrained", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pretrained_encoder", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvEncoder.__init__": [[140, 188], ["FairseqEncoder.__init__", "len", "dictionary.pad", "fconv_self_att.Embedding", "fconv_self_att.PositionalEmbedding", "fconv_self_att.FConvEncoder.FConvEncoder.__init__.expand_bool_array"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "dropout", "=", "0.1", ",", "attention", "=", "False", ",", "\n", "attention_nheads", "=", "1", ",", "left_pad", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "num_attention_layers", "=", "None", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", "left_pad", "=", "self", ".", "left_pad", ",", "\n", ")", "\n", "\n", "def", "expand_bool_array", "(", "val", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "                ", "return", "[", "val", "]", "*", "len", "(", "convolutions", ")", "\n", "", "return", "val", "\n", "\n", "", "attention", "=", "expand_bool_array", "(", "attention", ")", "\n", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attproj", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "\n", "Linear", "(", "in_channels", ",", "out_channels", ")", "if", "in_channels", "!=", "out_channels", "else", "None", "\n", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "ConvTBC", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "\n", "self", ".", "attention", ".", "append", "(", "\n", "SelfAttention", "(", "out_channels", ",", "embed_dim", ",", "attention_nheads", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvEncoder.forward": [[189, 241], ["torch.dropout", "torch.dropout", "torch.dropout", "attention.transpose", "fconv_self_att.FConvEncoder.FConvEncoder.fc1", "src_tokens.eq().t", "attention.transpose", "zip", "attention.transpose", "fconv_self_att.FConvEncoder.FConvEncoder.fc2", "fairseq.modules.GradMultiply.apply", "fconv_self_att.FConvEncoder.FConvEncoder.embed_tokens", "fconv_self_att.FConvEncoder.FConvEncoder.embed_positions", "encoder_padding_mask.t.t.any", "torch.dropout", "torch.dropout", "torch.dropout", "torch.pad", "torch.pad", "torch.pad", "conv", "torch.glu", "torch.glu", "torch.glu", "encoder_padding_mask.t.t.t", "attention.masked_fill", "math.sqrt", "src_tokens.eq", "proj", "attention.masked_fill", "attention", "math.sqrt", "encoder_padding_mask.t.t.unsqueeze", "attention.transpose.transpose", "encoder_padding_mask.t.t.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "# embed tokens and positions", "\n", "        ", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "+", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "input_embedding", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "# -> T x B", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# temporal convolutions", "\n", "for", "proj", ",", "conv", ",", "attention", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "                ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "padding_l", "=", "(", "conv", ".", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", "\n", "padding_r", "=", "conv", ".", "kernel_size", "[", "0", "]", "//", "2", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "padding_r", ")", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "x", "=", "attention", "(", "x", ")", "\n", "", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# project back to size of embedding", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_padding_mask", "=", "encoder_padding_mask", ".", "t", "(", ")", "# -> B x T", "\n", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "# scale gradients (this only affects backward, not forward)", "\n", "", "x", "=", "GradMultiply", ".", "apply", "(", "x", ",", "1.0", "/", "(", "2.0", "*", "self", ".", "num_attention_layers", ")", ")", "\n", "\n", "# add output to input embedding for attention", "\n", "y", "=", "(", "x", "+", "input_embedding", ".", "transpose", "(", "0", ",", "1", ")", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "y", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvEncoder.reorder_encoder_out": [[243, 259], ["tuple", "encoder_out[].index_select", "tuple", "eo.index_select", "eo.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "encoder_out", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "for", "eo", "in", "encoder_out", "[", "'encoder_out'", "]", "\n", ")", "\n", "\n", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "", "if", "'pretrained'", "in", "encoder_out", ":", "\n", "            ", "encoder_out", "[", "'pretrained'", "]", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "for", "eo", "in", "encoder_out", "[", "'pretrained'", "]", "[", "'encoder_out'", "]", "\n", ")", "\n", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvEncoder.max_positions": [[260, 263], ["fconv_self_att.FConvEncoder.FConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvDecoder.__init__": [[267, 373], ["FairseqDecoder.__init__", "fconv_self_att.FConvDecoder.FConvDecoder.register_buffer", "fconv_self_att.FConvDecoder.FConvDecoder.__init__.expand_bool_array"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "out_embed_dim", "=", "256", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "8", ",", "attention", "=", "True", ",", "dropout", "=", "0.1", ",", "\n", "selfattention", "=", "False", ",", "attention_nheads", "=", "1", ",", "selfattention_nheads", "=", "1", ",", "\n", "project_input", "=", "False", ",", "gated_attention", "=", "False", ",", "downsample", "=", "False", ",", "\n", "pretrained", "=", "False", ",", "trained_decoder", "=", "None", ",", "left_pad", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "pretrained_decoder", "=", "trained_decoder", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "need_attn", "=", "True", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "\n", "def", "expand_bool_array", "(", "val", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "                ", "return", "[", "val", "]", "*", "len", "(", "convolutions", ")", "\n", "", "return", "val", "\n", "\n", "", "attention", "=", "expand_bool_array", "(", "attention", ")", "\n", "selfattention", "=", "expand_bool_array", "(", "selfattention", ")", "\n", "\n", "if", "not", "isinstance", "(", "attention", ",", "list", ")", "or", "len", "(", "attention", ")", "!=", "len", "(", "convolutions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Attention is expected to be a list of booleans of '", "\n", "'length equal to the number of layers.'", ")", "\n", "\n", "", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", "left_pad", "=", "self", ".", "left_pad", ",", "\n", ")", "\n", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "selfattention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attproj", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "\n", "Linear", "(", "in_channels", ",", "out_channels", ")", "if", "in_channels", "!=", "out_channels", "else", "None", "\n", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "LinearizedConv1d", "(", "\n", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", ",", "dropout", "=", "dropout", ",", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "attention", ".", "append", "(", "\n", "DownsampledMultiHeadAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "attention_nheads", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "\n", "self", ".", "attproj", ".", "append", "(", "\n", "Linear", "(", "out_channels", ",", "embed_dim", ",", "dropout", "=", "dropout", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "self", ".", "selfattention", ".", "append", "(", "\n", "SelfAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "selfattention_nheads", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "gated_attention", ",", "\n", "downsample", "=", "downsample", ",", "\n", ")", "if", "selfattention", "[", "i", "]", "else", "None", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "out_embed_dim", ")", "\n", "self", ".", "fc3", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout", ")", "\n", "\n", "# model fusion", "\n", "if", "self", ".", "pretrained", ":", "\n", "# independent gates are learned from the concatenated input", "\n", "            ", "self", ".", "gate1", "=", "nn", ".", "Sequential", "(", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "gate2", "=", "nn", ".", "Sequential", "(", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "# pretrained and trained models are joined", "\n", "self", ".", "joining", "=", "nn", ".", "Sequential", "(", "\n", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_embed_dim", ",", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_embed_dim", ",", "out_embed_dim", ")", ",", "\n", "nn", ".", "LayerNorm", "(", "out_embed_dim", ")", "\n", ")", "\n", "# pretrained model contains an output layer that is nhid -> vocab size", "\n", "# but the models are combined in their hidden state", "\n", "# the hook stores the output of the pretrained model forward", "\n", "self", ".", "pretrained_outputs", "=", "{", "}", "\n", "\n", "def", "save_output", "(", ")", ":", "\n", "                ", "def", "hook", "(", "a", ",", "b", ",", "output", ")", ":", "\n", "                    ", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "=", "output", "\n", "", "return", "hook", "\n", "\n", "", "self", ".", "pretrained_decoder", ".", "fc2", ".", "register_forward_hook", "(", "save_output", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvDecoder.forward": [[374, 444], ["fconv_self_att.FConvDecoder.FConvDecoder._split_encoder_out", "fconv_self_att.FConvDecoder.FConvDecoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "fconv_self_att.FConvDecoder.FConvDecoder.fc1", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "zip", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "fconv_self_att.FConvDecoder.FConvDecoder.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "fconv_self_att.FConvDecoder.FConvDecoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "conv", "torch.glu", "torch.glu", "torch.glu", "fconv_self_att.FConvDecoder.FConvDecoder.fc3", "fconv_self_att.FConvDecoder.FConvDecoder.pretrained_decoder.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fconv_self_att.FConvDecoder.FConvDecoder.gate1", "fconv_self_att.FConvDecoder.FConvDecoder.gate2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fconv_self_att.FConvDecoder.FConvDecoder.joining", "fconv_self_att.FConvDecoder.FConvDecoder.fc3", "proj", "attention", "fconv_self_att.FConvDecoder.FConvDecoder.", "math.sqrt", "attproj", "avg_attn_scores.add_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._split_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.forward"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out_dict", ")", ":", "\n", "        ", "encoder_out", "=", "encoder_out_dict", "[", "'encoder'", "]", "[", "'encoder_out'", "]", "\n", "trained_encoder_out", "=", "encoder_out_dict", "[", "'pretrained'", "]", "if", "self", ".", "pretrained", "else", "None", "\n", "\n", "encoder_a", ",", "encoder_b", "=", "self", ".", "_split_encoder_out", "(", "encoder_out", ")", "\n", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "prev_output_tokens", ")", "\n", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "+", "positions", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "target_embedding", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# temporal convolutions", "\n", "avg_attn_scores", "=", "None", "\n", "for", "proj", ",", "conv", ",", "attention", ",", "selfattention", ",", "attproj", "in", "zip", "(", "\n", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ",", "self", ".", "selfattention", ",", "self", ".", "attproj", "\n", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "# attention", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "r", "=", "x", "\n", "x", ",", "attn_scores", "=", "attention", "(", "attproj", "(", "x", ")", "+", "target_embedding", ",", "encoder_a", ",", "encoder_b", ")", "\n", "x", "=", "x", "+", "r", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "                    ", "if", "avg_attn_scores", "is", "None", ":", "\n", "                        ", "avg_attn_scores", "=", "attn_scores", "\n", "", "else", ":", "\n", "                        ", "avg_attn_scores", ".", "add_", "(", "attn_scores", ")", "\n", "\n", "", "", "", "if", "selfattention", "is", "not", "None", ":", "\n", "                ", "x", "=", "selfattention", "(", "x", ")", "\n", "\n", "", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project back to size of vocabulary", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "not", "self", ".", "pretrained", ":", "\n", "            ", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n", "# fusion gating", "\n", "", "if", "self", ".", "pretrained", ":", "\n", "            ", "trained_x", ",", "_", "=", "self", ".", "pretrained_decoder", ".", "forward", "(", "prev_output_tokens", ",", "trained_encoder_out", ")", "\n", "y", "=", "torch", ".", "cat", "(", "[", "x", ",", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "gate1", "=", "self", ".", "gate1", "(", "y", ")", "\n", "gate2", "=", "self", ".", "gate2", "(", "y", ")", "\n", "gated_x1", "=", "gate1", "*", "x", "\n", "gated_x2", "=", "gate2", "*", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "\n", "fusion", "=", "torch", ".", "cat", "(", "[", "gated_x1", ",", "gated_x2", "]", ",", "dim", "=", "-", "1", ")", "\n", "fusion", "=", "self", ".", "joining", "(", "fusion", ")", "\n", "fusion_output", "=", "self", ".", "fc3", "(", "fusion", ")", "\n", "return", "fusion_output", ",", "avg_attn_scores", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "avg_attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvDecoder.max_positions": [[445, 448], ["fconv_self_att.FConvDecoder.FConvDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvDecoder.make_generation_fast_": [[449, 451], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.FConvDecoder._split_encoder_out": [[452, 460], ["encoder_a.transpose().contiguous.transpose().contiguous.transpose().contiguous", "encoder_b.transpose().contiguous.transpose().contiguous.transpose().contiguous", "encoder_a.transpose().contiguous.transpose().contiguous.transpose", "encoder_b.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["None"], ["", "def", "_split_encoder_out", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"Split and transpose encoder outputs.\"\"\"", "\n", "# transpose only once to speed up attention layers", "\n", "encoder_a", ",", "encoder_b", "=", "encoder_out", "\n", "encoder_a", "=", "encoder_a", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "encoder_b", "=", "encoder_b", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "result", "=", "(", "encoder_a", ",", "encoder_b", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.SelfAttention.__init__": [[464, 474], ["torch.Module.__init__", "fairseq.modules.DownsampledMultiHeadAttention", "fconv_self_att.Linear", "fconv_self_att.Linear", "fconv_self_att.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm"], ["    ", "def", "__init__", "(", "self", ",", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "project_input", "=", "False", ",", "gated", "=", "False", ",", "downsample", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "DownsampledMultiHeadAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0", ",", "bias", "=", "True", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "gated", ",", "downsample", "=", "downsample", ",", "\n", ")", "\n", "self", ".", "in_proj_q", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "in_proj_k", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "in_proj_v", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "ln", "=", "nn", ".", "LayerNorm", "(", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.SelfAttention.forward": [[475, 482], ["fconv_self_att.SelfAttention.SelfAttention.in_proj_q", "fconv_self_att.SelfAttention.SelfAttention.in_proj_k", "fconv_self_att.SelfAttention.SelfAttention.in_proj_v", "fconv_self_att.SelfAttention.SelfAttention.attention", "fconv_self_att.SelfAttention.SelfAttention.ln"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.multihead_attention.MultiheadAttention.in_proj_v"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "query", "=", "self", ".", "in_proj_q", "(", "x", ")", "\n", "key", "=", "self", ".", "in_proj_k", "(", "x", ")", "\n", "value", "=", "self", ".", "in_proj_v", "(", "x", ")", "\n", "x", ",", "_", "=", "self", ".", "attention", "(", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "True", ",", "use_scalar_bias", "=", "True", ")", "\n", "return", "self", ".", "ln", "(", "x", "+", "residual", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.Embedding": [[484, 488], ["torch.Embedding", "nn.Embedding.weight.data.normal_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.PositionalEmbedding": [[490, 494], ["fairseq.modules.LearnedPositionalEmbedding", "fairseq.modules.LearnedPositionalEmbedding.weight.data.normal_"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", ":", "\n", "    ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.Linear": [[496, 502], ["torch.Linear", "nn.Linear.weight.data.normal_", "nn.Linear.bias.data.zero_", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.LinearizedConv1d": [[504, 511], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "fairseq.modules.LinearizedConvolution.weight.data.normal_", "fairseq.modules.LinearizedConvolution.bias.data.zero_"], "function", ["None"], ["", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.ConvTBC": [[513, 521], ["fconv_self_att.ConvTBC", "math.sqrt", "ConvTBC.weight.data.normal_", "ConvTBC.bias.data.zero_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.ConvTBC"], ["", "def", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer\"\"\"", "\n", "from", "fairseq", ".", "modules", "import", "ConvTBC", "\n", "m", "=", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.base_architecture": [[523, 542], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_self_att'", ",", "'fconv_self_att'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(512, 3)] * 3'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 3)] * 8'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'True'", ")", "\n", "args", ".", "self_attention", "=", "getattr", "(", "args", ",", "'self_attention'", ",", "'False'", ")", "\n", "args", ".", "encoder_attention", "=", "getattr", "(", "args", ",", "'encoder_attention'", ",", "'False'", ")", "\n", "args", ".", "multihead_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_attention_nheads'", ",", "1", ")", "\n", "args", ".", "multihead_self_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_self_attention_nheads'", ",", "1", ")", "\n", "args", ".", "encoder_attention_nheads", "=", "getattr", "(", "args", ",", "'encoder_attention_nheads'", ",", "1", ")", "\n", "args", ".", "project_input", "=", "getattr", "(", "args", ",", "'project_input'", ",", "'False'", ")", "\n", "args", ".", "gated_attention", "=", "getattr", "(", "args", ",", "'gated_attention'", ",", "'False'", ")", "\n", "args", ".", "downsample", "=", "getattr", "(", "args", ",", "'downsample'", ",", "'False'", ")", "\n", "args", ".", "pretrained_checkpoint", "=", "getattr", "(", "args", ",", "'pretrained_checkpoint'", ",", "''", ")", "\n", "args", ".", "pretrained", "=", "getattr", "(", "args", ",", "'pretrained'", ",", "'False'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv_self_att.fconv_self_att_wp": [[544, 557], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv_self_att.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_self_att'", ",", "'fconv_self_att_wp'", ")", "\n", "def", "fconv_self_att_wp", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(128, 3)] * 2 + [(512,3)] * 1'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 4)] * 4 + [(768, 4)] * 2 + [(1024, 4)] * 1'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "self_attention", "=", "getattr", "(", "args", ",", "'self_attention'", ",", "'True'", ")", "\n", "args", ".", "multihead_self_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_self_attention_nheads'", ",", "4", ")", "\n", "args", ".", "project_input", "=", "getattr", "(", "args", ",", "'project_input'", ",", "'True'", ")", "\n", "args", ".", "gated_attention", "=", "getattr", "(", "args", ",", "'gated_attention'", ",", "'True'", ")", "\n", "args", ".", "downsample", "=", "getattr", "(", "args", ",", "'downsample'", ",", "'True'", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_encoder.FairseqEncoder.__init__": [[14, 17], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_encoder.FairseqEncoder.forward": [[18, 27], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_encoder.FairseqEncoder.reorder_encoder_out": [[28, 40], ["None"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to `new_order`.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            `encoder_out` rearranged according to `new_order`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_encoder.FairseqEncoder.max_positions": [[41, 44], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "1e6", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_encoder.FairseqEncoder.upgrade_state_dict": [[45, 48], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.__init__": [[30, 32], ["FairseqDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.forward": [[33, 51], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.reorder_incremental_state": [[52, 68], ["set", "fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "set.add", "module.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder incremental state.\n\n        This should be called when the order of the input has changed from the\n        previous time step. A typical use case is beam search, where the input\n        order changes between time steps based on the selection of beams.\n        \"\"\"", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_reorder_incremental_state", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'reorder_incremental_state'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_reorder_incremental_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.set_beam_size": [[69, 82], ["getattr", "set", "fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "set.add", "module.set_beam_size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.set_beam_size"], ["", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\"Sets the beam size in the decoder and all children.\"\"\"", "\n", "if", "getattr", "(", "self", ",", "'_beam_size'", ",", "-", "1", ")", "!=", "beam_size", ":", "\n", "            ", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_set_beam_size", "(", "module", ")", ":", "\n", "                ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'set_beam_size'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                    ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "set_beam_size", "(", "beam_size", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_set_beam_size", ")", "\n", "self", ".", "_beam_size", "=", "beam_size", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELModel.__init__": [[109, 111], ["FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELModel.add_args": [[112, 186], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "\n", "\"\"\"\n        MODIFIED: additional command line options\n        \"\"\"", "\n", "parser", ".", "add_argument", "(", "'--use-copy-scores'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'copy scores are calculated and '", "\n", "'added to decoder softmax outputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--predict-edit-labels'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'encoder also predicts whether '", "\n", "'each source token should be edited'", ")", "\n", "parser", ".", "add_argument", "(", "'--decode-with-edit-labels'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'decoder uses edit labels through \"gating\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--copy-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of heads for copy scores attention '", "\n", "'(0: same number as decoder_attention_heads)'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha-warmup'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'gradually allow alpha (copying ratio) to'", "\n", "'increase for the first N steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--pad-copied-words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'zero out the probability of a word under '", "\n", "'the generative distribution, if it is '", "\n", "'a source word that could be copied'", ")", "\n", "\n", "# Default Transformer hyperparameters", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELModel.build_model": [[188, 253], ["copy_augmented_transformer_el.base_architecture", "encoder_module", "decoder_module", "copy_augmented_transformer_el.CopyAugmentedTransformerELModel", "hasattr", "hasattr", "len", "dictionary.pad", "transformer.Embedding", "copy_augmented_transformer_el.CopyAugmentedTransformerELModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "\"\"\"\n        MODIFIED: \n        Optionally use encoder with edit labels or a copy-augmented decoder.\n        \n        Also raise errors for invalid argument combinations.\n        \"\"\"", "\n", "if", "not", "(", "args", ".", "use_copy_scores", "or", "args", ".", "predict_edit_labels", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Either use_copy_scores or predict_edit_labels\"", "\n", "\"must be True. Otherwise just use -a transformer.\"", ")", "\n", "", "if", "not", "args", ".", "predict_edit_labels", "and", "args", ".", "decode_with_edit_labels", ":", "\n", "            ", "raise", "ValueError", "(", "\"decode_with_edit_labels cannot be True \"", "\n", "\"if predict_edit_labels is False\"", ")", "\n", "\n", "", "encoder_module", "=", "TransformerELEncoder", "\n", "decoder_module", "=", "CopyAugmentedTransformerELDecoder", "\n", "\n", "encoder", "=", "encoder_module", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "decoder_module", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ")", "\n", "return", "CopyAugmentedTransformerELModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.TransformerELEncoder.__init__": [[270, 304], ["FairseqEncoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "copy_augmented_transformer_el.TransformerELEncoder.layers.extend", "copy_augmented_transformer_el.TransformerELEncoder.register_buffer", "transformer.PositionalEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "transformer.LayerNorm", "transformer.Linear", "transformer.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "left_pad", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerEncoderLayer", "(", "args", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "encoder_normalize_before", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "", "\"\"\"\n        MODIFIED: a readout layer for edit label prediction\n        \"\"\"", "\n", "self", ".", "predict_edit_labels", "=", "args", ".", "predict_edit_labels", "\n", "if", "self", ".", "predict_edit_labels", ":", "\n", "            ", "self", ".", "edit_label_layer", "=", "Linear", "(", "embed_dim", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "edit_label_layer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.TransformerELEncoder.forward": [[305, 356], ["torch.dropout", "torch.dropout", "torch.dropout", "copy_augmented_transformer_el.TransformerELEncoder.transpose", "src_tokens.eq", "copy_augmented_transformer_el.TransformerELEncoder.embed_tokens", "copy_augmented_transformer_el.TransformerELEncoder.embed_positions", "src_tokens.eq.any", "layer", "copy_augmented_transformer_el.TransformerELEncoder.layer_norm", "copy_augmented_transformer_el.TransformerELEncoder.edit_label_layer"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n                - **encoder_edit_logits** (Tensor): output edit labels per token\n                  in logits\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "encoder_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "\"\"\"\n        MODIFIED: edit labels (represented as logits)\n        \"\"\"", "\n", "if", "self", ".", "predict_edit_labels", ":", "\n", "            ", "edit_logits", "=", "self", ".", "edit_label_layer", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "edit_logits", "=", "None", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "'encoder_edit_logits'", ":", "edit_logits", ",", "# T x B x 1", "\n", "'encoder_input_tokens'", ":", "src_tokens", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.TransformerELEncoder.reorder_encoder_out": [[358, 385], ["encoder_out[].index_select", "encoder_out[].index_select", "encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "\"\"\"\n        MODIFIED: also reorder edit scores and input tokens, just in case\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_edit_logits'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_edit_logits'", "]", "=", "encoder_out", "[", "'encoder_edit_logits'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_input_tokens'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_input_tokens'", "]", "=", "encoder_out", "[", "'encoder_input_tokens'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.TransformerELEncoder.max_positions": [[386, 391], ["min", "copy_augmented_transformer_el.TransformerELEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.TransformerELEncoder.upgrade_state_dict_named": [[392, 406], ["isinstance", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "", "version_key", "=", "'{}.version'", ".", "format", "(", "name", ")", "\n", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "version_key", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "version_key", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.__init__": [[425, 512], ["FairseqIncrementalDecoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.layers.extend", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.register_buffer", "transformer.Linear", "transformer.PositionalEmbedding", "fairseq.modules.MultiheadAttention", "transformer.Linear", "transformer.Linear", "fairseq.modules.AdaptiveSoftmax", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "transformer.LayerNorm", "transformer.TransformerDecoderLayer", "len", "fairseq.options.eval_str_list", "NotImplementedError", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ",", "\n", "left_pad", "=", "False", ",", "final_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "# todo: try with input_embed_dim", "\n", "\n", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerDecoderLayer", "(", "args", ",", "no_encoder_attn", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "\"\"\"\n        MODIFIED: add copying mechanism as a separate multi-head attention\n        \"\"\"", "\n", "if", "args", ".", "use_copy_scores", ":", "\n", "            ", "assert", "not", "no_encoder_attn", ",", "\"copy scores cannot be computed if \"", "\"there is no encoder-decoder attention\"", "\n", "# Number of heads in copy attention layer is an optional argument", "\n", "self", ".", "copy_attention_heads", "=", "(", "args", ".", "decoder_attention_heads", "\n", "if", "args", ".", "copy_attention_heads", "==", "0", "\n", "else", "args", ".", "copy_attention_heads", ")", "\n", "self", ".", "copy_attention", "=", "MultiheadAttention", "(", "\n", "embed_dim", ",", "self", ".", "copy_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "copy_balancing_layer", "=", "Linear", "(", "input_embed_dim", ",", "1", ")", "\n", "if", "args", ".", "decode_with_edit_labels", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "copy_attention", "=", "None", "\n", "self", ".", "copy_balancing_layer", "=", "None", "\n", "\n", "# Alpha scheduler & diagnostic checker", "\n", "", "self", ".", "alpha_warmup", "=", "args", ".", "alpha_warmup", "\n", "self", ".", "num_batches", "=", "0", "\n", "self", ".", "num_copies", "=", "0", "\n", "self", ".", "mean_alpha", "=", "0.0", "\n", "\n", "# Zero out generative probability of a word if also in source sentence", "\n", "self", ".", "pad_copied_words", "=", "args", ".", "pad_copied_words", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "args", ".", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "\n", "len", "(", "dictionary", ")", ",", "\n", "output_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", ",", "\n", "dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", "adaptive_inputs", "=", "embed_tokens", "if", "args", ".", "tie_adaptive_weights", "else", "None", ",", "\n", "factor", "=", "args", ".", "adaptive_softmax_factor", ",", "\n", "tie_proj", "=", "args", ".", "tie_adaptive_proj", ",", "\n", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "\"\"\"\n            MODIFIED: require share_input_output_embed for copying mechanism\n            \"\"\"", "\n", "raise", "NotImplementedError", "(", "\n", "\"copying mechanism requires share_input_output_embed\"", "\n", ")", "\n", "", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "decoder_normalize_before", "and", "final_norm", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.forward": [[513, 672], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.log.transpose", "torch.log.transpose", "torch.log.transpose", "torch.log.transpose", "torch.log.transpose", "torch.log.transpose", "torch.log.size", "torch.log.size", "torch.log.size", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.embed_positions", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.embed_tokens", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.project_in_dim", "layer", "inner_states.append", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.layer_norm", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.copy_attention", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "torch.sigmoid().transpose", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.project_out_dim", "torch.linear", "torch.linear", "torch.linear", "copy_augmented_transformer_el.compute_copy_probs", "torch.clamp.expand", "torch.clamp.expand", "torch.clamp.expand", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.clamp.mean().item", "torch.clamp.mean().item", "torch.clamp.mean().item", "gen_logits.size", "gen_logits.masked_fill_", "min", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "print", "print", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.clamp.mean", "torch.clamp.mean", "torch.clamp.mean", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.buffered_future_mask", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.copy_balancing_layer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.compute_copy_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.buffered_future_mask"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", ",", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "\"\"\"\n        MODIFIED: combine with copy attention\n        \n        _Compared to the paper, we take the output of the attention layer to\n        compute the copy-generate balancing factor (\"alpha\"). \n        \n        shapes:\n            x (Q): T_dec x B x C\n            copy_scores: B x T_dec x T_enc\n            alphas: B x T_dec x 1\n        \"\"\"", "\n", "if", "self", ".", "copy_attention", "is", "not", "None", ":", "\n", "            ", "assert", "encoder_out", "is", "not", "None", "\n", "# attn_output: T_dec x B x C, copy_scores: B x T_dec x T_enc", "\n", "attn_output", ",", "copy_scores", "=", "self", ".", "copy_attention", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", "[", "'encoder_out'", "]", ",", "\n", "value", "=", "encoder_out", "[", "'encoder_out'", "]", ",", "\n", "key_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ",", "\n", "static_kv", "=", "True", ",", "# ??", "\n", "need_weights", "=", "True", "\n", ")", "\n", "# Balancing factor between generative & copy distributions", "\n", "alphas", "=", "torch", ".", "sigmoid", "(", "\n", "self", ".", "copy_balancing_layer", "(", "attn_output", ")", "\n", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "else", ":", "\n", "# Default values to be returned", "\n", "            ", "copy_scores", "=", "None", "\n", "alphas", "=", "None", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "batch_size", ",", "tgt_len", ",", "dim_model", "=", "x", ".", "size", "(", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "# project back to size of vocabulary", "\n", "            ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "\n", "", "\"\"\"\n        MODIFIED: combine copy distribution with generative distribution\n        \"\"\"", "\n", "gen_probs", "=", "None", "\n", "copy_probs", "=", "None", "\n", "self", ".", "num_batches", "+=", "1", "\n", "\n", "if", "self", ".", "copy_attention", "is", "not", "None", ":", "\n", "# Generative distribution: B x T_dec x V", "\n", "            ", "gen_logits", "=", "x", "\n", "vocab_size", "=", "self", ".", "embed_tokens", ".", "num_embeddings", "\n", "assert", "gen_logits", ".", "size", "(", ")", "==", "(", "batch_size", ",", "tgt_len", ",", "vocab_size", ")", ",", "f\"invalid shape for decoder logits \"", "f\"(possibly changed while taking softmax)\"", "\n", "\n", "# Copy distribution: B x T_dec x V", "\n", "copy_probs", "=", "compute_copy_probs", "(", "\n", "copy_scores", ",", "encoder_out", "[", "'encoder_input_tokens'", "]", ",", "vocab_size", "\n", ")", "\n", "\n", "# Optionally zero-out the probability of copied words", "\n", "if", "self", ".", "pad_copied_words", ":", "\n", "                ", "gen_logits", ".", "masked_fill_", "(", "copy_probs", ">", "0", ",", "-", "1e8", ")", "\n", "\n", "# Optionally increase copying ratio during initial training steps", "\n", "", "if", "self", ".", "alpha_warmup", ">", "0", "and", "self", ".", "training", ":", "\n", "                ", "clamp_factor", "=", "min", "(", "1.", ",", "self", ".", "num_batches", "/", "self", ".", "alpha_warmup", ")", "\n", "alphas", "=", "torch", ".", "clamp", "(", "alphas", ",", "max", "=", "clamp_factor", ")", "\n", "\n", "# alphas: B x T_dec x 1 -> B x T_dec x V", "\n", "", "alphas_", "=", "alphas", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "vocab_size", ")", "\n", "\n", "# Combine copy & generative distributions using alphas", "\n", "gen_probs", "=", "torch", ".", "softmax", "(", "gen_logits", ",", "dim", "=", "-", "1", ")", "\n", "combined_probs", "=", "(", "1", "-", "alphas_", ")", "*", "gen_probs", "+", "alphas_", "*", "copy_probs", "\n", "x", "=", "torch", ".", "log", "(", "combined_probs", "+", "1e-8", ")", "# stability... but inefficient", "\n", "\n", "# Diagnostic check", "\n", "mean_alpha", "=", "alphas", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "if", "mean_alpha", ">", "0.9", ":", "\n", "                ", "self", ".", "num_copies", "+=", "1", "\n", "print", "(", "f\"WARNING: reached mean copying ratio of {mean_alpha:.5f}\"", "\n", "f\", copy count: {self.num_copies}/{self.num_batches}\"", ")", "\n", "", "self", ".", "mean_alpha", "+=", "mean_alpha", "\n", "if", "self", ".", "num_batches", "%", "1000", "==", "0", ":", "\n", "                ", "print", "(", "f\"INFO: number of batches {self.num_batches}, \"", "\n", "f\"mean copying ratio (alpha): \"", "\n", "f\"{self.mean_alpha / 1000:.5f}\"", ")", "\n", "self", ".", "mean_alpha", "=", "0.0", "\n", "\n", "", "", "\"\"\"\n        MODIFIED: also return copy scores, balancing factors (alpha),\n            copy & generative probs, as well as edit labels from the encoder.\n            (None if unavailable.)\n        \"\"\"", "\n", "return", "x", ",", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", ",", "\n", "'alphas'", ":", "alphas", ",", "'copy_scores'", ":", "copy_scores", ",", "\n", "'gen_probs'", ":", "gen_probs", ",", "'copy_probs'", ":", "copy_probs", ",", "\n", "'edit_logits'", ":", "encoder_out", "[", "'encoder_edit_logits'", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.max_positions": [[673, 678], ["min", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.buffered_future_mask": [[679, 686], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder._future_mask.resize_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.CopyAugmentedTransformerELDecoder.upgrade_state_dict_named": [[687, 715], ["isinstance", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "layer_norm_map.items", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "# update layer norms", "\n", "            ", "layer_norm_map", "=", "{", "\n", "'0'", ":", "'self_attn_layer_norm'", ",", "\n", "'1'", ":", "'encoder_attn_layer_norm'", ",", "\n", "'2'", ":", "'final_layer_norm'", "\n", "}", "\n", "for", "old", ",", "new", "in", "layer_norm_map", ".", "items", "(", ")", ":", "\n", "                ", "for", "m", "in", "(", "'weight'", ",", "'bias'", ")", ":", "\n", "                    ", "k", "=", "'{}.layers.{}.layer_norms.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "old", ",", "m", ")", "\n", "if", "k", "in", "state_dict", ":", "\n", "                        ", "state_dict", "[", "'{}.layers.{}.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "new", ",", "m", ")", "]", "=", "state_dict", "[", "k", "]", "\n", "del", "state_dict", "[", "k", "]", "\n", "", "", "", "", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "'{}.version'", ".", "format", "(", "name", ")", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "'{}.version'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.compute_copy_probs": [[722, 743], ["copy_scores.size", "copy_scores.new_zeros", "src_tokens.unsqueeze().expand", "copy_scores.new_zeros.scatter_add", "src_tokens.unsqueeze"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["def", "compute_copy_probs", "(", "copy_scores", ",", "src_tokens", ",", "vocab_size", ")", ":", "\n", "    ", "\"\"\"Converts copy scores on source tokens into a probability distribution.\n\n    The `scatter_add()` operation is the same as:\n    ```python\n    for b in range(batch_size):\n        for t in range(tgt_len):\n            for s in range(src_len):\n                copy_probs[b, t, src_tokens[b, s]] += copy_scores[b, t, s]\n    ```\n\n    :param copy_scores: torch.FloatTensor, (batch_size, tgt_len, src_len)\n    :param src_tokens: torch.LongTensor, (batch_size, src_len)\n    :param vocab_size: int\n    :return: torch.FloatTensor, (batch_size, tgt_len, vocab_size)\n    \"\"\"", "\n", "batch_size", ",", "tgt_len", ",", "src_len", "=", "copy_scores", ".", "size", "(", ")", "\n", "copy_probs", "=", "copy_scores", ".", "new_zeros", "(", "(", "batch_size", ",", "tgt_len", ",", "vocab_size", ")", ",", "\n", "requires_grad", "=", "copy_scores", ".", "requires_grad", ")", "\n", "src_tokens_expanded", "=", "src_tokens", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "tgt_len", ",", "-", "1", ")", "\n", "return", "copy_probs", ".", "scatter_add", "(", "2", ",", "src_tokens_expanded", ",", "copy_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.base_architecture": [[762, 804], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'copy_augmented_transformer_el'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "\"\"\"Copy-augmented transformer with edit-augmented encoder-decoder attention.\n\n    Same model size as the original Transformer,\n    except double FFN size (4096) and larger dropout rate (0.2).\n    \"\"\"", "\n", "\n", "# Defaults to all True.", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "True", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "True", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "True", ")", "\n", "args", ".", "copy_attention_heads", "=", "getattr", "(", "args", ",", "'copy_attention_heads'", ",", "0", ")", "\n", "args", ".", "alpha_warmup", "=", "getattr", "(", "args", ",", "'alpha_warmup'", ",", "0", ")", "\n", "args", ".", "pad_copied_words", "=", "getattr", "(", "args", ",", "'pad_copied_words'", ",", "False", ")", "\n", "\n", "# Model size parameters.", "\n", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.2", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "True", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "True", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.transformer_aux_el": [[806, 813], ["register_model_architecture", "getattr", "getattr", "getattr", "copy_augmented_transformer_el.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'transformer_aux_el'", ")", "\n", "def", "transformer_aux_el", "(", "args", ")", ":", "\n", "    ", "\"\"\"Vanilla Transformer that also predicts edit labels as auxiliary task.\"\"\"", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "False", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "True", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "False", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.transformer_el": [[815, 822], ["register_model_architecture", "getattr", "getattr", "getattr", "copy_augmented_transformer_el.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'transformer_el'", ")", "\n", "def", "transformer_el", "(", "args", ")", ":", "\n", "    ", "\"\"\"Vanilla Transformer with edit score-augmented encoder-decoder attention.\"\"\"", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "False", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "True", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "True", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.copy_augmented_transformer": [[824, 831], ["register_model_architecture", "getattr", "getattr", "getattr", "copy_augmented_transformer_el.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'copy_augmented_transformer'", ")", "\n", "def", "copy_augmented_transformer", "(", "args", ")", ":", "\n", "    ", "\"\"\"Copy-augmented Transformer *without* edit label prediction.\"\"\"", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "True", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "False", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "False", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.copy_augmented_transformer_aux_el": [[833, 842], ["register_model_architecture", "getattr", "getattr", "getattr", "copy_augmented_transformer_el.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'copy_augmented_transformer_aux_el'", ")", "\n", "def", "copy_augmented_transformer_aux_el", "(", "args", ")", ":", "\n", "    ", "\"\"\"Copy-augmented Transformer with edit label prediction.\n    *Most similar to the original paper's setup.*\n    \"\"\"", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "True", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "True", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "False", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.copy_augmented_transformer_el.copy_augmented_transformer_aux_el_t2t": [[844, 866], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "copy_augmented_transformer_el.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'copy_augmented_transformer_el'", ",", "'copy_augmented_transformer_aux_el_t2t'", ")", "\n", "def", "copy_augmented_transformer_aux_el_t2t", "(", "args", ")", ":", "\n", "    ", "\"\"\"Copy-augmented Transformer with edit label prediction.\n    T2T ver: larger model size (1024-4096), layernorm *before* attention.\"\"\"", "\n", "args", ".", "use_copy_scores", "=", "getattr", "(", "args", ",", "'use_copy_scores'", ",", "True", ")", "\n", "args", ".", "predict_edit_labels", "=", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "True", ")", "\n", "args", ".", "decode_with_edit_labels", "=", "getattr", "(", "args", ",", "'decode_with_edit_labels'", ",", "False", ")", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "True", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "True", ")", "\n", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.1", ")", "\n", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMModel.__init__": [[22, 24], ["FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMModel.add_args": [[25, 76], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-freeze-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'freeze encoder embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-hidden-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder hidden size'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-bidirectional'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'make all layers of encoder bidirectional'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-freeze-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'freeze decoder embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-hidden-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder hidden size'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'decoder attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "\n", "# Granular dropout settings (if not specified these default to --dropout)", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder output'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder output'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMModel.build_model": [[78, 170], ["lstm.base_architecture", "lstm.LSTMEncoder", "lstm.LSTMDecoder", "cls", "ValueError", "len", "dictionary.pad", "lstm.Embedding", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "fairseq.utils.load_embedding", "lstm.LSTMModel.build_model.load_pretrained_embedding_from_file"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure that all args are properly defaulted (in case there are any new ones)", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "args", ".", "encoder_layers", "!=", "args", ".", "decoder_layers", ":", "\n", "            ", "raise", "ValueError", "(", "'--encoder-layers must match --decoder-layers'", ")", "\n", "\n", "", "def", "load_pretrained_embedding_from_file", "(", "embed_path", ",", "dictionary", ",", "embed_dim", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "embed_dict", ",", "dictionary", ")", "\n", "return", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "embed_tokens", ")", "\n", "\n", "", "if", "args", ".", "encoder_embed_path", ":", "\n", "            ", "pretrained_encoder_embed", "=", "load_pretrained_embedding_from_file", "(", "\n", "args", ".", "encoder_embed_path", ",", "task", ".", "source_dictionary", ",", "args", ".", "encoder_embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "task", ".", "source_dictionary", ")", "\n", "pretrained_encoder_embed", "=", "Embedding", "(", "\n", "num_embeddings", ",", "args", ".", "encoder_embed_dim", ",", "task", ".", "source_dictionary", ".", "pad", "(", ")", "\n", ")", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "# double check all parameters combinations are valid", "\n", "            ", "if", "task", ".", "source_dictionary", "!=", "task", ".", "target_dictionary", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joint dictionary'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embed not compatible with --decoder-embed-path'", "\n", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to '", "\n", "'match --decoder-embed-dim'", "\n", ")", "\n", "", "pretrained_decoder_embed", "=", "pretrained_encoder_embed", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "# separate decoder input embeddings", "\n", "            ", "pretrained_decoder_embed", "=", "None", "\n", "if", "args", ".", "decoder_embed_path", ":", "\n", "                ", "pretrained_decoder_embed", "=", "load_pretrained_embedding_from_file", "(", "\n", "args", ".", "decoder_embed_path", ",", "\n", "task", ".", "target_dictionary", ",", "\n", "args", ".", "decoder_embed_dim", "\n", ")", "\n", "# one last double check of parameter combinations", "\n", "", "", "if", "args", ".", "share_decoder_input_output_embed", "and", "(", "\n", "args", ".", "decoder_embed_dim", "!=", "args", ".", "decoder_out_embed_dim", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'--share-decoder-input-output-embeddings requires '", "\n", "'--decoder-embed-dim to match --decoder-out-embed-dim'", "\n", ")", "\n", "\n", "", "if", "args", ".", "encoder_freeze_embed", ":", "\n", "            ", "pretrained_encoder_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "if", "args", ".", "decoder_freeze_embed", ":", "\n", "            ", "pretrained_decoder_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "", "encoder", "=", "LSTMEncoder", "(", "\n", "dictionary", "=", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "hidden_size", "=", "args", ".", "encoder_hidden_size", ",", "\n", "num_layers", "=", "args", ".", "encoder_layers", ",", "\n", "dropout_in", "=", "args", ".", "encoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "encoder_dropout_out", ",", "\n", "bidirectional", "=", "args", ".", "encoder_bidirectional", ",", "\n", "pretrained_embed", "=", "pretrained_encoder_embed", ",", "\n", ")", "\n", "decoder", "=", "LSTMDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "hidden_size", "=", "args", ".", "decoder_hidden_size", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "num_layers", "=", "args", ".", "decoder_layers", ",", "\n", "dropout_in", "=", "args", ".", "decoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "decoder_dropout_out", ",", "\n", "attention", "=", "options", ".", "eval_bool", "(", "args", ".", "decoder_attention", ")", ",", "\n", "encoder_output_units", "=", "encoder", ".", "output_units", ",", "\n", "pretrained_embed", "=", "pretrained_decoder_embed", ",", "\n", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", ",", "\n", "adaptive_softmax_cutoff", "=", "(", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", "\n", "if", "args", ".", "criterion", "==", "'adaptive_loss'", "else", "None", "\n", ")", ",", "\n", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMEncoder.__init__": [[174, 206], ["FairseqEncoder.__init__", "len", "dictionary.pad", "lstm.LSTM", "lstm.Embedding"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTM", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "hidden_size", "=", "512", ",", "num_layers", "=", "1", ",", "\n", "dropout_in", "=", "0.1", ",", "dropout_out", "=", "0.1", ",", "bidirectional", "=", "False", ",", "\n", "left_pad", "=", "True", ",", "pretrained_embed", "=", "None", ",", "padding_value", "=", "0.", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "if", "pretrained_embed", "is", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "pretrained_embed", "\n", "\n", "", "self", ".", "lstm", "=", "LSTM", "(", "\n", "input_size", "=", "embed_dim", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "self", ".", "dropout_out", "if", "num_layers", ">", "1", "else", "0.", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", ")", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "padding_value", "=", "padding_value", "\n", "\n", "self", ".", "output_units", "=", "hidden_size", "\n", "if", "bidirectional", ":", "\n", "            ", "self", ".", "output_units", "*=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMEncoder.forward": [[207, 256], ["fairseq.utils.convert_padding_direction.size", "lstm.LSTMEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout.transpose", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.dropout.new_zeros", "torch.dropout.new_zeros", "lstm.LSTMEncoder.lstm", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.dropout", "torch.dropout", "torch.dropout", "fairseq.utils.convert_padding_direction.eq().t", "fairseq.utils.convert_padding_direction", "src_lengths.data.tolist", "list", "lstm.LSTMEncoder.forward.combine_bidir"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.convert_padding_direction"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "if", "self", ".", "left_pad", ":", "\n", "# convert left-padding to right-padding", "\n", "            ", "src_tokens", "=", "utils", ".", "convert_padding_direction", "(", "\n", "src_tokens", ",", "\n", "self", ".", "padding_idx", ",", "\n", "left_to_right", "=", "True", ",", "\n", ")", "\n", "\n", "", "bsz", ",", "seqlen", "=", "src_tokens", ".", "size", "(", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# pack embedded source tokens into a PackedSequence", "\n", "packed_x", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "x", ",", "src_lengths", ".", "data", ".", "tolist", "(", ")", ")", "\n", "\n", "# apply LSTM", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "state_size", "=", "2", "*", "self", ".", "num_layers", ",", "bsz", ",", "self", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "state_size", "=", "self", ".", "num_layers", ",", "bsz", ",", "self", ".", "hidden_size", "\n", "", "h0", "=", "x", ".", "new_zeros", "(", "*", "state_size", ")", "\n", "c0", "=", "x", ".", "new_zeros", "(", "*", "state_size", ")", "\n", "packed_outs", ",", "(", "final_hiddens", ",", "final_cells", ")", "=", "self", ".", "lstm", "(", "packed_x", ",", "(", "h0", ",", "c0", ")", ")", "\n", "\n", "# unpack outputs and apply dropout", "\n", "x", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_outs", ",", "padding_value", "=", "self", ".", "padding_value", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "assert", "list", "(", "x", ".", "size", "(", ")", ")", "==", "[", "seqlen", ",", "bsz", ",", "self", ".", "output_units", "]", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "\n", "            ", "def", "combine_bidir", "(", "outs", ")", ":", "\n", "                ", "out", "=", "outs", ".", "view", "(", "self", ".", "num_layers", ",", "2", ",", "bsz", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "return", "out", ".", "view", "(", "self", ".", "num_layers", ",", "bsz", ",", "-", "1", ")", "\n", "\n", "", "final_hiddens", "=", "combine_bidir", "(", "final_hiddens", ")", "\n", "final_cells", "=", "combine_bidir", "(", "final_cells", ")", "\n", "\n", "", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "final_hiddens", ",", "final_cells", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", "if", "encoder_padding_mask", ".", "any", "(", ")", "else", "None", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMEncoder.reorder_encoder_out": [[258, 267], ["tuple", "encoder_out[].index_select", "eo.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "encoder_out", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "for", "eo", "in", "encoder_out", "[", "'encoder_out'", "]", "\n", ")", "\n", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMEncoder.max_positions": [[268, 271], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.AttentionLayer.__init__": [[274, 279], ["torch.Module.__init__", "lstm.Linear", "lstm.Linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "input_embed_dim", ",", "source_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_proj", "=", "Linear", "(", "input_embed_dim", ",", "source_embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "output_proj", "=", "Linear", "(", "input_embed_dim", "+", "source_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.AttentionLayer.forward": [[280, 304], ["lstm.AttentionLayer.input_proj", "torch.softmax", "torch.softmax", "torch.softmax", "torch.tanh", "torch.tanh", "torch.tanh", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float().masked_fill_().type_as", "lstm.AttentionLayer.output_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh.unsqueeze", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float().masked_fill_", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.unsqueeze", "float", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "source_hids", ",", "encoder_padding_mask", ")", ":", "\n", "# input: bsz x input_embed_dim", "\n", "# source_hids: srclen x bsz x output_embed_dim", "\n", "\n", "# x: bsz x output_embed_dim", "\n", "        ", "x", "=", "self", ".", "input_proj", "(", "input", ")", "\n", "\n", "# compute attention", "\n", "attn_scores", "=", "(", "source_hids", "*", "x", ".", "unsqueeze", "(", "0", ")", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "\n", "# don't attend over padding", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "attn_scores", "=", "attn_scores", ".", "float", "(", ")", ".", "masked_fill_", "(", "\n", "encoder_padding_mask", ",", "\n", "float", "(", "'-inf'", ")", "\n", ")", ".", "type_as", "(", "attn_scores", ")", "# FP16 support: cast to float and back", "\n", "\n", "", "attn_scores", "=", "F", ".", "softmax", "(", "attn_scores", ",", "dim", "=", "0", ")", "# srclen x bsz", "\n", "\n", "# sum weighted sources", "\n", "x", "=", "(", "attn_scores", ".", "unsqueeze", "(", "2", ")", "*", "source_hids", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "\n", "x", "=", "F", ".", "tanh", "(", "self", ".", "output_proj", "(", "torch", ".", "cat", "(", "(", "x", ",", "input", ")", ",", "dim", "=", "1", ")", ")", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMDecoder.__init__": [[308, 355], ["FairseqIncrementalDecoder.__init__", "len", "dictionary.pad", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lstm.Embedding", "lstm.Linear", "lstm.Linear", "lstm.AttentionLayer", "lstm.Linear", "fairseq.modules.AdaptiveSoftmax", "lstm.LSTMCell", "lstm.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "hidden_size", "=", "512", ",", "out_embed_dim", "=", "512", ",", "\n", "num_layers", "=", "1", ",", "dropout_in", "=", "0.1", ",", "dropout_out", "=", "0.1", ",", "attention", "=", "True", ",", "\n", "encoder_output_units", "=", "512", ",", "pretrained_embed", "=", "None", ",", "\n", "share_input_output_embed", "=", "False", ",", "adaptive_softmax_cutoff", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "share_input_output_embed", "=", "share_input_output_embed", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "if", "pretrained_embed", "is", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "pretrained_embed", "\n", "\n", "", "self", ".", "encoder_output_units", "=", "encoder_output_units", "\n", "if", "encoder_output_units", "!=", "hidden_size", ":", "\n", "            ", "self", ".", "encoder_hidden_proj", "=", "Linear", "(", "encoder_output_units", ",", "hidden_size", ")", "\n", "self", ".", "encoder_cell_proj", "=", "Linear", "(", "encoder_output_units", ",", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_hidden_proj", "=", "self", ".", "encoder_cell_proj", "=", "None", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "\n", "LSTMCell", "(", "\n", "input_size", "=", "hidden_size", "+", "embed_dim", "if", "layer", "==", "0", "else", "hidden_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", ")", "\n", "for", "layer", "in", "range", "(", "num_layers", ")", "\n", "]", ")", "\n", "if", "attention", ":", "\n", "# TODO make bias configurable", "\n", "            ", "self", ".", "attention", "=", "AttentionLayer", "(", "hidden_size", ",", "encoder_output_units", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "attention", "=", "None", "\n", "", "if", "hidden_size", "!=", "out_embed_dim", ":", "\n", "            ", "self", ".", "additional_fc", "=", "Linear", "(", "hidden_size", ",", "out_embed_dim", ")", "\n", "", "if", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "# setting adaptive_softmax dropout to dropout_out for now but can be redefined", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "num_embeddings", ",", "embed_dim", ",", "adaptive_softmax_cutoff", ",", "\n", "dropout", "=", "dropout_out", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "fc_out", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMDecoder.forward": [[356, 446], ["prev_output_tokens.size", "encoder_outs.size", "lstm.LSTMDecoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "lstm.LSTMDecoder.transpose", "fairseq.utils.get_incremental_state", "lstm.LSTMDecoder.new_zeros", "range", "fairseq.utils.set_incremental_state", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "lstm.LSTMDecoder.transpose", "len", "lstm.LSTMDecoder.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "torch.dropout", "torch.dropout", "torch.dropout", "outs.append", "attn_scores.transpose.transpose.transpose", "hasattr", "rnn", "torch.dropout", "torch.dropout", "torch.dropout", "lstm.LSTMDecoder.attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "lstm.LSTMDecoder.additional_fc", "torch.dropout", "torch.dropout", "torch.dropout", "torch.linear", "torch.linear", "torch.linear", "lstm.LSTMDecoder.fc_out", "range", "range", "lstm.LSTMDecoder.encoder_hidden_proj", "lstm.LSTMDecoder.encoder_cell_proj"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out_dict", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "encoder_out", "=", "encoder_out_dict", "[", "'encoder_out'", "]", "\n", "encoder_padding_mask", "=", "encoder_out_dict", "[", "'encoder_padding_mask'", "]", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bsz", ",", "seqlen", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "\n", "# get outputs from encoder", "\n", "encoder_outs", ",", "encoder_hiddens", ",", "encoder_cells", "=", "encoder_out", "[", ":", "3", "]", "\n", "srclen", "=", "encoder_outs", ".", "size", "(", "0", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# initialize previous states (or get from cache during incremental generation)", "\n", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "not", "None", ":", "\n", "            ", "prev_hiddens", ",", "prev_cells", ",", "input_feed", "=", "cached_state", "\n", "", "else", ":", "\n", "            ", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "prev_hiddens", "=", "[", "encoder_hiddens", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "prev_cells", "=", "[", "encoder_cells", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "if", "self", ".", "encoder_hidden_proj", "is", "not", "None", ":", "\n", "                ", "prev_hiddens", "=", "[", "self", ".", "encoder_hidden_proj", "(", "x", ")", "for", "x", "in", "prev_hiddens", "]", "\n", "prev_cells", "=", "[", "self", ".", "encoder_cell_proj", "(", "x", ")", "for", "x", "in", "prev_cells", "]", "\n", "", "input_feed", "=", "x", ".", "new_zeros", "(", "bsz", ",", "self", ".", "hidden_size", ")", "\n", "\n", "", "attn_scores", "=", "x", ".", "new_zeros", "(", "srclen", ",", "seqlen", ",", "bsz", ")", "\n", "outs", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "seqlen", ")", ":", "\n", "# input feeding: concatenate context vector from previous time step", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "(", "x", "[", "j", ",", ":", ",", ":", "]", ",", "input_feed", ")", ",", "dim", "=", "1", ")", "\n", "\n", "for", "i", ",", "rnn", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "# recurrent cell", "\n", "                ", "hidden", ",", "cell", "=", "rnn", "(", "input", ",", "(", "prev_hiddens", "[", "i", "]", ",", "prev_cells", "[", "i", "]", ")", ")", "\n", "\n", "# hidden state becomes the input to the next layer", "\n", "input", "=", "F", ".", "dropout", "(", "hidden", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# save state for next time step", "\n", "prev_hiddens", "[", "i", "]", "=", "hidden", "\n", "prev_cells", "[", "i", "]", "=", "cell", "\n", "\n", "# apply attention using the last layer's hidden state", "\n", "", "if", "self", ".", "attention", "is", "not", "None", ":", "\n", "                ", "out", ",", "attn_scores", "[", ":", ",", "j", ",", ":", "]", "=", "self", ".", "attention", "(", "hidden", ",", "encoder_outs", ",", "encoder_padding_mask", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "hidden", "\n", "", "out", "=", "F", ".", "dropout", "(", "out", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# input feeding", "\n", "input_feed", "=", "out", "\n", "\n", "# save final output", "\n", "outs", ".", "append", "(", "out", ")", "\n", "\n", "# cache previous states (no-op except during incremental generation)", "\n", "", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "incremental_state", ",", "'cached_state'", ",", "\n", "(", "prev_hiddens", ",", "prev_cells", ",", "input_feed", ")", ",", "\n", ")", "\n", "\n", "# collect outputs across time steps", "\n", "x", "=", "torch", ".", "cat", "(", "outs", ",", "dim", "=", "0", ")", ".", "view", "(", "seqlen", ",", "bsz", ",", "self", ".", "hidden_size", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# srclen x tgtlen x bsz -> bsz x tgtlen x srclen", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "            ", "attn_scores", "=", "attn_scores", ".", "transpose", "(", "0", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "attn_scores", "=", "None", "\n", "\n", "# project back to size of vocabulary", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "'additional_fc'", ")", ":", "\n", "                ", "x", "=", "self", ".", "additional_fc", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "self", ".", "fc_out", "(", "x", ")", "\n", "", "", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMDecoder.reorder_incremental_state": [[447, 460], ["super().reorder_incremental_state", "fairseq.utils.get_incremental_state", "tuple", "fairseq.utils.set_incremental_state", "isinstance", "state.index_select", "map", "lstm.LSTMDecoder.reorder_incremental_state.reorder_state"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "super", "(", ")", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "def", "reorder_state", "(", "state", ")", ":", "\n", "            ", "if", "isinstance", "(", "state", ",", "list", ")", ":", "\n", "                ", "return", "[", "reorder_state", "(", "state_i", ")", "for", "state_i", "in", "state", "]", "\n", "", "return", "state", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "", "new_state", "=", "tuple", "(", "map", "(", "reorder_state", ",", "cached_state", ")", ")", "\n", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ",", "new_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMDecoder.max_positions": [[461, 464], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMDecoder.make_generation_fast_": [[465, 467], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.Embedding": [[469, 474], ["torch.Embedding", "torch.init.uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "m", ".", "weight", ",", "-", "0.1", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTM": [[476, 482], ["torch.LSTM", "nn.LSTM.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTM"], ["", "def", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMCell": [[484, 490], ["torch.LSTMCell", "nn.LSTMCell.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.LSTMCell"], ["", "def", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.Linear": [[492, 499], ["torch.Linear", "nn.Linear.weight.data.uniform_", "nn.Linear.bias.data.uniform_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "if", "bias", ":", "\n", "        ", "m", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.base_architecture": [[501, 524], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_freeze_embed", "=", "getattr", "(", "args", ",", "'encoder_freeze_embed'", ",", "False", ")", "\n", "args", ".", "encoder_hidden_size", "=", "getattr", "(", "args", ",", "'encoder_hidden_size'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "1", ")", "\n", "args", ".", "encoder_bidirectional", "=", "getattr", "(", "args", ",", "'encoder_bidirectional'", ",", "False", ")", "\n", "args", ".", "encoder_dropout_in", "=", "getattr", "(", "args", ",", "'encoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_freeze_embed", "=", "getattr", "(", "args", ",", "'decoder_freeze_embed'", ",", "False", ")", "\n", "args", ".", "decoder_hidden_size", "=", "getattr", "(", "args", ",", "'decoder_hidden_size'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "1", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'1'", ")", "\n", "args", ".", "decoder_dropout_in", "=", "getattr", "(", "args", ",", "'decoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,50000,200000'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.lstm_wiseman_iwslt_de_en": [[526, 537], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_wiseman_iwslt_de_en'", ")", "\n", "def", "lstm_wiseman_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_dropout_in", "=", "getattr", "(", "args", ",", "'encoder_dropout_in'", ",", "0", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "0", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_dropout_in", "=", "getattr", "(", "args", ",", "'decoder_dropout_in'", ",", "0", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lstm.lstm_luong_wmt_en_de": [[539, 549], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_luong_wmt_en_de'", ")", "\n", "def", "lstm_luong_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1000", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "4", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "0", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1000", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "4", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "1000", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "0", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvModel.__init__": [[48, 50], ["FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvModel.add_args": [[51, 123], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.options.eval_str_list", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability of the inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-conv-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "\n", "\"\"\"LightConv and DynamicConv arguments\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--encoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-softmax'", ",", "default", "=", "True", ",", "type", "=", "options", ".", "eval_bool", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for conv weights'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvModel.build_model": [[124, 173], ["lightconv.base_architecture", "lightconv.LightConvEncoder", "lightconv.LightConvDecoder", "lightconv.LightConvModel", "hasattr", "hasattr", "len", "dictionary.pad", "lightconv.Embedding", "lightconv.LightConvModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "RuntimeError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "encoder", "=", "LightConvEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "LightConvDecoder", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ")", "\n", "return", "LightConvModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvLanguageModel.__init__": [[177, 179], ["FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvLanguageModel.add_args": [[180, 249], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability of the inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-output-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-input-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder input dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses character embedding convolutions to produce token embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-filters'", ",", "type", "=", "str", ",", "metavar", "=", "'LIST'", ",", "\n", "default", "=", "'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]'", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embedding-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "4", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--char-embedder-highway-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "2", ",", "\n", "help", "=", "'number of highway layers for character token embeddder'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive input cutoff points.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-weights'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-proj'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the projection weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "\n", "\"\"\"LightConv and DynamicConv arguments\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--decoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-softmax'", ",", "default", "=", "True", ",", "type", "=", "options", ".", "eval_bool", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for conv weights'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvLanguageModel.build_model": [[250, 284], ["lightconv.base_lm_architecture", "lightconv.LightConvDecoder", "lightconv.LightConvLanguageModel", "hasattr", "hasattr", "fairseq.modules.CharacterTokenEmbedder", "eval", "fairseq.modules.AdaptiveInput", "lightconv.Embedding", "len", "task.dictionary.pad", "fairseq.options.eval_str_list", "len", "task.dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "args", ".", "tokens_per_sample", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "args", ".", "tokens_per_sample", "\n", "\n", "", "if", "args", ".", "character_embeddings", ":", "\n", "            ", "embed_tokens", "=", "CharacterTokenEmbedder", "(", "task", ".", "dictionary", ",", "eval", "(", "args", ".", "character_filters", ")", ",", "\n", "args", ".", "character_embedding_dim", ",", "\n", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "char_embedder_highway_layers", ",", "\n", ")", "\n", "", "elif", "args", ".", "adaptive_input", ":", "\n", "            ", "embed_tokens", "=", "AdaptiveInput", "(", "len", "(", "task", ".", "dictionary", ")", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ",", "args", ".", "decoder_input_dim", ",", "\n", "args", ".", "adaptive_input_factor", ",", "args", ".", "decoder_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_input_cutoff", ",", "type", "=", "int", ")", ")", "\n", "", "else", ":", "\n", "            ", "embed_tokens", "=", "Embedding", "(", "len", "(", "task", ".", "dictionary", ")", ",", "args", ".", "decoder_input_dim", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ")", "\n", "\n", "", "if", "args", ".", "tie_adaptive_weights", ":", "\n", "            ", "assert", "args", ".", "adaptive_input", "\n", "assert", "args", ".", "adaptive_input_factor", "==", "args", ".", "adaptive_softmax_factor", "\n", "assert", "args", ".", "adaptive_softmax_cutoff", "==", "args", ".", "adaptive_input_cutoff", ",", "'{} != {}'", ".", "format", "(", "\n", "args", ".", "adaptive_softmax_cutoff", ",", "args", ".", "adaptive_input_cutoff", ")", "\n", "assert", "args", ".", "decoder_input_dim", "==", "args", ".", "decoder_output_dim", "\n", "\n", "", "decoder", "=", "LightConvDecoder", "(", "args", ",", "task", ".", "output_dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "True", ",", "final_norm", "=", "False", ")", "\n", "return", "LightConvLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoder.__init__": [[299, 324], ["FairseqEncoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.LightConvEncoder.layers.extend", "lightconv.LightConvEncoder.register_buffer", "lightconv.PositionalEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "lightconv.LayerNorm", "lightconv.LightConvEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "left_pad", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "LightConvEncoderLayer", "(", "args", ",", "kernel_size", "=", "args", ".", "encoder_kernel_size_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "encoder_normalize_before", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoder.forward": [[325, 364], ["torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoder.transpose", "src_tokens.eq", "lightconv.LightConvEncoder.embed_tokens", "lightconv.LightConvEncoder.embed_positions", "src_tokens.eq.any", "layer", "lightconv.LightConvEncoder.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "encoder_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoder.reorder_encoder_out": [[366, 384], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoder.max_positions": [[385, 390], ["min", "lightconv.LightConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoder.__init__": [[407, 458], ["FairseqIncrementalDecoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.LightConvDecoder.layers.extend", "lightconv.LightConvDecoder.register_buffer", "lightconv.Linear", "lightconv.PositionalEmbedding", "lightconv.Linear", "fairseq.modules.AdaptiveSoftmax", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "lightconv.LayerNorm", "lightconv.LightConvDecoderLayer", "len", "fairseq.options.eval_str_list", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ",", "left_pad", "=", "False", ",", "final_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "# todo: try with input_embed_dim", "\n", "\n", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "LightConvDecoderLayer", "(", "args", ",", "no_encoder_attn", ",", "kernel_size", "=", "args", ".", "decoder_kernel_size_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "args", ".", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "\n", "len", "(", "dictionary", ")", ",", "\n", "output_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", ",", "\n", "dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", "adaptive_inputs", "=", "embed_tokens", "if", "args", ".", "tie_adaptive_weights", "else", "None", ",", "\n", "factor", "=", "args", ".", "adaptive_softmax_factor", ",", "\n", "tie_proj", "=", "args", ".", "tie_adaptive_proj", ",", "\n", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "output_embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "output_embed_dim", "**", "-", "0.5", ")", "\n", "", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "decoder_normalize_before", "and", "final_norm", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoder.forward": [[459, 530], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.linear.transpose", "torch.linear.transpose", "lightconv.LightConvDecoder.embed_positions", "lightconv.LightConvDecoder.embed_tokens", "lightconv.LightConvDecoder.project_in_dim", "layer", "inner_states.append", "lightconv.LightConvDecoder.layer_norm", "lightconv.LightConvDecoder.project_out_dim", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "# project back to size of vocabulary", "\n", "            ", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_out", ")", "\n", "\n", "", "", "return", "x", ",", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoder.max_positions": [[531, 536], ["min", "lightconv.LightConvDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoder.buffered_future_mask": [[537, 544], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "lightconv.LightConvDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "lightconv.LightConvDecoder._future_mask.resize_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoderLayer.__init__": [[554, 587], ["torch.Module.__init__", "lightconv.Linear", "lightconv.Linear", "lightconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.Linear", "torch.GLU", "torch.GLU", "torch.GLU", "lightconv.Linear", "fairseq.modules.LightweightConv1dTBC", "fairseq.modules.DynamicConv1dTBC", "lightconv.LayerNorm", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "kernel_size", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "encoder_embed_dim", "\n", "self", ".", "conv_dim", "=", "args", ".", "encoder_conv_dim", "\n", "padding_l", "=", "kernel_size", "//", "2", "if", "kernel_size", "%", "2", "==", "1", "else", "(", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "kernel_size", "//", "2", ")", "\n", "\n", "if", "args", ".", "encoder_glu", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "2", "*", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "nn", ".", "GLU", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "None", "\n", "", "if", "args", ".", "encoder_conv_type", "==", "'lightweight'", ":", "\n", "            ", "self", ".", "conv", "=", "LightweightConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "padding_l", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "elif", "args", ".", "encoder_conv_type", "==", "'dynamic'", ":", "\n", "            ", "self", ".", "conv", "=", "DynamicConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "padding_l", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "linear2", "=", "Linear", "(", "self", ".", "conv_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "input_dropout", "=", "args", ".", "input_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "encoder_normalize_before", "\n", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "encoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "layer_norms", "=", "nn", ".", "ModuleList", "(", "[", "LayerNorm", "(", "self", ".", "embed_dim", ")", "for", "_", "in", "range", "(", "2", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoderLayer.forward": [[588, 621], ["lightconv.LightConvEncoderLayer.maybe_layer_norm", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.linear1", "lightconv.LightConvEncoderLayer.conv", "lightconv.LightConvEncoderLayer.linear2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "lightconv.LightConvEncoderLayer.act", "x.masked_fill.masked_fill.masked_fill", "lightconv.LightConvEncoderLayer.fc1", "encoder_padding_mask.transpose().unsqueeze", "encoder_padding_mask.transpose"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_padding_mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "input_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "linear1", "(", "x", ")", "\n", "if", "self", ".", "act", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "transpose", "(", "0", ",", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "0", ")", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "linear2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoderLayer.maybe_layer_norm": [[622, 628], ["None"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "i", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "self", ".", "layer_norms", "[", "i", "]", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvEncoderLayer.extra_repr": [[629, 632], ["None"], "methods", ["None"], ["", "", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'", ".", "format", "(", "\n", "self", ".", "dropout", ",", "self", ".", "relu_dropout", ",", "self", ".", "input_dropout", ",", "self", ".", "normalize_before", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoderLayer.__init__": [[644, 690], ["torch.Module.__init__", "lightconv.Linear", "lightconv.LayerNorm", "lightconv.Linear", "lightconv.Linear", "lightconv.LayerNorm", "lightconv.Linear", "torch.GLU", "torch.GLU", "torch.GLU", "lightconv.Linear", "fairseq.modules.LightweightConv1dTBC", "fairseq.modules.MultiheadAttention", "lightconv.LayerNorm", "fairseq.modules.DynamicConv1dTBC"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ",", "kernel_size", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "conv_dim", "=", "args", ".", "decoder_conv_dim", "\n", "if", "args", ".", "decoder_glu", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "2", "*", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "nn", ".", "GLU", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "None", "\n", "", "if", "args", ".", "decoder_conv_type", "==", "'lightweight'", ":", "\n", "            ", "self", ".", "conv", "=", "LightweightConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "kernel_size", "-", "1", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "decoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "elif", "args", ".", "decoder_conv_type", "==", "'dynamic'", ":", "\n", "            ", "self", ".", "conv", "=", "DynamicConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "kernel_size", "-", "1", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "decoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "linear2", "=", "Linear", "(", "self", ".", "conv_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "input_dropout", "=", "args", ".", "input_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "decoder_normalize_before", "\n", "\n", "self", ".", "conv_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "decoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoderLayer.forward": [[691, 751], ["lightconv.LightConvDecoderLayer.maybe_layer_norm", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.linear1", "lightconv.LightConvDecoderLayer.conv", "lightconv.LightConvDecoderLayer.linear2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.conv._set_input_buffer", "lightconv.LightConvDecoderLayer.act", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.fc1", "lightconv.LightConvDecoderLayer.encoder_attn._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_out", ",", "encoder_padding_mask", ",", "incremental_state", ",", "\n", "prev_conv_state", "=", "None", ",", "prev_attn_state", "=", "None", ",", "conv_mask", "=", "None", ",", "\n", "conv_padding_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "conv_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_conv_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "self", ".", "conv", ".", "_set_input_buffer", "(", "incremental_state", ",", "prev_conv_state", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "input_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "linear1", "(", "x", ")", "\n", "if", "self", ".", "act", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ",", "incremental_state", "=", "incremental_state", ")", "\n", "x", "=", "self", ".", "linear2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "conv_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "attn", "=", "None", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "                ", "if", "incremental_state", "is", "None", ":", "\n", "                    ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoderLayer.maybe_layer_norm": [[752, 758], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoderLayer.make_generation_fast_": [[759, 761], ["None"], "methods", ["None"], ["", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LightConvDecoderLayer.extra_repr": [[762, 765], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'", ".", "format", "(", "\n", "self", ".", "dropout", ",", "self", ".", "relu_dropout", ",", "self", ".", "input_dropout", ",", "self", ".", "normalize_before", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.Embedding": [[767, 772], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.LayerNorm": [[774, 777], ["torch.LayerNorm"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm"], ["", "def", "LayerNorm", "(", "embedding_dim", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LayerNorm", "(", "embedding_dim", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.Linear": [[779, 785], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.PositionalEmbedding": [[787, 795], ["fairseq.modules.LearnedPositionalEmbedding", "torch.init.normal_", "torch.init.constant_", "fairseq.modules.SinusoidalPositionalEmbedding"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ",", "learned", "=", "False", ")", ":", "\n", "    ", "if", "learned", ":", "\n", "        ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", "+", "padding_idx", "+", "1", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "        ", "m", "=", "SinusoidalPositionalEmbedding", "(", "embedding_dim", ",", "padding_idx", ",", "left_pad", ",", "num_embeddings", "+", "padding_idx", "+", "1", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.base_lm_architecture": [[797, 826], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv_lm'", ",", "'lightconv_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "adaptive_softmax_factor", "=", "getattr", "(", "args", ",", "'adaptive_softmax_factor'", ",", "4", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "\n", "args", ".", "character_embeddings", "=", "getattr", "(", "args", ",", "'character_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "# The model training is not stable without this", "\n", "args", ".", "decoder_normalize_before", "=", "True", "\n", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "False", ")", "\n", "args", ".", "adaptive_input_factor", "=", "getattr", "(", "args", ",", "'adaptive_input_factor'", ",", "4", ")", "\n", "args", ".", "adaptive_input_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_input_cutoff'", ",", "None", ")", "\n", "\n", "args", ".", "tie_adaptive_weights", "=", "getattr", "(", "args", ",", "'tie_adaptive_weights'", ",", "False", ")", "\n", "args", ".", "tie_adaptive_proj", "=", "getattr", "(", "args", ",", "'tie_adaptive_proj'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'decoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "if", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "decoder_kernel_size_list", "=", "args", ".", "decoder_kernel_size_list", "*", "args", ".", "decoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_lm_gbw": [[828, 836], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture"], ["", "", "@", "register_model_architecture", "(", "'lightconv_lm'", ",", "'lightconv_lm_gbw'", ")", "\n", "def", "lightconv_lm_gbw", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.base_architecture": [[838, 881], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "7", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "args", ".", "encoder_conv_dim", "=", "getattr", "(", "args", ",", "'encoder_conv_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_conv_dim", "=", "getattr", "(", "args", ",", "'decoder_conv_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "args", ".", "encoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'encoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "args", ".", "decoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'decoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "if", "len", "(", "args", ".", "encoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "encoder_kernel_size_list", "=", "args", ".", "encoder_kernel_size_list", "*", "args", ".", "encoder_layers", "\n", "", "if", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "decoder_kernel_size_list", "=", "args", ".", "decoder_kernel_size_list", "*", "args", ".", "decoder_layers", "\n", "", "assert", "len", "(", "args", ".", "encoder_kernel_size_list", ")", "==", "args", ".", "encoder_layers", ",", "\"encoder_kernel_size_list doesn't match encoder_layers\"", "\n", "assert", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "args", ".", "decoder_layers", ",", "\"decoder_kernel_size_list doesn't match decoder_layers\"", "\n", "args", ".", "encoder_glu", "=", "getattr", "(", "args", ",", "'encoder_glu'", ",", "True", ")", "\n", "args", ".", "decoder_glu", "=", "getattr", "(", "args", ",", "'decoder_glu'", ",", "True", ")", "\n", "args", ".", "input_dropout", "=", "getattr", "(", "args", ",", "'input_dropout'", ",", "0.1", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "args", ".", "attention_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_iwslt_de_en": [[883, 899], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_iwslt_de_en'", ")", "\n", "def", "lightconv_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "7", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_glu", "=", "getattr", "(", "args", ",", "'encoder_glu'", ",", "False", ")", "\n", "args", ".", "decoder_glu", "=", "getattr", "(", "args", ",", "'decoder_glu'", ",", "False", ")", "\n", "args", ".", "input_dropout", "=", "getattr", "(", "args", ",", "'input_dropout'", ",", "0.0", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_en_de": [[901, 904], ["register_model_architecture", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_de'", ")", "\n", "def", "lightconv_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_en_de_big": [[906, 918], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_de_big'", ")", "\n", "def", "lightconv_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_en_fr_big": [[920, 924], ["register_model_architecture", "getattr", "lightconv.lightconv_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_fr_big'", ")", "\n", "def", "lightconv_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "lightconv_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_zh_en_big": [[926, 932], ["register_model_architecture", "getattr", "getattr", "getattr", "lightconv.lightconv_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.lightconv.lightconv_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_zh_en_big'", ")", "\n", "def", "lightconv_wmt_zh_en_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.2", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.2", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "0.2", ")", "\n", "lightconv_wmt_en_de_big", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvModel.__init__": [[44, 47], ["FairseqModel.__init__", "sum"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "self", ".", "encoder", ".", "num_attention_layers", "=", "sum", "(", "layer", "is", "not", "None", "for", "layer", "in", "decoder", ".", "attention", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvModel.add_args": [[48, 72], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share input and output embeddings (requires'", "\n", "' --decoder-out-embed-dim and --decoder-embed-dim'", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvModel.build_model": [[76, 112], ["fconv.base_architecture", "fconv.FConvEncoder", "fconv.FConvDecoder", "fconv.FConvModel", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "eval", "eval", "eval"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure that all args are properly defaulted (in case there are any new ones)", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "encoder_embed_dict", "=", "None", "\n", "if", "args", ".", "encoder_embed_path", ":", "\n", "            ", "encoder_embed_dict", "=", "utils", ".", "parse_embedding", "(", "args", ".", "encoder_embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "encoder_embed_dict", ",", "task", ".", "source_dictionary", ")", "\n", "\n", "", "decoder_embed_dict", "=", "None", "\n", "if", "args", ".", "decoder_embed_path", ":", "\n", "            ", "decoder_embed_dict", "=", "utils", ".", "parse_embedding", "(", "args", ".", "decoder_embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "decoder_embed_dict", ",", "task", ".", "target_dictionary", ")", "\n", "\n", "", "encoder", "=", "FConvEncoder", "(", "\n", "dictionary", "=", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "embed_dict", "=", "encoder_embed_dict", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "encoder_layers", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_source_positions", ",", "\n", ")", "\n", "decoder", "=", "FConvDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "embed_dict", "=", "decoder_embed_dict", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_target_positions", ",", "\n", "share_embed", "=", "args", ".", "share_input_output_embed", ",", "\n", ")", "\n", "return", "FConvModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvLanguageModel.__init__": [[116, 118], ["FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvLanguageModel.add_args": [[119, 137], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvLanguageModel.build_model": [[138, 164], ["fconv.base_lm_architecture", "hasattr", "fconv.FConvDecoder", "fconv.FConvLanguageModel", "eval", "eval", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "tokens_per_sample", "=", "args", ".", "max_target_positions", "\n", "\n", "", "decoder", "=", "FConvDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "tokens_per_sample", ",", "\n", "share_embed", "=", "False", ",", "\n", "positional_embeddings", "=", "False", ",", "\n", "adaptive_softmax_cutoff", "=", "(", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", "\n", "if", "args", ".", "criterion", "==", "'adaptive_loss'", "else", "None", "\n", ")", ",", "\n", "adaptive_softmax_dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", ")", "\n", "return", "FConvLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvEncoder.__init__": [[186, 235], ["FairseqEncoder.__init__", "len", "dictionary.pad", "fconv.Embedding", "fconv.PositionalEmbedding", "fconv.extend_conv_spec", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "fconv.Linear", "fairseq.utils.load_embedding", "fconv.FConvEncoder.projections.append", "fconv.FConvEncoder.convolutions.append", "fconv.FConvEncoder.residuals.append", "layer_in_channels.append", "fconv.ConvTBC", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.extend_conv_spec", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.ConvTBC", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "embed_dict", "=", "None", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "dropout", "=", "0.1", ",", "left_pad", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "num_attention_layers", "=", "None", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "if", "embed_dict", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "utils", ".", "load_embedding", "(", "embed_dict", ",", "self", ".", "dictionary", ",", "self", ".", "embed_tokens", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", "left_pad", "=", "self", ".", "left_pad", ",", "\n", ")", "\n", "\n", "convolutions", "=", "extend_conv_spec", "(", "convolutions", ")", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residuals", "=", "[", "]", "\n", "\n", "layer_in_channels", "=", "[", "in_channels", "]", "\n", "for", "_", ",", "(", "out_channels", ",", "kernel_size", ",", "residual", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "if", "residual", "==", "0", ":", "\n", "                ", "residual_dim", "=", "out_channels", "\n", "", "else", ":", "\n", "                ", "residual_dim", "=", "layer_in_channels", "[", "-", "residual", "]", "\n", "", "self", ".", "projections", ".", "append", "(", "Linear", "(", "residual_dim", ",", "out_channels", ")", "\n", "if", "residual_dim", "!=", "out_channels", "else", "None", ")", "\n", "if", "kernel_size", "%", "2", "==", "1", ":", "\n", "                ", "padding", "=", "kernel_size", "//", "2", "\n", "", "else", ":", "\n", "                ", "padding", "=", "0", "\n", "", "self", ".", "convolutions", ".", "append", "(", "\n", "ConvTBC", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "dropout", "=", "dropout", ",", "padding", "=", "padding", ")", "\n", ")", "\n", "self", ".", "residuals", ".", "append", "(", "residual", ")", "\n", "in_channels", "=", "out_channels", "\n", "layer_in_channels", ".", "append", "(", "out_channels", ")", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvEncoder.forward": [[236, 316], ["torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvEncoder.fc1", "src_tokens.eq().t", "conv.transpose", "zip", "conv.transpose", "fconv.FConvEncoder.fc2", "fairseq.modules.GradMultiply.apply", "fconv.FConvEncoder.embed_tokens", "fconv.FConvEncoder.embed_positions", "encoder_padding_mask.t.t.any", "torch.dropout", "torch.dropout", "torch.dropout", "torch.glu", "torch.glu", "torch.glu", "residuals.append", "encoder_padding_mask.t.t.t", "conv.masked_fill", "math.sqrt", "src_tokens.eq", "conv.masked_fill", "conv", "torch.pad", "torch.pad", "torch.pad", "conv", "encoder_padding_mask.t.t.unsqueeze", "proj", "encoder_padding_mask.t.t.unsqueeze", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (tuple): a tuple with two elements, where the\n                  first element is the last encoder layer's output and the\n                  second element is the same quantity summed with the input\n                  embedding (used for attention). The shape of both tensors is\n                  `(batch, src_len, embed_dim)`.\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "+", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "input_embedding", "=", "x", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# used to mask padding in input", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "# -> T x B", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "residuals", "=", "[", "x", "]", "\n", "# temporal convolutions", "\n", "for", "proj", ",", "conv", ",", "res_layer", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "residuals", ")", ":", "\n", "            ", "if", "res_layer", ">", "0", ":", "\n", "                ", "residual", "=", "residuals", "[", "-", "res_layer", "]", "\n", "residual", "=", "residual", "if", "proj", "is", "None", "else", "proj", "(", "residual", ")", "\n", "", "else", ":", "\n", "                ", "residual", "=", "None", "\n", "\n", "", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "                ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "conv", ".", "kernel_size", "[", "0", "]", "%", "2", "==", "1", ":", "\n", "# padding is implicit in the conv", "\n", "                ", "x", "=", "conv", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "padding_l", "=", "(", "conv", ".", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", "\n", "padding_r", "=", "conv", ".", "kernel_size", "[", "0", "]", "//", "2", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "padding_r", ")", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "if", "residual", "is", "not", "None", ":", "\n", "                ", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "", "residuals", ".", "append", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# project back to size of embedding", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_padding_mask", "=", "encoder_padding_mask", ".", "t", "(", ")", "# -> B x T", "\n", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "# scale gradients (this only affects backward, not forward)", "\n", "", "x", "=", "GradMultiply", ".", "apply", "(", "x", ",", "1.0", "/", "(", "2.0", "*", "self", ".", "num_attention_layers", ")", ")", "\n", "\n", "# add output to input embedding for attention", "\n", "y", "=", "(", "x", "+", "input_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "y", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvEncoder.reorder_encoder_out": [[318, 328], ["encoder_out[].index_select", "[].index_select", "[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "(", "\n", "encoder_out", "[", "'encoder_out'", "]", "[", "0", "]", ".", "index_select", "(", "0", ",", "new_order", ")", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "[", "1", "]", ".", "index_select", "(", "0", ",", "new_order", ")", ",", "\n", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvEncoder.max_positions": [[329, 332], ["fconv.FConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.AttentionLayer.__init__": [[335, 343], ["torch.Module.__init__", "fconv.Linear", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "conv_channels", ",", "embed_dim", ",", "bmm", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# projects from output of convolution to embedding dimension", "\n", "self", ".", "in_projection", "=", "Linear", "(", "conv_channels", ",", "embed_dim", ")", "\n", "# projects from embedding dimension to convolution size", "\n", "self", ".", "out_projection", "=", "Linear", "(", "embed_dim", ",", "conv_channels", ")", "\n", "\n", "self", ".", "bmm", "=", "bmm", "if", "bmm", "is", "not", "None", "else", "torch", ".", "bmm", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.AttentionLayer.forward": [[344, 378], ["fconv.AttentionLayer.bmm", "x.float().masked_fill().type_as.float().masked_fill().type_as.size", "torch.softmax", "torch.softmax", "torch.softmax", "x.float().masked_fill().type_as.float().masked_fill().type_as.view", "fconv.AttentionLayer.bmm", "encoder_out[].size", "math.sqrt", "x.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "x.float().masked_fill().type_as.float().masked_fill().type_as.view", "s.unsqueeze.unsqueeze.unsqueeze", "math.sqrt", "fconv.AttentionLayer.in_projection", "encoder_padding_mask.type_as().sum", "fconv.AttentionLayer.out_projection", "x.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "math.sqrt", "s.unsqueeze.unsqueeze.rsqrt", "encoder_padding_mask.unsqueeze", "float", "encoder_padding_mask.type_as", "x.float().masked_fill().type_as.float().masked_fill().type_as.float"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "forward", "(", "self", ",", "x", ",", "target_embedding", ",", "encoder_out", ",", "encoder_padding_mask", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "\n", "# attention", "\n", "x", "=", "(", "self", ".", "in_projection", "(", "x", ")", "+", "target_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "0", "]", ")", "\n", "\n", "# don't attend over padding", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "encoder_padding_mask", ".", "unsqueeze", "(", "1", ")", ",", "\n", "float", "(", "'-inf'", ")", "\n", ")", ".", "type_as", "(", "x", ")", "# FP16 support: cast to float and back", "\n", "\n", "# softmax over last dim", "\n", "", "sz", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "F", ".", "softmax", "(", "x", ".", "view", "(", "sz", "[", "0", "]", "*", "sz", "[", "1", "]", ",", "sz", "[", "2", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "sz", ")", "\n", "attn_scores", "=", "x", "\n", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "1", "]", ")", "\n", "\n", "# scale attention output (respecting potentially different lengths)", "\n", "s", "=", "encoder_out", "[", "1", "]", ".", "size", "(", "1", ")", "\n", "if", "encoder_padding_mask", "is", "None", ":", "\n", "            ", "x", "=", "x", "*", "(", "s", "*", "math", ".", "sqrt", "(", "1.0", "/", "s", ")", ")", "\n", "", "else", ":", "\n", "            ", "s", "=", "s", "-", "encoder_padding_mask", ".", "type_as", "(", "x", ")", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# exclude padding", "\n", "s", "=", "s", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "x", "*", "(", "s", "*", "s", ".", "rsqrt", "(", ")", ")", "\n", "\n", "# project back", "\n", "", "x", "=", "(", "self", ".", "out_projection", "(", "x", ")", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.AttentionLayer.make_generation_fast_": [[379, 384], ["fconv.AttentionLayer.add_module", "fairseq.modules.BeamableMM"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "beamable_mm_beam_size", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Replace torch.bmm with BeamableMM.\"\"\"", "\n", "if", "beamable_mm_beam_size", "is", "not", "None", ":", "\n", "            ", "del", "self", ".", "bmm", "\n", "self", ".", "add_module", "(", "'bmm'", ",", "BeamableMM", "(", "beamable_mm_beam_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.__init__": [[389, 465], ["FairseqIncrementalDecoder.__init__", "fconv.FConvDecoder.register_buffer", "fconv.extend_conv_spec", "isinstance", "len", "dictionary.pad", "fconv.Embedding", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "ValueError", "fairseq.utils.load_embedding", "fconv.PositionalEmbedding", "fconv.FConvDecoder.projections.append", "fconv.FConvDecoder.convolutions.append", "fconv.FConvDecoder.attention.append", "fconv.FConvDecoder.residuals.append", "layer_in_channels.append", "fairseq.modules.AdaptiveSoftmax", "fconv.Linear", "len", "isinstance", "len", "len", "fconv.LinearizedConv1d", "torch.Linear", "torch.Linear", "torch.Linear", "fconv.Linear", "fconv.Linear", "fconv.AttentionLayer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.extend_conv_spec", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.load_embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.LinearizedConv1d", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "embed_dict", "=", "None", ",", "out_embed_dim", "=", "256", ",", "\n", "max_positions", "=", "1024", ",", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "attention", "=", "True", ",", "\n", "dropout", "=", "0.1", ",", "share_embed", "=", "False", ",", "positional_embeddings", "=", "True", ",", "\n", "adaptive_softmax_cutoff", "=", "None", ",", "adaptive_softmax_dropout", "=", "0", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "convolutions", "=", "extend_conv_spec", "(", "convolutions", ")", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "if", "isinstance", "(", "attention", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "            ", "attention", "=", "[", "attention", "]", "*", "len", "(", "convolutions", ")", "\n", "", "if", "not", "isinstance", "(", "attention", ",", "list", ")", "or", "len", "(", "attention", ")", "!=", "len", "(", "convolutions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Attention is expected to be a list of booleans of '", "\n", "'length equal to the number of layers.'", ")", "\n", "\n", "", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "if", "embed_dict", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "utils", ".", "load_embedding", "(", "embed_dict", ",", "self", ".", "dictionary", ",", "self", ".", "embed_tokens", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", "left_pad", "=", "self", ".", "left_pad", ",", "\n", ")", "if", "positional_embeddings", "else", "None", "\n", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residuals", "=", "[", "]", "\n", "\n", "layer_in_channels", "=", "[", "in_channels", "]", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ",", "residual", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "if", "residual", "==", "0", ":", "\n", "                ", "residual_dim", "=", "out_channels", "\n", "", "else", ":", "\n", "                ", "residual_dim", "=", "layer_in_channels", "[", "-", "residual", "]", "\n", "", "self", ".", "projections", ".", "append", "(", "Linear", "(", "residual_dim", ",", "out_channels", ")", "\n", "if", "residual_dim", "!=", "out_channels", "else", "None", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "self", ".", "attention", ".", "append", "(", "AttentionLayer", "(", "out_channels", ",", "embed_dim", ")", "\n", "if", "attention", "[", "i", "]", "else", "None", ")", "\n", "self", ".", "residuals", ".", "append", "(", "residual", ")", "\n", "in_channels", "=", "out_channels", "\n", "layer_in_channels", ".", "append", "(", "out_channels", ")", "\n", "\n", "", "self", ".", "adaptive_softmax", "=", "None", "\n", "self", ".", "fc2", "=", "self", ".", "fc3", "=", "None", "\n", "\n", "if", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "assert", "not", "share_embed", "\n", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "num_embeddings", ",", "in_channels", ",", "adaptive_softmax_cutoff", ",", "\n", "dropout", "=", "adaptive_softmax_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "out_embed_dim", ")", "\n", "if", "share_embed", ":", "\n", "                ", "assert", "out_embed_dim", "==", "embed_dim", ",", "\"Shared embed weights implies same dimensions \"", "\" out_embed_dim={} vs embed_dim={}\"", ".", "format", "(", "out_embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ")", "\n", "self", ".", "fc3", ".", "weight", "=", "self", ".", "embed_tokens", ".", "weight", "\n", "", "else", ":", "\n", "                ", "self", ".", "fc3", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.forward": [[466, 540], ["fconv.FConvDecoder._embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc1", "fconv.FConvDecoder._transpose_if_training", "len", "zip", "fconv.FConvDecoder._transpose_if_training", "fconv.FConvDecoder._split_encoder_out", "fconv.FConvDecoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "conv", "torch.glu", "torch.glu", "torch.glu", "residuals.append", "fconv.FConvDecoder.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc3", "fconv.FConvDecoder._transpose_if_training", "attention", "fconv.FConvDecoder._transpose_if_training", "proj", "math.sqrt", "avg_attn_scores.add_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._embed_tokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._split_encoder_out", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._transpose_if_training"], ["", "", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out_dict", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "encoder_out_dict", "is", "not", "None", ":", "\n", "            ", "encoder_out", "=", "encoder_out_dict", "[", "'encoder_out'", "]", "\n", "encoder_padding_mask", "=", "encoder_out_dict", "[", "'encoder_padding_mask'", "]", "\n", "\n", "# split and transpose encoder outputs", "\n", "encoder_a", ",", "encoder_b", "=", "self", ".", "_split_encoder_out", "(", "encoder_out", ",", "incremental_state", ")", "\n", "\n", "", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "pos_embed", "=", "self", ".", "embed_positions", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "", "else", ":", "\n", "            ", "pos_embed", "=", "0", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "x", "=", "self", ".", "_embed_tokens", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "\n", "# embed tokens and combine with positional embeddings", "\n", "x", "+=", "pos_embed", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "target_embedding", "=", "x", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# temporal convolutions", "\n", "avg_attn_scores", "=", "None", "\n", "num_attn_layers", "=", "len", "(", "self", ".", "attention", ")", "\n", "residuals", "=", "[", "x", "]", "\n", "for", "proj", ",", "conv", ",", "attention", ",", "res_layer", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ",", "\n", "self", ".", "residuals", ")", ":", "\n", "            ", "if", "res_layer", ">", "0", ":", "\n", "                ", "residual", "=", "residuals", "[", "-", "res_layer", "]", "\n", "residual", "=", "residual", "if", "proj", "is", "None", "else", "proj", "(", "residual", ")", "\n", "", "else", ":", "\n", "                ", "residual", "=", "None", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "conv", "(", "x", ",", "incremental_state", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "# attention", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "x", ",", "attn_scores", "=", "attention", "(", "x", ",", "target_embedding", ",", "(", "encoder_a", ",", "encoder_b", ")", ",", "encoder_padding_mask", ")", "\n", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "                    ", "attn_scores", "=", "attn_scores", "/", "num_attn_layers", "\n", "if", "avg_attn_scores", "is", "None", ":", "\n", "                        ", "avg_attn_scores", "=", "attn_scores", "\n", "", "else", ":", "\n", "                        ", "avg_attn_scores", ".", "add_", "(", "attn_scores", ")", "\n", "\n", "", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# residual", "\n", "", "if", "residual", "is", "not", "None", ":", "\n", "                ", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "", "residuals", ".", "append", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# project back to size of vocabulary if not using adaptive softmax", "\n", "if", "self", ".", "fc2", "is", "not", "None", "and", "self", ".", "fc3", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n", "", "return", "x", ",", "avg_attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state": [[541, 547], ["super().reorder_incremental_state", "fairseq.utils.get_incremental_state", "tuple", "fairseq.utils.set_incremental_state", "eo.index_select"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.reorder_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "super", "(", ")", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "encoder_out", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ")", "\n", "if", "encoder_out", "is", "not", "None", ":", "\n", "            ", "encoder_out", "=", "tuple", "(", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "for", "eo", "in", "encoder_out", ")", "\n", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ",", "encoder_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.max_positions": [[548, 551], ["fconv.FConvDecoder.embed_positions.max_positions", "float"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "float", "(", "'inf'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.upgrade_state_dict": [[552, 561], ["fairseq.utils.item", "enumerate", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "'decoder.version'", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# old models use incorrect weight norm dimension", "\n", "            ", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "convolutions", ")", ":", "\n", "# reconfigure weight norm", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "conv", ")", "\n", "self", ".", "convolutions", "[", "i", "]", "=", "nn", ".", "utils", ".", "weight_norm", "(", "conv", ",", "dim", "=", "0", ")", "\n", "", "state_dict", "[", "'decoder.version'", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder.make_generation_fast_": [[562, 564], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._embed_tokens": [[565, 570], ["fconv.FConvDecoder.embed_tokens"], "methods", ["None"], ["", "def", "_embed_tokens", "(", "self", ",", "tokens", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# keep only the last token for incremental forward pass", "\n", "            ", "tokens", "=", "tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "return", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._split_encoder_out": [[571, 588], ["fairseq.utils.get_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose().contiguous", "fairseq.utils.set_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.set_incremental_state"], ["", "def", "_split_encoder_out", "(", "self", ",", "encoder_out", ",", "incremental_state", ")", ":", "\n", "        ", "\"\"\"Split and transpose encoder outputs.\n\n        This is cached when doing incremental inference.\n        \"\"\"", "\n", "cached_result", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ")", "\n", "if", "cached_result", "is", "not", "None", ":", "\n", "            ", "return", "cached_result", "\n", "\n", "# transpose only once to speed up attention layers", "\n", "", "encoder_a", ",", "encoder_b", "=", "encoder_out", "\n", "encoder_a", "=", "encoder_a", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "result", "=", "(", "encoder_a", ",", "encoder_b", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ",", "result", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.FConvDecoder._transpose_if_training": [[589, 593], ["x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "_transpose_if_training", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "None", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.extend_conv_spec": [[595, 610], ["tuple", "len", "extended.append", "len", "extended.append", "Exception", "str"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "extend_conv_spec", "(", "convolutions", ")", ":", "\n", "    ", "\"\"\"\n    Extends convolutional spec that is a list of tuples of 2 or 3 parameters\n    (kernel size, dim size and optionally how many layers behind to look for residual)\n    to default the residual propagation param if it is not specified\n    \"\"\"", "\n", "extended", "=", "[", "]", "\n", "for", "spec", "in", "convolutions", ":", "\n", "        ", "if", "len", "(", "spec", ")", "==", "3", ":", "\n", "            ", "extended", ".", "append", "(", "spec", ")", "\n", "", "elif", "len", "(", "spec", ")", "==", "2", ":", "\n", "            ", "extended", ".", "append", "(", "spec", "+", "(", "1", ",", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'invalid number of parameters in convolution spec '", "+", "str", "(", "spec", ")", "+", "'. expected 2 or 3'", ")", "\n", "", "", "return", "tuple", "(", "extended", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.Embedding": [[612, 617], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.PositionalEmbedding": [[619, 624], ["fairseq.modules.LearnedPositionalEmbedding", "torch.init.normal_", "torch.init.constant_"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", ":", "\n", "    ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.Linear": [[626, 632], ["torch.Linear", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.LinearizedConv1d": [[634, 641], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm"], "function", ["None"], ["", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.ConvTBC": [[643, 651], ["fconv.ConvTBC", "math.sqrt", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.ConvTBC"], ["", "def", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer\"\"\"", "\n", "from", "fairseq", ".", "modules", "import", "ConvTBC", "\n", "m", "=", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.base_lm_architecture": [[653, 661], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "128", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(1268, 4)] * 13'", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_lm_dauphin_wikitext103": [[663, 677], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "fconv.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm_dauphin_wikitext103'", ")", "\n", "def", "fconv_lm_dauphin_wikitext103", "(", "args", ")", ":", "\n", "    ", "layers", "=", "'[(850, 6)] * 3'", "\n", "layers", "+=", "' + [(850, 1)] * 1'", "\n", "layers", "+=", "' + [(850, 5)] * 4'", "\n", "layers", "+=", "' + [(850, 1)] * 1'", "\n", "layers", "+=", "' + [(850, 4)] * 3'", "\n", "layers", "+=", "' + [(1024, 4)] * 1'", "\n", "layers", "+=", "' + [(2048, 4)] * 1'", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "280", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "layers", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,20000,200000'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_lm_dauphin_gbw": [[679, 691], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "fconv.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm_dauphin_gbw'", ")", "\n", "def", "fconv_lm_dauphin_gbw", "(", "args", ")", ":", "\n", "    ", "layers", "=", "'[(512, 5)]'", "\n", "layers", "+=", "' + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3'", "\n", "layers", "+=", "' + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3'", "\n", "layers", "+=", "' + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)] * 6'", "\n", "layers", "+=", "' + [(1024, 1, 0), (1024, 5, 0), (4096, 1, 3)]'", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "128", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "layers", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,50000,200000'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.base_architecture": [[693, 705], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'True'", ")", "\n", "args", ".", "share_input_output_embed", "=", "getattr", "(", "args", ",", "'share_input_output_embed'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_iwslt_de_en": [[707, 715], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_iwslt_de_en'", ")", "\n", "def", "fconv_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(256, 3)] * 4'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(256, 3)] * 3'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_wmt_en_ro": [[717, 721], ["register_model_architecture", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_ro'", ")", "\n", "def", "fconv_wmt_en_ro", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_wmt_en_de": [[723, 735], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_de'", ")", "\n", "def", "fconv_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "convs", "=", "'[(512, 3)] * 9'", "# first 9 layers have 512 units", "\n", "convs", "+=", "' + [(1024, 3)] * 4'", "# next 4 layers have 1024 units", "\n", "convs", "+=", "' + [(2048, 1)] * 2'", "# final 2 layers use 1x1 convolutions", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fconv.fconv_wmt_en_fr": [[737, 751], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_fr'", ")", "\n", "def", "fconv_wmt_en_fr", "(", "args", ")", ":", "\n", "    ", "convs", "=", "'[(512, 3)] * 6'", "# first 6 layers have 512 units", "\n", "convs", "+=", "' + [(768, 3)] * 4'", "# next 4 layers have 768 units", "\n", "convs", "+=", "' + [(1024, 3)] * 3'", "# next 3 layers have 1024 units", "\n", "convs", "+=", "' + [(2048, 1)] * 1'", "# next 1 layer uses 1x1 convolutions", "\n", "convs", "+=", "' + [(4096, 1)] * 1'", "# final 1 layer uses 1x1 convolutions", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.__init__": [[20, 23], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_is_generation_fast", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.add_args": [[24, 28], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.build_model": [[29, 33], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "raise", "NotImplementedError", "(", "'FairseqModels must implement the build_model method'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_targets": [[34, 37], ["None"], "methods", ["None"], ["", "def", "get_targets", "(", "self", ",", "sample", ",", "net_output", ")", ":", "\n", "        ", "\"\"\"Get targets from either the sample or the net's output.\"\"\"", "\n", "return", "sample", "[", "'target'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_normalized_probs": [[38, 49], ["hasattr", "fairseq_model.BaseFairseqModel.decoder.get_normalized_probs", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "net_output.float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "if", "hasattr", "(", "self", ",", "'decoder'", ")", ":", "\n", "            ", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "net_output", ")", ":", "\n", "            ", "logits", "=", "net_output", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "                ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.max_positions": [[50, 53], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.max_decoder_positions": [[54, 57], ["fairseq_model.BaseFairseqModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.load_state_dict": [[58, 67], ["fairseq_model.BaseFairseqModel.upgrade_state_dict", "super().load_state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "\"\"\"Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n        \"\"\"", "\n", "self", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "super", "(", ")", ".", "load_state_dict", "(", "state_dict", ",", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.upgrade_state_dict": [[68, 71], ["fairseq_model.BaseFairseqModel.upgrade_state_dict_named"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.upgrade_state_dict_named"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade old state dicts to work with newer code.\"\"\"", "\n", "self", ".", "upgrade_state_dict_named", "(", "state_dict", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.upgrade_state_dict_named": [[72, 94], ["fairseq_model.BaseFairseqModel.upgrade_state_dict_named.do_upgrade"], "methods", ["None"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade old state dicts to work with newer code.\n\n        Args:\n            state_dict (dict): state dictionary to upgrade, in place\n            name (str): the state dict key corresponding to the current module\n        \"\"\"", "\n", "assert", "state_dict", "is", "not", "None", "\n", "\n", "def", "do_upgrade", "(", "m", ",", "prefix", ")", ":", "\n", "            ", "if", "len", "(", "prefix", ")", ">", "0", ":", "\n", "                ", "prefix", "+=", "'.'", "\n", "\n", "", "for", "n", ",", "c", "in", "m", ".", "named_children", "(", ")", ":", "\n", "                ", "name", "=", "prefix", "+", "n", "\n", "if", "hasattr", "(", "c", ",", "'upgrade_state_dict_named'", ")", ":", "\n", "                    ", "c", ".", "upgrade_state_dict_named", "(", "state_dict", ",", "name", ")", "\n", "", "elif", "hasattr", "(", "c", ",", "'upgrade_state_dict'", ")", ":", "\n", "                    ", "c", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "", "do_upgrade", "(", "c", ",", "name", ")", "\n", "\n", "", "", "do_upgrade", "(", "self", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.make_generation_fast_": [[95, 127], ["fairseq_model.BaseFairseqModel.apply", "set", "fairseq_model.BaseFairseqModel.apply", "fairseq_model.BaseFairseqModel.eval", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "hasattr", "set.add", "module.make_generation_fast_", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_"], ["", "def", "make_generation_fast_", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Optimize model for faster generation.\"\"\"", "\n", "if", "self", ".", "_is_generation_fast", ":", "\n", "            ", "return", "# only apply once", "\n", "", "self", ".", "_is_generation_fast", "=", "True", "\n", "\n", "# remove weight norm from all modules in the network", "\n", "def", "apply_remove_weight_norm", "(", "module", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "module", ")", "\n", "", "except", "ValueError", ":", "# this module didn't have weight norm", "\n", "                ", "return", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_remove_weight_norm", ")", "\n", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_make_generation_fast_", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'make_generation_fast_'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "make_generation_fast_", "(", "**", "kwargs", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_make_generation_fast_", ")", "\n", "\n", "def", "train", "(", "mode", "=", "True", ")", ":", "\n", "            ", "if", "mode", ":", "\n", "                ", "raise", "RuntimeError", "(", "'cannot train after make_generation_fast'", ")", "\n", "\n", "# this model should no longer be used for training", "\n", "", "", "self", ".", "eval", "(", ")", "\n", "self", ".", "train", "=", "train", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.prepare_for_onnx_export_": [[128, 139], ["set", "fairseq_model.BaseFairseqModel.apply", "hasattr", "set.add", "module.prepare_for_onnx_export_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.prepare_for_onnx_export_"], ["", "def", "prepare_for_onnx_export_", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Make model exportable via ONNX trace.\"\"\"", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_prepare_for_onnx_export_", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'prepare_for_onnx_export_'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "prepare_for_onnx_export_", "(", "**", "kwargs", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_prepare_for_onnx_export_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqModel.__init__": [[149, 156], ["fairseq_model.BaseFairseqModel.__init__", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "assert", "isinstance", "(", "self", ".", "encoder", ",", "FairseqEncoder", ")", "\n", "assert", "isinstance", "(", "self", ".", "decoder", ",", "FairseqDecoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqModel.forward": [[157, 181], ["fairseq_model.FairseqModel.encoder", "fairseq_model.FairseqModel.decoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "prev_output_tokens", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward pass for an encoder-decoder model.\n\n        First feed a batch of source tokens through the encoder. Then, feed the\n        encoder output and previous decoder outputs (i.e., input feeding/teacher\n        forcing) to the decoder to produce the next outputs::\n\n            encoder_out = self.encoder(src_tokens, src_lengths)\n            return self.decoder(prev_output_tokens, encoder_out)\n\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n\n        Returns:\n            the decoder's output, typically of shape `(batch, tgt_len, vocab)`\n        \"\"\"", "\n", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", ")", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", ")", "\n", "return", "decoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqModel.max_positions": [[182, 185], ["fairseq_model.FairseqModel.encoder.max_positions", "fairseq_model.FairseqModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "(", "self", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.__init__": [[189, 200], ["fairseq_model.BaseFairseqModel.__init__", "list", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "encoders.keys", "decoders.keys", "encoders.keys", "isinstance", "isinstance", "fairseq_model.FairseqModel"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoders", ",", "decoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "encoders", ".", "keys", "(", ")", "==", "decoders", ".", "keys", "(", ")", "\n", "self", ".", "keys", "=", "list", "(", "encoders", ".", "keys", "(", ")", ")", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "assert", "isinstance", "(", "encoders", "[", "key", "]", ",", "FairseqEncoder", ")", "\n", "assert", "isinstance", "(", "decoders", "[", "key", "]", ",", "FairseqDecoder", ")", "\n", "\n", "", "self", ".", "models", "=", "nn", ".", "ModuleDict", "(", "{", "\n", "key", ":", "FairseqModel", "(", "encoders", "[", "key", "]", ",", "decoders", "[", "key", "]", ")", "\n", "for", "key", "in", "self", ".", "keys", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.build_shared_embeddings": [[202, 232], ["any", "build_embedding", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_shared_embeddings", "(", "\n", "dicts", ":", "Dict", "[", "str", ",", "Dictionary", "]", ",", "\n", "langs", ":", "List", "[", "str", "]", ",", "\n", "embed_dim", ":", "int", ",", "\n", "build_embedding", ":", "callable", ",", "\n", "pretrained_embed_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Helper function to build shared embeddings for a set of languages after\n        checking that all dicts corresponding to those languages are equivalent.\n\n        Args:\n            dicts: Dict of lang_id to its corresponding Dictionary\n            langs: languages that we want to share embeddings for\n            embed_dim: embedding dimension\n            build_embedding: callable function to actually build the embedding\n            pretrained_embed_path: Optional path to load pretrained embeddings\n        \"\"\"", "\n", "shared_dict", "=", "dicts", "[", "langs", "[", "0", "]", "]", "\n", "if", "any", "(", "dicts", "[", "lang", "]", "!=", "shared_dict", "for", "lang", "in", "langs", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'--share-*-embeddings requires a joined dictionary: '", "\n", "'--share-encoder-embeddings requires a joined source '", "\n", "'dictionary, --share-decoder-embeddings requires a joined '", "\n", "'target dictionary, and --share-all-embeddings requires a '", "\n", "'joint source + target dictionary.'", "\n", ")", "\n", "", "return", "build_embedding", "(", "\n", "shared_dict", ",", "embed_dim", ",", "pretrained_embed_path", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.forward": [[234, 240], ["fairseq_model.FairseqMultiModel.models[].encoder", "fairseq_model.FairseqMultiModel.models[].decoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "prev_output_tokens", ")", ":", "\n", "        ", "decoder_outs", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "encoder_out", "=", "self", ".", "models", "[", "key", "]", ".", "encoder", "(", "src_tokens", ",", "src_lengths", ")", "\n", "decoder_outs", "[", "key", "]", "=", "self", ".", "models", "[", "key", "]", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", ")", "\n", "", "return", "decoder_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.max_positions": [[241, 246], ["fairseq_model.FairseqMultiModel.models[].encoder.max_positions", "fairseq_model.FairseqMultiModel.models[].decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "{", "\n", "key", ":", "(", "self", ".", "models", "[", "key", "]", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "models", "[", "key", "]", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "for", "key", "in", "self", ".", "keys", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.max_decoder_positions": [[248, 251], ["min", "model.decoder.max_positions", "fairseq_model.FairseqMultiModel.models.values"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the decoder.\"\"\"", "\n", "return", "min", "(", "model", ".", "decoder", ".", "max_positions", "(", ")", "for", "model", "in", "self", ".", "models", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.encoder": [[252, 255], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "models", "[", "self", ".", "keys", "[", "0", "]", "]", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder": [[256, 259], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "models", "[", "self", ".", "keys", "[", "0", "]", "]", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqLanguageModel.__init__": [[268, 272], ["fairseq_model.BaseFairseqModel.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decoder", "=", "decoder", "\n", "assert", "isinstance", "(", "self", ".", "decoder", ",", "FairseqDecoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqLanguageModel.forward": [[273, 288], ["fairseq_model.FairseqLanguageModel.decoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward pass for a decoder-only model.\n\n        Feeds a batch of tokens through the decoder to predict the next tokens.\n\n        Args:\n            src_tokens (LongTensor): tokens on which to condition the decoder,\n                of shape `(batch, tgt_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            the decoder's output, typically of shape `(batch, seq_len, vocab)`\n        \"\"\"", "\n", "return", "self", ".", "decoder", "(", "src_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqLanguageModel.max_positions": [[289, 292], ["fairseq_model.FairseqLanguageModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqLanguageModel.supported_targets": [[293, 296], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supported_targets", "(", "self", ")", ":", "\n", "        ", "return", "{", "'future'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqLanguageModel.remove_head": [[297, 300], ["NotImplementedError"], "methods", ["None"], ["", "def", "remove_head", "(", "self", ")", ":", "\n", "        ", "\"\"\"Removes the head of the model (e.g. the softmax layer) to conserve space when it is not needed\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.build_model": [[32, 34], ["ARCH_MODEL_REGISTRY[].build_model"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model": [[36, 64], ["ValueError", "issubclass", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture": [[66, 104], ["ARCH_MODEL_INV_REGISTRY.setdefault().append", "ValueError", "ValueError", "callable", "ValueError", "ARCH_MODEL_INV_REGISTRY.setdefault"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Tags.setdefault"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.composite_encoder.CompositeEncoder.__init__": [[22, 27], ["FairseqEncoder.__init__", "composite_encoder.CompositeEncoder.add_module", "next", "iter", "encoders.values"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["def", "__init__", "(", "self", ",", "encoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "next", "(", "iter", "(", "encoders", ".", "values", "(", ")", ")", ")", ".", "dictionary", ")", "\n", "self", ".", "encoders", "=", "encoders", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "self", ".", "add_module", "(", "key", ",", "self", ".", "encoders", "[", "key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.composite_encoder.CompositeEncoder.forward": [[28, 44], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                the outputs from each Encoder\n        \"\"\"", "\n", "encoder_out", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "encoder_out", "[", "key", "]", "=", "self", ".", "encoders", "[", "key", "]", "(", "src_tokens", ",", "src_lengths", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.composite_encoder.CompositeEncoder.reorder_encoder_out": [[45, 50], ["composite_encoder.CompositeEncoder.encoders[].reorder_encoder_out"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder encoder output according to new_order.\"\"\"", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "encoder_out", "[", "key", "]", "=", "self", ".", "encoders", "[", "key", "]", ".", "reorder_encoder_out", "(", "encoder_out", "[", "key", "]", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.composite_encoder.CompositeEncoder.max_positions": [[51, 53], ["min", "composite_encoder.CompositeEncoder.encoders[].max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "min", "(", "[", "self", ".", "encoders", "[", "key", "]", ".", "max_positions", "(", ")", "for", "key", "in", "self", ".", "encoders", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.composite_encoder.CompositeEncoder.upgrade_state_dict": [[54, 58], ["composite_encoder.CompositeEncoder.encoders[].upgrade_state_dict"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "self", ".", "encoders", "[", "key", "]", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.__init__": [[15, 18], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.forward": [[19, 35], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape\n                  `(batch, tgt_len, vocab)`\n                - the last decoder layer's attention weights of shape\n                  `(batch, tgt_len, src_len)`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs": [[36, 53], ["net_output[].float", "hasattr", "fairseq_decoder.FairseqDecoder.adaptive_softmax.get_log_prob", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "fairseq_decoder.FairseqDecoder.exp_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.adaptive_softmax.AdaptiveSoftmax.get_log_prob"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "\n", "if", "hasattr", "(", "self", ",", "'adaptive_softmax'", ")", "and", "self", ".", "adaptive_softmax", "is", "not", "None", ":", "\n", "            ", "if", "sample", "is", "not", "None", ":", "\n", "                ", "assert", "'target'", "in", "sample", "\n", "target", "=", "sample", "[", "'target'", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "None", "\n", "", "out", "=", "self", ".", "adaptive_softmax", ".", "get_log_prob", "(", "net_output", "[", "0", "]", ",", "target", "=", "target", ")", "\n", "return", "out", ".", "exp_", "(", ")", "if", "not", "log_probs", "else", "out", "\n", "\n", "", "logits", "=", "net_output", "[", "0", "]", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "            ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.max_positions": [[54, 57], ["None"], "methods", ["None"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the decoder.\"\"\"", "\n", "return", "1e6", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict": [[58, 61], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerModel.__init__": [[46, 48], ["FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerModel.add_args": [[49, 99], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerModel.build_model": [[101, 150], ["transformer.base_architecture", "transformer.TransformerEncoder", "transformer.TransformerDecoder", "transformer.TransformerModel", "hasattr", "hasattr", "len", "dictionary.pad", "transformer.Embedding", "transformer.TransformerModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "encoder", "=", "TransformerEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "TransformerDecoder", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ")", "\n", "return", "TransformerModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerLanguageModel.__init__": [[154, 156], ["FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerLanguageModel.add_args": [[157, 213], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-output-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-input-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder input dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses character embedding convolutions to produce token embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-filters'", ",", "type", "=", "str", ",", "metavar", "=", "'LIST'", ",", "\n", "default", "=", "'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]'", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embedding-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "4", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--char-embedder-highway-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "2", ",", "\n", "help", "=", "'number of highway layers for character token embeddder'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive input cutoff points.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-weights'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-proj'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the projection weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerLanguageModel.build_model": [[215, 257], ["transformer.base_lm_architecture", "transformer.TransformerDecoder", "transformer.TransformerLanguageModel", "hasattr", "hasattr", "hasattr", "fairseq.modules.CharacterTokenEmbedder", "eval", "fairseq.modules.AdaptiveInput", "transformer.Embedding", "len", "task.dictionary.pad", "fairseq.options.eval_str_list", "len", "task.dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.tasks.translation_moe.eval", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "hasattr", "(", "args", ",", "'no_tie_adaptive_proj'", ")", "and", "args", ".", "no_tie_adaptive_proj", "is", "False", ":", "\n", "# backward compatibility", "\n", "            ", "args", ".", "tie_adaptive_proj", "=", "True", "\n", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "args", ".", "tokens_per_sample", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "args", ".", "tokens_per_sample", "\n", "\n", "", "if", "args", ".", "character_embeddings", ":", "\n", "            ", "embed_tokens", "=", "CharacterTokenEmbedder", "(", "\n", "task", ".", "dictionary", ",", "eval", "(", "args", ".", "character_filters", ")", ",", "\n", "args", ".", "character_embedding_dim", ",", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "char_embedder_highway_layers", ",", "\n", ")", "\n", "", "elif", "args", ".", "adaptive_input", ":", "\n", "            ", "embed_tokens", "=", "AdaptiveInput", "(", "\n", "len", "(", "task", ".", "dictionary", ")", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ",", "args", ".", "decoder_input_dim", ",", "\n", "args", ".", "adaptive_input_factor", ",", "args", ".", "decoder_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_input_cutoff", ",", "type", "=", "int", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "embed_tokens", "=", "Embedding", "(", "len", "(", "task", ".", "dictionary", ")", ",", "args", ".", "decoder_input_dim", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ")", "\n", "\n", "", "if", "args", ".", "tie_adaptive_weights", ":", "\n", "            ", "assert", "args", ".", "adaptive_input", "\n", "assert", "args", ".", "adaptive_input_factor", "==", "args", ".", "adaptive_softmax_factor", "\n", "assert", "args", ".", "adaptive_softmax_cutoff", "==", "args", ".", "adaptive_input_cutoff", ",", "'{} != {}'", ".", "format", "(", "\n", "args", ".", "adaptive_softmax_cutoff", ",", "args", ".", "adaptive_input_cutoff", ")", "\n", "assert", "args", ".", "decoder_input_dim", "==", "args", ".", "decoder_output_dim", "\n", "\n", "", "decoder", "=", "TransformerDecoder", "(", "\n", "args", ",", "task", ".", "output_dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "True", ",", "final_norm", "=", "False", ",", "\n", ")", "\n", "return", "TransformerLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.__init__": [[272, 297], ["FairseqEncoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.TransformerEncoder.layers.extend", "transformer.TransformerEncoder.register_buffer", "transformer.PositionalEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "transformer.LayerNorm", "transformer.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "left_pad", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerEncoderLayer", "(", "args", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "encoder_normalize_before", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.forward": [[298, 337], ["torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerEncoder.transpose", "src_tokens.eq", "transformer.TransformerEncoder.embed_tokens", "transformer.TransformerEncoder.embed_positions", "src_tokens.eq.any", "layer", "transformer.TransformerEncoder.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "encoder_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.reorder_encoder_out": [[339, 357], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.max_positions": [[358, 363], ["min", "transformer.TransformerEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoder.upgrade_state_dict_named": [[364, 378], ["isinstance", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "", "version_key", "=", "'{}.version'", ".", "format", "(", "name", ")", "\n", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "version_key", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "version_key", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.__init__": [[397, 448], ["FairseqIncrementalDecoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.TransformerDecoder.layers.extend", "transformer.TransformerDecoder.register_buffer", "transformer.Linear", "transformer.PositionalEmbedding", "transformer.Linear", "fairseq.modules.AdaptiveSoftmax", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "transformer.LayerNorm", "transformer.TransformerDecoderLayer", "len", "fairseq.options.eval_str_list", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ",", "left_pad", "=", "False", ",", "final_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "# todo: try with input_embed_dim", "\n", "\n", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "left_pad", "=", "left_pad", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerDecoderLayer", "(", "args", ",", "no_encoder_attn", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "args", ".", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "\n", "len", "(", "dictionary", ")", ",", "\n", "output_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", ",", "\n", "dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", "adaptive_inputs", "=", "embed_tokens", "if", "args", ".", "tie_adaptive_weights", "else", "None", ",", "\n", "factor", "=", "args", ".", "adaptive_softmax_factor", ",", "\n", "tie_proj", "=", "args", ".", "tie_adaptive_proj", ",", "\n", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "output_embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "output_embed_dim", "**", "-", "0.5", ")", "\n", "", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "decoder_normalize_before", "and", "final_norm", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.forward": [[449, 521], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.linear.transpose", "torch.linear.transpose", "transformer.TransformerDecoder.embed_positions", "transformer.TransformerDecoder.embed_tokens", "transformer.TransformerDecoder.project_in_dim", "layer", "inner_states.append", "transformer.TransformerDecoder.layer_norm", "transformer.TransformerDecoder.project_out_dim", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "transformer.TransformerDecoder.buffered_future_mask"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.buffered_future_mask"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", ",", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "# project back to size of vocabulary", "\n", "            ", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_out", ")", "\n", "\n", "", "", "return", "x", ",", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions": [[522, 527], ["min", "transformer.TransformerDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.buffered_future_mask": [[528, 535], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "transformer.TransformerDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "transformer.TransformerDecoder._future_mask.resize_"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoder.upgrade_state_dict_named": [[536, 564], ["isinstance", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "layer_norm_map.items", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "# update layer norms", "\n", "            ", "layer_norm_map", "=", "{", "\n", "'0'", ":", "'self_attn_layer_norm'", ",", "\n", "'1'", ":", "'encoder_attn_layer_norm'", ",", "\n", "'2'", ":", "'final_layer_norm'", "\n", "}", "\n", "for", "old", ",", "new", "in", "layer_norm_map", ".", "items", "(", ")", ":", "\n", "                ", "for", "m", "in", "(", "'weight'", ",", "'bias'", ")", ":", "\n", "                    ", "k", "=", "'{}.layers.{}.layer_norms.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "old", ",", "m", ")", "\n", "if", "k", "in", "state_dict", ":", "\n", "                        ", "state_dict", "[", "'{}.layers.{}.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "new", ",", "m", ")", "]", "=", "state_dict", "[", "k", "]", "\n", "del", "state_dict", "[", "k", "]", "\n", "", "", "", "", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "'{}.version'", ".", "format", "(", "name", ")", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "'{}.version'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoderLayer.__init__": [[581, 594], ["torch.Module.__init__", "fairseq.modules.MultiheadAttention", "transformer.Linear", "transformer.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.LayerNorm", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "encoder_embed_dim", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "encoder_normalize_before", "\n", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "encoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "layer_norms", "=", "nn", ".", "ModuleList", "(", "[", "LayerNorm", "(", "self", ".", "embed_dim", ")", "for", "i", "in", "range", "(", "2", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoderLayer.forward": [[595, 621], ["transformer.TransformerEncoderLayer.maybe_layer_norm", "transformer.TransformerEncoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerEncoderLayer.maybe_layer_norm", "transformer.TransformerEncoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerEncoderLayer.maybe_layer_norm", "transformer.TransformerEncoderLayer.fc1"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_padding_mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "before", "=", "True", ")", "\n", "x", ",", "_", "=", "self", ".", "self_attn", "(", "query", "=", "x", ",", "key", "=", "x", ",", "value", "=", "x", ",", "key_padding_mask", "=", "encoder_padding_mask", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerEncoderLayer.maybe_layer_norm": [[622, 628], ["None"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "i", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "self", ".", "layer_norms", "[", "i", "]", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.__init__": [[647, 677], ["torch.Module.__init__", "fairseq.modules.MultiheadAttention", "transformer.LayerNorm", "transformer.Linear", "transformer.Linear", "transformer.LayerNorm", "fairseq.modules.MultiheadAttention", "transformer.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "decoder_normalize_before", "\n", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "decoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.prepare_for_onnx_export_": [[678, 680], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.forward": [[681, 750], ["transformer.TransformerDecoderLayer.maybe_layer_norm", "transformer.TransformerDecoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerDecoderLayer.maybe_layer_norm", "transformer.TransformerDecoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerDecoderLayer.maybe_layer_norm", "transformer.TransformerDecoderLayer.self_attn._set_input_buffer", "transformer.TransformerDecoderLayer.maybe_layer_norm", "transformer.TransformerDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerDecoderLayer.maybe_layer_norm", "transformer.TransformerDecoderLayer.fc1", "transformer.TransformerDecoderLayer.self_attn._get_input_buffer", "transformer.TransformerDecoderLayer.encoder_attn._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.kakaobrain_helo_word.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_out", ",", "encoder_padding_mask", ",", "incremental_state", ",", "\n", "prev_self_attn_state", "=", "None", ",", "prev_attn_state", "=", "None", ",", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "_", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "attn", "=", "None", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "                ", "if", "incremental_state", "is", "None", ":", "\n", "                    ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "saved_state", "=", "self", ".", "self_attn", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "self_attn_state", "=", "saved_state", "[", "\"prev_key\"", "]", ",", "saved_state", "[", "\"prev_value\"", "]", "\n", "return", "x", ",", "attn", ",", "self_attn_state", "\n", "", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.maybe_layer_norm": [[751, 757], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.TransformerDecoderLayer.make_generation_fast_": [[758, 760], ["None"], "methods", ["None"], ["", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding": [[762, 767], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm": [[769, 772], ["torch.LayerNorm"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.LayerNorm"], ["", "def", "LayerNorm", "(", "embedding_dim", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LayerNorm", "(", "embedding_dim", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear": [[774, 780], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.PositionalEmbedding": [[782, 790], ["fairseq.modules.LearnedPositionalEmbedding", "torch.init.normal_", "torch.init.constant_", "fairseq.modules.SinusoidalPositionalEmbedding"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ",", "learned", "=", "False", ")", ":", "\n", "    ", "if", "learned", ":", "\n", "        ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", "+", "padding_idx", "+", "1", ",", "embedding_dim", ",", "padding_idx", ",", "left_pad", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "        ", "m", "=", "SinusoidalPositionalEmbedding", "(", "embedding_dim", ",", "padding_idx", ",", "left_pad", ",", "num_embeddings", "+", "padding_idx", "+", "1", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture": [[792, 817], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "adaptive_softmax_factor", "=", "getattr", "(", "args", ",", "'adaptive_softmax_factor'", ",", "4", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "\n", "args", ".", "character_embeddings", "=", "getattr", "(", "args", ",", "'character_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "# The model training is not stable without this", "\n", "args", ".", "decoder_normalize_before", "=", "True", "\n", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "False", ")", "\n", "args", ".", "adaptive_input_factor", "=", "getattr", "(", "args", ",", "'adaptive_input_factor'", ",", "4", ")", "\n", "args", ".", "adaptive_input_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_input_cutoff'", ",", "None", ")", "\n", "\n", "args", ".", "tie_adaptive_weights", "=", "getattr", "(", "args", ",", "'tie_adaptive_weights'", ",", "False", ")", "\n", "args", ".", "tie_adaptive_proj", "=", "getattr", "(", "args", ",", "'tie_adaptive_proj'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_lm_big": [[819, 825], ["register_model_architecture", "getattr", "getattr", "getattr", "transformer.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_big'", ")", "\n", "def", "transformer_lm_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_lm_wiki103": [[827, 831], ["register_model_architecture", "getattr", "transformer.transformer_lm_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_lm_big"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_wiki103'", ")", "\n", "def", "transformer_lm_wiki103", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "transformer_lm_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_lm_gbw": [[833, 839], ["register_model_architecture", "getattr", "getattr", "getattr", "transformer.transformer_lm_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_lm_big"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gbw'", ")", "\n", "def", "transformer_lm_gbw", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "transformer_lm_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture": [[841, 868], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_iwslt_de_en": [[870, 881], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_iwslt_de_en'", ")", "\n", "def", "transformer_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_wmt_en_de": [[883, 886], ["register_model_architecture", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de'", ")", "\n", "def", "transformer_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_vaswani_wmt_en_de_big": [[889, 900], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_vaswani_wmt_en_de_big'", ")", "\n", "def", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_vaswani_wmt_en_fr_big": [[902, 906], ["register_model_architecture", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_vaswani_wmt_en_fr_big'", ")", "\n", "def", "transformer_vaswani_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_wmt_en_de_big": [[908, 912], ["register_model_architecture", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de_big'", ")", "\n", "def", "transformer_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_wmt_en_de_big_t2t": [[915, 922], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de_big_t2t'", ")", "\n", "def", "transformer_wmt_en_de_big_t2t", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "True", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "True", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.__init__": [[41, 43], ["FairseqMultiModel.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "encoders", ",", "decoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoders", ",", "decoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.add_args": [[44, 56], ["transformer.TransformerModel.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "TransformerModel", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "'--share-encoder-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder embeddings across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder embeddings across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-encoders'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoders across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoders'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoders across languages'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.build_model": [[57, 166], ["isinstance", "multilingual_transformer.base_multilingual_architecture", "zip", "multilingual_transformer.MultilingualTransformerModel", "hasattr", "hasattr", "len", "dictionary.pad", "transformer.Embedding", "FairseqMultiModel.build_shared_embeddings", "multilingual_transformer.MultilingualTransformerModel.build_model.get_encoder"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.base_multilingual_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.Embedding", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.FairseqMultiModel.build_shared_embeddings"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "assert", "isinstance", "(", "task", ",", "MultilingualTranslationTask", ")", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_multilingual_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_langs", "=", "[", "lang_pair", ".", "split", "(", "'-'", ")", "[", "0", "]", "for", "lang_pair", "in", "task", ".", "lang_pairs", "]", "\n", "tgt_langs", "=", "[", "lang_pair", ".", "split", "(", "'-'", ")", "[", "1", "]", "for", "lang_pair", "in", "task", ".", "lang_pairs", "]", "\n", "\n", "if", "args", ".", "share_encoders", ":", "\n", "            ", "args", ".", "share_encoder_embeddings", "=", "True", "\n", "", "if", "args", ".", "share_decoders", ":", "\n", "            ", "args", ".", "share_decoder_embeddings", "=", "True", "\n", "\n", "", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "# build shared embeddings (if applicable)", "\n", "", "shared_encoder_embed_tokens", ",", "shared_decoder_embed_tokens", "=", "None", ",", "None", "\n", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "shared_encoder_embed_tokens", "=", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "task", ".", "langs", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "encoder_embed_path", ",", "\n", ")", "\n", "shared_decoder_embed_tokens", "=", "shared_encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "share_encoder_embeddings", ":", "\n", "                ", "shared_encoder_embed_tokens", "=", "(", "\n", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "src_langs", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "encoder_embed_path", ",", "\n", ")", "\n", ")", "\n", "", "if", "args", ".", "share_decoder_embeddings", ":", "\n", "                ", "shared_decoder_embed_tokens", "=", "(", "\n", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "tgt_langs", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "decoder_embed_path", ",", "\n", ")", "\n", ")", "\n", "\n", "# encoders/decoders for each language", "\n", "", "", "lang_encoders", ",", "lang_decoders", "=", "{", "}", ",", "{", "}", "\n", "\n", "def", "get_encoder", "(", "lang", ")", ":", "\n", "            ", "if", "lang", "not", "in", "lang_encoders", ":", "\n", "                ", "if", "shared_encoder_embed_tokens", "is", "not", "None", ":", "\n", "                    ", "encoder_embed_tokens", "=", "shared_encoder_embed_tokens", "\n", "", "else", ":", "\n", "                    ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "task", ".", "dicts", "[", "lang", "]", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "", "lang_encoders", "[", "lang", "]", "=", "TransformerEncoder", "(", "args", ",", "task", ".", "dicts", "[", "lang", "]", ",", "encoder_embed_tokens", ")", "\n", "", "return", "lang_encoders", "[", "lang", "]", "\n", "\n", "", "def", "get_decoder", "(", "lang", ")", ":", "\n", "            ", "if", "lang", "not", "in", "lang_decoders", ":", "\n", "                ", "if", "shared_decoder_embed_tokens", "is", "not", "None", ":", "\n", "                    ", "decoder_embed_tokens", "=", "shared_decoder_embed_tokens", "\n", "", "else", ":", "\n", "                    ", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "task", ".", "dicts", "[", "lang", "]", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "", "lang_decoders", "[", "lang", "]", "=", "TransformerDecoder", "(", "args", ",", "task", ".", "dicts", "[", "lang", "]", ",", "decoder_embed_tokens", ")", "\n", "", "return", "lang_decoders", "[", "lang", "]", "\n", "\n", "# shared encoders/decoders (if applicable)", "\n", "", "shared_encoder", ",", "shared_decoder", "=", "None", ",", "None", "\n", "if", "args", ".", "share_encoders", ":", "\n", "            ", "shared_encoder", "=", "get_encoder", "(", "src_langs", "[", "0", "]", ")", "\n", "", "if", "args", ".", "share_decoders", ":", "\n", "            ", "shared_decoder", "=", "get_decoder", "(", "tgt_langs", "[", "0", "]", ")", "\n", "\n", "", "encoders", ",", "decoders", "=", "OrderedDict", "(", ")", ",", "OrderedDict", "(", ")", "\n", "for", "lang_pair", ",", "src", ",", "tgt", "in", "zip", "(", "task", ".", "lang_pairs", ",", "src_langs", ",", "tgt_langs", ")", ":", "\n", "            ", "encoders", "[", "lang_pair", "]", "=", "shared_encoder", "if", "shared_encoder", "is", "not", "None", "else", "get_encoder", "(", "src", ")", "\n", "decoders", "[", "lang_pair", "]", "=", "shared_decoder", "if", "shared_decoder", "is", "not", "None", "else", "get_decoder", "(", "tgt", ")", "\n", "\n", "", "return", "MultilingualTransformerModel", "(", "encoders", ",", "decoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.MultilingualTransformerModel.load_state_dict": [[167, 175], ["state_dict.copy", "state_dict.items", "super().load_state_dict", "k.startswith", "k.split"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.copy", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "state_dict_subset", "=", "state_dict", ".", "copy", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "assert", "k", ".", "startswith", "(", "'models.'", ")", "\n", "lang_pair", "=", "k", ".", "split", "(", "'.'", ")", "[", "1", "]", "\n", "if", "lang_pair", "not", "in", "self", ".", "models", ":", "\n", "                ", "del", "state_dict_subset", "[", "k", "]", "\n", "", "", "super", "(", ")", ".", "load_state_dict", "(", "state_dict_subset", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.base_multilingual_architecture": [[177, 184], ["register_model_architecture", "transformer.base_architecture", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.transformer.base_architecture"], ["", "", "@", "register_model_architecture", "(", "'multilingual_transformer'", ",", "'multilingual_transformer'", ")", "\n", "def", "base_multilingual_architecture", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "args", ".", "share_encoder_embeddings", "=", "getattr", "(", "args", ",", "'share_encoder_embeddings'", ",", "False", ")", "\n", "args", ".", "share_decoder_embeddings", "=", "getattr", "(", "args", ",", "'share_decoder_embeddings'", ",", "False", ")", "\n", "args", ".", "share_encoders", "=", "getattr", "(", "args", ",", "'share_encoders'", ",", "False", ")", "\n", "args", ".", "share_decoders", "=", "getattr", "(", "args", ",", "'share_decoders'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.multilingual_transformer_iwslt_de_en": [[186, 197], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "multilingual_transformer.base_multilingual_architecture"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.multilingual_transformer.base_multilingual_architecture"], ["", "@", "register_model_architecture", "(", "'multilingual_transformer'", ",", "'multilingual_transformer_iwslt_de_en'", ")", "\n", "def", "multilingual_transformer_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_multilingual_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.distributed_fairseq_model.DistributedFairseqModel": [[16, 68], ["isinstance", "_DistributedFairseqModel", "dict", "dict", "ValueError", "super().__init__", "super().__getattr__", "hasattr", "super().__getattr__", "inspect.getargspec", "getattr"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__getattr__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Word.__getattr__"], ["def", "DistributedFairseqModel", "(", "args", ",", "model", ")", ":", "\n", "    ", "\"\"\"\n    Wrap a *model* to support distributed data parallel training.\n\n    This is similar to the built-in DistributedDataParallel, but allows\n    additional configuration of the DistributedDataParallel class to\n    use, and also provides easier access to the wrapped model by\n    forwarding requests for missing attributes to the wrapped model.\n\n    Args:\n        args (argparse.Namespace): fairseq args\n        model (BaseFairseqModel): model to wrap\n    \"\"\"", "\n", "\n", "# determine which DDP class to extend", "\n", "assert", "isinstance", "(", "model", ",", "BaseFairseqModel", ")", "\n", "if", "args", ".", "ddp_backend", "==", "'c10d'", ":", "\n", "        ", "ddp_class", "=", "parallel", ".", "DistributedDataParallel", "\n", "init_kwargs", "=", "dict", "(", "\n", "module", "=", "model", ",", "\n", "device_ids", "=", "[", "args", ".", "device_id", "]", ",", "\n", "output_device", "=", "args", ".", "device_id", ",", "\n", "broadcast_buffers", "=", "False", ",", "\n", "bucket_cap_mb", "=", "args", ".", "bucket_cap_mb", ",", "\n", ")", "\n", "# Maintain backward compatibility for 0.4 or earlier", "\n", "if", "'check_reduction'", "in", "inspect", ".", "getargspec", "(", "ddp_class", ")", "[", "0", "]", ":", "\n", "            ", "init_kwargs", "[", "'check_reduction'", "]", "=", "True", "\n", "", "", "elif", "args", ".", "ddp_backend", "==", "'no_c10d'", ":", "\n", "        ", "ddp_class", "=", "LegacyDistributedDataParallel", "\n", "init_kwargs", "=", "dict", "(", "\n", "module", "=", "model", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ",", "\n", "buffer_size", "=", "2", "**", "28", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown --ddp-backend: '", "+", "args", ".", "ddp_backend", ")", "\n", "\n", "", "class", "_DistributedFairseqModel", "(", "ddp_class", ")", ":", "\n", "        ", "\"\"\"Extend DistributedDataParallel to check for missing\n        attributes in the wrapped module.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "            ", "wrapped_module", "=", "super", "(", ")", ".", "__getattr__", "(", "'module'", ")", "\n", "if", "hasattr", "(", "wrapped_module", ",", "name", ")", ":", "\n", "                ", "return", "getattr", "(", "wrapped_module", ",", "name", ")", "\n", "", "return", "super", "(", ")", ".", "__getattr__", "(", "name", ")", "\n", "\n", "", "", "return", "_DistributedFairseqModel", "(", "**", "init_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.__init__": [[13, 17], ["torch.nn.modules.loss._Loss.__init__", "task.target_dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "padding_idx", "=", "task", ".", "target_dictionary", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.add_args": [[18, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.build_criterion": [[23, 26], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_criterion", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.forward": [[27, 36], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.aggregate_logging_outputs": [[37, 41], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.fairseq_criterion.FairseqCriterion.grad_denom": [[42, 46], ["sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "grad_denom", "(", "sample_sizes", ")", ":", "\n", "        ", "\"\"\"Compute the gradient denominator for a set of sample sizes.\"\"\"", "\n", "return", "sum", "(", "sample_sizes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.cross_entropy.CrossEntropyCriterion.__init__": [[19, 21], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.cross_entropy.CrossEntropyCriterion.forward": [[22, 40], ["model", "cross_entropy.CrossEntropyCriterion.compute_loss", "sample[].size", "sample[].size", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.compute_loss", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "loss", ",", "_", "=", "self", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "reduce", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.cross_entropy.CrossEntropyCriterion.compute_loss": [[41, 48], ["model.get_normalized_probs", "lprobs.view.view.view", "model.get_targets().view", "torch.nll_loss", "lprobs.view.view.size", "model.get_targets"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_targets"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "lprobs", ",", "target", ",", "size_average", "=", "False", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduce", "=", "reduce", ")", "\n", "return", "loss", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.cross_entropy.CrossEntropyCriterion.aggregate_logging_outputs": [[49, 65], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.adaptive_loss.AdaptiveLoss.__init__": [[22, 28], ["FairseqCriterion.__init__", "Exception"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n", "if", "args", ".", "ddp_backend", "==", "'c10d'", ":", "\n", "            ", "raise", "Exception", "(", "\n", "'AdaptiveLoss is not compatible with the c10d '", "\n", "'version of DistributedDataParallel. Please use '", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.adaptive_loss.AdaptiveLoss.forward": [[32, 73], ["model", "model.get_targets", "orig_target.view.view.size", "orig_target.view.view.view", "orig_target.view.view.size", "adaptive_softmax", "net_output[].new().zero_", "range", "fairseq.utils.strip_pad", "fairseq.utils.strip_pad.numel", "hasattr", "len", "len", "len", "sample[].size", "net_output[].new", "torch.cross_entropy", "fairseq.utils.item", "target[].min", "target[].max", "logits[].size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_targets", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "\n", "assert", "hasattr", "(", "model", ".", "decoder", ",", "'adaptive_softmax'", ")", "and", "model", ".", "decoder", ".", "adaptive_softmax", "is", "not", "None", "\n", "adaptive_softmax", "=", "model", ".", "decoder", ".", "adaptive_softmax", "\n", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "orig_target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", "\n", "\n", "nsentences", "=", "orig_target", ".", "size", "(", "0", ")", "\n", "orig_target", "=", "orig_target", ".", "view", "(", "-", "1", ")", "\n", "\n", "bsz", "=", "orig_target", ".", "size", "(", "0", ")", "\n", "\n", "logits", ",", "target", "=", "adaptive_softmax", "(", "net_output", "[", "0", "]", ",", "orig_target", ")", "\n", "assert", "len", "(", "target", ")", "==", "len", "(", "logits", ")", "\n", "\n", "loss", "=", "net_output", "[", "0", "]", ".", "new", "(", "1", "if", "reduce", "else", "bsz", ")", ".", "zero_", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", ":", "\n", "            ", "if", "target", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "assert", "(", "target", "[", "i", "]", ".", "min", "(", ")", ">=", "0", "and", "target", "[", "i", "]", ".", "max", "(", ")", "<=", "logits", "[", "i", "]", ".", "size", "(", "1", ")", ")", "\n", "loss", "+=", "F", ".", "cross_entropy", "(", "logits", "[", "i", "]", ",", "target", "[", "i", "]", ",", "size_average", "=", "False", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduce", "=", "reduce", ")", "\n", "\n", "", "", "orig", "=", "utils", ".", "strip_pad", "(", "orig_target", ",", "self", ".", "padding_idx", ")", "\n", "ntokens", "=", "orig", ".", "numel", "(", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "ntokens", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.adaptive_loss.AdaptiveLoss.aggregate_logging_outputs": [[74, 91], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'nll_loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.__init__": [[18, 21], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "eps", "=", "args", ".", "label_smoothing", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args": [[22, 28], ["parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--label-smoothing'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for label smoothing, 0 means no label smoothing'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.forward": [[30, 49], ["model", "label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "sample[].size", "sample[].size", "fairseq.utils.item", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.compute_loss", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "loss", ",", "nll_loss", "=", "self", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "reduce", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'nll_loss'", ":", "utils", ".", "item", "(", "nll_loss", ".", "data", ")", "if", "reduce", "else", "nll_loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss": [[50, 63], ["model.get_normalized_probs", "lprobs.view.view.view", "model.get_targets().view", "model.get_targets().view.ne", "lprobs.view.view.size", "nll_loss.sum.sum.sum", "smooth_loss.sum.sum.sum", "lprobs.view.view.size", "model.get_targets", "lprobs.view.view.gather", "lprobs.view.view.sum"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_targets"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "non_pad_mask", "=", "target", ".", "ne", "(", "self", ".", "padding_idx", ")", "\n", "nll_loss", "=", "-", "lprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "target", ")", "[", "non_pad_mask", "]", "\n", "smooth_loss", "=", "-", "lprobs", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "[", "non_pad_mask", "]", "\n", "if", "reduce", ":", "\n", "            ", "nll_loss", "=", "nll_loss", ".", "sum", "(", ")", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "", "eps_i", "=", "self", ".", "eps", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "self", ".", "eps", ")", "*", "nll_loss", "+", "eps_i", "*", "smooth_loss", "\n", "return", "loss", ",", "nll_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs": [[64, 76], ["sum", "sum", "sum", "log.get", "log.get", "log.get", "math.log", "math.log", "sum", "sum", "log.get", "log.get"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "return", "{", "\n", "'loss'", ":", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'nll_loss'", ":", "sum", "(", "log", ".", "get", "(", "'nll_loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.__init__": [[29, 50], ["FairseqCriterion.__init__", "task.source_dictionary.pad", "print", "print", "ValueError", "getattr", "print", "getattr"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "# Note: self.padding_idx defaults to the target dictionary's pad index", "\n", "self", ".", "src_padding_idx", "=", "task", ".", "source_dictionary", ".", "pad", "(", ")", "\n", "\n", "self", ".", "edit_weighted_loss", "=", "args", ".", "edit_weighted_loss", "\n", "if", "self", ".", "edit_weighted_loss", "!=", "1.0", ":", "\n", "            ", "print", "(", "f\"using edit-weighted MLE loss \"", "\n", "f\"with scale {self.edit_weighted_loss}\"", ")", "\n", "", "self", ".", "edit_label_prediction", "=", "args", ".", "edit_label_prediction", "\n", "if", "self", ".", "edit_label_prediction", ">", "0.0", ":", "\n", "            ", "print", "(", "f\"using auxiliary edit label prediction loss \"", "\n", "f\"with scale {self.edit_label_prediction}\"", ")", "\n", "\n", "# Check that the model provides required options.", "\n", "", "if", "(", "self", ".", "edit_label_prediction", ">", "0.0", "and", "\n", "not", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "None", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"model must have predict_edit_labels==True\"", ")", "\n", "", "if", "(", "self", ".", "edit_label_prediction", "==", "0.0", "and", "\n", "getattr", "(", "args", ",", "'predict_edit_labels'", ",", "None", ")", ")", ":", "\n", "            ", "print", "(", "\"WARNING: edit labels are predicted by the model \"", "\n", "\"but not included in the training objective.\"", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.add_args": [[52, 61], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--edit-weighted-loss'", ",", "type", "=", "float", ",", "default", "=", "3.0", ",", "\n", "help", "=", "'use edit-weighted MLE loss for targets'", "\n", "'(default: 3.0; ignored if 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--edit-label-prediction'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'additionally predict edit labels from '", "\n", "'encoder outputs, using given scale.'", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.forward": [[65, 86], ["model", "gec_loss.GECLossCriterion.compute_loss", "sample[].size", "sample[].size", "fairseq.utils.item", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.compute_loss", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "loss", ",", "edit_label_loss", "=", "self", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "reduce", ")", "\n", "if", "edit_label_loss", "is", "not", "None", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "edit_label_prediction", "*", "edit_label_loss", "\n", "", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'edit_label_loss'", ":", "utils", ".", "item", "(", "edit_label_loss", ".", "data", ")", "if", "edit_label_loss", "is", "not", "None", "else", "0.0", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.compute_loss": [[87, 118], ["model.get_normalized_probs", "lprobs.view.view.view", "model.get_targets().view", "sample[].float().view", "torch.nll_loss", "torch.nll_loss", "lprobs.view.view.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "[].ne", "[].squeeze().transpose", "sample[].float", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "model.get_targets", "sample[].float", "[].squeeze"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_model.BaseFairseqModel.get_targets"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "# (B x T_dec) x V", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", ".", "view", "(", "-", "1", ")", "# (B x T_dec)", "\n", "\n", "# Compute token-level weights based on target-side token labels.", "\n", "# weights: edit_weighted_loss if tgt_labels == 1 else 1.0", "\n", "edit_weights", "=", "sample", "[", "'tgt_labels'", "]", ".", "float", "(", ")", ".", "view", "(", "-", "1", ")", "# B x T_dec", "\n", "edit_weights", "=", "(", "self", ".", "edit_weighted_loss", "-", "1.", ")", "*", "edit_weights", "+", "1.", "\n", "loss", "=", "F", ".", "nll_loss", "(", "lprobs", ",", "target", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduction", "=", "'none'", ")", "\n", "loss", "=", "edit_weights", "*", "loss", "\n", "if", "reduce", ":", "\n", "            ", "loss", "=", "torch", ".", "sum", "(", "loss", ")", "\n", "\n", "# Optionally add auxiliary loss from source-side edit label prediction.", "\n", "# Always reduced (dimension differs from loss).", "\n", "", "if", "self", ".", "edit_label_prediction", ">", "0.0", ":", "\n", "# All three tensors have the same shape: (B x T_enc)", "\n", "            ", "src_nonpads", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ".", "ne", "(", "self", ".", "src_padding_idx", ")", "\n", "edit_logits", "=", "net_output", "[", "1", "]", "[", "'edit_logits'", "]", ".", "squeeze", "(", "-", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "src_labels", "=", "sample", "[", "'src_labels'", "]", ".", "float", "(", ")", "\n", "edit_label_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "edit_logits", "[", "src_nonpads", "]", ",", "\n", "src_labels", "[", "src_nonpads", "]", ",", "\n", "reduction", "=", "'sum'", "\n", ")", "\n", "", "else", ":", "\n", "            ", "edit_label_loss", "=", "None", "\n", "\n", "", "return", "loss", ",", "edit_label_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.gec_loss.GECLossCriterion.aggregate_logging_outputs": [[119, 137], ["sum", "sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "edit_label_loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'edit_label_loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'edit_label_loss'", ":", "edit_label_loss_sum", "/", "sample_size", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.__init__.build_criterion": [[18, 20], ["CRITERION_REGISTRY[].build_criterion"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.__init__.register_criterion": [[22, 39], ["CRITERION_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq_cli.eval_lm.WordStat.add"], []], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.add_args": [[19, 25], ["parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--underlying-criterion'", ",", "type", "=", "str", ",", "metavar", "=", "'VAL'", ",", "required", "=", "True", ",", "\n", "help", "=", "'underlying criterion to use for the composite loss'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_underlying_criterion": [[27, 35], ["task.build_criterion"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion"], ["", "@", "staticmethod", "\n", "def", "build_underlying_criterion", "(", "args", ",", "task", ")", ":", "\n", "        ", "saved_criterion", "=", "args", ".", "criterion", "\n", "args", ".", "criterion", "=", "args", ".", "underlying_criterion", "\n", "assert", "saved_criterion", "!=", "args", ".", "underlying_criterion", "\n", "underlying_criterion", "=", "task", ".", "build_criterion", "(", "args", ")", "\n", "args", ".", "criterion", "=", "saved_criterion", "\n", "return", "underlying_criterion", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_criterion": [[36, 94], ["composite_loss.CompositeLoss.build_underlying_criterion", "_CompositeLoss", "FairseqCriterion.__init__", "composite_loss.CompositeLoss.model.get_normalized_probs", "FairseqCriterion.__init__", "model", "targets[].size", "[].new().float().zero_", "zip", "[].new().float().zero_.div_", "len", "composite_loss.CompositeLoss.build_underlying_criterion", "FakeModel", "composite_loss.CompositeLoss.underlying_criterion", "len", "fairseq.utils.item", "[].new().float", "[].new"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_underlying_criterion", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.criterions.composite_loss.CompositeLoss.build_underlying_criterion", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.item"], ["", "@", "classmethod", "\n", "def", "build_criterion", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "underlying_criterion", "=", "CompositeLoss", ".", "build_underlying_criterion", "(", "args", ",", "task", ")", "\n", "\n", "class", "FakeModel", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "            ", "def", "__init__", "(", "self", ",", "model", ",", "net_out", ",", "target", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "net_out", "=", "net_out", "\n", "self", ".", "target", "=", "target", "\n", "\n", "", "def", "forward", "(", "self", ",", "**", "unused", ")", ":", "\n", "                ", "return", "self", ".", "net_out", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "                ", "return", "self", ".", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", "=", "sample", ")", "\n", "\n", "", "def", "get_targets", "(", "self", ",", "*", "unused", ")", ":", "\n", "                ", "return", "self", ".", "target", "\n", "\n", "", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "                ", "return", "self", ".", "model", ".", "decoder", "\n", "\n", "", "", "class", "_CompositeLoss", "(", "FairseqCriterion", ")", ":", "\n", "\n", "            ", "def", "__init__", "(", "self", ",", "args", ",", "task", ",", "underlying_criterion", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "underlying_criterion", "=", "underlying_criterion", "\n", "\n", "", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "                ", "net_outputs", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "targets", "=", "sample", "[", "'target'", "]", "\n", "\n", "bsz", "=", "targets", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "loss", "=", "net_outputs", "[", "0", "]", "[", "0", "]", ".", "new", "(", "1", "if", "reduce", "else", "bsz", ")", ".", "float", "(", ")", ".", "zero_", "(", ")", "\n", "\n", "sample_size", "=", "0", "\n", "logging_output", "=", "{", "}", "\n", "for", "o", ",", "t", "in", "zip", "(", "net_outputs", "[", "0", "]", ",", "targets", ")", ":", "\n", "                    ", "m", "=", "FakeModel", "(", "model", ",", "(", "o", ",", "net_outputs", "[", "1", "]", ")", ",", "t", ")", "\n", "sample", "[", "'target'", "]", "=", "t", "\n", "l", ",", "ss", ",", "logging_output", "=", "self", ".", "underlying_criterion", "(", "m", ",", "sample", ",", "reduce", ")", "\n", "loss", "+=", "l", "\n", "sample_size", "+=", "ss", "\n", "\n", "", "loss", ".", "div_", "(", "len", "(", "targets", ")", ")", "\n", "sample_size", "/=", "len", "(", "targets", ")", "\n", "\n", "logging_output", "[", "'loss'", "]", "=", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n", "", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "                ", "return", "underlying_criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n", "", "", "return", "_CompositeLoss", "(", "args", ",", "task", ",", "underlying_criterion", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.__getitem__": [[14, 16], ["None"], "methods", ["None"], ["def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.__len__": [[17, 19], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.collater": [[20, 30], ["None"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[int]): sample indices to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.get_dummy_batch": [[31, 34], ["None"], "methods", ["None"], ["", "def", "get_dummy_batch", "(", "self", ",", "num_tokens", ",", "max_positions", ")", ":", "\n", "        ", "\"\"\"Return a dummy batch with a given number of tokens.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.num_tokens": [[35, 39], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.size": [[40, 44], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.ordered_indices": [[45, 49], ["None"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.supports_prefetch": [[50, 54], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether this dataset supports prefetching.\"\"\"", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.fairseq_dataset.FairseqDataset.prefetch": [[55, 58], ["None"], "methods", ["None"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Prefetch the data required for this epoch.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.__init__": [[78, 91], ["torch.cuda.is_available"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tgt_dataset", ",", "\n", "backtranslation_fn", ",", "\n", "output_collater", "=", "None", ",", "\n", "cuda", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self", ".", "tgt_dataset", "=", "tgt_dataset", "\n", "self", ".", "backtranslation_fn", "=", "backtranslation_fn", "\n", "self", ".", "output_collater", "=", "output_collater", "if", "output_collater", "is", "not", "None", "else", "tgt_dataset", ".", "collater", "\n", "self", ".", "cuda", "=", "cuda", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.__getitem__": [[92, 99], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        Returns a single sample from *tgt_dataset*. Note that backtranslation is\n        not applied in this step; use :func:`collater` instead to backtranslate\n        a batch of samples.\n        \"\"\"", "\n", "return", "self", ".", "tgt_dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "tgt_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.collater": [[103, 131], ["backtranslation_dataset.backtranslate_samples", "backtranslation_dataset.BacktranslationDataset.output_collater", "backtranslation_dataset.BacktranslationDataset.backtranslation_fn"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.backtranslate_samples"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge and backtranslate a list of samples to form a mini-batch.\n\n        Using the samples from *tgt_dataset*, load a collated target sample to\n        feed to the backtranslation model. Then take the backtranslation with\n        the best score as the source and the original input as the target.\n\n        Note: we expect *tgt_dataset* to provide a function `collater()` that\n        will collate samples into the format expected by *backtranslation_fn*.\n        After backtranslation, we will feed the new list of samples (i.e., the\n        `(backtranslated source, original source)` pairs) to *output_collater*\n        and return the result.\n\n        Args:\n            samples (List[dict]): samples to backtranslate and collate\n\n        Returns:\n            dict: a mini-batch with keys coming from *output_collater*\n        \"\"\"", "\n", "samples", "=", "backtranslate_samples", "(", "\n", "samples", "=", "samples", ",", "\n", "collate_fn", "=", "self", ".", "tgt_dataset", ".", "collater", ",", "\n", "generate_fn", "=", "(", "\n", "lambda", "net_input", ":", "self", ".", "backtranslation_fn", "(", "net_input", ")", "\n", ")", ",", "\n", "cuda", "=", "self", ".", "cuda", ",", "\n", ")", "\n", "return", "self", ".", "output_collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.get_dummy_batch": [[132, 135], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.get_dummy_batch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch"], ["", "def", "get_dummy_batch", "(", "self", ",", "num_tokens", ",", "max_positions", ")", ":", "\n", "        ", "\"\"\"Just use the tgt dataset get_dummy_batch\"\"\"", "\n", "return", "self", ".", "tgt_dataset", ".", "get_dummy_batch", "(", "num_tokens", ",", "max_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.num_tokens": [[136, 139], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Just use the tgt dataset num_tokens\"\"\"", "\n", "return", "self", ".", "tgt_dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.ordered_indices": [[140, 143], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Just use the tgt dataset ordered_indices\"\"\"", "\n", "return", "self", ".", "tgt_dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.size": [[144, 154], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used\n        when filtering a dataset with ``--max-positions``.\n\n        Note: we use *tgt_dataset* to approximate the length of the source\n        sentence, since we do not know the actual length until after\n        backtranslation.\n        \"\"\"", "\n", "tgt_size", "=", "self", ".", "tgt_dataset", ".", "size", "(", "index", ")", "[", "0", "]", "\n", "return", "(", "tgt_size", ",", "tgt_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.supports_prefetch": [[155, 158], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "tgt_dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.BacktranslationDataset.prefetch": [[159, 161], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.backtranslation_dataset.backtranslate_samples": [[15, 55], ["collate_fn", "generate_fn", "fairseq.utils.move_to_cuda", "backtranslation_dataset.backtranslate_samples.update_sample"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.move_to_cuda"], ["def", "backtranslate_samples", "(", "samples", ",", "collate_fn", ",", "generate_fn", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "\"\"\"Backtranslate a list of samples.\n\n    Given an input (*samples*) of the form:\n\n        [{'id': 1, 'source': 'hallo welt'}]\n\n    this will return:\n\n        [{'id': 1, 'source': 'hello world', 'target': 'hallo welt'}]\n\n    Args:\n        samples (List[dict]): samples to backtranslate. Individual samples are\n            expected to have a 'source' key, which will become the 'target'\n            after backtranslation.\n        collate_fn (callable): function to collate samples into a mini-batch\n        generate_fn (callable): function to generate backtranslations\n        cuda (bool): use GPU for generation (default: ``True``)\n\n    Returns:\n        List[dict]: an updated list of samples with a backtranslated source\n    \"\"\"", "\n", "collated_samples", "=", "collate_fn", "(", "samples", ")", "\n", "s", "=", "utils", ".", "move_to_cuda", "(", "collated_samples", ")", "if", "cuda", "else", "collated_samples", "\n", "generated_sources", "=", "generate_fn", "(", "s", "[", "'net_input'", "]", ")", "\n", "\n", "def", "update_sample", "(", "sample", ",", "generated_source", ")", ":", "\n", "        ", "sample", "[", "'target'", "]", "=", "sample", "[", "'source'", "]", "# the original source becomes the target", "\n", "sample", "[", "'source'", "]", "=", "generated_source", "\n", "return", "sample", "\n", "\n", "# Go through each tgt sentence in batch and its corresponding best", "\n", "# generated hypothesis and create a backtranslation data pair", "\n", "# {id: id, source: generated backtranslation, target: original tgt}", "\n", "", "return", "[", "\n", "update_sample", "(", "\n", "sample", "=", "input_sample", ",", "\n", "generated_source", "=", "hypos", "[", "0", "]", "[", "'tokens'", "]", ".", "cpu", "(", ")", ",", "# highest scoring hypo is first", "\n", ")", "\n", "for", "input_sample", ",", "hypos", "in", "zip", "(", "samples", ",", "generated_sources", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_block_dataset.TokenBlockDataset.__init__": [[34, 102], ["FairseqDataset.__init__", "numpy.array", "numpy.array", "numpy.array", "numpy.empty", "enumerate", "len", "len", "sum", "math.ceil", "min", "token_block_dataset.TokenBlockDataset.__init__.block_at"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dataset", ",", "sizes", ",", "block_size", ",", "pad", ",", "eos", ",", "break_mode", "=", "None", ",", "include_targets", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "pad", "=", "pad", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "include_targets", "=", "include_targets", "\n", "self", ".", "slice_indices", "=", "[", "]", "\n", "\n", "assert", "len", "(", "dataset", ")", "==", "len", "(", "sizes", ")", "\n", "sizes", "=", "np", ".", "array", "(", "sizes", ",", "dtype", "=", "int", ")", "\n", "if", "break_mode", "is", "None", "or", "break_mode", "==", "'none'", ":", "\n", "            ", "total_size", "=", "sum", "(", "sizes", ")", "\n", "length", "=", "math", ".", "ceil", "(", "total_size", "/", "block_size", ")", "\n", "\n", "def", "block_at", "(", "i", ")", ":", "\n", "                ", "start", "=", "i", "*", "block_size", "\n", "end", "=", "min", "(", "start", "+", "block_size", ",", "total_size", ")", "\n", "return", "(", "start", ",", "end", ")", "\n", "\n", "", "self", ".", "slice_indices", "=", "[", "block_at", "(", "i", ")", "for", "i", "in", "range", "(", "length", ")", "]", "\n", "", "elif", "break_mode", "==", "'complete'", ":", "\n", "            ", "tok_idx", "=", "0", "\n", "sz_idx", "=", "0", "\n", "curr_size", "=", "0", "\n", "while", "sz_idx", "<", "len", "(", "sizes", ")", ":", "\n", "                ", "if", "curr_size", "+", "sizes", "[", "sz_idx", "]", "<=", "block_size", "or", "curr_size", "==", "0", ":", "\n", "                    ", "curr_size", "+=", "sizes", "[", "sz_idx", "]", "\n", "sz_idx", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "self", ".", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "tok_idx", "+=", "curr_size", "\n", "curr_size", "=", "0", "\n", "", "", "if", "curr_size", ">", "0", ":", "\n", "                ", "self", ".", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "", "", "elif", "break_mode", "==", "'eos'", ":", "\n", "            ", "self", ".", "slice_indices", "=", "np", ".", "empty", "(", "(", "len", "(", "sizes", ")", ",", "2", ")", ",", "dtype", "=", "int", ")", "\n", "curr", "=", "0", "\n", "for", "i", ",", "sz", "in", "enumerate", "(", "sizes", ")", ":", "\n", "                ", "self", ".", "slice_indices", "[", "i", "]", "=", "(", "curr", ",", "curr", "+", "sz", ")", "\n", "curr", "+=", "sz", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid break_mode: '", "+", "break_mode", ")", "\n", "\n", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "[", "e", "-", "s", "for", "s", ",", "e", "in", "self", ".", "slice_indices", "]", ")", "\n", "self", ".", "slice_indices", "=", "np", ".", "array", "(", "self", ".", "slice_indices", ",", "dtype", "=", "int", ")", "\n", "\n", "# build index mapping block indices to the underlying dataset indices", "\n", "self", ".", "block_to_dataset_index", "=", "np", ".", "empty", "(", "(", "len", "(", "self", ".", "slice_indices", ")", ",", "3", ")", ",", "dtype", "=", "int", ")", "\n", "ds_idx", ",", "ds_remaining", "=", "-", "1", ",", "0", "\n", "for", "i", ",", "(", "s", ",", "e", ")", "in", "enumerate", "(", "self", ".", "slice_indices", ")", ":", "\n", "            ", "to_consume", "=", "e", "-", "s", "\n", "if", "ds_remaining", "==", "0", ":", "\n", "                ", "ds_idx", "+=", "1", "\n", "ds_remaining", "=", "sizes", "[", "ds_idx", "]", "\n", "", "start_ds_idx", "=", "ds_idx", "\n", "start_offset", "=", "sizes", "[", "ds_idx", "]", "-", "ds_remaining", "\n", "while", "to_consume", ">", "ds_remaining", ":", "\n", "                ", "to_consume", "-=", "ds_remaining", "\n", "ds_idx", "+=", "1", "\n", "ds_remaining", "=", "sizes", "[", "ds_idx", "]", "\n", "", "ds_remaining", "-=", "to_consume", "\n", "self", ".", "block_to_dataset_index", "[", "i", "]", "=", "(", "\n", "start_ds_idx", ",", "# starting index in dataset", "\n", "start_offset", ",", "# starting offset within starting index", "\n", "ds_idx", ",", "# ending index in dataset", "\n", ")", "\n", "", "assert", "ds_remaining", "==", "0", "\n", "assert", "ds_idx", "==", "len", "(", "self", ".", "dataset", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_block_dataset.TokenBlockDataset.__getitem__": [[103, 129], ["torch.cat", "torch.cat", "torch.cat", "range", "torch.cat", "item.new", "item.new", "item.new"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "start_ds_idx", ",", "start_offset", ",", "end_ds_idx", "=", "self", ".", "block_to_dataset_index", "[", "index", "]", "\n", "buffer", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "dataset", "[", "idx", "]", "for", "idx", "in", "range", "(", "start_ds_idx", ",", "end_ds_idx", "+", "1", ")", "\n", "]", ")", "\n", "slice_s", ",", "slice_e", "=", "self", ".", "slice_indices", "[", "index", "]", "\n", "length", "=", "slice_e", "-", "slice_s", "\n", "s", ",", "e", "=", "start_offset", ",", "start_offset", "+", "length", "\n", "item", "=", "buffer", "[", "s", ":", "e", "]", "\n", "\n", "if", "self", ".", "include_targets", ":", "\n", "# *target* is the original sentence (=item)", "\n", "# *source* is rotated left by 1 (maybe left-padded with eos)", "\n", "# *past_target* is rotated left by 2 (left-padded as needed)", "\n", "            ", "if", "s", "==", "0", ":", "\n", "                ", "source", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "1", "]", "]", ")", "\n", "past_target", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "pad", ",", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "2", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "source", "=", "buffer", "[", "s", "-", "1", ":", "e", "-", "1", "]", "\n", "if", "s", "==", "1", ":", "\n", "                    ", "past_target", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "2", "]", "]", ")", "\n", "", "else", ":", "\n", "                    ", "past_target", "=", "buffer", "[", "s", "-", "2", ":", "e", "-", "2", "]", "\n", "\n", "", "", "return", "source", ",", "item", ",", "past_target", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_block_dataset.TokenBlockDataset.__len__": [[130, 132], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "slice_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_block_dataset.TokenBlockDataset.supports_prefetch": [[133, 136], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_block_dataset.TokenBlockDataset.prefetch": [[137, 143], ["token_block_dataset.TokenBlockDataset.dataset.prefetch", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "{", "\n", "ds_idx", "\n", "for", "index", "in", "indices", "\n", "for", "start_ds_idx", ",", "_", ",", "end_ds_idx", "in", "[", "self", ".", "block_to_dataset_index", "[", "index", "]", "]", "\n", "for", "ds_idx", "in", "range", "(", "start_ds_idx", ",", "end_ds_idx", "+", "1", ")", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.__init__": [[29, 34], ["iter", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "itr", "=", "iter", "(", "self", ")", "\n", "self", ".", "len", "=", "len", "(", "iterable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.__len__": [[35, 37], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "len", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.__iter__": [[38, 42], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "x", "in", "self", ".", "iterable", ":", "\n", "            ", "self", ".", "count", "+=", "1", "\n", "yield", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.__next__": [[43, 45], ["next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.has_next": [[46, 49], ["len"], "methods", ["None"], ["", "def", "has_next", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether the iterator has been exhausted.\"\"\"", "\n", "return", "self", ".", "count", "<", "len", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.skip": [[50, 55], ["next", "itertools.islice"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "def", "skip", "(", "self", ",", "num_to_skip", ")", ":", "\n", "        ", "\"\"\"Fast-forward the iterator by skipping *num_to_skip* elements.\"\"\"", "\n", "next", "(", "itertools", ".", "islice", "(", "self", ".", "itr", ",", "num_to_skip", ",", "num_to_skip", ")", ",", "None", ")", "\n", "self", ".", "len", "-=", "num_to_skip", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.__init__": [[84, 101], ["isinstance", "tuple", "getattr"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "dataset", ",", "collate_fn", ",", "batch_sampler", ",", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "\n", "num_workers", "=", "0", ",", "\n", ")", ":", "\n", "        ", "assert", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "collate_fn", "=", "collate_fn", "\n", "self", ".", "frozen_batches", "=", "tuple", "(", "batch_sampler", ")", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "num_shards", "=", "num_shards", "\n", "self", ".", "shard_id", "=", "shard_id", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "_cur_epoch_itr", "=", "None", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "self", ".", "_supports_prefetch", "=", "getattr", "(", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.__len__": [[102, 104], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "frozen_batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.next_epoch_itr": [[105, 124], ["iterators.EpochBatchIterator._get_iterator_for_epoch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator._get_iterator_for_epoch"], ["", "def", "next_epoch_itr", "(", "self", ",", "shuffle", "=", "True", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n        \"\"\"", "\n", "if", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_next_epoch_itr", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "epoch", "+=", "1", "\n", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "\n", "self", ".", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "fix_batches_to_gpus", ",", "\n", ")", "\n", "", "return", "self", ".", "_cur_epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.end_of_epoch": [[125, 128], ["iterators.EpochBatchIterator._cur_epoch_itr.has_next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.has_next"], ["", "def", "end_of_epoch", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"", "\n", "return", "not", "self", ".", "_cur_epoch_itr", ".", "has_next", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.iterations_in_epoch": [[129, 137], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterations_in_epoch", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of consumed batches in the current epoch.\"\"\"", "\n", "if", "self", ".", "_cur_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_cur_epoch_itr", ".", "count", "\n", "", "elif", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_next_epoch_itr", ".", "count", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.state_dict": [[138, 143], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"", "\n", "return", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'iterations_in_epoch'", ":", "self", ".", "iterations_in_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator.load_state_dict": [[145, 154], ["state_dict.get", "iterators.EpochBatchIterator._get_iterator_for_epoch", "state_dict.get", "len", "iterators.EpochBatchIterator.skip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator._get_iterator_for_epoch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.XMLNode.get", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.CountingIterator.skip"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"", "\n", "self", ".", "epoch", "=", "state_dict", "[", "'epoch'", "]", "\n", "itr_pos", "=", "state_dict", ".", "get", "(", "'iterations_in_epoch'", ",", "0", ")", "\n", "if", "itr_pos", ">", "0", ":", "\n", "# fast-forward epoch iterator", "\n", "            ", "itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "self", ".", "epoch", ",", "state_dict", ".", "get", "(", "'shuffle'", ",", "True", ")", ")", "\n", "if", "itr_pos", "<", "len", "(", "itr", ")", ":", "\n", "                ", "self", ".", "_next_epoch_itr", "=", "itr", ".", "skip", "(", "itr_pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.EpochBatchIterator._get_iterator_for_epoch": [[155, 189], ["iterators.CountingIterator", "list", "iterators.EpochBatchIterator.dataset.prefetch", "iterators.ShardedIterator", "torch.utils.data.DataLoader", "data_utils.numpy_seed", "numpy.random.shuffle", "iterators.EpochBatchIterator._get_iterator_for_epoch.shuffle_batches"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed"], ["", "", "", "def", "_get_iterator_for_epoch", "(", "self", ",", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "\n", "        ", "def", "shuffle_batches", "(", "batches", ",", "seed", ")", ":", "\n", "# set seed based on the seed and epoch number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "            ", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "                ", "np", ".", "random", ".", "shuffle", "(", "batches", ")", "\n", "", "return", "batches", "\n", "\n", "", "if", "self", ".", "_supports_prefetch", ":", "\n", "            ", "batches", "=", "self", ".", "frozen_batches", "\n", "\n", "if", "shuffle", "and", "not", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "\n", "", "batches", "=", "list", "(", "ShardedIterator", "(", "\n", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", ")", ")", "\n", "self", ".", "dataset", ".", "prefetch", "(", "[", "i", "for", "s", "in", "batches", "for", "i", "in", "s", "]", ")", "\n", "\n", "if", "shuffle", "and", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "batches", ",", "self", ".", "seed", "+", "epoch", "+", "self", ".", "shard_id", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "self", ".", "frozen_batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "batches", "=", "self", ".", "frozen_batches", "\n", "", "batches", "=", "ShardedIterator", "(", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", ")", "\n", "\n", "", "return", "CountingIterator", "(", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ".", "dataset", ",", "\n", "collate_fn", "=", "self", ".", "collate_fn", ",", "\n", "batch_sampler", "=", "batches", ",", "\n", "num_workers", "=", "self", ".", "num_workers", ",", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.GroupedIterator.__init__": [[200, 204], ["int", "math.ceil", "len", "float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "chunk_size", ")", ":", "\n", "        ", "self", ".", "_len", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "iterable", ")", "/", "float", "(", "chunk_size", ")", ")", ")", "\n", "self", ".", "itr", "=", "iterable", "\n", "self", ".", "chunk_size", "=", "chunk_size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.GroupedIterator.__len__": [[205, 207], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.GroupedIterator.__iter__": [[208, 210], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.GroupedIterator.__next__": [[211, 220], ["range", "chunk.append", "next", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "chunk", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "_", "in", "range", "(", "self", ".", "chunk_size", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "next", "(", "self", ".", "itr", ")", ")", "\n", "", "", "except", "StopIteration", "as", "e", ":", "\n", "            ", "if", "len", "(", "chunk", ")", "==", "0", ":", "\n", "                ", "raise", "e", "\n", "", "", "return", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__init__": [[233, 245], ["itertools.zip_longest", "ValueError", "len", "range", "itertools.islice", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "iterable", ",", "num_shards", ",", "shard_id", ",", "fill_value", "=", "None", ")", ":", "\n", "        ", "if", "shard_id", "<", "0", "or", "shard_id", ">=", "num_shards", ":", "\n", "            ", "raise", "ValueError", "(", "'shard_id must be between 0 and num_shards'", ")", "\n", "\n", "", "self", ".", "_sharded_len", "=", "len", "(", "iterable", ")", "//", "num_shards", "\n", "if", "len", "(", "iterable", ")", "%", "num_shards", ">", "0", ":", "\n", "            ", "self", ".", "_sharded_len", "+=", "1", "\n", "\n", "", "self", ".", "itr", "=", "itertools", ".", "zip_longest", "(", "\n", "range", "(", "self", ".", "_sharded_len", ")", ",", "\n", "itertools", ".", "islice", "(", "iterable", ",", "shard_id", ",", "len", "(", "iterable", ")", ",", "num_shards", ")", ",", "\n", "fillvalue", "=", "fill_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__len__": [[247, 249], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sharded_len", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__iter__": [[250, 252], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.iterators.ShardedIterator.__next__": [[253, 255], ["next"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.next"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "[", "1", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.infer_language_pair": [[13, 21], ["os.listdir", "filename.split", "parts[].split", "len", "len", "parts[].split"], "function", ["None"], ["def", "infer_language_pair", "(", "path", ")", ":", "\n", "    ", "\"\"\"Infer language pair from filename: <split>.<lang1>-<lang2>.(...).idx\"\"\"", "\n", "src", ",", "dst", "=", "None", ",", "None", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "        ", "parts", "=", "filename", ".", "split", "(", "'.'", ")", "\n", "if", "len", "(", "parts", ")", ">=", "3", "and", "len", "(", "parts", "[", "1", "]", ".", "split", "(", "'-'", ")", ")", "==", "2", ":", "\n", "            ", "return", "parts", "[", "1", "]", ".", "split", "(", "'-'", ")", "\n", "", "", "return", "src", ",", "dst", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.collate_tokens": [[23, 40], ["max", "values[].new().fill_", "enumerate", "data_utils.collate_tokens.copy_tensor"], "function", ["None"], ["", "def", "collate_tokens", "(", "values", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "    ", "\"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"", "\n", "size", "=", "max", "(", "v", ".", "size", "(", "0", ")", "for", "v", "in", "values", ")", "\n", "res", "=", "values", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "\n", "def", "copy_tensor", "(", "src", ",", "dst", ")", ":", "\n", "        ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "if", "move_eos_to_beginning", ":", "\n", "            ", "assert", "src", "[", "-", "1", "]", "==", "eos_idx", "\n", "dst", "[", "0", "]", "=", "eos_idx", "\n", "dst", "[", "1", ":", "]", "=", "src", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "        ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", "size", "-", "len", "(", "v", ")", ":", "]", "if", "left_pad", "else", "res", "[", "i", "]", "[", ":", "len", "(", "v", ")", "]", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed": [[42, 55], ["numpy.random.get_state", "numpy.random.seed", "numpy.random.set_state"], "function", ["None"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "numpy_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n    restores the state afterward\"\"\"", "\n", "if", "seed", "is", "None", ":", "\n", "        ", "yield", "\n", "return", "\n", "", "state", "=", "np", ".", "random", ".", "get_state", "(", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "try", ":", "\n", "        ", "yield", "\n", "", "finally", ":", "\n", "        ", "np", ".", "random", ".", "set_state", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.collect_filtered": [[57, 72], ["function", "filtered.append"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "collect_filtered", "(", "function", ",", "iterable", ",", "filtered", ")", ":", "\n", "    ", "\"\"\"\n    Similar to :func:`filter` but collects filtered elements in ``filtered``.\n\n    Args:\n        function (callable): function that returns ``False`` for elements that\n            should be filtered\n        iterable (iterable): iterable to filter\n        filtered (list): list to store filtered elements\n    \"\"\"", "\n", "for", "el", "in", "iterable", ":", "\n", "        ", "if", "function", "(", "el", ")", ":", "\n", "            ", "yield", "el", "\n", "", "else", ":", "\n", "            ", "filtered", ".", "append", "(", "el", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.filter_by_size": [[74, 118], ["data_utils.collect_filtered", "len", "print", "isinstance", "isinstance", "isinstance", "Exception", "size_fn", "size_fn", "isinstance", "all", "all", "len", "len", "set", "set", "size_fn", "max_positions.keys", "size_fn.keys", "all", "zip", "size_fn", "zip"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.collect_filtered", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "", "", "def", "filter_by_size", "(", "indices", ",", "size_fn", ",", "max_positions", ",", "raise_exception", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Filter indices based on their size.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        size_fn (callable): function that returns the size of a given index\n        max_positions (tuple): filter elements larger than this size.\n            Comparisons are done component-wise.\n        raise_exception (bool, optional): if ``True``, raise an exception if\n            any elements are filtered (default: False).\n    \"\"\"", "\n", "def", "check_size", "(", "idx", ")", ":", "\n", "        ", "if", "isinstance", "(", "max_positions", ",", "float", ")", "or", "isinstance", "(", "max_positions", ",", "int", ")", ":", "\n", "            ", "return", "size_fn", "(", "idx", ")", "<=", "max_positions", "\n", "", "elif", "isinstance", "(", "max_positions", ",", "dict", ")", ":", "\n", "            ", "idx_size", "=", "size_fn", "(", "idx", ")", "\n", "assert", "isinstance", "(", "idx_size", ",", "dict", ")", "\n", "intersect_keys", "=", "set", "(", "max_positions", ".", "keys", "(", ")", ")", "&", "set", "(", "idx_size", ".", "keys", "(", ")", ")", "\n", "return", "all", "(", "\n", "all", "(", "a", "is", "None", "or", "b", "is", "None", "or", "a", "<=", "b", "\n", "for", "a", ",", "b", "in", "zip", "(", "idx_size", "[", "key", "]", ",", "max_positions", "[", "key", "]", ")", ")", "\n", "for", "key", "in", "intersect_keys", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "all", "(", "a", "is", "None", "or", "b", "is", "None", "or", "a", "<=", "b", "\n", "for", "a", ",", "b", "in", "zip", "(", "size_fn", "(", "idx", ")", ",", "max_positions", ")", ")", "\n", "\n", "", "", "ignored", "=", "[", "]", "\n", "itr", "=", "collect_filtered", "(", "check_size", ",", "indices", ",", "ignored", ")", "\n", "\n", "for", "idx", "in", "itr", ":", "\n", "        ", "if", "len", "(", "ignored", ")", ">", "0", "and", "raise_exception", ":", "\n", "            ", "raise", "Exception", "(", "(", "\n", "'Size of sample #{} is invalid (={}) since max_positions={}, '", "\n", "'skip this example with --skip-invalid-size-inputs-valid-test'", "\n", ")", ".", "format", "(", "ignored", "[", "0", "]", ",", "size_fn", "(", "ignored", "[", "0", "]", ")", ",", "max_positions", ")", ")", "\n", "", "yield", "idx", "\n", "\n", "", "if", "len", "(", "ignored", ")", ">", "0", ":", "\n", "        ", "print", "(", "(", "\n", "'| WARNING: {} samples have invalid sizes and will be skipped, '", "\n", "'max_positions={}, first few sample ids={}'", "\n", ")", ".", "format", "(", "len", "(", "ignored", ")", ",", "max_positions", ",", "ignored", "[", ":", "10", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.batch_by_size": [[120, 175], ["float", "float", "sample_lens.append", "max", "data_utils.batch_by_size.is_batch_full"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "batch_by_size", "(", "\n", "indices", ",", "num_tokens_fn", ",", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "\n", "required_batch_size_multiple", "=", "1", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Yield mini-batches of indices bucketed by size. Batches may contain\n    sequences of different lengths.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        num_tokens_fn (callable): function that returns the number of tokens at\n            a given index\n        max_tokens (int, optional): max number of tokens in each batch\n            (default: None).\n        max_sentences (int, optional): max number of sentences in each\n            batch (default: None).\n        required_batch_size_multiple (int, optional): require batch size to\n            be a multiple of N (default: 1).\n    \"\"\"", "\n", "max_tokens", "=", "max_tokens", "if", "max_tokens", "is", "not", "None", "else", "float", "(", "'Inf'", ")", "\n", "max_sentences", "=", "max_sentences", "if", "max_sentences", "is", "not", "None", "else", "float", "(", "'Inf'", ")", "\n", "bsz_mult", "=", "required_batch_size_multiple", "\n", "\n", "batch", "=", "[", "]", "\n", "\n", "def", "is_batch_full", "(", "num_tokens", ")", ":", "\n", "        ", "if", "len", "(", "batch", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "", "if", "len", "(", "batch", ")", "==", "max_sentences", ":", "\n", "            ", "return", "True", "\n", "", "if", "num_tokens", ">", "max_tokens", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "sample_len", "=", "0", "\n", "sample_lens", "=", "[", "]", "\n", "for", "idx", "in", "indices", ":", "\n", "        ", "sample_lens", ".", "append", "(", "num_tokens_fn", "(", "idx", ")", ")", "\n", "sample_len", "=", "max", "(", "sample_len", ",", "sample_lens", "[", "-", "1", "]", ")", "\n", "assert", "sample_len", "<=", "max_tokens", ",", "f\"sentence at index {idx} exceeds max_tokens limit!\"", "\n", "num_tokens", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "sample_len", "\n", "if", "is_batch_full", "(", "num_tokens", ")", ":", "\n", "            ", "mod_len", "=", "max", "(", "\n", "bsz_mult", "*", "(", "len", "(", "batch", ")", "//", "bsz_mult", ")", ",", "\n", "len", "(", "batch", ")", "%", "bsz_mult", ",", "\n", ")", "\n", "yield", "batch", "[", ":", "mod_len", "]", "\n", "batch", "=", "batch", "[", "mod_len", ":", "]", "\n", "sample_lens", "=", "sample_lens", "[", "mod_len", ":", "]", "\n", "sample_len", "=", "max", "(", "sample_lens", ")", "if", "len", "(", "sample_lens", ")", ">", "0", "else", "0", "\n", "\n", "", "batch", ".", "append", "(", "idx", ")", "\n", "\n", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "        ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.process_bpe_symbol": [[177, 183], ["sentence.replace().replace().strip.replace().replace().strip", "sentence.replace().replace().strip.replace().replace", "sentence.replace().replace().strip.replace"], "function", ["None"], ["", "", "def", "process_bpe_symbol", "(", "sentence", ":", "str", ",", "bpe_symbol", ":", "str", ")", ":", "\n", "    ", "if", "bpe_symbol", "==", "'sentencepiece'", ":", "\n", "        ", "sentence", "=", "sentence", ".", "replace", "(", "' '", ",", "''", ")", ".", "replace", "(", "'\\u2581'", ",", "' '", ")", ".", "strip", "(", ")", "\n", "", "elif", "bpe_symbol", "is", "not", "None", ":", "\n", "        ", "sentence", "=", "(", "sentence", "+", "' '", ")", ".", "replace", "(", "bpe_symbol", ",", "''", ")", ".", "rstrip", "(", ")", "\n", "", "return", "sentence", "\n", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.__init__": [[21, 32], ["dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol"], ["def", "__init__", "(", "self", ",", "pad", "=", "'<pad>'", ",", "eos", "=", "'</s>'", ",", "unk", "=", "'<unk>'", ")", ":", "\n", "        ", "self", ".", "unk_word", ",", "self", ".", "pad_word", ",", "self", ".", "eos_word", "=", "unk", ",", "pad", ",", "eos", "\n", "self", ".", "symbols", "=", "[", "]", "\n", "self", ".", "count", "=", "[", "]", "\n", "self", ".", "indices", "=", "{", "}", "\n", "# dictionary indexing starts at 1 for consistency with Lua", "\n", "self", ".", "add_symbol", "(", "'<Lua heritage>'", ")", "\n", "self", ".", "pad_index", "=", "self", ".", "add_symbol", "(", "pad", ")", "\n", "self", ".", "eos_index", "=", "self", ".", "add_symbol", "(", "eos", ")", "\n", "self", ".", "unk_index", "=", "self", ".", "add_symbol", "(", "unk", ")", "\n", "self", ".", "nspecial", "=", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.__eq__": [[33, 35], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "indices", "==", "other", ".", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.__getitem__": [[36, 40], ["len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "idx", "<", "len", "(", "self", ".", "symbols", ")", ":", "\n", "            ", "return", "self", ".", "symbols", "[", "idx", "]", "\n", "", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.__len__": [[41, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of symbols in the dictionary\"\"\"", "\n", "return", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index": [[45, 50], ["None"], "methods", ["None"], ["", "def", "index", "(", "self", ",", "sym", ")", ":", "\n", "        ", "\"\"\"Returns the index of the specified symbol\"\"\"", "\n", "if", "sym", "in", "self", ".", "indices", ":", "\n", "            ", "return", "self", ".", "indices", "[", "sym", "]", "\n", "", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.string": [[51, 67], ["fairseq.data.data_utils.process_bpe_symbol", "torch.is_tensor", "tensor.dim", "dictionary.Dictionary.unk", "dictionary.Dictionary.unk_string", "dictionary.Dictionary.string.token_string"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.process_bpe_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk_string"], ["", "def", "string", "(", "self", ",", "tensor", ",", "bpe_symbol", "=", "None", ",", "escape_unk", "=", "False", ")", ":", "\n", "        ", "\"\"\"Helper for converting a tensor of token indices to a string.\n\n        Can optionally remove BPE symbols or escape <unk> words.\n        \"\"\"", "\n", "if", "torch", ".", "is_tensor", "(", "tensor", ")", "and", "tensor", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "return", "'\\n'", ".", "join", "(", "self", ".", "string", "(", "t", ")", "for", "t", "in", "tensor", ")", "\n", "\n", "", "def", "token_string", "(", "i", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "unk", "(", ")", ":", "\n", "                ", "return", "self", ".", "unk_string", "(", "escape_unk", ")", "\n", "", "else", ":", "\n", "                ", "return", "self", "[", "i", "]", "\n", "\n", "", "", "sent", "=", "' '", ".", "join", "(", "token_string", "(", "i", ")", "for", "i", "in", "tensor", "if", "i", "!=", "self", ".", "eos", "(", ")", ")", "\n", "return", "data_utils", ".", "process_bpe_symbol", "(", "sent", ",", "bpe_symbol", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk_string": [[68, 74], ["None"], "methods", ["None"], ["", "def", "unk_string", "(", "self", ",", "escape", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return unknown string, optionally escaped as: <<unk>>\"\"\"", "\n", "if", "escape", ":", "\n", "            ", "return", "'<{}>'", ".", "format", "(", "self", ".", "unk_word", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol": [[75, 87], ["len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "add_symbol", "(", "self", ",", "word", ",", "n", "=", "1", ")", ":", "\n", "        ", "\"\"\"Adds a word to the dictionary\"\"\"", "\n", "if", "word", "in", "self", ".", "indices", ":", "\n", "            ", "idx", "=", "self", ".", "indices", "[", "word", "]", "\n", "self", ".", "count", "[", "idx", "]", "=", "self", ".", "count", "[", "idx", "]", "+", "n", "\n", "return", "idx", "\n", "", "else", ":", "\n", "            ", "idx", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "idx", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "n", ")", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update": [[88, 100], ["len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "def", "update", "(", "self", ",", "new_dict", ")", ":", "\n", "        ", "\"\"\"Updates counts from new dictionary.\"\"\"", "\n", "for", "word", "in", "new_dict", ".", "symbols", ":", "\n", "            ", "idx2", "=", "new_dict", ".", "indices", "[", "word", "]", "\n", "if", "word", "in", "self", ".", "indices", ":", "\n", "                ", "idx", "=", "self", ".", "indices", "[", "word", "]", "\n", "self", ".", "count", "[", "idx", "]", "=", "self", ".", "count", "[", "idx", "]", "+", "new_dict", ".", "count", "[", "idx2", "]", "\n", "", "else", ":", "\n", "                ", "idx", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "idx", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "new_dict", ".", "count", "[", "idx2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.finalize": [[101, 145], ["dict", "collections.Counter", "collections.Counter.most_common", "len", "list", "list", "len", "zip", "dict", "len", "len", "range", "zip", "len", "new_symbols.append", "new_count.append", "len", "new_symbols.append", "new_count.append", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "", "", "def", "finalize", "(", "self", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Sort symbols by frequency in descending order, ignoring special ones.\n\n        Args:\n            - threshold defines the minimum word count\n            - nwords defines the total number of words in the final dictionary,\n                including special symbols\n            - padding_factor can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"", "\n", "if", "nwords", "<=", "0", ":", "\n", "            ", "nwords", "=", "len", "(", "self", ")", "\n", "\n", "", "new_indices", "=", "dict", "(", "zip", "(", "self", ".", "symbols", "[", ":", "self", ".", "nspecial", "]", ",", "range", "(", "self", ".", "nspecial", ")", ")", ")", "\n", "new_symbols", "=", "self", ".", "symbols", "[", ":", "self", ".", "nspecial", "]", "\n", "new_count", "=", "self", ".", "count", "[", ":", "self", ".", "nspecial", "]", "\n", "\n", "c", "=", "Counter", "(", "dict", "(", "zip", "(", "self", ".", "symbols", "[", "self", ".", "nspecial", ":", "]", ",", "self", ".", "count", "[", "self", ".", "nspecial", ":", "]", ")", ")", ")", "\n", "for", "symbol", ",", "count", "in", "c", ".", "most_common", "(", "nwords", "-", "self", ".", "nspecial", ")", ":", "\n", "            ", "if", "count", ">=", "threshold", ":", "\n", "                ", "new_indices", "[", "symbol", "]", "=", "len", "(", "new_symbols", ")", "\n", "new_symbols", ".", "append", "(", "symbol", ")", "\n", "new_count", ".", "append", "(", "count", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "threshold_nwords", "=", "len", "(", "new_symbols", ")", "\n", "if", "padding_factor", ">", "1", ":", "\n", "            ", "i", "=", "0", "\n", "while", "threshold_nwords", "%", "padding_factor", "!=", "0", ":", "\n", "                ", "symbol", "=", "'madeupword{:04d}'", ".", "format", "(", "i", ")", "\n", "new_indices", "[", "symbol", "]", "=", "len", "(", "new_symbols", ")", "\n", "new_symbols", ".", "append", "(", "symbol", ")", "\n", "new_count", ".", "append", "(", "0", ")", "\n", "i", "+=", "1", "\n", "threshold_nwords", "+=", "1", "\n", "\n", "", "", "assert", "len", "(", "new_symbols", ")", "%", "padding_factor", "==", "0", "\n", "assert", "len", "(", "new_symbols", ")", "==", "len", "(", "new_indices", ")", "\n", "\n", "self", ".", "count", "=", "list", "(", "new_count", ")", "\n", "self", ".", "symbols", "=", "list", "(", "new_symbols", ")", "\n", "self", ".", "indices", "=", "new_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad": [[146, 149], ["None"], "methods", ["None"], ["", "def", "pad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of pad symbol\"\"\"", "\n", "return", "self", ".", "pad_index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos": [[150, 153], ["None"], "methods", ["None"], ["", "def", "eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of end-of-sentence symbol\"\"\"", "\n", "return", "self", ".", "eos_index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk": [[154, 157], ["None"], "methods", ["None"], ["", "def", "unk", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of unk symbol\"\"\"", "\n", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load": [[158, 195], ["isinstance", "cls", "f.readlines", "cls._load_meta", "line.rfind", "int", "len", "cls.symbols.append", "cls.count.append", "ValueError", "Exception", "open", "cls.load", "open", "cls.load"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._load_meta", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.load"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "f", ",", "ignore_utf_errors", "=", "False", ")", ":", "\n", "        ", "\"\"\"Loads the dictionary from a text file with the format:\n\n        ```\n        <symbol0> <count0>\n        <symbol1> <count1>\n        ...\n        ```\n        \"\"\"", "\n", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "not", "ignore_utf_errors", ":", "\n", "                    ", "with", "open", "(", "f", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                        ", "return", "cls", ".", "load", "(", "fd", ")", "\n", "", "", "else", ":", "\n", "                    ", "with", "open", "(", "f", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "fd", ":", "\n", "                        ", "return", "cls", ".", "load", "(", "fd", ")", "\n", "", "", "", "except", "FileNotFoundError", "as", "fnfe", ":", "\n", "                ", "raise", "fnfe", "\n", "", "except", "UnicodeError", ":", "\n", "                ", "raise", "Exception", "(", "\"Incorrect encoding detected in {}, please \"", "\n", "\"rebuild the dataset\"", ".", "format", "(", "f", ")", ")", "\n", "\n", "", "", "d", "=", "cls", "(", ")", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "indices_start_line", "=", "d", ".", "_load_meta", "(", "lines", ")", "\n", "for", "line", "in", "lines", "[", "indices_start_line", ":", "]", ":", "\n", "            ", "idx", "=", "line", ".", "rfind", "(", "' '", ")", "\n", "if", "idx", "==", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Incorrect dictionary format, expected '<token> <cnt>'\"", ")", "\n", "", "word", "=", "line", "[", ":", "idx", "]", "\n", "count", "=", "int", "(", "line", "[", "idx", "+", "1", ":", "]", ")", "\n", "d", ".", "indices", "[", "word", "]", "=", "len", "(", "d", ".", "symbols", ")", "\n", "d", ".", "symbols", ".", "append", "(", "word", ")", "\n", "d", ".", "count", ".", "append", "(", "count", ")", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._save": [[196, 203], ["isinstance", "os.makedirs", "print", "os.path.dirname", "open", "dictionary.Dictionary.save"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save"], ["", "def", "_save", "(", "self", ",", "f", ",", "kv_iterator", ")", ":", "\n", "        ", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "f", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "f", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                ", "return", "self", ".", "save", "(", "fd", ")", "\n", "", "", "for", "k", ",", "v", "in", "kv_iterator", ":", "\n", "            ", "print", "(", "'{} {}'", ".", "format", "(", "k", ",", "v", ")", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._get_meta": [[204, 206], ["None"], "methods", ["None"], ["", "", "def", "_get_meta", "(", "self", ")", ":", "\n", "        ", "return", "[", "]", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._load_meta": [[207, 209], ["None"], "methods", ["None"], ["", "def", "_load_meta", "(", "self", ",", "lines", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.save": [[210, 214], ["dictionary.Dictionary._get_meta", "dictionary.Dictionary._save", "zip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._get_meta", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._save", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "def", "save", "(", "self", ",", "f", ")", ":", "\n", "        ", "\"\"\"Stores dictionary into a text file\"\"\"", "\n", "ex_keys", ",", "ex_vals", "=", "self", ".", "_get_meta", "(", ")", "\n", "self", ".", "_save", "(", "f", ",", "zip", "(", "ex_keys", "+", "self", ".", "symbols", "[", "self", ".", "nspecial", ":", "]", ",", "ex_vals", "+", "self", ".", "count", "[", "self", ".", "nspecial", ":", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence": [[215, 219], ["torch.Tensor().uniform_().long", "dictionary.Dictionary.eos", "torch.Tensor().uniform_", "len", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "dummy_sentence", "(", "self", ",", "length", ")", ":", "\n", "        ", "t", "=", "torch", ".", "Tensor", "(", "length", ")", ".", "uniform_", "(", "self", ".", "nspecial", "+", "1", ",", "len", "(", "self", ")", ")", ".", "long", "(", ")", "\n", "t", "[", "-", "1", "]", "=", "self", ".", "eos", "(", ")", "\n", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line": [[220, 239], ["line_tokenizer", "len", "torch.IntTensor", "enumerate", "list", "reversed", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.index", "consumer"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.index"], ["", "def", "encode_line", "(", "self", ",", "line", ",", "line_tokenizer", "=", "tokenize_line", ",", "add_if_not_exist", "=", "True", ",", "\n", "consumer", "=", "None", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "        ", "words", "=", "line_tokenizer", "(", "line", ")", "\n", "if", "reverse_order", ":", "\n", "            ", "words", "=", "list", "(", "reversed", "(", "words", ")", ")", "\n", "", "nwords", "=", "len", "(", "words", ")", "\n", "ids", "=", "torch", ".", "IntTensor", "(", "nwords", "+", "1", "if", "append_eos", "else", "nwords", ")", "\n", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "if", "add_if_not_exist", ":", "\n", "                ", "idx", "=", "self", ".", "add_symbol", "(", "word", ")", "\n", "", "else", ":", "\n", "                ", "idx", "=", "self", ".", "index", "(", "word", ")", "\n", "", "if", "consumer", "is", "not", "None", ":", "\n", "                ", "consumer", "(", "word", ",", "idx", ")", "\n", "", "ids", "[", "i", "]", "=", "idx", "\n", "", "if", "append_eos", ":", "\n", "            ", "ids", "[", "nwords", "]", "=", "self", ".", "eos_index", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary._add_file_to_dictionary_single_worker": [[240, 260], ["collections.Counter", "open", "f.seek", "f.readline", "os.fstat", "fairseq.binarizer.safe_readline", "tokenize", "collections.Counter.update", "f.readline", "f.fileno", "collections.Counter.update", "f.tell"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.binarizer.safe_readline", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.update"], ["", "@", "staticmethod", "\n", "def", "_add_file_to_dictionary_single_worker", "(", "filename", ",", "tokenize", ",", "eos_word", ",", "worker_id", "=", "0", ",", "num_workers", "=", "1", ")", ":", "\n", "        ", "counter", "=", "Counter", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "size", "=", "os", ".", "fstat", "(", "f", ".", "fileno", "(", ")", ")", ".", "st_size", "\n", "chunk_size", "=", "size", "//", "num_workers", "\n", "offset", "=", "worker_id", "*", "chunk_size", "\n", "end", "=", "offset", "+", "chunk_size", "\n", "f", ".", "seek", "(", "offset", ")", "\n", "if", "offset", ">", "0", ":", "\n", "                ", "safe_readline", "(", "f", ")", "# drop first incomplete line", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "                ", "for", "word", "in", "tokenize", "(", "line", ")", ":", "\n", "                    ", "counter", ".", "update", "(", "[", "word", "]", ")", "\n", "", "counter", ".", "update", "(", "[", "eos_word", "]", ")", "\n", "if", "f", ".", "tell", "(", ")", ">", "end", ":", "\n", "                    ", "break", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "counter", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.add_file_to_dictionary": [[261, 281], ["counter.items", "multiprocessing.Pool", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "dictionary.Dictionary.add_file_to_dictionary.merge_result"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "@", "staticmethod", "\n", "def", "add_file_to_dictionary", "(", "filename", ",", "dict", ",", "tokenize", ",", "num_workers", ")", ":", "\n", "        ", "def", "merge_result", "(", "counter", ")", ":", "\n", "            ", "for", "w", ",", "c", "in", "counter", ".", "items", "(", ")", ":", "\n", "                ", "dict", ".", "add_symbol", "(", "w", ",", "c", ")", "\n", "\n", "", "", "if", "num_workers", ">", "1", ":", "\n", "            ", "pool", "=", "Pool", "(", "processes", "=", "num_workers", ")", "\n", "results", "=", "[", "]", "\n", "for", "worker_id", "in", "range", "(", "num_workers", ")", ":", "\n", "                ", "results", ".", "append", "(", "pool", ".", "apply_async", "(", "\n", "Dictionary", ".", "_add_file_to_dictionary_single_worker", ",", "\n", "(", "filename", ",", "tokenize", ",", "dict", ".", "eos_word", ",", "worker_id", ",", "num_workers", ")", "\n", ")", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "for", "r", "in", "results", ":", "\n", "                ", "merge_result", "(", "r", ".", "get", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "merge_result", "(", "Dictionary", ".", "_add_file_to_dictionary_single_worker", "(", "filename", ",", "tokenize", ",", "dict", ".", "eos_word", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.TruncatedDictionary.__init__": [[284, 293], ["type", "min", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "wrapped_dict", ",", "length", ")", ":", "\n", "        ", "self", ".", "__class__", "=", "type", "(", "\n", "wrapped_dict", ".", "__class__", ".", "__name__", ",", "\n", "(", "self", ".", "__class__", ",", "wrapped_dict", ".", "__class__", ")", ",", "\n", "{", "}", "\n", ")", "\n", "self", ".", "__dict__", "=", "wrapped_dict", ".", "__dict__", "\n", "self", ".", "wrapped_dict", "=", "wrapped_dict", "\n", "self", ".", "length", "=", "min", "(", "len", "(", "self", ".", "wrapped_dict", ")", ",", "length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.TruncatedDictionary.__len__": [[294, 296], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.TruncatedDictionary.__getitem__": [[297, 301], ["dictionary.TruncatedDictionary.wrapped_dict.unk"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "self", ".", "length", ":", "\n", "            ", "return", "self", ".", "wrapped_dict", "[", "i", "]", "\n", "", "return", "self", ".", "wrapped_dict", ".", "unk", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.__init__": [[122, 131], ["token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.read_data", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["def", "__init__", "(", "self", ",", "path", ",", "dictionary", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokens_list", "=", "[", "]", "\n", "self", ".", "labels_list", "=", "[", "]", "\n", "self", ".", "lines", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "append_eos", "=", "append_eos", "\n", "self", ".", "reverse_order", "=", "reverse_order", "\n", "self", ".", "read_data", "(", "path", ",", "dictionary", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.read_data": [[132, 169], ["numpy.array", "open", "itertools.zip_longest", "token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.lines.append", "dictionary.encode_line().long", "token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.tokens_list.append", "token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.sizes.append", "encode_labels_line().long", "token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.labels_list.append", "tokens_line.strip", "len", "len", "len", "dictionary.encode_line", "token_labeled_language_pair_dataset.encode_labels_line", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.encode_labels_line"], ["", "def", "read_data", "(", "self", ",", "path", ",", "dictionary", ")", ":", "\n", "        ", "\"\"\"Reads in a file that looks like:\n\n        ```text\n        My teacher is going to move to change his job .\n        0 0 0 0 0 0 0 0 0 0 0\n        And he took in my favorite subject like soccer .\n        0 0 0 0 0 0 1 0 0 0\n        ...\n        ```\n\n        For now, labels are always assumed to be in the labels dictionary.\n        \"\"\"", "\n", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# Iterate over every pair of lines without loading all at once", "\n", "            ", "for", "tokens_line", ",", "labels_line", "in", "itertools", ".", "zip_longest", "(", "*", "[", "f", "]", "*", "2", ")", ":", "\n", "# Tokens proceed the same as in a default IndexedDataset", "\n", "                ", "self", ".", "lines", ".", "append", "(", "tokens_line", ".", "strip", "(", "'\\n'", ")", ")", "\n", "tokens", "=", "dictionary", ".", "encode_line", "(", "\n", "tokens_line", ",", "add_if_not_exist", "=", "False", ",", "\n", "append_eos", "=", "self", ".", "append_eos", ",", "reverse_order", "=", "self", ".", "reverse_order", ",", "\n", ")", ".", "long", "(", ")", "\n", "self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "# Labels stored in parallel", "\n", "labels", "=", "encode_labels_line", "(", "\n", "labels_line", ",", "\n", "append_eos", "=", "self", ".", "append_eos", ",", "reverse_order", "=", "self", ".", "reverse_order", "\n", ")", ".", "long", "(", ")", "\n", "self", ".", "labels_list", ".", "append", "(", "labels", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "labels", ")", ",", "f\"TokenLabeledIndexedRawTextDataset.read_data: \"", "f\"number of tokens ({len(tokens)}) and \"", "f\"labels ({len(labels)}) not matching\"", "\n", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.check_index": [[170, 173], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.__getitem__": [[174, 180], ["token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "\"\"\"Returns a **dict** of tokens and labels.\"\"\"", "\n", "self", ".", "check_index", "(", "i", ")", "\n", "return", "{", "\n", "\"tokens\"", ":", "self", ".", "tokens_list", "[", "i", "]", ",", "\n", "\"labels\"", ":", "self", ".", "labels_list", "[", "i", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.get_original_text": [[182, 185], ["token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "lines", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.__del__": [[186, 188], ["None"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.__len__": [[189, 191], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledIndexedRawTextDataset.exists": [[192, 195], ["os.path.exists"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.__init__": [[231, 244], ["fairseq.data.language_pair_dataset.LanguagePairDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "\n", "self", ",", "src", ",", "src_sizes", ",", "src_dict", ",", "\n", "tgt", "=", "None", ",", "tgt_sizes", "=", "None", ",", "tgt_dict", "=", "None", ",", "\n", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "max_source_positions", "=", "1024", ",", "max_target_positions", "=", "1024", ",", "\n", "shuffle", "=", "True", ",", "input_feeding", "=", "True", ",", "remove_eos_from_source", "=", "False", ",", "append_eos_to_target", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "src", ",", "src_sizes", ",", "src_dict", ",", "\n", "tgt", ",", "tgt_sizes", ",", "tgt_dict", ",", "\n", "left_pad_source", ",", "left_pad_target", ",", "\n", "max_source_positions", ",", "max_target_positions", ",", "\n", "shuffle", ",", "input_feeding", ",", "remove_eos_from_source", ",", "append_eos_to_target", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.__getitem__": [[246, 275], ["token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.src_dict.eos", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.tgt_dict.eos", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.src_dict.eos", "torch.cat", "torch.cat", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "tgt_item", "=", "self", ".", "tgt", "[", "index", "]", "if", "self", ".", "tgt", "is", "not", "None", "else", "None", "\n", "src_item", "=", "self", ".", "src", "[", "index", "]", "\n", "# Append EOS to end of tgt sentence if it does not have an EOS and remove", "\n", "# EOS from end of src sentence if it exists. This is useful when we use", "\n", "# use existing datasets for opposite directions i.e., when we want to", "\n", "# use tgt_dataset as src_dataset and vice versa", "\n", "# **Change in TokenLabeled**: also append 0 to labels if EOS is added.", "\n", "if", "self", ".", "append_eos_to_target", ":", "\n", "            ", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", "if", "self", ".", "tgt_dict", "else", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "if", "self", ".", "tgt", "and", "self", ".", "tgt", "[", "index", "]", "[", "\"tokens\"", "]", "[", "-", "1", "]", "!=", "eos", ":", "\n", "                ", "tgt_tokens", "=", "torch", ".", "cat", "(", "[", "self", ".", "tgt", "[", "index", "]", "[", "\"tokens\"", "]", ",", "\n", "torch", ".", "LongTensor", "(", "[", "eos", "]", ")", "]", ")", "\n", "tgt_labels", "=", "torch", ".", "cat", "(", "[", "self", ".", "tgt", "[", "index", "]", "[", "\"labels\"", "]", ",", "\n", "torch", ".", "LongTensor", "(", "[", "0", "]", ")", "]", ")", "\n", "tgt_item", "=", "{", "\"tokens\"", ":", "tgt_tokens", ",", "\"labels\"", ":", "tgt_labels", "}", "\n", "\n", "", "", "if", "self", ".", "remove_eos_from_source", ":", "\n", "            ", "eos", "=", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "# tokens and labels have same length", "\n", "if", "self", ".", "src", "[", "index", "]", "[", "\"tokens\"", "]", "[", "-", "1", "]", "==", "eos", ":", "\n", "                ", "src_tokens", "=", "self", ".", "src", "[", "index", "]", "[", "\"tokens\"", "]", "[", ":", "-", "1", "]", "\n", "src_labels", "=", "self", ".", "src", "[", "index", "]", "[", "\"labels\"", "]", "[", ":", "-", "1", "]", "\n", "src_item", "=", "{", "\"tokens\"", ":", "src_tokens", ",", "\"labels\"", ":", "src_labels", "}", "\n", "\n", "", "", "return", "{", "\n", "'id'", ":", "index", ",", "\n", "'source'", ":", "src_item", ",", "\n", "'target'", ":", "tgt_item", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.collater": [[277, 312], ["token_labeled_language_pair_dataset.collate", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.src_dict.pad", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.src_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the left if *left_pad_source* is ``True``.\n                  - `src_lengths` (LongTensor): 1D Tensor of the unpadded\n                    lengths of each source sentence of shape `(bsz)`\n                  - `src_labels` (LongTensor): 1D Tensor of the token-level\n                    labels of each source sentence of shape `(bsz, src_len)`.\n                  - `prev_output_tokens` (LongTensor): a padded 2D Tensor of\n                    tokens in the target sentence, shifted right by one position\n                    for input feeding/teacher forcing, of shape `(bsz,\n                    tgt_len)`. This key will not be present if *input_feeding*\n                    is ``False``. Padding will appear on the left if\n                    *left_pad_target* is ``True``.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the left if *left_pad_target* is ``True``.\n        \"\"\"", "\n", "return", "collate", "(", "\n", "samples", ",", "pad_idx", "=", "self", ".", "src_dict", ".", "pad", "(", ")", ",", "eos_idx", "=", "self", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", "left_pad_source", "=", "self", ".", "left_pad_source", ",", "left_pad_target", "=", "self", ".", "left_pad_target", ",", "\n", "input_feeding", "=", "self", ".", "input_feeding", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.get_dummy_batch": [[314, 338], ["fairseq.utils.resolve_max_positions", "max", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.src_dict.dummy_sentence", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.tgt_dict.dummy_sentence", "token_labeled_language_pair_dataset.TokenLabeledLanguagePairDataset.collater", "max", "range", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "get_dummy_batch", "(", "self", ",", "num_tokens", ",", "max_positions", ",", "src_len", "=", "128", ",", "tgt_len", "=", "128", ")", ":", "\n", "        ", "\"\"\"Return a dummy batch with a given number of tokens.\"\"\"", "\n", "src_len", ",", "tgt_len", "=", "utils", ".", "resolve_max_positions", "(", "\n", "(", "src_len", ",", "tgt_len", ")", ",", "\n", "max_positions", ",", "\n", "(", "self", ".", "max_source_positions", ",", "self", ".", "max_target_positions", ")", ",", "\n", ")", "\n", "bsz", "=", "max", "(", "num_tokens", "//", "max", "(", "src_len", ",", "tgt_len", ")", ",", "1", ")", "\n", "\n", "src_dummy", "=", "self", ".", "src_dict", ".", "dummy_sentence", "(", "src_len", ")", "\n", "tgt_dummy", "=", "self", ".", "tgt_dict", ".", "dummy_sentence", "(", "tgt_len", ")", "\n", "return", "self", ".", "collater", "(", "[", "\n", "{", "\n", "'id'", ":", "i", ",", "\n", "'source'", ":", "{", "\n", "\"tokens\"", ":", "src_dummy", ",", "\n", "\"labels\"", ":", "torch", ".", "zeros_like", "(", "src_dummy", ")", ",", "\n", "}", ",", "\n", "'target'", ":", "{", "\n", "\"tokens\"", ":", "tgt_dummy", ",", "\n", "\"labels\"", ":", "torch", ".", "zeros_like", "(", "tgt_dummy", ")", ",", "\n", "}", "if", "self", ".", "tgt_dict", "is", "not", "None", "else", "None", ",", "\n", "}", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.collate": [[24, 94], ["torch.LongTensor", "token_labeled_language_pair_dataset.collate.merge"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.merge"], ["def", "collate", "(", "\n", "samples", ",", "pad_idx", ",", "eos_idx", ",", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "input_feeding", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Create batches out of items in a TokenLabeledLanguagePairDataset.\n\n    Note that labels are always binary, meaning the same index may be used for\n    both label and padding. Still, we can recover only the valid labels by\n    looking at either src_tokens or src_lengths.\n    \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "tokens_or_labels", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "\"\"\"Each source or target item is now a dict that looks like\n        {'tokens': tokens, 'labels': labels}.\n        \"\"\"", "\n", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "[", "tokens_or_labels", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", ",", "\n", ")", "\n", "\n", "", "id", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", "\n", "src_tokens", "=", "merge", "(", "'source'", ",", "'tokens'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "src_labels", "=", "merge", "(", "'source'", ",", "'labels'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "# sort by descending source length", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'source'", "]", "[", "'tokens'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "]", ")", "\n", "src_lengths", ",", "sort_order", "=", "src_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "id", "=", "id", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_tokens", "=", "src_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_labels", "=", "src_labels", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "prev_output_tokens", "=", "None", "\n", "tgt_tokens", ",", "tgt_labels", "=", "None", ",", "None", "\n", "if", "samples", "[", "0", "]", ".", "get", "(", "'target'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "tgt_tokens", "=", "merge", "(", "'target'", ",", "'tokens'", ",", "left_pad", "=", "left_pad_target", ")", "\n", "tgt_labels", "=", "merge", "(", "'target'", ",", "'labels'", ",", "left_pad", "=", "left_pad_target", ")", "\n", "tgt_tokens", "=", "tgt_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "tgt_labels", "=", "tgt_labels", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'target'", "]", "[", "'tokens'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "if", "input_feeding", ":", "\n", "# we create a shifted version of targets for feeding the", "\n", "# previous output token(s) into the next decoder step", "\n", "            ", "prev_output_tokens", "=", "merge", "(", "\n", "'target'", ",", "\n", "'tokens'", ",", "\n", "left_pad", "=", "left_pad_target", ",", "\n", "move_eos_to_beginning", "=", "True", ",", "\n", ")", "\n", "prev_output_tokens", "=", "prev_output_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "", "", "else", ":", "\n", "        ", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'source'", "]", "[", "'tokens'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "", "batch", "=", "{", "\n", "'id'", ":", "id", ",", "\n", "'nsentences'", ":", "len", "(", "samples", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n", "'target'", ":", "tgt_tokens", ",", "\n", "# additional token-level label targets", "\n", "'src_labels'", ":", "src_labels", ",", "\n", "'tgt_labels'", ":", "tgt_labels", ",", "\n", "}", "\n", "if", "prev_output_tokens", "is", "not", "None", ":", "\n", "        ", "batch", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", "=", "prev_output_tokens", "\n", "", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.token_labeled_language_pair_dataset.encode_labels_line": [[96, 113], ["all", "torch.tensor", "int", "list", "list.append", "fairseq.tokenizer.tokenize_line", "reversed"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.tokenizer.tokenize_line"], ["", "def", "encode_labels_line", "(", "labels_line", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "    ", "\"\"\"Custom helper:\n    Encode a string of space-separated binary labels into LongTensor.\n\n    Mimicks fairseq.data.dictionary.Dictionary.encode_line().\n    eos always gets a zero token (no change).\n\n    Returns a torch.IntTensor, analogous to dictionary's encode_line() method.\n    \"\"\"", "\n", "labels", "=", "[", "int", "(", "label", ")", "for", "label", "in", "tokenize_line", "(", "labels_line", ")", "]", "\n", "assert", "all", "(", "[", "label", "in", "[", "0", ",", "1", "]", "for", "label", "in", "labels", "]", ")", ",", "f\"encode_labels_line: token-level labels must be binary!\"", "\n", "if", "reverse_order", ":", "\n", "        ", "labels", "=", "list", "(", "reversed", "(", "labels", ")", ")", "\n", "", "if", "append_eos", ":", "\n", "        ", "labels", ".", "append", "(", "0", ")", "\n", "", "return", "torch", ".", "tensor", "(", "labels", ",", "dtype", "=", "torch", ".", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.__init__": [[27, 62], ["torch.LongTensor", "isinstance", "ValueError", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "eos", ",", "\n", "append_eos_to_src", "=", "False", ",", "\n", "remove_eos_from_src", "=", "False", ",", "\n", "append_eos_to_tgt", "=", "False", ",", "\n", "remove_eos_from_tgt", "=", "False", ",", "\n", "has_target", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'dataset must be an instance of FairseqDataset'", ")", "\n", "", "if", "append_eos_to_src", "and", "remove_eos_from_src", ":", "\n", "            ", "raise", "ValueError", "(", "'cannot combine append_eos_to_src and remove_eos_from_src'", ")", "\n", "", "if", "append_eos_to_tgt", "and", "remove_eos_from_tgt", ":", "\n", "            ", "raise", "ValueError", "(", "'cannot combine append_eos_to_tgt and remove_eos_from_tgt'", ")", "\n", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "eos", "=", "torch", ".", "LongTensor", "(", "[", "eos", "]", ")", "\n", "self", ".", "append_eos_to_src", "=", "append_eos_to_src", "\n", "self", ".", "remove_eos_from_src", "=", "remove_eos_from_src", "\n", "self", ".", "append_eos_to_tgt", "=", "append_eos_to_tgt", "\n", "self", ".", "remove_eos_from_tgt", "=", "remove_eos_from_tgt", "\n", "self", ".", "has_target", "=", "has_target", "\n", "\n", "# precompute how we should adjust the reported sizes", "\n", "self", ".", "_src_delta", "=", "0", "\n", "self", ".", "_src_delta", "+=", "1", "if", "append_eos_to_src", "else", "0", "\n", "self", ".", "_src_delta", "-=", "1", "if", "remove_eos_from_src", "else", "0", "\n", "self", ".", "_tgt_delta", "=", "0", "\n", "self", ".", "_tgt_delta", "+=", "1", "if", "append_eos_to_tgt", "else", "0", "\n", "self", ".", "_tgt_delta", "-=", "1", "if", "remove_eos_from_tgt", "else", "0", "\n", "\n", "self", ".", "_checked_src", "=", "False", "\n", "self", ".", "_checked_tgt", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_src": [[63, 67], ["None"], "methods", ["None"], ["", "def", "_check_src", "(", "self", ",", "src", ",", "expect_eos", ")", ":", "\n", "        ", "if", "not", "self", ".", "_checked_src", ":", "\n", "            ", "assert", "(", "src", "[", "-", "1", "]", "==", "self", ".", "eos", "[", "0", "]", ")", "==", "expect_eos", "\n", "self", ".", "_checked_src", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_tgt": [[68, 72], ["None"], "methods", ["None"], ["", "", "def", "_check_tgt", "(", "self", ",", "tgt", ",", "expect_eos", ")", ":", "\n", "        ", "if", "self", ".", "has_target", "and", "not", "self", ".", "_checked_tgt", ":", "\n", "            ", "assert", "(", "tgt", "[", "-", "1", "]", "==", "self", ".", "eos", "[", "0", "]", ")", "==", "expect_eos", "\n", "self", ".", "_checked_tgt", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.__getitem__": [[73, 75], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.__len__": [[76, 78], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.collater": [[79, 98], ["list", "transform_eos_dataset.TransformEosDataset.dataset.collater", "map", "transform_eos_dataset.TransformEosDataset._check_src", "torch.cat", "transform_eos_dataset.TransformEosDataset._check_src", "transform_eos_dataset.TransformEosDataset._check_tgt", "torch.cat", "transform_eos_dataset.TransformEosDataset._check_tgt"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_src", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_src", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_tgt", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset._check_tgt"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "\n", "        ", "def", "transform", "(", "item", ")", ":", "\n", "            ", "if", "self", ".", "append_eos_to_src", ":", "\n", "                ", "self", ".", "_check_src", "(", "item", "[", "'source'", "]", ",", "expect_eos", "=", "False", ")", "\n", "item", "[", "'source'", "]", "=", "torch", ".", "cat", "(", "[", "item", "[", "'source'", "]", ",", "self", ".", "eos", "]", ")", "\n", "", "if", "self", ".", "remove_eos_from_src", ":", "\n", "                ", "self", ".", "_check_src", "(", "item", "[", "'source'", "]", ",", "expect_eos", "=", "True", ")", "\n", "item", "[", "'source'", "]", "=", "item", "[", "'source'", "]", "[", ":", "-", "1", "]", "\n", "", "if", "self", ".", "append_eos_to_tgt", ":", "\n", "                ", "self", ".", "_check_tgt", "(", "item", "[", "'target'", "]", ",", "expect_eos", "=", "False", ")", "\n", "item", "[", "'target'", "]", "=", "torch", ".", "cat", "(", "[", "item", "[", "'target'", "]", ",", "self", ".", "eos", "]", ")", "\n", "", "if", "self", ".", "remove_eos_from_tgt", ":", "\n", "                ", "self", ".", "_check_tgt", "(", "item", "[", "'target'", "]", ",", "expect_eos", "=", "True", ")", "\n", "item", "[", "'target'", "]", "=", "item", "[", "'target'", "]", "[", ":", "-", "1", "]", "\n", "", "return", "item", "\n", "\n", "", "samples", "=", "list", "(", "map", "(", "transform", ",", "samples", ")", ")", "\n", "return", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.get_dummy_batch": [[99, 101], ["transform_eos_dataset.TransformEosDataset.dataset.get_dummy_batch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch"], ["", "def", "get_dummy_batch", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "get_dummy_batch", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.num_tokens": [[102, 104], ["transform_eos_dataset.TransformEosDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.size": [[105, 111], ["transform_eos_dataset.TransformEosDataset.dataset.size", "transform_eos_dataset.TransformEosDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "has_target", ":", "\n", "            ", "src_len", ",", "tgt_len", "=", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "return", "(", "src_len", "+", "self", ".", "_src_delta", ",", "tgt_len", "+", "self", ".", "_tgt_delta", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.ordered_indices": [[112, 116], ["transform_eos_dataset.TransformEosDataset.dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices"], ["", "", "def", "ordered_indices", "(", "self", ")", ":", "\n", "# NOTE: we assume that the ordering does not change based on the", "\n", "# addition or removal of eos", "\n", "        ", "return", "self", ".", "dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.supports_prefetch": [[117, 120], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.transform_eos_dataset.TransformEosDataset.prefetch": [[121, 123], ["transform_eos_dataset.TransformEosDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordNoising.__init__": [[16, 34], ["numpy.array", "numpy.array", "noising.WordNoising.dictionary[].endswith", "range", "noising.WordNoising.dictionary[].endswith", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "bpe_end", "=", "None", "\n", "if", "bpe_cont_marker", ":", "\n", "            ", "self", ".", "bpe_end", "=", "np", ".", "array", "(", "[", "\n", "not", "self", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont_marker", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "dictionary", ")", ")", "\n", "]", ")", "\n", "", "elif", "bpe_end_marker", ":", "\n", "            ", "self", ".", "bpe_end", "=", "np", ".", "array", "(", "[", "\n", "self", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_end_marker", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "dictionary", ")", ")", "\n", "]", ")", "\n", "\n", "", "self", ".", "get_word_idx", "=", "(", "\n", "self", ".", "_get_bpe_word_idx", "\n", "if", "self", ".", "bpe_end", "is", "not", "None", "\n", "else", "self", ".", "_get_token_idx", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordNoising.noising": [[36, 38], ["NotImplementedError"], "methods", ["None"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "noising_prob", "=", "0.0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordNoising._get_bpe_word_idx": [[39, 59], ["numpy.array", "bpe_end[].cumsum", "x.size", "x.size", "word_idx.max"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "_get_bpe_word_idx", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Given a list of BPE tokens, for every index in the tokens list,\n        return the index of the word grouping that it belongs to.\n        For example, for input x corresponding to [\"how\", \"are\", \"y@@\", \"ou\"],\n        return [[0], [1], [2], [2]].\n        \"\"\"", "\n", "# x: (T x B)", "\n", "bpe_end", "=", "self", ".", "bpe_end", "[", "x", "]", "\n", "\n", "if", "(", "x", ".", "size", "(", "0", ")", "==", "1", "and", "x", ".", "size", "(", "1", ")", "==", "1", ")", ":", "\n", "# Special case when we only have one word in x. If x = [[N]],", "\n", "# bpe_end is a scalar (bool) instead of a 2-dim array of bools,", "\n", "# which makes the sum operation below fail.", "\n", "            ", "return", "np", ".", "array", "(", "[", "[", "0", "]", "]", ")", "\n", "\n", "# do a reduce front sum to generate word ids", "\n", "", "word_idx", "=", "bpe_end", "[", ":", ":", "-", "1", "]", ".", "cumsum", "(", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "word_idx", "=", "word_idx", ".", "max", "(", "0", ")", "[", "None", ",", ":", "]", "-", "word_idx", "\n", "return", "word_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordNoising._get_token_idx": [[60, 68], ["torch.t", "numpy.array", "numpy.transpose", "range", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "_get_token_idx", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        This is to extend noising functions to be able to apply to non-bpe\n        tokens, e.g. word or characters.\n        \"\"\"", "\n", "x", "=", "torch", ".", "t", "(", "x", ")", "\n", "word_idx", "=", "np", ".", "array", "(", "[", "range", "(", "len", "(", "x_i", ")", ")", "for", "x_i", "in", "x", "]", ")", "\n", "return", "np", ".", "transpose", "(", "word_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordDropout.__init__": [[75, 77], ["noising.WordNoising.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ",", "bpe_cont_marker", ",", "bpe_end_marker", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordDropout.noising": [[78, 141], ["noising.WordDropout.get_word_idx", "range", "torch.LongTensor", "torch.LongTensor().fill_", "range", "lengths.size", "x[].tolist", "sentences.append", "torch.LongTensor.append", "noising.WordDropout.dictionary.pad", "torch.LongTensor.size", "modified_x[].copy_", "max", "noising.WordDropout.dictionary.eos", "numpy.append", "len", "new_s.insert", "len", "torch.LongTensor", "torch.LongTensor", "numpy.random.rand", "numpy.random.rand", "enumerate", "len", "torch.LongTensor.max", "torch.LongTensor.size", "numpy.random.randint", "len", "noising.WordDropout.dictionary.eos", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.insert", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "dropout_prob", "=", "0.1", ",", "blank_idx", "=", "None", ")", ":", "\n", "# x: (T x B), lengths: B", "\n", "        ", "if", "dropout_prob", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n", "", "assert", "0", "<", "dropout_prob", "<", "1", "\n", "\n", "# be sure to drop entire words", "\n", "word_idx", "=", "self", ".", "get_word_idx", "(", "x", ")", "\n", "sentences", "=", "[", "]", "\n", "modified_lengths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "# Since dropout probabilities need to apply over non-pad tokens,", "\n", "# it is not trivial to generate the keep mask without consider", "\n", "# input lengths; otherwise, this could be done outside the loop", "\n", "\n", "# We want to drop whole words based on word_idx grouping", "\n", "            ", "num_words", "=", "max", "(", "word_idx", "[", ":", ",", "i", "]", ")", "+", "1", "\n", "\n", "# ith example: [x0, x1, ..., eos, pad, ..., pad]", "\n", "# We should only generate keep probs for non-EOS tokens. Thus if the", "\n", "# input sentence ends in EOS, the last word idx is not included in", "\n", "# the dropout mask generation and we append True to always keep EOS.", "\n", "# Otherwise, just generate the dropout mask for all word idx", "\n", "# positions.", "\n", "has_eos", "=", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", "\n", "if", "has_eos", ":", "# has eos?", "\n", "                ", "keep", "=", "np", ".", "random", ".", "rand", "(", "num_words", "-", "1", ")", ">=", "dropout_prob", "\n", "keep", "=", "np", ".", "append", "(", "keep", ",", "[", "True", "]", ")", "# keep EOS symbol", "\n", "", "else", ":", "\n", "                ", "keep", "=", "np", ".", "random", ".", "rand", "(", "num_words", ")", ">=", "dropout_prob", "\n", "\n", "", "words", "=", "x", "[", ":", "lengths", "[", "i", "]", ",", "i", "]", ".", "tolist", "(", ")", "\n", "\n", "# TODO: speed up the following loop", "\n", "# drop words from the input according to keep", "\n", "new_s", "=", "[", "\n", "w", "if", "keep", "[", "word_idx", "[", "j", ",", "i", "]", "]", "else", "blank_idx", "\n", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "\n", "]", "\n", "new_s", "=", "[", "w", "for", "w", "in", "new_s", "if", "w", "is", "not", "None", "]", "\n", "# we need to have at least one word in the sentence (more than the", "\n", "# start / end sentence symbols)", "\n", "if", "len", "(", "new_s", ")", "<=", "1", ":", "\n", "# insert at beginning in case the only token left is EOS", "\n", "# EOS should be at end of list.", "\n", "                ", "new_s", ".", "insert", "(", "0", ",", "words", "[", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "words", ")", ")", "]", ")", "\n", "", "assert", "len", "(", "new_s", ")", ">=", "1", "and", "(", "\n", "not", "has_eos", "# Either don't have EOS at end or last token is EOS", "\n", "or", "(", "len", "(", "new_s", ")", ">=", "2", "and", "new_s", "[", "-", "1", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", ")", "\n", ")", ",", "\"New sentence is invalid.\"", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "modified_lengths", ".", "append", "(", "len", "(", "new_s", ")", ")", "\n", "# re-construct input", "\n", "", "modified_lengths", "=", "torch", ".", "LongTensor", "(", "modified_lengths", ")", "\n", "modified_x", "=", "torch", ".", "LongTensor", "(", "\n", "modified_lengths", ".", "max", "(", ")", ",", "\n", "modified_lengths", ".", "size", "(", "0", ")", "\n", ")", ".", "fill_", "(", "self", ".", "dictionary", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "modified_lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "modified_x", "[", ":", "modified_lengths", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "\n", "", "return", "modified_x", ",", "modified_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordShuffle.__init__": [[146, 148], ["noising.WordNoising.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ",", "bpe_cont_marker", ",", "bpe_end_marker", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.WordShuffle.noising": [[149, 181], ["numpy.random.uniform", "noising.WordShuffle.get_word_idx", "x.clone", "range", "lengths.size", "scores.argsort", "x2[].copy_", "noising.WordShuffle.dictionary.eos", "numpy.arange", "x.size", "x.size", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "max_shuffle_distance", "=", "3", ")", ":", "\n", "# x: (T x B), lengths: B", "\n", "        ", "if", "max_shuffle_distance", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n", "# max_shuffle_distance < 1 will return the same sequence", "\n", "", "assert", "max_shuffle_distance", ">", "1", "\n", "\n", "# define noise word scores", "\n", "noise", "=", "np", ".", "random", ".", "uniform", "(", "\n", "0", ",", "\n", "max_shuffle_distance", ",", "\n", "size", "=", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", ",", "\n", ")", "\n", "noise", "[", "0", "]", "=", "-", "1", "# do not move start sentence symbol", "\n", "# be sure to shuffle entire words", "\n", "word_idx", "=", "self", ".", "get_word_idx", "(", "x", ")", "\n", "x2", "=", "x", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "length_no_eos", "=", "lengths", "[", "i", "]", "\n", "if", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "\n", "                ", "length_no_eos", "=", "lengths", "[", "i", "]", "-", "1", "\n", "# generate a random permutation", "\n", "", "scores", "=", "word_idx", "[", ":", "length_no_eos", ",", "i", "]", "+", "noise", "[", "word_idx", "[", ":", "length_no_eos", ",", "i", "]", ",", "i", "]", "\n", "# ensure no reordering inside a word", "\n", "scores", "+=", "1e-6", "*", "np", ".", "arange", "(", "length_no_eos", ")", "\n", "permutation", "=", "scores", ".", "argsort", "(", ")", "\n", "# shuffle words", "\n", "x2", "[", ":", "length_no_eos", ",", "i", "]", ".", "copy_", "(", "\n", "x2", "[", ":", "length_no_eos", ",", "i", "]", "[", "torch", ".", "from_numpy", "(", "permutation", ")", "]", "\n", ")", "\n", "", "return", "x2", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.__init__": [[188, 211], ["noising.WordNoising.__init__", "noising.WordDropout", "noising.WordShuffle"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dictionary", ",", "\n", "max_word_shuffle_distance", ",", "\n", "word_dropout_prob", ",", "\n", "word_blanking_prob", ",", "\n", "bpe_cont_marker", "=", "\"@@\"", ",", "\n", "bpe_end_marker", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "max_word_shuffle_distance", "=", "max_word_shuffle_distance", "\n", "self", ".", "word_dropout_prob", "=", "word_dropout_prob", "\n", "self", ".", "word_blanking_prob", "=", "word_blanking_prob", "\n", "\n", "self", ".", "word_dropout", "=", "WordDropout", "(", "\n", "dictionary", "=", "dictionary", ",", "\n", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "\n", "bpe_end_marker", "=", "bpe_end_marker", ",", "\n", ")", "\n", "self", ".", "word_shuffle", "=", "WordShuffle", "(", "\n", "dictionary", "=", "dictionary", ",", "\n", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "\n", "bpe_end_marker", "=", "bpe_end_marker", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising": [[213, 235], ["noising.UnsupervisedMTNoising.word_shuffle.noising", "noising.UnsupervisedMTNoising.word_dropout.noising", "noising.UnsupervisedMTNoising.word_dropout.noising", "noising.UnsupervisedMTNoising.dictionary.unk"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "# 1. Word Shuffle", "\n", "        ", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_shuffle", ".", "noising", "(", "\n", "x", "=", "x", ",", "\n", "lengths", "=", "lengths", ",", "\n", "max_shuffle_distance", "=", "self", ".", "max_word_shuffle_distance", ",", "\n", ")", "\n", "# 2. Word Dropout", "\n", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_dropout", ".", "noising", "(", "\n", "x", "=", "noisy_src_tokens", ",", "\n", "lengths", "=", "noisy_src_lengths", ",", "\n", "dropout_prob", "=", "self", ".", "word_dropout_prob", ",", "\n", ")", "\n", "# 3. Word Blanking", "\n", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_dropout", ".", "noising", "(", "\n", "x", "=", "noisy_src_tokens", ",", "\n", "lengths", "=", "noisy_src_lengths", ",", "\n", "dropout_prob", "=", "self", ".", "word_blanking_prob", ",", "\n", "blank_idx", "=", "self", ".", "dictionary", ".", "unk", "(", ")", ",", "\n", ")", "\n", "\n", "return", "noisy_src_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.NoisingDataset.__init__": [[238, 276], ["noising_class"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "src_dataset", ",", "\n", "src_dict", ",", "\n", "seed", ",", "\n", "noiser", "=", "None", ",", "\n", "noising_class", "=", "UnsupervisedMTNoising", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Wrap a :class:`~torch.utils.data.Dataset` and apply noise to the\n        samples based on the supplied noising configuration.\n\n        Args:\n            src_dataset (~torch.utils.data.Dataset): dataset to wrap.\n                to build self.src_dataset --\n                a LanguagePairDataset with src dataset as the source dataset and\n                None as the target dataset. Should NOT have padding so that\n                src_lengths are accurately calculated by language_pair_dataset\n                collate function.\n                We use language_pair_dataset here to encapsulate the tgt_dataset\n                so we can re-use the LanguagePairDataset collater to format the\n                batches in the structure that SequenceGenerator expects.\n            src_dict (~fairseq.data.Dictionary): source dictionary\n            seed (int): seed to use when generating random noise\n            noiser (WordNoising): a pre-initialized :class:`WordNoising`\n                instance. If this is None, a new instance will be created using\n                *noising_class* and *kwargs*.\n            noising_class (class, optional): class to use to initialize a\n                default :class:`WordNoising` instance.\n            kwargs (dict, optional): arguments to initialize the default\n                :class:`WordNoising` instance given by *noiser*.\n        \"\"\"", "\n", "self", ".", "src_dataset", "=", "src_dataset", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "noiser", "=", "noiser", "if", "noiser", "is", "not", "None", "else", "noising_class", "(", "\n", "dictionary", "=", "src_dict", ",", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.NoisingDataset.__getitem__": [[278, 298], ["torch.LongTensor", "src_tokens.unsqueeze.unsqueeze.unsqueeze", "torch.t", "torch.t", "fairseq.data.data_utils.numpy_seed", "noising.NoisingDataset.noiser.noising", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.UnsupervisedMTNoising.noising"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        Returns a single noisy sample. Multiple samples are fed to the collater\n        create a noising dataset batch.\n        \"\"\"", "\n", "src_tokens", "=", "self", ".", "src_dataset", "[", "index", "]", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "len", "(", "src_tokens", ")", "]", ")", "\n", "src_tokens", "=", "src_tokens", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Transpose src tokens to fit expected shape of x in noising function", "\n", "# (batch size, sequence length) -> (sequence length, batch size)", "\n", "src_tokens_t", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", "+", "index", ")", ":", "\n", "            ", "noisy_src_tokens", "=", "self", ".", "noiser", ".", "noising", "(", "src_tokens_t", ",", "src_lengths", ")", "\n", "\n", "# Transpose back to expected src_tokens format", "\n", "# (sequence length, 1) -> (1, sequence length)", "\n", "", "noisy_src_tokens", "=", "torch", ".", "t", "(", "noisy_src_tokens", ")", "\n", "return", "noisy_src_tokens", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.noising.NoisingDataset.__len__": [[299, 304], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The length of the noising dataset is the length of src.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "src_dataset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.__init__": [[53, 59], ["super().__init__", "indexed_dataset.IndexedDataset.read_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.read_index"], ["def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fix_lua_indexing", "=", "fix_lua_indexing", "\n", "self", ".", "read_index", "(", "path", ")", "\n", "self", ".", "data_file", "=", "None", "\n", "self", ".", "path", "=", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.read_index": [[60, 72], ["open", "f.read", "f.read", "struct.unpack", "struct.unpack", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.index_file_path", "struct.unpack", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.index_file_path"], ["", "def", "read_index", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "magic", "==", "b'TNTIDX\\x00\\x00'", "\n", "version", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "struct", ".", "unpack", "(", "'<Q'", ",", "version", ")", "==", "(", "1", ",", ")", "\n", "code", ",", "self", ".", "element_size", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dtype", "=", "dtypes", "[", "code", "]", "\n", "self", ".", "size", ",", "self", ".", "s", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dim_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "data_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "sizes", "=", "read_longs", "(", "f", ",", "self", ".", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.read_data": [[73, 75], ["open", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path"], ["", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "open", "(", "data_file_path", "(", "path", ")", ",", "'rb'", ",", "buffering", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.check_index": [[76, 79], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.__del__": [[80, 83], ["indexed_dataset.IndexedDataset.data_file.close"], "methods", ["None"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.__getitem__": [[84, 96], ["indexed_dataset.IndexedDataset.check_index", "int", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto", "torch.from_numpy().long", "indexed_dataset.IndexedDataset.read_data", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "int", "(", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", ")", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "item", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.__len__": [[97, 99], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.exists": [[100, 105], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "\n", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDataset.supports_prefetch": [[107, 110], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "False", "# avoid prefetching to save memory", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedCachedDataset.__init__": [[114, 118], ["indexed_dataset.IndexedDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "path", ",", "fix_lua_indexing", "=", "fix_lua_indexing", ")", "\n", "self", ".", "cache", "=", "None", "\n", "self", ".", "cache_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedCachedDataset.supports_prefetch": [[119, 122], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedCachedDataset.prefetch": [[123, 142], ["all", "sorted", "numpy.empty", "indexed_dataset.IndexedCachedDataset.cache_index.clear", "indexed_dataset.IndexedCachedDataset.read_data", "set", "indexed_dataset.IndexedCachedDataset.data_file.seek", "indexed_dataset.IndexedCachedDataset.data_file.readinto"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "if", "all", "(", "i", "in", "self", ".", "cache_index", "for", "i", "in", "indices", ")", ":", "\n", "            ", "return", "\n", "", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "indices", "=", "sorted", "(", "set", "(", "indices", ")", ")", "\n", "total_size", "=", "0", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "total_size", "+=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "", "self", ".", "cache", "=", "np", ".", "empty", "(", "total_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "0", "\n", "self", ".", "cache_index", ".", "clear", "(", ")", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "self", ".", "cache_index", "[", "i", "]", "=", "ptx", "\n", "size", "=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "a", "=", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "size", "]", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "ptx", "+=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedCachedDataset.__getitem__": [[143, 153], ["indexed_dataset.IndexedCachedDataset.check_index", "numpy.empty", "numpy.copyto", "torch.from_numpy().long", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "self", ".", "cache_index", "[", "i", "]", "\n", "np", ".", "copyto", "(", "a", ",", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "a", ".", "size", "]", ")", "\n", "item", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.__init__": [[159, 167], ["indexed_dataset.IndexedRawTextDataset.read_data", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["def", "__init__", "(", "self", ",", "path", ",", "dictionary", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokens_list", "=", "[", "]", "\n", "self", ".", "lines", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "append_eos", "=", "append_eos", "\n", "self", ".", "reverse_order", "=", "reverse_order", "\n", "self", ".", "read_data", "(", "path", ",", "dictionary", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.read_data": [[168, 179], ["numpy.array", "open", "indexed_dataset.IndexedRawTextDataset.lines.append", "dictionary.encode_line().long", "indexed_dataset.IndexedRawTextDataset.tokens_list.append", "indexed_dataset.IndexedRawTextDataset.sizes.append", "line.strip", "len", "dictionary.encode_line"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.encode_line"], ["", "def", "read_data", "(", "self", ",", "path", ",", "dictionary", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "self", ".", "lines", ".", "append", "(", "line", ".", "strip", "(", "'\\n'", ")", ")", "\n", "tokens", "=", "dictionary", ".", "encode_line", "(", "\n", "line", ",", "add_if_not_exist", "=", "False", ",", "\n", "append_eos", "=", "self", ".", "append_eos", ",", "reverse_order", "=", "self", ".", "reverse_order", ",", "\n", ")", ".", "long", "(", ")", "\n", "self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index": [[180, 183], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.__getitem__": [[184, 187], ["indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "tokens_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.get_original_text": [[188, 191], ["indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "lines", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.__del__": [[192, 194], ["None"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.__len__": [[195, 197], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists": [[198, 201], ["os.path.exists"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedRawTextDataset.exists"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.__init__": [[214, 221], ["open"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int32", ")", ":", "\n", "        ", "self", ".", "out_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "dtype", "=", "dtype", "\n", "self", ".", "data_offsets", "=", "[", "0", "]", "\n", "self", ".", "dim_offsets", "=", "[", "0", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "element_size", "=", "self", ".", "element_sizes", "[", "self", ".", "dtype", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.add_item": [[222, 229], ["indexed_dataset.IndexedDatasetBuilder.out_file.write", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "tensor.size", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "numpy.array", "indexed_dataset.IndexedDatasetBuilder.sizes.append", "len", "tensor.numpy", "tensor.size"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "# +1 for Lua compatibility", "\n", "        ", "bytes", "=", "self", ".", "out_file", ".", "write", "(", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", "+", "1", ",", "dtype", "=", "self", ".", "dtype", ")", ")", "\n", "self", ".", "data_offsets", ".", "append", "(", "self", ".", "data_offsets", "[", "-", "1", "]", "+", "bytes", "/", "self", ".", "element_size", ")", "\n", "for", "s", "in", "tensor", ".", "size", "(", ")", ":", "\n", "            ", "self", ".", "sizes", ".", "append", "(", "s", ")", "\n", "", "self", ".", "dim_offsets", ".", "append", "(", "self", ".", "dim_offsets", "[", "-", "1", "]", "+", "len", "(", "tensor", ".", "size", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.merge_file_": [[230, 249], ["indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDatasetBuilder.sizes.extend", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "open", "indexed_dataset.data_file_path", "f.read", "indexed_dataset.IndexedDatasetBuilder.out_file.write"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.extend", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "        ", "index", "=", "IndexedDataset", "(", "another_file", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "dtype", "\n", "\n", "begin", "=", "self", ".", "data_offsets", "[", "-", "1", "]", "\n", "for", "offset", "in", "index", ".", "data_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "data_offsets", ".", "append", "(", "begin", "+", "offset", ")", "\n", "", "self", ".", "sizes", ".", "extend", "(", "index", ".", "sizes", ")", "\n", "begin", "=", "self", ".", "dim_offsets", "[", "-", "1", "]", "\n", "for", "dim_offset", "in", "index", ".", "dim_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "dim_offsets", ".", "append", "(", "begin", "+", "dim_offset", ")", "\n", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "data", "=", "f", ".", "read", "(", "1024", ")", "\n", "if", "data", ":", "\n", "                    ", "self", ".", "out_file", ".", "write", "(", "data", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.IndexedDatasetBuilder.finalize": [[250, 261], ["indexed_dataset.IndexedDatasetBuilder.out_file.close", "open", "open.write", "open.write", "open.write", "open.write", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "open.close", "struct.pack", "struct.pack", "struct.pack", "indexed_dataset.code", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.code"], ["", "", "", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "out_file", ".", "close", "(", ")", "\n", "index", "=", "open", "(", "index_file", ",", "'wb'", ")", "\n", "index", ".", "write", "(", "b'TNTIDX\\x00\\x00'", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<Q'", ",", "1", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "code", "(", "self", ".", "dtype", ")", ",", "self", ".", "element_size", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "len", "(", "self", ".", "data_offsets", ")", "-", "1", ",", "len", "(", "self", ".", "sizes", ")", ")", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "dim_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "data_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "sizes", ")", "\n", "index", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.read_longs": [[15, 19], ["numpy.empty", "f.readinto"], "function", ["None"], ["def", "read_longs", "(", "f", ",", "n", ")", ":", "\n", "    ", "a", "=", "np", ".", "empty", "(", "n", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "f", ".", "readinto", "(", "a", ")", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.write_longs": [[21, 23], ["f.write", "numpy.array"], "function", ["None"], ["", "def", "write_longs", "(", "f", ",", "a", ")", ":", "\n", "    ", "f", ".", "write", "(", "np", ".", "array", "(", "a", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.code": [[36, 40], ["dtypes.keys"], "function", ["None"], ["def", "code", "(", "dtype", ")", ":", "\n", "    ", "for", "k", "in", "dtypes", ".", "keys", "(", ")", ":", "\n", "        ", "if", "dtypes", "[", "k", "]", "==", "dtype", ":", "\n", "            ", "return", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.index_file_path": [[42, 44], ["None"], "function", ["None"], ["", "", "", "def", "index_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.idx'", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.indexed_dataset.data_file_path": [[46, 48], ["None"], "function", ["None"], ["", "def", "data_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.bin'", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.__init__": [[64, 78], ["numpy.array", "all", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "sizes", ",", "src_vocab", ",", "tgt_vocab", ",", "add_eos_for_other_targets", ",", "shuffle", ",", "\n", "targets", "=", "None", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "sizes", "=", "np", ".", "array", "(", "sizes", ")", "\n", "self", ".", "vocab", "=", "src_vocab", "\n", "self", ".", "tgt_vocab", "=", "tgt_vocab", "\n", "self", ".", "add_eos_for_other_targets", "=", "add_eos_for_other_targets", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n", "assert", "targets", "is", "None", "or", "all", "(", "t", "in", "{", "'self'", ",", "'future'", ",", "'past'", "}", "for", "t", "in", "targets", ")", ",", "\"targets must be none or one of 'self', 'future', 'past'\"", "\n", "if", "targets", "is", "not", "None", "and", "len", "(", "targets", ")", "==", "0", ":", "\n", "            ", "targets", "=", "None", "\n", "", "self", ".", "targets", "=", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.__getitem__": [[79, 87], ["monolingual_dataset.MonolingualDataset._make_source_target"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset._make_source_target"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "targets", "is", "not", "None", ":", "\n", "            ", "source", ",", "future_target", ",", "past_target", "=", "self", ".", "dataset", "[", "index", "]", "\n", "source", ",", "target", "=", "self", ".", "_make_source_target", "(", "source", ",", "future_target", ",", "past_target", ")", "\n", "", "else", ":", "\n", "            ", "source", "=", "self", ".", "dataset", "[", "index", "]", "\n", "target", "=", "None", "\n", "", "return", "{", "'id'", ":", "index", ",", "'source'", ":", "source", ",", "'target'", ":", "target", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.__len__": [[88, 90], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset._make_source_target": [[91, 123], ["monolingual_dataset.MonolingualDataset._filter_vocab", "torch.cat", "len", "monolingual_dataset.MonolingualDataset.vocab.eos", "torch.cat", "torch.cat", "target.append", "torch.cat.new", "target.append", "torch.cat.new", "torch.cat.new", "target.append", "Exception", "monolingual_dataset.MonolingualDataset.vocab.eos", "monolingual_dataset.MonolingualDataset.vocab.pad", "monolingual_dataset.MonolingualDataset.vocab.pad"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset._filter_vocab", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad"], ["", "def", "_make_source_target", "(", "self", ",", "source", ",", "future_target", ",", "past_target", ")", ":", "\n", "        ", "if", "self", ".", "targets", "is", "not", "None", ":", "\n", "            ", "target", "=", "[", "]", "\n", "\n", "if", "self", ".", "add_eos_for_other_targets", "and", "(", "(", "'self'", "in", "self", ".", "targets", ")", "or", "(", "'past'", "in", "self", ".", "targets", ")", ")", "and", "source", "[", "-", "1", "]", "!=", "self", ".", "vocab", ".", "eos", "(", ")", ":", "\n", "# append eos at the end of source", "\n", "                ", "source", "=", "torch", ".", "cat", "(", "[", "source", ",", "source", ".", "new", "(", "[", "self", ".", "vocab", ".", "eos", "(", ")", "]", ")", "]", ")", "\n", "\n", "if", "'future'", "in", "self", ".", "targets", ":", "\n", "                    ", "future_target", "=", "torch", ".", "cat", "(", "[", "future_target", ",", "future_target", ".", "new", "(", "[", "self", ".", "vocab", ".", "pad", "(", ")", "]", ")", "]", ")", "\n", "", "if", "'past'", "in", "self", ".", "targets", ":", "\n", "# first token is before the start of sentence which is only used in \"none\" break mode when", "\n", "# add_eos_for_other_targets is False", "\n", "                    ", "past_target", "=", "torch", ".", "cat", "(", "[", "past_target", ".", "new", "(", "[", "self", ".", "vocab", ".", "pad", "(", ")", "]", ")", ",", "past_target", "[", "1", ":", "]", ",", "source", "[", "-", "2", ",", "None", "]", "]", ")", "\n", "\n", "", "", "for", "t", "in", "self", ".", "targets", ":", "\n", "                ", "if", "t", "==", "'self'", ":", "\n", "                    ", "target", ".", "append", "(", "source", ")", "\n", "", "elif", "t", "==", "'future'", ":", "\n", "                    ", "target", ".", "append", "(", "future_target", ")", "\n", "", "elif", "t", "==", "'past'", ":", "\n", "                    ", "target", ".", "append", "(", "past_target", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'invalid target '", "+", "t", ")", "\n", "\n", "", "", "if", "len", "(", "target", ")", "==", "1", ":", "\n", "                ", "target", "=", "target", "[", "0", "]", "\n", "", "", "else", ":", "\n", "            ", "target", "=", "future_target", "\n", "\n", "", "return", "source", ",", "self", ".", "_filter_vocab", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset._filter_vocab": [[124, 136], ["len", "len", "isinstance", "monolingual_dataset.MonolingualDataset._filter_vocab._filter"], "methods", ["None"], ["", "def", "_filter_vocab", "(", "self", ",", "target", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "tgt_vocab", ")", "!=", "len", "(", "self", ".", "vocab", ")", ":", "\n", "            ", "def", "_filter", "(", "target", ")", ":", "\n", "                ", "mask", "=", "target", ".", "ge", "(", "len", "(", "self", ".", "tgt_vocab", ")", ")", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "                    ", "target", "[", "mask", "]", "=", "self", ".", "tgt_vocab", ".", "unk", "(", ")", "\n", "", "return", "target", "\n", "\n", "", "if", "isinstance", "(", "target", ",", "list", ")", ":", "\n", "                ", "return", "[", "_filter", "(", "t", ")", "for", "t", "in", "target", "]", "\n", "", "return", "_filter", "(", "target", ")", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.collater": [[137, 159], ["monolingual_dataset.collate", "monolingual_dataset.MonolingualDataset.vocab.pad", "monolingual_dataset.MonolingualDataset.vocab.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the right.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the right.\n        \"\"\"", "\n", "return", "collate", "(", "samples", ",", "self", ".", "vocab", ".", "pad", "(", ")", ",", "self", ".", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.get_dummy_batch": [[160, 172], ["max", "monolingual_dataset.MonolingualDataset.vocab.dummy_sentence", "monolingual_dataset.MonolingualDataset._make_source_target", "monolingual_dataset.MonolingualDataset.collater", "isinstance", "isinstance", "min", "range"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset._make_source_target", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range"], ["", "def", "get_dummy_batch", "(", "self", ",", "num_tokens", ",", "max_positions", ",", "tgt_len", "=", "128", ")", ":", "\n", "        ", "\"\"\"Return a dummy batch with a given number of tokens.\"\"\"", "\n", "if", "isinstance", "(", "max_positions", ",", "float", ")", "or", "isinstance", "(", "max_positions", ",", "int", ")", ":", "\n", "            ", "tgt_len", "=", "min", "(", "tgt_len", ",", "max_positions", ")", "\n", "", "bsz", "=", "max", "(", "num_tokens", "//", "tgt_len", ",", "1", ")", "\n", "target", "=", "self", ".", "vocab", ".", "dummy_sentence", "(", "tgt_len", "+", "2", ")", "\n", "source", ",", "past_target", ",", "future_target", "=", "target", "[", "1", ":", "-", "1", "]", ",", "target", "[", "2", ":", "]", ",", "target", "[", ":", "-", "2", "]", "\n", "source", ",", "target", "=", "self", ".", "_make_source_target", "(", "source", ",", "past_target", ",", "future_target", ")", "\n", "\n", "return", "self", ".", "collater", "(", "[", "\n", "{", "'id'", ":", "i", ",", "'source'", ":", "source", ",", "'target'", ":", "target", "}", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.num_tokens": [[174, 178], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.size": [[179, 183], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.ordered_indices": [[184, 193], ["order.append", "numpy.lexsort", "numpy.random.permutation", "numpy.arange", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "order", "=", "[", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "order", "=", "[", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "]", "\n", "", "order", ".", "append", "(", "self", ".", "sizes", ")", "\n", "return", "np", ".", "lexsort", "(", "order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.supports_prefetch": [[194, 197], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.MonolingualDataset.prefetch": [[198, 200], ["monolingual_dataset.MonolingualDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.monolingual_dataset.collate": [[14, 49], ["monolingual_dataset.collate.merge"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.merge"], ["def", "collate", "(", "samples", ",", "pad_idx", ",", "eos_idx", ")", ":", "\n", "    ", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "is_list", "=", "False", ")", ":", "\n", "        ", "if", "is_list", ":", "\n", "            ", "res", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "samples", "[", "0", "]", "[", "key", "]", ")", ")", ":", "\n", "                ", "res", ".", "append", "(", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "[", "i", "]", "for", "s", "in", "samples", "]", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", "=", "False", ",", "\n", ")", ")", "\n", "", "return", "res", "\n", "", "else", ":", "\n", "            ", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", "=", "False", ",", "\n", ")", "\n", "\n", "", "", "src_tokens", "=", "merge", "(", "'source'", ")", "\n", "if", "samples", "[", "0", "]", "[", "'target'", "]", "is", "not", "None", ":", "\n", "        ", "is_target_list", "=", "isinstance", "(", "samples", "[", "0", "]", "[", "'target'", "]", ",", "list", ")", "\n", "target", "=", "merge", "(", "'target'", ",", "is_target_list", ")", "\n", "", "else", ":", "\n", "        ", "target", "=", "src_tokens", "\n", "\n", "", "return", "{", "\n", "'id'", ":", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", ",", "\n", "'nsentences'", ":", "len", "(", "samples", ")", ",", "\n", "'ntokens'", ":", "sum", "(", "len", "(", "s", "[", "'source'", "]", ")", "for", "s", "in", "samples", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "torch", ".", "LongTensor", "(", "[", "\n", "s", "[", "'source'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "\n", "]", ")", ",", "\n", "}", ",", "\n", "'target'", ":", "target", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.cumsum": [[16, 24], ["zip", "r.append", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Text.append"], ["    ", "@", "staticmethod", "\n", "def", "cumsum", "(", "sequence", ",", "sample_ratios", ")", ":", "\n", "        ", "r", ",", "s", "=", "[", "]", ",", "0", "\n", "for", "e", ",", "ratio", "in", "zip", "(", "sequence", ",", "sample_ratios", ")", ":", "\n", "            ", "l", "=", "ratio", "*", "len", "(", "e", ")", "\n", "r", ".", "append", "(", "l", "+", "s", ")", "\n", "s", "+=", "l", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.__init__": [[25, 34], ["FairseqDataset.__init__", "list", "isinstance", "concat_dataset.ConcatDataset.cumsum", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.cumsum"], ["", "def", "__init__", "(", "self", ",", "datasets", ",", "sample_ratios", "=", "1", ")", ":", "\n", "        ", "super", "(", "ConcatDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "len", "(", "datasets", ")", ">", "0", ",", "'datasets should not be an empty iterable'", "\n", "self", ".", "datasets", "=", "list", "(", "datasets", ")", "\n", "if", "isinstance", "(", "sample_ratios", ",", "int", ")", ":", "\n", "            ", "sample_ratios", "=", "[", "sample_ratios", "]", "*", "len", "(", "self", ".", "datasets", ")", "\n", "", "self", ".", "sample_ratios", "=", "sample_ratios", "\n", "self", ".", "cumulative_sizes", "=", "self", ".", "cumsum", "(", "self", ".", "datasets", ",", "sample_ratios", ")", "\n", "self", ".", "real_sizes", "=", "[", "len", "(", "d", ")", "for", "d", "in", "self", ".", "datasets", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.__len__": [[35, 37], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.__getitem__": [[38, 46], ["bisect.bisect_right"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "dataset_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "dataset_idx", "==", "0", ":", "\n", "            ", "sample_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "sample_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "dataset_idx", "-", "1", "]", "\n", "", "sample_idx", "=", "sample_idx", "%", "self", ".", "real_sizes", "[", "dataset_idx", "]", "\n", "return", "self", ".", "datasets", "[", "dataset_idx", "]", "[", "sample_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.sizes": [[47, 50], ["numpy.concatenate", "numpy.tile", "zip"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "[", "np", ".", "tile", "(", "ds", ".", "sizes", ",", "sr", ")", "for", "ds", ",", "sr", "in", "zip", "(", "self", ".", "datasets", ",", "self", ".", "sample_ratios", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.supports_prefetch": [[51, 54], ["all"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "all", "(", "[", "d", ".", "supports_prefetch", "for", "d", "in", "self", ".", "datasets", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.concat_dataset.ConcatDataset.prefetch": [[55, 61], ["zip", "len", "ds.prefetch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.zip", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "frm", "=", "0", "\n", "for", "to", ",", "ds", "in", "zip", "(", "self", ".", "cumulative_sizes", ",", "self", ".", "datasets", ")", ":", "\n", "            ", "real_size", "=", "len", "(", "ds", ")", "\n", "ds", ".", "prefetch", "(", "[", "(", "i", "-", "frm", ")", "%", "real_size", "for", "i", "in", "indices", "if", "frm", "<=", "i", "<", "to", "]", ")", "\n", "frm", "=", "to", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.__init__": [[101, 126], ["numpy.array", "numpy.array", "src_dict.pad", "tgt_dict.pad", "src_dict.eos", "tgt_dict.eos", "src_dict.unk", "tgt_dict.unk"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.unk"], ["def", "__init__", "(", "\n", "self", ",", "src", ",", "src_sizes", ",", "src_dict", ",", "\n", "tgt", "=", "None", ",", "tgt_sizes", "=", "None", ",", "tgt_dict", "=", "None", ",", "\n", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "max_source_positions", "=", "1024", ",", "max_target_positions", "=", "1024", ",", "\n", "shuffle", "=", "True", ",", "input_feeding", "=", "True", ",", "remove_eos_from_source", "=", "False", ",", "append_eos_to_target", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "tgt_dict", "is", "not", "None", ":", "\n", "            ", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "", "self", ".", "src", "=", "src", "\n", "self", ".", "tgt", "=", "tgt", "\n", "self", ".", "src_sizes", "=", "np", ".", "array", "(", "src_sizes", ")", "\n", "self", ".", "tgt_sizes", "=", "np", ".", "array", "(", "tgt_sizes", ")", "if", "tgt_sizes", "is", "not", "None", "else", "None", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "left_pad_source", "=", "left_pad_source", "\n", "self", ".", "left_pad_target", "=", "left_pad_target", "\n", "self", ".", "max_source_positions", "=", "max_source_positions", "\n", "self", ".", "max_target_positions", "=", "max_target_positions", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "input_feeding", "=", "input_feeding", "\n", "self", ".", "remove_eos_from_source", "=", "remove_eos_from_source", "\n", "self", ".", "append_eos_to_target", "=", "append_eos_to_target", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.__getitem__": [[127, 148], ["language_pair_dataset.LanguagePairDataset.src_dict.eos", "language_pair_dataset.LanguagePairDataset.tgt_dict.eos", "language_pair_dataset.LanguagePairDataset.src_dict.eos", "torch.cat", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "tgt_item", "=", "self", ".", "tgt", "[", "index", "]", "if", "self", ".", "tgt", "is", "not", "None", "else", "None", "\n", "src_item", "=", "self", ".", "src", "[", "index", "]", "\n", "# Append EOS to end of tgt sentence if it does not have an EOS and remove", "\n", "# EOS from end of src sentence if it exists. This is useful when we use", "\n", "# use existing datasets for opposite directions i.e., when we want to", "\n", "# use tgt_dataset as src_dataset and vice versa", "\n", "if", "self", ".", "append_eos_to_target", ":", "\n", "            ", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", "if", "self", ".", "tgt_dict", "else", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "if", "self", ".", "tgt", "and", "self", ".", "tgt", "[", "index", "]", "[", "-", "1", "]", "!=", "eos", ":", "\n", "                ", "tgt_item", "=", "torch", ".", "cat", "(", "[", "self", ".", "tgt", "[", "index", "]", ",", "torch", ".", "LongTensor", "(", "[", "eos", "]", ")", "]", ")", "\n", "\n", "", "", "if", "self", ".", "remove_eos_from_source", ":", "\n", "            ", "eos", "=", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "if", "self", ".", "src", "[", "index", "]", "[", "-", "1", "]", "==", "eos", ":", "\n", "                ", "src_item", "=", "self", ".", "src", "[", "index", "]", "[", ":", "-", "1", "]", "\n", "\n", "", "", "return", "{", "\n", "'id'", ":", "index", ",", "\n", "'source'", ":", "src_item", ",", "\n", "'target'", ":", "tgt_item", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.__len__": [[150, 152], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.collater": [[153, 186], ["language_pair_dataset.collate", "language_pair_dataset.LanguagePairDataset.src_dict.pad", "language_pair_dataset.LanguagePairDataset.src_dict.eos"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the left if *left_pad_source* is ``True``.\n                  - `src_lengths` (LongTensor): 1D Tensor of the unpadded\n                    lengths of each source sentence of shape `(bsz)`\n                  - `prev_output_tokens` (LongTensor): a padded 2D Tensor of\n                    tokens in the target sentence, shifted right by one position\n                    for input feeding/teacher forcing, of shape `(bsz,\n                    tgt_len)`. This key will not be present if *input_feeding*\n                    is ``False``. Padding will appear on the left if\n                    *left_pad_target* is ``True``.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the left if *left_pad_target* is ``True``.\n        \"\"\"", "\n", "return", "collate", "(", "\n", "samples", ",", "pad_idx", "=", "self", ".", "src_dict", ".", "pad", "(", ")", ",", "eos_idx", "=", "self", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", "left_pad_source", "=", "self", ".", "left_pad_source", ",", "left_pad_target", "=", "self", ".", "left_pad_target", ",", "\n", "input_feeding", "=", "self", ".", "input_feeding", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.get_dummy_batch": [[188, 203], ["fairseq.utils.resolve_max_positions", "max", "language_pair_dataset.LanguagePairDataset.collater", "max", "language_pair_dataset.LanguagePairDataset.src_dict.dummy_sentence", "range", "language_pair_dataset.LanguagePairDataset.tgt_dict.dummy_sentence"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Chunk.range", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.dictionary.Dictionary.dummy_sentence"], ["", "def", "get_dummy_batch", "(", "self", ",", "num_tokens", ",", "max_positions", ",", "src_len", "=", "128", ",", "tgt_len", "=", "128", ")", ":", "\n", "        ", "\"\"\"Return a dummy batch with a given number of tokens.\"\"\"", "\n", "src_len", ",", "tgt_len", "=", "utils", ".", "resolve_max_positions", "(", "\n", "(", "src_len", ",", "tgt_len", ")", ",", "\n", "max_positions", ",", "\n", "(", "self", ".", "max_source_positions", ",", "self", ".", "max_target_positions", ")", ",", "\n", ")", "\n", "bsz", "=", "max", "(", "num_tokens", "//", "max", "(", "src_len", ",", "tgt_len", ")", ",", "1", ")", "\n", "return", "self", ".", "collater", "(", "[", "\n", "{", "\n", "'id'", ":", "i", ",", "\n", "'source'", ":", "self", ".", "src_dict", ".", "dummy_sentence", "(", "src_len", ")", ",", "\n", "'target'", ":", "self", ".", "tgt_dict", ".", "dummy_sentence", "(", "tgt_len", ")", "if", "self", ".", "tgt_dict", "is", "not", "None", "else", "None", ",", "\n", "}", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.num_tokens": [[205, 209], ["max"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "return", "max", "(", "self", ".", "src_sizes", "[", "index", "]", ",", "self", ".", "tgt_sizes", "[", "index", "]", "if", "self", ".", "tgt_sizes", "is", "not", "None", "else", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.size": [[210, 214], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "(", "self", ".", "src_sizes", "[", "index", "]", ",", "self", ".", "tgt_sizes", "[", "index", "]", "if", "self", ".", "tgt_sizes", "is", "not", "None", "else", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.ordered_indices": [[215, 225], ["numpy.random.permutation", "numpy.arange", "len", "len", "numpy.argsort", "numpy.argsort"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "", "if", "self", ".", "tgt_sizes", "is", "not", "None", ":", "\n", "            ", "indices", "=", "indices", "[", "np", ".", "argsort", "(", "self", ".", "tgt_sizes", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "]", "\n", "", "return", "indices", "[", "np", ".", "argsort", "(", "self", ".", "src_sizes", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.supports_prefetch": [[226, 231], ["getattr", "getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "(", "\n", "getattr", "(", "self", ".", "src", ",", "'supports_prefetch'", ",", "False", ")", "\n", "and", "getattr", "(", "self", ".", "tgt", ",", "'supports_prefetch'", ",", "False", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.LanguagePairDataset.prefetch": [[233, 236], ["language_pair_dataset.LanguagePairDataset.src.prefetch", "language_pair_dataset.LanguagePairDataset.tgt.prefetch"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "src", ".", "prefetch", "(", "indices", ")", "\n", "self", ".", "tgt", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.language_pair_dataset.collate": [[16, 69], ["torch.LongTensor", "language_pair_dataset.collate.merge"], "function", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.scripts.score_moe.merge"], ["def", "collate", "(", "\n", "samples", ",", "pad_idx", ",", "eos_idx", ",", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "input_feeding", "=", "True", ",", "\n", ")", ":", "\n", "    ", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", ",", "\n", ")", "\n", "\n", "", "id", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", "\n", "src_tokens", "=", "merge", "(", "'source'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "# sort by descending source length", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'source'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "]", ")", "\n", "src_lengths", ",", "sort_order", "=", "src_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "id", "=", "id", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_tokens", "=", "src_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "prev_output_tokens", "=", "None", "\n", "target", "=", "None", "\n", "if", "samples", "[", "0", "]", ".", "get", "(", "'target'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "target", "=", "merge", "(", "'target'", ",", "left_pad", "=", "left_pad_target", ")", "\n", "target", "=", "target", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'target'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "if", "input_feeding", ":", "\n", "# we create a shifted version of targets for feeding the", "\n", "# previous output token(s) into the next decoder step", "\n", "            ", "prev_output_tokens", "=", "merge", "(", "\n", "'target'", ",", "\n", "left_pad", "=", "left_pad_target", ",", "\n", "move_eos_to_beginning", "=", "True", ",", "\n", ")", "\n", "prev_output_tokens", "=", "prev_output_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "", "", "else", ":", "\n", "        ", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'source'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "", "batch", "=", "{", "\n", "'id'", ":", "id", ",", "\n", "'nsentences'", ":", "len", "(", "samples", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n", "'target'", ":", "target", ",", "\n", "}", "\n", "if", "prev_output_tokens", "is", "not", "None", ":", "\n", "        ", "batch", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", "=", "prev_output_tokens", "\n", "", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__": [[28, 43], ["FairseqDataset.__init__", "isinstance", "datasets.items", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["def", "__init__", "(", "self", ",", "datasets", ",", "eval_key", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "datasets", ",", "OrderedDict", ")", "\n", "self", ".", "datasets", "=", "datasets", "\n", "self", ".", "eval_key", "=", "eval_key", "\n", "\n", "self", ".", "longest_dataset", "=", "None", "\n", "self", ".", "longest_dataset_key", "=", "None", "\n", "for", "key", ",", "dataset", "in", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "if", "self", ".", "longest_dataset", "is", "None", "or", "len", "(", "dataset", ")", ">", "len", "(", "self", ".", "longest_dataset", ")", ":", "\n", "                ", "self", ".", "longest_dataset", "=", "dataset", "\n", "self", ".", "longest_dataset_key", "=", "key", "\n", "\n", "", "", "self", ".", "_ordered_indices", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index": [[44, 48], ["len"], "methods", ["None"], ["", "def", "_map_index", "(", "self", ",", "key", ",", "index", ")", ":", "\n", "        ", "assert", "self", ".", "_ordered_indices", "is", "not", "None", ",", "'Must call RoundRobinZipDatasets.ordered_indices() first'", "\n", "return", "self", ".", "_ordered_indices", "[", "key", "]", "[", "index", "%", "len", "(", "self", ".", "datasets", "[", "key", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__getitem__": [[49, 58], ["collections.OrderedDict", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "eval_key", "is", "None", ":", "\n", "            ", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", "[", "self", ".", "_map_index", "(", "key", ",", "index", ")", "]", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "# at evaluation time it's useful to pass-through batches from a single key", "\n", "            ", "return", "self", ".", "datasets", "[", "self", ".", "eval_key", "]", "[", "self", ".", "_map_index", "(", "self", ".", "eval_key", ",", "index", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.__len__": [[59, 61], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "longest_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater": [[62, 74], ["len", "collections.OrderedDict", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets[].collater", "dataset.collater", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "if", "self", ".", "eval_key", "is", "None", ":", "\n", "            ", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", ".", "collater", "(", "[", "sample", "[", "key", "]", "for", "sample", "in", "samples", "]", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "# at evaluation time it's useful to pass-through batches from a single key", "\n", "            ", "return", "self", ".", "datasets", "[", "self", ".", "eval_key", "]", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch": [[75, 85], ["collections.OrderedDict", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets[].get_dummy_batch", "dataset.get_dummy_batch", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.get_dummy_batch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "", "def", "get_dummy_batch", "(", "self", ",", "max_tokens", ",", "max_positions", ")", ":", "\n", "        ", "if", "self", ".", "eval_key", "is", "None", ":", "\n", "# TODO should max_tokens be used independently for each batch like this?", "\n", "            ", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", ".", "get_dummy_batch", "(", "max_tokens", ",", "max_positions", "[", "key", "]", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "# at evaluation time it's useful to return a single batch directly", "\n", "            ", "return", "self", ".", "datasets", "[", "self", ".", "eval_key", "]", ".", "get_dummy_batch", "(", "max_tokens", ",", "max_positions", "[", "self", ".", "eval_key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.num_tokens": [[86, 92], ["max", "dataset.num_tokens", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.num_tokens", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's length (number of tokens), used for batching.\"\"\"", "\n", "# TODO make it configurable whether to use max() or sum() here", "\n", "return", "max", "(", "\n", "dataset", ".", "num_tokens", "(", "self", ".", "_map_index", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size": [[94, 100], ["dataset.size", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.size", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "{", "\n", "key", ":", "dataset", ".", "size", "(", "self", ".", "_map_index", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices": [[102, 113], ["numpy.arange", "collections.OrderedDict", "len", "dataset.ordered_indices", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices", "home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Ordered indices for batching.\"\"\"", "\n", "if", "self", ".", "_ordered_indices", "is", "None", ":", "\n", "# Call the underlying dataset's ordered_indices() here, so that we", "\n", "# get the same random ordering as we would have from using the", "\n", "# underlying dataset directly.", "\n", "            ", "self", ".", "_ordered_indices", "=", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", ".", "ordered_indices", "(", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "return", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.supports_prefetch": [[114, 119], ["all", "getattr", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.values"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "all", "(", "\n", "getattr", "(", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "for", "dataset", "in", "self", ".", "datasets", ".", "values", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch": [[121, 124], ["round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items", "dataset.prefetch", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], "methods", ["home.repos.pwc.inspect_result.kakaobrain_helo_word.None.tree.Map.items", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch", "home.repos.pwc.inspect_result.kakaobrain_helo_word.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "dataset", ".", "prefetch", "(", "[", "self", ".", "_map_index", "(", "key", ",", "index", ")", "for", "index", "in", "indices", "]", ")", "\n", "", "", "", ""]]}