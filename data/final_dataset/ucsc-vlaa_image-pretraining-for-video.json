{"home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.inference.main": [[60, 124], ["timm.utils.setup_default_logging", "parser.parse_args", "timm.models.create_model", "_logger.info", "timm.data.resolve_data_config", "timm.data.create_loader", "model.cuda.eval", "min", "timm.utils.AverageMeter", "time.time", "numpy.concatenate", "vars", "timm.models.apply_test_time_pool", "torch.nn.DataParallel().cuda", "model.cuda.cuda", "timm.data.ImageDataset", "torch.no_grad", "enumerate", "open", "timm.data.create_loader.dataset.filenames", "zip", "input.cuda.cuda", "model.cuda.", "np.concatenate.append", "timm.utils.AverageMeter.update", "time.time", "os.path.join", "out_file.write", "sum", "torch.nn.DataParallel", "model.topk", "topk.cpu().numpy", "_logger.info", "time.time", "m.numel", "list", "topk.cpu", "len", "model.cuda.parameters", "range", "str"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.config.resolve_data_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.create_loader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.test_time_pool.apply_test_time_pool", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filenames", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# might as well try to do something useful...", "\n", "args", ".", "pretrained", "=", "args", ".", "pretrained", "or", "not", "args", ".", "checkpoint", "\n", "\n", "# create model", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "in_chans", "=", "3", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "checkpoint_path", "=", "args", ".", "checkpoint", ")", "\n", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "\n", "(", "args", ".", "model", ",", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", ")", ")", "\n", "\n", "config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ")", "\n", "model", ",", "test_time_pool", "=", "(", "model", ",", "False", ")", "if", "args", ".", "no_test_pool", "else", "apply_test_time_pool", "(", "model", ",", "config", ")", "\n", "\n", "if", "args", ".", "num_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "list", "(", "range", "(", "args", ".", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "", "loader", "=", "create_loader", "(", "\n", "ImageDataset", "(", "args", ".", "data", ")", ",", "\n", "input_size", "=", "config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "use_prefetcher", "=", "True", ",", "\n", "interpolation", "=", "config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "config", "[", "'mean'", "]", ",", "\n", "std", "=", "config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "crop_pct", "=", "1.0", "if", "test_time_pool", "else", "config", "[", "'crop_pct'", "]", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "k", "=", "min", "(", "args", ".", "topk", ",", "args", ".", "num_classes", ")", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "topk_ids", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "_", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "labels", "=", "model", "(", "input", ")", "\n", "topk", "=", "labels", ".", "topk", "(", "k", ")", "[", "1", "]", "\n", "topk_ids", ".", "append", "(", "topk", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "batch_idx", "%", "args", ".", "log_freq", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "'Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})'", ".", "format", "(", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ")", ")", "\n", "\n", "", "", "", "topk_ids", "=", "np", ".", "concatenate", "(", "topk_ids", ",", "axis", "=", "0", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'./topk_ids.csv'", ")", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "filenames", "=", "loader", ".", "dataset", ".", "filenames", "(", "basename", "=", "True", ")", "\n", "for", "filename", ",", "label", "in", "zip", "(", "filenames", ",", "topk_ids", ")", ":", "\n", "            ", "out_file", ".", "write", "(", "'{0},{1}\\n'", ".", "format", "(", "\n", "filename", ",", "','", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "label", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train._parse_args": [[334, 349], ["config_parser.parse_known_args", "parser.parse_args", "yaml.safe_dump", "open", "yaml.safe_load", "parser.set_defaults"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "_parse_args", "(", ")", ":", "\n", "# Do we have a config file to parse?", "\n", "    ", "args_config", ",", "remaining", "=", "config_parser", ".", "parse_known_args", "(", ")", "\n", "if", "args_config", ".", "config", ":", "\n", "        ", "with", "open", "(", "args_config", ".", "config", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "cfg", "=", "yaml", ".", "safe_load", "(", "f", ")", "\n", "parser", ".", "set_defaults", "(", "**", "cfg", ")", "\n", "\n", "# The main arg parser parses the rest of the args, the usual", "\n", "# defaults will have been overridden if config file specified.", "\n", "", "", "args", "=", "parser", ".", "parse_args", "(", "remaining", ")", "\n", "\n", "# Cache the args as a text string to save them in the output dir later", "\n", "args_text", "=", "yaml", ".", "safe_dump", "(", "args", ".", "__dict__", ",", "default_flow_style", "=", "False", ")", "\n", "return", "args", ",", "args_text", "\n", "#", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.main": [[352, 773], ["setup_default_logging", "train._parse_args", "random_seed", "timm.data.resolve_data_config", "torch.nn.parallel.DistributedDataParallel.cuda", "timm.optim.create_optimizer_v2", "timm.scheduler.create_scheduler", "timm.data.create_dataset", "timm.data.create_dataset", "timm.data.create_loader", "timm.data.create_loader", "LabelSmoothingCrossEntropy.cuda", "torch.CrossEntropyLoss().cuda", "torch.cuda.set_device", "torch.cuda.set_device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.distributed.get_rank", "_logger.info", "_logger.info", "timm.models.create_model", "mmcv.cnn.get_model_complexity_info", "print", "print", "timm.models.create_model", "mmcv.cnn.get_model_complexity_info", "print", "print", "hasattr", "_logger.info", "vars", "timm.models.convert_splitbn_model", "torch.nn.parallel.DistributedDataParallel.to", "torch.jit.script", "torch.jit.script", "amp.initialize", "timm.utils.ApexScaler", "timm.models.resume_checkpoint", "os.path.isfile", "ModelEmaV2", "lr_scheduler.step", "_logger.info", "dict", "timm.data.AugMixDataset", "JsdCrossEntropy", "get_outdir", "CheckpointSaver", "train.validate", "print", "exit", "range", "_logger.info", "wandb.finish", "int", "_logger.warning", "max", "convert_syncbn_model", "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "_logger.info", "timm.optim.optimizer_kwargs", "_logger.info", "timm.utils.NativeScaler", "torch.load", "torch.load", "timm.models.load_checkpoint", "ApexDDP", "torch.nn.parallel.DistributedDataParallel", "timm.data.FastCollateMixup", "timm.data.Mixup", "torch.CrossEntropyLoss", "open", "f.write", "train.train_one_epoch", "train.validate", "wandb.init", "wandb.config.update", "torch.load", "torch.load", "str", "wandb.init", "_logger.warning", "_logger.info", "_logger.info", "isinstance", "collections.OrderedDict", "checkpoint[].items", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "_logger.info", "_logger.info", "BinaryCrossEntropy", "SoftTargetCrossEntropy", "torch.CrossEntropyLoss", "os.path.join", "hasattr", "timm.data.create_loader.sampler.set_epoch", "distribute_bn", "train.validate", "lr_scheduler.step", "update_summary", "CheckpointSaver.save_checkpoint", "timm.models.safe_model_name", "sum", "BinaryCrossEntropy", "LabelSmoothingCrossEntropy", "datetime.datetime.now().strftime", "timm.models.safe_model_name", "str", "len", "next", "next", "range", "wandb.log", "_logger.info", "distribute_bn", "os.path.join", "str", "str", "str", "k.startswith", "iter", "iter", "len", "train_sample.unsqueeze.unsqueeze", "val_sample.unsqueeze.unsqueeze", "m.numel", "datetime.datetime.now", "list", "wandb.Image", "wandb.Image", "torch.nn.parallel.DistributedDataParallel.parameters", "train_sample.unsqueeze.size", "[].detach().cpu().numpy", "range", "[].detach().cpu().numpy", "range", "str", "train_sample.unsqueeze.size", "str", "val_sample.unsqueeze.size", "[].detach().cpu", "[].detach().cpu", "[].detach", "[].detach", "train_sample[].permute", "val_sample[].permute"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train._parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.random.random_seed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.config.resolve_data_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler_factory.create_scheduler", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory.create_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory.create_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.create_loader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.create_loader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_batchnorm.convert_splitbn_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.resume_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.summary.get_outdir", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.validate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.optimizer_kwargs", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.train_one_epoch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.validate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.set_epoch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.distribute_bn", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.validate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.summary.update_summary", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.save_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.safe_model_name", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.safe_model_name", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.distribute_bn"], ["", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", ",", "args_text", "=", "_parse_args", "(", ")", "\n", "\n", "args", ".", "prefetcher", "=", "not", "args", ".", "no_prefetcher", "\n", "args", ".", "distributed", "=", "False", "\n", "if", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "distributed", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", ">", "1", "\n", "", "args", ".", "device", "=", "'cuda:0'", "\n", "args", ".", "world_size", "=", "1", "\n", "args", ".", "rank", "=", "0", "# global rank", "\n", "if", "args", ".", "distributed", ":", "\n", "        ", "args", ".", "device", "=", "'cuda:%d'", "%", "args", ".", "local_rank", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "'env://'", ")", "\n", "args", ".", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "args", ".", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "_logger", ".", "info", "(", "'Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.'", "\n", "%", "(", "args", ".", "rank", ",", "args", ".", "world_size", ")", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "info", "(", "'Training with a single process on 1 GPUs.'", ")", "\n", "", "assert", "args", ".", "rank", ">=", "0", "\n", "\n", "if", "args", ".", "log_wandb", ":", "\n", "        ", "if", "has_wandb", "and", "args", ".", "rank", "==", "0", ":", "\n", "            ", "if", "args", ".", "wandb_offline", ":", "\n", "                ", "os", ".", "environ", "[", "\"WANDB_MODE\"", "]", "=", "'offline'", "\n", "", "if", "not", "args", ".", "resume", ":", "\n", "                ", "wandb", ".", "init", "(", "project", "=", "str", "(", "args", ".", "project", ")", ",", "name", "=", "str", "(", "args", ".", "experiment", ")", ",", "entity", "=", "\"xianhangli\"", ")", "\n", "args", ".", "wandb_id", "=", "wandb", ".", "run", ".", "id", "\n", "wandb", ".", "config", ".", "update", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ")", "\n", "args", ".", "wandb_id", "=", "checkpoint", "[", "'args'", "]", ".", "wandb_id", "\n", "id", "=", "str", "(", "args", ".", "wandb_id", ")", "\n", "wandb", ".", "init", "(", "project", "=", "str", "(", "args", ".", "project", ")", ",", "\n", "id", "=", "id", ",", "resume", "=", "'allow'", ")", "\n", "", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"You've requested to log metrics to wandb but package not found. \"", "\n", "\"Metrics not being logged to wandb, try `pip install wandb`\"", ")", "\n", "\n", "# resolve AMP arguments based on PyTorch / Apex availability", "\n", "", "", "use_amp", "=", "None", "\n", "if", "args", ".", "amp", ":", "\n", "# `--amp` chooses native amp before apex (APEX ver not actively maintained)", "\n", "        ", "if", "has_native_amp", ":", "\n", "            ", "args", ".", "native_amp", "=", "True", "\n", "", "elif", "has_apex", ":", "\n", "            ", "args", ".", "apex_amp", "=", "True", "\n", "", "", "if", "args", ".", "apex_amp", "and", "has_apex", ":", "\n", "        ", "use_amp", "=", "'apex'", "\n", "", "elif", "args", ".", "native_amp", "and", "has_native_amp", ":", "\n", "        ", "use_amp", "=", "'native'", "\n", "", "elif", "args", ".", "apex_amp", "or", "args", ".", "native_amp", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Neither APEX or native Torch AMP is available, using float32. \"", "\n", "\"Install NVIDA apex or upgrade to PyTorch 1.6\"", ")", "\n", "\n", "", "random_seed", "(", "args", ".", "seed", ",", "args", ".", "rank", ")", "\n", "if", "'v3d'", "in", "args", ".", "model", ":", "\n", "        ", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_connect_rate", "=", "args", ".", "drop_connect", ",", "# DEPRECATED, use drop_path", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "args", ".", "drop_block", ",", "\n", "global_pool", "=", "args", ".", "gp", ",", "\n", "bn_tf", "=", "args", ".", "bn_tf", ",", "\n", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "bn_eps", "=", "args", ".", "bn_eps", ",", "\n", "scriptable", "=", "args", ".", "torchscript", ",", "\n", "checkpoint_path", "=", "args", ".", "initial_checkpoint", ",", "\n", "pretrained2d", "=", "args", ".", "pretrained2d", ",", "\n", "pretrain_dataset", "=", "args", ".", "pretrain_dataset", "\n", ")", "\n", "macs", ",", "params", "=", "get_model_complexity_info", "(", "model", ",", "(", "\n", "3", ",", "224", ",", "224", ")", ",", "\n", "as_strings", "=", "True", ",", "\n", "print_per_layer_stat", "=", "False", ")", "\n", "print", "(", "'{:<30}  {:<8}'", ".", "format", "(", "'Computational complexity: '", ",", "macs", ")", ")", "\n", "print", "(", "'{:<30}  {:<8}'", ".", "format", "(", "'Number of parameters: '", ",", "params", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_connect_rate", "=", "args", ".", "drop_connect", ",", "# DEPRECATED, use drop_path", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "args", ".", "drop_block", ",", "\n", "global_pool", "=", "args", ".", "gp", ",", "\n", "bn_tf", "=", "args", ".", "bn_tf", ",", "\n", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "bn_eps", "=", "args", ".", "bn_eps", ",", "\n", "scriptable", "=", "args", ".", "torchscript", ",", "\n", "checkpoint_path", "=", "args", ".", "initial_checkpoint", ",", "\n", ")", "\n", "macs", ",", "params", "=", "get_model_complexity_info", "(", "model", ",", "(", "\n", "3", ",", "224", ",", "224", ")", ",", "\n", "as_strings", "=", "True", ",", "\n", "print_per_layer_stat", "=", "False", ")", "\n", "print", "(", "'{:<30}  {:<8}'", ".", "format", "(", "'Computational complexity: '", ",", "macs", ")", ")", "\n", "print", "(", "'{:<30}  {:<8}'", ".", "format", "(", "'Number of parameters: '", ",", "params", ")", ")", "\n", "", "if", "args", ".", "num_classes", "is", "None", ":", "\n", "        ", "assert", "hasattr", "(", "model", ",", "'num_classes'", ")", ",", "'Model must have `num_classes` attr if not set on cmd line/config.'", "\n", "args", ".", "num_classes", "=", "model", ".", "num_classes", "# FIXME handle model default vs config num_classes more elegantly", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "\n", "f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}'", ")", "\n", "\n", "", "data_config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ",", "verbose", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "\n", "# setup augmentation batch splits for contrastive loss or split bn", "\n", "num_aug_splits", "=", "0", "\n", "if", "args", ".", "aug_splits", ">", "0", ":", "\n", "        ", "assert", "args", ".", "aug_splits", ">", "1", ",", "'A split of 1 makes no sense'", "\n", "num_aug_splits", "=", "args", ".", "aug_splits", "\n", "\n", "# enable split bn (separate bn stats per batch-portion)", "\n", "", "if", "args", ".", "split_bn", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "or", "args", ".", "resplit", "\n", "model", "=", "convert_splitbn_model", "(", "model", ",", "max", "(", "num_aug_splits", ",", "2", ")", ")", "\n", "\n", "# move model to GPU, enable channels last layout if set", "\n", "", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "channels_last", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "# setup synchronized BatchNorm for distributed training", "\n", "", "if", "args", ".", "distributed", "and", "args", ".", "sync_bn", ":", "\n", "        ", "assert", "not", "args", ".", "split_bn", "\n", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex SyncBN preferred unless native amp is activated", "\n", "            ", "model", "=", "convert_syncbn_model", "(", "model", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "SyncBatchNorm", ".", "convert_sync_batchnorm", "(", "model", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "\n", "'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '", "\n", "'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.'", ")", "\n", "\n", "", "", "if", "args", ".", "torchscript", ":", "\n", "        ", "assert", "not", "use_amp", "==", "'apex'", ",", "'Cannot use APEX AMP with torchscripted model'", "\n", "assert", "not", "args", ".", "sync_bn", ",", "'Cannot use SyncBatchNorm with torchscripted model'", "\n", "model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "\n", "\n", "#para = filter(lambda p: p.requires_grad, model.parameters())", "\n", "\n", "#.classifier.parameters()", "\n", "", "optimizer", "=", "create_optimizer_v2", "(", "model", ",", "**", "optimizer_kwargs", "(", "cfg", "=", "args", ")", ")", "\n", "\n", "# setup automatic mixed-precision (AMP) loss scaling and op casting", "\n", "amp_autocast", "=", "suppress", "# do nothing", "\n", "loss_scaler", "=", "None", "\n", "if", "use_amp", "==", "'apex'", ":", "\n", "        ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O1'", ")", "\n", "loss_scaler", "=", "ApexScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using NVIDIA APEX AMP. Training in mixed precision.'", ")", "\n", "", "", "elif", "use_amp", "==", "'native'", ":", "\n", "        ", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "\n", "loss_scaler", "=", "NativeScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using native Torch AMP. Training in mixed precision.'", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'AMP not enabled. Training in float32.'", ")", "\n", "\n", "# optionally resume from a checkpoint", "\n", "", "", "resume_epoch", "=", "None", "\n", "if", "args", ".", "resume", "and", "not", "args", ".", "eval", ":", "\n", "        ", "resume_epoch", "=", "resume_checkpoint", "(", "\n", "model", ",", "args", ".", "resume", ",", "\n", "optimizer", "=", "None", "if", "args", ".", "no_resume_opt", "else", "optimizer", ",", "\n", "loss_scaler", "=", "None", "if", "args", ".", "no_resume_opt", "else", "loss_scaler", ",", "\n", "log_info", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "", "if", "args", ".", "eval", ":", "\n", "        ", "if", "os", ".", "path", ".", "isfile", "(", "args", ".", "resume", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", "and", "'state_dict'", "in", "checkpoint", ":", "\n", "                ", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'state_dict'", "]", ".", "items", "(", ")", ":", "\n", "                    ", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "model", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "\n", "# setup exponential moving average of model weights, SWA could be used here too", "\n", "", "", "", "model_ema", "=", "None", "\n", "if", "args", ".", "model_ema", ":", "\n", "# Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper", "\n", "        ", "model_ema", "=", "ModelEmaV2", "(", "\n", "model", ",", "decay", "=", "args", ".", "model_ema_decay", ",", "device", "=", "'cpu'", "if", "args", ".", "model_ema_force_cpu", "else", "None", ")", "\n", "if", "args", ".", "resume", ":", "\n", "            ", "load_checkpoint", "(", "model_ema", ".", "module", ",", "args", ".", "resume", ",", "use_ema", "=", "True", ")", "\n", "\n", "# setup distributed training", "\n", "", "", "if", "args", ".", "distributed", ":", "\n", "        ", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex DDP preferred unless native amp is activated", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using NVIDIA APEX DistributedDataParallel.\"", ")", "\n", "", "model", "=", "ApexDDP", "(", "model", ",", "delay_allreduce", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using native Torch DistributedDataParallel.\"", ")", "\n", "", "model", "=", "NativeDDP", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "not", "args", ".", "no_ddp_bb", ")", "\n", "# NOTE: EMA model does not need to be wrapped by DDP", "\n", "\n", "# setup learning rate schedule and starting epoch", "\n", "", "", "lr_scheduler", ",", "num_epochs", "=", "create_scheduler", "(", "args", ",", "optimizer", ")", "\n", "start_epoch", "=", "0", "\n", "if", "args", ".", "start_epoch", "is", "not", "None", ":", "\n", "# a specified start_epoch will always override the resume epoch", "\n", "        ", "start_epoch", "=", "args", ".", "start_epoch", "\n", "", "elif", "resume_epoch", "is", "not", "None", ":", "\n", "        ", "start_epoch", "=", "resume_epoch", "\n", "", "if", "lr_scheduler", "is", "not", "None", "and", "start_epoch", ">", "0", ":", "\n", "        ", "lr_scheduler", ".", "step", "(", "start_epoch", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "'Scheduled epochs: {}'", ".", "format", "(", "num_epochs", ")", ")", "\n", "\n", "\n", "# create the train and eval datasets", "\n", "", "dataset_train", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "train_split", ",", "is_training", "=", "True", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "repeats", "=", "args", ".", "epoch_repeats", ",", "\n", "train_val_list", "=", "args", ".", "train_list", ")", "\n", "dataset_eval", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "val_split", ",", "is_training", "=", "False", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "train_val_list", "=", "args", ".", "val_list", ")", "\n", "\n", "# setup mixup / cutmix", "\n", "collate_fn", "=", "None", "\n", "mixup_fn", "=", "None", "\n", "mixup_active", "=", "args", ".", "mixup", ">", "0", "or", "args", ".", "cutmix", ">", "0.", "or", "args", ".", "cutmix_minmax", "is", "not", "None", "\n", "if", "mixup_active", ":", "\n", "        ", "mixup_args", "=", "dict", "(", "\n", "mixup_alpha", "=", "args", ".", "mixup", ",", "cutmix_alpha", "=", "args", ".", "cutmix", ",", "cutmix_minmax", "=", "args", ".", "cutmix_minmax", ",", "\n", "prob", "=", "args", ".", "mixup_prob", ",", "switch_prob", "=", "args", ".", "mixup_switch_prob", ",", "mode", "=", "args", ".", "mixup_mode", ",", "\n", "label_smoothing", "=", "args", ".", "smoothing", ",", "num_classes", "=", "args", ".", "num_classes", ")", "\n", "if", "args", ".", "prefetcher", ":", "\n", "            ", "assert", "not", "num_aug_splits", "# collate conflict (need to support deinterleaving in collate mixup)", "\n", "collate_fn", "=", "FastCollateMixup", "(", "**", "mixup_args", ")", "\n", "", "else", ":", "\n", "            ", "mixup_fn", "=", "Mixup", "(", "**", "mixup_args", ")", "\n", "\n", "# wrap dataset in AugMix helper", "\n", "", "", "if", "num_aug_splits", ">", "1", ":", "\n", "        ", "dataset_train", "=", "AugMixDataset", "(", "dataset_train", ",", "num_splits", "=", "num_aug_splits", ")", "\n", "\n", "# create data loaders w/ augmentation pipeiine", "\n", "", "train_interpolation", "=", "args", ".", "train_interpolation", "\n", "if", "args", ".", "no_aug", "or", "not", "train_interpolation", ":", "\n", "        ", "train_interpolation", "=", "data_config", "[", "'interpolation'", "]", "\n", "", "loader_train", "=", "create_loader", "(", "\n", "dataset_train", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "is_training", "=", "True", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "no_aug", "=", "args", ".", "no_aug", ",", "\n", "re_prob", "=", "args", ".", "reprob", ",", "\n", "re_mode", "=", "args", ".", "remode", ",", "\n", "re_count", "=", "args", ".", "recount", ",", "\n", "re_split", "=", "args", ".", "resplit", ",", "\n", "scale", "=", "args", ".", "scale", ",", "\n", "ratio", "=", "args", ".", "ratio", ",", "\n", "hflip", "=", "args", ".", "hflip", ",", "\n", "vflip", "=", "args", ".", "vflip", ",", "\n", "color_jitter", "=", "args", ".", "color_jitter", ",", "\n", "auto_augment", "=", "args", ".", "aa", ",", "\n", "num_aug_repeats", "=", "args", ".", "aug_repeats", ",", "\n", "num_aug_splits", "=", "num_aug_splits", ",", "\n", "interpolation", "=", "train_interpolation", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "use_multi_epochs_loader", "=", "args", ".", "use_multi_epochs_loader", ",", "\n", "worker_seeding", "=", "args", ".", "worker_seeding", ",", "\n", ")", "\n", "\n", "loader_eval", "=", "create_loader", "(", "\n", "dataset_eval", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "validation_batch_size", "or", "args", ".", "batch_size", "//", "8", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "interpolation", "=", "data_config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "crop_pct", "=", "data_config", "[", "'crop_pct'", "]", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", ")", "\n", "\n", "# setup loss function", "\n", "if", "args", ".", "jsd_loss", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "# JSD only valid with aug splits set", "\n", "train_loss_fn", "=", "JsdCrossEntropy", "(", "num_splits", "=", "num_aug_splits", ",", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "elif", "mixup_active", ":", "\n", "# smoothing is handled with mixup target transform which outputs sparse, soft targets", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "SoftTargetCrossEntropy", "(", ")", "\n", "", "", "elif", "args", ".", "smoothing", ":", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ",", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "", "else", ":", "\n", "        ", "train_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "train_loss_fn", "=", "train_loss_fn", ".", "cuda", "(", ")", "\n", "validate_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "\n", "\n", "# setup checkpoint saver and eval metric tracking", "\n", "eval_metric", "=", "args", ".", "eval_metric", "\n", "best_metric", "=", "None", "\n", "best_epoch", "=", "None", "\n", "saver", "=", "None", "\n", "output_dir", "=", "None", "\n", "if", "args", ".", "rank", "==", "0", ":", "\n", "        ", "if", "args", ".", "experiment", ":", "\n", "            ", "exp_name", "=", "args", ".", "experiment", "\n", "", "else", ":", "\n", "            ", "exp_name", "=", "'-'", ".", "join", "(", "[", "\n", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y%m%d-%H%M%S\"", ")", ",", "\n", "safe_model_name", "(", "args", ".", "model", ")", ",", "\n", "str", "(", "data_config", "[", "'input_size'", "]", "[", "-", "1", "]", ")", "\n", "]", ")", "\n", "", "output_dir", "=", "get_outdir", "(", "args", ".", "output", "if", "args", ".", "output", "else", "'./output/train'", ",", "exp_name", ")", "\n", "decreasing", "=", "True", "if", "eval_metric", "==", "'loss'", "else", "False", "\n", "saver", "=", "CheckpointSaver", "(", "\n", "model", "=", "model", ",", "optimizer", "=", "optimizer", ",", "args", "=", "args", ",", "model_ema", "=", "model_ema", ",", "amp_scaler", "=", "loss_scaler", ",", "\n", "checkpoint_dir", "=", "output_dir", ",", "recovery_dir", "=", "output_dir", ",", "decreasing", "=", "decreasing", ",", "max_history", "=", "args", ".", "checkpoint_hist", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'args.yaml'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "args_text", ")", "\n", "", "", "if", "args", ".", "eval", ":", "\n", "        ", "test_stats", "=", "validate", "(", "model", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ")", "\n", "print", "(", "f\"Accuracy of the network on the {len(dataset_eval)} test images: {test_stats['top1']:.1f}%\"", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "", "try", ":", "\n", "        ", "for", "epoch", "in", "range", "(", "start_epoch", ",", "num_epochs", ")", ":", "\n", "            ", "if", "args", ".", "log_image", "and", "has_wandb", "and", "args", ".", "log_wandb", ":", "\n", "                ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                    ", "train_sample", ",", "_", "=", "next", "(", "iter", "(", "loader_train", ")", ")", "\n", "val_sample", ",", "_", "=", "next", "(", "iter", "(", "loader_eval", ")", ")", "\n", "image", "=", "{", "}", "\n", "if", "len", "(", "list", "(", "train_sample", ".", "size", "(", ")", ")", ")", "<", "5", ":", "\n", "                        ", "train_sample", "=", "train_sample", ".", "unsqueeze", "(", "2", ")", "\n", "val_sample", "=", "val_sample", ".", "unsqueeze", "(", "2", ")", "\n", "", "for", "i", "in", "range", "(", "5", ")", ":", "\n", "                        ", "image", "[", "'train_'", "+", "str", "(", "i", ")", "]", "=", "[", "\n", "wandb", ".", "Image", "(", "train_sample", "[", "i", "]", ".", "permute", "(", "1", ",", "2", ",", "3", ",", "0", ")", "[", "p", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "for", "p", "in", "\n", "range", "(", "train_sample", ".", "size", "(", "2", ")", ")", "]", "\n", "image", "[", "'val_'", "+", "str", "(", "i", ")", "]", "=", "[", "\n", "wandb", ".", "Image", "(", "val_sample", "[", "i", "]", ".", "permute", "(", "1", ",", "2", ",", "3", ",", "0", ")", "[", "p", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "for", "p", "in", "\n", "range", "(", "val_sample", ".", "size", "(", "2", ")", ")", "]", "\n", "", "wandb", ".", "log", "(", "image", ",", "step", "=", "epoch", ")", "\n", "", "", "if", "args", ".", "distributed", "and", "hasattr", "(", "loader_train", ".", "sampler", ",", "'set_epoch'", ")", ":", "\n", "                ", "loader_train", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "train_metrics", "=", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader_train", ",", "optimizer", ",", "train_loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "lr_scheduler", ",", "saver", "=", "saver", ",", "output_dir", "=", "output_dir", ",", "\n", "amp_autocast", "=", "amp_autocast", ",", "loss_scaler", "=", "loss_scaler", ",", "model_ema", "=", "model_ema", ",", "mixup_fn", "=", "mixup_fn", ")", "\n", "\n", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\"Distributing BatchNorm running means and vars\"", ")", "\n", "", "distribute_bn", "(", "model", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "\n", "\n", "\n", "", "eval_metrics", "=", "validate", "(", "model", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ")", "\n", "\n", "if", "model_ema", "is", "not", "None", "and", "not", "args", ".", "model_ema_force_cpu", ":", "\n", "                ", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                    ", "distribute_bn", "(", "model_ema", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "", "ema_eval_metrics", "=", "validate", "(", "\n", "model_ema", ".", "module", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ",", "log_suffix", "=", "' (EMA)'", ")", "\n", "eval_metrics", "=", "ema_eval_metrics", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", "and", "not", "args", ".", "step_by_iteration", ":", "\n", "# step LR for next epoch", "\n", "                ", "lr_scheduler", ".", "step", "(", "epoch", "+", "1", ",", "eval_metrics", "[", "eval_metric", "]", ")", "\n", "\n", "", "if", "output_dir", "is", "not", "None", "and", "args", ".", "local_rank", "==", "0", ":", "\n", "                  ", "update_summary", "(", "\n", "epoch", ",", "train_metrics", ",", "eval_metrics", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'summary.csv'", ")", ",", "\n", "write_header", "=", "best_metric", "is", "None", ",", "log_wandb", "=", "args", ".", "log_wandb", "and", "has_wandb", ")", "\n", "\n", "", "if", "saver", "is", "not", "None", ":", "\n", "# save proper checkpoint with eval metric", "\n", "                ", "save_metric", "=", "eval_metrics", "[", "eval_metric", "]", "\n", "best_metric", ",", "best_epoch", "=", "saver", ".", "save_checkpoint", "(", "epoch", ",", "metric", "=", "save_metric", ")", "\n", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "pass", "\n", "", "if", "best_metric", "is", "not", "None", ":", "\n", "        ", "_logger", ".", "info", "(", "'*** Best metric: {0} (epoch {1})'", ".", "format", "(", "best_metric", ",", "best_epoch", ")", ")", "\n", "", "if", "has_wandb", "and", "args", ".", "log_wandb", "and", "args", ".", "local_rank", "==", "0", ":", "\n", "         ", "wandb", ".", "finish", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.train_one_epoch": [[775, 883], ["AverageMeter", "AverageMeter", "AverageMeter", "model.train", "time.time", "enumerate", "hasattr", "collections.OrderedDict", "hasattr", "len", "len", "AverageMeter.update", "optimizer.zero_grad", "torch.cuda.synchronize", "torch.cuda.synchronize", "AverageMeter.update", "time.time", "optimizer.sync_lookahead", "input.contiguous.contiguous", "amp_autocast", "model", "loss_fn", "AverageMeter.update", "loss_scaler", "loss_fn.backward", "optimizer.step", "model_ema.update", "saver.save_recovery", "lr_scheduler.step_update", "lr_scheduler.step", "time.time", "input.contiguous.cuda", "target.cuda", "mixup_fn", "loss_fn.item", "input.contiguous.size", "dispatch_clip_grad", "time.time", "sum", "len", "reduce_tensor", "AverageMeter.update", "_logger.info", "timm.models.model_parameters", "timm.models.model_parameters", "reduce_tensor.item", "input.contiguous.size", "torchvision.utils.save_image", "len", "os.path.join", "input.contiguous.size", "input.contiguous.size"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.sync_lookahead", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.save_recovery", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.step_update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.clip_grad.dispatch_clip_grad", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.model_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.model_parameters"], ["", "", "def", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader", ",", "optimizer", ",", "loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "None", ",", "saver", "=", "None", ",", "output_dir", "=", "None", ",", "amp_autocast", "=", "suppress", ",", "\n", "loss_scaler", "=", "None", ",", "model_ema", "=", "None", ",", "mixup_fn", "=", "None", ")", ":", "\n", "\n", "    ", "if", "args", ".", "mixup_off_epoch", "and", "epoch", ">=", "args", ".", "mixup_off_epoch", ":", "\n", "        ", "if", "args", ".", "prefetcher", "and", "loader", ".", "mixup_enabled", ":", "\n", "            ", "loader", ".", "mixup_enabled", "=", "False", "\n", "", "elif", "mixup_fn", "is", "not", "None", ":", "\n", "            ", "mixup_fn", ".", "mixup_enabled", "=", "False", "\n", "\n", "", "", "second_order", "=", "hasattr", "(", "optimizer", ",", "'is_second_order'", ")", "and", "optimizer", ".", "is_second_order", "\n", "batch_time_m", "=", "AverageMeter", "(", ")", "\n", "data_time_m", "=", "AverageMeter", "(", ")", "\n", "losses_m", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "num_updates", "=", "epoch", "*", "len", "(", "loader", ")", "\n", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "        ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "data_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "            ", "input", ",", "target", "=", "input", ".", "cuda", "(", ")", ",", "target", ".", "cuda", "(", ")", "\n", "if", "mixup_fn", "is", "not", "None", ":", "\n", "                ", "input", ",", "target", "=", "mixup_fn", "(", "input", ",", "target", ")", "\n", "", "", "if", "args", ".", "channels_last", ":", "\n", "            ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "            ", "output", "=", "model", "(", "input", ")", "\n", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "not", "args", ".", "distributed", ":", "\n", "            ", "losses_m", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "loss_scaler", "is", "not", "None", ":", "\n", "            ", "loss_scaler", "(", "\n", "loss", ",", "optimizer", ",", "\n", "clip_grad", "=", "args", ".", "clip_grad", ",", "clip_mode", "=", "args", ".", "clip_mode", ",", "\n", "parameters", "=", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "create_graph", "=", "second_order", ")", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", "create_graph", "=", "second_order", ")", "\n", "if", "args", ".", "clip_grad", "is", "not", "None", ":", "\n", "                ", "dispatch_clip_grad", "(", "\n", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "value", "=", "args", ".", "clip_grad", ",", "mode", "=", "args", ".", "clip_mode", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "model_ema", "is", "not", "None", ":", "\n", "            ", "model_ema", ".", "update", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "num_updates", "+=", "1", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "            ", "lrl", "=", "[", "param_group", "[", "'lr'", "]", "for", "param_group", "in", "optimizer", ".", "param_groups", "]", "\n", "lr", "=", "sum", "(", "lrl", ")", "/", "len", "(", "lrl", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\n", "'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '", "\n", "'Loss: {loss.val:#.4g} ({loss.avg:#.3g})  '", "\n", "'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '", "\n", "'({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '", "\n", "'LR: {lr:.3e}  '", "\n", "'Data: {data_time.val:.3f} ({data_time.avg:.3f})'", ".", "format", "(", "\n", "epoch", ",", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "\n", "100.", "*", "batch_idx", "/", "last_idx", ",", "\n", "loss", "=", "losses_m", ",", "\n", "batch_time", "=", "batch_time_m", ",", "\n", "rate", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "val", ",", "\n", "rate_avg", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "avg", ",", "\n", "lr", "=", "lr", ",", "\n", "data_time", "=", "data_time_m", ")", ")", "\n", "\n", "if", "args", ".", "save_images", "and", "output_dir", ":", "\n", "                    ", "torchvision", ".", "utils", ".", "save_image", "(", "\n", "input", ",", "\n", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'train-batch-%d.jpg'", "%", "batch_idx", ")", ",", "\n", "padding", "=", "0", ",", "\n", "normalize", "=", "True", ")", "\n", "\n", "", "", "", "if", "saver", "is", "not", "None", "and", "args", ".", "recovery_interval", "and", "(", "\n", "last_batch", "or", "(", "batch_idx", "+", "1", ")", "%", "args", ".", "recovery_interval", "==", "0", ")", ":", "\n", "            ", "saver", ".", "save_recovery", "(", "epoch", ",", "batch_idx", "=", "batch_idx", ")", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", "and", "args", ".", "step_by_iteration", ":", "\n", "            ", "lr_scheduler", ".", "step_update", "(", "num_updates", "=", "num_updates", ",", "metric", "=", "losses_m", ".", "avg", ")", "\n", "", "if", "lr_scheduler", "and", "args", ".", "step_by_iteration", ":", "\n", "            ", "lr_scheduler", ".", "step", "(", "epoch", "=", "num_updates", ",", "metric", "=", "losses_m", ".", "avg", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "# end for", "\n", "\n", "", "if", "hasattr", "(", "optimizer", ",", "'sync_lookahead'", ")", ":", "\n", "        ", "optimizer", ".", "sync_lookahead", "(", ")", "\n", "\n", "", "return", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.train.validate": [[885, 947], ["AverageMeter", "AverageMeter", "AverageMeter", "AverageMeter", "model.eval", "time.time", "collections.OrderedDict", "len", "torch.no_grad", "torch.no_grad", "enumerate", "isinstance", "loss_fn", "accuracy", "torch.cuda.synchronize", "torch.cuda.synchronize", "AverageMeter.update", "AverageMeter.update", "AverageMeter.update", "AverageMeter.update", "time.time", "input.contiguous.cuda", "target.cuda.cuda", "input.contiguous.contiguous", "amp_autocast", "model", "output.unfold().mean.unfold().mean", "reduce_tensor", "reduce_tensor", "reduce_tensor", "reduce_tensor.item", "input.contiguous.size", "reduce_tensor.item", "output.unfold().mean.size", "reduce_tensor.item", "output.unfold().mean.size", "_logger.info", "time.time", "output.unfold().mean.unfold", "target.cuda.size"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.accuracy", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.reduce_tensor", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.reduce_tensor"], ["", "def", "validate", "(", "model", ",", "loader", ",", "loss_fn", ",", "args", ",", "amp_autocast", "=", "suppress", ",", "log_suffix", "=", "''", ")", ":", "\n", "    ", "batch_time_m", "=", "AverageMeter", "(", ")", "\n", "losses_m", "=", "AverageMeter", "(", ")", "\n", "top1_m", "=", "AverageMeter", "(", ")", "\n", "top5_m", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "                ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "target", "=", "target", ".", "cuda", "(", ")", "\n", "", "if", "args", ".", "channels_last", ":", "\n", "                ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "model", "(", "input", ")", "\n", "", "if", "isinstance", "(", "output", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                ", "output", "=", "output", "[", "0", "]", "\n", "\n", "# augmentation reduction", "\n", "", "reduce_factor", "=", "args", ".", "tta", "\n", "if", "reduce_factor", ">", "1", ":", "\n", "                ", "output", "=", "output", ".", "unfold", "(", "0", ",", "reduce_factor", ",", "reduce_factor", ")", ".", "mean", "(", "dim", "=", "2", ")", "\n", "target", "=", "target", "[", "0", ":", "target", ".", "size", "(", "0", ")", ":", "reduce_factor", "]", "\n", "\n", "", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "acc1", ",", "acc5", "=", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "acc1", "=", "reduce_tensor", "(", "acc1", ",", "args", ".", "world_size", ")", "\n", "acc5", "=", "reduce_tensor", "(", "acc5", ",", "args", ".", "world_size", ")", "\n", "", "else", ":", "\n", "                ", "reduced_loss", "=", "loss", ".", "data", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "top1_m", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "top5_m", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "and", "(", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ")", ":", "\n", "                ", "log_name", "=", "'Test'", "+", "log_suffix", "\n", "_logger", ".", "info", "(", "\n", "'{0}: [{1:>4d}/{2}]  '", "\n", "'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '", "\n", "'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '", "\n", "'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  '", "\n", "'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'", ".", "format", "(", "\n", "log_name", ",", "batch_idx", ",", "last_idx", ",", "batch_time", "=", "batch_time_m", ",", "\n", "loss", "=", "losses_m", ",", "top1", "=", "top1_m", ",", "top5", "=", "top5_m", ")", ")", "\n", "\n", "", "", "", "metrics", "=", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", ",", "(", "'top1'", ",", "top1_m", ".", "avg", ")", ",", "(", "'top5'", ",", "top5_m", ".", "avg", ")", "]", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.clean_checkpoint.main": [[31, 39], ["parser.parse_args", "os.path.exists", "clean_checkpoint.clean_checkpoint", "print", "exit"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.clean_checkpoint.clean_checkpoint"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output", ")", ":", "\n", "        ", "print", "(", "\"Error: Output filename ({}) already exists.\"", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "clean_checkpoint", "(", "args", ".", "checkpoint", ",", "args", ".", "output", ",", "not", "args", ".", "no_use_ema", ",", "args", ".", "clean_aux_bn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.clean_checkpoint.clean_checkpoint": [[41, 77], ["os.path.isfile", "print", "timm.models.helpers.load_state_dict", "timm.models.helpers.load_state_dict.items", "print", "shutil.move", "print", "print", "torch.save", "open", "hashlib.sha256().hexdigest", "os.path.split", "os.path.join", "k.startswith", "torch.save", "os.path.splitext", "os.path.splitext", "hashlib.sha256", "f.read"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "def", "clean_checkpoint", "(", "checkpoint", ",", "output", "=", "''", ",", "use_ema", "=", "True", ",", "clean_aux_bn", "=", "False", ")", ":", "\n", "# Load an existing checkpoint to CPU, strip everything but the state_dict and re-save", "\n", "    ", "if", "checkpoint", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint", ")", ":", "\n", "        ", "print", "(", "\"=> Loading checkpoint '{}'\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "state_dict", "=", "load_state_dict", "(", "checkpoint", ",", "use_ema", "=", "use_ema", ")", "\n", "new_state_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "clean_aux_bn", "and", "'aux_bn'", "in", "k", ":", "\n", "# If all aux_bn keys are removed, the SplitBN layers will end up as normal and", "\n", "# load with the unmodified model using BatchNorm2d.", "\n", "                ", "continue", "\n", "", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "print", "(", "\"=> Loaded state_dict from '{}'\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "\n", "try", ":", "\n", "            ", "torch", ".", "save", "(", "new_state_dict", ",", "_TEMP_NAME", ",", "_use_new_zipfile_serialization", "=", "False", ")", "\n", "", "except", ":", "\n", "            ", "torch", ".", "save", "(", "new_state_dict", ",", "_TEMP_NAME", ")", "\n", "\n", "", "with", "open", "(", "_TEMP_NAME", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "sha_hash", "=", "hashlib", ".", "sha256", "(", "f", ".", "read", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "\n", "", "if", "output", ":", "\n", "            ", "checkpoint_root", ",", "checkpoint_base", "=", "os", ".", "path", ".", "split", "(", "output", ")", "\n", "checkpoint_base", "=", "os", ".", "path", ".", "splitext", "(", "checkpoint_base", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "checkpoint_root", "=", "''", "\n", "checkpoint_base", "=", "os", ".", "path", ".", "splitext", "(", "checkpoint", ")", "[", "0", "]", "\n", "", "final_filename", "=", "'-'", ".", "join", "(", "[", "checkpoint_base", ",", "sha_hash", "[", ":", "8", "]", "]", ")", "+", "'.pth'", "\n", "shutil", ".", "move", "(", "_TEMP_NAME", ",", "os", ".", "path", ".", "join", "(", "checkpoint_root", ",", "final_filename", ")", ")", "\n", "print", "(", "\"=> Saved state_dict to '{}, SHA256: {}'\"", ".", "format", "(", "final_filename", ",", "sha_hash", ")", ")", "\n", "return", "final_filename", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Error: Checkpoint ({}) doesn't exist\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.BenchmarkRunner.__init__": [[187, 227], ["benchmark.resolve_precision", "kwargs.pop", "timm.models.create_model", "benchmark.BenchmarkRunner.model.to", "benchmark.count_params", "_logger.info", "timm.data.resolve_data_config", "kwargs.pop", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "torch.jit.script", "functools.partial", "kwargs.pop", "kwargs.pop"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.resolve_precision", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.count_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.config.resolve_data_config"], ["    ", "def", "__init__", "(", "\n", "self", ",", "model_name", ",", "detail", "=", "False", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "precision", "=", "'float32'", ",", "\n", "num_warm_iter", "=", "10", ",", "num_bench_iter", "=", "50", ",", "use_train_size", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "model_name", "=", "model_name", "\n", "self", ".", "detail", "=", "detail", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "use_amp", ",", "self", ".", "model_dtype", ",", "self", ".", "data_dtype", "=", "resolve_precision", "(", "precision", ")", "\n", "self", ".", "channels_last", "=", "kwargs", ".", "pop", "(", "'channels_last'", ",", "False", ")", "\n", "self", ".", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "if", "self", ".", "use_amp", "else", "suppress", "\n", "\n", "self", ".", "model", "=", "create_model", "(", "\n", "model_name", ",", "\n", "num_classes", "=", "kwargs", ".", "pop", "(", "'num_classes'", ",", "None", ")", ",", "\n", "in_chans", "=", "3", ",", "\n", "global_pool", "=", "kwargs", ".", "pop", "(", "'gp'", ",", "'fast'", ")", ",", "\n", "scriptable", "=", "torchscript", ")", "\n", "self", ".", "model", ".", "to", "(", "\n", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "model_dtype", ",", "\n", "memory_format", "=", "torch", ".", "channels_last", "if", "self", ".", "channels_last", "else", "None", ")", "\n", "self", ".", "num_classes", "=", "self", ".", "model", ".", "num_classes", "\n", "self", ".", "param_count", "=", "count_params", "(", "self", ".", "model", ")", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "(", "model_name", ",", "self", ".", "param_count", ")", ")", "\n", "self", ".", "scripted", "=", "False", "\n", "if", "torchscript", ":", "\n", "            ", "self", ".", "model", "=", "torch", ".", "jit", ".", "script", "(", "self", ".", "model", ")", "\n", "self", ".", "scripted", "=", "True", "\n", "\n", "", "data_config", "=", "resolve_data_config", "(", "kwargs", ",", "model", "=", "self", ".", "model", ",", "use_test_size", "=", "not", "use_train_size", ")", "\n", "self", ".", "input_size", "=", "data_config", "[", "'input_size'", "]", "\n", "self", ".", "batch_size", "=", "kwargs", ".", "pop", "(", "'batch_size'", ",", "256", ")", "\n", "\n", "self", ".", "example_inputs", "=", "None", "\n", "self", ".", "num_warm_iter", "=", "num_warm_iter", "\n", "self", ".", "num_bench_iter", "=", "num_bench_iter", "\n", "self", ".", "log_freq", "=", "num_bench_iter", "//", "5", "\n", "if", "'cuda'", "in", "self", ".", "device", ":", "\n", "            ", "self", ".", "time_fn", "=", "partial", "(", "cuda_timestamp", ",", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "time_fn", "=", "timestamp", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.BenchmarkRunner._init_input": [[228, 233], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "benchmark.BenchmarkRunner.example_inputs.contiguous"], "methods", ["None"], ["", "", "def", "_init_input", "(", "self", ")", ":", "\n", "        ", "self", ".", "example_inputs", "=", "torch", ".", "randn", "(", "\n", "(", "self", ".", "batch_size", ",", ")", "+", "self", ".", "input_size", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "data_dtype", ")", "\n", "if", "self", ".", "channels_last", ":", "\n", "            ", "self", ".", "example_inputs", "=", "self", ".", "example_inputs", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.InferenceBenchmarkRunner.__init__": [[237, 240], ["benchmark.BenchmarkRunner.__init__", "benchmark.InferenceBenchmarkRunner.model.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "torchscript", "=", "torchscript", ",", "**", "kwargs", ")", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.InferenceBenchmarkRunner.run": [[241, 302], ["_logger.info", "dict", "_logger.info", "benchmark.InferenceBenchmarkRunner.time_fn", "benchmark.InferenceBenchmarkRunner.time_fn", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "benchmark.InferenceBenchmarkRunner._init_input", "range", "benchmark.InferenceBenchmarkRunner.time_fn", "range", "benchmark.InferenceBenchmarkRunner.time_fn", "benchmark.InferenceBenchmarkRunner.amp_autocast", "benchmark.InferenceBenchmarkRunner.model", "benchmark.InferenceBenchmarkRunner.run._step"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.BenchmarkRunner._init_input"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "def", "_step", "(", ")", ":", "\n", "            ", "t_step_start", "=", "self", ".", "time_fn", "(", ")", "\n", "with", "self", ".", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "model", "(", "self", ".", "example_inputs", ")", "\n", "", "t_step_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "return", "t_step_end", "-", "t_step_start", "\n", "\n", "", "_logger", ".", "info", "(", "\n", "f'Running inference benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "_init_input", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "num_warm_iter", ")", ":", "\n", "                ", "_step", "(", ")", "\n", "\n", "", "total_step", "=", "0.", "\n", "num_samples", "=", "0", "\n", "t_run_start", "=", "self", ".", "time_fn", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_fwd", "=", "_step", "(", ")", "\n", "total_step", "+=", "delta_fwd", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "num_steps", "=", "i", "+", "1", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\n", "f\"Infer [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_step / num_steps:0.3f} ms/step.\"", ")", "\n", "", "", "t_run_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "t_run_elapsed", "=", "t_run_end", "-", "t_run_start", "\n", "\n", "", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "retries", "=", "0", "if", "self", ".", "scripted", "else", "2", "# skip profiling if model is scripted", "\n", "while", "retries", ":", "\n", "            ", "retries", "-=", "1", "\n", "try", ":", "\n", "                ", "if", "has_deepspeed_profiling", ":", "\n", "                    ", "macs", ",", "_", "=", "profile_deepspeed", "(", "self", ".", "model", ",", "self", ".", "input_size", ")", "\n", "results", "[", "'gmacs'", "]", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", "\n", "", "elif", "has_fvcore_profiling", ":", "\n", "                    ", "macs", ",", "activations", "=", "profile_fvcore", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "force_cpu", "=", "not", "retries", ")", "\n", "results", "[", "'gmacs'", "]", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", "\n", "results", "[", "'macts'", "]", "=", "round", "(", "activations", "/", "1e6", ",", "2", ")", "\n", "", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "pass", "\n", "\n", "", "", "_logger", ".", "info", "(", "\n", "f\"Inference benchmark of {self.model_name} done. \"", "\n", "f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/step\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.TrainBenchmarkRunner.__init__": [[306, 320], ["benchmark.BenchmarkRunner.__init__", "benchmark.TrainBenchmarkRunner.model.train", "tuple", "timm.optim.create_optimizer_v2", "kwargs.pop", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "torch.CrossEntropyLoss().to", "kwargs.pop", "kwargs.pop", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.create_optimizer_v2"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "torchscript", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "torchscript", "=", "torchscript", ",", "**", "kwargs", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "if", "kwargs", ".", "pop", "(", "'smoothing'", ",", "0", ")", ">", "0", ":", "\n", "            ", "self", ".", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "target_shape", "=", "tuple", "(", ")", "\n", "\n", "self", ".", "optimizer", "=", "create_optimizer_v2", "(", "\n", "self", ".", "model", ",", "\n", "opt", "=", "kwargs", ".", "pop", "(", "'opt'", ",", "'sgd'", ")", ",", "\n", "lr", "=", "kwargs", ".", "pop", "(", "'lr'", ",", "1e-4", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.TrainBenchmarkRunner._gen_target": [[321, 324], ["torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty().random_", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "def", "_gen_target", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "\n", "(", "batch_size", ",", ")", "+", "self", ".", "target_shape", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "long", ")", ".", "random_", "(", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.TrainBenchmarkRunner.run": [[325, 422], ["_logger.info", "benchmark.TrainBenchmarkRunner._init_input", "range", "benchmark.TrainBenchmarkRunner.time_fn", "_logger.info", "benchmark.TrainBenchmarkRunner.optimizer.zero_grad", "benchmark.TrainBenchmarkRunner.time_fn", "benchmark.TrainBenchmarkRunner.optimizer.step", "benchmark.TrainBenchmarkRunner.time_fn", "benchmark.TrainBenchmarkRunner.run._step"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.BenchmarkRunner._init_input", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "def", "_step", "(", "detail", "=", "False", ")", ":", "\n", "            ", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "# can this be ignored?", "\n", "t_start", "=", "self", ".", "time_fn", "(", ")", "\n", "t_fwd_end", "=", "t_start", "\n", "t_bwd_end", "=", "t_start", "\n", "with", "self", ".", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "model", "(", "self", ".", "example_inputs", ")", "\n", "if", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "                    ", "output", "=", "output", "[", "0", "]", "\n", "", "if", "detail", ":", "\n", "                    ", "t_fwd_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "", "target", "=", "self", ".", "_gen_target", "(", "output", ".", "shape", "[", "0", "]", ")", "\n", "self", ".", "loss", "(", "output", ",", "target", ")", ".", "backward", "(", ")", "\n", "if", "detail", ":", "\n", "                    ", "t_bwd_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "t_end", "=", "self", ".", "time_fn", "(", "True", ")", "\n", "if", "detail", ":", "\n", "                ", "delta_fwd", "=", "t_fwd_end", "-", "t_start", "\n", "delta_bwd", "=", "t_bwd_end", "-", "t_fwd_end", "\n", "delta_opt", "=", "t_end", "-", "t_bwd_end", "\n", "return", "delta_fwd", ",", "delta_bwd", ",", "delta_opt", "\n", "", "else", ":", "\n", "                ", "delta_step", "=", "t_end", "-", "t_start", "\n", "return", "delta_step", "\n", "\n", "", "", "_logger", ".", "info", "(", "\n", "f'Running train benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "self", ".", "_init_input", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "num_warm_iter", ")", ":", "\n", "            ", "_step", "(", ")", "\n", "\n", "", "t_run_start", "=", "self", ".", "time_fn", "(", ")", "\n", "if", "self", ".", "detail", ":", "\n", "            ", "total_fwd", "=", "0.", "\n", "total_bwd", "=", "0.", "\n", "total_opt", "=", "0.", "\n", "num_samples", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_fwd", ",", "delta_bwd", ",", "delta_opt", "=", "_step", "(", "True", ")", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "total_fwd", "+=", "delta_fwd", "\n", "total_bwd", "+=", "delta_bwd", "\n", "total_opt", "+=", "delta_opt", "\n", "num_steps", "=", "(", "i", "+", "1", ")", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "total_step", "=", "total_fwd", "+", "total_bwd", "+", "total_opt", "\n", "_logger", ".", "info", "(", "\n", "f\"Train [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_fwd / num_steps:0.3f} ms/step fwd,\"", "\n", "f\" {1000 * total_bwd / num_steps:0.3f} ms/step bwd,\"", "\n", "f\" {1000 * total_opt / num_steps:0.3f} ms/step opt.\"", "\n", ")", "\n", "", "", "total_step", "=", "total_fwd", "+", "total_bwd", "+", "total_opt", "\n", "t_run_elapsed", "=", "self", ".", "time_fn", "(", ")", "-", "t_run_start", "\n", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "fwd_time", "=", "round", "(", "1000", "*", "total_fwd", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "bwd_time", "=", "round", "(", "1000", "*", "total_bwd", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "opt_time", "=", "round", "(", "1000", "*", "total_opt", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "total_step", "=", "0.", "\n", "num_samples", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "num_bench_iter", ")", ":", "\n", "                ", "delta_step", "=", "_step", "(", "False", ")", "\n", "num_samples", "+=", "self", ".", "batch_size", "\n", "total_step", "+=", "delta_step", "\n", "num_steps", "=", "(", "i", "+", "1", ")", "\n", "if", "num_steps", "%", "self", ".", "log_freq", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\n", "f\"Train [{num_steps}/{self.num_bench_iter}].\"", "\n", "f\" {num_samples / total_step:0.2f} samples/sec.\"", "\n", "f\" {1000 * total_step / num_steps:0.3f} ms/step.\"", ")", "\n", "", "", "t_run_elapsed", "=", "self", ".", "time_fn", "(", ")", "-", "t_run_start", "\n", "results", "=", "dict", "(", "\n", "samples_per_sec", "=", "round", "(", "num_samples", "/", "t_run_elapsed", ",", "2", ")", ",", "\n", "step_time", "=", "round", "(", "1000", "*", "total_step", "/", "self", ".", "num_bench_iter", ",", "3", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "", "_logger", ".", "info", "(", "\n", "f\"Train benchmark of {self.model_name} done. \"", "\n", "f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/sample\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.ProfileRunner.__init__": [[426, 436], ["benchmark.BenchmarkRunner.__init__", "benchmark.ProfileRunner.model.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model_name", ",", "device", "=", "'cuda'", ",", "profiler", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model_name", "=", "model_name", ",", "device", "=", "device", ",", "**", "kwargs", ")", "\n", "if", "not", "profiler", ":", "\n", "            ", "if", "has_deepspeed_profiling", ":", "\n", "                ", "profiler", "=", "'deepspeed'", "\n", "", "elif", "has_fvcore_profiling", ":", "\n", "                ", "profiler", "=", "'fvcore'", "\n", "", "", "assert", "profiler", ",", "\"One of deepspeed or fvcore needs to be installed for profiling to work.\"", "\n", "self", ".", "profiler", "=", "profiler", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.ProfileRunner.run": [[437, 462], ["_logger.info", "dict", "_logger.info", "benchmark.profile_deepspeed", "benchmark.profile_fvcore", "round", "round", "round"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.profile_deepspeed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.profile_fvcore"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "\n", "f'Running profiler on {self.model_name} w/ '", "\n", "f'input size {self.input_size} and batch size {self.batch_size}.'", ")", "\n", "\n", "macs", "=", "0", "\n", "activations", "=", "0", "\n", "if", "self", ".", "profiler", "==", "'deepspeed'", ":", "\n", "            ", "macs", ",", "_", "=", "profile_deepspeed", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "batch_size", "=", "self", ".", "batch_size", ",", "detailed", "=", "True", ")", "\n", "", "elif", "self", ".", "profiler", "==", "'fvcore'", ":", "\n", "            ", "macs", ",", "activations", "=", "profile_fvcore", "(", "self", ".", "model", ",", "self", ".", "input_size", ",", "batch_size", "=", "self", ".", "batch_size", ",", "detailed", "=", "True", ")", "\n", "\n", "", "results", "=", "dict", "(", "\n", "gmacs", "=", "round", "(", "macs", "/", "1e9", ",", "2", ")", ",", "\n", "macts", "=", "round", "(", "activations", "/", "1e6", ",", "2", ")", ",", "\n", "batch_size", "=", "self", ".", "batch_size", ",", "\n", "img_size", "=", "self", ".", "input_size", "[", "-", "1", "]", ",", "\n", "param_count", "=", "round", "(", "self", ".", "param_count", "/", "1e6", ",", "2", ")", ",", "\n", ")", "\n", "\n", "_logger", ".", "info", "(", "\n", "f\"Profile of {self.model_name} done. \"", "\n", "f\"{results['gmacs']:.2f} GMACs, {results['param_count']:.2f} M params.\"", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.timestamp": [[129, 131], ["time.perf_counter"], "function", ["None"], ["def", "timestamp", "(", "sync", "=", "False", ")", ":", "\n", "    ", "return", "time", ".", "perf_counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.cuda_timestamp": [[133, 137], ["time.perf_counter", "torch.cuda.synchronize", "torch.cuda.synchronize", "torch.cuda.synchronize"], "function", ["None"], ["", "def", "cuda_timestamp", "(", "sync", "=", "False", ",", "device", "=", "None", ")", ":", "\n", "    ", "if", "sync", ":", "\n", "        ", "torch", ".", "cuda", ".", "synchronize", "(", "device", "=", "device", ")", "\n", "", "return", "time", ".", "perf_counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.count_params": [[139, 141], ["sum", "m.numel", "model.parameters"], "function", ["None"], ["", "def", "count_params", "(", "model", ":", "nn", ".", "Module", ")", ":", "\n", "    ", "return", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.resolve_precision": [[143, 157], ["None"], "function", ["None"], ["", "def", "resolve_precision", "(", "precision", ":", "str", ")", ":", "\n", "    ", "assert", "precision", "in", "(", "'amp'", ",", "'float16'", ",", "'bfloat16'", ",", "'float32'", ")", "\n", "use_amp", "=", "False", "\n", "model_dtype", "=", "torch", ".", "float32", "\n", "data_dtype", "=", "torch", ".", "float32", "\n", "if", "precision", "==", "'amp'", ":", "\n", "        ", "use_amp", "=", "True", "\n", "", "elif", "precision", "==", "'float16'", ":", "\n", "        ", "model_dtype", "=", "torch", ".", "float16", "\n", "data_dtype", "=", "torch", ".", "float16", "\n", "", "elif", "precision", "==", "'bfloat16'", ":", "\n", "        ", "model_dtype", "=", "torch", ".", "bfloat16", "\n", "data_dtype", "=", "torch", ".", "bfloat16", "\n", "", "return", "use_amp", ",", "model_dtype", ",", "data_dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.profile_deepspeed": [[159, 171], ["get_model_profile"], "function", ["None"], ["", "def", "profile_deepspeed", "(", "model", ",", "input_size", "=", "(", "3", ",", "224", ",", "224", ")", ",", "batch_size", "=", "1", ",", "detailed", "=", "False", ")", ":", "\n", "    ", "macs", ",", "_", "=", "get_model_profile", "(", "\n", "model", "=", "model", ",", "\n", "input_res", "=", "(", "batch_size", ",", ")", "+", "input_size", ",", "# input shape or input to the input_constructor", "\n", "input_constructor", "=", "None", ",", "# if specified, a constructor taking input_res is used as input to the model", "\n", "print_profile", "=", "detailed", ",", "# prints the model graph with the measured profile attached to each module", "\n", "detailed", "=", "detailed", ",", "# print the detailed profile", "\n", "warm_up", "=", "10", ",", "# the number of warm-ups before measuring the time of each module", "\n", "as_string", "=", "False", ",", "# print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)", "\n", "output_file", "=", "None", ",", "# path to the output file. If None, the profiler prints to stdout.", "\n", "ignore_modules", "=", "None", ")", "# the list of modules to ignore in the profiling", "\n", "return", "macs", ",", "0", "# no activation count in DS", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.profile_fvcore": [[173, 184], ["torch.ones", "torch.ones", "torch.ones", "FlopCountAnalysis", "ActivationCountAnalysis", "model.to.to", "flop_count_str", "print", "FlopCountAnalysis.total", "ActivationCountAnalysis.total", "next", "next", "model.to.parameters", "model.to.parameters"], "function", ["None"], ["", "def", "profile_fvcore", "(", "model", ",", "input_size", "=", "(", "3", ",", "224", ",", "224", ")", ",", "batch_size", "=", "1", ",", "detailed", "=", "False", ",", "force_cpu", "=", "False", ")", ":", "\n", "    ", "if", "force_cpu", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "'cpu'", ")", "\n", "", "device", ",", "dtype", "=", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "device", ",", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", "example_input", "=", "torch", ".", "ones", "(", "(", "batch_size", ",", ")", "+", "input_size", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "\n", "fca", "=", "FlopCountAnalysis", "(", "model", ",", "example_input", ")", "\n", "aca", "=", "ActivationCountAnalysis", "(", "model", ",", "example_input", ")", "\n", "if", "detailed", ":", "\n", "        ", "fcs", "=", "flop_count_str", "(", "fca", ")", "\n", "print", "(", "fcs", ")", "\n", "", "return", "fca", ".", "total", "(", ")", ",", "aca", ".", "total", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.decay_batch_exp": [[464, 471], ["max", "int"], "function", ["None"], ["", "", "def", "decay_batch_exp", "(", "batch_size", ",", "factor", "=", "0.5", ",", "divisor", "=", "16", ")", ":", "\n", "    ", "out_batch_size", "=", "batch_size", "*", "factor", "\n", "if", "out_batch_size", ">", "divisor", ":", "\n", "        ", "out_batch_size", "=", "(", "out_batch_size", "+", "1", ")", "//", "divisor", "*", "divisor", "\n", "", "else", ":", "\n", "        ", "out_batch_size", "=", "batch_size", "-", "1", "\n", "", "return", "max", "(", "0", ",", "int", "(", "out_batch_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark._try_run": [[473, 491], ["dict", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "benchmark.decay_batch_exp", "bench_fn", "bench_fn.run", "str", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.decay_batch_exp", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run"], ["", "def", "_try_run", "(", "model_name", ",", "bench_fn", ",", "initial_batch_size", ",", "bench_kwargs", ")", ":", "\n", "    ", "batch_size", "=", "initial_batch_size", "\n", "results", "=", "dict", "(", ")", "\n", "while", "batch_size", ">=", "1", ":", "\n", "        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "try", ":", "\n", "            ", "bench", "=", "bench_fn", "(", "model_name", "=", "model_name", ",", "batch_size", "=", "batch_size", ",", "**", "bench_kwargs", ")", "\n", "results", "=", "bench", ".", "run", "(", ")", "\n", "return", "results", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "            ", "e_str", "=", "str", "(", "e", ")", "\n", "print", "(", "e_str", ")", "\n", "if", "'channels_last'", "in", "e_str", ":", "\n", "                ", "print", "(", "f'Error: {model_name} not supported in channels_last, skipping.'", ")", "\n", "break", "\n", "", "print", "(", "f'Error: \"{e_str}\" while running benchmark. Reducing batch size to {batch_size} for retry.'", ")", "\n", "", "batch_size", "=", "decay_batch_exp", "(", "batch_size", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.benchmark": [[493, 538], ["_logger.info", "vars().copy", "vars().copy.pop", "vars().copy.pop", "vars().copy.pop", "collections.OrderedDict", "zip", "collections.OrderedDict.pop", "collections.OrderedDict.setdefault", "collections.OrderedDict.pop", "_logger.warning", "benchmark._try_run", "collections.OrderedDict.update", "collections.OrderedDict.pop", "dict", "vars", "args.bench.startswith", "_try_run.items"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark._try_run", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "benchmark", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "amp", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Overriding precision to 'amp' since --amp flag set.\"", ")", "\n", "args", ".", "precision", "=", "'amp'", "\n", "", "_logger", ".", "info", "(", "f'Benchmarking in {args.precision} precision. '", "\n", "f'{\"NHWC\" if args.channels_last else \"NCHW\"} layout. '", "\n", "f'torchscript {\"enabled\" if args.torchscript else \"disabled\"}'", ")", "\n", "\n", "bench_kwargs", "=", "vars", "(", "args", ")", ".", "copy", "(", ")", "\n", "bench_kwargs", ".", "pop", "(", "'amp'", ")", "\n", "model", "=", "bench_kwargs", ".", "pop", "(", "'model'", ")", "\n", "batch_size", "=", "bench_kwargs", ".", "pop", "(", "'batch_size'", ")", "\n", "\n", "bench_fns", "=", "(", "InferenceBenchmarkRunner", ",", ")", "\n", "prefixes", "=", "(", "'infer'", ",", ")", "\n", "if", "args", ".", "bench", "==", "'both'", ":", "\n", "        ", "bench_fns", "=", "(", "\n", "InferenceBenchmarkRunner", ",", "\n", "TrainBenchmarkRunner", "\n", ")", "\n", "prefixes", "=", "(", "'infer'", ",", "'train'", ")", "\n", "", "elif", "args", ".", "bench", "==", "'train'", ":", "\n", "        ", "bench_fns", "=", "TrainBenchmarkRunner", ",", "\n", "prefixes", "=", "'train'", ",", "\n", "", "elif", "args", ".", "bench", ".", "startswith", "(", "'profile'", ")", ":", "\n", "# specific profiler used if included in bench mode string, otherwise default to deepspeed, fallback to fvcore", "\n", "        ", "if", "'deepspeed'", "in", "args", ".", "bench", ":", "\n", "            ", "assert", "has_deepspeed_profiling", ",", "\"deepspeed must be installed to use deepspeed flop counter\"", "\n", "bench_kwargs", "[", "'profiler'", "]", "=", "'deepspeed'", "\n", "", "elif", "'fvcore'", "in", "args", ".", "bench", ":", "\n", "            ", "assert", "has_fvcore_profiling", ",", "\"fvcore must be installed to use fvcore flop counter\"", "\n", "bench_kwargs", "[", "'profiler'", "]", "=", "'fvcore'", "\n", "", "bench_fns", "=", "ProfileRunner", ",", "\n", "batch_size", "=", "1", "\n", "\n", "", "model_results", "=", "OrderedDict", "(", "model", "=", "model", ")", "\n", "for", "prefix", ",", "bench_fn", "in", "zip", "(", "prefixes", ",", "bench_fns", ")", ":", "\n", "        ", "run_results", "=", "_try_run", "(", "model", ",", "bench_fn", ",", "initial_batch_size", "=", "batch_size", ",", "bench_kwargs", "=", "bench_kwargs", ")", "\n", "if", "prefix", ":", "\n", "            ", "run_results", "=", "{", "'_'", ".", "join", "(", "[", "prefix", ",", "k", "]", ")", ":", "v", "for", "k", ",", "v", "in", "run_results", ".", "items", "(", ")", "}", "\n", "", "model_results", ".", "update", "(", "run_results", ")", "\n", "", "param_count", "=", "model_results", ".", "pop", "(", "'infer_param_count'", ",", "model_results", ".", "pop", "(", "'train_param_count'", ",", "0", ")", ")", "\n", "model_results", ".", "setdefault", "(", "'param_count'", ",", "param_count", ")", "\n", "model_results", ".", "pop", "(", "'train_param_count'", ",", "0", ")", "\n", "return", "model_results", "if", "model_results", "[", "'param_count'", "]", "else", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.main": [[540, 588], ["timm.utils.setup_default_logging", "parser.parse_args", "len", "json.dumps", "print", "_logger.info", "sorted", "len", "benchmark.benchmark", "open", "timm.models.list_models", "benchmark.write_results", "line.rstrip", "timm.models.is_model", "timm.models.list_models", "benchmark.benchmark", "time.sleep", "benchmark.append"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.setup_default_logging", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.benchmark", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.list_models", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.write_results", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.list_models", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.benchmark"], ["", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "model_cfgs", "=", "[", "]", "\n", "model_names", "=", "[", "]", "\n", "\n", "if", "args", ".", "model_list", ":", "\n", "        ", "args", ".", "model", "=", "''", "\n", "with", "open", "(", "args", ".", "model_list", ")", "as", "f", ":", "\n", "            ", "model_names", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "f", "]", "\n", "", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "", "elif", "args", ".", "model", "==", "'all'", ":", "\n", "# validate all models in a list of names with pretrained checkpoints", "\n", "        ", "args", ".", "pretrained", "=", "True", "\n", "model_names", "=", "list_models", "(", "pretrained", "=", "True", ",", "exclude_filters", "=", "[", "'*in21k'", "]", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "", "elif", "not", "is_model", "(", "args", ".", "model", ")", ":", "\n", "# model name doesn't exist, try as wildcard filter", "\n", "        ", "model_names", "=", "list_models", "(", "args", ".", "model", ")", "\n", "model_cfgs", "=", "[", "(", "n", ",", "None", ")", "for", "n", "in", "model_names", "]", "\n", "\n", "", "if", "len", "(", "model_cfgs", ")", ":", "\n", "        ", "results_file", "=", "args", ".", "results_file", "or", "'./benchmark.csv'", "\n", "_logger", ".", "info", "(", "'Running bulk validation on these pretrained models: {}'", ".", "format", "(", "', '", ".", "join", "(", "model_names", ")", ")", ")", "\n", "results", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "m", ",", "_", "in", "model_cfgs", ":", "\n", "                ", "if", "not", "m", ":", "\n", "                    ", "continue", "\n", "", "args", ".", "model", "=", "m", "\n", "r", "=", "benchmark", "(", "args", ")", "\n", "if", "r", ":", "\n", "                    ", "results", ".", "append", "(", "r", ")", "\n", "", "time", ".", "sleep", "(", "10", ")", "\n", "", "", "except", "KeyboardInterrupt", "as", "e", ":", "\n", "            ", "pass", "\n", "", "sort_key", "=", "'infer_samples_per_sec'", "\n", "if", "'train'", "in", "args", ".", "bench", ":", "\n", "            ", "sort_key", "=", "'train_samples_per_sec'", "\n", "", "elif", "'profile'", "in", "args", ".", "bench", ":", "\n", "            ", "sort_key", "=", "'infer_gmacs'", "\n", "", "results", "=", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "sort_key", "]", ",", "reverse", "=", "True", ")", "\n", "if", "len", "(", "results", ")", ":", "\n", "            ", "write_results", "(", "results_file", ",", "results", ")", "\n", "", "", "else", ":", "\n", "        ", "results", "=", "benchmark", "(", "args", ")", "\n", "", "json_str", "=", "json", ".", "dumps", "(", "results", ",", "indent", "=", "4", ")", "\n", "print", "(", "json_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.benchmark.write_results": [[590, 597], ["open", "csv.DictWriter", "csv.DictWriter.writeheader", "cf.flush", "csv.DictWriter.writerow", "results[].keys"], "function", ["None"], ["", "def", "write_results", "(", "results_file", ",", "results", ")", ":", "\n", "    ", "with", "open", "(", "results_file", ",", "mode", "=", "'w'", ")", "as", "cf", ":", "\n", "        ", "dw", "=", "csv", ".", "DictWriter", "(", "cf", ",", "fieldnames", "=", "results", "[", "0", "]", ".", "keys", "(", ")", ")", "\n", "dw", ".", "writeheader", "(", ")", "\n", "for", "r", "in", "results", ":", "\n", "            ", "dw", ".", "writerow", "(", "r", ")", "\n", "", "cf", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.avg_checkpoints.checkpoint_metric": [[36, 49], ["print", "torch.load", "os.path.isfile", "print"], "function", ["None"], ["def", "checkpoint_metric", "(", "checkpoint_path", ")", ":", "\n", "    ", "if", "not", "checkpoint_path", "or", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "return", "{", "}", "\n", "", "print", "(", "\"=> Extracting metric from checkpoint '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "metric", "=", "None", "\n", "if", "'metric'", "in", "checkpoint", ":", "\n", "        ", "metric", "=", "checkpoint", "[", "'metric'", "]", "\n", "", "elif", "'metrics'", "in", "checkpoint", "and", "'metric_name'", "in", "checkpoint", ":", "\n", "        ", "metrics", "=", "checkpoint", "[", "'metrics'", "]", "\n", "print", "(", "metrics", ")", "\n", "metric", "=", "metrics", "[", "checkpoint", "[", "'metric_name'", "]", "]", "\n", "", "return", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.avg_checkpoints.main": [[51, 118], ["parser.parse_args", "os.path.exists", "glob.glob", "avg_state_dict.items", "torch.finfo", "avg_state_dict.items", "print", "print", "exit", "list", "print", "print", "timm.models.helpers.load_state_dict", "timm.models.helpers.load_state_dict.items", "v.clamp.div_", "v.clamp.clamp", "v.clamp.to", "torch.save", "open", "hashlib.sha256().hexdigest", "parser.parse_args.input.endswith", "parser.parse_args.filter.startswith", "avg_checkpoints.checkpoint_metric", "sorted", "print", "print", "print", "torch.save", "list.append", "v.clamp.clone().to", "v.clamp.to", "hashlib.sha256", "f.read", "v.clamp.clone"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.Image_Pre_Training.avg_checkpoints.checkpoint_metric"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# by default use the EMA weights (if present)", "\n", "args", ".", "use_ema", "=", "not", "args", ".", "no_use_ema", "\n", "# by default sort by checkpoint metric (if present) and avg top n checkpoints", "\n", "args", ".", "sort", "=", "not", "args", ".", "no_sort", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output", ")", ":", "\n", "        ", "print", "(", "\"Error: Output filename ({}) already exists.\"", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "pattern", "=", "args", ".", "input", "\n", "if", "not", "args", ".", "input", ".", "endswith", "(", "os", ".", "path", ".", "sep", ")", "and", "not", "args", ".", "filter", ".", "startswith", "(", "os", ".", "path", ".", "sep", ")", ":", "\n", "        ", "pattern", "+=", "os", ".", "path", ".", "sep", "\n", "", "pattern", "+=", "args", ".", "filter", "\n", "checkpoints", "=", "glob", ".", "glob", "(", "pattern", ",", "recursive", "=", "True", ")", "\n", "\n", "if", "args", ".", "sort", ":", "\n", "        ", "checkpoint_metrics", "=", "[", "]", "\n", "for", "c", "in", "checkpoints", ":", "\n", "            ", "metric", "=", "checkpoint_metric", "(", "c", ")", "\n", "if", "metric", "is", "not", "None", ":", "\n", "                ", "checkpoint_metrics", ".", "append", "(", "(", "metric", ",", "c", ")", ")", "\n", "", "", "checkpoint_metrics", "=", "list", "(", "sorted", "(", "checkpoint_metrics", ")", ")", "\n", "checkpoint_metrics", "=", "checkpoint_metrics", "[", "-", "args", ".", "n", ":", "]", "\n", "print", "(", "\"Selected checkpoints:\"", ")", "\n", "[", "print", "(", "m", ",", "c", ")", "for", "m", ",", "c", "in", "checkpoint_metrics", "]", "\n", "avg_checkpoints", "=", "[", "c", "for", "m", ",", "c", "in", "checkpoint_metrics", "]", "\n", "", "else", ":", "\n", "        ", "avg_checkpoints", "=", "checkpoints", "\n", "print", "(", "\"Selected checkpoints:\"", ")", "\n", "[", "print", "(", "c", ")", "for", "c", "in", "checkpoints", "]", "\n", "\n", "", "avg_state_dict", "=", "{", "}", "\n", "avg_counts", "=", "{", "}", "\n", "for", "c", "in", "avg_checkpoints", ":", "\n", "        ", "new_state_dict", "=", "load_state_dict", "(", "c", ",", "args", ".", "use_ema", ")", "\n", "if", "not", "new_state_dict", ":", "\n", "            ", "print", "(", "\"Error: Checkpoint ({}) doesn't exist\"", ".", "format", "(", "args", ".", "checkpoint", ")", ")", "\n", "continue", "\n", "\n", "", "for", "k", ",", "v", "in", "new_state_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "not", "in", "avg_state_dict", ":", "\n", "                ", "avg_state_dict", "[", "k", "]", "=", "v", ".", "clone", "(", ")", ".", "to", "(", "dtype", "=", "torch", ".", "float64", ")", "\n", "avg_counts", "[", "k", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "avg_state_dict", "[", "k", "]", "+=", "v", ".", "to", "(", "dtype", "=", "torch", ".", "float64", ")", "\n", "avg_counts", "[", "k", "]", "+=", "1", "\n", "\n", "", "", "", "for", "k", ",", "v", "in", "avg_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "v", ".", "div_", "(", "avg_counts", "[", "k", "]", ")", "\n", "\n", "# float32 overflow seems unlikely based on weights seen to date, but who knows", "\n", "", "float32_info", "=", "torch", ".", "finfo", "(", "torch", ".", "float32", ")", "\n", "final_state_dict", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "avg_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "v", "=", "v", ".", "clamp", "(", "float32_info", ".", "min", ",", "float32_info", ".", "max", ")", "\n", "final_state_dict", "[", "k", "]", "=", "v", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "", "try", ":", "\n", "        ", "torch", ".", "save", "(", "final_state_dict", ",", "args", ".", "output", ",", "_use_new_zipfile_serialization", "=", "False", ")", "\n", "", "except", ":", "\n", "        ", "torch", ".", "save", "(", "final_state_dict", ",", "args", ".", "output", ")", "\n", "\n", "", "with", "open", "(", "args", ".", "output", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "sha_hash", "=", "hashlib", ".", "sha256", "(", "f", ".", "read", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "", "print", "(", "\"=> Saved state_dict to '{}, SHA256: {}'\"", ".", "format", "(", "args", ".", "output", ",", "sha_hash", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.__init__": [[22, 62], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ",", "\n", "optimizer", ",", "\n", "args", "=", "None", ",", "\n", "model_ema", "=", "None", ",", "\n", "amp_scaler", "=", "None", ",", "\n", "checkpoint_prefix", "=", "'checkpoint'", ",", "\n", "recovery_prefix", "=", "'recovery'", ",", "\n", "checkpoint_dir", "=", "''", ",", "\n", "recovery_dir", "=", "''", ",", "\n", "decreasing", "=", "False", ",", "\n", "max_history", "=", "10", ",", "\n", "unwrap_fn", "=", "unwrap_model", ")", ":", "\n", "\n", "# objects to save state_dicts of", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "model_ema", "=", "model_ema", "\n", "self", ".", "amp_scaler", "=", "amp_scaler", "\n", "\n", "# state", "\n", "self", ".", "checkpoint_files", "=", "[", "]", "# (filename, metric) tuples in order of decreasing betterness", "\n", "self", ".", "best_epoch", "=", "None", "\n", "self", ".", "best_metric", "=", "None", "\n", "self", ".", "curr_recovery_file", "=", "''", "\n", "self", ".", "last_recovery_file", "=", "''", "\n", "\n", "# config", "\n", "self", ".", "checkpoint_dir", "=", "checkpoint_dir", "\n", "self", ".", "recovery_dir", "=", "recovery_dir", "\n", "self", ".", "save_prefix", "=", "checkpoint_prefix", "\n", "self", ".", "recovery_prefix", "=", "recovery_prefix", "\n", "self", ".", "extension", "=", "'.pth.tar'", "\n", "self", ".", "decreasing", "=", "decreasing", "# a lower metric is better if True", "\n", "self", ".", "cmp", "=", "operator", ".", "lt", "if", "decreasing", "else", "operator", ".", "gt", "# True if lhs better than rhs", "\n", "self", ".", "max_history", "=", "max_history", "\n", "self", ".", "unwrap_fn", "=", "unwrap_fn", "\n", "assert", "self", ".", "max_history", ">=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.save_checkpoint": [[63, 98], ["os.path.join", "os.path.join", "checkpoint_saver.CheckpointSaver._save", "os.path.exists", "os.rename", "os.unlink", "checkpoint_saver.CheckpointSaver.cmp", "os.path.join", "os.link", "checkpoint_saver.CheckpointSaver.checkpoint_files.append", "sorted", "_logger.info", "len", "len", "checkpoint_saver.CheckpointSaver._cleanup_checkpoints", "os.path.join", "os.path.exists", "os.link", "checkpoint_saver.CheckpointSaver.cmp", "os.unlink", "str"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver._save", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver._cleanup_checkpoints"], ["", "def", "save_checkpoint", "(", "self", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "assert", "epoch", ">=", "0", "\n", "tmp_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'tmp'", "+", "self", ".", "extension", ")", "\n", "last_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'last'", "+", "self", ".", "extension", ")", "\n", "self", ".", "_save", "(", "tmp_save_path", ",", "epoch", ",", "metric", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "last_save_path", ")", ":", "\n", "            ", "os", ".", "unlink", "(", "last_save_path", ")", "# required for Windows support.", "\n", "", "os", ".", "rename", "(", "tmp_save_path", ",", "last_save_path", ")", "\n", "worst_file", "=", "self", ".", "checkpoint_files", "[", "-", "1", "]", "if", "self", ".", "checkpoint_files", "else", "None", "\n", "if", "(", "len", "(", "self", ".", "checkpoint_files", ")", "<", "self", ".", "max_history", "\n", "or", "metric", "is", "None", "or", "self", ".", "cmp", "(", "metric", ",", "worst_file", "[", "1", "]", ")", ")", ":", "\n", "            ", "if", "len", "(", "self", ".", "checkpoint_files", ")", ">=", "self", ".", "max_history", ":", "\n", "                ", "self", ".", "_cleanup_checkpoints", "(", "1", ")", "\n", "", "filename", "=", "'-'", ".", "join", "(", "[", "self", ".", "save_prefix", ",", "str", "(", "epoch", ")", "]", ")", "+", "self", ".", "extension", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "filename", ")", "\n", "os", ".", "link", "(", "last_save_path", ",", "save_path", ")", "\n", "self", ".", "checkpoint_files", ".", "append", "(", "(", "save_path", ",", "metric", ")", ")", "\n", "self", ".", "checkpoint_files", "=", "sorted", "(", "\n", "self", ".", "checkpoint_files", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n", "reverse", "=", "not", "self", ".", "decreasing", ")", "# sort in descending order if a lower metric is not better", "\n", "\n", "checkpoints_str", "=", "\"Current checkpoints:\\n\"", "\n", "for", "c", "in", "self", ".", "checkpoint_files", ":", "\n", "                ", "checkpoints_str", "+=", "' {}\\n'", ".", "format", "(", "c", ")", "\n", "", "_logger", ".", "info", "(", "checkpoints_str", ")", "\n", "\n", "if", "metric", "is", "not", "None", "and", "(", "self", ".", "best_metric", "is", "None", "or", "self", ".", "cmp", "(", "metric", ",", "self", ".", "best_metric", ")", ")", ":", "\n", "                ", "self", ".", "best_epoch", "=", "epoch", "\n", "self", ".", "best_metric", "=", "metric", "\n", "best_save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'model_best'", "+", "self", ".", "extension", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "best_save_path", ")", ":", "\n", "                    ", "os", ".", "unlink", "(", "best_save_path", ")", "\n", "", "os", ".", "link", "(", "last_save_path", ",", "best_save_path", ")", "\n", "\n", "", "", "return", "(", "None", ",", "None", ")", "if", "self", ".", "best_metric", "is", "None", "else", "(", "self", ".", "best_metric", ",", "self", ".", "best_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver._save": [[99, 117], ["torch.save", "type().__name__.lower", "model.get_state_dict", "checkpoint_saver.CheckpointSaver.optimizer.state_dict", "checkpoint_saver.CheckpointSaver.amp_scaler.state_dict", "model.get_state_dict", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.get_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.get_state_dict"], ["", "def", "_save", "(", "self", ",", "save_path", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "save_state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'arch'", ":", "type", "(", "self", ".", "model", ")", ".", "__name__", ".", "lower", "(", ")", ",", "\n", "'state_dict'", ":", "get_state_dict", "(", "self", ".", "model", ",", "self", ".", "unwrap_fn", ")", ",", "\n", "'optimizer'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'version'", ":", "2", ",", "# version < 2 increments epoch before save", "\n", "}", "\n", "if", "self", ".", "args", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'arch'", "]", "=", "self", ".", "args", ".", "model", "\n", "save_state", "[", "'args'", "]", "=", "self", ".", "args", "\n", "", "if", "self", ".", "amp_scaler", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "self", ".", "amp_scaler", ".", "state_dict_key", "]", "=", "self", ".", "amp_scaler", ".", "state_dict", "(", ")", "\n", "", "if", "self", ".", "model_ema", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'state_dict_ema'", "]", "=", "get_state_dict", "(", "self", ".", "model_ema", ",", "self", ".", "unwrap_fn", ")", "\n", "", "if", "metric", "is", "not", "None", ":", "\n", "            ", "save_state", "[", "'metric'", "]", "=", "metric", "\n", "", "torch", ".", "save", "(", "save_state", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver._cleanup_checkpoints": [[118, 131], ["min", "len", "len", "_logger.debug", "os.remove", "_logger.error"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove"], ["", "def", "_cleanup_checkpoints", "(", "self", ",", "trim", "=", "0", ")", ":", "\n", "        ", "trim", "=", "min", "(", "len", "(", "self", ".", "checkpoint_files", ")", ",", "trim", ")", "\n", "delete_index", "=", "self", ".", "max_history", "-", "trim", "\n", "if", "delete_index", "<", "0", "or", "len", "(", "self", ".", "checkpoint_files", ")", "<=", "delete_index", ":", "\n", "            ", "return", "\n", "", "to_delete", "=", "self", ".", "checkpoint_files", "[", "delete_index", ":", "]", "\n", "for", "d", "in", "to_delete", ":", "\n", "            ", "try", ":", "\n", "                ", "_logger", ".", "debug", "(", "\"Cleaning checkpoint: {}\"", ".", "format", "(", "d", ")", ")", "\n", "os", ".", "remove", "(", "d", "[", "0", "]", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "_logger", ".", "error", "(", "\"Exception '{}' while deleting checkpoint\"", ".", "format", "(", "e", ")", ")", "\n", "", "", "self", ".", "checkpoint_files", "=", "self", ".", "checkpoint_files", "[", ":", "delete_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.save_recovery": [[132, 145], ["os.path.join", "checkpoint_saver.CheckpointSaver._save", "os.path.exists", "_logger.debug", "os.remove", "str", "str", "_logger.error"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver._save", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove"], ["", "def", "save_recovery", "(", "self", ",", "epoch", ",", "batch_idx", "=", "0", ")", ":", "\n", "        ", "assert", "epoch", ">=", "0", "\n", "filename", "=", "'-'", ".", "join", "(", "[", "self", ".", "recovery_prefix", ",", "str", "(", "epoch", ")", ",", "str", "(", "batch_idx", ")", "]", ")", "+", "self", ".", "extension", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "recovery_dir", ",", "filename", ")", "\n", "self", ".", "_save", "(", "save_path", ",", "epoch", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "last_recovery_file", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "_logger", ".", "debug", "(", "\"Cleaning recovery: {}\"", ".", "format", "(", "self", ".", "last_recovery_file", ")", ")", "\n", "os", ".", "remove", "(", "self", ".", "last_recovery_file", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "_logger", ".", "error", "(", "\"Exception '{}' while removing {}\"", ".", "format", "(", "e", ",", "self", ".", "last_recovery_file", ")", ")", "\n", "", "", "self", ".", "last_recovery_file", "=", "self", ".", "curr_recovery_file", "\n", "self", ".", "curr_recovery_file", "=", "save_path", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.checkpoint_saver.CheckpointSaver.find_recovery": [[146, 151], ["os.path.join", "glob.glob", "sorted", "len"], "methods", ["None"], ["", "def", "find_recovery", "(", "self", ")", ":", "\n", "        ", "recovery_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "recovery_dir", ",", "self", ".", "recovery_prefix", ")", "\n", "files", "=", "glob", ".", "glob", "(", "recovery_path", "+", "'*'", "+", "self", ".", "extension", ")", "\n", "files", "=", "sorted", "(", "files", ")", "\n", "return", "files", "[", "0", "]", "if", "len", "(", "files", ")", "else", "''", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma.__init__": [[37, 50], ["copy.deepcopy", "model_ema.ModelEma.ema.eval", "hasattr", "model_ema.ModelEma.ema.parameters", "model_ema.ModelEma.ema.to", "model_ema.ModelEma._load_checkpoint", "p.requires_grad_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint"], ["def", "__init__", "(", "self", ",", "model", ",", "decay", "=", "0.9999", ",", "device", "=", "''", ",", "resume", "=", "''", ")", ":", "\n", "# make a copy of the model for accumulating moving average of weights", "\n", "        ", "self", ".", "ema", "=", "deepcopy", "(", "model", ")", "\n", "self", ".", "ema", ".", "eval", "(", ")", "\n", "self", ".", "decay", "=", "decay", "\n", "self", ".", "device", "=", "device", "# perform ema on different device from model if set", "\n", "if", "device", ":", "\n", "            ", "self", ".", "ema", ".", "to", "(", "device", "=", "device", ")", "\n", "", "self", ".", "ema_has_module", "=", "hasattr", "(", "self", ".", "ema", ",", "'module'", ")", "\n", "if", "resume", ":", "\n", "            ", "self", ".", "_load_checkpoint", "(", "resume", ")", "\n", "", "for", "p", "in", "self", ".", "ema", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad_", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint": [[51, 67], ["torch.load", "torch.load", "torch.load", "torch.load", "isinstance", "collections.OrderedDict", "checkpoint[].items", "model_ema.ModelEma.ema.load_state_dict", "_logger.info", "_logger.warning", "k.startswith"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "", "def", "_load_checkpoint", "(", "self", ",", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "assert", "isinstance", "(", "checkpoint", ",", "dict", ")", "\n", "if", "'state_dict_ema'", "in", "checkpoint", ":", "\n", "            ", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'state_dict_ema'", "]", ".", "items", "(", ")", ":", "\n", "# ema model may have been wrapped by DataParallel, and need module prefix", "\n", "                ", "if", "self", ".", "ema_has_module", ":", "\n", "                    ", "name", "=", "'module.'", "+", "k", "if", "not", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "", "else", ":", "\n", "                    ", "name", "=", "k", "\n", "", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "self", ".", "ema", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "_logger", ".", "info", "(", "\"Loaded state_dict_ema\"", ")", "\n", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Failed to find state_dict_ema, starting from loaded model weights\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma.update": [[68, 80], ["hasattr", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.state_dict", "model_ema.ModelEma.ema.state_dict().items", "msd[].detach", "ema_v.copy_", "model_ema.ModelEma.ema.state_dict", "model_v.to.to.to"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "update", "(", "self", ",", "model", ")", ":", "\n", "# correct a mismatch in state dict keys", "\n", "        ", "needs_module", "=", "hasattr", "(", "model", ",", "'module'", ")", "and", "not", "self", ".", "ema_has_module", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "msd", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", ",", "ema_v", "in", "self", ".", "ema", ".", "state_dict", "(", ")", ".", "items", "(", ")", ":", "\n", "                ", "if", "needs_module", ":", "\n", "                    ", "k", "=", "'module.'", "+", "k", "\n", "", "model_v", "=", "msd", "[", "k", "]", ".", "detach", "(", ")", "\n", "if", "self", ".", "device", ":", "\n", "                    ", "model_v", "=", "model_v", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "ema_v", ".", "copy_", "(", "ema_v", "*", "self", ".", "decay", "+", "(", "1.", "-", "self", ".", "decay", ")", "*", "model_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.__init__": [[105, 114], ["torch.Module.__init__", "copy.deepcopy", "model_ema.ModelEmaV2.module.eval", "model_ema.ModelEmaV2.module.to"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "model", ",", "decay", "=", "0.9999", ",", "device", "=", "None", ")", ":", "\n", "        ", "super", "(", "ModelEmaV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# make a copy of the model for accumulating moving average of weights", "\n", "self", ".", "module", "=", "deepcopy", "(", "model", ")", "\n", "self", ".", "module", ".", "eval", "(", ")", "\n", "self", ".", "decay", "=", "decay", "\n", "self", ".", "device", "=", "device", "# perform ema on different device from model if set", "\n", "if", "self", ".", "device", "is", "not", "None", ":", "\n", "            ", "self", ".", "module", ".", "to", "(", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2._update": [[115, 121], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model_ema.ModelEmaV2.module.state_dict().values", "model.state_dict().values", "ema_v.copy_", "model_v.to.to.to", "update_fn", "model_ema.ModelEmaV2.module.state_dict", "model.state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "_update", "(", "self", ",", "model", ",", "update_fn", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "ema_v", ",", "model_v", "in", "zip", "(", "self", ".", "module", ".", "state_dict", "(", ")", ".", "values", "(", ")", ",", "model", ".", "state_dict", "(", ")", ".", "values", "(", ")", ")", ":", "\n", "                ", "if", "self", ".", "device", "is", "not", "None", ":", "\n", "                    ", "model_v", "=", "model_v", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "", "ema_v", ".", "copy_", "(", "update_fn", "(", "ema_v", ",", "model_v", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.update": [[122, 124], ["model_ema.ModelEmaV2._update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2._update"], ["", "", "", "def", "update", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "_update", "(", "model", ",", "update_fn", "=", "lambda", "e", ",", "m", ":", "self", ".", "decay", "*", "e", "+", "(", "1.", "-", "self", ".", "decay", ")", "*", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set": [[125, 127], ["model_ema.ModelEmaV2._update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2._update"], ["", "def", "set", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "_update", "(", "model", ",", "update_fn", "=", "lambda", "e", ",", "m", ":", "m", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.summary.get_outdir": [[13, 27], ["os.path.join", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "str", "str"], "function", ["None"], ["", "def", "get_outdir", "(", "path", ",", "*", "paths", ",", "inc", "=", "False", ")", ":", "\n", "    ", "outdir", "=", "os", ".", "path", ".", "join", "(", "path", ",", "*", "paths", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "outdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "outdir", ")", "\n", "", "elif", "inc", ":", "\n", "        ", "count", "=", "1", "\n", "outdir_inc", "=", "outdir", "+", "'-'", "+", "str", "(", "count", ")", "\n", "while", "os", ".", "path", ".", "exists", "(", "outdir_inc", ")", ":", "\n", "            ", "count", "=", "count", "+", "1", "\n", "outdir_inc", "=", "outdir", "+", "'-'", "+", "str", "(", "count", ")", "\n", "assert", "count", "<", "100", "\n", "", "outdir", "=", "outdir_inc", "\n", "os", ".", "makedirs", "(", "outdir", ")", "\n", "", "return", "outdir", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.summary.update_summary": [[29, 40], ["collections.OrderedDict", "collections.OrderedDict.update", "collections.OrderedDict.update", "wandb.log", "open", "csv.DictWriter", "csv.DictWriter.writerow", "csv.DictWriter.writeheader", "train_metrics.items", "eval_metrics.items", "collections.OrderedDict.keys"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "update_summary", "(", "epoch", ",", "train_metrics", ",", "eval_metrics", ",", "filename", ",", "write_header", "=", "False", ",", "log_wandb", "=", "False", ",", ")", ":", "\n", "    ", "rowd", "=", "OrderedDict", "(", "epoch", "=", "epoch", ")", "\n", "rowd", ".", "update", "(", "[", "(", "'train_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "train_metrics", ".", "items", "(", ")", "]", ")", "\n", "rowd", ".", "update", "(", "[", "(", "'eval_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "eval_metrics", ".", "items", "(", ")", "]", ")", "\n", "if", "log_wandb", ":", "\n", "        ", "wandb", ".", "log", "(", "rowd", ",", "step", "=", "epoch", ")", "\n", "", "with", "open", "(", "filename", ",", "mode", "=", "'a'", ")", "as", "cf", ":", "\n", "        ", "dw", "=", "csv", ".", "DictWriter", "(", "cf", ",", "fieldnames", "=", "rowd", ".", "keys", "(", ")", ")", "\n", "if", "write_header", ":", "# first iteration (epoch == 1 can't be used)", "\n", "            ", "dw", ".", "writeheader", "(", ")", "\n", "", "dw", ".", "writerow", "(", "rowd", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.agc.unitwise_norm": [[21, 28], ["x.norm", "x.norm", "tuple", "range"], "function", ["None"], ["def", "unitwise_norm", "(", "x", ",", "norm_type", "=", "2.0", ")", ":", "\n", "    ", "if", "x", ".", "ndim", "<=", "1", ":", "\n", "        ", "return", "x", ".", "norm", "(", "norm_type", ")", "\n", "", "else", ":", "\n", "# works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor", "\n", "# might need special cases for other weights (possibly MHA) where this may not be true", "\n", "        ", "return", "x", ".", "norm", "(", "norm_type", ",", "dim", "=", "tuple", "(", "range", "(", "1", ",", "x", ".", "ndim", ")", ")", ",", "keepdim", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.agc.adaptive_clip_grad": [[30, 43], ["isinstance", "p.detach", "p.grad.detach", "unitwise_norm().clamp_().mul_", "agc.unitwise_norm", "torch.where", "p.grad.detach().copy_", "unitwise_norm().clamp_", "unitwise_norm.clamp", "p.grad.detach", "agc.unitwise_norm"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.agc.unitwise_norm", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.agc.unitwise_norm"], ["", "", "def", "adaptive_clip_grad", "(", "parameters", ",", "clip_factor", "=", "0.01", ",", "eps", "=", "1e-3", ",", "norm_type", "=", "2.0", ")", ":", "\n", "    ", "if", "isinstance", "(", "parameters", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "parameters", "=", "[", "parameters", "]", "\n", "", "for", "p", "in", "parameters", ":", "\n", "        ", "if", "p", ".", "grad", "is", "None", ":", "\n", "            ", "continue", "\n", "", "p_data", "=", "p", ".", "detach", "(", ")", "\n", "g_data", "=", "p", ".", "grad", ".", "detach", "(", ")", "\n", "max_norm", "=", "unitwise_norm", "(", "p_data", ",", "norm_type", "=", "norm_type", ")", ".", "clamp_", "(", "min", "=", "eps", ")", ".", "mul_", "(", "clip_factor", ")", "\n", "grad_norm", "=", "unitwise_norm", "(", "g_data", ",", "norm_type", "=", "norm_type", ")", "\n", "clipped_grad", "=", "g_data", "*", "(", "max_norm", "/", "grad_norm", ".", "clamp", "(", "min", "=", "1e-6", ")", ")", "\n", "new_grads", "=", "torch", ".", "where", "(", "grad_norm", "<", "max_norm", ",", "g_data", ",", "clipped_grad", ")", "\n", "p", ".", "grad", ".", "detach", "(", ")", ".", "copy_", "(", "new_grads", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.reduce_tensor": [[11, 16], ["tensor.clone", "torch.distributed.all_reduce"], "function", ["None"], ["def", "reduce_tensor", "(", "tensor", ",", "n", ")", ":", "\n", "    ", "rt", "=", "tensor", ".", "clone", "(", ")", "\n", "dist", ".", "all_reduce", "(", "rt", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "rt", "/=", "n", "\n", "return", "rt", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.distributed.distribute_bn": [[18, 29], ["model.unwrap_model().named_buffers", "model.unwrap_model", "torch.distributed.all_reduce", "float", "torch.distributed.broadcast"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unwrap_model"], ["", "def", "distribute_bn", "(", "model", ",", "world_size", ",", "reduce", "=", "False", ")", ":", "\n", "# ensure every node has the same running bn stats", "\n", "    ", "for", "bn_name", ",", "bn_buf", "in", "unwrap_model", "(", "model", ")", ".", "named_buffers", "(", "recurse", "=", "True", ")", ":", "\n", "        ", "if", "(", "'running_mean'", "in", "bn_name", ")", "or", "(", "'running_var'", "in", "bn_name", ")", ":", "\n", "            ", "if", "reduce", ":", "\n", "# average bn stats across whole group", "\n", "                ", "torch", ".", "distributed", ".", "all_reduce", "(", "bn_buf", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "bn_buf", "/=", "float", "(", "world_size", ")", "\n", "", "else", ":", "\n", "# broadcast bn stats from rank 0 to whole group", "\n", "                ", "torch", ".", "distributed", ".", "broadcast", "(", "bn_buf", ",", "0", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.__init__": [[9, 11], ["metrics.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.TarState.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.reset": [[12, 17], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update": [[18, 23], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.accuracy": [[25, 33], ["min", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "max", "target.reshape().expand_as", "output.size", "target.reshape", "correct[].reshape().float().sum", "correct[].reshape().float", "correct[].reshape", "min"], "function", ["None"], ["", "", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "maxk", "=", "min", "(", "max", "(", "topk", ")", ",", "output", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "return", "[", "correct", "[", ":", "min", "(", "k", ",", "maxk", ")", "]", ".", "reshape", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "*", "100.", "/", "batch_size", "for", "k", "in", "topk", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.clip_grad.dispatch_clip_grad": [[6, 23], ["torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_value_", "timm.utils.agc.adaptive_clip_grad"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.agc.adaptive_clip_grad"], ["def", "dispatch_clip_grad", "(", "parameters", ",", "value", ":", "float", ",", "mode", ":", "str", "=", "'norm'", ",", "norm_type", ":", "float", "=", "2.0", ")", ":", "\n", "    ", "\"\"\" Dispatch to gradient clipping method\n\n    Args:\n        parameters (Iterable): model parameters to clip\n        value (float): clipping value/factor/norm, mode dependant\n        mode (str): clipping mode, one of 'norm', 'value', 'agc'\n        norm_type (float): p-norm, default 2.0\n    \"\"\"", "\n", "if", "mode", "==", "'norm'", ":", "\n", "        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "parameters", ",", "value", ",", "norm_type", "=", "norm_type", ")", "\n", "", "elif", "mode", "==", "'value'", ":", "\n", "        ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_value_", "(", "parameters", ",", "value", ")", "\n", "", "elif", "mode", "==", "'agc'", ":", "\n", "        ", "adaptive_clip_grad", "(", "parameters", ",", "value", ",", "norm_type", "=", "norm_type", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "f\"Unknown clip mode ({mode}).\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.random.random_seed": [[6, 10], ["torch.manual_seed", "numpy.random.seed", "random.seed"], "function", ["None"], ["def", "random_seed", "(", "seed", "=", "42", ",", "rank", "=", "0", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", "+", "rank", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "random", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.__init__": [[60, 70], ["dict", "zip", "len", "len", "ValueError", "model.ActivationStatsHook.register_hook"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook"], ["def", "__init__", "(", "self", ",", "model", ",", "hook_fn_locs", ",", "hook_fns", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "hook_fn_locs", "=", "hook_fn_locs", "\n", "self", ".", "hook_fns", "=", "hook_fns", "\n", "if", "len", "(", "hook_fn_locs", ")", "!=", "len", "(", "hook_fns", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Please provide `hook_fns` for each `hook_fn_locs`, \\\n                their lengths are different.\"", ")", "\n", "", "self", ".", "stats", "=", "dict", "(", "(", "hook_fn", ".", "__name__", ",", "[", "]", ")", "for", "hook_fn", "in", "hook_fns", ")", "\n", "for", "hook_fn_loc", ",", "hook_fn", "in", "zip", "(", "hook_fn_locs", ",", "hook_fns", ")", ":", "\n", "            ", "self", ".", "register_hook", "(", "hook_fn_loc", ",", "hook_fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook._create_hook": [[71, 77], ["hook_fn", "model.ActivationStatsHook.stats[].append"], "methods", ["None"], ["", "", "def", "_create_hook", "(", "self", ",", "hook_fn", ")", ":", "\n", "        ", "def", "append_activation_stats", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "            ", "out", "=", "hook_fn", "(", "module", ",", "input", ",", "output", ")", "\n", "self", ".", "stats", "[", "hook_fn", ".", "__name__", "]", ".", "append", "(", "out", ")", "\n", "\n", "", "return", "append_activation_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook": [[78, 83], ["model.ActivationStatsHook.model.named_modules", "module.register_forward_hook", "fnmatch.fnmatch", "model.ActivationStatsHook._create_hook"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook._create_hook"], ["", "def", "register_hook", "(", "self", ",", "hook_fn_loc", ",", "hook_fn", ")", ":", "\n", "        ", "for", "name", ",", "module", "in", "self", ".", "model", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "not", "fnmatch", ".", "fnmatch", "(", "name", ",", "hook_fn_loc", ")", ":", "\n", "                ", "continue", "\n", "", "module", ".", "register_forward_hook", "(", "self", ".", "_create_hook", "(", "hook_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unwrap_model": [[13, 18], ["isinstance", "model.unwrap_model", "hasattr"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unwrap_model"], ["def", "unwrap_model", "(", "model", ")", ":", "\n", "    ", "if", "isinstance", "(", "model", ",", "ModelEma", ")", ":", "\n", "        ", "return", "unwrap_model", "(", "model", ".", "ema", ")", "\n", "", "else", ":", "\n", "        ", "return", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.get_state_dict": [[20, 22], ["unwrap_fn().state_dict", "unwrap_fn"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "", "def", "get_state_dict", "(", "model", ",", "unwrap_fn", "=", "unwrap_model", ")", ":", "\n", "    ", "return", "unwrap_fn", "(", "model", ")", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.avg_sq_ch_mean": [[24, 28], ["torch.mean().item", "torch.mean", "output.mean"], "function", ["None"], ["", "def", "avg_sq_ch_mean", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel square mean of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "mean", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", "**", "2", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.avg_ch_var": [[30, 34], ["torch.mean().item", "torch.mean", "output.var"], "function", ["None"], ["", "def", "avg_ch_var", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel variance of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "var", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.avg_ch_var_residual": [[36, 40], ["torch.mean().item", "torch.mean", "output.var"], "function", ["None"], ["", "def", "avg_ch_var_residual", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "    ", "\"\"\" calculate average channel variance of output activations\n    \"\"\"", "\n", "return", "torch", ".", "mean", "(", "output", ".", "var", "(", "axis", "=", "[", "0", ",", "2", ",", "3", "]", ")", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.extract_spp_stats": [[85, 101], ["torch.normal", "model.ActivationStatsHook", "model"], "function", ["None"], ["", "", "", "def", "extract_spp_stats", "(", "\n", "model", ",", "\n", "hook_fn_locs", ",", "\n", "hook_fns", ",", "\n", "input_shape", "=", "[", "8", ",", "3", ",", "224", ",", "224", "]", ")", ":", "\n", "    ", "\"\"\"Extract average square channel mean and variance of activations during \n    forward pass to plot Signal Propogation Plots (SPP).\n    \n    Paper: https://arxiv.org/abs/2101.08692\n\n    Example Usage: https://gist.github.com/amaarora/6e56942fcb46e67ba203f3009b30d950\n    \"\"\"", "\n", "x", "=", "torch", ".", "normal", "(", "0.", ",", "1.", ",", "input_shape", ")", "\n", "hook", "=", "ActivationStatsHook", "(", "model", ",", "hook_fn_locs", "=", "hook_fn_locs", ",", "hook_fns", "=", "hook_fns", ")", "\n", "_", "=", "model", "(", "x", ")", "\n", "return", "hook", ".", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.freeze_batch_norm_2d": [[103, 134], ["isinstance", "torchvision.ops.misc.FrozenBatchNorm2d", "module.named_children", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "model.freeze_batch_norm_2d", "torchvision.ops.misc.FrozenBatchNorm2d.add_module", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.freeze_batch_norm_2d"], ["", "def", "freeze_batch_norm_2d", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"", "\n", "res", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "        ", "res", "=", "FrozenBatchNorm2d", "(", "module", ".", "num_features", ")", "\n", "res", ".", "num_features", "=", "module", ".", "num_features", "\n", "res", ".", "affine", "=", "module", ".", "affine", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "res", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "res", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "res", ".", "running_mean", ".", "data", "=", "module", ".", "running_mean", ".", "data", "\n", "res", ".", "running_var", ".", "data", "=", "module", ".", "running_var", ".", "data", "\n", "res", ".", "eps", "=", "module", ".", "eps", "\n", "", "else", ":", "\n", "        ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "            ", "new_child", "=", "freeze_batch_norm_2d", "(", "child", ")", "\n", "if", "new_child", "is", "not", "child", ":", "\n", "                ", "res", ".", "add_module", "(", "name", ",", "new_child", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unfreeze_batch_norm_2d": [[136, 165], ["isinstance", "torch.nn.BatchNorm2d", "module.named_children", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "model.unfreeze_batch_norm_2d", "torch.nn.BatchNorm2d.add_module", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unfreeze_batch_norm_2d"], ["", "def", "unfreeze_batch_norm_2d", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Converts all `FrozenBatchNorm2d` layers of provided module into `BatchNorm2d`. If `module` is itself and instance\n    of `FrozenBatchNorm2d`, it is converted into `BatchNorm2d` and returned. Otherwise, the module is walked\n    recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"", "\n", "res", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "FrozenBatchNorm2d", ")", ":", "\n", "        ", "res", "=", "torch", ".", "nn", ".", "BatchNorm2d", "(", "module", ".", "num_features", ")", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "res", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "res", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "res", ".", "running_mean", ".", "data", "=", "module", ".", "running_mean", ".", "data", "\n", "res", ".", "running_var", ".", "data", "=", "module", ".", "running_var", ".", "data", "\n", "res", ".", "eps", "=", "module", ".", "eps", "\n", "", "else", ":", "\n", "        ", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "            ", "new_child", "=", "unfreeze_batch_norm_2d", "(", "child", ")", "\n", "if", "new_child", "is", "not", "child", ":", "\n", "                ", "res", ".", "add_module", "(", "name", ",", "new_child", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model._freeze_unfreeze": [[167, 224], ["isinstance", "isinstance", "zip", "AssertionError", "root_module.get_submodule", "len", "list", "m.parameters", "zip", "name.rsplit", "model.freeze_batch_norm_2d", "isinstance", "model.unfreeze_batch_norm_2d", "isinstance", "root_module.named_children", "len", "module.get_submodule().add_module", "module.add_module", "model._freeze_unfreeze._add_submodule"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.freeze_batch_norm_2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unfreeze_batch_norm_2d"], ["", "def", "_freeze_unfreeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ",", "mode", "=", "'freeze'", ")", ":", "\n", "    ", "\"\"\"\n    Freeze or unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is\n    done in place.\n    Args:\n        root_module (nn.Module, optional): Root module relative to which the `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be (un)frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be (un)frozen. Defaults to []\n        include_bn_running_stats (bool): Whether to also (un)freeze the running statistics of batch norm 2d layers.\n            Defaults to `True`.\n        mode (bool): Whether to freeze (\"freeze\") or unfreeze (\"unfreeze\"). Defaults to `\"freeze\"`.\n    \"\"\"", "\n", "assert", "mode", "in", "[", "\"freeze\"", ",", "\"unfreeze\"", "]", ",", "'`mode` must be one of \"freeze\" or \"unfreeze\"'", "\n", "\n", "if", "isinstance", "(", "root_module", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "# Raise assertion here because we can't convert it in place", "\n", "        ", "raise", "AssertionError", "(", "\n", "\"You have provided a batch norm layer as the `root module`. Please use \"", "\n", "\"`timm.utils.model.freeze_batch_norm_2d` or `timm.utils.model.unfreeze_batch_norm_2d` instead.\"", ")", "\n", "\n", "", "if", "isinstance", "(", "submodules", ",", "str", ")", ":", "\n", "        ", "submodules", "=", "[", "submodules", "]", "\n", "\n", "", "named_modules", "=", "submodules", "\n", "submodules", "=", "[", "root_module", ".", "get_submodule", "(", "m", ")", "for", "m", "in", "submodules", "]", "\n", "\n", "if", "not", "len", "(", "submodules", ")", ":", "\n", "        ", "named_modules", ",", "submodules", "=", "list", "(", "zip", "(", "*", "root_module", ".", "named_children", "(", ")", ")", ")", "\n", "\n", "", "for", "n", ",", "m", "in", "zip", "(", "named_modules", ",", "submodules", ")", ":", "\n", "# (Un)freeze parameters", "\n", "        ", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "if", "mode", "==", "'freeze'", "else", "True", "\n", "", "if", "include_bn_running_stats", ":", "\n", "# Helper to add submodule specified as a named_module", "\n", "            ", "def", "_add_submodule", "(", "module", ",", "name", ",", "submodule", ")", ":", "\n", "                ", "split", "=", "name", ".", "rsplit", "(", "'.'", ",", "1", ")", "\n", "if", "len", "(", "split", ")", ">", "1", ":", "\n", "                    ", "module", ".", "get_submodule", "(", "split", "[", "0", "]", ")", ".", "add_module", "(", "split", "[", "1", "]", ",", "submodule", ")", "\n", "", "else", ":", "\n", "                    ", "module", ".", "add_module", "(", "name", ",", "submodule", ")", "\n", "\n", "# Freeze batch norm", "\n", "", "", "if", "mode", "==", "'freeze'", ":", "\n", "                ", "res", "=", "freeze_batch_norm_2d", "(", "m", ")", "\n", "# It's possible that `m` is a type of BatchNorm in itself, in which case `unfreeze_batch_norm_2d` won't", "\n", "# convert it in place, but will return the converted result. In this case `res` holds the converted", "\n", "# result and we may try to re-assign the named module", "\n", "if", "isinstance", "(", "m", ",", "(", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "BatchNorm2d", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "SyncBatchNorm", ")", ")", ":", "\n", "                    ", "_add_submodule", "(", "root_module", ",", "n", ",", "res", ")", "\n", "# Unfreeze batch norm", "\n", "", "", "else", ":", "\n", "                ", "res", "=", "unfreeze_batch_norm_2d", "(", "m", ")", "\n", "# Ditto. See note above in mode == 'freeze' branch", "\n", "if", "isinstance", "(", "m", ",", "FrozenBatchNorm2d", ")", ":", "\n", "                    ", "_add_submodule", "(", "root_module", ",", "n", ",", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.freeze": [[226, 258], ["model._freeze_unfreeze"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model._freeze_unfreeze"], ["", "", "", "", "", "def", "freeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Freeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of modules for which the parameters will be frozen. They are to be provided as\n            named modules relative to the root module (accessible via `root_module.named_modules()`). An empty list\n            means that the whole root module will be frozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also freeze the running statistics of `BatchNorm2d` and\n            `SyncBatchNorm` layers. These will be converted to `FrozenBatchNorm2d` in place. Hint: During fine tuning,\n            it's good practice to freeze batch norm stats. And note that these are different to the affine parameters\n            which are just normal PyTorch parameters. Defaults to `True`.\n\n    Hint: If you want to freeze batch norm ONLY, use `timm.utils.model.freeze_batch_norm_2d`.\n\n    Examples::\n\n        >>> model = timm.create_model('resnet18')\n        >>> # Freeze up to and including layer2\n        >>> submodules = [n for n, _ in model.named_children()]\n        >>> print(submodules)\n        ['conv1', 'bn1', 'act1', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'global_pool', 'fc']\n        >>> freeze(model, submodules[:submodules.index('layer2') + 1])\n        >>> # Check for yourself that it works as expected\n        >>> print(model.layer2[0].conv1.weight.requires_grad)\n        False\n        >>> print(model.layer3[0].conv1.weight.requires_grad)\n        True\n        >>> # Unfreeze\n        >>> unfreeze(model)\n    \"\"\"", "\n", "_freeze_unfreeze", "(", "root_module", ",", "submodules", ",", "include_bn_running_stats", "=", "include_bn_running_stats", ",", "mode", "=", "\"freeze\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.unfreeze": [[260, 274], ["model._freeze_unfreeze"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model._freeze_unfreeze"], ["", "def", "unfreeze", "(", "root_module", ",", "submodules", "=", "[", "]", ",", "include_bn_running_stats", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Unfreeze parameters of the specified modules and those of all their hierarchical descendants. This is done in place.\n    Args:\n        root_module (nn.Module): Root module relative to which `submodules` are referenced.\n        submodules (list[str]): List of submodules for which the parameters will be (un)frozen. They are to be provided\n            as named modules relative to the root module (accessible via `root_module.named_modules()`). An empty\n            list means that the whole root module will be unfrozen. Defaults to `[]`.\n        include_bn_running_stats (bool): Whether to also unfreeze the running statistics of `FrozenBatchNorm2d` layers.\n            These will be converted to `BatchNorm2d` in place. Defaults to `True`.\n\n    See example in docstring for `freeze`.\n    \"\"\"", "\n", "_freeze_unfreeze", "(", "root_module", ",", "submodules", ",", "include_bn_running_stats", "=", "include_bn_running_stats", ",", "mode", "=", "\"unfreeze\"", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.natural_key": [[8, 11], ["s.isdigit", "int", "re.split", "string_.lower"], "function", ["None"], ["def", "natural_key", "(", "string_", ")", ":", "\n", "    ", "\"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"", "\n", "return", "[", "int", "(", "s", ")", "if", "s", ".", "isdigit", "(", ")", "else", "s", "for", "s", "in", "re", ".", "split", "(", "r'(\\d+)'", ",", "string_", ".", "lower", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.add_bool_arg": [[13, 19], ["name.replace", "parser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "parser.set_defaults"], "function", ["None"], ["", "def", "add_bool_arg", "(", "parser", ",", "name", ",", "default", "=", "False", ",", "help", "=", "''", ")", ":", "\n", "    ", "dest_name", "=", "name", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "group", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "group", ".", "add_argument", "(", "'--'", "+", "name", ",", "dest", "=", "dest_name", ",", "action", "=", "'store_true'", ",", "help", "=", "help", ")", "\n", "group", ".", "add_argument", "(", "'--no-'", "+", "name", ",", "dest", "=", "dest_name", ",", "action", "=", "'store_false'", ",", "help", "=", "help", ")", "\n", "parser", ".", "set_defaults", "(", "**", "{", "dest_name", ":", "default", "}", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.__init__": [[10, 12], ["logging.Formatter.__init__", "logging.Formatter.__init__", "logging.Formatter.__init__", "logging.Formatter.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fmt", "=", "'%(levelname)s: %(message)s'", ")", ":", "\n", "        ", "logging", ".", "Formatter", ".", "__init__", "(", "self", ",", "fmt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format": [[13, 17], ["logging.Formatter.format", "logging.Formatter.format", "logging.Formatter.format", "logging.Formatter.format", "str", "record.getMessage"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format"], ["", "def", "format", "(", "self", ",", "record", ")", ":", "\n", "        ", "if", "record", ".", "levelno", "==", "logging", ".", "INFO", ":", "\n", "            ", "return", "str", "(", "record", ".", "getMessage", "(", ")", ")", "\n", "", "return", "logging", ".", "Formatter", ".", "format", "(", "self", ",", "record", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.setup_default_logging": [[19, 29], ["logging.StreamHandler", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logging.root.addHandler", "logging.root.addHandler", "logging.root.setLevel", "logging.root.setLevel", "log.FormatterNoInfo", "logging.handlers.RotatingFileHandler", "logging.handlers.RotatingFileHandler", "logging.Formatter", "logging.Formatter", "logging.handlers.RotatingFileHandler.setFormatter", "logging.root.addHandler", "logging.root.addHandler"], "function", ["None"], ["", "", "def", "setup_default_logging", "(", "default_level", "=", "logging", ".", "INFO", ",", "log_path", "=", "''", ")", ":", "\n", "    ", "console_handler", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "console_handler", ".", "setFormatter", "(", "FormatterNoInfo", "(", ")", ")", "\n", "logging", ".", "root", ".", "addHandler", "(", "console_handler", ")", "\n", "logging", ".", "root", ".", "setLevel", "(", "default_level", ")", "\n", "if", "log_path", ":", "\n", "        ", "file_handler", "=", "logging", ".", "handlers", ".", "RotatingFileHandler", "(", "log_path", ",", "maxBytes", "=", "(", "1024", "**", "2", "*", "2", ")", ",", "backupCount", "=", "3", ")", "\n", "file_formatter", "=", "logging", ".", "Formatter", "(", "\"%(asctime)s - %(name)20s: [%(levelname)8s] - %(message)s\"", ")", "\n", "file_handler", ".", "setFormatter", "(", "file_formatter", ")", "\n", "logging", ".", "root", ".", "addHandler", "(", "file_handler", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.jit.set_jit_legacy": [[8, 18], ["hasattr", "torch._C._jit_set_profiling_executor", "torch._C._jit_set_profiling_mode", "torch._C._jit_override_can_fuse_on_gpu"], "function", ["None"], ["def", "set_jit_legacy", "(", ")", ":", "\n", "    ", "\"\"\" Set JIT executor to legacy w/ support for op fusion\n    This is hopefully a temporary need in 1.5/1.5.1/1.6 to restore performance due to changes\n    in the JIT exectutor. These API are not supported so could change.\n    \"\"\"", "\n", "#", "\n", "assert", "hasattr", "(", "torch", ".", "_C", ",", "'_jit_set_profiling_executor'", ")", ",", "\"Old JIT behavior doesn't exist!\"", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_executor", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_set_profiling_mode", "(", "False", ")", "\n", "torch", ".", "_C", ".", "_jit_override_can_fuse_on_gpu", "(", "True", ")", "\n", "#torch._C._jit_set_texpr_fuser_enabled(True)", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.ApexScaler.__call__": [[20, 26], ["optimizer.step", "amp.scale_loss", "scaled_loss.backward", "clip_grad.dispatch_clip_grad", "amp.master_params"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.clip_grad.dispatch_clip_grad"], ["def", "__call__", "(", "self", ",", "loss", ",", "optimizer", ",", "clip_grad", "=", "None", ",", "clip_mode", "=", "'norm'", ",", "parameters", "=", "None", ",", "create_graph", "=", "False", ")", ":", "\n", "        ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "            ", "scaled_loss", ".", "backward", "(", "create_graph", "=", "create_graph", ")", "\n", "", "if", "clip_grad", "is", "not", "None", ":", "\n", "            ", "dispatch_clip_grad", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "clip_grad", ",", "mode", "=", "clip_mode", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.ApexScaler.state_dict": [[27, 30], ["amp.state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "if", "'state_dict'", "in", "amp", ".", "__dict__", ":", "\n", "            ", "return", "amp", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.ApexScaler.load_state_dict": [[31, 34], ["amp.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "if", "'load_state_dict'", "in", "amp", ".", "__dict__", ":", "\n", "            ", "amp", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.NativeScaler.__init__": [[39, 41], ["torch.cuda.amp.GradScaler"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_scaler", "=", "torch", ".", "cuda", ".", "amp", ".", "GradScaler", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.NativeScaler.__call__": [[42, 50], ["cuda.NativeScaler._scaler.scale().backward", "cuda.NativeScaler._scaler.step", "cuda.NativeScaler._scaler.update", "cuda.NativeScaler._scaler.unscale_", "clip_grad.dispatch_clip_grad", "cuda.NativeScaler._scaler.scale"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.clip_grad.dispatch_clip_grad"], ["", "def", "__call__", "(", "self", ",", "loss", ",", "optimizer", ",", "clip_grad", "=", "None", ",", "clip_mode", "=", "'norm'", ",", "parameters", "=", "None", ",", "create_graph", "=", "False", ")", ":", "\n", "        ", "self", ".", "_scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", "create_graph", "=", "create_graph", ")", "\n", "if", "clip_grad", "is", "not", "None", ":", "\n", "            ", "assert", "parameters", "is", "not", "None", "\n", "self", ".", "_scaler", ".", "unscale_", "(", "optimizer", ")", "# unscale the gradients of optimizer's assigned params in-place", "\n", "dispatch_clip_grad", "(", "parameters", ",", "clip_grad", ",", "mode", "=", "clip_mode", ")", "\n", "", "self", ".", "_scaler", ".", "step", "(", "optimizer", ")", "\n", "self", ".", "_scaler", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.NativeScaler.state_dict": [[51, 53], ["cuda.NativeScaler._scaler.state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_scaler", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.cuda.NativeScaler.load_state_dict": [[54, 56], ["cuda.NativeScaler._scaler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "_scaler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.decorators.import_module_error_func": [[5, 20], ["ImportError"], "function", ["None"], ["def", "import_module_error_func", "(", "module_name", ")", ":", "\n", "    ", "\"\"\"When a function is imported incorrectly due to a missing module, raise\n    an import error when the function is called.\"\"\"", "\n", "\n", "def", "decorate", "(", "func", ")", ":", "\n", "\n", "        ", "def", "new_func", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "f'Please install {module_name} to use {func.__name__}. '", "\n", "'For OpenMMLAB codebases, you may need to install mmcv-full '", "\n", "'first before you install the particular codebase. '", ")", "\n", "\n", "", "return", "new_func", "\n", "\n", "", "return", "decorate", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.decorators.import_module_error_class": [[22, 38], ["types.MethodType", "ImportError"], "function", ["None"], ["", "def", "import_module_error_class", "(", "module_name", ")", ":", "\n", "    ", "\"\"\"When a class is imported incorrectly due to a missing module, raise an\n    import error when the class is instantiated.\"\"\"", "\n", "\n", "def", "decorate", "(", "cls", ")", ":", "\n", "\n", "        ", "def", "import_error_init", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "f'Please install {module_name} to use {cls.__name__}. '", "\n", "'For OpenMMLAB codebases, you may need to install mmcv-full '", "\n", "'first before you install the particular codebase. '", ")", "\n", "\n", "", "cls", ".", "__init__", "=", "MethodType", "(", "import_error_init", ",", "cls", ")", "\n", "return", "cls", "\n", "\n", "", "return", "decorate", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM.__init__": [[16, 47], ["isinstance", "gradcam_utils.GradCAM.model.eval", "plt.get_cmap", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "gradcam_utils.GradCAM._register_hooks", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._register_hooks"], ["def", "__init__", "(", "self", ",", "model", ",", "target_layer_name", ",", "colormap", "=", "'viridis'", ")", ":", "\n", "        ", "\"\"\"Create GradCAM class with recognizer, target layername & colormap.\n\n        Args:\n            model (nn.Module): the recognizer model to be used.\n            target_layer_name (str): name of convolutional layer to\n                be used to get gradients and feature maps from for creating\n                localization maps.\n            colormap (Optional[str]): matplotlib colormap used to create\n                heatmap. Default: 'viridis'. For more information, please visit\n                https://matplotlib.org/3.3.0/tutorials/colors/colormaps.html\n        \"\"\"", "\n", "from", ".", ".", "models", ".", "recognizers", "import", "Recognizer2D", ",", "Recognizer3D", "\n", "if", "isinstance", "(", "model", ",", "Recognizer2D", ")", ":", "\n", "            ", "self", ".", "is_recognizer2d", "=", "True", "\n", "", "elif", "isinstance", "(", "model", ",", "Recognizer3D", ")", ":", "\n", "            ", "self", ".", "is_recognizer2d", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'GradCAM utils only support Recognizer2D & Recognizer3D.'", ")", "\n", "\n", "", "self", ".", "model", "=", "model", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "target_gradients", "=", "None", "\n", "self", ".", "target_activations", "=", "None", "\n", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "self", ".", "colormap", "=", "plt", ".", "get_cmap", "(", "colormap", ")", "\n", "self", ".", "data_mean", "=", "torch", ".", "tensor", "(", "model", ".", "cfg", ".", "img_norm_cfg", "[", "'mean'", "]", ")", "\n", "self", ".", "data_std", "=", "torch", ".", "tensor", "(", "model", ".", "cfg", ".", "img_norm_cfg", "[", "'std'", "]", ")", "\n", "self", ".", "_register_hooks", "(", "target_layer_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._register_hooks": [[48, 70], ["layer_name.split", "target_layer.register_forward_hook", "target_layer.register_backward_hook", "grad_output[].detach", "output.clone().detach", "output.clone"], "methods", ["None"], ["", "def", "_register_hooks", "(", "self", ",", "layer_name", ")", ":", "\n", "        ", "\"\"\"Register forward and backward hook to a layer, given layer_name, to\n        obtain gradients and activations.\n\n        Args:\n            layer_name (str): name of the layer.\n        \"\"\"", "\n", "\n", "def", "get_gradients", "(", "module", ",", "grad_input", ",", "grad_output", ")", ":", "\n", "            ", "self", ".", "target_gradients", "=", "grad_output", "[", "0", "]", ".", "detach", "(", ")", "\n", "\n", "", "def", "get_activations", "(", "module", ",", "input", ",", "output", ")", ":", "\n", "            ", "self", ".", "target_activations", "=", "output", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "\n", "", "layer_ls", "=", "layer_name", ".", "split", "(", "'/'", ")", "\n", "prev_module", "=", "self", ".", "model", "\n", "for", "layer", "in", "layer_ls", ":", "\n", "            ", "prev_module", "=", "prev_module", ".", "_modules", "[", "layer", "]", "\n", "\n", "", "target_layer", "=", "prev_module", "\n", "target_layer", ".", "register_forward_hook", "(", "get_activations", ")", "\n", "target_layer", ".", "register_backward_hook", "(", "get_gradients", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._calculate_localization_map": [[71, 153], ["inputs[].clone", "gradcam_utils.GradCAM.model", "gradcam_utils.GradCAM.model.zero_grad", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.gather.backward", "torch.gather.backward", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "weights.view.view.view", "activations.permute.permute.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.relu", "torch.relu", "torch.interpolate.permute", "torch.interpolate", "torch.interpolate", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "inputs[].size", "inputs[].size", "gradients.permute.permute.size", "gradients.permute.permute.size", "gradients.permute.permute.permute", "activations.permute.permute.permute", "gradients.permute.permute.view", "torch.interpolate.squeeze", "labels.unsqueeze.unsqueeze.unsqueeze", "torch.max", "torch.max", "torch.max", "torch.max", "list", "torch.min", "torch.min", "torch.min", "torch.min", "torch.max", "torch.max", "torch.max", "torch.max", "torch.interpolate.view", "torch.interpolate.view", "activations.permute.permute.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward"], ["", "def", "_calculate_localization_map", "(", "self", ",", "inputs", ",", "use_labels", ",", "delta", "=", "1e-20", ")", ":", "\n", "        ", "\"\"\"Calculate localization map for all inputs with Grad-CAM.\n\n        Args:\n            inputs (dict): model inputs, generated by test pipeline,\n                at least including two keys, ``imgs`` and ``label``.\n            use_labels (bool): Whether to use given labels to generate\n                localization map. Labels are in ``inputs['label']``.\n            delta (float): used in localization map normalization,\n                must be small enough. Please make sure\n                `localization_map_max - localization_map_min >> delta`\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]: (localization_map, preds)\n                localization_map (torch.Tensor): the localization map for\n                    input imgs.\n                preds (torch.Tensor): Model predictions for `inputs` with\n                    shape (batch_size, num_classes).\n        \"\"\"", "\n", "inputs", "[", "'imgs'", "]", "=", "inputs", "[", "'imgs'", "]", ".", "clone", "(", ")", "\n", "\n", "# model forward & backward", "\n", "preds", "=", "self", ".", "model", "(", "gradcam", "=", "True", ",", "**", "inputs", ")", "\n", "if", "use_labels", ":", "\n", "            ", "labels", "=", "inputs", "[", "'label'", "]", "\n", "if", "labels", ".", "ndim", "==", "1", ":", "\n", "                ", "labels", "=", "labels", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "score", "=", "torch", ".", "gather", "(", "preds", ",", "dim", "=", "1", ",", "index", "=", "labels", ")", "\n", "", "else", ":", "\n", "            ", "score", "=", "torch", ".", "max", "(", "preds", ",", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "", "self", ".", "model", ".", "zero_grad", "(", ")", "\n", "score", "=", "torch", ".", "sum", "(", "score", ")", "\n", "score", ".", "backward", "(", ")", "\n", "\n", "if", "self", ".", "is_recognizer2d", ":", "\n", "# [batch_size, num_segments, 3, H, W]", "\n", "            ", "b", ",", "t", ",", "_", ",", "h", ",", "w", "=", "inputs", "[", "'imgs'", "]", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "# [batch_size, num_crops*num_clips, 3, clip_len, H, W]", "\n", "            ", "b1", ",", "b2", ",", "_", ",", "t", ",", "h", ",", "w", "=", "inputs", "[", "'imgs'", "]", ".", "size", "(", ")", "\n", "b", "=", "b1", "*", "b2", "\n", "\n", "", "gradients", "=", "self", ".", "target_gradients", "\n", "activations", "=", "self", ".", "target_activations", "\n", "if", "self", ".", "is_recognizer2d", ":", "\n", "# [B*Tg, C', H', W']", "\n", "            ", "b_tg", ",", "c", ",", "_", ",", "_", "=", "gradients", ".", "size", "(", ")", "\n", "tg", "=", "b_tg", "//", "b", "\n", "", "else", ":", "\n", "# source shape: [B, C', Tg, H', W']", "\n", "            ", "_", ",", "c", ",", "tg", ",", "_", ",", "_", "=", "gradients", ".", "size", "(", ")", "\n", "# target shape: [B, Tg, C', H', W']", "\n", "gradients", "=", "gradients", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "activations", "=", "activations", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "\n", "# calculate & resize to [B, 1, T, H, W]", "\n", "", "weights", "=", "torch", ".", "mean", "(", "gradients", ".", "view", "(", "b", ",", "tg", ",", "c", ",", "-", "1", ")", ",", "dim", "=", "3", ")", "\n", "weights", "=", "weights", ".", "view", "(", "b", ",", "tg", ",", "c", ",", "1", ",", "1", ")", "\n", "activations", "=", "activations", ".", "view", "(", "[", "b", ",", "tg", ",", "c", "]", "+", "\n", "list", "(", "activations", ".", "size", "(", ")", "[", "-", "2", ":", "]", ")", ")", "\n", "localization_map", "=", "torch", ".", "sum", "(", "\n", "weights", "*", "activations", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "localization_map", "=", "F", ".", "relu", "(", "localization_map", ")", "\n", "localization_map", "=", "localization_map", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "localization_map", "=", "F", ".", "interpolate", "(", "\n", "localization_map", ",", "\n", "size", "=", "(", "t", ",", "h", ",", "w", ")", ",", "\n", "mode", "=", "'trilinear'", ",", "\n", "align_corners", "=", "False", ")", "\n", "\n", "# Normalize the localization map.", "\n", "localization_map_min", ",", "localization_map_max", "=", "(", "\n", "torch", ".", "min", "(", "localization_map", ".", "view", "(", "b", ",", "-", "1", ")", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", ",", "\n", "torch", ".", "max", "(", "localization_map", ".", "view", "(", "b", ",", "-", "1", ")", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", ")", "\n", "localization_map_min", "=", "torch", ".", "reshape", "(", "\n", "localization_map_min", ",", "shape", "=", "(", "b", ",", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "localization_map_max", "=", "torch", ".", "reshape", "(", "\n", "localization_map_max", ",", "shape", "=", "(", "b", ",", "1", ",", "1", ",", "1", ",", "1", ")", ")", "\n", "localization_map", "=", "(", "localization_map", "-", "localization_map_min", ")", "/", "(", "\n", "localization_map_max", "-", "localization_map_min", "+", "delta", ")", "\n", "localization_map", "=", "localization_map", ".", "data", "\n", "\n", "return", "localization_map", ".", "squeeze", "(", "dim", "=", "1", ")", ",", "preds", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._alpha_blending": [[154, 196], ["localization_map.cpu.cpu.cpu", "gradcam_utils.GradCAM.colormap", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "curr_inp.permute.permute.cpu", "localization_map.cpu.cpu.detach().numpy", "input_imgs.permute", "input_imgs.view", "curr_inp.permute.permute.permute", "localization_map.cpu.cpu.detach", "list", "input_imgs.size"], "methods", ["None"], ["", "def", "_alpha_blending", "(", "self", ",", "localization_map", ",", "input_imgs", ",", "alpha", ")", ":", "\n", "        ", "\"\"\"Blend heatmaps and model input images and get visulization results.\n\n        Args:\n            localization_map (torch.Tensor): localization map for all inputs,\n                generated with Grad-CAM\n            input_imgs (torch.Tensor): model inputs, normed images.\n            alpha (float): transparency level of the heatmap,\n                in the range [0, 1].\n        Returns:\n            torch.Tensor: blending results for localization map and input\n                images, with shape [B, T, H, W, 3] and pixel values in\n                RGB order within range [0, 1].\n        \"\"\"", "\n", "# localization_map shape [B, T, H, W]", "\n", "localization_map", "=", "localization_map", ".", "cpu", "(", ")", "\n", "\n", "# heatmap shape [B, T, H, W, 3] in RGB order", "\n", "heatmap", "=", "self", ".", "colormap", "(", "localization_map", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "heatmap", "=", "heatmap", "[", ":", ",", ":", ",", ":", ",", ":", ",", ":", "3", "]", "\n", "heatmap", "=", "torch", ".", "from_numpy", "(", "heatmap", ")", "\n", "\n", "# Permute input imgs to [B, T, H, W, 3], like heatmap", "\n", "if", "self", ".", "is_recognizer2d", ":", "\n", "# Recognizer2D input (B, T, C, H, W)", "\n", "            ", "curr_inp", "=", "input_imgs", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "4", ",", "2", ")", "\n", "", "else", ":", "\n", "# Recognizer3D input (B', num_clips*num_crops, C, T, H, W)", "\n", "# B = B' * num_clips * num_crops", "\n", "            ", "curr_inp", "=", "input_imgs", ".", "view", "(", "[", "-", "1", "]", "+", "list", "(", "input_imgs", ".", "size", "(", ")", "[", "2", ":", "]", ")", ")", "\n", "curr_inp", "=", "curr_inp", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "4", ",", "1", ")", "\n", "\n", "# renormalize input imgs to [0, 1]", "\n", "", "curr_inp", "=", "curr_inp", ".", "cpu", "(", ")", "\n", "curr_inp", "*=", "self", ".", "data_std", "\n", "curr_inp", "+=", "self", ".", "data_mean", "\n", "curr_inp", "/=", "255.", "\n", "\n", "# alpha blending", "\n", "blended_imgs", "=", "alpha", "*", "heatmap", "+", "(", "1", "-", "alpha", ")", "*", "curr_inp", "\n", "\n", "return", "blended_imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM.__call__": [[197, 233], ["gradcam_utils.GradCAM._calculate_localization_map", "gradcam_utils.GradCAM._alpha_blending"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._calculate_localization_map", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.gradcam_utils.GradCAM._alpha_blending"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "use_labels", "=", "False", ",", "alpha", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"Visualize the localization maps on their corresponding inputs as\n        heatmap, using Grad-CAM.\n\n        Generate visualization results for **ALL CROPS**.\n        For example, for I3D model, if `clip_len=32, num_clips=10` and\n        use `ThreeCrop` in test pipeline, then for every model inputs,\n        there are 960(32*10*3) images generated.\n\n        Args:\n            inputs (dict): model inputs, generated by test pipeline,\n                at least including two keys, ``imgs`` and ``label``.\n            use_labels (bool): Whether to use given labels to generate\n                localization map. Labels are in ``inputs['label']``.\n            alpha (float): transparency level of the heatmap,\n                in the range [0, 1].\n        Returns:\n            blended_imgs (torch.Tensor): Visualization results, blended by\n                localization maps and model inputs.\n            preds (torch.Tensor): Model predictions for inputs.\n        \"\"\"", "\n", "\n", "# localization_map shape [B, T, H, W]", "\n", "# preds shape [batch_size, num_classes]", "\n", "localization_map", ",", "preds", "=", "self", ".", "_calculate_localization_map", "(", "\n", "inputs", ",", "use_labels", "=", "use_labels", ")", "\n", "\n", "# blended_imgs shape [B, T, H, W, 3]", "\n", "blended_imgs", "=", "self", ".", "_alpha_blending", "(", "localization_map", ",", "inputs", "[", "'imgs'", "]", ",", "\n", "alpha", ")", "\n", "\n", "# blended_imgs shape [B, T, H, W, 3]", "\n", "# preds shape [batch_size, num_classes]", "\n", "# Recognizer2D: B = batch_size, T = num_segments", "\n", "# Recognizer3D: B = batch_size * num_crops * num_clips, T = clip_len", "\n", "return", "blended_imgs", ",", "preds", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger": [[7, 26], ["mmcv.utils.get_logger", "__name__.split"], "function", ["None"], ["def", "get_root_logger", "(", "log_file", "=", "None", ",", "log_level", "=", "logging", ".", "INFO", ")", ":", "\n", "    ", "\"\"\"Use ``get_logger`` method in mmcv to get the root logger.\n\n    The logger will be initialized if it has not been initialized. By default a\n    StreamHandler will be added. If ``log_file`` is specified, a FileHandler\n    will also be added. The name of the root logger is the top-level package\n    name, e.g., \"mmaction\".\n\n    Args:\n        log_file (str | None): The log filename. If specified, a FileHandler\n            will be added to the root logger.\n        log_level (int): The root logger level. Note that only the process of\n            rank 0 is affected, while other processes will set the level to\n            \"Error\" and be silent most of the time.\n\n    Returns:\n        :obj:`logging.Logger`: The root logger.\n    \"\"\"", "\n", "return", "get_logger", "(", "__name__", ".", "split", "(", "'.'", ")", "[", "0", "]", ",", "log_file", ",", "log_level", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_random_string": [[7, 16], ["random.choice", "range"], "function", ["None"], ["\n", "def", "natural_key", "(", "string_", ")", ":", "\n", "    ", "\"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"", "\n", "return", "[", "int", "(", "s", ")", "if", "s", ".", "isdigit", "(", ")", "else", "s", "for", "s", "in", "re", ".", "split", "(", "r'(\\d+)'", ",", "string_", ".", "lower", "(", ")", ")", "]", "\n", "\n", "\n", "", "def", "add_bool_arg", "(", "parser", ",", "name", ",", "default", "=", "False", ",", "help", "=", "''", ")", ":", "\n", "    ", "dest_name", "=", "name", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "group", "=", "parser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "group", ".", "add_argument", "(", "'--'", "+", "name", ",", "dest", "=", "dest_name", ",", "action", "=", "'store_true'", ",", "help", "=", "help", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_thread_id": [[18, 23], ["ctypes.CDLL().syscall", "ctypes.CDLL"], "function", ["None"], ["parser", ".", "set_defaults", "(", "**", "{", "dest_name", ":", "default", "}", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_shm_dir": [[25, 28], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.PreciseBNHook.__init__": [[133, 140], ["isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataloader", ",", "num_iters", "=", "200", ",", "interval", "=", "1", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "dataloader", ",", "DataLoader", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'dataloader must be a pytorch DataLoader, but got'", "\n", "f' {type(dataloader)}'", ")", "\n", "", "self", ".", "dataloader", "=", "dataloader", "\n", "self", ".", "interval", "=", "interval", "\n", "self", ".", "num_iters", "=", "num_iters", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.PreciseBNHook.after_train_epoch": [[141, 156], ["precise_bn.PreciseBNHook.every_n_epochs", "time.sleep", "mmcv.utils.print_log", "precise_bn.update_bn_stats", "mmcv.utils.print_log", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.update_bn_stats"], ["", "def", "after_train_epoch", "(", "self", ",", "runner", ")", ":", "\n", "        ", "if", "self", ".", "every_n_epochs", "(", "runner", ",", "self", ".", "interval", ")", ":", "\n", "# sleep to avoid possible deadlock", "\n", "            ", "time", ".", "sleep", "(", "2.", ")", "\n", "print_log", "(", "\n", "f'Running Precise BN for {self.num_iters} iterations'", ",", "\n", "logger", "=", "runner", ".", "logger", ")", "\n", "update_bn_stats", "(", "\n", "runner", ".", "model", ",", "\n", "self", ".", "dataloader", ",", "\n", "self", ".", "num_iters", ",", "\n", "logger", "=", "runner", ".", "logger", ")", "\n", "print_log", "(", "'BN stats updated'", ",", "logger", "=", "runner", ".", "logger", ")", "\n", "# sleep to avoid possible deadlock", "\n", "time", ".", "sleep", "(", "2.", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.is_parallel_module": [[19, 34], ["bool", "isinstance"], "function", ["None"], ["def", "is_parallel_module", "(", "module", ")", ":", "\n", "    ", "\"\"\"Check if a module is a parallel module.\n\n    The following 3 modules (and their subclasses) are regarded as parallel\n    modules: DataParallel, DistributedDataParallel,\n    MMDistributedDataParallel (the deprecated version).\n\n    Args:\n        module (nn.Module): The module to be checked.\n    Returns:\n        bool: True if the input module is a parallel module.\n    \"\"\"", "\n", "parallels", "=", "(", "DataParallel", ",", "DistributedDataParallel", ",", "\n", "MMDistributedDataParallel", ")", "\n", "return", "bool", "(", "isinstance", "(", "module", ",", "parallels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.update_bn_stats": [[36, 121], ["torch.no_grad", "model.train", "precise_bn.is_parallel_module", "mmcv.utils.print_log", "model.modules", "mmcv.ProgressBar", "enumerate", "enumerate", "len", "len", "mmcv.utils.print_log", "torch.zeros_like", "torch.zeros_like", "len", "mmcv.ProgressBar.update", "enumerate", "len", "model.modules", "isinstance", "mmcv.utils.print_log", "torch.no_grad", "parallel_module", "isinstance", "len"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.precise_bn.is_parallel_module", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "update_bn_stats", "(", "model", ",", "data_loader", ",", "num_iters", "=", "200", ",", "logger", "=", "None", ")", ":", "\n", "    ", "\"\"\"Recompute and update the batch norm stats to make them more precise.\n\n    During\n    training both BN stats and the weight are changing after every iteration,\n    so the running average can not precisely reflect the actual stats of the\n    current model.\n    In this function, the BN stats are recomputed with fixed weights, to make\n    the running average more precise. Specifically, it computes the true\n    average of per-batch mean/variance instead of the running average.\n\n    Args:\n        model (nn.Module): The model whose bn stats will be recomputed.\n        data_loader (iterator): The DataLoader iterator.\n        num_iters (int): number of iterations to compute the stats.\n        logger (:obj:`logging.Logger` | None): Logger for logging.\n            Default: None.\n    \"\"\"", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "assert", "len", "(", "data_loader", ")", ">=", "num_iters", ",", "(", "\n", "f'length of dataloader {len(data_loader)} must be greater than '", "\n", "f'iteration number {num_iters}'", ")", "\n", "\n", "if", "is_parallel_module", "(", "model", ")", ":", "\n", "        ", "parallel_module", "=", "model", "\n", "model", "=", "model", ".", "module", "\n", "", "else", ":", "\n", "        ", "parallel_module", "=", "model", "\n", "# Finds all the bn layers with training=True.", "\n", "", "bn_layers", "=", "[", "\n", "m", "for", "m", "in", "model", ".", "modules", "(", ")", "if", "m", ".", "training", "and", "isinstance", "(", "m", ",", "_BatchNorm", ")", "\n", "]", "\n", "\n", "if", "len", "(", "bn_layers", ")", "==", "0", ":", "\n", "        ", "print_log", "(", "'No BN found in model'", ",", "logger", "=", "logger", ",", "level", "=", "logging", ".", "WARNING", ")", "\n", "return", "\n", "", "print_log", "(", "f'{len(bn_layers)} BN found'", ",", "logger", "=", "logger", ")", "\n", "\n", "# Finds all the other norm layers with training=True.", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "        ", "if", "m", ".", "training", "and", "isinstance", "(", "m", ",", "(", "_InstanceNorm", ",", "GroupNorm", ")", ")", ":", "\n", "            ", "print_log", "(", "\n", "'IN/GN stats will be updated like training.'", ",", "\n", "logger", "=", "logger", ",", "\n", "level", "=", "logging", ".", "WARNING", ")", "\n", "\n", "# In order to make the running stats only reflect the current batch, the", "\n", "# momentum is disabled.", "\n", "# bn.running_mean = (1 - momentum) * bn.running_mean + momentum *", "\n", "# batch_mean", "\n", "# Setting the momentum to 1.0 to compute the stats without momentum.", "\n", "", "", "momentum_actual", "=", "[", "bn", ".", "momentum", "for", "bn", "in", "bn_layers", "]", "# pyre-ignore", "\n", "for", "bn", "in", "bn_layers", ":", "\n", "        ", "bn", ".", "momentum", "=", "1.0", "\n", "\n", "# Note that running_var actually means \"running average of variance\"", "\n", "", "running_mean", "=", "[", "torch", ".", "zeros_like", "(", "bn", ".", "running_mean", ")", "for", "bn", "in", "bn_layers", "]", "\n", "running_var", "=", "[", "torch", ".", "zeros_like", "(", "bn", ".", "running_var", ")", "for", "bn", "in", "bn_layers", "]", "\n", "\n", "finish_before_loader", "=", "False", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "len", "(", "data_loader", ")", ")", "\n", "for", "ind", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "parallel_module", "(", "**", "data", ",", "return_loss", "=", "False", ")", "\n", "", "prog_bar", ".", "update", "(", ")", "\n", "for", "i", ",", "bn", "in", "enumerate", "(", "bn_layers", ")", ":", "\n", "# Accumulates the bn stats.", "\n", "            ", "running_mean", "[", "i", "]", "+=", "(", "bn", ".", "running_mean", "-", "running_mean", "[", "i", "]", ")", "/", "(", "ind", "+", "1", ")", "\n", "# running var is actually", "\n", "running_var", "[", "i", "]", "+=", "(", "bn", ".", "running_var", "-", "running_var", "[", "i", "]", ")", "/", "(", "ind", "+", "1", ")", "\n", "\n", "", "if", "(", "ind", "+", "1", ")", ">=", "num_iters", ":", "\n", "            ", "finish_before_loader", "=", "True", "\n", "break", "\n", "", "", "assert", "finish_before_loader", ",", "'Dataloader stopped before '", "f'iteration {num_iters}'", "\n", "\n", "for", "i", ",", "bn", "in", "enumerate", "(", "bn_layers", ")", ":", "\n", "# Sets the precise bn stats.", "\n", "        ", "bn", ".", "running_mean", "=", "running_mean", "[", "i", "]", "\n", "bn", ".", "running_var", "=", "running_var", "[", "i", "]", "\n", "bn", ".", "momentum", "=", "momentum_actual", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.collect_env.collect_env": [[8, 13], ["mmcv.utils.collect_env", "mmcv.utils.get_git_hash"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.collect_env.collect_env"], ["def", "collect_env", "(", ")", ":", "\n", "    ", "env_info", "=", "collect_basic_env", "(", ")", "\n", "env_info", "[", "'MMAction2'", "]", "=", "(", "\n", "mmaction", ".", "__version__", "+", "'+'", "+", "get_git_hash", "(", "digits", "=", "7", ")", ")", "\n", "return", "env_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.GPUNormalize.__init__": [[51, 71], ["torch.tensor", "torch.tensor", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_format", ",", "mean", ",", "std", ")", ":", "\n", "        ", "if", "input_format", "not", "in", "[", "'NCTHW'", ",", "'NCHW'", ",", "'NCHW_Flow'", ",", "'NPTCHW'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "f'The input format {input_format} is invalid.'", ")", "\n", "", "self", ".", "input_format", "=", "input_format", "\n", "_mean", "=", "torch", ".", "tensor", "(", "mean", ")", "\n", "_std", "=", "torch", ".", "tensor", "(", "std", ")", "\n", "if", "input_format", "==", "'NCTHW'", ":", "\n", "            ", "self", ".", "_mean", "=", "_mean", "[", "None", ",", ":", ",", "None", ",", "None", ",", "None", "]", "\n", "self", ".", "_std", "=", "_std", "[", "None", ",", ":", ",", "None", ",", "None", ",", "None", "]", "\n", "", "elif", "input_format", "==", "'NCHW'", ":", "\n", "            ", "self", ".", "_mean", "=", "_mean", "[", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "self", ".", "_std", "=", "_std", "[", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "", "elif", "input_format", "==", "'NCHW_Flow'", ":", "\n", "            ", "self", ".", "_mean", "=", "_mean", "[", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "self", ".", "_std", "=", "_std", "[", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "", "elif", "input_format", "==", "'NPTCHW'", ":", "\n", "            ", "self", ".", "_mean", "=", "_mean", "[", "None", ",", "None", ",", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "self", ".", "_std", "=", "_std", "[", "None", ",", "None", ",", "None", ",", ":", ",", "None", ",", "None", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f'The input format {input_format} is invalid.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.GPUNormalize.hook_func": [[72, 89], ["module_hooks.GPUNormalize._mean.to", "module_hooks.GPUNormalize._std.to", "torch.no_grad", "x.float().sub_().div_.float().sub_().div_.float().sub_().div_", "x.float().sub_().div_.float().sub_().div_.float().sub_", "x.float().sub_().div_.float().sub_().div_.float"], "methods", ["None"], ["", "", "def", "hook_func", "(", "self", ")", ":", "\n", "\n", "        ", "def", "normalize_hook", "(", "Module", ",", "input", ")", ":", "\n", "            ", "x", "=", "input", "[", "0", "]", "\n", "assert", "x", ".", "dtype", "==", "torch", ".", "uint8", ",", "(", "\n", "f'The previous augmentation should use uint8 data type to '", "\n", "f'speed up computation, but get {x.dtype}'", ")", "\n", "\n", "mean", "=", "self", ".", "_mean", ".", "to", "(", "x", ".", "device", ")", "\n", "std", "=", "self", ".", "_std", ".", "to", "(", "x", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "x", "=", "x", ".", "float", "(", ")", ".", "sub_", "(", "mean", ")", ".", "div_", "(", "std", ")", "\n", "\n", "", "return", "(", "x", ",", "*", "input", "[", "1", ":", "]", ")", "\n", "\n", "", "return", "normalize_hook", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.register_module_hooks": [[8, 33], ["module_hook_cfg.pop", "getattr", "module_hook_cfg.pop", "handles.append", "hasattr", "ValueError", "getattr.register_forward_pre_hook", "mmcv.utils.build_from_cfg().hook_func", "getattr.register_forward_hook", "mmcv.utils.build_from_cfg().hook_func", "getattr.register_backward_hook", "ValueError", "mmcv.utils.build_from_cfg", "mmcv.utils.build_from_cfg().hook_func", "mmcv.utils.build_from_cfg", "mmcv.utils.build_from_cfg"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.GPUNormalize.hook_func", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.GPUNormalize.hook_func", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.GPUNormalize.hook_func"], ["def", "register_module_hooks", "(", "Module", ",", "module_hooks_list", ")", ":", "\n", "    ", "handles", "=", "[", "]", "\n", "for", "module_hook_cfg", "in", "module_hooks_list", ":", "\n", "        ", "hooked_module_name", "=", "module_hook_cfg", ".", "pop", "(", "'hooked_module'", ",", "'backbone'", ")", "\n", "if", "not", "hasattr", "(", "Module", ",", "hooked_module_name", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'{Module.__class__} has no {hooked_module_name}!'", ")", "\n", "", "hooked_module", "=", "getattr", "(", "Module", ",", "hooked_module_name", ")", "\n", "hook_pos", "=", "module_hook_cfg", ".", "pop", "(", "'hook_pos'", ",", "'forward_pre'", ")", "\n", "\n", "if", "hook_pos", "==", "'forward_pre'", ":", "\n", "            ", "handle", "=", "hooked_module", ".", "register_forward_pre_hook", "(", "\n", "build_from_cfg", "(", "module_hook_cfg", ",", "MODULE_HOOKS", ")", ".", "hook_func", "(", ")", ")", "\n", "", "elif", "hook_pos", "==", "'forward'", ":", "\n", "            ", "handle", "=", "hooked_module", ".", "register_forward_hook", "(", "\n", "build_from_cfg", "(", "module_hook_cfg", ",", "MODULE_HOOKS", ")", ".", "hook_func", "(", ")", ")", "\n", "", "elif", "hook_pos", "==", "'backward'", ":", "\n", "            ", "handle", "=", "hooked_module", ".", "register_backward_hook", "(", "\n", "build_from_cfg", "(", "module_hook_cfg", ",", "MODULE_HOOKS", ")", ".", "hook_func", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'hook_pos must be `forward_pre`, `forward` or `backward`, '", "\n", "f'but get {hook_pos}'", ")", "\n", "", "handles", ".", "append", "(", "handle", ")", "\n", "", "return", "handles", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.__init__": [[56, 70], ["graph.Graph.get_edge", "graph.get_hop_distance", "graph.Graph.get_adjacency"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.get_edge", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.get_hop_distance", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.get_adjacency"], ["def", "__init__", "(", "self", ",", "\n", "layout", "=", "'openpose'", ",", "\n", "strategy", "=", "'uniform'", ",", "\n", "max_hop", "=", "1", ",", "\n", "dilation", "=", "1", ")", ":", "\n", "        ", "self", ".", "max_hop", "=", "max_hop", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "assert", "layout", "in", "[", "'openpose'", ",", "'ntu-rgb+d'", ",", "'ntu_edge'", ",", "'coco'", "]", "\n", "assert", "strategy", "in", "[", "'uniform'", ",", "'distance'", ",", "'spatial'", "]", "\n", "self", ".", "get_edge", "(", "layout", ")", "\n", "self", ".", "hop_dis", "=", "get_hop_distance", "(", "\n", "self", ".", "num_node", ",", "self", ".", "edge", ",", "max_hop", "=", "max_hop", ")", "\n", "self", ".", "get_adjacency", "(", "strategy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.__str__": [[71, 73], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "A", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.get_edge": [[74, 120], ["range", "range", "ValueError", "range", "range"], "methods", ["None"], ["", "def", "get_edge", "(", "self", ",", "layout", ")", ":", "\n", "        ", "\"\"\"This method returns the edge pairs of the layout.\"\"\"", "\n", "\n", "if", "layout", "==", "'openpose'", ":", "\n", "            ", "self", ".", "num_node", "=", "18", "\n", "self_link", "=", "[", "(", "i", ",", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_node", ")", "]", "\n", "neighbor_link", "=", "[", "(", "4", ",", "3", ")", ",", "(", "3", ",", "2", ")", ",", "(", "7", ",", "6", ")", ",", "(", "6", ",", "5", ")", ",", "\n", "(", "13", ",", "12", ")", ",", "(", "12", ",", "11", ")", ",", "(", "10", ",", "9", ")", ",", "(", "9", ",", "8", ")", ",", "(", "11", ",", "5", ")", ",", "\n", "(", "8", ",", "2", ")", ",", "(", "5", ",", "1", ")", ",", "(", "2", ",", "1", ")", ",", "(", "0", ",", "1", ")", ",", "(", "15", ",", "0", ")", ",", "(", "14", ",", "0", ")", ",", "\n", "(", "17", ",", "15", ")", ",", "(", "16", ",", "14", ")", "]", "\n", "self", ".", "edge", "=", "self_link", "+", "neighbor_link", "\n", "self", ".", "center", "=", "1", "\n", "", "elif", "layout", "==", "'ntu-rgb+d'", ":", "\n", "            ", "self", ".", "num_node", "=", "25", "\n", "self_link", "=", "[", "(", "i", ",", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_node", ")", "]", "\n", "neighbor_1base", "=", "[", "(", "1", ",", "2", ")", ",", "(", "2", ",", "21", ")", ",", "(", "3", ",", "21", ")", ",", "\n", "(", "4", ",", "3", ")", ",", "(", "5", ",", "21", ")", ",", "(", "6", ",", "5", ")", ",", "(", "7", ",", "6", ")", ",", "(", "8", ",", "7", ")", ",", "(", "9", ",", "21", ")", ",", "\n", "(", "10", ",", "9", ")", ",", "(", "11", ",", "10", ")", ",", "(", "12", ",", "11", ")", ",", "(", "13", ",", "1", ")", ",", "(", "14", ",", "13", ")", ",", "\n", "(", "15", ",", "14", ")", ",", "(", "16", ",", "15", ")", ",", "(", "17", ",", "1", ")", ",", "(", "18", ",", "17", ")", ",", "(", "19", ",", "18", ")", ",", "\n", "(", "20", ",", "19", ")", ",", "(", "22", ",", "23", ")", ",", "(", "23", ",", "8", ")", ",", "(", "24", ",", "25", ")", ",", "(", "25", ",", "12", ")", "]", "\n", "neighbor_link", "=", "[", "(", "i", "-", "1", ",", "j", "-", "1", ")", "for", "(", "i", ",", "j", ")", "in", "neighbor_1base", "]", "\n", "self", ".", "edge", "=", "self_link", "+", "neighbor_link", "\n", "self", ".", "center", "=", "21", "-", "1", "\n", "", "elif", "layout", "==", "'ntu_edge'", ":", "\n", "            ", "self", ".", "num_node", "=", "24", "\n", "self_link", "=", "[", "(", "i", ",", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_node", ")", "]", "\n", "neighbor_1base", "=", "[", "(", "1", ",", "2", ")", ",", "(", "3", ",", "2", ")", ",", "(", "4", ",", "3", ")", ",", "(", "5", ",", "2", ")", ",", "(", "6", ",", "5", ")", ",", "(", "7", ",", "6", ")", ",", "\n", "(", "8", ",", "7", ")", ",", "(", "9", ",", "2", ")", ",", "(", "10", ",", "9", ")", ",", "(", "11", ",", "10", ")", ",", "(", "12", ",", "11", ")", ",", "\n", "(", "13", ",", "1", ")", ",", "(", "14", ",", "13", ")", ",", "(", "15", ",", "14", ")", ",", "(", "16", ",", "15", ")", ",", "(", "17", ",", "1", ")", ",", "\n", "(", "18", ",", "17", ")", ",", "(", "19", ",", "18", ")", ",", "(", "20", ",", "19", ")", ",", "(", "21", ",", "22", ")", ",", "(", "22", ",", "8", ")", ",", "\n", "(", "23", ",", "24", ")", ",", "(", "24", ",", "12", ")", "]", "\n", "neighbor_link", "=", "[", "(", "i", "-", "1", ",", "j", "-", "1", ")", "for", "(", "i", ",", "j", ")", "in", "neighbor_1base", "]", "\n", "self", ".", "edge", "=", "self_link", "+", "neighbor_link", "\n", "self", ".", "center", "=", "2", "\n", "", "elif", "layout", "==", "'coco'", ":", "\n", "            ", "self", ".", "num_node", "=", "17", "\n", "self_link", "=", "[", "(", "i", ",", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_node", ")", "]", "\n", "neighbor_1base", "=", "[", "[", "16", ",", "14", "]", ",", "[", "14", ",", "12", "]", ",", "[", "17", ",", "15", "]", ",", "[", "15", ",", "13", "]", ",", "[", "12", ",", "13", "]", ",", "\n", "[", "6", ",", "12", "]", ",", "[", "7", ",", "13", "]", ",", "[", "6", ",", "7", "]", ",", "[", "8", ",", "6", "]", ",", "[", "9", ",", "7", "]", ",", "\n", "[", "10", ",", "8", "]", ",", "[", "11", ",", "9", "]", ",", "[", "2", ",", "3", "]", ",", "[", "2", ",", "1", "]", ",", "[", "3", ",", "1", "]", ",", "[", "4", ",", "2", "]", ",", "\n", "[", "5", ",", "3", "]", ",", "[", "4", ",", "6", "]", ",", "[", "5", ",", "7", "]", "]", "\n", "neighbor_link", "=", "[", "(", "i", "-", "1", ",", "j", "-", "1", ")", "for", "(", "i", ",", "j", ")", "in", "neighbor_1base", "]", "\n", "self", ".", "edge", "=", "self_link", "+", "neighbor_link", "\n", "self", ".", "center", "=", "0", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Do Not Exist This Layout.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.Graph.get_adjacency": [[121, 166], ["range", "numpy.zeros", "graph.normalize_digraph", "numpy.zeros", "numpy.zeros", "enumerate", "numpy.stack", "ValueError", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "range", "numpy.stack.append", "numpy.stack.append", "numpy.stack.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.normalize_digraph"], ["", "", "def", "get_adjacency", "(", "self", ",", "strategy", ")", ":", "\n", "        ", "\"\"\"This method returns the adjacency matrix according to strategy.\"\"\"", "\n", "\n", "valid_hop", "=", "range", "(", "0", ",", "self", ".", "max_hop", "+", "1", ",", "self", ".", "dilation", ")", "\n", "adjacency", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "for", "hop", "in", "valid_hop", ":", "\n", "            ", "adjacency", "[", "self", ".", "hop_dis", "==", "hop", "]", "=", "1", "\n", "", "normalize_adjacency", "=", "normalize_digraph", "(", "adjacency", ")", "\n", "\n", "if", "strategy", "==", "'uniform'", ":", "\n", "            ", "A", "=", "np", ".", "zeros", "(", "(", "1", ",", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "A", "[", "0", "]", "=", "normalize_adjacency", "\n", "self", ".", "A", "=", "A", "\n", "", "elif", "strategy", "==", "'distance'", ":", "\n", "            ", "A", "=", "np", ".", "zeros", "(", "(", "len", "(", "valid_hop", ")", ",", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "for", "i", ",", "hop", "in", "enumerate", "(", "valid_hop", ")", ":", "\n", "                ", "A", "[", "i", "]", "[", "self", ".", "hop_dis", "==", "hop", "]", "=", "normalize_adjacency", "[", "self", ".", "hop_dis", "==", "\n", "hop", "]", "\n", "", "self", ".", "A", "=", "A", "\n", "", "elif", "strategy", "==", "'spatial'", ":", "\n", "            ", "A", "=", "[", "]", "\n", "for", "hop", "in", "valid_hop", ":", "\n", "                ", "a_root", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "a_close", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "a_further", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_node", ",", "self", ".", "num_node", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_node", ")", ":", "\n", "                    ", "for", "j", "in", "range", "(", "self", ".", "num_node", ")", ":", "\n", "                        ", "if", "self", ".", "hop_dis", "[", "j", ",", "i", "]", "==", "hop", ":", "\n", "                            ", "if", "self", ".", "hop_dis", "[", "j", ",", "self", ".", "center", "]", "==", "self", ".", "hop_dis", "[", "\n", "i", ",", "self", ".", "center", "]", ":", "\n", "                                ", "a_root", "[", "j", ",", "i", "]", "=", "normalize_adjacency", "[", "j", ",", "i", "]", "\n", "", "elif", "self", ".", "hop_dis", "[", "j", ",", "self", ".", "center", "]", ">", "self", ".", "hop_dis", "[", "\n", "i", ",", "self", ".", "center", "]", ":", "\n", "                                ", "a_close", "[", "j", ",", "i", "]", "=", "normalize_adjacency", "[", "j", ",", "i", "]", "\n", "", "else", ":", "\n", "                                ", "a_further", "[", "j", ",", "i", "]", "=", "normalize_adjacency", "[", "j", ",", "i", "]", "\n", "", "", "", "", "if", "hop", "==", "0", ":", "\n", "                    ", "A", ".", "append", "(", "a_root", ")", "\n", "", "else", ":", "\n", "                    ", "A", ".", "append", "(", "a_root", "+", "a_close", ")", "\n", "A", ".", "append", "(", "a_further", ")", "\n", "", "", "A", "=", "np", ".", "stack", "(", "A", ")", "\n", "self", ".", "A", "=", "A", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Do Not Exist This Strategy'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.get_hop_distance": [[4, 19], ["numpy.zeros", "range", "numpy.zeros", "numpy.linalg.matrix_power", "numpy.stack", "range"], "function", ["None"], ["def", "get_hop_distance", "(", "num_node", ",", "edge", ",", "max_hop", "=", "1", ")", ":", "\n", "    ", "adj_mat", "=", "np", ".", "zeros", "(", "(", "num_node", ",", "num_node", ")", ")", "\n", "for", "i", ",", "j", "in", "edge", ":", "\n", "        ", "adj_mat", "[", "i", ",", "j", "]", "=", "1", "\n", "adj_mat", "[", "j", ",", "i", "]", "=", "1", "\n", "\n", "# compute hop steps", "\n", "", "hop_dis", "=", "np", ".", "zeros", "(", "(", "num_node", ",", "num_node", ")", ")", "+", "np", ".", "inf", "\n", "transfer_mat", "=", "[", "\n", "np", ".", "linalg", ".", "matrix_power", "(", "adj_mat", ",", "d", ")", "for", "d", "in", "range", "(", "max_hop", "+", "1", ")", "\n", "]", "\n", "arrive_mat", "=", "(", "np", ".", "stack", "(", "transfer_mat", ")", ">", "0", ")", "\n", "for", "d", "in", "range", "(", "max_hop", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "hop_dis", "[", "arrive_mat", "[", "d", "]", "]", "=", "d", "\n", "", "return", "hop_dis", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.graph.normalize_digraph": [[21, 30], ["numpy.sum", "numpy.zeros", "range", "numpy.dot"], "function", ["None"], ["", "def", "normalize_digraph", "(", "adj_matrix", ")", ":", "\n", "    ", "Dl", "=", "np", ".", "sum", "(", "adj_matrix", ",", "0", ")", "\n", "num_nodes", "=", "adj_matrix", ".", "shape", "[", "0", "]", "\n", "Dn", "=", "np", ".", "zeros", "(", "(", "num_nodes", ",", "num_nodes", ")", ")", "\n", "for", "i", "in", "range", "(", "num_nodes", ")", ":", "\n", "        ", "if", "Dl", "[", "i", "]", ">", "0", ":", "\n", "            ", "Dn", "[", "i", ",", "i", "]", "=", "Dl", "[", "i", "]", "**", "(", "-", "1", ")", "\n", "", "", "norm_matrix", "=", "np", ".", "dot", "(", "adj_matrix", ",", "Dn", ")", "\n", "return", "norm_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.post_processing.post_processing": [[5, 46], ["range", "len", "soft_nms", "min", "float", "proposal_list.append", "float", "len", "result[].argsort", "max", "min"], "function", ["None"], ["def", "post_processing", "(", "result", ",", "video_info", ",", "soft_nms_alpha", ",", "soft_nms_low_threshold", ",", "\n", "soft_nms_high_threshold", ",", "post_process_top_k", ",", "\n", "feature_extraction_interval", ")", ":", "\n", "    ", "\"\"\"Post process for temporal proposals generation.\n\n    Args:\n        result (np.ndarray): Proposals generated by network.\n        video_info (dict): Meta data of video. Required keys are\n            'duration_frame', 'duration_second'.\n        soft_nms_alpha (float): Alpha value of Gaussian decaying function.\n        soft_nms_low_threshold (float): Low threshold for soft nms.\n        soft_nms_high_threshold (float): High threshold for soft nms.\n        post_process_top_k (int): Top k values to be considered.\n        feature_extraction_interval (int): Interval used in feature extraction.\n\n    Returns:\n        list[dict]: The updated proposals, e.g.\n            [{'score': 0.9, 'segment': [0, 1]},\n             {'score': 0.8, 'segment': [0, 2]},\n            ...].\n    \"\"\"", "\n", "if", "len", "(", "result", ")", ">", "1", ":", "\n", "        ", "result", "=", "soft_nms", "(", "result", ",", "soft_nms_alpha", ",", "soft_nms_low_threshold", ",", "\n", "soft_nms_high_threshold", ",", "post_process_top_k", ")", "\n", "\n", "", "result", "=", "result", "[", "result", "[", ":", ",", "-", "1", "]", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "]", "\n", "video_duration", "=", "float", "(", "\n", "video_info", "[", "'duration_frame'", "]", "//", "feature_extraction_interval", "*", "\n", "feature_extraction_interval", "\n", ")", "/", "video_info", "[", "'duration_frame'", "]", "*", "video_info", "[", "'duration_second'", "]", "\n", "proposal_list", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "min", "(", "post_process_top_k", ",", "len", "(", "result", ")", ")", ")", ":", "\n", "        ", "proposal", "=", "{", "}", "\n", "proposal", "[", "'score'", "]", "=", "float", "(", "result", "[", "j", ",", "-", "1", "]", ")", "\n", "proposal", "[", "'segment'", "]", "=", "[", "\n", "max", "(", "0", ",", "result", "[", "j", ",", "0", "]", ")", "*", "video_duration", ",", "\n", "min", "(", "1", ",", "result", "[", "j", ",", "1", "]", ")", "*", "video_duration", "\n", "]", "\n", "proposal_list", ".", "append", "(", "proposal", ")", "\n", "", "return", "proposal_list", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.poly_lr.PolyLRScheduler.__init__": [[24, 68], ["scheduler.Scheduler.__init__", "_logger.warning", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "power", ":", "float", "=", "0.5", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "k_decay", "=", ".5", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "if", "t_initial", "==", "1", "and", "cycle_mul", "==", "1", "and", "cycle_decay", "==", "1", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Cosine annealing scheduler will have no effect on the learning \"", "\n", "\"rate since t_initial = t_mul = eta_mul = 1.\"", ")", "\n", "", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "self", ".", "k_decay", "=", "k_decay", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.poly_lr.PolyLRScheduler._get_lr": [[69, 98], ["math.floor", "math.log"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "k", "=", "self", ".", "k_decay", "\n", "\n", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "-", "t_curr", "**", "k", "/", "t_i", "**", "k", ")", "**", "self", ".", "power", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.poly_lr.PolyLRScheduler.get_epoch_values": [[99, 104], ["poly_lr.PolyLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.poly_lr.PolyLRScheduler.get_update_values": [[105, 110], ["poly_lr.PolyLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.poly_lr.PolyLRScheduler.get_cycle_length": [[111, 117], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler.__init__": [[15, 59], ["scheduler.Scheduler.__init__", "torch.optim.lr_scheduler.ReduceLROnPlateau", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ",", "\n", "decay_rate", "=", "0.1", ",", "\n", "patience_t", "=", "10", ",", "\n", "verbose", "=", "True", ",", "\n", "threshold", "=", "1e-4", ",", "\n", "cooldown_t", "=", "0", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "lr_min", "=", "0", ",", "\n", "mode", "=", "'max'", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_type", "=", "'normal'", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "None", ",", "\n", "initialize", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "'lr'", ",", "initialize", "=", "initialize", ")", "\n", "\n", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "self", ".", "optimizer", ",", "\n", "patience", "=", "patience_t", ",", "\n", "factor", "=", "decay_rate", ",", "\n", "verbose", "=", "verbose", ",", "\n", "threshold", "=", "threshold", ",", "\n", "cooldown", "=", "cooldown_t", ",", "\n", "mode", "=", "mode", ",", "\n", "min_lr", "=", "lr_min", "\n", ")", "\n", "\n", "self", ".", "noise_range", "=", "noise_range_t", "\n", "self", ".", "noise_pct", "=", "noise_pct", "\n", "self", ".", "noise_type", "=", "noise_type", "\n", "self", ".", "noise_std", "=", "noise_std", "\n", "self", ".", "noise_seed", "=", "noise_seed", "if", "noise_seed", "is", "not", "None", "else", "42", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "", "self", ".", "restore_lr", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler.state_dict": [[60, 64], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'best'", ":", "self", ".", "lr_scheduler", ".", "best", ",", "\n", "'last_epoch'", ":", "self", ".", "lr_scheduler", ".", "last_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler.load_state_dict": [[66, 70], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "lr_scheduler", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "if", "'last_epoch'", "in", "state_dict", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "state_dict", "[", "'last_epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler.step": [[72, 92], ["super().update_groups", "plateau_lr.PlateauLRScheduler.lr_scheduler.step", "enumerate", "isinstance", "plateau_lr.PlateauLRScheduler._apply_noise"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler._apply_noise"], ["", "", "def", "step", "(", "self", ",", "epoch", ",", "metric", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "<=", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "epoch", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "lrs", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "restore_lr", "is", "not", "None", ":", "\n", "# restore actual LR from before our last noise perturbation before stepping base", "\n", "                ", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                    ", "param_group", "[", "'lr'", "]", "=", "self", ".", "restore_lr", "[", "i", "]", "\n", "", "self", ".", "restore_lr", "=", "None", "\n", "\n", "", "self", ".", "lr_scheduler", ".", "step", "(", "metric", ",", "epoch", ")", "# step the base scheduler", "\n", "\n", "if", "self", ".", "noise_range", "is", "not", "None", ":", "\n", "                ", "if", "isinstance", "(", "self", ".", "noise_range", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                    ", "apply_noise", "=", "self", ".", "noise_range", "[", "0", "]", "<=", "epoch", "<", "self", ".", "noise_range", "[", "1", "]", "\n", "", "else", ":", "\n", "                    ", "apply_noise", "=", "epoch", ">=", "self", ".", "noise_range", "\n", "", "if", "apply_noise", ":", "\n", "                    ", "self", ".", "_apply_noise", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.plateau_lr.PlateauLRScheduler._apply_noise": [[93, 114], ["torch.Generator", "torch.Generator.manual_seed", "enumerate", "float", "restore_lr.append", "torch.randn().item", "abs", "torch.randn", "torch.rand().item", "torch.rand"], "methods", ["None"], ["", "", "", "", "def", "_apply_noise", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "noise_seed", "+", "epoch", ")", "\n", "if", "self", ".", "noise_type", "==", "'normal'", ":", "\n", "            ", "while", "True", ":", "\n", "# resample if noise out of percent limit, brute force but shouldn't spin much", "\n", "                ", "noise", "=", "torch", ".", "randn", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "\n", "if", "abs", "(", "noise", ")", "<", "self", ".", "noise_pct", ":", "\n", "                    ", "break", "\n", "", "", "", "else", ":", "\n", "            ", "noise", "=", "2", "*", "(", "torch", ".", "rand", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "-", "0.5", ")", "*", "self", ".", "noise_pct", "\n", "\n", "# apply the noise on top of previous LR, cache the old value so we can restore for normal", "\n", "# stepping of base scheduler", "\n", "", "restore_lr", "=", "[", "]", "\n", "for", "i", ",", "param_group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "            ", "old_lr", "=", "float", "(", "param_group", "[", "'lr'", "]", ")", "\n", "restore_lr", ".", "append", "(", "old_lr", ")", "\n", "new_lr", "=", "old_lr", "+", "old_lr", "*", "noise", "\n", "param_group", "[", "'lr'", "]", "=", "new_lr", "\n", "", "self", ".", "restore_lr", "=", "restore_lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler_factory.create_scheduler": [[12, 108], ["dict", "dict", "getattr", "getattr", "isinstance", "cosine_lr.CosineLRScheduler", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "tanh_lr.TanhLRScheduler", "len", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "step_lr.StepLRScheduler", "multistep_lr.MultiStepLRScheduler", "plateau_lr.PlateauLRScheduler", "poly_lr.PolyLRScheduler", "getattr", "poly_lr.PolyLRScheduler.get_cycle_length", "getattr"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length"], ["def", "create_scheduler", "(", "args", ",", "optimizer", ")", ":", "\n", "    ", "num_epochs", "=", "args", ".", "epochs", "\n", "\n", "if", "getattr", "(", "args", ",", "'lr_noise'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "lr_noise", "=", "getattr", "(", "args", ",", "'lr_noise'", ")", "\n", "if", "isinstance", "(", "lr_noise", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "noise_range", "=", "[", "n", "*", "num_epochs", "for", "n", "in", "lr_noise", "]", "\n", "if", "len", "(", "noise_range", ")", "==", "1", ":", "\n", "                ", "noise_range", "=", "noise_range", "[", "0", "]", "\n", "", "", "else", ":", "\n", "            ", "noise_range", "=", "lr_noise", "*", "num_epochs", "\n", "", "", "else", ":", "\n", "        ", "noise_range", "=", "None", "\n", "", "noise_args", "=", "dict", "(", "\n", "noise_range_t", "=", "noise_range", ",", "\n", "noise_pct", "=", "getattr", "(", "args", ",", "'lr_noise_pct'", ",", "0.67", ")", ",", "\n", "noise_std", "=", "getattr", "(", "args", ",", "'lr_noise_std'", ",", "1.", ")", ",", "\n", "noise_seed", "=", "getattr", "(", "args", ",", "'seed'", ",", "42", ")", ",", "\n", ")", "\n", "cycle_args", "=", "dict", "(", "\n", "cycle_mul", "=", "getattr", "(", "args", ",", "'lr_cycle_mul'", ",", "1.", ")", ",", "\n", "cycle_decay", "=", "getattr", "(", "args", ",", "'lr_cycle_decay'", ",", "0.1", ")", ",", "\n", "cycle_limit", "=", "getattr", "(", "args", ",", "'lr_cycle_limit'", ",", "1", ")", ",", "\n", ")", "\n", "\n", "lr_scheduler", "=", "None", "\n", "if", "args", ".", "sched", "==", "'cosine'", ":", "\n", "        ", "lr_scheduler", "=", "CosineLRScheduler", "(", "\n", "optimizer", ",", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "k_decay", "=", "getattr", "(", "args", ",", "'lr_k_decay'", ",", "1.0", ")", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "", "elif", "args", ".", "sched", "==", "'tanh'", ":", "\n", "        ", "lr_scheduler", "=", "TanhLRScheduler", "(", "\n", "optimizer", ",", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "", "elif", "args", ".", "sched", "==", "'step'", ":", "\n", "        ", "lr_scheduler", "=", "StepLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_t", "=", "args", ".", "decay_epochs", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'multistep'", ":", "\n", "        ", "lr_scheduler", "=", "MultiStepLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_t", "=", "args", ".", "decay_epochs", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'plateau'", ":", "\n", "        ", "mode", "=", "'min'", "if", "'loss'", "in", "getattr", "(", "args", ",", "'eval_metric'", ",", "''", ")", "else", "'max'", "\n", "lr_scheduler", "=", "PlateauLRScheduler", "(", "\n", "optimizer", ",", "\n", "decay_rate", "=", "args", ".", "decay_rate", ",", "\n", "patience_t", "=", "args", ".", "patience_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "mode", "=", "mode", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "cooldown_t", "=", "0", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "", "elif", "args", ".", "sched", "==", "'poly'", ":", "\n", "        ", "lr_scheduler", "=", "PolyLRScheduler", "(", "\n", "optimizer", ",", "\n", "power", "=", "args", ".", "decay_rate", ",", "# overloading 'decay_rate' as polynomial power", "\n", "t_initial", "=", "num_epochs", ",", "\n", "lr_min", "=", "args", ".", "min_lr", ",", "\n", "warmup_lr_init", "=", "args", ".", "warmup_lr", ",", "\n", "warmup_t", "=", "args", ".", "warmup_epochs", ",", "\n", "k_decay", "=", "getattr", "(", "args", ",", "'lr_k_decay'", ",", "1.0", ")", ",", "\n", "**", "cycle_args", ",", "\n", "**", "noise_args", ",", "\n", ")", "\n", "num_epochs", "=", "lr_scheduler", ".", "get_cycle_length", "(", ")", "+", "args", ".", "cooldown_epochs", "\n", "\n", "", "return", "lr_scheduler", ",", "num_epochs", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.cosine_lr.CosineLRScheduler.__init__": [[29, 71], ["scheduler.Scheduler.__init__", "_logger.warning", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "k_decay", "=", "1.0", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "if", "t_initial", "==", "1", "and", "cycle_mul", "==", "1", "and", "cycle_decay", "==", "1", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"Cosine annealing scheduler will have no effect on the learning \"", "\n", "\"rate since t_initial = t_mul = eta_mul = 1.\"", ")", "\n", "", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "self", ".", "k_decay", "=", "k_decay", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.cosine_lr.CosineLRScheduler._get_lr": [[72, 101], ["math.floor", "math.log", "math.cos"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "k", "=", "self", ".", "k_decay", "\n", "\n", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "0.5", "*", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "t_curr", "**", "k", "/", "t_i", "**", "k", ")", ")", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.cosine_lr.CosineLRScheduler.get_epoch_values": [[102, 107], ["cosine_lr.CosineLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.cosine_lr.CosineLRScheduler.get_update_values": [[108, 113], ["cosine_lr.CosineLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.cosine_lr.CosineLRScheduler.get_cycle_length": [[114, 120], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.step_lr.StepLRScheduler.__init__": [[17, 45], ["scheduler.Scheduler.__init__", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "decay_t", ":", "float", ",", "\n", "decay_rate", ":", "float", "=", "1.", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "self", ".", "decay_t", "=", "decay_t", "\n", "self", ".", "decay_rate", "=", "decay_rate", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.step_lr.StepLRScheduler._get_lr": [[46, 52], ["None"], "methods", ["None"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "lrs", "=", "[", "v", "*", "(", "self", ".", "decay_rate", "**", "(", "t", "//", "self", ".", "decay_t", ")", ")", "for", "v", "in", "self", ".", "base_values", "]", "\n", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.step_lr.StepLRScheduler.get_epoch_values": [[53, 58], ["step_lr.StepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.step_lr.StepLRScheduler.get_update_values": [[59, 64], ["step_lr.StepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.__init__": [[25, 54], ["scheduler.Scheduler.update_groups", "enumerate", "enumerate", "group.setdefault", "KeyError", "KeyError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "param_group_field", ":", "str", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_type", "=", "'normal'", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "None", ",", "\n", "initialize", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "param_group_field", "=", "param_group_field", "\n", "self", ".", "_initial_param_group_field", "=", "f\"initial_{param_group_field}\"", "\n", "if", "initialize", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "f\"{param_group_field} missing from param_groups[{i}]\"", ")", "\n", "", "group", ".", "setdefault", "(", "self", ".", "_initial_param_group_field", ",", "group", "[", "param_group_field", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "self", ".", "_initial_param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "f\"{self._initial_param_group_field} missing from param_groups[{i}]\"", ")", "\n", "", "", "", "self", ".", "base_values", "=", "[", "group", "[", "self", ".", "_initial_param_group_field", "]", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", "]", "\n", "self", ".", "metric", "=", "None", "# any point to having this for all?", "\n", "self", ".", "noise_range_t", "=", "noise_range_t", "\n", "self", ".", "noise_pct", "=", "noise_pct", "\n", "self", ".", "noise_type", "=", "noise_type", "\n", "self", ".", "noise_std", "=", "noise_std", "\n", "self", ".", "noise_seed", "=", "noise_seed", "if", "noise_seed", "is", "not", "None", "else", "42", "\n", "self", ".", "update_groups", "(", "self", ".", "base_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.state_dict": [[55, 57], ["scheduler.Scheduler.__dict__.items"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "return", "{", "key", ":", "value", "for", "key", ",", "value", "in", "self", ".", "__dict__", ".", "items", "(", ")", "if", "key", "!=", "'optimizer'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.load_state_dict": [[58, 60], ["scheduler.Scheduler.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.get_epoch_values": [[61, 63], ["None"], "methods", ["None"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.get_update_values": [[64, 66], ["None"], "methods", ["None"], ["", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.step": [[67, 73], ["scheduler.Scheduler.get_epoch_values", "scheduler.Scheduler._add_noise", "scheduler.Scheduler.update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_epoch_values", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler._add_noise", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["", "def", "step", "(", "self", ",", "epoch", ":", "int", ",", "metric", ":", "float", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "metric", "=", "metric", "\n", "values", "=", "self", ".", "get_epoch_values", "(", "epoch", ")", "\n", "if", "values", "is", "not", "None", ":", "\n", "            ", "values", "=", "self", ".", "_add_noise", "(", "values", ",", "epoch", ")", "\n", "self", ".", "update_groups", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.step_update": [[74, 80], ["scheduler.Scheduler.get_update_values", "scheduler.Scheduler._add_noise", "scheduler.Scheduler.update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_update_values", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler._add_noise", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["", "", "def", "step_update", "(", "self", ",", "num_updates", ":", "int", ",", "metric", ":", "float", "=", "None", ")", ":", "\n", "        ", "self", ".", "metric", "=", "metric", "\n", "values", "=", "self", ".", "get_update_values", "(", "num_updates", ")", "\n", "if", "values", "is", "not", "None", ":", "\n", "            ", "values", "=", "self", ".", "_add_noise", "(", "values", ",", "num_updates", ")", "\n", "self", ".", "update_groups", "(", "values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups": [[81, 86], ["zip", "isinstance", "len"], "methods", ["None"], ["", "", "def", "update_groups", "(", "self", ",", "values", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "values", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "values", "=", "[", "values", "]", "*", "len", "(", "self", ".", "optimizer", ".", "param_groups", ")", "\n", "", "for", "param_group", ",", "value", "in", "zip", "(", "self", ".", "optimizer", ".", "param_groups", ",", "values", ")", ":", "\n", "            ", "param_group", "[", "self", ".", "param_group_field", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler._add_noise": [[87, 106], ["isinstance", "torch.Generator", "torch.Generator.manual_seed", "torch.randn().item", "abs", "torch.randn", "torch.rand().item", "torch.rand"], "methods", ["None"], ["", "", "def", "_add_noise", "(", "self", ",", "lrs", ",", "t", ")", ":", "\n", "        ", "if", "self", ".", "noise_range_t", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "noise_range_t", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "                ", "apply_noise", "=", "self", ".", "noise_range_t", "[", "0", "]", "<=", "t", "<", "self", ".", "noise_range_t", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "apply_noise", "=", "t", ">=", "self", ".", "noise_range_t", "\n", "", "if", "apply_noise", ":", "\n", "                ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "noise_seed", "+", "t", ")", "\n", "if", "self", ".", "noise_type", "==", "'normal'", ":", "\n", "                    ", "while", "True", ":", "\n", "# resample if noise out of percent limit, brute force but shouldn't spin much", "\n", "                        ", "noise", "=", "torch", ".", "randn", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "\n", "if", "abs", "(", "noise", ")", "<", "self", ".", "noise_pct", ":", "\n", "                            ", "break", "\n", "", "", "", "else", ":", "\n", "                    ", "noise", "=", "2", "*", "(", "torch", ".", "rand", "(", "1", ",", "generator", "=", "g", ")", ".", "item", "(", ")", "-", "0.5", ")", "*", "self", ".", "noise_pct", "\n", "", "lrs", "=", "[", "v", "+", "v", "*", "noise", "for", "v", "in", "lrs", "]", "\n", "", "", "return", "lrs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler.__init__": [[14, 42], ["timm.scheduler.scheduler.Scheduler.__init__", "super().update_groups"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "decay_t", ":", "List", "[", "int", "]", ",", "\n", "decay_rate", ":", "float", "=", "1.", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "self", ".", "decay_t", "=", "decay_t", "\n", "self", ".", "decay_rate", "=", "decay_rate", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "self", ".", "base_values", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler.get_curr_decay_steps": [[43, 47], ["bisect.bisect_right"], "methods", ["None"], ["", "", "def", "get_curr_decay_steps", "(", "self", ",", "t", ")", ":", "\n", "# find where in the array t goes,", "\n", "# assumes self.decay_t is sorted", "\n", "        ", "return", "bisect", ".", "bisect_right", "(", "self", ".", "decay_t", ",", "t", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler._get_lr": [[48, 54], ["multistep_lr.MultiStepLRScheduler.get_curr_decay_steps"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler.get_curr_decay_steps"], ["", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "lrs", "=", "[", "v", "*", "(", "self", ".", "decay_rate", "**", "self", ".", "get_curr_decay_steps", "(", "t", ")", ")", "for", "v", "in", "self", ".", "base_values", "]", "\n", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler.get_epoch_values": [[55, 60], ["multistep_lr.MultiStepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.multistep_lr.MultiStepLRScheduler.get_update_values": [[61, 66], ["multistep_lr.MultiStepLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.__init__": [[24, 70], ["scheduler.Scheduler.__init__", "super().update_groups", "tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.scheduler.Scheduler.update_groups", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "lb", ":", "float", "=", "-", "7.", ",", "\n", "ub", ":", "float", "=", "3.", ",", "\n", "lr_min", ":", "float", "=", "0.", ",", "\n", "cycle_mul", ":", "float", "=", "1.", ",", "\n", "cycle_decay", ":", "float", "=", "1.", ",", "\n", "cycle_limit", ":", "int", "=", "1", ",", "\n", "warmup_t", "=", "0", ",", "\n", "warmup_lr_init", "=", "0", ",", "\n", "warmup_prefix", "=", "False", ",", "\n", "t_in_epochs", "=", "True", ",", "\n", "noise_range_t", "=", "None", ",", "\n", "noise_pct", "=", "0.67", ",", "\n", "noise_std", "=", "1.0", ",", "\n", "noise_seed", "=", "42", ",", "\n", "initialize", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "optimizer", ",", "param_group_field", "=", "\"lr\"", ",", "\n", "noise_range_t", "=", "noise_range_t", ",", "noise_pct", "=", "noise_pct", ",", "noise_std", "=", "noise_std", ",", "noise_seed", "=", "noise_seed", ",", "\n", "initialize", "=", "initialize", ")", "\n", "\n", "assert", "t_initial", ">", "0", "\n", "assert", "lr_min", ">=", "0", "\n", "assert", "lb", "<", "ub", "\n", "assert", "cycle_limit", ">=", "0", "\n", "assert", "warmup_t", ">=", "0", "\n", "assert", "warmup_lr_init", ">=", "0", "\n", "self", ".", "lb", "=", "lb", "\n", "self", ".", "ub", "=", "ub", "\n", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "lr_min", "=", "lr_min", "\n", "self", ".", "cycle_mul", "=", "cycle_mul", "\n", "self", ".", "cycle_decay", "=", "cycle_decay", "\n", "self", ".", "cycle_limit", "=", "cycle_limit", "\n", "self", ".", "warmup_t", "=", "warmup_t", "\n", "self", ".", "warmup_lr_init", "=", "warmup_lr_init", "\n", "self", ".", "warmup_prefix", "=", "warmup_prefix", "\n", "self", ".", "t_in_epochs", "=", "t_in_epochs", "\n", "if", "self", ".", "warmup_t", ":", "\n", "            ", "t_v", "=", "self", ".", "base_values", "if", "self", ".", "warmup_prefix", "else", "self", ".", "_get_lr", "(", "self", ".", "warmup_t", ")", "\n", "self", ".", "warmup_steps", "=", "[", "(", "v", "-", "warmup_lr_init", ")", "/", "self", ".", "warmup_t", "for", "v", "in", "t_v", "]", "\n", "super", "(", ")", ".", "update_groups", "(", "self", ".", "warmup_lr_init", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_steps", "=", "[", "1", "for", "_", "in", "self", ".", "base_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler._get_lr": [[71, 99], ["math.floor", "math.log", "math.tanh"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "", "def", "_get_lr", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "t", "<", "self", ".", "warmup_t", ":", "\n", "            ", "lrs", "=", "[", "self", ".", "warmup_lr_init", "+", "t", "*", "s", "for", "s", "in", "self", ".", "warmup_steps", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "warmup_prefix", ":", "\n", "                ", "t", "=", "t", "-", "self", ".", "warmup_t", "\n", "\n", "", "if", "self", ".", "cycle_mul", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "t", "/", "self", ".", "t_initial", "*", "(", "1", "-", "self", ".", "cycle_mul", ")", ",", "self", ".", "cycle_mul", ")", ")", "\n", "t_i", "=", "self", ".", "cycle_mul", "**", "i", "*", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "1", "-", "self", ".", "cycle_mul", "**", "i", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", "*", "self", ".", "t_initial", "\n", "", "else", ":", "\n", "                ", "i", "=", "t", "//", "self", ".", "t_initial", "\n", "t_i", "=", "self", ".", "t_initial", "\n", "t_curr", "=", "t", "-", "(", "self", ".", "t_initial", "*", "i", ")", "\n", "\n", "", "if", "i", "<", "self", ".", "cycle_limit", ":", "\n", "                ", "gamma", "=", "self", ".", "cycle_decay", "**", "i", "\n", "lr_max_values", "=", "[", "v", "*", "gamma", "for", "v", "in", "self", ".", "base_values", "]", "\n", "\n", "tr", "=", "t_curr", "/", "t_i", "\n", "lrs", "=", "[", "\n", "self", ".", "lr_min", "+", "0.5", "*", "(", "lr_max", "-", "self", ".", "lr_min", ")", "*", "(", "1", "-", "math", ".", "tanh", "(", "self", ".", "lb", "*", "(", "1.", "-", "tr", ")", "+", "self", ".", "ub", "*", "tr", ")", ")", "\n", "for", "lr_max", "in", "lr_max_values", "\n", "]", "\n", "", "else", ":", "\n", "                ", "lrs", "=", "[", "self", ".", "lr_min", "for", "_", "in", "self", ".", "base_values", "]", "\n", "", "", "return", "lrs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_epoch_values": [[100, 105], ["tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "def", "get_epoch_values", "(", "self", ",", "epoch", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_update_values": [[106, 111], ["tanh_lr.TanhLRScheduler._get_lr"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr"], ["", "", "def", "get_update_values", "(", "self", ",", "num_updates", ":", "int", ")", ":", "\n", "        ", "if", "not", "self", ".", "t_in_epochs", ":", "\n", "            ", "return", "self", ".", "_get_lr", "(", "num_updates", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.tanh_lr.TanhLRScheduler.get_cycle_length": [[112, 118], ["max", "int", "math.floor"], "methods", ["None"], ["", "", "def", "get_cycle_length", "(", "self", ",", "cycles", "=", "0", ")", ":", "\n", "        ", "cycles", "=", "max", "(", "1", ",", "cycles", "or", "self", ".", "cycle_limit", ")", "\n", "if", "self", ".", "cycle_mul", "==", "1.0", ":", "\n", "            ", "return", "self", ".", "t_initial", "*", "cycles", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "math", ".", "floor", "(", "-", "self", ".", "t_initial", "*", "(", "self", ".", "cycle_mul", "**", "cycles", "-", "1", ")", "/", "(", "1", "-", "self", ".", "cycle_mul", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.lr_updater.TINLrUpdaterHook.__init__": [[9, 12], ["mmcv.runner.LrUpdaterHook.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "min_lr", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "min_lr", "=", "min_lr", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.lr_updater.TINLrUpdaterHook.get_warmup_lr": [[13, 26], ["None"], "methods", ["None"], ["", "def", "get_warmup_lr", "(", "self", ",", "cur_iters", ")", ":", "\n", "        ", "if", "self", ".", "warmup", "==", "'linear'", ":", "\n", "# 'linear' warmup is rewritten according to TIN repo:", "\n", "# https://github.com/deepcs233/TIN/blob/master/main.py#L409-L412", "\n", "            ", "k", "=", "(", "cur_iters", "/", "self", ".", "warmup_iters", ")", "*", "(", "\n", "1", "-", "self", ".", "warmup_ratio", ")", "+", "self", ".", "warmup_ratio", "\n", "warmup_lr", "=", "[", "_lr", "*", "k", "for", "_lr", "in", "self", ".", "regular_lr", "]", "\n", "", "elif", "self", ".", "warmup", "==", "'constant'", ":", "\n", "            ", "warmup_lr", "=", "[", "_lr", "*", "self", ".", "warmup_ratio", "for", "_lr", "in", "self", ".", "regular_lr", "]", "\n", "", "elif", "self", ".", "warmup", "==", "'exp'", ":", "\n", "            ", "k", "=", "self", ".", "warmup_ratio", "**", "(", "1", "-", "cur_iters", "/", "self", ".", "warmup_iters", ")", "\n", "warmup_lr", "=", "[", "_lr", "*", "k", "for", "_lr", "in", "self", ".", "regular_lr", "]", "\n", "", "return", "warmup_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.scheduler.lr_updater.TINLrUpdaterHook.get_lr": [[27, 41], ["mmcv.runner.hooks.lr_updater.annealing_cos"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ",", "runner", ",", "base_lr", ")", ":", "\n", "        ", "if", "self", ".", "by_epoch", ":", "\n", "            ", "progress", "=", "runner", ".", "epoch", "\n", "max_progress", "=", "runner", ".", "max_epochs", "\n", "", "else", ":", "\n", "            ", "progress", "=", "runner", ".", "iter", "\n", "max_progress", "=", "runner", ".", "max_iters", "\n", "\n", "", "target_lr", "=", "self", ".", "min_lr", "\n", "if", "self", ".", "warmup", "is", "not", "None", ":", "\n", "            ", "progress", "=", "progress", "-", "self", ".", "warmup_iters", "\n", "max_progress", "=", "max_progress", "-", "self", ".", "warmup_iters", "\n", "", "factor", "=", "progress", "/", "max_progress", "\n", "return", "annealing_cos", "(", "base_lr", ",", "target_lr", ",", "factor", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lars.Lars.__init__": [[35, 69], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ",", "\n", "lr", "=", "1.0", ",", "\n", "momentum", "=", "0", ",", "\n", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "\n", "nesterov", "=", "False", ",", "\n", "trust_coeff", "=", "0.001", ",", "\n", "eps", "=", "1e-8", ",", "\n", "trust_clip", "=", "False", ",", "\n", "always_adapt", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid learning rate: {lr}\"", ")", "\n", "", "if", "momentum", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid momentum value: {momentum}\"", ")", "\n", "", "if", "weight_decay", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid weight_decay value: {weight_decay}\"", ")", "\n", "", "if", "nesterov", "and", "(", "momentum", "<=", "0", "or", "dampening", "!=", "0", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nesterov momentum requires a momentum and zero dampening\"", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "\n", "momentum", "=", "momentum", ",", "\n", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "\n", "trust_coeff", "=", "trust_coeff", ",", "\n", "eps", "=", "eps", ",", "\n", "trust_clip", "=", "trust_clip", ",", "\n", "always_adapt", "=", "always_adapt", ",", "\n", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lars.Lars.__setstate__": [[70, 74], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "\"nesterov\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lars.Lars.step": [[75, 136], ["torch.no_grad", "torch.tensor", "torch.enable_grad", "closure", "p.add_", "p.norm", "grad.add.add.norm", "torch.where", "grad.add.add.add", "grad.add.add.mul_", "torch.where", "torch.minimum", "torch.clone().detach", "torch.clone().detach.mul_().add_", "grad.add.add.add", "torch.clone", "torch.clone().detach.mul_"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "device", "=", "self", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "device", "\n", "one_tensor", "=", "torch", ".", "tensor", "(", "1.0", ",", "device", "=", "device", ")", "# because torch.where doesn't handle scalars correctly", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "trust_coeff", "=", "group", "[", "'trust_coeff'", "]", "\n", "eps", "=", "group", "[", "'eps'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "\n", "# apply LARS LR adaptation, LARC clipping, weight decay", "\n", "# ref: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py", "\n", "if", "weight_decay", "!=", "0", "or", "group", "[", "'always_adapt'", "]", ":", "\n", "                    ", "w_norm", "=", "p", ".", "norm", "(", "2.0", ")", "\n", "g_norm", "=", "grad", ".", "norm", "(", "2.0", ")", "\n", "trust_ratio", "=", "trust_coeff", "*", "w_norm", "/", "(", "g_norm", "+", "w_norm", "*", "weight_decay", "+", "eps", ")", "\n", "# FIXME nested where required since logical and/or not working in PT XLA", "\n", "trust_ratio", "=", "torch", ".", "where", "(", "\n", "w_norm", ">", "0", ",", "\n", "torch", ".", "where", "(", "g_norm", ">", "0", ",", "trust_ratio", ",", "one_tensor", ")", ",", "\n", "one_tensor", ",", "\n", ")", "\n", "if", "group", "[", "'trust_clip'", "]", ":", "\n", "                        ", "trust_ratio", "=", "torch", ".", "minimum", "(", "trust_ratio", "/", "group", "[", "'lr'", "]", ",", "one_tensor", ")", "\n", "", "grad", ".", "add", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "grad", ".", "mul_", "(", "trust_ratio", ")", "\n", "\n", "# apply SGD update https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100", "\n", "", "if", "momentum", "!=", "0", ":", "\n", "                    ", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                        ", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "clone", "(", "grad", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                        ", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "dampening", ")", "\n", "", "if", "nesterov", ":", "\n", "                        ", "grad", "=", "grad", ".", "add", "(", "buf", ",", "alpha", "=", "momentum", ")", "\n", "", "else", ":", "\n", "                        ", "grad", "=", "buf", "\n", "\n", "", "", "p", ".", "add_", "(", "grad", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.madgrad.MADGRAD.__init__": [[55, 76], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "params", ":", "_params_t", ",", "\n", "lr", ":", "float", "=", "1e-2", ",", "\n", "momentum", ":", "float", "=", "0.9", ",", "\n", "weight_decay", ":", "float", "=", "0", ",", "\n", "eps", ":", "float", "=", "1e-6", ",", "\n", "decoupled_decay", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "momentum", "<", "0", "or", "momentum", ">=", "1", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Momentum {momentum} must be in the range [0,1]\"", ")", "\n", "", "if", "lr", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Learning rate {lr} must be positive\"", ")", "\n", "", "if", "weight_decay", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Weight decay {weight_decay} must be non-negative\"", ")", "\n", "", "if", "eps", "<", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Eps must be non-negative\"", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "momentum", "=", "momentum", ",", "weight_decay", "=", "weight_decay", ",", "decoupled_decay", "=", "decoupled_decay", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.madgrad.MADGRAD.supports_memory_efficient_fp16": [[77, 80], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.madgrad.MADGRAD.supports_flat_params": [[81, 84], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_flat_params", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.madgrad.MADGRAD.step": [[85, 185], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "math.sqrt", "grad.coalesce.coalesce.coalesce", "grad.coalesce.coalesce._values", "p.sparse_mask", "grad_sum_sq.sparse_mask", "s.sparse_mask", "grad_sum_sq.sparse_mask._values().pow().add_", "p.sparse_mask._values().addcdiv", "grad_sum_sq.add_", "grad_sum_sq.sparse_mask.add_", "grad_sum_sq.sparse_mask._values().pow_().add_", "s.add_", "s.sparse_mask._values().add_", "p.sparse_mask._values().addcdiv.addcdiv", "p.sparse_mask._values().add_", "p.add_", "grad_sum_sq.addcmul_", "grad_sum_sq.pow().add_", "s.add_", "torch.clone().detach", "torch.clone().detach", "torch.clone().detach", "torch.clone().detach", "p.mul_", "grad.coalesce.coalesce.add_", "s.sparse_mask._values", "s.sparse_mask._values", "grad_sum_sq.pow().add_", "p.addcdiv", "p.copy_", "p.addcdiv.addcdiv", "p.mul_().add_", "RuntimeError", "grad_sum_sq.sparse_mask._values().pow", "p.sparse_mask._values", "grad_sum_sq.sparse_mask._values().pow_", "s.sparse_mask._values", "p.sparse_mask._values", "grad_sum_sq.pow", "p.addcdiv.addcdiv", "torch.clone", "torch.clone", "torch.clone", "torch.clone", "grad_sum_sq.pow", "p.mul_", "grad_sum_sq.sparse_mask._values", "grad_sum_sq.sparse_mask._values"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "float", "]", "]", "=", "None", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "eps", "=", "group", "[", "'eps'", "]", "\n", "lr", "=", "group", "[", "'lr'", "]", "+", "eps", "\n", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "ck", "=", "1", "-", "momentum", "\n", "\n", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "momentum", "!=", "0.0", "and", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"momentum != 0 is not compatible with sparse gradients\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'grad_sum_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'s'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "momentum", "!=", "0", ":", "\n", "                        ", "state", "[", "'x0'", "]", "=", "torch", ".", "clone", "(", "p", ")", ".", "detach", "(", ")", "\n", "\n", "", "", "state", "[", "'step'", "]", "+=", "1", "\n", "grad_sum_sq", "=", "state", "[", "'grad_sum_sq'", "]", "\n", "s", "=", "state", "[", "'s'", "]", "\n", "lamb", "=", "lr", "*", "math", ".", "sqrt", "(", "state", "[", "'step'", "]", ")", "\n", "\n", "# Apply weight decay", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                        ", "p", ".", "mul_", "(", "1.0", "-", "group", "[", "'lr'", "]", "*", "weight_decay", ")", "\n", "", "else", ":", "\n", "                        ", "if", "grad", ".", "is_sparse", ":", "\n", "                            ", "raise", "RuntimeError", "(", "\"weight_decay option is not compatible with sparse gradients\"", ")", "\n", "", "grad", ".", "add_", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "\n", "", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "grad", "=", "grad", ".", "coalesce", "(", ")", "\n", "grad_val", "=", "grad", ".", "_values", "(", ")", "\n", "\n", "p_masked", "=", "p", ".", "sparse_mask", "(", "grad", ")", "\n", "grad_sum_sq_masked", "=", "grad_sum_sq", ".", "sparse_mask", "(", "grad", ")", "\n", "s_masked", "=", "s", ".", "sparse_mask", "(", "grad", ")", "\n", "\n", "# Compute x_0 from other known quantities", "\n", "rms_masked_vals", "=", "grad_sum_sq_masked", ".", "_values", "(", ")", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "x0_masked_vals", "=", "p_masked", ".", "_values", "(", ")", ".", "addcdiv", "(", "s_masked", ".", "_values", "(", ")", ",", "rms_masked_vals", ",", "value", "=", "1", ")", "\n", "\n", "# Dense + sparse op", "\n", "grad_sq", "=", "grad", "*", "grad", "\n", "grad_sum_sq", ".", "add_", "(", "grad_sq", ",", "alpha", "=", "lamb", ")", "\n", "grad_sum_sq_masked", ".", "add_", "(", "grad_sq", ",", "alpha", "=", "lamb", ")", "\n", "\n", "rms_masked_vals", "=", "grad_sum_sq_masked", ".", "_values", "(", ")", ".", "pow_", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "\n", "s", ".", "add_", "(", "grad", ",", "alpha", "=", "lamb", ")", "\n", "s_masked", ".", "_values", "(", ")", ".", "add_", "(", "grad_val", ",", "alpha", "=", "lamb", ")", "\n", "\n", "# update masked copy of p", "\n", "p_kp1_masked_vals", "=", "x0_masked_vals", ".", "addcdiv", "(", "s_masked", ".", "_values", "(", ")", ",", "rms_masked_vals", ",", "value", "=", "-", "1", ")", "\n", "# Copy updated masked p to dense p using an add operation", "\n", "p_masked", ".", "_values", "(", ")", ".", "add_", "(", "p_kp1_masked_vals", ",", "alpha", "=", "-", "1", ")", "\n", "p", ".", "add_", "(", "p_masked", ",", "alpha", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                    ", "if", "momentum", "==", "0", ":", "\n", "# Compute x_0 from other known quantities", "\n", "                        ", "rms", "=", "grad_sum_sq", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "x0", "=", "p", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "x0", "=", "state", "[", "'x0'", "]", "\n", "\n", "# Accumulate second moments", "\n", "", "grad_sum_sq", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "lamb", ")", "\n", "rms", "=", "grad_sum_sq", ".", "pow", "(", "1", "/", "3", ")", ".", "add_", "(", "eps", ")", "\n", "\n", "# Update s", "\n", "s", ".", "add_", "(", "grad", ",", "alpha", "=", "lamb", ")", "\n", "\n", "# Step", "\n", "if", "momentum", "==", "0", ":", "\n", "                        ", "p", ".", "copy_", "(", "x0", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "z", "=", "x0", ".", "addcdiv", "(", "s", ",", "rms", ",", "value", "=", "-", "1", ")", "\n", "\n", "# p is a moving average of z", "\n", "p", ".", "mul_", "(", "1", "-", "ck", ")", ".", "add_", "(", "z", ",", "alpha", "=", "ck", ")", "\n", "\n", "", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.__init__": [[26, 53], ["torch.Generator().manual_seed", "dict", "super().__init__", "adahessian.Adahessian.get_params", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "torch.Generator"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "0.1", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.0", ",", "\n", "hessian_power", "=", "1.0", ",", "update_each", "=", "1", ",", "n_samples", "=", "1", ",", "avg_conv_kernel", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid learning rate: {lr}\"", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid epsilon value: {eps}\"", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid beta parameter at index 0: {betas[0]}\"", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid beta parameter at index 1: {betas[1]}\"", ")", "\n", "", "if", "not", "0.0", "<=", "hessian_power", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f\"Invalid Hessian power value: {hessian_power}\"", ")", "\n", "\n", "", "self", ".", "n_samples", "=", "n_samples", "\n", "self", ".", "update_each", "=", "update_each", "\n", "self", ".", "avg_conv_kernel", "=", "avg_conv_kernel", "\n", "\n", "# use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training", "\n", "self", ".", "seed", "=", "2147483647", "\n", "self", ".", "generator", "=", "torch", ".", "Generator", "(", ")", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "hessian_power", "=", "hessian_power", ")", "\n", "super", "(", "Adahessian", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n", "for", "p", "in", "self", ".", "get_params", "(", ")", ":", "\n", "            ", "p", ".", "hess", "=", "0.0", "\n", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.is_second_order": [[54, 57], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "is_second_order", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.get_params": [[58, 64], ["None"], "methods", ["None"], ["", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Gets all parameters in all param_groups with gradients\n        \"\"\"", "\n", "\n", "return", "(", "p", "for", "group", "in", "self", ".", "param_groups", "for", "p", "in", "group", "[", "'params'", "]", "if", "p", ".", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.zero_hessian": [[65, 73], ["adahessian.Adahessian.get_params", "p.hess.zero_", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "def", "zero_hessian", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Zeros out the accumalated hessian traces.\n        \"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "get_params", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "p", ".", "hess", ",", "float", ")", "and", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "%", "self", ".", "update_each", "==", "0", ":", "\n", "                ", "p", ".", "hess", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.set_hessian": [[74, 101], ["torch.no_grad", "filter", "range", "adahessian.Adahessian.get_params", "len", "torch.Generator().manual_seed", "torch.autograd.grad", "zip", "params.append", "torch.Generator", "torch.randint", "p.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "set_hessian", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n        \"\"\"", "\n", "\n", "params", "=", "[", "]", "\n", "for", "p", "in", "filter", "(", "lambda", "p", ":", "p", ".", "grad", "is", "not", "None", ",", "self", ".", "get_params", "(", ")", ")", ":", "\n", "            ", "if", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "%", "self", ".", "update_each", "==", "0", ":", "# compute the trace only each `update_each` step", "\n", "                ", "params", ".", "append", "(", "p", ")", "\n", "", "self", ".", "state", "[", "p", "]", "[", "\"hessian step\"", "]", "+=", "1", "\n", "\n", "", "if", "len", "(", "params", ")", "==", "0", ":", "\n", "            ", "return", "\n", "\n", "", "if", "self", ".", "generator", ".", "device", "!=", "params", "[", "0", "]", ".", "device", ":", "# hackish way of casting the generator to the right device", "\n", "            ", "self", ".", "generator", "=", "torch", ".", "Generator", "(", "params", "[", "0", "]", ".", "device", ")", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n", "", "grads", "=", "[", "p", ".", "grad", "for", "p", "in", "params", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "n_samples", ")", ":", "\n", "# Rademacher distribution {-1.0, 1.0}", "\n", "            ", "zs", "=", "[", "torch", ".", "randint", "(", "0", ",", "2", ",", "p", ".", "size", "(", ")", ",", "generator", "=", "self", ".", "generator", ",", "device", "=", "p", ".", "device", ")", "*", "2.0", "-", "1.0", "for", "p", "in", "params", "]", "\n", "h_zs", "=", "torch", ".", "autograd", ".", "grad", "(", "\n", "grads", ",", "params", ",", "grad_outputs", "=", "zs", ",", "only_inputs", "=", "True", ",", "retain_graph", "=", "i", "<", "self", ".", "n_samples", "-", "1", ")", "\n", "for", "h_z", ",", "z", ",", "p", "in", "zip", "(", "h_zs", ",", "zs", ",", "params", ")", ":", "\n", "                ", "p", ".", "hess", "+=", "h_z", "*", "z", "/", "self", ".", "n_samples", "# approximate the expected values of z*(H@z)", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.step": [[102, 157], ["torch.no_grad", "adahessian.Adahessian.zero_hessian", "adahessian.Adahessian.set_hessian", "closure", "p.mul_", "exp_avg.mul_().add_", "exp_hessian_diag_sq.mul_().addcmul_", "p.addcdiv_", "torch.abs().mean().expand_as().clone", "len", "torch.zeros_like", "torch.zeros_like", "p.dim", "exp_avg.mul_", "exp_hessian_diag_sq.mul_", "torch.abs().mean().expand_as", "torch.abs().mean", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.zero_hessian", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adahessian.Adahessian.set_hessian"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Performs a single optimization step.\n        Arguments:\n            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n        \"\"\"", "\n", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "self", ".", "zero_hessian", "(", ")", "\n", "self", ".", "set_hessian", "(", ")", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", "or", "p", ".", "hess", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "avg_conv_kernel", "and", "p", ".", "dim", "(", ")", "==", "4", ":", "\n", "                    ", "p", ".", "hess", "=", "torch", ".", "abs", "(", "p", ".", "hess", ")", ".", "mean", "(", "dim", "=", "[", "2", ",", "3", "]", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "p", ".", "hess", ")", ".", "clone", "(", ")", "\n", "\n", "# Perform correct stepweight decay as in AdamW", "\n", "", "p", ".", "mul_", "(", "1", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "1", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of Hessian diagonal square values", "\n", "state", "[", "'exp_hessian_diag_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "exp_avg", ",", "exp_hessian_diag_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_hessian_diag_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "p", ".", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_hessian_diag_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "p", ".", "hess", ",", "p", ".", "hess", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "k", "=", "group", "[", "'hessian_power'", "]", "\n", "denom", "=", "(", "exp_hessian_diag_sq", "/", "bias_correction2", ")", ".", "pow_", "(", "k", "/", "2", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "# make update", "\n", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adabelief.AdaBelief.__init__": [[42, 65], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "len", "range", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-16", ",", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ",", "\n", "decoupled_decay", "=", "True", ",", "fixed_decay", "=", "False", ",", "rectify", "=", "True", ",", "degenerated_to_sgd", "=", "True", ")", ":", "\n", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "\n", "", "if", "isinstance", "(", "params", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "params", ")", ">", "0", "and", "isinstance", "(", "params", "[", "0", "]", ",", "dict", ")", ":", "\n", "            ", "for", "param", "in", "params", ":", "\n", "                ", "if", "'betas'", "in", "param", "and", "(", "param", "[", "'betas'", "]", "[", "0", "]", "!=", "betas", "[", "0", "]", "or", "param", "[", "'betas'", "]", "[", "1", "]", "!=", "betas", "[", "1", "]", ")", ":", "\n", "                    ", "param", "[", "'buffer'", "]", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", "\n", "\n", "", "", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ",", "\n", "degenerated_to_sgd", "=", "degenerated_to_sgd", ",", "decoupled_decay", "=", "decoupled_decay", ",", "rectify", "=", "rectify", ",", "\n", "fixed_decay", "=", "fixed_decay", ",", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", ")", "\n", "super", "(", "AdaBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adabelief.AdaBelief.__setstate__": [[66, 70], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdaBelief", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adabelief.AdaBelief.reset": [[71, 88], ["torch.no_grad", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "# State initialization", "\n", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                    ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adabelief.AdaBelief.step": [[89, 202], ["torch.no_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "grad.float.float.float", "RuntimeError", "p_fp32.float.float.float", "len", "torch.zeros_like", "torch.zeros_like", "torch.max", "p_fp32.float.float.addcdiv_", "p.copy_", "torch.zeros_like", "p_fp32.float.float.mul_", "p_fp32.float.float.mul_", "grad.float.float.add_", "exp_avg.mul_", "exp_avg_var.mul_", "exp_avg_var.add_", "exp_avg_var.sqrt().add_", "p_fp32.float.float.addcdiv_", "int", "p_fp32.float.float.add_", "max_exp_avg_var.sqrt", "math.sqrt", "exp_avg_var.add_().sqrt", "math.sqrt", "math.sqrt", "exp_avg_var.sqrt", "exp_avg_var.add_"], "methods", ["None"], ["", "", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "grad", "=", "grad", ".", "float", "(", ")", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'AdaBelief does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "p_fp32", "=", "p", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p_fp32", "=", "p_fp32", ".", "float", "(", ")", "\n", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "\n", "# perform weight decay, check if decoupled weight decay", "\n", "", "", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                    ", "if", "not", "group", "[", "'fixed_decay'", "]", ":", "\n", "                        ", "p_fp32", ".", "mul_", "(", "1.0", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "p_fp32", ".", "mul_", "(", "1.0", "-", "group", "[", "'weight_decay'", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                        ", "grad", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# get current state variable", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "# Update first and second moment running average", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "grad_residual", "=", "grad", "-", "exp_avg", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad_residual", ",", "grad_residual", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ".", "add_", "(", "group", "[", "'eps'", "]", ")", ",", "out", "=", "max_exp_avg_var", ")", "\n", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "(", "max_exp_avg_var", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "(", "exp_avg_var", ".", "add_", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "# update", "\n", "", "if", "not", "group", "[", "'rectify'", "]", ":", "\n", "# Default update", "\n", "                    ", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "", "else", ":", "\n", "# Rectified update, forked from RAdam", "\n", "                    ", "buffered", "=", "group", "[", "'buffer'", "]", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                        ", "num_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                        ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "num_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "num_sma", "=", "num_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "num_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "num_sma", ">=", "5", ":", "\n", "                            ", "step_size", "=", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "*", "\n", "(", "num_sma", "-", "4", ")", "/", "(", "num_sma_max", "-", "4", ")", "*", "\n", "(", "num_sma", "-", "2", ")", "/", "num_sma", "*", "\n", "num_sma_max", "/", "(", "num_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "elif", "group", "[", "'degenerated_to_sgd'", "]", ":", "\n", "                            ", "step_size", "=", "1.0", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                            ", "step_size", "=", "-", "1", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "num_sma", ">=", "5", ":", "\n", "                        ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "group", "[", "'lr'", "]", ")", "\n", "", "elif", "step_size", ">", "0", ":", "\n", "                        ", "p_fp32", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", "*", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamw.AdamW.__init__": [[39, 52], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "1e-2", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamw.AdamW.__setstate__": [[53, 57], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamw.AdamW.step": [[58, 123], ["torch.no_grad", "torch.enable_grad", "closure", "p.data.mul_", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.max", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "math.sqrt", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "# Perform stepweight decay", "\n", "", "p", ".", "data", ".", "mul_", "(", "1", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# Perform optimization step", "\n", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "(", "max_exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.sgdp.SGDP.__init__": [[20, 26], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "momentum", "=", "0", ",", "dampening", "=", "0", ",", "\n", "weight_decay", "=", "0", ",", "nesterov", "=", "False", ",", "eps", "=", "1e-8", ",", "delta", "=", "0.1", ",", "wd_ratio", "=", "0.1", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "dampening", "=", "dampening", ",", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "eps", "=", "eps", ",", "delta", "=", "delta", ",", "wd_ratio", "=", "wd_ratio", ")", "\n", "super", "(", "SGDP", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.sgdp.SGDP.step": [[27, 71], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "buf.mul_().add_", "p.add_", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "adamp.projection", "p.mul_", "buf.mul_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.projection"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'momentum'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# SGD", "\n", "", "buf", "=", "state", "[", "'momentum'", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "dampening", ")", "\n", "if", "nesterov", ":", "\n", "                    ", "d_p", "=", "grad", "+", "momentum", "*", "buf", "\n", "", "else", ":", "\n", "                    ", "d_p", "=", "buf", "\n", "\n", "# Projection", "\n", "", "wd_ratio", "=", "1.", "\n", "if", "len", "(", "p", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "d_p", ",", "wd_ratio", "=", "projection", "(", "p", ",", "grad", ",", "d_p", ",", "group", "[", "'delta'", "]", ",", "group", "[", "'wd_ratio'", "]", ",", "group", "[", "'eps'", "]", ")", "\n", "\n", "# Weight decay", "\n", "", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", "*", "wd_ratio", "/", "(", "1", "-", "momentum", ")", ")", "\n", "\n", "# Step", "\n", "", "p", ".", "add_", "(", "d_p", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.nvnovograd.NvNovoGrad.__init__": [[32, 48], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.95", ",", "0.98", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "grad_averaging", "=", "False", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "grad_averaging", "=", "grad_averaging", ",", "\n", "amsgrad", "=", "amsgrad", ")", "\n", "\n", "super", "(", "NvNovoGrad", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.nvnovograd.NvNovoGrad.__setstate__": [[49, 53], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "NvNovoGrad", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.nvnovograd.NvNovoGrad.step": [[54, 121], ["torch.no_grad", "torch.enable_grad", "closure", "torch.sum", "grad.div_", "exp_avg.mul_().add_", "p.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros().to", "torch.pow", "exp_avg_sq.copy_", "exp_avg_sq.mul_().add_", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "grad.add_", "grad.mul_", "torch.zeros().to", "exp_avg.mul_", "torch.zeros", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt", "torch.zeros"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Sparse gradients are not supported.'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros", "(", "[", "]", ")", ".", "to", "(", "state", "[", "'exp_avg'", "]", ".", "device", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros", "(", "[", "]", ")", ".", "to", "(", "state", "[", "'exp_avg'", "]", ".", "device", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "norm", "=", "torch", ".", "sum", "(", "torch", ".", "pow", "(", "grad", ",", "2", ")", ")", "\n", "\n", "if", "exp_avg_sq", "==", "0", ":", "\n", "                    ", "exp_avg_sq", ".", "copy_", "(", "norm", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "add_", "(", "norm", ",", "alpha", "=", "1", "-", "beta2", ")", "\n", "\n", "", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "grad", ".", "div_", "(", "denom", ")", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "grad", ".", "add_", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "", "if", "group", "[", "'grad_averaging'", "]", ":", "\n", "                    ", "grad", ".", "mul_", "(", "1", "-", "beta1", ")", "\n", "", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ")", "\n", "\n", "p", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.nadam.Nadam.__init__": [[30, 37], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "2e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "schedule_decay", "=", "4e-3", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "schedule_decay", "=", "schedule_decay", ")", "\n", "super", "(", "Nadam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.nadam.Nadam.step": [[38, 93], ["torch.no_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.addcdiv_", "p.addcdiv_", "len", "torch.zeros_like", "torch.zeros_like", "grad.add.add.add", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'m_schedule'", "]", "=", "1.", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Warming momentum schedule", "\n", "", "m_schedule", "=", "state", "[", "'m_schedule'", "]", "\n", "schedule_decay", "=", "group", "[", "'schedule_decay'", "]", "\n", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "eps", "=", "group", "[", "'eps'", "]", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "t", "=", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "t", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "grad", "=", "grad", ".", "add", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "momentum_cache_t", "=", "beta1", "*", "(", "1.", "-", "0.5", "*", "(", "0.96", "**", "(", "t", "*", "schedule_decay", ")", ")", ")", "\n", "momentum_cache_t_1", "=", "beta1", "*", "(", "1.", "-", "0.5", "*", "(", "0.96", "**", "(", "(", "t", "+", "1", ")", "*", "schedule_decay", ")", ")", ")", "\n", "m_schedule_new", "=", "m_schedule", "*", "momentum_cache_t", "\n", "m_schedule_next", "=", "m_schedule", "*", "momentum_cache_t", "*", "momentum_cache_t_1", "\n", "state", "[", "'m_schedule'", "]", "=", "m_schedule_new", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "eps", ")", "\n", "p", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "-", "group", "[", "'lr'", "]", "*", "(", "1.", "-", "momentum_cache_t", ")", "/", "(", "1.", "-", "m_schedule_new", ")", ")", "\n", "p", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "group", "[", "'lr'", "]", "*", "momentum_cache_t_1", "/", "(", "1.", "-", "m_schedule_next", ")", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.add_weight_decay": [[31, 44], ["model.named_parameters", "name.endswith", "no_decay.append", "decay.append", "len"], "function", ["None"], ["", "def", "add_weight_decay", "(", "model", ",", "weight_decay", "=", "1e-5", ",", "skip_list", "=", "(", ")", ")", ":", "\n", "    ", "decay", "=", "[", "]", "\n", "no_decay", "=", "[", "]", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "not", "param", ".", "requires_grad", ":", "\n", "            ", "continue", "# frozen weights", "\n", "", "if", "len", "(", "param", ".", "shape", ")", "==", "1", "or", "name", ".", "endswith", "(", "\".bias\"", ")", "or", "name", "in", "skip_list", ":", "\n", "            ", "no_decay", ".", "append", "(", "param", ")", "\n", "", "else", ":", "\n", "            ", "decay", ".", "append", "(", "param", ")", "\n", "", "", "return", "[", "\n", "{", "'params'", ":", "no_decay", ",", "'weight_decay'", ":", "0.", "}", ",", "\n", "{", "'params'", ":", "decay", ",", "'weight_decay'", ":", "weight_decay", "}", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.optimizer_kwargs": [[46, 62], ["dict", "getattr", "getattr", "getattr", "dict.update"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "optimizer_kwargs", "(", "cfg", ")", ":", "\n", "    ", "\"\"\" cfg/argparse to kwargs helper\n    Convert optimizer args in argparse args or cfg like object to keyword args for updated create fn.\n    \"\"\"", "\n", "kwargs", "=", "dict", "(", "\n", "opt", "=", "cfg", ".", "opt", ",", "\n", "lr", "=", "cfg", ".", "lr", ",", "\n", "weight_decay", "=", "cfg", ".", "weight_decay", ",", "\n", "momentum", "=", "cfg", ".", "momentum", ")", "\n", "if", "getattr", "(", "cfg", ",", "'opt_eps'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'eps'", "]", "=", "cfg", ".", "opt_eps", "\n", "", "if", "getattr", "(", "cfg", ",", "'opt_betas'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'betas'", "]", "=", "cfg", ".", "opt_betas", "\n", "", "if", "getattr", "(", "cfg", ",", "'opt_args'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "kwargs", ".", "update", "(", "cfg", ".", "opt_args", ")", "\n", "", "return", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.create_optimizer": [[64, 72], ["optim_factory.create_optimizer_v2", "optim_factory.optimizer_kwargs"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.create_optimizer_v2", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.optimizer_kwargs"], ["", "def", "create_optimizer", "(", "args", ",", "model", ",", "filter_bias_and_bn", "=", "True", ")", ":", "\n", "    ", "\"\"\" Legacy optimizer factory for backwards compatibility.\n    NOTE: Use create_optimizer_v2 for new code.\n    \"\"\"", "\n", "return", "create_optimizer_v2", "(", "\n", "model", ",", "\n", "**", "optimizer_kwargs", "(", "cfg", "=", "args", ")", ",", "\n", "filter_bias_and_bn", "=", "filter_bias_and_bn", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.create_optimizer_v2": [[75, 218], ["isinstance", "opt.lower", "opt.lower.split", "dict", "dict.setdefault", "dict.pop", "torch.SGD", "len", "hasattr", "optim_factory.add_weight_decay", "model_or_params.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "dict.pop", "torch.SGD", "lookahead.Lookahead", "model_or_params.no_weight_decay", "sgdp.SGDP", "torch.Adam", "torch.AdamW", "adamp.AdamP", "torch.Nadam", "radam.RAdam", "nadam.Nadam", "torch.Adamax", "adabelief.AdaBelief", "adabelief.AdaBelief", "torch.Adadelta", "dict.setdefault", "torch.Adagrad", "adafactor.Adafactor", "lamb.Lamb", "lamb.Lamb", "lars.Lars", "lars.Lars", "lars.Lars", "lars.Lars", "madgrad.MADGRAD", "madgrad.MADGRAD", "nvnovograd.NvNovoGrad", "torch.RMSprop", "rmsprop_tf.RMSpropTF", "adahessian.Adahessian", "dict.pop", "FusedSGD", "dict.pop", "FusedSGD", "FusedAdam", "FusedAdam", "FusedLAMB", "dict.setdefault", "FusedNovoGrad"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.optim_factory.add_weight_decay"], ["", "def", "create_optimizer_v2", "(", "\n", "model_or_params", ",", "\n", "opt", ":", "str", "=", "'sgd'", ",", "\n", "lr", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "weight_decay", ":", "float", "=", "0.", ",", "\n", "momentum", ":", "float", "=", "0.9", ",", "\n", "filter_bias_and_bn", ":", "bool", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Create an optimizer.\n\n    TODO currently the model is passed in and all parameters are selected for optimization.\n    For more general use an interface that allows selection of parameters to optimize and lr groups, one of:\n      * a filter fn interface that further breaks params into groups in a weight_decay compatible fashion\n      * expose the parameters interface and leave it up to caller\n\n    Args:\n        model_or_params (nn.Module): model containing parameters to optimize\n        opt: name of optimizer to create\n        lr: initial learning rate\n        weight_decay: weight decay to apply in optimizer\n        momentum:  momentum for momentum based optimizers (others may use betas via kwargs)\n        filter_bias_and_bn:  filter out bias, bn and other 1d params from weight decay\n        **kwargs: extra optimizer specific kwargs to pass through\n\n    Returns:\n        Optimizer\n    \"\"\"", "\n", "if", "isinstance", "(", "model_or_params", ",", "nn", ".", "Module", ")", ":", "\n", "# a model was passed in, extract parameters and add weight decays to appropriate layers", "\n", "        ", "if", "weight_decay", "and", "filter_bias_and_bn", ":", "\n", "            ", "skip", "=", "{", "}", "\n", "if", "hasattr", "(", "model_or_params", ",", "'no_weight_decay'", ")", ":", "\n", "                ", "skip", "=", "model_or_params", ".", "no_weight_decay", "(", ")", "\n", "", "parameters", "=", "add_weight_decay", "(", "model_or_params", ",", "weight_decay", ",", "skip", ")", "\n", "weight_decay", "=", "0.", "\n", "", "else", ":", "\n", "            ", "parameters", "=", "model_or_params", ".", "parameters", "(", ")", "\n", "", "", "else", ":", "\n", "# iterable of parameters or param groups passed in", "\n", "        ", "parameters", "=", "model_or_params", "\n", "\n", "", "opt_lower", "=", "opt", ".", "lower", "(", ")", "\n", "opt_split", "=", "opt_lower", ".", "split", "(", "'_'", ")", "\n", "opt_lower", "=", "opt_split", "[", "-", "1", "]", "\n", "if", "'fused'", "in", "opt_lower", ":", "\n", "        ", "assert", "has_apex", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "'APEX and CUDA required for fused optimizers'", "\n", "\n", "", "opt_args", "=", "dict", "(", "weight_decay", "=", "weight_decay", ",", "**", "kwargs", ")", "\n", "if", "lr", "is", "not", "None", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'lr'", ",", "lr", ")", "\n", "\n", "# basic SGD & related", "\n", "", "if", "opt_lower", "==", "'sgd'", "or", "opt_lower", "==", "'nesterov'", ":", "\n", "# NOTE 'sgd' refers to SGD + nesterov momentum for legacy / backwards compat reasons", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'momentum'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'sgdp'", ":", "\n", "        ", "optimizer", "=", "SGDP", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "\n", "# adaptive", "\n", "", "elif", "opt_lower", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamw'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "AdamW", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamp'", ":", "\n", "        ", "optimizer", "=", "AdamP", "(", "parameters", ",", "wd_ratio", "=", "0.01", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nadam'", ":", "\n", "        ", "try", ":", "\n", "# NOTE PyTorch >= 1.10 should have native NAdam", "\n", "            ", "optimizer", "=", "optim", ".", "Nadam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "except", "AttributeError", ":", "\n", "            ", "optimizer", "=", "Nadam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "", "elif", "opt_lower", "==", "'radam'", ":", "\n", "        ", "optimizer", "=", "RAdam", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adamax'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adamax", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adabelief'", ":", "\n", "        ", "optimizer", "=", "AdaBelief", "(", "parameters", ",", "rectify", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'radabelief'", ":", "\n", "        ", "optimizer", "=", "AdaBelief", "(", "parameters", ",", "rectify", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adadelta'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adadelta", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adagrad'", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'eps'", ",", "1e-8", ")", "\n", "optimizer", "=", "optim", ".", "Adagrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'adafactor'", ":", "\n", "        ", "optimizer", "=", "Adafactor", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lamb'", ":", "\n", "        ", "optimizer", "=", "Lamb", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lambc'", ":", "\n", "        ", "optimizer", "=", "Lamb", "(", "parameters", ",", "trust_clip", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'larc'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "trust_clip", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'lars'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nlarc'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "trust_clip", "=", "True", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'nlars'", ":", "\n", "        ", "optimizer", "=", "Lars", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'madgrad'", ":", "\n", "        ", "optimizer", "=", "MADGRAD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'madgradw'", ":", "\n", "        ", "optimizer", "=", "MADGRAD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "decoupled_decay", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'novograd'", "or", "opt_lower", "==", "'nvnovograd'", ":", "\n", "        ", "optimizer", "=", "NvNovoGrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'rmsprop'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "RMSprop", "(", "parameters", ",", "alpha", "=", "0.9", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'rmsproptf'", ":", "\n", "        ", "optimizer", "=", "RMSpropTF", "(", "parameters", ",", "alpha", "=", "0.9", ",", "momentum", "=", "momentum", ",", "**", "opt_args", ")", "\n", "\n", "# second order", "\n", "", "elif", "opt_lower", "==", "'adahessian'", ":", "\n", "        ", "optimizer", "=", "Adahessian", "(", "parameters", ",", "**", "opt_args", ")", "\n", "\n", "# NVIDIA fused optimizers, require APEX to be installed", "\n", "", "elif", "opt_lower", "==", "'fusedsgd'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "FusedSGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedmomentum'", ":", "\n", "        ", "opt_args", ".", "pop", "(", "'eps'", ",", "None", ")", "\n", "optimizer", "=", "FusedSGD", "(", "parameters", ",", "momentum", "=", "momentum", ",", "nesterov", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedadam'", ":", "\n", "        ", "optimizer", "=", "FusedAdam", "(", "parameters", ",", "adam_w_mode", "=", "False", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedadamw'", ":", "\n", "        ", "optimizer", "=", "FusedAdam", "(", "parameters", ",", "adam_w_mode", "=", "True", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusedlamb'", ":", "\n", "        ", "optimizer", "=", "FusedLAMB", "(", "parameters", ",", "**", "opt_args", ")", "\n", "", "elif", "opt_lower", "==", "'fusednovograd'", ":", "\n", "        ", "opt_args", ".", "setdefault", "(", "'betas'", ",", "(", "0.95", ",", "0.98", ")", ")", "\n", "optimizer", "=", "FusedNovoGrad", "(", "parameters", ",", "**", "opt_args", ")", "\n", "\n", "", "else", ":", "\n", "        ", "assert", "False", "and", "\"Invalid optimizer\"", "\n", "raise", "ValueError", "\n", "\n", "", "if", "len", "(", "opt_split", ")", ">", "1", ":", "\n", "        ", "if", "opt_split", "[", "0", "]", "==", "'lookahead'", ":", "\n", "            ", "optimizer", "=", "Lookahead", "(", "optimizer", ")", "\n", "\n", "", "", "return", "optimizer", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lamb.Lamb.__init__": [[87, 95], ["dict", "torch.optim.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "bias_correction", "=", "True", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "\n", "weight_decay", "=", "0.01", ",", "grad_averaging", "=", "True", ",", "max_grad_norm", "=", "1.0", ",", "trust_clip", "=", "False", ",", "always_adapt", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "bias_correction", "=", "bias_correction", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "grad_averaging", "=", "grad_averaging", ",", "max_grad_norm", "=", "max_grad_norm", ",", "\n", "trust_clip", "=", "trust_clip", ",", "always_adapt", "=", "always_adapt", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lamb.Lamb.step": [[96, 193], ["torch.no_grad", "torch.tensor", "torch.zeros", "torch.sqrt", "torch.tensor", "torch.where", "torch.enable_grad", "closure", "torch.sqrt.add_", "p.grad.div_", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.add_", "RuntimeError", "p.grad.div_.pow().sum", "len", "torch.zeros_like", "torch.zeros_like", "update.add_", "p.norm", "update.norm", "torch.where", "update.mul_", "exp_avg.mul_", "exp_avg_sq.mul_", "torch.where", "torch.minimum", "p.grad.div_.pow", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "device", "=", "self", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "device", "\n", "one_tensor", "=", "torch", ".", "tensor", "(", "1.0", ",", "device", "=", "device", ")", "# because torch.where doesn't handle scalars correctly", "\n", "global_grad_norm", "=", "torch", ".", "zeros", "(", "1", ",", "device", "=", "device", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Lamb does not support sparse gradients, consider SparseAdam instad.'", ")", "\n", "", "global_grad_norm", ".", "add_", "(", "grad", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", ")", "\n", "\n", "", "", "global_grad_norm", "=", "torch", ".", "sqrt", "(", "global_grad_norm", ")", "\n", "# FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes", "\n", "# scalar types properly https://github.com/pytorch/pytorch/issues/9190", "\n", "max_grad_norm", "=", "torch", ".", "tensor", "(", "self", ".", "defaults", "[", "'max_grad_norm'", "]", ",", "device", "=", "device", ")", "\n", "clip_global_grad_norm", "=", "torch", ".", "where", "(", "\n", "global_grad_norm", ">", "max_grad_norm", ",", "\n", "global_grad_norm", "/", "max_grad_norm", ",", "\n", "one_tensor", ")", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "bias_correction", "=", "1", "if", "group", "[", "'bias_correction'", "]", "else", "0", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "grad_averaging", "=", "1", "if", "group", "[", "'grad_averaging'", "]", "else", "0", "\n", "beta3", "=", "1", "-", "beta1", "if", "grad_averaging", "else", "1.0", "\n", "\n", "# assume same step across group now to simplify things", "\n", "# per parameter step can be easily support by making it tensor, or pass list into kernel", "\n", "if", "'step'", "in", "group", ":", "\n", "                ", "group", "[", "'step'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "group", "[", "'step'", "]", "=", "1", "\n", "\n", "", "if", "bias_correction", ":", "\n", "                ", "bias_correction1", "=", "1", "-", "beta1", "**", "group", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "group", "[", "'step'", "]", "\n", "", "else", ":", "\n", "                ", "bias_correction1", ",", "bias_correction2", "=", "1.0", ",", "1.0", "\n", "\n", "", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "div_", "(", "clip_global_grad_norm", ")", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "# Exponential moving average of gradient valuesa", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "beta3", ")", "# m_t", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "# v_t", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "update", "=", "(", "exp_avg", "/", "bias_correction1", ")", ".", "div_", "(", "denom", ")", "\n", "\n", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "update", ".", "add_", "(", "p", ",", "alpha", "=", "weight_decay", ")", "\n", "\n", "", "if", "weight_decay", "!=", "0", "or", "group", "[", "'always_adapt'", "]", ":", "\n", "# Layer-wise LR adaptation. By default, skip adaptation on parameters that are", "\n", "# excluded from weight decay, unless always_adapt == True, then always enabled.", "\n", "                    ", "w_norm", "=", "p", ".", "norm", "(", "2.0", ")", "\n", "g_norm", "=", "update", ".", "norm", "(", "2.0", ")", "\n", "# FIXME nested where required since logical and/or not working in PT XLA", "\n", "trust_ratio", "=", "torch", ".", "where", "(", "\n", "w_norm", ">", "0", ",", "\n", "torch", ".", "where", "(", "g_norm", ">", "0", ",", "w_norm", "/", "g_norm", ",", "one_tensor", ")", ",", "\n", "one_tensor", ",", "\n", ")", "\n", "if", "group", "[", "'trust_clip'", "]", ":", "\n", "# LAMBC trust clipping, upper bound fixed at one", "\n", "                        ", "trust_ratio", "=", "torch", ".", "minimum", "(", "trust_ratio", ",", "one_tensor", ")", "\n", "", "update", ".", "mul_", "(", "trust_ratio", ")", "\n", "\n", "", "p", ".", "add_", "(", "update", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.radam.RAdam.__init__": [[12, 17], ["dict", "torch.optim.optimizer.Optimizer.__init__", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "_", "in", "range", "(", "10", ")", "]", ")", "\n", "super", "(", "RAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.radam.RAdam.__setstate__": [[18, 20], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RAdam", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.radam.RAdam.step": [[21, 90], ["torch.no_grad", "torch.enable_grad", "closure", "p.grad.float", "p.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "p.float.add_", "exp_avg_sq.sqrt().add_", "p.float.addcdiv_", "p.float.add_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RAdam does not support sparse gradients'", ")", "\n", "\n", "", "p_fp32", "=", "p", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "buffered", "=", "group", "[", "'buffer'", "]", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "num_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "num_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "num_sma", "=", "num_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "num_sma", "\n", "\n", "# more conservative since it's an approximated value", "\n", "if", "num_sma", ">=", "5", ":", "\n", "                        ", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "*", "\n", "(", "num_sma", "-", "4", ")", "/", "(", "num_sma_max", "-", "4", ")", "*", "\n", "(", "num_sma", "-", "2", ")", "/", "num_sma", "*", "\n", "num_sma_max", "/", "(", "num_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "group", "[", "'lr'", "]", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ")", "\n", "\n", "# more conservative since it's an approximated value", "\n", "", "if", "num_sma", ">=", "5", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_fp32", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "", "else", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.__init__": [[13, 29], ["dict", "lookahead.Lookahead.defaults.update", "collections.defaultdict", "dict.items", "ValueError", "ValueError", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["    ", "def", "__init__", "(", "self", ",", "base_optimizer", ",", "alpha", "=", "0.5", ",", "k", "=", "6", ")", ":", "\n", "# NOTE super().__init__() not called on purpose", "\n", "        ", "if", "not", "0.0", "<=", "alpha", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid slow update rate: {alpha}'", ")", "\n", "", "if", "not", "1", "<=", "k", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid lookahead steps: {k}'", ")", "\n", "", "defaults", "=", "dict", "(", "lookahead_alpha", "=", "alpha", ",", "lookahead_k", "=", "k", ",", "lookahead_step", "=", "0", ")", "\n", "self", ".", "_base_optimizer", "=", "base_optimizer", "\n", "self", ".", "param_groups", "=", "base_optimizer", ".", "param_groups", "\n", "self", ".", "defaults", "=", "base_optimizer", ".", "defaults", "\n", "self", ".", "defaults", ".", "update", "(", "defaults", ")", "\n", "self", ".", "state", "=", "defaultdict", "(", "dict", ")", "\n", "# manually add our defaults to the param groups", "\n", "for", "name", ",", "default", "in", "defaults", ".", "items", "(", ")", ":", "\n", "            ", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "                ", "group", ".", "setdefault", "(", "name", ",", "default", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.update_slow": [[30, 42], ["torch.no_grad", "slow.add_", "fast_p.copy_", "torch.empty_like", "param_state[].copy_"], "methods", ["None"], ["", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "update_slow", "(", "self", ",", "group", ")", ":", "\n", "        ", "for", "fast_p", "in", "group", "[", "\"params\"", "]", ":", "\n", "            ", "if", "fast_p", ".", "grad", "is", "None", ":", "\n", "                ", "continue", "\n", "", "param_state", "=", "self", ".", "_base_optimizer", ".", "state", "[", "fast_p", "]", "\n", "if", "'lookahead_slow_buff'", "not", "in", "param_state", ":", "\n", "                ", "param_state", "[", "'lookahead_slow_buff'", "]", "=", "torch", ".", "empty_like", "(", "fast_p", ")", "\n", "param_state", "[", "'lookahead_slow_buff'", "]", ".", "copy_", "(", "fast_p", ")", "\n", "", "slow", "=", "param_state", "[", "'lookahead_slow_buff'", "]", "\n", "slow", ".", "add_", "(", "fast_p", "-", "slow", ",", "alpha", "=", "group", "[", "'lookahead_alpha'", "]", ")", "\n", "fast_p", ".", "copy_", "(", "slow", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.sync_lookahead": [[43, 46], ["lookahead.Lookahead.update_slow"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.update_slow"], ["", "", "def", "sync_lookahead", "(", "self", ")", ":", "\n", "        ", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "            ", "self", ".", "update_slow", "(", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.step": [[47, 55], ["torch.no_grad", "lookahead.Lookahead._base_optimizer.step", "lookahead.Lookahead.update_slow"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.update_slow"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "self", ".", "_base_optimizer", ".", "step", "(", "closure", ")", "\n", "for", "group", "in", "self", ".", "_base_optimizer", ".", "param_groups", ":", "\n", "            ", "group", "[", "'lookahead_step'", "]", "+=", "1", "\n", "if", "group", "[", "'lookahead_step'", "]", "%", "group", "[", "'lookahead_k'", "]", "==", "0", ":", "\n", "                ", "self", ".", "update_slow", "(", "group", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict": [[56, 58], ["lookahead.Lookahead._base_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_base_optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.load_state_dict": [[59, 62], ["lookahead.Lookahead._base_optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "_base_optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "self", ".", "param_groups", "=", "self", ".", "_base_optimizer", ".", "param_groups", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__init__": [[48, 65], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-2", ",", "alpha", "=", "0.9", ",", "eps", "=", "1e-10", ",", "weight_decay", "=", "0", ",", "momentum", "=", "0.", ",", "centered", "=", "False", ",", "\n", "decoupled_decay", "=", "False", ",", "lr_in_momentum", "=", "True", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "momentum", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid momentum value: {}\"", ".", "format", "(", "momentum", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "if", "not", "0.0", "<=", "alpha", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid alpha value: {}\"", ".", "format", "(", "alpha", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "alpha", "=", "alpha", ",", "eps", "=", "eps", ",", "centered", "=", "centered", ",", "weight_decay", "=", "weight_decay", ",", "\n", "decoupled_decay", "=", "decoupled_decay", ",", "lr_in_momentum", "=", "lr_in_momentum", ")", "\n", "super", "(", "RMSpropTF", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__": [[66, 71], ["super().__setstate__", "group.setdefault", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "RMSpropTF", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'momentum'", ",", "0", ")", "\n", "group", ".", "setdefault", "(", "'centered'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.rmsprop_tf.RMSpropTF.step": [[72, 140], ["torch.no_grad", "torch.enable_grad", "closure", "square_avg.add_", "RuntimeError", "len", "torch.ones_like", "grad_avg.add_", "square_avg.addcmul().add().sqrt_", "square_avg.add().sqrt_", "p.addcdiv_", "torch.zeros_like", "torch.zeros_like", "p.mul_", "grad.add.add.add", "grad.add.add.pow", "buf.mul_().addcdiv_", "p.add_", "buf.mul_().addcdiv_", "p.add_", "square_avg.addcmul().add", "square_avg.add", "buf.mul_", "buf.mul_", "square_avg.addcmul"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'RMSprop does not support sparse gradients'", ")", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'square_avg'", "]", "=", "torch", ".", "ones_like", "(", "p", ")", "# PyTorch inits to zero", "\n", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                        ", "state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "", "if", "group", "[", "'centered'", "]", ":", "\n", "                        ", "state", "[", "'grad_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "square_avg", "=", "state", "[", "'square_avg'", "]", "\n", "one_minus_alpha", "=", "1.", "-", "group", "[", "'alpha'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "if", "group", "[", "'decoupled_decay'", "]", ":", "\n", "                        ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "grad", "=", "grad", ".", "add", "(", "p", ",", "alpha", "=", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "# Tensorflow order of ops for updating squared avg", "\n", "", "", "square_avg", ".", "add_", "(", "grad", ".", "pow", "(", "2", ")", "-", "square_avg", ",", "alpha", "=", "one_minus_alpha", ")", "\n", "# square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)  # PyTorch original", "\n", "\n", "if", "group", "[", "'centered'", "]", ":", "\n", "                    ", "grad_avg", "=", "state", "[", "'grad_avg'", "]", "\n", "grad_avg", ".", "add_", "(", "grad", "-", "grad_avg", ",", "alpha", "=", "one_minus_alpha", ")", "\n", "avg", "=", "square_avg", ".", "addcmul", "(", "grad_avg", ",", "grad_avg", ",", "value", "=", "-", "1", ")", ".", "add", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt_", "(", ")", "# eps in sqrt", "\n", "# grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)  # PyTorch original", "\n", "", "else", ":", "\n", "                    ", "avg", "=", "square_avg", ".", "add", "(", "group", "[", "'eps'", "]", ")", ".", "sqrt_", "(", ")", "# eps moved in sqrt", "\n", "\n", "", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                    ", "buf", "=", "state", "[", "'momentum_buffer'", "]", "\n", "# Tensorflow accumulates the LR scaling in the momentum buffer", "\n", "if", "group", "[", "'lr_in_momentum'", "]", ":", "\n", "                        ", "buf", ".", "mul_", "(", "group", "[", "'momentum'", "]", ")", ".", "addcdiv_", "(", "grad", ",", "avg", ",", "value", "=", "group", "[", "'lr'", "]", ")", "\n", "p", ".", "add_", "(", "-", "buf", ")", "\n", "", "else", ":", "\n", "# PyTorch scales the param update by LR", "\n", "                        ", "buf", ".", "mul_", "(", "group", "[", "'momentum'", "]", ")", ".", "addcdiv_", "(", "grad", ",", "avg", ")", "\n", "p", ".", "add_", "(", "buf", ",", "alpha", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "p", ".", "addcdiv_", "(", "grad", ",", "avg", ",", "value", "=", "-", "group", "[", "'lr'", "]", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.AdamP.__init__": [[44, 50], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "delta", "=", "0.1", ",", "wd_ratio", "=", "0.1", ",", "nesterov", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "\n", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "delta", "=", "delta", ",", "wd_ratio", "=", "wd_ratio", ",", "nesterov", "=", "nesterov", ")", "\n", "super", "(", "AdamP", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.AdamP.step": [[51, 106], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.add_", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "len", "adamp.projection", "p.mul_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.projection"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "grad", "=", "p", ".", "grad", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "# Adam", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1", "-", "beta1", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1", "-", "beta2", ")", "\n", "\n", "denom", "=", "(", "exp_avg_sq", ".", "sqrt", "(", ")", "/", "math", ".", "sqrt", "(", "bias_correction2", ")", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "step_size", "=", "group", "[", "'lr'", "]", "/", "bias_correction1", "\n", "\n", "if", "nesterov", ":", "\n", "                    ", "perturb", "=", "(", "beta1", "*", "exp_avg", "+", "(", "1", "-", "beta1", ")", "*", "grad", ")", "/", "denom", "\n", "", "else", ":", "\n", "                    ", "perturb", "=", "exp_avg", "/", "denom", "\n", "\n", "# Projection", "\n", "", "wd_ratio", "=", "1.", "\n", "if", "len", "(", "p", ".", "shape", ")", ">", "1", ":", "\n", "                    ", "perturb", ",", "wd_ratio", "=", "projection", "(", "p", ",", "grad", ",", "perturb", ",", "group", "[", "'delta'", "]", ",", "group", "[", "'wd_ratio'", "]", ",", "group", "[", "'eps'", "]", ")", "\n", "\n", "# Weight decay", "\n", "", "if", "group", "[", "'weight_decay'", "]", ">", "0", ":", "\n", "                    ", "p", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", "*", "wd_ratio", ")", "\n", "\n", "# Step", "\n", "", "p", ".", "add_", "(", "perturb", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp._channel_view": [[17, 19], ["x.reshape", "x.size"], "function", ["None"], ["def", "_channel_view", "(", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "x", ".", "reshape", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp._layer_view": [[21, 23], ["x.reshape"], "function", ["None"], ["", "def", "_layer_view", "(", "x", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "x", ".", "reshape", "(", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.projection": [[25, 41], ["view_func", "view_func", "torch.cosine_similarity().abs_", "F.cosine_similarity().abs_.max", "len", "torch.cosine_similarity", "math.sqrt", "view_func.norm().add_().reshape", "view_func().sum().reshape", "view_func.size", "view_func.norm().add_", "view_func().sum", "view_func.norm", "view_func"], "function", ["None"], ["", "def", "projection", "(", "p", ",", "grad", ",", "perturb", ",", "delta", ":", "float", ",", "wd_ratio", ":", "float", ",", "eps", ":", "float", ")", ":", "\n", "    ", "wd", "=", "1.", "\n", "expand_size", "=", "(", "-", "1", ",", ")", "+", "(", "1", ",", ")", "*", "(", "len", "(", "p", ".", "shape", ")", "-", "1", ")", "\n", "for", "view_func", "in", "[", "_channel_view", ",", "_layer_view", "]", ":", "\n", "        ", "param_view", "=", "view_func", "(", "p", ")", "\n", "grad_view", "=", "view_func", "(", "grad", ")", "\n", "cosine_sim", "=", "F", ".", "cosine_similarity", "(", "grad_view", ",", "param_view", ",", "dim", "=", "1", ",", "eps", "=", "eps", ")", ".", "abs_", "(", ")", "\n", "\n", "# FIXME this is a problem for PyTorch XLA", "\n", "if", "cosine_sim", ".", "max", "(", ")", "<", "delta", "/", "math", ".", "sqrt", "(", "param_view", ".", "size", "(", "1", ")", ")", ":", "\n", "            ", "p_n", "=", "p", "/", "param_view", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "1", ")", ".", "add_", "(", "eps", ")", ".", "reshape", "(", "expand_size", ")", "\n", "perturb", "-=", "p_n", "*", "view_func", "(", "p_n", "*", "perturb", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "reshape", "(", "expand_size", ")", "\n", "wd", "=", "wd_ratio", "\n", "return", "perturb", ",", "wd", "\n", "\n", "", "", "return", "perturb", ",", "wd", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.__init__": [[41, 52], ["dict", "super().__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "None", ",", "eps", "=", "1e-30", ",", "eps_scale", "=", "1e-3", ",", "clip_threshold", "=", "1.0", ",", "\n", "decay_rate", "=", "-", "0.8", ",", "betas", "=", "None", ",", "weight_decay", "=", "0.0", ",", "scale_parameter", "=", "True", ",", "warmup_init", "=", "False", ")", ":", "\n", "        ", "relative_step", "=", "not", "lr", "\n", "if", "warmup_init", "and", "not", "relative_step", ":", "\n", "            ", "raise", "ValueError", "(", "'warmup_init requires relative_step=True'", ")", "\n", "\n", "", "beta1", "=", "None", "if", "betas", "is", "None", "else", "betas", "[", "0", "]", "# make it compat with standard betas arg", "\n", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "eps_scale", "=", "eps_scale", ",", "clip_threshold", "=", "clip_threshold", ",", "decay_rate", "=", "decay_rate", ",", "\n", "beta1", "=", "beta1", ",", "weight_decay", "=", "weight_decay", ",", "scale_parameter", "=", "scale_parameter", ",", "\n", "relative_step", "=", "relative_step", ",", "warmup_init", "=", "warmup_init", ")", "\n", "super", "(", "Adafactor", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr": [[53, 63], ["min", "max", "math.sqrt"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_lr", "(", "param_group", ",", "param_state", ")", ":", "\n", "        ", "if", "param_group", "[", "'relative_step'", "]", ":", "\n", "            ", "min_step", "=", "1e-6", "*", "param_state", "[", "'step'", "]", "if", "param_group", "[", "'warmup_init'", "]", "else", "1e-2", "\n", "lr_t", "=", "min", "(", "min_step", ",", "1.0", "/", "math", ".", "sqrt", "(", "param_state", "[", "'step'", "]", ")", ")", "\n", "param_scale", "=", "1.0", "\n", "if", "param_group", "[", "'scale_parameter'", "]", ":", "\n", "                ", "param_scale", "=", "max", "(", "param_group", "[", "'eps_scale'", "]", ",", "param_state", "[", "'RMS'", "]", ")", "\n", "", "param_group", "[", "'lr'", "]", "=", "lr_t", "*", "param_scale", "\n", "", "return", "param_group", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_options": [[64, 69], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_options", "(", "param_group", ",", "param_shape", ")", ":", "\n", "        ", "factored", "=", "len", "(", "param_shape", ")", ">=", "2", "\n", "use_first_moment", "=", "param_group", "[", "'beta1'", "]", "is", "not", "None", "\n", "return", "factored", ",", "use_first_moment", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._rms": [[70, 73], ["tensor.norm", "tensor.numel"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_rms", "(", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "norm", "(", "2", ")", "/", "(", "tensor", ".", "numel", "(", ")", "**", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._approx_sq_grad": [[74, 78], ["exp_avg_sq_col.unsqueeze().rsqrt", "torch.mul", "exp_avg_sq_col.unsqueeze", "exp_avg_sq_row.mean"], "methods", ["None"], ["", "def", "_approx_sq_grad", "(", "self", ",", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", ":", "\n", "        ", "r_factor", "=", "(", "exp_avg_sq_row", "/", "exp_avg_sq_row", ".", "mean", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ".", "rsqrt_", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "c_factor", "=", "exp_avg_sq_col", ".", "unsqueeze", "(", "-", "2", ")", ".", "rsqrt", "(", ")", "\n", "return", "torch", ".", "mul", "(", "r_factor", ",", "c_factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor.step": [[79, 168], ["torch.no_grad", "torch.enable_grad", "closure", "adafactor.Adafactor._get_options", "adafactor.Adafactor._rms", "adafactor.Adafactor._get_lr", "exp_avg_sq.rsqrt().mul_.div_", "exp_avg_sq.rsqrt().mul_.mul_", "p_fp32.float.float.add_", "grad.float.float.float", "RuntimeError", "len", "p_fp32.float.float.float", "math.pow", "exp_avg_sq_row.mul_().add_", "exp_avg_sq_col.mul_().add_", "adafactor.Adafactor._approx_sq_grad", "exp_avg_sq.rsqrt().mul_.mul_", "exp_avg_sq.mul_().add_", "exp_avg_sq.rsqrt().mul_", "exp_avg.mul_().add_", "p_fp32.float.float.add_", "p.copy_", "torch.zeros_like", "torch.zeros().to", "torch.zeros().to", "torch.zeros_like", "state[].to", "state[].to", "state[].to", "state[].to", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq.rsqrt().mul_.mean", "exp_avg_sq_row.mul_", "exp_avg_sq_col.mul_", "exp_avg_sq.mul_", "exp_avg_sq.rsqrt", "exp_avg.mul_", "torch.zeros", "torch.zeros", "adafactor.Adafactor._rms"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_options", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._rms", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._get_lr", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._approx_sq_grad", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adafactor.Adafactor._rms"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", "\n", "if", "grad", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "grad", "=", "grad", ".", "float", "(", ")", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adafactor does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "factored", ",", "use_first_moment", "=", "self", ".", "_get_options", "(", "group", ",", "grad", ".", "shape", ")", "\n", "# State Initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "\n", "if", "use_first_moment", ":", "\n", "# Exponential moving average of gradient values", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "torch", ".", "zeros", "(", "grad", ".", "shape", "[", ":", "-", "1", "]", ")", ".", "to", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "torch", ".", "zeros", "(", "grad", ".", "shape", "[", ":", "-", "2", "]", "+", "grad", ".", "shape", "[", "-", "1", ":", "]", ")", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "\n", "", "state", "[", "'RMS'", "]", "=", "0", "\n", "", "else", ":", "\n", "                    ", "if", "use_first_moment", ":", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "to", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "state", "[", "'exp_avg_sq_row'", "]", ".", "to", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "state", "[", "'exp_avg_sq_col'", "]", ".", "to", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "to", "(", "grad", ")", "\n", "\n", "", "", "p_fp32", "=", "p", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p_fp32", "=", "p_fp32", ".", "float", "(", ")", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "state", "[", "'RMS'", "]", "=", "self", ".", "_rms", "(", "p_fp32", ")", "\n", "lr_t", "=", "self", ".", "_get_lr", "(", "group", ",", "state", ")", "\n", "\n", "beta2t", "=", "1.0", "-", "math", ".", "pow", "(", "state", "[", "'step'", "]", ",", "group", "[", "'decay_rate'", "]", ")", "\n", "update", "=", "grad", "**", "2", "+", "group", "[", "'eps'", "]", "\n", "if", "factored", ":", "\n", "                    ", "exp_avg_sq_row", "=", "state", "[", "'exp_avg_sq_row'", "]", "\n", "exp_avg_sq_col", "=", "state", "[", "'exp_avg_sq_col'", "]", "\n", "\n", "exp_avg_sq_row", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ".", "mean", "(", "dim", "=", "-", "1", ")", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "exp_avg_sq_col", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ".", "mean", "(", "dim", "=", "-", "2", ")", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "\n", "# Approximation of exponential moving average of square of gradient", "\n", "update", "=", "self", ".", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ")", "\n", "update", ".", "mul_", "(", "grad", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", "=", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "update", ",", "alpha", "=", "1.0", "-", "beta2t", ")", "\n", "update", "=", "exp_avg_sq", ".", "rsqrt", "(", ")", ".", "mul_", "(", "grad", ")", "\n", "\n", "", "update", ".", "div_", "(", "(", "self", ".", "_rms", "(", "update", ")", "/", "group", "[", "'clip_threshold'", "]", ")", ".", "clamp_", "(", "min", "=", "1.0", ")", ")", "\n", "update", ".", "mul_", "(", "lr_t", ")", "\n", "\n", "if", "use_first_moment", ":", "\n", "                    ", "exp_avg", "=", "state", "[", "'exp_avg'", "]", "\n", "exp_avg", ".", "mul_", "(", "group", "[", "'beta1'", "]", ")", ".", "add_", "(", "update", ",", "alpha", "=", "1", "-", "group", "[", "'beta1'", "]", ")", "\n", "update", "=", "exp_avg", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_fp32", ".", "add_", "(", "p_fp32", ",", "alpha", "=", "-", "group", "[", "'weight_decay'", "]", "*", "lr_t", ")", "\n", "\n", "", "p_fp32", ".", "add_", "(", "-", "update", ")", "\n", "if", "p", ".", "dtype", "in", "{", "torch", ".", "float16", ",", "torch", ".", "bfloat16", "}", ":", "\n", "                    ", "p", ".", "copy_", "(", "p_fp32", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.get_cache_dir": [[26, 39], ["os.getenv", "get_dir", "os.path.join", "os.makedirs", "_logger.warning"], "function", ["None"], ["def", "get_cache_dir", "(", "child_dir", "=", "''", ")", ":", "\n", "    ", "\"\"\"\n    Returns the location of the directory where models are cached (and creates it if necessary).\n    \"\"\"", "\n", "# Issue warning to move data if old env is set", "\n", "if", "os", ".", "getenv", "(", "'TORCH_MODEL_ZOO'", ")", ":", "\n", "        ", "_logger", ".", "warning", "(", "'TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead'", ")", "\n", "\n", "", "hub_dir", "=", "get_dir", "(", ")", "\n", "child_dir", "=", "(", ")", "if", "not", "child_dir", "else", "(", "child_dir", ",", ")", "\n", "model_dir", "=", "os", ".", "path", ".", "join", "(", "hub_dir", ",", "'checkpoints'", ",", "*", "child_dir", ")", "\n", "os", ".", "makedirs", "(", "model_dir", ",", "exist_ok", "=", "True", ")", "\n", "return", "model_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.download_cached_file": [[41, 53], ["torch.hub.urlparse", "os.path.basename", "os.path.join", "hub.get_cache_dir", "os.path.exists", "_logger.info", "torch.hub.download_url_to_file", "torch.hub.HASH_REGEX.search", "HASH_REGEX.search.group"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.get_cache_dir"], ["", "def", "download_cached_file", "(", "url", ",", "check_hash", "=", "True", ",", "progress", "=", "False", ")", ":", "\n", "    ", "parts", "=", "urlparse", "(", "url", ")", "\n", "filename", "=", "os", ".", "path", ".", "basename", "(", "parts", ".", "path", ")", "\n", "cached_file", "=", "os", ".", "path", ".", "join", "(", "get_cache_dir", "(", ")", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cached_file", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "'Downloading: \"{}\" to {}\\n'", ".", "format", "(", "url", ",", "cached_file", ")", ")", "\n", "hash_prefix", "=", "None", "\n", "if", "check_hash", ":", "\n", "            ", "r", "=", "HASH_REGEX", ".", "search", "(", "filename", ")", "# r is Optional[Match[str]]", "\n", "hash_prefix", "=", "r", ".", "group", "(", "1", ")", "if", "r", "else", "None", "\n", "", "download_url_to_file", "(", "url", ",", "cached_file", ",", "hash_prefix", ",", "progress", "=", "progress", ")", "\n", "", "return", "cached_file", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.has_hf_hub": [[55, 61], ["RuntimeError"], "function", ["None"], ["", "def", "has_hf_hub", "(", "necessary", "=", "False", ")", ":", "\n", "    ", "if", "hf_hub_url", "is", "None", "and", "necessary", ":", "\n", "# if no HF Hub module installed and it is necessary to continue, raise error", "\n", "        ", "raise", "RuntimeError", "(", "\n", "'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.'", ")", "\n", "", "return", "hf_hub_url", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.hf_split": [[63, 69], ["hf_id.split", "len", "len"], "function", ["None"], ["", "def", "hf_split", "(", "hf_id", ")", ":", "\n", "    ", "rev_split", "=", "hf_id", ".", "split", "(", "'@'", ")", "\n", "assert", "0", "<", "len", "(", "rev_split", ")", "<=", "2", ",", "'hf_hub id should only contain one @ character to identify revision.'", "\n", "hf_model_id", "=", "rev_split", "[", "0", "]", "\n", "hf_revision", "=", "rev_split", "[", "-", "1", "]", "if", "len", "(", "rev_split", ")", ">", "1", "else", "None", "\n", "return", "hf_model_id", ",", "hf_revision", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_cfg_from_json": [[71, 75], ["json.loads", "open", "reader.read"], "function", ["None"], ["", "def", "load_cfg_from_json", "(", "json_file", ":", "Union", "[", "str", ",", "os", ".", "PathLike", "]", ")", ":", "\n", "    ", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "json", ".", "loads", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub._download_from_hf": [[77, 81], ["hub.hf_split", "hf_hub_url", "cached_download", "hub.get_cache_dir"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.hf_split", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.get_cache_dir"], ["", "def", "_download_from_hf", "(", "model_id", ":", "str", ",", "filename", ":", "str", ")", ":", "\n", "    ", "hf_model_id", ",", "hf_revision", "=", "hf_split", "(", "model_id", ")", "\n", "url", "=", "hf_hub_url", "(", "hf_model_id", ",", "filename", ",", "revision", "=", "hf_revision", ")", "\n", "return", "cached_download", "(", "url", ",", "cache_dir", "=", "get_cache_dir", "(", "'hf'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_model_config_from_hf": [[83, 90], ["hub.has_hf_hub", "hub._download_from_hf", "hub.load_cfg_from_json", "load_cfg_from_json.get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub._download_from_hf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_cfg_from_json", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "load_model_config_from_hf", "(", "model_id", ":", "str", ")", ":", "\n", "    ", "assert", "has_hf_hub", "(", "True", ")", "\n", "cached_file", "=", "_download_from_hf", "(", "model_id", ",", "'config.json'", ")", "\n", "default_cfg", "=", "load_cfg_from_json", "(", "cached_file", ")", "\n", "default_cfg", "[", "'hf_hub'", "]", "=", "model_id", "# insert hf_hub id for pretrained weight load during model creation", "\n", "model_name", "=", "default_cfg", ".", "get", "(", "'architecture'", ")", "\n", "return", "default_cfg", ",", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_state_dict_from_hf": [[92, 97], ["hub.has_hf_hub", "hub._download_from_hf", "torch.load"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub._download_from_hf"], ["", "def", "load_state_dict_from_hf", "(", "model_id", ":", "str", ")", ":", "\n", "    ", "assert", "has_hf_hub", "(", "True", ")", "\n", "cached_file", "=", "_download_from_hf", "(", "model_id", ",", "'pytorch_model.bin'", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "cached_file", ",", "map_location", "=", "'cpu'", ")", "\n", "return", "state_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.register_model": [[21, 47], ["fn.__module__.split", "hasattr", "_module_to_models[].add", "len", "mod.__all__.append", "hasattr", "copy.deepcopy", "_model_has_pretrained.add"], "function", ["None"], ["def", "register_model", "(", "fn", ")", ":", "\n", "# lookup containing module", "\n", "    ", "mod", "=", "sys", ".", "modules", "[", "fn", ".", "__module__", "]", "\n", "module_name_split", "=", "fn", ".", "__module__", ".", "split", "(", "'.'", ")", "\n", "module_name", "=", "module_name_split", "[", "-", "1", "]", "if", "len", "(", "module_name_split", ")", "else", "''", "\n", "\n", "# add model to __all__ in module", "\n", "model_name", "=", "fn", ".", "__name__", "\n", "if", "hasattr", "(", "mod", ",", "'__all__'", ")", ":", "\n", "        ", "mod", ".", "__all__", ".", "append", "(", "model_name", ")", "\n", "", "else", ":", "\n", "        ", "mod", ".", "__all__", "=", "[", "model_name", "]", "\n", "\n", "# add entries to registry dict/sets", "\n", "", "_model_entrypoints", "[", "model_name", "]", "=", "fn", "\n", "_model_to_module", "[", "model_name", "]", "=", "module_name", "\n", "_module_to_models", "[", "module_name", "]", ".", "add", "(", "model_name", ")", "\n", "has_pretrained", "=", "False", "# check if model has a pretrained url to allow filtering on this", "\n", "if", "hasattr", "(", "mod", ",", "'default_cfgs'", ")", "and", "model_name", "in", "mod", ".", "default_cfgs", ":", "\n", "# this will catch all models that have entrypoint matching cfg key, but miss any aliasing", "\n", "# entrypoints or non-matching combos", "\n", "        ", "has_pretrained", "=", "'url'", "in", "mod", ".", "default_cfgs", "[", "model_name", "]", "and", "'http'", "in", "mod", ".", "default_cfgs", "[", "model_name", "]", "[", "'url'", "]", "\n", "_model_default_cfgs", "[", "model_name", "]", "=", "deepcopy", "(", "mod", ".", "default_cfgs", "[", "model_name", "]", ")", "\n", "", "if", "has_pretrained", ":", "\n", "        ", "_model_has_pretrained", ".", "add", "(", "model_name", ")", "\n", "", "return", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry._natural_key": [[49, 51], ["s.isdigit", "int", "re.split", "string_.lower"], "function", ["None"], ["", "def", "_natural_key", "(", "string_", ")", ":", "\n", "    ", "return", "[", "int", "(", "s", ")", "if", "s", ".", "isdigit", "(", ")", "else", "s", "for", "s", "in", "re", ".", "split", "(", "r'(\\d+)'", ",", "string_", ".", "lower", "(", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.list_models": [[53, 92], ["list", "list", "_model_entrypoints.keys", "_model_has_pretrained.intersection", "set().intersection", "sorted", "isinstance", "fnmatch.filter", "len", "isinstance", "fnmatch.filter", "len", "set().union", "set().difference", "set", "set", "set"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.intersection", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.intersection", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["", "def", "list_models", "(", "filter", "=", "''", ",", "module", "=", "''", ",", "pretrained", "=", "False", ",", "exclude_filters", "=", "''", ",", "name_matches_cfg", "=", "False", ")", ":", "\n", "    ", "\"\"\" Return list of available model names, sorted alphabetically\n\n    Args:\n        filter (str) - Wildcard filter string that works with fnmatch\n        module (str) - Limit model selection to a specific sub-module (ie 'gen_efficientnet')\n        pretrained (bool) - Include only models with pretrained weights if True\n        exclude_filters (str or list[str]) - Wildcard filters to exclude models after including them with filter\n        name_matches_cfg (bool) - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n\n    Example:\n        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n    \"\"\"", "\n", "if", "module", ":", "\n", "        ", "all_models", "=", "list", "(", "_module_to_models", "[", "module", "]", ")", "\n", "", "else", ":", "\n", "        ", "all_models", "=", "_model_entrypoints", ".", "keys", "(", ")", "\n", "", "if", "filter", ":", "\n", "        ", "models", "=", "[", "]", "\n", "include_filters", "=", "filter", "if", "isinstance", "(", "filter", ",", "(", "tuple", ",", "list", ")", ")", "else", "[", "filter", "]", "\n", "for", "f", "in", "include_filters", ":", "\n", "            ", "include_models", "=", "fnmatch", ".", "filter", "(", "all_models", ",", "f", ")", "# include these models", "\n", "if", "len", "(", "include_models", ")", ":", "\n", "                ", "models", "=", "set", "(", "models", ")", ".", "union", "(", "include_models", ")", "\n", "", "", "", "else", ":", "\n", "        ", "models", "=", "all_models", "\n", "", "if", "exclude_filters", ":", "\n", "        ", "if", "not", "isinstance", "(", "exclude_filters", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "exclude_filters", "=", "[", "exclude_filters", "]", "\n", "", "for", "xf", "in", "exclude_filters", ":", "\n", "            ", "exclude_models", "=", "fnmatch", ".", "filter", "(", "models", ",", "xf", ")", "# exclude these models", "\n", "if", "len", "(", "exclude_models", ")", ":", "\n", "                ", "models", "=", "set", "(", "models", ")", ".", "difference", "(", "exclude_models", ")", "\n", "", "", "", "if", "pretrained", ":", "\n", "        ", "models", "=", "_model_has_pretrained", ".", "intersection", "(", "models", ")", "\n", "", "if", "name_matches_cfg", ":", "\n", "        ", "models", "=", "set", "(", "_model_default_cfgs", ")", ".", "intersection", "(", "models", ")", "\n", "", "return", "list", "(", "sorted", "(", "models", ",", "key", "=", "_natural_key", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model": [[94, 98], ["None"], "function", ["None"], ["", "def", "is_model", "(", "model_name", ")", ":", "\n", "    ", "\"\"\" Check if a model name exists\n    \"\"\"", "\n", "return", "model_name", "in", "_model_entrypoints", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.model_entrypoint": [[100, 104], ["None"], "function", ["None"], ["", "def", "model_entrypoint", "(", "model_name", ")", ":", "\n", "    ", "\"\"\"Fetch a model entrypoint for specified model name\n    \"\"\"", "\n", "return", "_model_entrypoints", "[", "model_name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.list_modules": [[106, 111], ["_module_to_models.keys", "list", "sorted"], "function", ["None"], ["", "def", "list_modules", "(", ")", ":", "\n", "    ", "\"\"\" Return list of module names that contain models / model entrypoints\n    \"\"\"", "\n", "modules", "=", "_module_to_models", ".", "keys", "(", ")", "\n", "return", "list", "(", "sorted", "(", "modules", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model_in_modules": [[113, 121], ["isinstance", "any"], "function", ["None"], ["", "def", "is_model_in_modules", "(", "model_name", ",", "module_names", ")", ":", "\n", "    ", "\"\"\"Check if a model exists within a subset of modules\n    Args:\n        model_name (str) - name of model to check\n        module_names (tuple, list, set) - names of modules to search in\n    \"\"\"", "\n", "assert", "isinstance", "(", "module_names", ",", "(", "tuple", ",", "list", ",", "set", ")", ")", "\n", "return", "any", "(", "model_name", "in", "_module_to_models", "[", "n", "]", "for", "n", "in", "module_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.has_model_default_key": [[123, 129], ["None"], "function", ["None"], ["", "def", "has_model_default_key", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Query model default_cfgs for existence of a specific key.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_default_cfgs", "and", "cfg_key", "in", "_model_default_cfgs", "[", "model_name", "]", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model_default_key": [[131, 137], ["_model_default_cfgs[].get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "is_model_default_key", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Return truthy value for specified model default_cfg key, False if does not exist.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_default_cfgs", "and", "_model_default_cfgs", "[", "model_name", "]", ".", "get", "(", "cfg_key", ",", "False", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.get_model_default_value": [[139, 146], ["_model_default_cfgs[].get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "get_model_default_value", "(", "model_name", ",", "cfg_key", ")", ":", "\n", "    ", "\"\"\" Get a specific model default_cfg value by key. None if it doesn't exist.\n    \"\"\"", "\n", "if", "model_name", "in", "_model_default_cfgs", ":", "\n", "        ", "return", "_model_default_cfgs", "[", "model_name", "]", ".", "get", "(", "cfg_key", ",", "None", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model_pretrained": [[148, 150], ["None"], "function", ["None"], ["", "", "def", "is_model_pretrained", "(", "model_name", ")", ":", "\n", "    ", "return", "model_name", "in", "_model_has_pretrained", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.split_model_name": [[7, 15], ["model_name.split", "len"], "function", ["None"], ["def", "split_model_name", "(", "model_name", ")", ":", "\n", "    ", "model_split", "=", "model_name", ".", "split", "(", "':'", ",", "1", ")", "\n", "if", "len", "(", "model_split", ")", "==", "1", ":", "\n", "        ", "return", "''", ",", "model_split", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "source_name", ",", "model_name", "=", "model_split", "\n", "assert", "source_name", "in", "(", "'timm'", ",", "'hf_hub'", ")", "\n", "return", "source_name", ",", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.safe_model_name": [[17, 23], ["factory.safe_model_name.make_safe"], "function", ["None"], ["", "", "def", "safe_model_name", "(", "model_name", ",", "remove_source", "=", "True", ")", ":", "\n", "    ", "def", "make_safe", "(", "name", ")", ":", "\n", "        ", "return", "''", ".", "join", "(", "c", "if", "c", ".", "isalnum", "(", ")", "else", "'_'", "for", "c", "in", "name", ")", ".", "rstrip", "(", "'_'", ")", "\n", "", "if", "remove_source", ":", "\n", "        ", "model_name", "=", "split_model_name", "(", "model_name", ")", "[", "-", "1", "]", "\n", "", "return", "make_safe", "(", "model_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model": [[25, 87], ["factory.split_model_name", "registry.is_model_in_modules", "kwargs.pop", "registry.is_model", "kwargs.pop", "kwargs.pop", "kwargs.pop", "print", "hub.load_model_config_from_hf", "registry.model_entrypoint", "RuntimeError", "layers.set_layer_config", "registry.model_entrypoint.", "helpers.load_checkpoint", "kwargs.get", "kwargs.items"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.split_model_name", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model_in_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.is_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_model_config_from_hf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.registry.model_entrypoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "create_model", "(", "\n", "model_name", ",", "\n", "pretrained", "=", "False", ",", "\n", "checkpoint_path", "=", "''", ",", "\n", "scriptable", "=", "None", ",", "\n", "exportable", "=", "None", ",", "\n", "no_jit", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Create a model\n\n    Args:\n        model_name (str): name of model to instantiate\n        pretrained (bool): load pretrained ImageNet-1k weights if true\n        checkpoint_path (str): path of checkpoint to load after model is initialized\n        scriptable (bool): set layer config so that model is jit scriptable (not working for all models yet)\n        exportable (bool): set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet)\n        no_jit (bool): set layer config so that model doesn't utilize jit scripted layers (so far activations only)\n\n    Keyword Args:\n        drop_rate (float): dropout rate for training (default: 0.0)\n        global_pool (str): global pool type (default: 'avg')\n        **: other kwargs are model specific\n    \"\"\"", "\n", "source_name", ",", "model_name", "=", "split_model_name", "(", "model_name", ")", "\n", "\n", "# Only EfficientNet and MobileNetV3 models have support for batchnorm params or drop_connect_rate passed as args", "\n", "is_efficientnet", "=", "is_model_in_modules", "(", "model_name", ",", "[", "'efficientnet'", ",", "'mobilenetv3'", "]", ")", "\n", "if", "not", "is_efficientnet", ":", "\n", "        ", "kwargs", ".", "pop", "(", "'bn_tf'", ",", "None", ")", "\n", "kwargs", ".", "pop", "(", "'bn_momentum'", ",", "None", ")", "\n", "kwargs", ".", "pop", "(", "'bn_eps'", ",", "None", ")", "\n", "\n", "# handle backwards compat with drop_connect -> drop_path change", "\n", "", "drop_connect_rate", "=", "kwargs", ".", "pop", "(", "'drop_connect_rate'", ",", "None", ")", "\n", "if", "drop_connect_rate", "is", "not", "None", "and", "kwargs", ".", "get", "(", "'drop_path_rate'", ",", "None", ")", "is", "None", ":", "\n", "        ", "print", "(", "\"WARNING: 'drop_connect' as an argument is deprecated, please use 'drop_path'.\"", "\n", "\" Setting drop_path to %f.\"", "%", "drop_connect_rate", ")", "\n", "kwargs", "[", "'drop_path_rate'", "]", "=", "drop_connect_rate", "\n", "\n", "# Parameters that aren't supported by all models or are intended to only override model defaults if set", "\n", "# should default to None in command line args/cfg. Remove them if they are present and not set so that", "\n", "# non-supporting models don't break and default args remain in effect.", "\n", "", "kwargs", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", "if", "v", "is", "not", "None", "}", "\n", "\n", "if", "source_name", "==", "'hf_hub'", ":", "\n", "# For model names specified in the form `hf_hub:path/architecture_name#revision`,", "\n", "# load model weights + default_cfg from Hugging Face hub.", "\n", "        ", "hf_default_cfg", ",", "model_name", "=", "load_model_config_from_hf", "(", "model_name", ")", "\n", "kwargs", "[", "'external_default_cfg'", "]", "=", "hf_default_cfg", "# FIXME revamp default_cfg interface someday", "\n", "\n", "", "if", "is_model", "(", "model_name", ")", ":", "\n", "        ", "create_fn", "=", "model_entrypoint", "(", "model_name", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "'Unknown model (%s)'", "%", "model_name", ")", "\n", "\n", "", "with", "set_layer_config", "(", "scriptable", "=", "scriptable", ",", "exportable", "=", "exportable", ",", "no_jit", "=", "no_jit", ")", ":", "\n", "        ", "model", "=", "create_fn", "(", "pretrained", "=", "pretrained", ",", "**", "kwargs", ")", "\n", "\n", "", "if", "checkpoint_path", ":", "\n", "        ", "load_checkpoint", "(", "model", ",", "checkpoint_path", ")", "\n", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict": [[24, 52], ["os.path.isfile", "torch.load", "torch.load", "isinstance", "_logger.info", "_logger.error", "FileNotFoundError", "collections.OrderedDict", "state_dict.items", "torch.load.get", "k.startswith", "torch.load.get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["def", "load_state_dict", "(", "checkpoint_path", ",", "use_ema", "=", "False", ")", ":", "\n", "    ", "if", "checkpoint_path", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "state_dict_key", "=", "''", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", ":", "\n", "            ", "if", "use_ema", "and", "checkpoint", ".", "get", "(", "'state_dict_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'state_dict_ema'", "\n", "", "elif", "use_ema", "and", "checkpoint", ".", "get", "(", "'model_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'model_ema'", "\n", "", "elif", "'state_dict'", "in", "checkpoint", ":", "\n", "                ", "state_dict_key", "=", "'state_dict'", "\n", "", "elif", "'model'", "in", "checkpoint", ":", "\n", "                ", "state_dict_key", "=", "'model'", "\n", "", "", "if", "state_dict_key", ":", "\n", "            ", "state_dict", "=", "checkpoint", "[", "state_dict_key", "]", "\n", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "# strip `module.` prefix", "\n", "                ", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "state_dict", "=", "new_state_dict", "\n", "", "else", ":", "\n", "            ", "state_dict", "=", "checkpoint", "\n", "", "_logger", ".", "info", "(", "\"Loaded {} from checkpoint '{}'\"", ".", "format", "(", "state_dict_key", ",", "checkpoint_path", ")", ")", "\n", "return", "state_dict", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "error", "(", "\"No checkpoint found at '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "raise", "FileNotFoundError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint": [[54, 64], ["helpers.load_state_dict", "model.load_state_dict", "[].lower", "hasattr", "model.load_pretrained", "NotImplementedError", "os.path.splitext"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_pretrained"], ["", "", "def", "load_checkpoint", "(", "model", ",", "checkpoint_path", ",", "use_ema", "=", "False", ",", "strict", "=", "True", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "splitext", "(", "checkpoint_path", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "in", "(", "'.npz'", ",", "'.npy'", ")", ":", "\n", "# numpy checkpoint, try to load via model specific load_pretrained fn", "\n", "        ", "if", "hasattr", "(", "model", ",", "'load_pretrained'", ")", ":", "\n", "            ", "model", ".", "load_pretrained", "(", "checkpoint_path", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Model cannot load numpy checkpoint'", ")", "\n", "", "return", "\n", "", "state_dict", "=", "load_state_dict", "(", "checkpoint_path", ",", "use_ema", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.resume_checkpoint": [[66, 104], ["os.path.isfile", "torch.load", "torch.load", "_logger.error", "FileNotFoundError", "isinstance", "collections.OrderedDict", "checkpoint[].items", "model.load_state_dict", "model.load_state_dict", "_logger.info", "optimizer.load_state_dict", "loss_scaler.load_state_dict", "_logger.info", "_logger.info", "k.startswith", "_logger.info", "_logger.info"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "def", "resume_checkpoint", "(", "model", ",", "checkpoint_path", ",", "optimizer", "=", "None", ",", "loss_scaler", "=", "None", ",", "log_info", "=", "True", ")", ":", "\n", "    ", "resume_epoch", "=", "None", "\n", "if", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", "and", "'state_dict'", "in", "checkpoint", ":", "\n", "            ", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "'Restoring model state from checkpoint...'", ")", "\n", "", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "checkpoint", "[", "'state_dict'", "]", ".", "items", "(", ")", ":", "\n", "                ", "name", "=", "k", "[", "7", ":", "]", "if", "k", ".", "startswith", "(", "'module'", ")", "else", "k", "\n", "new_state_dict", "[", "name", "]", "=", "v", "\n", "", "model", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "\n", "if", "optimizer", "is", "not", "None", "and", "'optimizer'", "in", "checkpoint", ":", "\n", "                ", "if", "log_info", ":", "\n", "                    ", "_logger", ".", "info", "(", "'Restoring optimizer state from checkpoint...'", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "\n", "", "if", "loss_scaler", "is", "not", "None", "and", "loss_scaler", ".", "state_dict_key", "in", "checkpoint", ":", "\n", "                ", "if", "log_info", ":", "\n", "                    ", "_logger", ".", "info", "(", "'Restoring AMP loss scaler state from checkpoint...'", ")", "\n", "", "loss_scaler", ".", "load_state_dict", "(", "checkpoint", "[", "loss_scaler", ".", "state_dict_key", "]", ")", "\n", "\n", "", "if", "'epoch'", "in", "checkpoint", ":", "\n", "                ", "resume_epoch", "=", "checkpoint", "[", "'epoch'", "]", "\n", "if", "'version'", "in", "checkpoint", "and", "checkpoint", "[", "'version'", "]", ">", "1", ":", "\n", "                    ", "resume_epoch", "+=", "1", "# start at the next epoch, old checkpoints incremented before save", "\n", "\n", "", "", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Loaded checkpoint '{}' (epoch {})\"", ".", "format", "(", "checkpoint_path", ",", "checkpoint", "[", "'epoch'", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "model", ".", "load_state_dict", "(", "checkpoint", ")", "\n", "if", "log_info", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Loaded checkpoint '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "", "", "return", "resume_epoch", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "error", "(", "\"No checkpoint found at '{}'\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "raise", "FileNotFoundError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_custom_pretrained": [[106, 140], ["default_cfg.get", "hub.download_cached_file", "getattr", "_logger.warning", "load_fn", "hasattr", "model.load_pretrained", "_logger.warning"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.download_cached_file", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_pretrained"], ["", "", "def", "load_custom_pretrained", "(", "model", ",", "default_cfg", "=", "None", ",", "load_fn", "=", "None", ",", "progress", "=", "False", ",", "check_hash", "=", "False", ")", ":", "\n", "    ", "r\"\"\"Loads a custom (read non .pth) weight file\n\n    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n    a passed in custom load fun, or the `load_pretrained` model member fn.\n\n    If the object is already present in `model_dir`, it's deserialized and returned.\n    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n\n    Args:\n        model: The instantiated model to load weights into\n        default_cfg (dict): Default pretrained model cfg\n        load_fn: An external stand alone fn that loads weights into provided model, otherwise a fn named\n            'laod_pretrained' on the model will be called if it exists\n        progress (bool, optional): whether or not to display a progress bar to stderr. Default: False\n        check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention\n            ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n            digits of the SHA256 hash of the contents of the file. The hash is used to\n            ensure unique names and to verify the contents of the file. Default: False\n    \"\"\"", "\n", "default_cfg", "=", "default_cfg", "or", "getattr", "(", "model", ",", "'default_cfg'", ",", "None", ")", "or", "{", "}", "\n", "pretrained_url", "=", "default_cfg", ".", "get", "(", "'url'", ",", "None", ")", "\n", "if", "not", "pretrained_url", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"No pretrained weights exist for this model. Using random initialization.\"", ")", "\n", "return", "\n", "", "cached_file", "=", "download_cached_file", "(", "default_cfg", "[", "'url'", "]", ",", "check_hash", "=", "check_hash", ",", "progress", "=", "progress", ")", "\n", "\n", "if", "load_fn", "is", "not", "None", ":", "\n", "        ", "load_fn", "(", "model", ",", "cached_file", ")", "\n", "", "elif", "hasattr", "(", "model", ",", "'load_pretrained'", ")", ":", "\n", "        ", "model", ".", "load_pretrained", "(", "cached_file", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Valid function to load pretrained weights is not available, using random initialization.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_input_conv": [[142, 165], ["conv_weight.sum.float", "conv_weight.sum.to", "conv_weight.sum.reshape", "conv_weight.sum.sum", "conv_weight.sum.sum", "NotImplementedError", "int", "math.ceil", "conv_weight.sum.repeat", "float"], "function", ["None"], ["", "", "def", "adapt_input_conv", "(", "in_chans", ",", "conv_weight", ")", ":", "\n", "    ", "conv_type", "=", "conv_weight", ".", "dtype", "\n", "conv_weight", "=", "conv_weight", ".", "float", "(", ")", "# Some weights are in torch.half, ensure it's float for sum on CPU", "\n", "O", ",", "I", ",", "J", ",", "K", "=", "conv_weight", ".", "shape", "\n", "if", "in_chans", "==", "1", ":", "\n", "        ", "if", "I", ">", "3", ":", "\n", "            ", "assert", "conv_weight", ".", "shape", "[", "1", "]", "%", "3", "==", "0", "\n", "# For models with space2depth stems", "\n", "conv_weight", "=", "conv_weight", ".", "reshape", "(", "O", ",", "I", "//", "3", ",", "3", ",", "J", ",", "K", ")", "\n", "conv_weight", "=", "conv_weight", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "conv_weight", "=", "conv_weight", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "", "", "elif", "in_chans", "!=", "3", ":", "\n", "        ", "if", "I", "!=", "3", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Weight format not supported by conversion.'", ")", "\n", "", "else", ":", "\n", "# NOTE this strategy should be better than random init, but there could be other combinations of", "\n", "# the original RGB input layer weights that'd work better for specific cases.", "\n", "            ", "repeat", "=", "int", "(", "math", ".", "ceil", "(", "in_chans", "/", "3", ")", ")", "\n", "conv_weight", "=", "conv_weight", ".", "repeat", "(", "1", ",", "repeat", ",", "1", ",", "1", ")", "[", ":", ",", ":", "in_chans", ",", ":", ",", ":", "]", "\n", "conv_weight", "*=", "(", "3", "/", "float", "(", "in_chans", ")", ")", "\n", "", "", "conv_weight", "=", "conv_weight", ".", "to", "(", "conv_type", ")", "\n", "return", "conv_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_pretrained": [[167, 235], ["default_cfg.get", "default_cfg.get", "default_cfg.get", "default_cfg.get", "default_cfg.get", "model.load_state_dict", "getattr", "_logger.warning", "hub.has_hf_hub", "_logger.info", "hub.load_state_dict_from_hf", "_logger.info", "hub.load_state_dict_from_url", "isinstance", "isinstance", "filter_fn", "filter_fn", "helpers.adapt_input_conv", "_logger.info", "_logger.warning"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.has_hf_hub", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.hub.load_state_dict_from_hf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_input_conv"], ["", "def", "load_pretrained", "(", "model", ",", "default_cfg", "=", "None", ",", "num_classes", "=", "1000", ",", "in_chans", "=", "3", ",", "filter_fn", "=", "None", ",", "strict", "=", "True", ",", "progress", "=", "False", ")", ":", "\n", "    ", "\"\"\" Load pretrained checkpoint\n\n    Args:\n        model (nn.Module) : PyTorch model module\n        default_cfg (Optional[Dict]): default configuration for pretrained weights / target dataset\n        num_classes (int): num_classes for model\n        in_chans (int): in_chans for model\n        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n        strict (bool): strict load of checkpoint\n        progress (bool): enable progress bar for weight download\n\n    \"\"\"", "\n", "default_cfg", "=", "default_cfg", "or", "getattr", "(", "model", ",", "'default_cfg'", ",", "None", ")", "or", "{", "}", "\n", "pretrained_url", "=", "default_cfg", ".", "get", "(", "'url'", ",", "None", ")", "\n", "hf_hub_id", "=", "default_cfg", ".", "get", "(", "'hf_hub'", ",", "None", ")", "\n", "if", "not", "pretrained_url", "and", "not", "hf_hub_id", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"No pretrained weights exist for this model. Using random initialization.\"", ")", "\n", "return", "\n", "", "if", "hf_hub_id", "and", "has_hf_hub", "(", "necessary", "=", "not", "pretrained_url", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Loading pretrained weights from Hugging Face hub ({hf_hub_id})'", ")", "\n", "state_dict", "=", "load_state_dict_from_hf", "(", "hf_hub_id", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Loading pretrained weights from url ({pretrained_url})'", ")", "\n", "state_dict", "=", "load_state_dict_from_url", "(", "pretrained_url", ",", "progress", "=", "progress", ",", "map_location", "=", "'cpu'", ")", "\n", "", "if", "filter_fn", "is", "not", "None", ":", "\n", "# for backwards compat with filter fn that take one arg, try one first, the two", "\n", "        ", "try", ":", "\n", "            ", "state_dict", "=", "filter_fn", "(", "state_dict", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "state_dict", "=", "filter_fn", "(", "state_dict", ",", "model", ")", "\n", "\n", "", "", "input_convs", "=", "default_cfg", ".", "get", "(", "'first_conv'", ",", "None", ")", "\n", "if", "input_convs", "is", "not", "None", "and", "in_chans", "!=", "3", ":", "\n", "        ", "if", "isinstance", "(", "input_convs", ",", "str", ")", ":", "\n", "            ", "input_convs", "=", "(", "input_convs", ",", ")", "\n", "", "for", "input_conv_name", "in", "input_convs", ":", "\n", "            ", "weight_name", "=", "input_conv_name", "+", "'.weight'", "\n", "try", ":", "\n", "                ", "state_dict", "[", "weight_name", "]", "=", "adapt_input_conv", "(", "in_chans", ",", "state_dict", "[", "weight_name", "]", ")", "\n", "_logger", ".", "info", "(", "\n", "f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)'", ")", "\n", "", "except", "NotImplementedError", "as", "e", ":", "\n", "                ", "del", "state_dict", "[", "weight_name", "]", "\n", "strict", "=", "False", "\n", "_logger", ".", "warning", "(", "\n", "f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.'", ")", "\n", "\n", "", "", "", "classifiers", "=", "default_cfg", ".", "get", "(", "'classifier'", ",", "None", ")", "\n", "label_offset", "=", "default_cfg", ".", "get", "(", "'label_offset'", ",", "0", ")", "\n", "if", "classifiers", "is", "not", "None", ":", "\n", "        ", "if", "isinstance", "(", "classifiers", ",", "str", ")", ":", "\n", "            ", "classifiers", "=", "(", "classifiers", ",", ")", "\n", "", "if", "num_classes", "!=", "default_cfg", "[", "'num_classes'", "]", ":", "\n", "            ", "for", "classifier_name", "in", "classifiers", ":", "\n", "# completely discard fully connected if model num_classes doesn't match pretrained weights", "\n", "                ", "del", "state_dict", "[", "classifier_name", "+", "'.weight'", "]", "\n", "del", "state_dict", "[", "classifier_name", "+", "'.bias'", "]", "\n", "", "strict", "=", "False", "\n", "", "elif", "label_offset", ">", "0", ":", "\n", "            ", "for", "classifier_name", "in", "classifiers", ":", "\n", "# special case for pretrained weights with an extra background class in pretrained weights", "\n", "                ", "classifier_weight", "=", "state_dict", "[", "classifier_name", "+", "'.weight'", "]", "\n", "state_dict", "[", "classifier_name", "+", "'.weight'", "]", "=", "classifier_weight", "[", "label_offset", ":", "]", "\n", "classifier_bias", "=", "state_dict", "[", "classifier_name", "+", "'.bias'", "]", "\n", "state_dict", "[", "classifier_name", "+", "'.bias'", "]", "=", "classifier_bias", "[", "label_offset", ":", "]", "\n", "\n", "", "", "", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.extract_layer": [[237, 253], ["layer.split.split", "hasattr", "hasattr", "hasattr", "l.isdigit", "getattr", "int"], "function", ["None"], ["", "def", "extract_layer", "(", "model", ",", "layer", ")", ":", "\n", "    ", "layer", "=", "layer", ".", "split", "(", "'.'", ")", "\n", "module", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "!=", "'module'", ":", "\n", "        ", "module", "=", "model", ".", "module", "\n", "", "if", "not", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "==", "'module'", ":", "\n", "        ", "layer", "=", "layer", "[", "1", ":", "]", "\n", "", "for", "l", "in", "layer", ":", "\n", "        ", "if", "hasattr", "(", "module", ",", "l", ")", ":", "\n", "            ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "                ", "module", "=", "getattr", "(", "module", ",", "l", ")", "\n", "", "else", ":", "\n", "                ", "module", "=", "module", "[", "int", "(", "l", ")", "]", "\n", "", "", "else", ":", "\n", "            ", "return", "module", "\n", "", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_layer": [[255, 277], ["layer.split.split", "setattr", "hasattr", "hasattr", "l.isdigit", "getattr", "l.isdigit", "getattr", "int", "int"], "function", ["None"], ["", "def", "set_layer", "(", "model", ",", "layer", ",", "val", ")", ":", "\n", "    ", "layer", "=", "layer", ".", "split", "(", "'.'", ")", "\n", "module", "=", "model", "\n", "if", "hasattr", "(", "model", ",", "'module'", ")", "and", "layer", "[", "0", "]", "!=", "'module'", ":", "\n", "        ", "module", "=", "model", ".", "module", "\n", "", "lst_index", "=", "0", "\n", "module2", "=", "module", "\n", "for", "l", "in", "layer", ":", "\n", "        ", "if", "hasattr", "(", "module2", ",", "l", ")", ":", "\n", "            ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "                ", "module2", "=", "getattr", "(", "module2", ",", "l", ")", "\n", "", "else", ":", "\n", "                ", "module2", "=", "module2", "[", "int", "(", "l", ")", "]", "\n", "", "lst_index", "+=", "1", "\n", "", "", "lst_index", "-=", "1", "\n", "for", "l", "in", "layer", "[", ":", "lst_index", "]", ":", "\n", "        ", "if", "not", "l", ".", "isdigit", "(", ")", ":", "\n", "            ", "module", "=", "getattr", "(", "module", ",", "l", ")", "\n", "", "else", ":", "\n", "            ", "module", "=", "module", "[", "int", "(", "l", ")", "]", "\n", "", "", "l", "=", "layer", "[", "lst_index", "]", "\n", "setattr", "(", "module", ",", "l", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_model_from_string": [[279, 327], ["model_string.split", "copy.deepcopy", "parent_module.named_modules", "copy.deepcopy.eval", "parent_module.eval", "k.split.split", "[].split", "helpers.extract_layer", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "conv", "helpers.set_layer", "torch.BatchNorm2d", "helpers.set_layer", "layers.Linear", "helpers.set_layer", "hasattr", "int"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.extract_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_layer"], ["", "def", "adapt_model_from_string", "(", "parent_module", ",", "model_string", ")", ":", "\n", "    ", "separator", "=", "'***'", "\n", "state_dict", "=", "{", "}", "\n", "lst_shape", "=", "model_string", ".", "split", "(", "separator", ")", "\n", "for", "k", "in", "lst_shape", ":", "\n", "        ", "k", "=", "k", ".", "split", "(", "':'", ")", "\n", "key", "=", "k", "[", "0", "]", "\n", "shape", "=", "k", "[", "1", "]", "[", "1", ":", "-", "1", "]", ".", "split", "(", "','", ")", "\n", "if", "shape", "[", "0", "]", "!=", "''", ":", "\n", "            ", "state_dict", "[", "key", "]", "=", "[", "int", "(", "i", ")", "for", "i", "in", "shape", "]", "\n", "\n", "", "", "new_module", "=", "deepcopy", "(", "parent_module", ")", "\n", "for", "n", ",", "m", "in", "parent_module", ".", "named_modules", "(", ")", ":", "\n", "        ", "old_module", "=", "extract_layer", "(", "parent_module", ",", "n", ")", "\n", "if", "isinstance", "(", "old_module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "old_module", ",", "Conv2dSame", ")", ":", "\n", "            ", "if", "isinstance", "(", "old_module", ",", "Conv2dSame", ")", ":", "\n", "                ", "conv", "=", "Conv2dSame", "\n", "", "else", ":", "\n", "                ", "conv", "=", "nn", ".", "Conv2d", "\n", "", "s", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "\n", "in_channels", "=", "s", "[", "1", "]", "\n", "out_channels", "=", "s", "[", "0", "]", "\n", "g", "=", "1", "\n", "if", "old_module", ".", "groups", ">", "1", ":", "\n", "                ", "in_channels", "=", "out_channels", "\n", "g", "=", "in_channels", "\n", "", "new_conv", "=", "conv", "(", "\n", "in_channels", "=", "in_channels", ",", "out_channels", "=", "out_channels", ",", "kernel_size", "=", "old_module", ".", "kernel_size", ",", "\n", "bias", "=", "old_module", ".", "bias", "is", "not", "None", ",", "padding", "=", "old_module", ".", "padding", ",", "dilation", "=", "old_module", ".", "dilation", ",", "\n", "groups", "=", "g", ",", "stride", "=", "old_module", ".", "stride", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_conv", ")", "\n", "", "if", "isinstance", "(", "old_module", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "            ", "new_bn", "=", "nn", ".", "BatchNorm2d", "(", "\n", "num_features", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "[", "0", "]", ",", "eps", "=", "old_module", ".", "eps", ",", "momentum", "=", "old_module", ".", "momentum", ",", "\n", "affine", "=", "old_module", ".", "affine", ",", "track_running_stats", "=", "True", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_bn", ")", "\n", "", "if", "isinstance", "(", "old_module", ",", "nn", ".", "Linear", ")", ":", "\n", "# FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?", "\n", "            ", "num_features", "=", "state_dict", "[", "n", "+", "'.weight'", "]", "[", "1", "]", "\n", "new_fc", "=", "Linear", "(", "\n", "in_features", "=", "num_features", ",", "out_features", "=", "old_module", ".", "out_features", ",", "bias", "=", "old_module", ".", "bias", "is", "not", "None", ")", "\n", "set_layer", "(", "new_module", ",", "n", ",", "new_fc", ")", "\n", "if", "hasattr", "(", "new_module", ",", "'num_features'", ")", ":", "\n", "                ", "new_module", ".", "num_features", "=", "num_features", "\n", "", "", "", "new_module", ".", "eval", "(", ")", "\n", "parent_module", ".", "eval", "(", ")", "\n", "\n", "return", "new_module", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_model_from_file": [[329, 333], ["os.path.join", "os.path.dirname", "open", "helpers.adapt_model_from_string", "f.read().strip", "f.read"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_model_from_string"], ["", "def", "adapt_model_from_file", "(", "parent_module", ",", "model_variant", ")", ":", "\n", "    ", "adapt_file", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "'pruned'", ",", "model_variant", "+", "'.txt'", ")", "\n", "with", "open", "(", "adapt_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "return", "adapt_model_from_string", "(", "parent_module", ",", "f", ".", "read", "(", ")", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.default_cfg_for_features": [[335, 342], ["copy.deepcopy", "copy.deepcopy.pop"], "function", ["None"], ["", "", "def", "default_cfg_for_features", "(", "default_cfg", ")", ":", "\n", "    ", "default_cfg", "=", "deepcopy", "(", "default_cfg", ")", "\n", "# remove default pretrained cfg fields that don't have much relevance for feature backbone", "\n", "to_remove", "=", "(", "'num_classes'", ",", "'crop_pct'", ",", "'classifier'", ",", "'global_pool'", ")", "# add default final pool size?", "\n", "for", "tr", "in", "to_remove", ":", "\n", "        ", "default_cfg", ".", "pop", "(", "tr", ",", "None", ")", "\n", "", "return", "default_cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.overlay_external_default_cfg": [[344, 352], ["kwargs.pop", "default_cfg.pop", "default_cfg.pop", "default_cfg.update"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "overlay_external_default_cfg", "(", "default_cfg", ",", "kwargs", ")", ":", "\n", "    ", "\"\"\" Overlay 'external_default_cfg' in kwargs on top of default_cfg arg.\n    \"\"\"", "\n", "external_default_cfg", "=", "kwargs", ".", "pop", "(", "'external_default_cfg'", ",", "None", ")", "\n", "if", "external_default_cfg", ":", "\n", "        ", "default_cfg", ".", "pop", "(", "'url'", ",", "None", ")", "# url should come from external cfg", "\n", "default_cfg", ".", "pop", "(", "'hf_hub'", ",", "None", ")", "# hf hub id should come from external cfg", "\n", "default_cfg", ".", "update", "(", "external_default_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_default_kwargs": [[354, 372], ["default_cfg.get", "kwargs.setdefault", "default_cfg.get", "default_cfg.get", "len", "kwargs.setdefault", "kwargs.setdefault", "len"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "", "def", "set_default_kwargs", "(", "kwargs", ",", "names", ",", "default_cfg", ")", ":", "\n", "    ", "for", "n", "in", "names", ":", "\n", "# for legacy reasons, model __init__args uses img_size + in_chans as separate args while", "\n", "# default_cfg has one input_size=(C, H ,W) entry", "\n", "        ", "if", "n", "==", "'img_size'", ":", "\n", "            ", "input_size", "=", "default_cfg", ".", "get", "(", "'input_size'", ",", "None", ")", "\n", "if", "input_size", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "input_size", ")", "==", "3", "\n", "kwargs", ".", "setdefault", "(", "n", ",", "input_size", "[", "-", "2", ":", "]", ")", "\n", "", "", "elif", "n", "==", "'in_chans'", ":", "\n", "            ", "input_size", "=", "default_cfg", ".", "get", "(", "'input_size'", ",", "None", ")", "\n", "if", "input_size", "is", "not", "None", ":", "\n", "                ", "assert", "len", "(", "input_size", ")", "==", "3", "\n", "kwargs", ".", "setdefault", "(", "n", ",", "input_size", "[", "0", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "default_val", "=", "default_cfg", ".", "get", "(", "n", ",", "None", ")", "\n", "if", "default_val", "is", "not", "None", ":", "\n", "                ", "kwargs", ".", "setdefault", "(", "n", ",", "default_cfg", "[", "n", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.filter_kwargs": [[374, 379], ["kwargs.pop"], "function", ["None"], ["", "", "", "", "def", "filter_kwargs", "(", "kwargs", ",", "names", ")", ":", "\n", "    ", "if", "not", "kwargs", "or", "not", "names", ":", "\n", "        ", "return", "\n", "", "for", "n", "in", "names", ":", "\n", "        ", "kwargs", ".", "pop", "(", "n", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.update_default_cfg_and_kwargs": [[381, 402], ["helpers.overlay_external_default_cfg", "default_cfg.get", "helpers.set_default_kwargs", "helpers.filter_kwargs"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.overlay_external_default_cfg", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.set_default_kwargs", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.filter_kwargs"], ["", "", "def", "update_default_cfg_and_kwargs", "(", "default_cfg", ",", "kwargs", ",", "kwargs_filter", ")", ":", "\n", "    ", "\"\"\" Update the default_cfg and kwargs before passing to model\n\n    FIXME this sequence of overlay default_cfg, set default kwargs, filter kwargs\n    could/should be replaced by an improved configuration mechanism\n\n    Args:\n        default_cfg: input default_cfg (updated in-place)\n        kwargs: keyword args passed to model build fn (updated in-place)\n        kwargs_filter: keyword arg keys that must be removed before model __init__\n    \"\"\"", "\n", "# Overlay default cfg values from `external_default_cfg` if it exists in kwargs", "\n", "overlay_external_default_cfg", "(", "default_cfg", ",", "kwargs", ")", "\n", "# Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)", "\n", "default_kwarg_names", "=", "(", "'num_classes'", ",", "'global_pool'", ",", "'in_chans'", ")", "\n", "if", "default_cfg", ".", "get", "(", "'fixed_input_size'", ",", "False", ")", ":", "\n", "# if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size", "\n", "        ", "default_kwarg_names", "+=", "(", "'img_size'", ",", ")", "\n", "", "set_default_kwargs", "(", "kwargs", ",", "names", "=", "default_kwarg_names", ",", "default_cfg", "=", "default_cfg", ")", "\n", "# Filter keyword args for task specific model variants (some 'features only' models, etc.)", "\n", "filter_kwargs", "(", "kwargs", ",", "names", "=", "kwargs_filter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.build_model_with_cfg": [[404, 486], ["kwargs.pop", "helpers.update_default_cfg_and_kwargs", "default_cfg.setdefault", "kwargs.pop", "copy.deepcopy", "feature_cfg.setdefault", "model_cls", "model_cls", "helpers.adapt_model_from_file", "getattr", "feature_cls.lower.", "helpers.default_cfg_for_features", "kwargs.pop", "kwargs.get", "helpers.load_custom_pretrained", "helpers.load_pretrained", "feature_cfg.pop", "isinstance", "feature_cls.lower.lower", "kwargs.get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.update_default_cfg_and_kwargs", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.adapt_model_from_file", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.default_cfg_for_features", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_custom_pretrained", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_pretrained", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "build_model_with_cfg", "(", "\n", "model_cls", ":", "Callable", ",", "\n", "variant", ":", "str", ",", "\n", "pretrained", ":", "bool", ",", "\n", "default_cfg", ":", "dict", ",", "\n", "model_cfg", ":", "Optional", "[", "Any", "]", "=", "None", ",", "\n", "feature_cfg", ":", "Optional", "[", "dict", "]", "=", "None", ",", "\n", "pretrained_strict", ":", "bool", "=", "True", ",", "\n", "pretrained_filter_fn", ":", "Optional", "[", "Callable", "]", "=", "None", ",", "\n", "pretrained_custom_load", ":", "bool", "=", "False", ",", "\n", "kwargs_filter", ":", "Optional", "[", "Tuple", "[", "str", "]", "]", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Build model with specified default_cfg and optional model_cfg\n\n    This helper fn aids in the construction of a model including:\n      * handling default_cfg and associated pretained weight loading\n      * passing through optional model_cfg for models with config based arch spec\n      * features_only model adaptation\n      * pruning config / model adaptation\n\n    Args:\n        model_cls (nn.Module): model class\n        variant (str): model variant name\n        pretrained (bool): load pretrained weights\n        default_cfg (dict): model's default pretrained/task config\n        model_cfg (Optional[Dict]): model's architecture config\n        feature_cfg (Optional[Dict]: feature extraction adapter config\n        pretrained_strict (bool): load pretrained weights strictly\n        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n        pretrained_custom_load (bool): use custom load fn, to load numpy or other non PyTorch weights\n        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n        **kwargs: model args passed through to model __init__\n    \"\"\"", "\n", "pruned", "=", "kwargs", ".", "pop", "(", "'pruned'", ",", "False", ")", "\n", "features", "=", "False", "\n", "feature_cfg", "=", "feature_cfg", "or", "{", "}", "\n", "default_cfg", "=", "deepcopy", "(", "default_cfg", ")", "if", "default_cfg", "else", "{", "}", "\n", "update_default_cfg_and_kwargs", "(", "default_cfg", ",", "kwargs", ",", "kwargs_filter", ")", "\n", "default_cfg", ".", "setdefault", "(", "'architecture'", ",", "variant", ")", "\n", "\n", "# Setup for feature extraction wrapper done at end of this fn", "\n", "if", "kwargs", ".", "pop", "(", "'features_only'", ",", "False", ")", ":", "\n", "        ", "features", "=", "True", "\n", "feature_cfg", ".", "setdefault", "(", "'out_indices'", ",", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", "\n", "if", "'out_indices'", "in", "kwargs", ":", "\n", "            ", "feature_cfg", "[", "'out_indices'", "]", "=", "kwargs", ".", "pop", "(", "'out_indices'", ")", "\n", "\n", "# Build the model", "\n", "", "", "model", "=", "model_cls", "(", "**", "kwargs", ")", "if", "model_cfg", "is", "None", "else", "model_cls", "(", "cfg", "=", "model_cfg", ",", "**", "kwargs", ")", "\n", "model", ".", "default_cfg", "=", "default_cfg", "\n", "\n", "if", "pruned", ":", "\n", "        ", "model", "=", "adapt_model_from_file", "(", "model", ",", "variant", ")", "\n", "\n", "# For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats", "\n", "", "num_classes_pretrained", "=", "0", "if", "features", "else", "getattr", "(", "model", ",", "'num_classes'", ",", "kwargs", ".", "get", "(", "'num_classes'", ",", "1000", ")", ")", "\n", "if", "pretrained", ":", "\n", "        ", "if", "pretrained_custom_load", ":", "\n", "            ", "load_custom_pretrained", "(", "model", ")", "\n", "", "else", ":", "\n", "            ", "load_pretrained", "(", "\n", "model", ",", "\n", "num_classes", "=", "num_classes_pretrained", ",", "\n", "in_chans", "=", "kwargs", ".", "get", "(", "'in_chans'", ",", "3", ")", ",", "\n", "filter_fn", "=", "pretrained_filter_fn", ",", "\n", "strict", "=", "pretrained_strict", ")", "\n", "\n", "# Wrap the model in a feature extraction module if enabled", "\n", "", "", "if", "features", ":", "\n", "        ", "feature_cls", "=", "FeatureListNet", "\n", "if", "'feature_cls'", "in", "feature_cfg", ":", "\n", "            ", "feature_cls", "=", "feature_cfg", ".", "pop", "(", "'feature_cls'", ")", "\n", "if", "isinstance", "(", "feature_cls", ",", "str", ")", ":", "\n", "                ", "feature_cls", "=", "feature_cls", ".", "lower", "(", ")", "\n", "if", "'hook'", "in", "feature_cls", ":", "\n", "                    ", "feature_cls", "=", "FeatureHookNet", "\n", "", "else", ":", "\n", "                    ", "assert", "False", ",", "f'Unknown feature class {feature_cls}'", "\n", "", "", "", "model", "=", "feature_cls", "(", "model", ",", "**", "feature_cfg", ")", "\n", "model", ".", "default_cfg", "=", "default_cfg_for_features", "(", "default_cfg", ")", "# add back default_cfg", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.model_parameters": [[488, 494], ["model.parameters", "model.parameters"], "function", ["None"], ["", "def", "model_parameters", "(", "model", ",", "exclude_head", "=", "False", ")", ":", "\n", "    ", "if", "exclude_head", ":", "\n", "# FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering", "\n", "        ", "return", "[", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", "[", ":", "-", "2", "]", "\n", "", "else", ":", "\n", "        ", "return", "model", ".", "parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_apply": [[496, 505], ["module.named_children", "fn", "helpers.named_apply", "fn"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_apply"], ["", "", "def", "named_apply", "(", "fn", ":", "Callable", ",", "module", ":", "nn", ".", "Module", ",", "name", "=", "''", ",", "depth_first", "=", "True", ",", "include_root", "=", "False", ")", "->", "nn", ".", "Module", ":", "\n", "    ", "if", "not", "depth_first", "and", "include_root", ":", "\n", "        ", "fn", "(", "module", "=", "module", ",", "name", "=", "name", ")", "\n", "", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "child_name", "=", "'.'", ".", "join", "(", "(", "name", ",", "child_name", ")", ")", "if", "name", "else", "child_name", "\n", "named_apply", "(", "fn", "=", "fn", ",", "module", "=", "child_module", ",", "name", "=", "child_name", ",", "depth_first", "=", "depth_first", ",", "include_root", "=", "True", ")", "\n", "", "if", "depth_first", "and", "include_root", ":", "\n", "        ", "fn", "(", "module", "=", "module", ",", "name", "=", "name", ")", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules": [[507, 516], ["module.named_children", "helpers.named_modules"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules"], ["", "def", "named_modules", "(", "module", ":", "nn", ".", "Module", ",", "name", "=", "''", ",", "depth_first", "=", "True", ",", "include_root", "=", "False", ")", ":", "\n", "    ", "if", "not", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "child_name", "=", "'.'", ".", "join", "(", "(", "name", ",", "child_name", ")", ")", "if", "name", "else", "child_name", "\n", "yield", "from", "named_modules", "(", "\n", "module", "=", "child_module", ",", "name", "=", "child_name", ",", "depth_first", "=", "depth_first", ",", "include_root", "=", "True", ")", "\n", "", "if", "depth_first", "and", "include_root", ":", "\n", "        ", "yield", "name", ",", "module", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.__init__": [[22, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "feature_info", ":", "List", "[", "Dict", "]", ",", "out_indices", ":", "Tuple", "[", "int", "]", ")", ":", "\n", "        ", "prev_reduction", "=", "1", "\n", "for", "fi", "in", "feature_info", ":", "\n", "# sanity check the mandatory fields, there may be additional fields depending on the model", "\n", "            ", "assert", "'num_chs'", "in", "fi", "and", "fi", "[", "'num_chs'", "]", ">", "0", "\n", "assert", "'reduction'", "in", "fi", "and", "fi", "[", "'reduction'", "]", ">=", "prev_reduction", "\n", "prev_reduction", "=", "fi", "[", "'reduction'", "]", "\n", "assert", "'module'", "in", "fi", "\n", "", "self", ".", "out_indices", "=", "out_indices", "\n", "self", ".", "info", "=", "feature_info", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.from_other": [[33, 35], ["features.FeatureInfo", "copy.deepcopy"], "methods", ["None"], ["", "def", "from_other", "(", "self", ",", "out_indices", ":", "Tuple", "[", "int", "]", ")", ":", "\n", "        ", "return", "FeatureInfo", "(", "deepcopy", "(", "self", ".", "info", ")", ",", "out_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.get": [[36, 48], ["isinstance"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "key", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" Get value by key at specified index (indices)\n        if idx == None, returns value for key at each output index\n        if idx is an integer, return value for that feature module index (ignoring output indices)\n        if idx is a list/tupple, return value for each module index (ignoring output indices)\n        \"\"\"", "\n", "if", "idx", "is", "None", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "[", "key", "]", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "if", "isinstance", "(", "idx", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "[", "key", "]", "for", "i", "in", "idx", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "info", "[", "idx", "]", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.get_dicts": [[49, 61], ["isinstance"], "methods", ["None"], ["", "", "def", "get_dicts", "(", "self", ",", "keys", "=", "None", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" return info dicts for specified keys (or all if None) at specified indices (or out_indices if None)\n        \"\"\"", "\n", "if", "idx", "is", "None", ":", "\n", "            ", "if", "keys", "is", "None", ":", "\n", "                ", "return", "[", "self", ".", "info", "[", "i", "]", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "else", ":", "\n", "                ", "return", "[", "{", "k", ":", "self", ".", "info", "[", "i", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "for", "i", "in", "self", ".", "out_indices", "]", "\n", "", "", "if", "isinstance", "(", "idx", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "return", "[", "self", ".", "info", "[", "i", "]", "if", "keys", "is", "None", "else", "{", "k", ":", "self", ".", "info", "[", "i", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "for", "i", "in", "idx", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "info", "[", "idx", "]", "if", "keys", "is", "None", "else", "{", "k", ":", "self", ".", "info", "[", "idx", "]", "[", "k", "]", "for", "k", "in", "keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.channels": [[62, 66], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "", "def", "channels", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature channels accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'num_chs'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.reduction": [[67, 71], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "reduction", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature reduction (output stride) accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'reduction'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.module_name": [[72, 76], ["features.FeatureInfo.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "module_name", "(", "self", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\" feature module name accessor\n        \"\"\"", "\n", "return", "self", ".", "get", "(", "'module'", ",", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.__getitem__": [[77, 79], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "info", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.__len__": [[80, 82], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHooks.__init__": [[92, 108], ["enumerate", "collections.defaultdict", "functools.partial", "m.register_forward_pre_hook", "m.register_forward_hook"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "hooks", ",", "named_modules", ",", "out_map", "=", "None", ",", "default_hook_type", "=", "'forward'", ")", ":", "\n", "# setup feature hooks", "\n", "        ", "modules", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "named_modules", "}", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "hooks", ")", ":", "\n", "            ", "hook_name", "=", "h", "[", "'module'", "]", "\n", "m", "=", "modules", "[", "hook_name", "]", "\n", "hook_id", "=", "out_map", "[", "i", "]", "if", "out_map", "else", "hook_name", "\n", "hook_fn", "=", "partial", "(", "self", ".", "_collect_output_hook", ",", "hook_id", ")", "\n", "hook_type", "=", "h", "[", "'hook_type'", "]", "if", "'hook_type'", "in", "h", "else", "default_hook_type", "\n", "if", "hook_type", "==", "'forward_pre'", ":", "\n", "                ", "m", ".", "register_forward_pre_hook", "(", "hook_fn", ")", "\n", "", "elif", "hook_type", "==", "'forward'", ":", "\n", "                ", "m", ".", "register_forward_hook", "(", "hook_fn", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"Unsupported hook type\"", "\n", "", "", "self", ".", "_feature_outputs", "=", "defaultdict", "(", "OrderedDict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHooks._collect_output_hook": [[109, 114], ["isinstance"], "methods", ["None"], ["", "def", "_collect_output_hook", "(", "self", ",", "hook_id", ",", "*", "args", ")", ":", "\n", "        ", "x", "=", "args", "[", "-", "1", "]", "# tensor we want is last argument, output for fwd, input for fwd_pre", "\n", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "            ", "x", "=", "x", "[", "0", "]", "# unwrap input tuple", "\n", "", "self", ".", "_feature_outputs", "[", "x", ".", "device", "]", "[", "hook_id", "]", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHooks.get_output": [[115, 119], ["collections.OrderedDict"], "methods", ["None"], ["", "def", "get_output", "(", "self", ",", "device", ")", "->", "Dict", "[", "str", ",", "torch", ".", "tensor", "]", ":", "\n", "        ", "output", "=", "self", ".", "_feature_outputs", "[", "device", "]", "\n", "self", ".", "_feature_outputs", "[", "device", "]", "=", "OrderedDict", "(", ")", "# clear after reading", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureDictNet.__init__": [[177, 199], ["torch.ModuleDict.__init__", "features._get_feature_info", "features._get_return_layers", "features._module_list", "set", "collections.OrderedDict", "features.FeatureDictNet.update", "_get_return_layers.keys", "str", "set.remove", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._get_feature_info", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._get_return_layers", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._module_list", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "        ", "super", "(", "FeatureDictNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "feature_info", "=", "_get_feature_info", "(", "model", ",", "out_indices", ")", "\n", "self", ".", "concat", "=", "feature_concat", "\n", "self", ".", "return_layers", "=", "{", "}", "\n", "return_layers", "=", "_get_return_layers", "(", "self", ".", "feature_info", ",", "out_map", ")", "\n", "modules", "=", "_module_list", "(", "model", ",", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "remaining", "=", "set", "(", "return_layers", ".", "keys", "(", ")", ")", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "for", "new_name", ",", "old_name", ",", "module", "in", "modules", ":", "\n", "            ", "layers", "[", "new_name", "]", "=", "module", "\n", "if", "old_name", "in", "remaining", ":", "\n", "# return id has to be consistently str type for torchscript", "\n", "                ", "self", ".", "return_layers", "[", "new_name", "]", "=", "str", "(", "return_layers", "[", "old_name", "]", ")", "\n", "remaining", ".", "remove", "(", "old_name", ")", "\n", "", "if", "not", "remaining", ":", "\n", "                ", "break", "\n", "", "", "assert", "not", "remaining", "and", "len", "(", "self", ".", "return_layers", ")", "==", "len", "(", "return_layers", ")", ",", "f'Return layers ({remaining}) are not present in model'", "\n", "self", ".", "update", "(", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureDictNet._collect": [[200, 213], ["collections.OrderedDict", "features.FeatureDictNet.items", "module", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "_collect", "(", "self", ",", "x", ")", "->", "(", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "out", "=", "OrderedDict", "(", ")", "\n", "for", "name", ",", "module", "in", "self", ".", "items", "(", ")", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "if", "name", "in", "self", ".", "return_layers", ":", "\n", "                ", "out_id", "=", "self", ".", "return_layers", "[", "name", "]", "\n", "if", "isinstance", "(", "x", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "# If model tap is a tuple or list, concat or select first element", "\n", "# FIXME this may need to be more generic / flexible for some nets", "\n", "                    ", "out", "[", "out_id", "]", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "if", "self", ".", "concat", "else", "x", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "out", "[", "out_id", "]", "=", "x", "\n", "", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureDictNet.forward": [[214, 216], ["features.FeatureDictNet._collect"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureDictNet._collect"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "return", "self", ".", "_collect", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureListNet.__init__": [[224, 230], ["features.FeatureDictNet.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "        ", "super", "(", "FeatureListNet", ",", "self", ")", ".", "__init__", "(", "\n", "model", ",", "out_indices", "=", "out_indices", ",", "out_map", "=", "out_map", ",", "feature_concat", "=", "feature_concat", ",", "\n", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureListNet.forward": [[231, 233], ["list", "features.FeatureListNet._collect().values", "features.FeatureListNet._collect"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureDictNet._collect"], ["", "def", "forward", "(", "self", ",", "x", ")", "->", "(", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "_collect", "(", "x", ")", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHookNet.__init__": [[248, 279], ["torch.ModuleDict.__init__", "features._get_feature_info", "collections.OrderedDict", "features.FeatureHookNet.update", "features.FeatureHooks", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "hasattr", "hooks.extend", "features._module_list", "model.named_modules", "model.reset_classifier", "features.FeatureHookNet.feature_info.get_dicts", "module.named_modules", "features.FeatureHookNet.feature_info.get_dicts", "hooks.append", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._get_feature_info", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._module_list", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.reset_classifier", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.get_dicts", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.get_dicts"], ["def", "__init__", "(", "\n", "self", ",", "model", ",", "\n", "out_indices", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "out_map", "=", "None", ",", "out_as_dict", "=", "False", ",", "no_rewrite", "=", "False", ",", "\n", "feature_concat", "=", "False", ",", "flatten_sequential", "=", "False", ",", "default_hook_type", "=", "'forward'", ")", ":", "\n", "        ", "super", "(", "FeatureHookNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", "\n", "self", ".", "feature_info", "=", "_get_feature_info", "(", "model", ",", "out_indices", ")", "\n", "self", ".", "out_as_dict", "=", "out_as_dict", "\n", "layers", "=", "OrderedDict", "(", ")", "\n", "hooks", "=", "[", "]", "\n", "if", "no_rewrite", ":", "\n", "            ", "assert", "not", "flatten_sequential", "\n", "if", "hasattr", "(", "model", ",", "'reset_classifier'", ")", ":", "# make sure classifier is removed?", "\n", "                ", "model", ".", "reset_classifier", "(", "0", ")", "\n", "", "layers", "[", "'body'", "]", "=", "model", "\n", "hooks", ".", "extend", "(", "self", ".", "feature_info", ".", "get_dicts", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "modules", "=", "_module_list", "(", "model", ",", "flatten_sequential", "=", "flatten_sequential", ")", "\n", "remaining", "=", "{", "f", "[", "'module'", "]", ":", "f", "[", "'hook_type'", "]", "if", "'hook_type'", "in", "f", "else", "default_hook_type", "\n", "for", "f", "in", "self", ".", "feature_info", ".", "get_dicts", "(", ")", "}", "\n", "for", "new_name", ",", "old_name", ",", "module", "in", "modules", ":", "\n", "                ", "layers", "[", "new_name", "]", "=", "module", "\n", "for", "fn", ",", "fm", "in", "module", ".", "named_modules", "(", "prefix", "=", "old_name", ")", ":", "\n", "                    ", "if", "fn", "in", "remaining", ":", "\n", "                        ", "hooks", ".", "append", "(", "dict", "(", "module", "=", "fn", ",", "hook_type", "=", "remaining", "[", "fn", "]", ")", ")", "\n", "del", "remaining", "[", "fn", "]", "\n", "", "", "if", "not", "remaining", ":", "\n", "                    ", "break", "\n", "", "", "assert", "not", "remaining", ",", "f'Return layers ({remaining}) are not present in model'", "\n", "", "self", ".", "update", "(", "layers", ")", "\n", "self", ".", "hooks", "=", "FeatureHooks", "(", "hooks", ",", "model", ".", "named_modules", "(", ")", ",", "out_map", "=", "out_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHookNet.forward": [[280, 285], ["features.FeatureHookNet.items", "features.FeatureHookNet.hooks.get_output", "module", "list", "features.FeatureHookNet.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureHooks.get_output"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "name", ",", "module", "in", "self", ".", "items", "(", ")", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "", "out", "=", "self", ".", "hooks", ".", "get_output", "(", "x", ".", "device", ")", "\n", "return", "out", "if", "self", ".", "out_as_dict", "else", "list", "(", "out", ".", "values", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._module_list": [[121, 133], ["module.named_children", "isinstance", "module.named_children", "ml.append", "ml.append"], "function", ["None"], ["", "", "def", "_module_list", "(", "module", ",", "flatten_sequential", "=", "False", ")", ":", "\n", "# a yield/iter would be better for this but wouldn't be compatible with torchscript", "\n", "    ", "ml", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "if", "flatten_sequential", "and", "isinstance", "(", "module", ",", "nn", ".", "Sequential", ")", ":", "\n", "# first level of Sequential containers is flattened into containing model", "\n", "            ", "for", "child_name", ",", "child_module", "in", "module", ".", "named_children", "(", ")", ":", "\n", "                ", "combined", "=", "[", "name", ",", "child_name", "]", "\n", "ml", ".", "append", "(", "(", "'_'", ".", "join", "(", "combined", ")", ",", "'.'", ".", "join", "(", "combined", ")", ",", "child_module", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "ml", ".", "append", "(", "(", "name", ",", "name", ",", "module", ")", ")", "\n", "", "", "return", "ml", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._get_feature_info": [[135, 143], ["getattr", "isinstance", "getattr.from_other", "isinstance", "features.FeatureInfo"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.from_other"], ["", "def", "_get_feature_info", "(", "net", ",", "out_indices", ")", ":", "\n", "    ", "feature_info", "=", "getattr", "(", "net", ",", "'feature_info'", ")", "\n", "if", "isinstance", "(", "feature_info", ",", "FeatureInfo", ")", ":", "\n", "        ", "return", "feature_info", ".", "from_other", "(", "out_indices", ")", "\n", "", "elif", "isinstance", "(", "feature_info", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "FeatureInfo", "(", "net", ".", "feature_info", ",", "out_indices", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"Provided feature_info is not valid\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features._get_return_layers": [[145, 151], ["feature_info.module_name", "enumerate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.features.FeatureInfo.module_name"], ["", "", "def", "_get_return_layers", "(", "feature_info", ",", "out_map", ")", ":", "\n", "    ", "module_names", "=", "feature_info", ".", "module_name", "(", ")", "\n", "return_layers", "=", "{", "}", "\n", "for", "i", ",", "name", "in", "enumerate", "(", "module_names", ")", ":", "\n", "        ", "return_layers", "[", "name", "]", "=", "out_map", "[", "i", "]", "if", "out_map", "is", "not", "None", "else", "feature_info", ".", "out_indices", "[", "i", "]", "\n", "", "return", "return_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.__init__": [[69, 103], ["torch.Module.__init__", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "v3d.V3D.init_weights", "layers.create_classifier", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.create_classifier", "layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.create_classifier", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.create_classifier"], ["    ", "def", "__init__", "(", "self", ",", "feature_extractor", ",", "num_classes", "=", "1000", ",", "pretrained", "=", "False", ",", "num_features", "=", "512", ",", "pretrain_dataset", "=", "'k400'", ",", "drop_rate", "=", "0.", ",", "default_cfg", "=", "None", ",", "drop_path_rate", "=", "0.", ",", "\n", "model_name", "=", "None", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "super", "(", "V3D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "default_cfg", "=", "default_cfg", "\n", "self", ".", "model_name", "=", "model_name", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "checkpoint_or_url", "=", "default_urls", "[", "model_name", "]", "[", "pretrain_dataset", "]", "\n", "self", ".", "pretrain_dataset", "=", "pretrain_dataset", "\n", "", "else", ":", "\n", "            ", "self", ".", "checkpoint_or_url", "=", "None", "\n", "\n", "", "self", ".", "get_feature", "=", "feature_extractor", "\n", "\n", "# Head + Pooling", "\n", "self", ".", "global_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "\n", "if", "self", ".", "model_name", "==", "'x3d'", ":", "\n", "            ", "_", ",", "self", ".", "fc1", "=", "create_classifier", "(", "\n", "self", ".", "get_feature", ".", "feat_dim", ",", "num_features", ",", "pool_type", "=", "global_pool", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "_", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "", "else", ":", "\n", "            ", "_", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n", "# if not pretrained:", "\n", "#     print('Using the one-init instead of zero-init')", "\n", "", "self", ".", "init_weights", "(", "pretrained", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.init_weights": [[105, 120], ["v3d.V3D.get_feature._init_weights", "v3d.V3D.named_modules", "_logger.info", "isinstance", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "isinstance", "torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules"], ["", "def", "init_weights", "(", "self", ",", "pretrained", ")", ":", "\n", "        ", "if", "pretrained", ":", "\n", "            ", "self", ".", "get_feature", ".", "_init_weights", "(", "self", ".", "get_feature", ",", "pretrained", "=", "self", ".", "checkpoint_or_url", ")", "\n", "if", "self", ".", "pretrain_dataset", "==", "'imagenet'", ":", "\n", "                ", "self", ".", "classifier", ".", "weight", "=", "load_state_dict_from_url", "(", "self", ".", "checkpoint_or_url", ")", "[", "'fc.weight'", "]", "\n", "self", ".", "classifier", ".", "bias", "=", "load_state_dict_from_url", "(", "self", ".", "checkpoint_or_url", ")", "[", "'fc.bias'", "]", "\n", "_logger", ".", "info", "(", "'loading the classifier successfully'", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "for", "n", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                    ", "nn", ".", "init", ".", "ones_", "(", "m", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "m", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.get_classifier": [[121, 123], ["None"], "methods", ["None"], ["", "", "", "", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "classifier", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.reset_classifier": [[124, 128], ["layers.create_classifier"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.create_classifier"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "'avg'", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "_", ",", "self", ".", "classifier", "=", "create_classifier", "(", "\n", "self", ".", "num_features", ",", "self", ".", "num_classes", ",", "pool_type", "=", "global_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.forward": [[129, 152], ["v3d.V3D.unsqueeze", "v3d.V3D.get_feature", "isinstance", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "v3d.V3D.global_pool", "v3d.V3D.view", "v3d.V3D.classifier", "v3d.V3D.global_pool", "v3d.V3D.view", "torch.dropout", "torch.dropout", "torch.dropout", "v3d.V3D.fc1", "v3d.V3D.relu", "v3d.V3D.classifier", "v3d.V3D.classifier", "v3d.V3D.size", "v3d.V3D.size", "v3d.V3D.global_pool", "v3d.V3D.global_pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "2", ")", "\n", "x", "=", "self", ".", "get_feature", "(", "x", ")", "\n", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "           ", "x", "=", "torch", ".", "cat", "(", "[", "self", ".", "global_pool", "(", "x", "[", "0", "]", ")", ",", "self", ".", "global_pool", "(", "x", "[", "1", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "", "if", "self", ".", "drop_rate", ">", "0.", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "drop_rate", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "self", ".", "model_name", "==", "'x3d'", ":", "\n", "            ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "classifier", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d._cfg": [[28, 37], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "# 'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,", "\n", "'crop_pct'", ":", ".875", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.filter_pretrained_weight": [[155, 166], ["model.state_dict", "state_dict.items", "torch.hub.load_state_dict_from_url", "k.replace.replace", "print"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.lookahead.Lookahead.state_dict"], ["", "", "", "def", "filter_pretrained_weight", "(", "pretrained_url", ",", "model", ")", ":", "\n", "\n", "    ", "state_dict", "=", "load_state_dict_from_url", "(", "pretrained_url", ",", "progress", "=", "False", ",", "map_location", "=", "'cpu'", ")", "[", "'state_dict'", "]", "\n", "model_state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "k", "=", "k", ".", "replace", "(", "'backbone.'", ",", "''", ")", "\n", "if", "k", "in", "model_state_dict", ":", "\n", "            ", "model_state_dict", "[", "k", "]", "=", "v", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Not using the weight of: '", ",", "k", ")", "\n", "", "", "return", "model_state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_slowonly50_224": [[169, 187], ["video_models.resnet3d_slowonly.ResNet3dSlowOnly", "v3d.V3D"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_slowonly50_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_slowonly50_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "pretrained2d", "=", "kwargs", "[", "'pretrained2d'", "]", "\n", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", "\n", "drop_path_rate", "=", "kwargs", "[", "'drop_path_rate'", "]", "\n", "kwargs", "=", "{", "}", "\n", "\n", "kwargs", "[", "'pretrained2d'", "]", "=", "pretrained2d", "\n", "kwargs", "[", "'drop_path_rate'", "]", "=", "drop_path_rate", "\n", "slowonly50", "=", "ResNet3dSlowOnly", "(", "depth", "=", "50", ",", "reshape_t", "=", "False", ",", "norm_eval", "=", "False", ",", "reshape_st", "=", "False", ",", "**", "kwargs", ")", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "slowonly50", ",", "num_features", "=", "2048", ",", "num_classes", "=", "num_classes", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'slowonly'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_slowfast50_4x16_224": [[190, 227], ["video_models.ResNet3dSlowFast", "v3d.V3D", "dict", "dict"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_slowfast50_4x16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_slowfast50_4x16_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "\n", "slowfast50", "=", "ResNet3dSlowFast", "(", "slow_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "True", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "conv1_kernel", "=", "(", "1", ",", "7", ",", "7", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "fusion_kernel", "=", "5", ",", "\n", "inflate", "=", "(", "0", ",", "0", ",", "1", ",", "1", ")", ",", "\n", "drop_path_rate", "=", "0.", ")", ",", "\n", "fast_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "base_channels", "=", "8", ",", "\n", "conv1_kernel", "=", "(", "5", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "drop_path_rate", "=", "0.", ")", ")", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "slowfast50", ",", "num_features", "=", "2304", ",", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'slowfast'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_slowfast50_8x8_224": [[228, 265], ["video_models.ResNet3dSlowFast", "v3d.V3D", "dict", "dict"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_slowfast50_8x8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_slowfast50_8x8_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "\n", "slowfast50", "=", "ResNet3dSlowFast", "(", "slow_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "True", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "conv1_kernel", "=", "(", "1", ",", "7", ",", "7", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "fusion_kernel", "=", "7", ",", "\n", "inflate", "=", "(", "0", ",", "0", ",", "1", ",", "1", ")", ",", "\n", "drop_path_rate", "=", "0.", ")", ",", "\n", "fast_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "base_channels", "=", "8", ",", "\n", "conv1_kernel", "=", "(", "5", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "drop_path_rate", "=", "0.", ")", ")", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "slowfast50", ",", "num_features", "=", "2304", ",", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'slowfast'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_r2plus1d34_224": [[266, 295], ["dict", "video_models.ResNet2Plus1d", "v3d.V3D", "dict", "dict"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_r2plus1d34_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_r2plus1d34_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "\n", "args", "=", "dict", "(", "\n", "depth", "=", "34", ",", "\n", "pretrained", "=", "pretrained", ",", "\n", "pretrained2d", "=", "False", ",", "\n", "norm_eval", "=", "False", ",", "\n", "frozen_stages", "=", "0", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv2plus1d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ",", "eps", "=", "1e-3", ")", ",", "\n", "conv1_kernel", "=", "(", "3", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "inflate", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "spatial_strides", "=", "(", "1", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "temporal_strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "zero_init_residual", "=", "False", ",", "\n", "drop_path_rate", "=", "kwargs", "[", "'drop_path_rate'", "]", ")", "\n", "\n", "resnet2plus1d", "=", "ResNet2Plus1d", "(", "**", "args", ")", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "resnet2plus1d", ",", "num_features", "=", "512", ",", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'r2plus1d34'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_csn50_ir_224": [[298, 322], ["dict", "video_models.resnet_csn.ResNet3dCSN", "v3d.V3D"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_csn50_ir_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_csn50_ir_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "\n", "args", "=", "dict", "(", "\n", "pretrained", "=", "None", ",", "\n", "depth", "=", "50", ",", "\n", "norm_eval", "=", "False", ",", "\n", "bottleneck_mode", "=", "'ir'", ",", "\n", "with_pool2", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "zero_init_residual", "=", "False", ",", "\n", "# frozen_stages=4,", "\n", "drop_path_rate", "=", "kwargs", "[", "'drop_path_rate'", "]", ")", "\n", "csn50", "=", "ResNet3dCSN", "(", "**", "args", ")", "\n", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "csn50", ",", "num_features", "=", "2048", ",", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'ir_csn_50'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.v3d_x3d_s_224": [[326, 341], ["dict", "video_models.X3D", "v3d.V3D"], "function", ["None"], ["", "@", "register_model", "\n", "def", "v3d_x3d_s_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" v3d_x3d_s_224\n    \"\"\"", "\n", "pretrain_dataset", "=", "kwargs", "[", "'pretrain_dataset'", "]", "\n", "num_classes", "=", "kwargs", "[", "'num_classes'", "]", "\n", "\n", "args", "=", "dict", "(", "\n", "gamma_w", "=", "1", ",", "gamma_b", "=", "2.25", ",", "gamma_d", "=", "2.2", ")", "\n", "x3d", "=", "X3D", "(", "**", "args", ")", "\n", "\n", "model", "=", "V3D", "(", "feature_extractor", "=", "x3d", ",", "num_features", "=", "2048", ",", "num_classes", "=", "num_classes", ",", "pretrained", "=", "pretrained", ",", "\n", "pretrain_dataset", "=", "pretrain_dataset", ",", "model_name", "=", "'x3d'", ",", "\n", "default_cfg", "=", "default_cfgs", ")", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_backbone": [[28, 31], ["BACKBONES.build"], "function", ["None"], ["def", "build_dataset", "(", "cfg", ",", "default_args", "=", "None", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_head": [[33, 36], ["HEADS.build"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_recognizer": [[38, 54], ["RECOGNIZERS.build", "warnings.warn", "cfg.get", "cfg.get", "dict"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["\n", "dataset", "=", "build_from_cfg", "(", "cfg", ",", "DATASETS", ",", "default_args", ")", "\n", "return", "dataset", "\n", "\n", "\n", "", "def", "build_dataloader", "(", "dataset", ",", "\n", "videos_per_gpu", ",", "\n", "workers_per_gpu", ",", "\n", "num_gpus", "=", "1", ",", "\n", "dist", "=", "True", ",", "\n", "shuffle", "=", "True", ",", "\n", "seed", "=", "None", ",", "\n", "drop_last", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "persistent_workers", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss": [[56, 59], ["LOSSES.build"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_localizer": [[61, 64], ["LOCALIZERS.build"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_model": [[66, 87], ["cfg.copy", "cfg.copy.pop", "ValueError", "builder.build_localizer", "builder.build_recognizer", "build_detector", "ImportError", "warnings.warn"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_localizer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_recognizer"], ["\n", "rank", ",", "world_size", "=", "get_dist_info", "(", ")", "\n", "sample_by_class", "=", "getattr", "(", "dataset", ",", "'sample_by_class'", ",", "False", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_neck": [[90, 93], ["NECKS.build"], "function", ["None"], ["        ", "if", "sample_by_class", ":", "\n", "            ", "dynamic_length", "=", "getattr", "(", "dataset", ",", "'dynamic_length'", ",", "True", ")", "\n", "sampler", "=", "ClassSpecificDistributedSampler", "(", "\n", "dataset", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.trace_utils._float_to_int": [[8, 14], ["int"], "function", ["None"], ["", "", "def", "_float_to_int", "(", "x", ":", "float", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Symbolic tracing helper to substitute for inbuilt `int`.\n    Hint: Inbuilt `int` can't accept an argument of type `Proxy`\n    \"\"\"", "\n", "return", "int", "(", "x", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.SwishJit.__init__": [[33, 35], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "SwishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.SwishJit.forward": [[36, 38], ["activations_jit.swish_jit"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.swish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "swish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.MishJit.__init__": [[41, 43], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "MishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.MishJit.forward": [[44, 46], ["activations_jit.mish_jit"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.mish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "mish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardSigmoidJit.__init__": [[55, 57], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoidJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardSigmoidJit.forward": [[58, 60], ["activations_jit.hard_sigmoid_jit"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_sigmoid_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_sigmoid_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardSwishJit.__init__": [[69, 71], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardSwishJit.forward": [[72, 74], ["activations_jit.hard_swish_jit"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_swish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_swish_jit", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardMishJit.__init__": [[86, 88], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMishJit", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.HardMishJit.forward": [[89, 91], ["activations_jit.hard_mish_jit"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_mish_jit"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_mish_jit", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.swish_jit": [[18, 23], ["x.mul", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "x", ".", "sigmoid", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.mish_jit": [[25, 30], ["x.mul", "torch.nn.functional.softplus().tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit", "(", "x", ",", "_inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_sigmoid_jit": [[48, 52], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "# return F.relu6(x + 3.) / 6.", "\n", "    ", "return", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "# clamp seems ever so slightly faster?", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_swish_jit": [[62, 66], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "# return x * (F.relu6(x + 3.) / 6)", "\n", "    ", "return", "x", "*", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "# clamp seems ever so slightly faster?", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_jit.hard_mish_jit": [[76, 83], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.test_time_pool.TestTimePoolHead.__init__": [[17, 30], ["torch.nn.Module.__init__", "test_time_pool.TestTimePoolHead.base.get_classifier", "isinstance", "test_time_pool.TestTimePoolHead.base.reset_classifier", "torch.nn.Conv2d", "test_time_pool.TestTimePoolHead.fc.weight.data.copy_", "test_time_pool.TestTimePoolHead.fc.bias.data.copy_", "test_time_pool.TestTimePoolHead.weight.data.view", "test_time_pool.TestTimePoolHead.bias.data.view", "test_time_pool.TestTimePoolHead.fc.weight.size", "test_time_pool.TestTimePoolHead.fc.bias.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.get_classifier", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.v3d.V3D.reset_classifier"], ["    ", "def", "__init__", "(", "self", ",", "base", ",", "original_pool", "=", "7", ")", ":", "\n", "        ", "super", "(", "TestTimePoolHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "base", "=", "base", "\n", "self", ".", "original_pool", "=", "original_pool", "\n", "base_fc", "=", "self", ".", "base", ".", "get_classifier", "(", ")", "\n", "if", "isinstance", "(", "base_fc", ",", "nn", ".", "Conv2d", ")", ":", "\n", "            ", "self", ".", "fc", "=", "base_fc", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc", "=", "nn", ".", "Conv2d", "(", "\n", "self", ".", "base", ".", "num_features", ",", "self", ".", "base", ".", "num_classes", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "fc", ".", "weight", ".", "data", ".", "copy_", "(", "base_fc", ".", "weight", ".", "data", ".", "view", "(", "self", ".", "fc", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "self", ".", "fc", ".", "bias", ".", "data", ".", "copy_", "(", "base_fc", ".", "bias", ".", "data", ".", "view", "(", "self", ".", "fc", ".", "bias", ".", "size", "(", ")", ")", ")", "\n", "", "self", ".", "base", ".", "reset_classifier", "(", "0", ")", "# delete original fc layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.test_time_pool.TestTimePoolHead.forward": [[31, 37], ["test_time_pool.TestTimePoolHead.base.forward_features", "torch.avg_pool2d", "test_time_pool.TestTimePoolHead.fc", "adaptive_avgmax_pool.adaptive_avgmax_pool2d", "adaptive_avgmax_pool.adaptive_avgmax_pool2d.view", "adaptive_avgmax_pool.adaptive_avgmax_pool2d.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "base", ".", "forward_features", "(", "x", ")", "\n", "x", "=", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", "=", "self", ".", "original_pool", ",", "stride", "=", "1", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "x", "=", "adaptive_avgmax_pool2d", "(", "x", ",", "1", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.test_time_pool.apply_test_time_pool": [[39, 53], ["_logger.info", "test_time_pool.TestTimePoolHead", "hasattr", "str", "str"], "function", ["None"], ["", "", "def", "apply_test_time_pool", "(", "model", ",", "config", ",", "use_test_size", "=", "True", ")", ":", "\n", "    ", "test_time_pool", "=", "False", "\n", "if", "not", "hasattr", "(", "model", ",", "'default_cfg'", ")", "or", "not", "model", ".", "default_cfg", ":", "\n", "        ", "return", "model", ",", "False", "\n", "", "if", "use_test_size", "and", "'test_input_size'", "in", "model", ".", "default_cfg", ":", "\n", "        ", "df_input_size", "=", "model", ".", "default_cfg", "[", "'test_input_size'", "]", "\n", "", "else", ":", "\n", "        ", "df_input_size", "=", "model", ".", "default_cfg", "[", "'input_size'", "]", "\n", "", "if", "config", "[", "'input_size'", "]", "[", "-", "1", "]", ">", "df_input_size", "[", "-", "1", "]", "and", "config", "[", "'input_size'", "]", "[", "-", "2", "]", ">", "df_input_size", "[", "-", "2", "]", ":", "\n", "        ", "_logger", ".", "info", "(", "'Target input size %s > pretrained default %s, using test time pooling'", "%", "\n", "(", "str", "(", "config", "[", "'input_size'", "]", "[", "-", "2", ":", "]", ")", ",", "str", "(", "df_input_size", "[", "-", "2", ":", "]", ")", ")", ")", "\n", "model", "=", "TestTimePoolHead", "(", "model", ",", "original_pool", "=", "model", ".", "default_cfg", "[", "'pool_size'", "]", ")", "\n", "test_time_pool", "=", "True", "\n", "", "return", "model", ",", "test_time_pool", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.Conv2dSame.__init__": [[24, 28], ["torch.Conv2d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "Conv2dSame", ",", "self", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", ",", "0", ",", "dilation", ",", "groups", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.Conv2dSame.forward": [[29, 31], ["conv2d_same.conv2d_same"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.conv2d_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "conv2d_same", "(", "x", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.conv2d_same": [[13, 18], ["padding.pad_same", "torch.conv2d"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["def", "conv2d_same", "(", "\n", "x", ",", "weight", ":", "torch", ".", "Tensor", ",", "bias", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "stride", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "0", ",", "0", ")", ",", "dilation", ":", "Tuple", "[", "int", ",", "int", "]", "=", "(", "1", ",", "1", ")", ",", "groups", ":", "int", "=", "1", ")", ":", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "weight", ".", "shape", "[", "-", "2", ":", "]", ",", "stride", ",", "dilation", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "bias", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.create_conv2d_pad": [[33, 41], ["kwargs.pop", "kwargs.setdefault", "padding.get_padding_value", "conv2d_same.Conv2dSame", "torch.Conv2d"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value"], ["", "", "def", "create_conv2d_pad", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "padding", "=", "kwargs", ".", "pop", "(", "'padding'", ",", "''", ")", "\n", "kwargs", ".", "setdefault", "(", "'bias'", ",", "False", ")", "\n", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "if", "is_dynamic", ":", "\n", "        ", "return", "Conv2dSame", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "return", "nn", ".", "Conv2d", "(", "in_chs", ",", "out_chs", ",", "kernel_size", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.StdConv2d.__init__": [[32, 41], ["torch.Conv2d.__init__", "padding.get_padding.get_padding"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding"], ["def", "__init__", "(", "\n", "self", ",", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "if", "padding", "is", "None", ":", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.StdConv2d.forward": [[42, 48], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.StdConv2d.weight.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "x", "=", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.StdConv2dSame.__init__": [[56, 65], ["padding.get_padding_value", "torch.Conv2d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "'SAME'", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "in_channel", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "same_pad", "=", "is_dynamic", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.StdConv2dSame.forward": [[66, 74], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "padding.pad_same", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.StdConv2dSame.weight.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same_pad", ":", "\n", "            ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "dilation", ")", "\n", "", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "x", "=", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.ScaledStdConv2d.__init__": [[85, 96], ["torch.Conv2d.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "padding.get_padding.get_padding", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "std_conv.ScaledStdConv2d.weight[].numel"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ",", "gamma", "=", "1.0", ",", "eps", "=", "1e-6", ",", "gain_init", "=", "1.0", ")", ":", "\n", "        ", "if", "padding", "is", "None", ":", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "stride", ",", "dilation", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "full", "(", "(", "self", ".", "out_channels", ",", "1", ",", "1", ",", "1", ")", ",", "gain_init", ")", ")", "\n", "self", ".", "scale", "=", "gamma", "*", "self", ".", "weight", "[", "0", "]", ".", "numel", "(", ")", "**", "-", "0.5", "# gamma * 1 / sqrt(fan-in)", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.ScaledStdConv2d.forward": [[97, 103], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.ScaledStdConv2d.weight.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "weight", "=", "(", "self", ".", "gain", "*", "self", ".", "scale", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.ScaledStdConv2dSame.__init__": [[114, 125], ["padding.get_padding_value", "torch.Conv2d.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "std_conv.ScaledStdConv2dSame.weight[].numel"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "'SAME'", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "True", ",", "gamma", "=", "1.0", ",", "eps", "=", "1e-6", ",", "gain_init", "=", "1.0", ")", ":", "\n", "        ", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "\n", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "self", ".", "gain", "=", "nn", ".", "Parameter", "(", "torch", ".", "full", "(", "(", "self", ".", "out_channels", ",", "1", ",", "1", ",", "1", ")", ",", "gain_init", ")", ")", "\n", "self", ".", "scale", "=", "gamma", "*", "self", ".", "weight", "[", "0", "]", ".", "numel", "(", ")", "**", "-", "0.5", "\n", "self", ".", "same_pad", "=", "is_dynamic", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.std_conv.ScaledStdConv2dSame.forward": [[126, 134], ["torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.batch_norm().reshape_as", "torch.conv2d", "torch.conv2d", "torch.conv2d", "padding.pad_same", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "std_conv.ScaledStdConv2dSame.weight.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same_pad", ":", "\n", "            ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "dilation", ")", "\n", "", "weight", "=", "F", ".", "batch_norm", "(", "\n", "self", ".", "weight", ".", "reshape", "(", "1", ",", "self", ".", "out_channels", ",", "-", "1", ")", ",", "None", ",", "None", ",", "\n", "weight", "=", "(", "self", ".", "gain", "*", "self", ".", "scale", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "training", "=", "True", ",", "momentum", "=", "0.", ",", "eps", "=", "self", ".", "eps", ")", ".", "reshape_as", "(", "self", ".", "weight", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "weight", ",", "self", ".", "bias", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "dilation", ",", "self", ".", "groups", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d": [[11, 32], ["isinstance", "mixed_conv2d.MixedConv2d", "kwargs.pop", "kwargs.pop", "cond_conv2d.CondConv2d", "conv2d_same.create_conv2d_pad"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.create_conv2d_pad"], ["def", "create_conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.\n\n    Used extensively by EfficientNet, MobileNetv3 and related networks.\n    \"\"\"", "\n", "if", "isinstance", "(", "kernel_size", ",", "list", ")", ":", "\n", "        ", "assert", "'num_experts'", "not", "in", "kwargs", "# MixNet + CondConv combo not supported currently", "\n", "assert", "'groups'", "not", "in", "kwargs", "# MixedConv groups are defined by kernel list", "\n", "# We're going to use only lists for defining the MixedConv2d kernel groups,", "\n", "# ints, tuples, other iterables will continue to pass to normal conv and specify h, w.", "\n", "m", "=", "MixedConv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "depthwise", "=", "kwargs", ".", "pop", "(", "'depthwise'", ",", "False", ")", "\n", "# for DW out_channels must be multiple of in_channels as must have out_channels % groups == 0", "\n", "groups", "=", "in_channels", "if", "depthwise", "else", "kwargs", ".", "pop", "(", "'groups'", ",", "1", ")", "\n", "if", "'num_experts'", "in", "kwargs", "and", "kwargs", "[", "'num_experts'", "]", ">", "0", ":", "\n", "            ", "m", "=", "CondConv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "groups", "=", "groups", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "create_conv2d_pad", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "groups", "=", "groups", ",", "**", "kwargs", ")", "\n", "", "", "return", "m", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.PosEmbedRel.__init__": [[60, 66], ["torch.Module.__init__", "helpers.to_2tuple", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "feat_size", ",", "dim_head", ",", "scale", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "height", ",", "self", ".", "width", "=", "to_2tuple", "(", "feat_size", ")", "\n", "self", ".", "dim_head", "=", "dim_head", "\n", "self", ".", "height_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "height", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "self", ".", "width_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "self", ".", "width", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.PosEmbedRel.forward": [[67, 81], ["q.transpose.transpose.reshape", "bottleneck_attn.rel_logits_1d", "q.transpose.transpose.transpose", "bottleneck_attn.rel_logits_1d", "rel_logits.reshape.reshape.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.rel_logits_1d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.rel_logits_1d"], ["", "def", "forward", "(", "self", ",", "q", ")", ":", "\n", "        ", "B", ",", "HW", ",", "_", "=", "q", ".", "shape", "\n", "\n", "# relative logits in width dimension.", "\n", "q", "=", "q", ".", "reshape", "(", "B", ",", "self", ".", "height", ",", "self", ".", "width", ",", "-", "1", ")", "\n", "rel_logits_w", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "width_rel", ",", "permute_mask", "=", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ")", ")", "\n", "\n", "# relative logits in height dimension.", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "\n", "rel_logits_h", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "height_rel", ",", "permute_mask", "=", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ")", "\n", "\n", "rel_logits", "=", "rel_logits_h", "+", "rel_logits_w", "\n", "rel_logits", "=", "rel_logits", ".", "reshape", "(", "B", ",", "HW", ",", "HW", ")", "\n", "return", "rel_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.BottleneckAttn.__init__": [[105, 128], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "bottleneck_attn.PosEmbedRel", "bottleneck_attn.BottleneckAttn.reset_parameters", "torch.AvgPool2d", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Identity", "torch.Identity", "torch.Identity", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "4", ",", "dim_head", "=", "None", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ",", "scale_pos_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "feat_size", "is", "not", "None", ",", "'A concrete feature size matching expected input (H, W) is required'", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_head_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "dim_head_v", "=", "dim_out", "//", "self", ".", "num_heads", "\n", "self", ".", "dim_out_qk", "=", "num_heads", "*", "self", ".", "dim_head_qk", "\n", "self", ".", "dim_out_v", "=", "num_heads", "*", "self", ".", "dim_head_v", "\n", "self", ".", "scale", "=", "self", ".", "dim_head_qk", "**", "-", "0.5", "\n", "self", ".", "scale_pos_embed", "=", "scale_pos_embed", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", "*", "2", "+", "self", ".", "dim_out_v", ",", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "\n", "# NOTE I'm only supporting relative pos embedding for now", "\n", "self", ".", "pos_embed", "=", "PosEmbedRel", "(", "feat_size", ",", "dim_head", "=", "self", ".", "dim_head_qk", ",", "scale", "=", "self", ".", "scale", ")", "\n", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "stride", "==", "2", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.BottleneckAttn.reset_parameters": [[129, 133], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "self", ".", "qkv", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "# fan-in", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "height_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "width_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.BottleneckAttn.forward": [[134, 157], ["bottleneck_attn.BottleneckAttn.qkv", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "q.reshape().transpose.reshape().transpose.reshape().transpose", "k.reshape.reshape.reshape", "v.reshape().transpose.reshape().transpose.reshape().transpose", "attn.softmax.softmax.softmax", "bottleneck_attn.BottleneckAttn.pool", "q.reshape().transpose.reshape().transpose.reshape", "v.reshape().transpose.reshape().transpose.reshape", "bottleneck_attn.BottleneckAttn.pos_embed", "bottleneck_attn.BottleneckAttn.pos_embed"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "assert", "H", "==", "self", ".", "pos_embed", ".", "height", "\n", "assert", "W", "==", "self", ".", "pos_embed", ".", "width", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", "# B, (2 * dim_head_qk + dim_head_v) * num_heads, H, W", "\n", "\n", "# NOTE head vs channel split ordering in qkv projection was decided before I allowed qk to differ from v", "\n", "# So, this is more verbose than if heads were before qkv splits, but throughput is not impacted.", "\n", "q", ",", "k", ",", "v", "=", "torch", ".", "split", "(", "x", ",", "[", "self", ".", "dim_out_qk", ",", "self", ".", "dim_out_qk", ",", "self", ".", "dim_out_v", "]", ",", "dim", "=", "1", ")", "\n", "q", "=", "q", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "k", "=", "k", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ")", "# no transpose, for q @ k", "\n", "v", "=", "v", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_v", ",", "-", "1", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "if", "self", ".", "scale_pos_embed", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", "+", "self", ".", "pos_embed", "(", "q", ")", ")", "*", "self", ".", "scale", "# B * num_heads, H * W, H * W", "\n", "", "else", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ")", "*", "self", ".", "scale", "+", "self", ".", "pos_embed", "(", "q", ")", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "out", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "reshape", "(", "B", ",", "self", ".", "dim_out_v", ",", "H", ",", "W", ")", "# B, dim_out, H, W", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.bottleneck_attn.rel_logits_1d": [[27, 53], ["x.reshape().expand.reshape", "torch.pad().flatten", "torch.pad", "x_pad.reshape.reshape", "x.reshape().expand.reshape().expand", "x.reshape().expand.permute", "rel_k.transpose", "torch.pad", "x.reshape().expand.reshape"], "function", ["None"], ["def", "rel_logits_1d", "(", "q", ",", "rel_k", ",", "permute_mask", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "\"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, heads, height, width, dim)\n        rel_k: (2 * width - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "dim", "=", "q", ".", "shape", "\n", "x", "=", "(", "q", "@", "rel_k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "W", ",", "2", "*", "W", "-", "1", ")", "\n", "\n", "# pad to shift from relative to absolute indexing", "\n", "x_pad", "=", "F", ".", "pad", "(", "x", ",", "[", "0", ",", "1", "]", ")", ".", "flatten", "(", "1", ")", "\n", "x_pad", "=", "F", ".", "pad", "(", "x_pad", ",", "[", "0", ",", "W", "-", "1", "]", ")", "\n", "\n", "# reshape and slice out the padded elements", "\n", "x_pad", "=", "x_pad", ".", "reshape", "(", "-", "1", ",", "W", "+", "1", ",", "2", "*", "W", "-", "1", ")", "\n", "x", "=", "x_pad", "[", ":", ",", ":", "W", ",", "W", "-", "1", ":", "]", "\n", "\n", "# reshape and tile", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "H", ",", "1", ",", "W", ",", "W", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "H", ",", "-", "1", ",", "-", "1", ")", "\n", "return", "x", ".", "permute", "(", "permute_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.blur_pool.BlurPool2d.__init__": [[29, 39], ["torch.Module.__init__", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "[].repeat", "blur_pool.BlurPool2d.register_buffer", "padding.get_padding", "numpy.poly1d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding"], ["def", "__init__", "(", "self", ",", "channels", ",", "filt_size", "=", "3", ",", "stride", "=", "2", ")", "->", "None", ":", "\n", "        ", "super", "(", "BlurPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "filt_size", ">", "1", "\n", "self", ".", "channels", "=", "channels", "\n", "self", ".", "filt_size", "=", "filt_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "[", "get_padding", "(", "filt_size", ",", "stride", ",", "dilation", "=", "1", ")", "]", "*", "4", "\n", "coeffs", "=", "torch", ".", "tensor", "(", "(", "np", ".", "poly1d", "(", "(", "0.5", ",", "0.5", ")", ")", "**", "(", "self", ".", "filt_size", "-", "1", ")", ")", ".", "coeffs", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "blur_filter", "=", "(", "coeffs", "[", ":", ",", "None", "]", "*", "coeffs", "[", "None", ",", ":", "]", ")", "[", "None", ",", "None", ",", ":", ",", ":", "]", ".", "repeat", "(", "self", ".", "channels", ",", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "register_buffer", "(", "'filt'", ",", "blur_filter", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.blur_pool.BlurPool2d.forward": [[40, 43], ["torch.pad", "torch.pad", "torch.pad", "torch.conv2d", "torch.conv2d", "torch.conv2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "self", ".", "padding", ",", "'reflect'", ")", "\n", "return", "F", ".", "conv2d", "(", "x", ",", "self", ".", "filt", ",", "stride", "=", "self", ".", "stride", ",", "groups", "=", "x", ".", "shape", "[", "1", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_no_jit.__init__": [[30, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_NO_JIT", "\n", "self", ".", "prev", "=", "_NO_JIT", "\n", "_NO_JIT", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_no_jit.__enter__": [[35, 37], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_no_jit.__exit__": [[38, 42], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_NO_JIT", "\n", "_NO_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_exportable.__init__": [[49, 53], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "self", ".", "prev", "=", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_exportable.__enter__": [[54, 56], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_exportable.__exit__": [[57, 61], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_scriptable.__init__": [[68, 72], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "mode", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_scriptable.__enter__": [[73, 75], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_scriptable.__exit__": [[76, 80], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_layer_config.__init__": [[86, 105], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "scriptable", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "exportable", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "no_jit", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "no_activation_jit", ":", "Optional", "[", "bool", "]", "=", "None", ")", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "global", "_EXPORTABLE", "\n", "global", "_NO_JIT", "\n", "global", "_NO_ACTIVATION_JIT", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", ",", "_EXPORTABLE", ",", "_NO_JIT", ",", "_NO_ACTIVATION_JIT", "\n", "if", "scriptable", "is", "not", "None", ":", "\n", "            ", "_SCRIPTABLE", "=", "scriptable", "\n", "", "if", "exportable", "is", "not", "None", ":", "\n", "            ", "_EXPORTABLE", "=", "exportable", "\n", "", "if", "no_jit", "is", "not", "None", ":", "\n", "            ", "_NO_JIT", "=", "no_jit", "\n", "", "if", "no_activation_jit", "is", "not", "None", ":", "\n", "            ", "_NO_ACTIVATION_JIT", "=", "no_activation_jit", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_layer_config.__enter__": [[106, 108], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.set_layer_config.__exit__": [[109, 116], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "global", "_EXPORTABLE", "\n", "global", "_NO_JIT", "\n", "global", "_NO_ACTIVATION_JIT", "\n", "_SCRIPTABLE", ",", "_EXPORTABLE", ",", "_NO_JIT", ",", "_NO_ACTIVATION_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_no_jit": [[25, 27], ["None"], "function", ["None"], ["def", "is_no_jit", "(", ")", ":", "\n", "    ", "return", "_NO_JIT", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable": [[44, 46], ["None"], "function", ["None"], ["", "", "def", "is_exportable", "(", ")", ":", "\n", "    ", "return", "_EXPORTABLE", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_scriptable": [[63, 65], ["None"], "function", ["None"], ["", "", "def", "is_scriptable", "(", ")", ":", "\n", "    ", "return", "_SCRIPTABLE", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.ChannelAttn.__init__": [[22, 32], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "create_act.create_act_layer", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "ChannelAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "channels", ",", "rd_channels", ",", "1", ",", "bias", "=", "mlp_bias", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "channels", ",", "1", ",", "bias", "=", "mlp_bias", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.ChannelAttn.forward": [[33, 37], ["cbam.ChannelAttn.fc2", "cbam.ChannelAttn.fc2", "cbam.ChannelAttn.act", "cbam.ChannelAttn.act", "cbam.ChannelAttn.gate", "cbam.ChannelAttn.fc1", "cbam.ChannelAttn.fc1", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_avg", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ")", ")", ")", "\n", "x_max", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", ")", ")", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_avg", "+", "x_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightChannelAttn.__init__": [[42, 47], ["cbam.ChannelAttn.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "LightChannelAttn", ",", "self", ")", ".", "__init__", "(", "\n", "channels", ",", "rd_ratio", ",", "rd_channels", ",", "rd_divisor", ",", "act_layer", ",", "gate_layer", ",", "mlp_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightChannelAttn.forward": [[48, 52], ["cbam.LightChannelAttn.fc2", "cbam.LightChannelAttn.act", "torch.sigmoid", "torch.sigmoid", "x.mean", "x.amax", "cbam.LightChannelAttn.fc1"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_pool", "=", "0.5", "*", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x_attn", "=", "self", ".", "fc2", "(", "self", ".", "act", "(", "self", ".", "fc1", "(", "x_pool", ")", ")", ")", "\n", "return", "x", "*", "F", ".", "sigmoid", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.SpatialAttn.__init__": [[57, 61], ["torch.nn.Module.__init__", "conv_bn_act.ConvBnAct", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "7", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "SpatialAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "ConvBnAct", "(", "2", ",", "1", ",", "kernel_size", ",", "act_layer", "=", "None", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.SpatialAttn.forward": [[62, 66], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "cbam.SpatialAttn.conv", "cbam.SpatialAttn.gate", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_attn", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ",", "x", ".", "amax", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "]", ",", "dim", "=", "1", ")", "\n", "x_attn", "=", "self", ".", "conv", "(", "x_attn", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightSpatialAttn.__init__": [[71, 75], ["torch.nn.Module.__init__", "conv_bn_act.ConvBnAct", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "7", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "LightSpatialAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "ConvBnAct", "(", "1", ",", "1", ",", "kernel_size", ",", "act_layer", "=", "None", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightSpatialAttn.forward": [[76, 80], ["cbam.LightSpatialAttn.conv", "cbam.LightSpatialAttn.gate", "x.mean", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_attn", "=", "0.5", "*", "x", ".", "mean", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "+", "0.5", "*", "x", ".", "amax", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x_attn", "=", "self", ".", "conv", "(", "x_attn", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_attn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.CbamModule.__init__": [[83, 91], ["torch.nn.Module.__init__", "cbam.ChannelAttn", "cbam.SpatialAttn"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "spatial_kernel_size", "=", "7", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "CbamModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channel", "=", "ChannelAttn", "(", "\n", "channels", ",", "rd_ratio", "=", "rd_ratio", ",", "rd_channels", "=", "rd_channels", ",", "\n", "rd_divisor", "=", "rd_divisor", ",", "act_layer", "=", "act_layer", ",", "gate_layer", "=", "gate_layer", ",", "mlp_bias", "=", "mlp_bias", ")", "\n", "self", ".", "spatial", "=", "SpatialAttn", "(", "spatial_kernel_size", ",", "gate_layer", "=", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.CbamModule.forward": [[92, 96], ["cbam.CbamModule.channel", "cbam.CbamModule.spatial"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "channel", "(", "x", ")", "\n", "x", "=", "self", ".", "spatial", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightCbamModule.__init__": [[99, 107], ["torch.nn.Module.__init__", "cbam.LightChannelAttn", "cbam.LightSpatialAttn"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "\n", "spatial_kernel_size", "=", "7", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ",", "mlp_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", "LightCbamModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channel", "=", "LightChannelAttn", "(", "\n", "channels", ",", "rd_ratio", "=", "rd_ratio", ",", "rd_channels", "=", "rd_channels", ",", "\n", "rd_divisor", "=", "rd_divisor", ",", "act_layer", "=", "act_layer", ",", "gate_layer", "=", "gate_layer", ",", "mlp_bias", "=", "mlp_bias", ")", "\n", "self", ".", "spatial", "=", "LightSpatialAttn", "(", "spatial_kernel_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cbam.LightCbamModule.forward": [[108, 112], ["cbam.LightCbamModule.channel", "cbam.LightCbamModule.spatial"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "channel", "(", "x", ")", "\n", "x", "=", "self", ".", "spatial", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.eca.EcaModule.__init__": [[60, 83], ["torch.nn.Module.__init__", "create_act.create_act_layer", "int", "max", "torch.nn.Conv1d", "create_act.create_act_layer", "torch.nn.Conv1d", "torch.nn.Conv1d", "helpers.make_divisible", "abs", "math.log"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", "=", "None", ",", "kernel_size", "=", "3", ",", "gamma", "=", "2", ",", "beta", "=", "1", ",", "act_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ",", "\n", "rd_ratio", "=", "1", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "use_mlp", "=", "False", ")", ":", "\n", "        ", "super", "(", "EcaModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "channels", "is", "not", "None", ":", "\n", "            ", "t", "=", "int", "(", "abs", "(", "math", ".", "log", "(", "channels", ",", "2", ")", "+", "beta", ")", "/", "gamma", ")", "\n", "kernel_size", "=", "max", "(", "t", "if", "t", "%", "2", "else", "t", "+", "1", ",", "3", ")", "\n", "", "assert", "kernel_size", "%", "2", "==", "1", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", "\n", "if", "use_mlp", ":", "\n", "# NOTE 'mlp' mode is a timm experiment, not in paper", "\n", "            ", "assert", "channels", "is", "not", "None", "\n", "if", "rd_channels", "is", "None", ":", "\n", "                ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "act_layer", "=", "act_layer", "or", "nn", ".", "ReLU", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "True", ")", "\n", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv1d", "(", "rd_channels", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "False", ")", "\n", "self", ".", "act", "=", "None", "\n", "self", ".", "conv2", "=", "None", "\n", "", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.eca.EcaModule.forward": [[84, 92], ["x.mean().view", "eca.EcaModule.conv", "eca.EcaModule.gate().view", "eca.EcaModule.act", "eca.EcaModule.conv2", "eca.EcaModule.expand_as", "x.mean", "eca.EcaModule.gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ",", "-", "1", ")", "# view for 1d conv", "\n", "y", "=", "self", ".", "conv", "(", "y", ")", "\n", "if", "self", ".", "conv2", "is", "not", "None", ":", "\n", "            ", "y", "=", "self", ".", "act", "(", "y", ")", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "", "y", "=", "self", ".", "gate", "(", "y", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "return", "x", "*", "y", ".", "expand_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.eca.CecaModule.__init__": [[121, 135], ["torch.nn.Module.__init__", "torch.nn.Conv1d", "create_act.create_act_layer", "int", "max", "abs", "math.log"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "channels", "=", "None", ",", "kernel_size", "=", "3", ",", "gamma", "=", "2", ",", "beta", "=", "1", ",", "act_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "CecaModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "channels", "is", "not", "None", ":", "\n", "            ", "t", "=", "int", "(", "abs", "(", "math", ".", "log", "(", "channels", ",", "2", ")", "+", "beta", ")", "/", "gamma", ")", "\n", "kernel_size", "=", "max", "(", "t", "if", "t", "%", "2", "else", "t", "+", "1", ",", "3", ")", "\n", "", "has_act", "=", "act_layer", "is", "not", "None", "\n", "assert", "kernel_size", "%", "2", "==", "1", "\n", "\n", "# PyTorch circular padding mode is buggy as of pytorch 1.4", "\n", "# see https://github.com/pytorch/pytorch/pull/17240", "\n", "# implement manual circular padding", "\n", "self", ".", "padding", "=", "(", "kernel_size", "-", "1", ")", "//", "2", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "1", ",", "1", ",", "kernel_size", "=", "kernel_size", ",", "padding", "=", "0", ",", "bias", "=", "has_act", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.eca.CecaModule.forward": [[136, 143], ["x.mean().view", "torch.pad", "eca.CecaModule.conv", "eca.CecaModule.gate().view", "eca.CecaModule.expand_as", "x.mean", "eca.CecaModule.gate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "1", ",", "-", "1", ")", "\n", "# Manually implement circular padding, F.pad does not seemed to be bugged", "\n", "y", "=", "F", ".", "pad", "(", "y", ",", "(", "self", ".", "padding", ",", "self", ".", "padding", ")", ",", "mode", "=", "'circular'", ")", "\n", "y", "=", "self", ".", "conv", "(", "y", ")", "\n", "y", "=", "self", ".", "gate", "(", "y", ")", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "return", "x", "*", "y", ".", "expand_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.global_context.GlobalContext.__init__": [[21, 42], ["torch.nn.Module.__init__", "create_act.get_act_layer", "create_act.create_act_layer", "global_context.GlobalContext.reset_parameters", "torch.nn.Conv2d", "helpers.make_divisible", "mlp.ConvMlp", "mlp.ConvMlp"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "use_attn", "=", "True", ",", "fuse_add", "=", "False", ",", "fuse_scale", "=", "True", ",", "init_last_zero", "=", "False", ",", "\n", "rd_ratio", "=", "1.", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "GlobalContext", ",", "self", ")", ".", "__init__", "(", ")", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "\n", "self", ".", "conv_attn", "=", "nn", ".", "Conv2d", "(", "channels", ",", "1", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "if", "use_attn", "else", "None", "\n", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "if", "fuse_add", ":", "\n", "            ", "self", ".", "mlp_add", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "LayerNorm2d", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mlp_add", "=", "None", "\n", "", "if", "fuse_scale", ":", "\n", "            ", "self", ".", "mlp_scale", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "LayerNorm2d", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "mlp_scale", "=", "None", "\n", "\n", "", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "self", ".", "init_last_zero", "=", "init_last_zero", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.global_context.GlobalContext.reset_parameters": [[43, 48], ["torch.nn.init.kaiming_normal_", "torch.nn.init.zeros_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "conv_attn", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "kaiming_normal_", "(", "self", ".", "conv_attn", ".", "weight", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "", "if", "self", ".", "mlp_add", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "mlp_add", ".", "fc2", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.global_context.GlobalContext.forward": [[49, 68], ["global_context.GlobalContext.conv_attn().reshape", "torch.softmax().unsqueeze", "x.mean.view", "x.mean", "global_context.GlobalContext.mlp_scale", "global_context.GlobalContext.mlp_add", "x.reshape().unsqueeze", "global_context.GlobalContext.gate", "global_context.GlobalContext.conv_attn", "torch.softmax", "x.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "\n", "if", "self", ".", "conv_attn", "is", "not", "None", ":", "\n", "            ", "attn", "=", "self", ".", "conv_attn", "(", "x", ")", ".", "reshape", "(", "B", ",", "1", ",", "H", "*", "W", ")", "# (B, 1, H * W)", "\n", "attn", "=", "F", ".", "softmax", "(", "attn", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "3", ")", "# (B, 1, H * W, 1)", "\n", "context", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "H", "*", "W", ")", ".", "unsqueeze", "(", "1", ")", "@", "attn", "\n", "context", "=", "context", ".", "view", "(", "B", ",", "C", ",", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "context", "=", "x", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "\n", "", "if", "self", ".", "mlp_scale", "is", "not", "None", ":", "\n", "            ", "mlp_x", "=", "self", ".", "mlp_scale", "(", "context", ")", "\n", "x", "=", "x", "*", "self", ".", "gate", "(", "mlp_x", ")", "\n", "", "if", "self", ".", "mlp_add", "is", "not", "None", ":", "\n", "            ", "mlp_x", "=", "self", ".", "mlp_add", "(", "context", ")", "\n", "x", "=", "x", "+", "mlp_x", "\n", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.median_pool.MedianPool2d.__init__": [[18, 24], ["torch.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "helpers.to_4tuple"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "same", "=", "False", ")", ":", "\n", "        ", "super", "(", "MedianPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "k", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "self", ".", "padding", "=", "to_4tuple", "(", "padding", ")", "# convert to l, r, t, b", "\n", "self", ".", "same", "=", "same", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.median_pool.MedianPool2d._padding": [[25, 44], ["x.size", "max", "max", "max", "max"], "methods", ["None"], ["", "def", "_padding", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "same", ":", "\n", "            ", "ih", ",", "iw", "=", "x", ".", "size", "(", ")", "[", "2", ":", "]", "\n", "if", "ih", "%", "self", ".", "stride", "[", "0", "]", "==", "0", ":", "\n", "                ", "ph", "=", "max", "(", "self", ".", "k", "[", "0", "]", "-", "self", ".", "stride", "[", "0", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "                ", "ph", "=", "max", "(", "self", ".", "k", "[", "0", "]", "-", "(", "ih", "%", "self", ".", "stride", "[", "0", "]", ")", ",", "0", ")", "\n", "", "if", "iw", "%", "self", ".", "stride", "[", "1", "]", "==", "0", ":", "\n", "                ", "pw", "=", "max", "(", "self", ".", "k", "[", "1", "]", "-", "self", ".", "stride", "[", "1", "]", ",", "0", ")", "\n", "", "else", ":", "\n", "                ", "pw", "=", "max", "(", "self", ".", "k", "[", "1", "]", "-", "(", "iw", "%", "self", ".", "stride", "[", "1", "]", ")", ",", "0", ")", "\n", "", "pl", "=", "pw", "//", "2", "\n", "pr", "=", "pw", "-", "pl", "\n", "pt", "=", "ph", "//", "2", "\n", "pb", "=", "ph", "-", "pt", "\n", "padding", "=", "(", "pl", ",", "pr", ",", "pt", ",", "pb", ")", "\n", "", "else", ":", "\n", "            ", "padding", "=", "self", ".", "padding", "\n", "", "return", "padding", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.median_pool.MedianPool2d.forward": [[45, 50], ["torch.pad", "torch.pad", "x.unfold().unfold.unfold().unfold.unfold().unfold", "median_pool.MedianPool2d._padding", "x.unfold().unfold.unfold().unfold.contiguous().view().median", "x.unfold().unfold.unfold().unfold.unfold", "x.unfold().unfold.unfold().unfold.contiguous().view", "x.unfold().unfold.unfold().unfold.contiguous", "x.unfold().unfold.unfold().unfold.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.median_pool.MedianPool2d._padding"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "self", ".", "_padding", "(", "x", ")", ",", "mode", "=", "'reflect'", ")", "\n", "x", "=", "x", ".", "unfold", "(", "2", ",", "self", ".", "k", "[", "0", "]", ",", "self", ".", "stride", "[", "0", "]", ")", ".", "unfold", "(", "3", ",", "self", ".", "k", "[", "1", "]", ",", "self", ".", "stride", "[", "1", "]", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "x", ".", "size", "(", ")", "[", ":", "4", "]", "+", "(", "-", "1", ",", ")", ")", ".", "median", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_batchnorm.SplitBatchNorm2d.__init__": [[20, 27], ["super().__init__", "torch.ModuleList", "torch.ModuleList", "torch.BatchNorm2d", "torch.BatchNorm2d", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "\n", "track_running_stats", "=", "True", ",", "num_splits", "=", "2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_features", ",", "eps", ",", "momentum", ",", "affine", ",", "track_running_stats", ")", "\n", "assert", "num_splits", ">", "1", ",", "'Should have at least one aux BN layer (num_splits at least 2)'", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "aux_bn", "=", "nn", ".", "ModuleList", "(", "[", "\n", "nn", ".", "BatchNorm2d", "(", "num_features", ",", "eps", ",", "momentum", ",", "affine", ",", "track_running_stats", ")", "for", "_", "in", "range", "(", "num_splits", "-", "1", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_batchnorm.SplitBatchNorm2d.forward": [[28, 39], ["input.split", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "super().forward", "super().forward", "x.append", "a"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "# aux BN only relevant while training", "\n", "            ", "split_size", "=", "input", ".", "shape", "[", "0", "]", "//", "self", ".", "num_splits", "\n", "assert", "input", ".", "shape", "[", "0", "]", "==", "split_size", "*", "self", ".", "num_splits", ",", "\"batch size must be evenly divisible by num_splits\"", "\n", "split_input", "=", "input", ".", "split", "(", "split_size", ")", "\n", "x", "=", "[", "super", "(", ")", ".", "forward", "(", "split_input", "[", "0", "]", ")", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "self", ".", "aux_bn", ")", ":", "\n", "                ", "x", ".", "append", "(", "a", "(", "split_input", "[", "i", "+", "1", "]", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "x", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_batchnorm.convert_splitbn_model": [[41, 76], ["isinstance", "isinstance", "module.named_children", "split_batchnorm.SplitBatchNorm2d", "SplitBatchNorm2d.add_module", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "module.running_mean.clone", "module.running_var.clone", "module.num_batches_tracked.clone", "split_batchnorm.convert_splitbn_model", "module.weight.data.clone().detach", "module.bias.data.clone().detach", "module.weight.data.clone", "module.bias.data.clone", "module.weight.data.clone", "module.bias.data.clone"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_batchnorm.convert_splitbn_model"], ["", "", "", "def", "convert_splitbn_model", "(", "module", ",", "num_splits", "=", "2", ")", ":", "\n", "    ", "\"\"\"\n    Recursively traverse module and its children to replace all instances of\n    ``torch.nn.modules.batchnorm._BatchNorm`` with `SplitBatchnorm2d`.\n    Args:\n        module (torch.nn.Module): input module\n        num_splits: number of separate batchnorm layers to split input across\n    Example::\n        >>> # model is an instance of torch.nn.Module\n        >>> model = timm.models.convert_splitbn_model(model, num_splits=2)\n    \"\"\"", "\n", "mod", "=", "module", "\n", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "instancenorm", ".", "_InstanceNorm", ")", ":", "\n", "        ", "return", "module", "\n", "", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "modules", ".", "batchnorm", ".", "_BatchNorm", ")", ":", "\n", "        ", "mod", "=", "SplitBatchNorm2d", "(", "\n", "module", ".", "num_features", ",", "module", ".", "eps", ",", "module", ".", "momentum", ",", "module", ".", "affine", ",", "\n", "module", ".", "track_running_stats", ",", "num_splits", "=", "num_splits", ")", "\n", "mod", ".", "running_mean", "=", "module", ".", "running_mean", "\n", "mod", ".", "running_var", "=", "module", ".", "running_var", "\n", "mod", ".", "num_batches_tracked", "=", "module", ".", "num_batches_tracked", "\n", "if", "module", ".", "affine", ":", "\n", "            ", "mod", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mod", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "for", "aux", "in", "mod", ".", "aux_bn", ":", "\n", "            ", "aux", ".", "running_mean", "=", "module", ".", "running_mean", ".", "clone", "(", ")", "\n", "aux", ".", "running_var", "=", "module", ".", "running_var", ".", "clone", "(", ")", "\n", "aux", ".", "num_batches_tracked", "=", "module", ".", "num_batches_tracked", ".", "clone", "(", ")", "\n", "if", "module", ".", "affine", ":", "\n", "                ", "aux", ".", "weight", ".", "data", "=", "module", ".", "weight", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "aux", ".", "bias", ".", "data", "=", "module", ".", "bias", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "", "", "for", "name", ",", "child", "in", "module", ".", "named_children", "(", ")", ":", "\n", "        ", "mod", ".", "add_module", "(", "name", ",", "convert_splitbn_model", "(", "child", ",", "num_splits", "=", "num_splits", ")", ")", "\n", "", "del", "module", "\n", "return", "mod", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.PosEmbedRel.__init__": [[66, 79], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "block_size", ",", "win_size", ",", "dim_head", ",", "scale", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            block_size (int): block size\n            win_size (int): neighbourhood window size\n            dim_head (int): attention head dim\n            scale (float): scale factor (for init)\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "dim_head", "=", "dim_head", "\n", "self", ".", "height_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "win_size", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "self", ".", "width_rel", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "win_size", "*", "2", "-", "1", ",", "dim_head", ")", "*", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.PosEmbedRel.forward": [[80, 94], ["q.transpose.transpose.reshape", "halo_attn.rel_logits_1d", "q.transpose.transpose.transpose", "halo_attn.rel_logits_1d", "rel_logits.reshape.reshape.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.rel_logits_1d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.rel_logits_1d"], ["", "def", "forward", "(", "self", ",", "q", ")", ":", "\n", "        ", "B", ",", "BB", ",", "HW", ",", "_", "=", "q", ".", "shape", "\n", "\n", "# relative logits in width dimension.", "\n", "q", "=", "q", ".", "reshape", "(", "-", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ",", "self", ".", "dim_head", ")", "\n", "rel_logits_w", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "width_rel", ",", "permute_mask", "=", "(", "0", ",", "1", ",", "3", ",", "2", ",", "4", ")", ")", "\n", "\n", "# relative logits in height dimension.", "\n", "q", "=", "q", ".", "transpose", "(", "1", ",", "2", ")", "\n", "rel_logits_h", "=", "rel_logits_1d", "(", "q", ",", "self", ".", "height_rel", ",", "permute_mask", "=", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ")", "\n", "\n", "rel_logits", "=", "rel_logits_h", "+", "rel_logits_w", "\n", "rel_logits", "=", "rel_logits", ".", "reshape", "(", "B", ",", "BB", ",", "HW", ",", "-", "1", ")", "\n", "return", "rel_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.HaloAttn.__init__": [[124, 160], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "halo_attn.PosEmbedRel", "halo_attn.HaloAttn.reset_parameters", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Identity", "torch.nn.Identity", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "8", ",", "dim_head", "=", "None", ",", "block_size", "=", "8", ",", "halo_size", "=", "3", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ",", "avg_down", "=", "False", ",", "scale_pos_embed", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", "\n", "assert", "stride", "in", "(", "1", ",", "2", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_head_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "dim_head_v", "=", "dim_out", "//", "self", ".", "num_heads", "\n", "self", ".", "dim_out_qk", "=", "num_heads", "*", "self", ".", "dim_head_qk", "\n", "self", ".", "dim_out_v", "=", "num_heads", "*", "self", ".", "dim_head_v", "\n", "self", ".", "scale", "=", "self", ".", "dim_head_qk", "**", "-", "0.5", "\n", "self", ".", "scale_pos_embed", "=", "scale_pos_embed", "\n", "self", ".", "block_size", "=", "self", ".", "block_size_ds", "=", "block_size", "\n", "self", ".", "halo_size", "=", "halo_size", "\n", "self", ".", "win_size", "=", "block_size", "+", "halo_size", "*", "2", "# neighbourhood window size", "\n", "self", ".", "block_stride", "=", "1", "\n", "use_avg_pool", "=", "False", "\n", "if", "stride", ">", "1", ":", "\n", "            ", "use_avg_pool", "=", "avg_down", "or", "block_size", "%", "stride", "!=", "0", "\n", "self", ".", "block_stride", "=", "1", "if", "use_avg_pool", "else", "stride", "\n", "self", ".", "block_size_ds", "=", "self", ".", "block_size", "//", "self", ".", "block_stride", "\n", "\n", "# FIXME not clear if this stride behaviour is what the paper intended", "\n", "# Also, the paper mentions using a 3D conv for dealing with the blocking/gather, and leaving", "\n", "# data in unfolded block form. I haven't wrapped my head around how that'd look.", "\n", "", "self", ".", "q", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", ",", "1", ",", "stride", "=", "self", ".", "block_stride", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "kv", "=", "nn", ".", "Conv2d", "(", "dim", ",", "self", ".", "dim_out_qk", "+", "self", ".", "dim_out_v", ",", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "\n", "self", ".", "pos_embed", "=", "PosEmbedRel", "(", "\n", "block_size", "=", "self", ".", "block_size_ds", ",", "win_size", "=", "self", ".", "win_size", ",", "dim_head", "=", "self", ".", "dim_head_qk", ",", "scale", "=", "self", ".", "scale", ")", "\n", "\n", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "use_avg_pool", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.HaloAttn.reset_parameters": [[161, 167], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "std", "=", "self", ".", "q", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", "# fan-in", "\n", "trunc_normal_", "(", "self", ".", "q", ".", "weight", ",", "std", "=", "std", ")", "\n", "trunc_normal_", "(", "self", ".", "kv", ".", "weight", ",", "std", "=", "std", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "height_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ".", "width_rel", ",", "std", "=", "self", ".", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.HaloAttn.forward": [[168, 210], ["halo_attn.HaloAttn.q", "q.reshape().transpose.reshape().transpose.reshape().permute", "q.reshape().transpose.reshape().transpose.reshape().transpose", "halo_attn.HaloAttn.kv", "torch.pad", "torch.pad", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute", "torch.split", "torch.split", "torch.split", "torch.split", "attn.softmax.softmax.softmax", "halo_attn.HaloAttn.reshape", "halo_attn.HaloAttn.permute().contiguous().view", "halo_attn.HaloAttn.pool", "q.reshape().transpose.reshape().transpose.reshape", "q.reshape().transpose.reshape().transpose.reshape", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold().reshape", "halo_attn.HaloAttn.pos_embed", "halo_attn.HaloAttn.permute().contiguous", "halo_attn.HaloAttn.pos_embed", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold().unfold", "k.transpose", "k.transpose", "halo_attn.HaloAttn.permute", "kv.unfold().unfold().reshape().permute.unfold().unfold().reshape().permute.unfold"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "assert", "H", "%", "self", ".", "block_size", "==", "0", "\n", "assert", "W", "%", "self", ".", "block_size", "==", "0", "\n", "num_h_blocks", "=", "H", "//", "self", ".", "block_size", "\n", "num_w_blocks", "=", "W", "//", "self", ".", "block_size", "\n", "num_blocks", "=", "num_h_blocks", "*", "num_w_blocks", "\n", "\n", "q", "=", "self", ".", "q", "(", "x", ")", "\n", "# unfold", "\n", "q", "=", "q", ".", "reshape", "(", "\n", "-", "1", ",", "self", ".", "dim_head_qk", ",", "\n", "num_h_blocks", ",", "self", ".", "block_size_ds", ",", "num_w_blocks", ",", "self", ".", "block_size_ds", ")", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "5", ",", "2", ",", "4", ")", "\n", "# B, num_heads * dim_head * block_size ** 2, num_blocks", "\n", "q", "=", "q", ".", "reshape", "(", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", ",", "-", "1", ",", "num_blocks", ")", ".", "transpose", "(", "1", ",", "3", ")", "\n", "# B * num_heads, num_blocks, block_size ** 2, dim_head", "\n", "\n", "kv", "=", "self", ".", "kv", "(", "x", ")", "\n", "# Generate overlapping windows for kv. This approach is good for GPU and CPU. However, unfold() is not", "\n", "# lowered for PyTorch XLA so it will be very slow. See code at bottom of file for XLA friendly approach.", "\n", "# FIXME figure out how to switch impl between this and conv2d if XLA being used.", "\n", "kv", "=", "F", ".", "pad", "(", "kv", ",", "[", "self", ".", "halo_size", ",", "self", ".", "halo_size", ",", "self", ".", "halo_size", ",", "self", ".", "halo_size", "]", ")", "\n", "kv", "=", "kv", ".", "unfold", "(", "2", ",", "self", ".", "win_size", ",", "self", ".", "block_size", ")", ".", "unfold", "(", "3", ",", "self", ".", "win_size", ",", "self", ".", "block_size", ")", ".", "reshape", "(", "\n", "B", "*", "self", ".", "num_heads", ",", "self", ".", "dim_head_qk", "+", "self", ".", "dim_head_v", ",", "num_blocks", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "k", ",", "v", "=", "torch", ".", "split", "(", "kv", ",", "[", "self", ".", "dim_head_qk", ",", "self", ".", "dim_head_v", "]", ",", "dim", "=", "-", "1", ")", "\n", "# B * num_heads, num_blocks, win_size ** 2, dim_head_qk or dim_head_v", "\n", "\n", "if", "self", ".", "scale_pos_embed", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "+", "self", ".", "pos_embed", "(", "q", ")", ")", "*", "self", ".", "scale", "\n", "", "else", ":", "\n", "            ", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "*", "self", ".", "scale", "+", "self", ".", "pos_embed", "(", "q", ")", "\n", "# B * num_heads, num_blocks, block_size ** 2, win_size ** 2", "\n", "", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "out", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "3", ")", "# B * num_heads, dim_head_v, block_size ** 2, num_blocks", "\n", "# fold", "\n", "out", "=", "out", ".", "reshape", "(", "-", "1", ",", "self", ".", "block_size_ds", ",", "self", ".", "block_size_ds", ",", "num_h_blocks", ",", "num_w_blocks", ")", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "4", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "B", ",", "self", ".", "dim_out_v", ",", "H", "//", "self", ".", "block_stride", ",", "W", "//", "self", ".", "block_stride", ")", "\n", "# B, dim_out, H // block_stride, W // block_stride", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.halo_attn.rel_logits_1d": [[29, 58], ["x.reshape().expand.reshape", "torch.pad().flatten", "torch.pad", "x_pad.reshape.reshape", "x.reshape().expand.reshape().expand", "x.reshape().expand.permute", "rel_k.transpose", "torch.pad", "x.reshape().expand.reshape"], "function", ["None"], ["def", "rel_logits_1d", "(", "q", ",", "rel_k", ",", "permute_mask", ":", "List", "[", "int", "]", ")", ":", "\n", "    ", "\"\"\" Compute relative logits along one dimension\n\n    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2\n    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925\n\n    Args:\n        q: (batch, height, width, dim)\n        rel_k: (2 * window - 1, dim)\n        permute_mask: permute output dim according to this\n    \"\"\"", "\n", "B", ",", "H", ",", "W", ",", "dim", "=", "q", ".", "shape", "\n", "rel_size", "=", "rel_k", ".", "shape", "[", "0", "]", "\n", "win_size", "=", "(", "rel_size", "+", "1", ")", "//", "2", "\n", "\n", "x", "=", "(", "q", "@", "rel_k", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "W", ",", "rel_size", ")", "\n", "\n", "# pad to shift from relative to absolute indexing", "\n", "x_pad", "=", "F", ".", "pad", "(", "x", ",", "[", "0", ",", "1", "]", ")", ".", "flatten", "(", "1", ")", "\n", "x_pad", "=", "F", ".", "pad", "(", "x_pad", ",", "[", "0", ",", "rel_size", "-", "W", "]", ")", "\n", "\n", "# reshape and slice out the padded elements", "\n", "x_pad", "=", "x_pad", ".", "reshape", "(", "-", "1", ",", "W", "+", "1", ",", "rel_size", ")", "\n", "x", "=", "x_pad", "[", ":", ",", ":", "W", ",", "win_size", "-", "1", ":", "]", "\n", "\n", "# reshape and tile", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "H", ",", "1", ",", "W", ",", "win_size", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "win_size", ",", "-", "1", ",", "-", "1", ")", "\n", "return", "x", ".", "permute", "(", "permute_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv_bn_act.ConvBnAct.__init__": [[12, 26], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_norm_act.convert_norm_act", "create_norm_act.convert_norm_act.", "aa_layer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.convert_norm_act"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "\n", "bias", "=", "False", ",", "apply_act", "=", "True", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "aa_layer", "=", "None", ",", "\n", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvBnAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "use_aa", "=", "aa_layer", "is", "not", "None", "\n", "\n", "self", ".", "conv", "=", "create_conv2d", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", "=", "1", "if", "use_aa", "else", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "groups", ",", "bias", "=", "bias", ")", "\n", "\n", "# NOTE for backwards compatibility with models that use separate norm and act layer definitions", "\n", "norm_act_layer", "=", "convert_norm_act", "(", "norm_layer", ",", "act_layer", ")", "\n", "self", ".", "bn", "=", "norm_act_layer", "(", "out_channels", ",", "apply_act", "=", "apply_act", ",", "drop_block", "=", "drop_block", ")", "\n", "self", ".", "aa", "=", "aa_layer", "(", "channels", "=", "out_channels", ")", "if", "stride", "==", "2", "and", "use_aa", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv_bn_act.ConvBnAct.in_channels": [[27, 30], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv_bn_act.ConvBnAct.out_channels": [[31, 34], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv_bn_act.ConvBnAct.forward": [[35, 41], ["conv_bn_act.ConvBnAct.conv", "conv_bn_act.ConvBnAct.bn", "conv_bn_act.ConvBnAct.aa"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "if", "self", ".", "aa", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "aa", "(", "x", ")", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm.GroupNorm.__init__": [[9, 12], ["torch.GroupNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_channels", ",", "num_groups", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ")", ":", "\n", "# NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_groups", ",", "num_channels", ",", "eps", "=", "eps", ",", "affine", "=", "affine", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm.GroupNorm.forward": [[13, 15], ["torch.group_norm", "torch.group_norm", "torch.group_norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "F", ".", "group_norm", "(", "x", ",", "self", ".", "num_groups", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm.LayerNorm2d.__init__": [[19, 21], ["torch.LayerNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_channels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm.LayerNorm2d.forward": [[22, 25], ["torch.layer_norm().permute", "torch.layer_norm().permute", "torch.layer_norm().permute", "torch.layer_norm", "torch.layer_norm", "torch.layer_norm", "x.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "layer_norm", "(", "\n", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mixed_conv2d.MixedConv2d.__init__": [[26, 46], ["torch.nn.ModuleDict.__init__", "len", "mixed_conv2d._split_channels", "mixed_conv2d._split_channels", "sum", "sum", "enumerate", "isinstance", "zip", "mixed_conv2d.MixedConv2d.add_module", "str", "conv2d_same.create_conv2d_pad"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mixed_conv2d._split_channels", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mixed_conv2d._split_channels", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.create_conv2d_pad"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "depthwise", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "MixedConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "kernel_size", "if", "isinstance", "(", "kernel_size", ",", "list", ")", "else", "[", "kernel_size", "]", "\n", "num_groups", "=", "len", "(", "kernel_size", ")", "\n", "in_splits", "=", "_split_channels", "(", "in_channels", ",", "num_groups", ")", "\n", "out_splits", "=", "_split_channels", "(", "out_channels", ",", "num_groups", ")", "\n", "self", ".", "in_channels", "=", "sum", "(", "in_splits", ")", "\n", "self", ".", "out_channels", "=", "sum", "(", "out_splits", ")", "\n", "for", "idx", ",", "(", "k", ",", "in_ch", ",", "out_ch", ")", "in", "enumerate", "(", "zip", "(", "kernel_size", ",", "in_splits", ",", "out_splits", ")", ")", ":", "\n", "            ", "conv_groups", "=", "in_ch", "if", "depthwise", "else", "1", "\n", "# use add_module to keep key space clean", "\n", "self", ".", "add_module", "(", "\n", "str", "(", "idx", ")", ",", "\n", "create_conv2d_pad", "(", "\n", "in_ch", ",", "out_ch", ",", "k", ",", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "dilation", "=", "dilation", ",", "groups", "=", "conv_groups", ",", "**", "kwargs", ")", "\n", ")", "\n", "", "self", ".", "splits", "=", "in_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mixed_conv2d.MixedConv2d.forward": [[47, 52], ["torch.split", "torch.cat", "c", "enumerate", "mixed_conv2d.MixedConv2d.values"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_split", "=", "torch", ".", "split", "(", "x", ",", "self", ".", "splits", ",", "1", ")", "\n", "x_out", "=", "[", "c", "(", "x_split", "[", "i", "]", ")", "for", "i", ",", "c", "in", "enumerate", "(", "self", ".", "values", "(", ")", ")", "]", "\n", "x", "=", "torch", ".", "cat", "(", "x_out", ",", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mixed_conv2d._split_channels": [[14, 18], ["sum", "range"], "function", ["None"], ["def", "_split_channels", "(", "num_chan", ",", "num_groups", ")", ":", "\n", "    ", "split", "=", "[", "num_chan", "//", "num_groups", "for", "_", "in", "range", "(", "num_groups", ")", "]", "\n", "split", "[", "0", "]", "+=", "num_chan", "-", "sum", "(", "split", ")", "\n", "return", "split", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_fn": [[105, 126], ["isinstance", "config.is_exportable", "config.is_no_jit", "config.is_exportable", "config.is_scriptable", "config.is_no_jit", "config.is_exportable"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_no_jit", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_scriptable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_no_jit", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable"], ["", "def", "get_act_fn", "(", "name", ":", "Union", "[", "Callable", ",", "str", "]", "=", "'relu'", ")", ":", "\n", "    ", "\"\"\" Activation Function Factory\n    Fetching activation fns by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"", "\n", "if", "not", "name", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "name", ",", "Callable", ")", ":", "\n", "        ", "return", "name", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", "or", "is_scriptable", "(", ")", ")", ":", "\n", "# If not exporting or scripting the model, first look for a memory-efficient version with", "\n", "# custom autograd, then fallback", "\n", "        ", "if", "name", "in", "_ACT_FN_ME", ":", "\n", "            ", "return", "_ACT_FN_ME", "[", "name", "]", "\n", "", "", "if", "is_exportable", "(", ")", "and", "name", "in", "(", "'silu'", ",", "'swish'", ")", ":", "\n", "# FIXME PyTorch SiLU doesn't ONNX export, this is a temp hack", "\n", "        ", "return", "swish", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_FN_JIT", ":", "\n", "            ", "return", "_ACT_FN_JIT", "[", "name", "]", "\n", "", "", "return", "_ACT_FN_DEFAULT", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer": [[128, 147], ["isinstance", "config.is_exportable", "config.is_no_jit", "config.is_exportable", "config.is_scriptable", "config.is_no_jit", "config.is_exportable"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_no_jit", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_scriptable", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_no_jit", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.config.is_exportable"], ["", "def", "get_act_layer", "(", "name", ":", "Union", "[", "Type", "[", "nn", ".", "Module", "]", ",", "str", "]", "=", "'relu'", ")", ":", "\n", "    ", "\"\"\" Activation Layer Factory\n    Fetching activation layers by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    \"\"\"", "\n", "if", "not", "name", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "name", ",", "type", ")", ":", "\n", "        ", "return", "name", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", "or", "is_scriptable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_LAYER_ME", ":", "\n", "            ", "return", "_ACT_LAYER_ME", "[", "name", "]", "\n", "", "", "if", "is_exportable", "(", ")", "and", "name", "in", "(", "'silu'", ",", "'swish'", ")", ":", "\n", "# FIXME PyTorch SiLU doesn't ONNX export, this is a temp hack", "\n", "        ", "return", "Swish", "\n", "", "if", "not", "(", "is_no_jit", "(", ")", "or", "is_exportable", "(", ")", ")", ":", "\n", "        ", "if", "name", "in", "_ACT_LAYER_JIT", ":", "\n", "            ", "return", "_ACT_LAYER_JIT", "[", "name", "]", "\n", "", "", "return", "_ACT_LAYER_DEFAULT", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer": [[149, 154], ["create_act.get_act_layer", "get_act_layer.", "get_act_layer."], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer"], ["", "def", "create_act_layer", "(", "name", ":", "Union", "[", "nn", ".", "Module", ",", "str", "]", ",", "inplace", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "act_layer", "=", "get_act_layer", "(", "name", ")", "\n", "if", "act_layer", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "return", "act_layer", "(", "**", "kwargs", ")", "if", "inplace", "is", "None", "else", "act_layer", "(", "inplace", "=", "inplace", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.DropBlock2d.__init__": [[112, 128], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "drop_prob", "=", "0.1", ",", "\n", "block_size", "=", "7", ",", "\n", "gamma_scale", "=", "1.0", ",", "\n", "with_noise", "=", "False", ",", "\n", "inplace", "=", "False", ",", "\n", "batchwise", "=", "False", ",", "\n", "fast", "=", "True", ")", ":", "\n", "        ", "super", "(", "DropBlock2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_prob", "=", "drop_prob", "\n", "self", ".", "gamma_scale", "=", "gamma_scale", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "with_noise", "=", "with_noise", "\n", "self", ".", "inplace", "=", "inplace", "\n", "self", ".", "batchwise", "=", "batchwise", "\n", "self", ".", "fast", "=", "fast", "# FIXME finish comparisons of fast vs not", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.DropBlock2d.forward": [[129, 138], ["drop.drop_block_fast_2d", "drop.drop_block_2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_block_fast_2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_block_2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "not", "self", ".", "training", "or", "not", "self", ".", "drop_prob", ":", "\n", "            ", "return", "x", "\n", "", "if", "self", ".", "fast", ":", "\n", "            ", "return", "drop_block_fast_2d", "(", "\n", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "block_size", ",", "self", ".", "gamma_scale", ",", "self", ".", "with_noise", ",", "self", ".", "inplace", ",", "self", ".", "batchwise", ")", "\n", "", "else", ":", "\n", "            ", "return", "drop_block_2d", "(", "\n", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "block_size", ",", "self", ".", "gamma_scale", ",", "self", ".", "with_noise", ",", "self", ".", "inplace", ",", "self", ".", "batchwise", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.DropPath.__init__": [[163, 166], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "drop_prob", "=", "None", ")", ":", "\n", "        ", "super", "(", "DropPath", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_prob", "=", "drop_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.DropPath.forward": [[167, 169], ["drop.drop_path"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "drop_path", "(", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "training", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_block_2d": [[22, 68], ["min", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.reshape().to", "torch.reshape().to", "torch.reshape().to", "min", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.rand", "torch.rand", "torch.rand", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.max_pool2d", "torch.reshape", "torch.reshape", "torch.reshape", "torch.randn", "torch.randn", "torch.randn", "torch.randn_like", "torch.randn_like", "torch.randn_like", "x.mul_().add_", "x.mul_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.mul_", "block_mask.numel", "block_mask.to().sum().add", "block_mask.to().sum", "block_mask.to"], "function", ["None"], ["def", "drop_block_2d", "(", "\n", "x", ",", "drop_prob", ":", "float", "=", "0.1", ",", "block_size", ":", "int", "=", "7", ",", "gamma_scale", ":", "float", "=", "1.0", ",", "\n", "with_noise", ":", "bool", "=", "False", ",", "inplace", ":", "bool", "=", "False", ",", "batchwise", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n    \"\"\"", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "total_size", "=", "W", "*", "H", "\n", "clipped_block_size", "=", "min", "(", "block_size", ",", "min", "(", "W", ",", "H", ")", ")", "\n", "# seed_drop_rate, the gamma parameter", "\n", "gamma", "=", "gamma_scale", "*", "drop_prob", "*", "total_size", "/", "clipped_block_size", "**", "2", "/", "(", "\n", "(", "W", "-", "block_size", "+", "1", ")", "*", "(", "H", "-", "block_size", "+", "1", ")", ")", "\n", "\n", "# Forces the block to be inside the feature map.", "\n", "w_i", ",", "h_i", "=", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "W", ")", ".", "to", "(", "x", ".", "device", ")", ",", "torch", ".", "arange", "(", "H", ")", ".", "to", "(", "x", ".", "device", ")", ")", "\n", "valid_block", "=", "(", "(", "w_i", ">=", "clipped_block_size", "//", "2", ")", "&", "(", "w_i", "<", "W", "-", "(", "clipped_block_size", "-", "1", ")", "//", "2", ")", ")", "&", "(", "(", "h_i", ">=", "clipped_block_size", "//", "2", ")", "&", "(", "h_i", "<", "H", "-", "(", "clipped_block_size", "-", "1", ")", "//", "2", ")", ")", "\n", "valid_block", "=", "torch", ".", "reshape", "(", "valid_block", ",", "(", "1", ",", "1", ",", "H", ",", "W", ")", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "\n", "if", "batchwise", ":", "\n", "# one mask for whole batch, quite a bit faster", "\n", "        ", "uniform_noise", "=", "torch", ".", "rand", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "", "else", ":", "\n", "        ", "uniform_noise", "=", "torch", ".", "rand_like", "(", "x", ")", "\n", "", "block_mask", "=", "(", "(", "2", "-", "gamma", "-", "valid_block", "+", "uniform_noise", ")", ">=", "1", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "block_mask", "=", "-", "F", ".", "max_pool2d", "(", "\n", "-", "block_mask", ",", "\n", "kernel_size", "=", "clipped_block_size", ",", "# block_size,", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "clipped_block_size", "//", "2", ")", "\n", "\n", "if", "with_noise", ":", "\n", "        ", "normal_noise", "=", "torch", ".", "randn", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "if", "batchwise", "else", "torch", ".", "randn_like", "(", "x", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", ")", ".", "add_", "(", "normal_noise", "*", "(", "1", "-", "block_mask", ")", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "+", "normal_noise", "*", "(", "1", "-", "block_mask", ")", "\n", "", "", "else", ":", "\n", "        ", "normalize_scale", "=", "(", "block_mask", ".", "numel", "(", ")", "/", "block_mask", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ".", "sum", "(", ")", ".", "add", "(", "1e-7", ")", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", "*", "normalize_scale", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "*", "normalize_scale", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_block_fast_2d": [[70, 107], ["min", "torch.max_pool2d", "min", "F.max_pool2d.to", "torch.rand", "torch.rand", "torch.rand", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.randn", "torch.randn", "torch.randn", "torch.randn_like", "torch.randn_like", "torch.randn_like", "x.mul_().add_", "x.mul_", "x.mul_", "F.max_pool2d.numel", "F.max_pool2d.to().sum().add", "F.max_pool2d.to().sum", "F.max_pool2d.to"], "function", ["None"], ["", "def", "drop_block_fast_2d", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "drop_prob", ":", "float", "=", "0.1", ",", "block_size", ":", "int", "=", "7", ",", "\n", "gamma_scale", ":", "float", "=", "1.0", ",", "with_noise", ":", "bool", "=", "False", ",", "inplace", ":", "bool", "=", "False", ",", "batchwise", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n    block mask at edges.\n    \"\"\"", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "total_size", "=", "W", "*", "H", "\n", "clipped_block_size", "=", "min", "(", "block_size", ",", "min", "(", "W", ",", "H", ")", ")", "\n", "gamma", "=", "gamma_scale", "*", "drop_prob", "*", "total_size", "/", "clipped_block_size", "**", "2", "/", "(", "\n", "(", "W", "-", "block_size", "+", "1", ")", "*", "(", "H", "-", "block_size", "+", "1", ")", ")", "\n", "\n", "if", "batchwise", ":", "\n", "# one mask for whole batch, quite a bit faster", "\n", "        ", "block_mask", "=", "torch", ".", "rand", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "<", "gamma", "\n", "", "else", ":", "\n", "# mask per batch element", "\n", "        ", "block_mask", "=", "torch", ".", "rand_like", "(", "x", ")", "<", "gamma", "\n", "", "block_mask", "=", "F", ".", "max_pool2d", "(", "\n", "block_mask", ".", "to", "(", "x", ".", "dtype", ")", ",", "kernel_size", "=", "clipped_block_size", ",", "stride", "=", "1", ",", "padding", "=", "clipped_block_size", "//", "2", ")", "\n", "\n", "if", "with_noise", ":", "\n", "        ", "normal_noise", "=", "torch", ".", "randn", "(", "(", "1", ",", "C", ",", "H", ",", "W", ")", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "if", "batchwise", "else", "torch", ".", "randn_like", "(", "x", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "1.", "-", "block_mask", ")", ".", "add_", "(", "normal_noise", "*", "block_mask", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "(", "1.", "-", "block_mask", ")", "+", "normal_noise", "*", "block_mask", "\n", "", "", "else", ":", "\n", "        ", "block_mask", "=", "1", "-", "block_mask", "\n", "normalize_scale", "=", "(", "block_mask", ".", "numel", "(", ")", "/", "block_mask", ".", "to", "(", "dtype", "=", "torch", ".", "float32", ")", ".", "sum", "(", ")", ".", "add", "(", "1e-7", ")", ")", ".", "to", "(", "dtype", "=", "x", ".", "dtype", ")", "\n", "if", "inplace", ":", "\n", "            ", "x", ".", "mul_", "(", "block_mask", "*", "normalize_scale", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "*", "block_mask", "*", "normalize_scale", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.drop.drop_path": [[140, 158], ["random_tensor.floor_", "torch.rand", "torch.rand", "torch.rand", "x.div"], "function", ["None"], ["", "", "", "def", "drop_path", "(", "x", ",", "drop_prob", ":", "float", "=", "0.", ",", "training", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"", "\n", "if", "drop_prob", "==", "0.", "or", "not", "training", ":", "\n", "        ", "return", "x", "\n", "", "keep_prob", "=", "1", "-", "drop_prob", "\n", "shape", "=", "(", "x", ".", "shape", "[", "0", "]", ",", ")", "+", "(", "1", ",", ")", "*", "(", "x", ".", "ndim", "-", "1", ")", "# work with diff dim tensors, not just 2D ConvNets", "\n", "random_tensor", "=", "keep_prob", "+", "torch", ".", "rand", "(", "shape", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "random_tensor", ".", "floor_", "(", ")", "# binarize", "\n", "output", "=", "x", ".", "div", "(", "keep_prob", ")", "*", "random_tensor", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.SwishJitAutoFn.symbolic": [[33, 36], ["g.op", "g.op"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "symbolic", "(", "g", ",", "x", ")", ":", "\n", "        ", "return", "g", ".", "op", "(", "\"Mul\"", ",", "x", ",", "g", ".", "op", "(", "\"Sigmoid\"", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.SwishJitAutoFn.forward": [[37, 41], ["ctx.save_for_backward", "activations_me.swish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.swish_jit_fwd"], ["", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "swish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.SwishJitAutoFn.backward": [[42, 46], ["activations_me.swish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.swish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "swish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.SwishMe.__init__": [[53, 55], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "SwishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.SwishMe.forward": [[56, 58], ["SwishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "SwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.MishJitAutoFn.forward": [[76, 80], ["ctx.save_for_backward", "activations_me.mish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.mish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "mish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.MishJitAutoFn.backward": [[81, 85], ["activations_me.mish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.mish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "mish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.MishMe.__init__": [[92, 94], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "MishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.MishMe.forward": [[95, 97], ["MishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "MishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSigmoidJitAutoFn.forward": [[111, 115], ["ctx.save_for_backward", "activations_me.hard_sigmoid_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_sigmoid_jit_fwd"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_sigmoid_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSigmoidJitAutoFn.backward": [[116, 120], ["activations_me.hard_sigmoid_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_sigmoid_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_sigmoid_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSigmoidMe.__init__": [[127, 129], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoidMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSigmoidMe.forward": [[130, 132], ["HardSigmoidJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardSigmoidJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSwishJitAutoFn.forward": [[148, 152], ["ctx.save_for_backward", "activations_me.hard_swish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_swish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_swish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSwishJitAutoFn.backward": [[153, 157], ["activations_me.hard_swish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_swish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_swish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSwishJitAutoFn.symbolic": [[158, 164], ["g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "g.op", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "symbolic", "(", "g", ",", "self", ")", ":", "\n", "        ", "input", "=", "g", ".", "op", "(", "\"Add\"", ",", "self", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "3", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "hardtanh_", "=", "g", ".", "op", "(", "\"Clip\"", ",", "input", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "torch", ".", "float", ")", ")", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "6", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "hardtanh_", "=", "g", ".", "op", "(", "\"Div\"", ",", "hardtanh_", ",", "g", ".", "op", "(", "'Constant'", ",", "value_t", "=", "torch", ".", "tensor", "(", "6", ",", "dtype", "=", "torch", ".", "float", ")", ")", ")", "\n", "return", "g", ".", "op", "(", "\"Mul\"", ",", "self", ",", "hardtanh_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSwishMe.__init__": [[171, 173], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardSwishMe.forward": [[174, 176], ["HardSwishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardSwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardMishJitAutoFn.forward": [[195, 199], ["ctx.save_for_backward", "activations_me.hard_mish_jit_fwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_mish_jit_fwd"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "x", ")", "\n", "return", "hard_mish_jit_fwd", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardMishJitAutoFn.backward": [[200, 204], ["activations_me.hard_mish_jit_bwd"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_mish_jit_bwd"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "x", "=", "ctx", ".", "saved_tensors", "[", "0", "]", "\n", "return", "hard_mish_jit_bwd", "(", "x", ",", "grad_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardMishMe.__init__": [[211, 213], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMishMe", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.HardMishMe.forward": [[214, 216], ["HardMishJitAutoFn.apply"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "HardMishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.swish_jit_fwd": [[17, 20], ["x.mul", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "mul", "(", "torch", ".", "sigmoid", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.swish_jit_bwd": [[22, 26], ["torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "swish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "return", "grad_output", "*", "(", "x_sigmoid", "*", "(", "1", "+", "x", "*", "(", "1", "-", "x_sigmoid", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.swish_me": [[48, 50], ["SwishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "swish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "SwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.mish_jit_fwd": [[60, 63], ["x.mul", "torch.tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", ".", "mul", "(", "torch", ".", "tanh", "(", "F", ".", "softplus", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.mish_jit_bwd": [[65, 70], ["torch.sigmoid", "torch.nn.functional.softplus().tanh", "grad_output.mul", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "mish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "x_tanh_sp", "=", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", "\n", "return", "grad_output", ".", "mul", "(", "x_tanh_sp", "+", "x", "*", "x_sigmoid", "*", "(", "1", "-", "x_tanh_sp", "*", "x_tanh_sp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.mish_me": [[87, 89], ["MishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "mish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "MishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_sigmoid_jit_fwd": [[99, 102], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit_fwd", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_sigmoid_jit_bwd": [[104, 108], ["torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_sigmoid_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "(", "x", ">=", "-", "3.", ")", "&", "(", "x", "<=", "3.", ")", ")", "/", "6.", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_sigmoid_me": [[122, 124], ["HardSigmoidJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_sigmoid_me", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "HardSigmoidJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_swish_jit_fwd": [[134, 137], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "(", "x", "+", "3", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "6", ")", ".", "div", "(", "6.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_swish_jit_bwd": [[139, 144], ["torch.where", "torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_swish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "x", ">=", "3.", ")", "\n", "m", "=", "torch", ".", "where", "(", "(", "x", ">=", "-", "3.", ")", "&", "(", "x", "<=", "3.", ")", ",", "x", "/", "3.", "+", ".5", ",", "m", ")", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_swish_me": [[166, 168], ["HardSwishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_swish_me", "(", "x", ",", "inplace", "=", "False", ")", ":", "\n", "    ", "return", "HardSwishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_mish_jit_fwd": [[178, 181], ["None"], "function", ["None"], ["", "", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit_fwd", "(", "x", ")", ":", "\n", "    ", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_mish_jit_bwd": [[183, 188], ["torch.where", "torch.ones_like"], "function", ["None"], ["", "@", "torch", ".", "jit", ".", "script", "\n", "def", "hard_mish_jit_bwd", "(", "x", ",", "grad_output", ")", ":", "\n", "    ", "m", "=", "torch", ".", "ones_like", "(", "x", ")", "*", "(", "x", ">=", "-", "2.", ")", "\n", "m", "=", "torch", ".", "where", "(", "(", "x", ">=", "-", "2.", ")", "&", "(", "x", "<=", "0.", ")", ",", "x", "+", "1.", ",", "m", ")", "\n", "return", "grad_output", "*", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations_me.hard_mish_me": [[206, 208], ["HardMishJitAutoFn.apply"], "function", ["None"], ["", "", "def", "hard_mish_me", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "HardMishJitAutoFn", ".", "apply", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.AvgPool2dSame.__init__": [[24, 28], ["helpers.to_2tuple", "helpers.to_2tuple", "torch.AvgPool2d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", ":", "int", ",", "stride", "=", "None", ",", "padding", "=", "0", ",", "ceil_mode", "=", "False", ",", "count_include_pad", "=", "True", ")", ":", "\n", "        ", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "super", "(", "AvgPool2dSame", ",", "self", ")", ".", "__init__", "(", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "ceil_mode", ",", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.AvgPool2dSame.forward": [[29, 33], ["padding.pad_same", "torch.avg_pool2d", "torch.avg_pool2d", "torch.avg_pool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ")", "\n", "return", "F", ".", "avg_pool2d", "(", "\n", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "self", ".", "padding", ",", "self", ".", "ceil_mode", ",", "self", ".", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.MaxPool2dSame.__init__": [[45, 50], ["helpers.to_2tuple", "helpers.to_2tuple", "helpers.to_2tuple", "torch.MaxPool2d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "kernel_size", ":", "int", ",", "stride", "=", "None", ",", "padding", "=", "0", ",", "dilation", "=", "1", ",", "ceil_mode", "=", "False", ")", ":", "\n", "        ", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "dilation", "=", "to_2tuple", "(", "dilation", ")", "\n", "super", "(", "MaxPool2dSame", ",", "self", ")", ".", "__init__", "(", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.MaxPool2dSame.forward": [[51, 54], ["padding.pad_same", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "float"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "pad_same", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "value", "=", "-", "float", "(", "'inf'", ")", ")", "\n", "return", "F", ".", "max_pool2d", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "stride", ",", "(", "0", ",", "0", ")", ",", "self", ".", "dilation", ",", "self", ".", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.avg_pool2d_same": [[14, 19], ["padding.pad_same", "torch.avg_pool2d"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["def", "avg_pool2d_same", "(", "x", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "stride", ":", "List", "[", "int", "]", ",", "padding", ":", "List", "[", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "ceil_mode", ":", "bool", "=", "False", ",", "count_include_pad", ":", "bool", "=", "True", ")", ":", "\n", "# FIXME how to deal with count_include_pad vs not for external padding?", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "kernel_size", ",", "stride", ")", "\n", "return", "F", ".", "avg_pool2d", "(", "x", ",", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "ceil_mode", ",", "count_include_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.max_pool2d_same": [[35, 40], ["padding.pad_same", "torch.max_pool2d", "float"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same"], ["", "", "def", "max_pool2d_same", "(", "\n", "x", ",", "kernel_size", ":", "List", "[", "int", "]", ",", "stride", ":", "List", "[", "int", "]", ",", "padding", ":", "List", "[", "int", "]", "=", "(", "0", ",", "0", ")", ",", "\n", "dilation", ":", "List", "[", "int", "]", "=", "(", "1", ",", "1", ")", ",", "ceil_mode", ":", "bool", "=", "False", ")", ":", "\n", "    ", "x", "=", "pad_same", "(", "x", ",", "kernel_size", ",", "stride", ",", "value", "=", "-", "float", "(", "'inf'", ")", ")", "\n", "return", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", ",", "stride", ",", "(", "0", ",", "0", ")", ",", "dilation", ",", "ceil_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.pool2d_same.create_pool2d": [[56, 74], ["kwargs.pop", "padding.get_padding_value", "pool2d_same.AvgPool2dSame", "torch.AvgPool2d", "pool2d_same.MaxPool2dSame", "torch.MaxPool2d"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value"], ["", "", "def", "create_pool2d", "(", "pool_type", ",", "kernel_size", ",", "stride", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "stride", "=", "stride", "or", "kernel_size", "\n", "padding", "=", "kwargs", ".", "pop", "(", "'padding'", ",", "''", ")", "\n", "padding", ",", "is_dynamic", "=", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "if", "is_dynamic", ":", "\n", "        ", "if", "pool_type", "==", "'avg'", ":", "\n", "            ", "return", "AvgPool2dSame", "(", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "return", "MaxPool2dSame", "(", "kernel_size", ",", "stride", "=", "stride", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f'Unsupported pool type {pool_type}'", "\n", "", "", "else", ":", "\n", "        ", "if", "pool_type", "==", "'avg'", ":", "\n", "            ", "return", "nn", ".", "AvgPool2d", "(", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "return", "nn", ".", "MaxPool2d", "(", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f'Unsupported pool type {pool_type}'", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.Mlp.__init__": [[13, 24], ["torch.nn.Module.__init__", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.Mlp.forward": [[25, 32], ["mlp.Mlp.fc1", "mlp.Mlp.act", "mlp.Mlp.drop1", "mlp.Mlp.fc2", "mlp.Mlp.drop2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.GluMlp.__init__": [[38, 50], ["torch.nn.Module.__init__", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "Sigmoid", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "assert", "hidden_features", "%", "2", "==", "0", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", "//", "2", ",", "out_features", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.GluMlp.init_weights": [[51, 56], ["torch.nn.init.ones_", "torch.nn.init.normal_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# override init of fc1 w/ gate portion set to weight near zero, bias=1", "\n", "        ", "fc1_mid", "=", "self", ".", "fc1", ".", "bias", ".", "shape", "[", "0", "]", "//", "2", "\n", "nn", ".", "init", ".", "ones_", "(", "self", ".", "fc1", ".", "bias", "[", "fc1_mid", ":", "]", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "fc1", ".", "weight", "[", "fc1_mid", ":", "]", ",", "std", "=", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.GluMlp.forward": [[57, 65], ["mlp.GluMlp.fc1", "mlp.GluMlp.chunk", "mlp.GluMlp.drop1", "mlp.GluMlp.fc2", "mlp.GluMlp.drop2", "mlp.GluMlp.act"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", ",", "gates", "=", "x", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "x", "=", "x", "*", "self", ".", "act", "(", "gates", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.GatedMlp.__init__": [[70, 88], ["torch.nn.Module.__init__", "helpers.to_2tuple", "torch.nn.Linear", "act_layer", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Dropout", "gate_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "\n", "gate_layer", "=", "None", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "drop_probs", "=", "to_2tuple", "(", "drop", ")", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "0", "]", ")", "\n", "if", "gate_layer", "is", "not", "None", ":", "\n", "            ", "assert", "hidden_features", "%", "2", "==", "0", "\n", "self", ".", "gate", "=", "gate_layer", "(", "hidden_features", ")", "\n", "hidden_features", "=", "hidden_features", "//", "2", "# FIXME base reduction on gate property?", "\n", "", "else", ":", "\n", "            ", "self", ".", "gate", "=", "nn", ".", "Identity", "(", ")", "\n", "", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ")", "\n", "self", ".", "drop2", "=", "nn", ".", "Dropout", "(", "drop_probs", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.GatedMlp.forward": [[89, 97], ["mlp.GatedMlp.fc1", "mlp.GatedMlp.act", "mlp.GatedMlp.drop1", "mlp.GatedMlp.gate", "mlp.GatedMlp.fc2", "mlp.GatedMlp.drop2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop1", "(", "x", ")", "\n", "x", "=", "self", ".", "gate", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.ConvMlp.__init__": [[102, 112], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Dropout", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "in_features", ",", "hidden_features", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "hidden_features", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "hidden_features", ",", "out_features", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.mlp.ConvMlp.forward": [[113, 120], ["mlp.ConvMlp.fc1", "mlp.ConvMlp.norm", "mlp.ConvMlp.act", "mlp.ConvMlp.drop", "mlp.ConvMlp.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_attn.RadixSoftmax.__init__": [[17, 21], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "radix", ",", "cardinality", ")", ":", "\n", "        ", "super", "(", "RadixSoftmax", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "radix", "=", "radix", "\n", "self", ".", "cardinality", "=", "cardinality", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_attn.RadixSoftmax.forward": [[22, 31], ["torch.sigmoid.size", "torch.sigmoid.size", "torch.sigmoid.view().transpose", "torch.sigmoid.view().transpose", "torch.softmax", "torch.softmax", "torch.sigmoid.reshape", "torch.sigmoid.reshape", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid.view", "torch.sigmoid.view"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "batch", ",", "self", ".", "cardinality", ",", "self", ".", "radix", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "x", "=", "F", ".", "softmax", "(", "x", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", ".", "reshape", "(", "batch", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_attn.SplitAttn.__init__": [[36, 60], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "act_layer", "torch.nn.Conv2d", "torch.nn.Conv2d", "split_attn.RadixSoftmax", "helpers.make_divisible", "norm_layer", "torch.nn.Identity", "torch.nn.Identity", "norm_layer", "torch.nn.Identity", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", "=", "None", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "padding", "=", "None", ",", "\n", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "radix", "=", "2", ",", "rd_ratio", "=", "0.25", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "drop_block", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "SplitAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_channels", "=", "out_channels", "or", "in_channels", "\n", "self", ".", "radix", "=", "radix", "\n", "self", ".", "drop_block", "=", "drop_block", "\n", "mid_chs", "=", "out_channels", "*", "radix", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "attn_chs", "=", "make_divisible", "(", "in_channels", "*", "radix", "*", "rd_ratio", ",", "min_value", "=", "32", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "else", ":", "\n", "            ", "attn_chs", "=", "rd_channels", "*", "radix", "\n", "\n", "", "padding", "=", "kernel_size", "//", "2", "if", "padding", "is", "None", "else", "padding", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "mid_chs", ",", "kernel_size", ",", "stride", ",", "padding", ",", "dilation", ",", "\n", "groups", "=", "groups", "*", "radix", ",", "bias", "=", "bias", ",", "**", "kwargs", ")", "\n", "self", ".", "bn0", "=", "norm_layer", "(", "mid_chs", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act0", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "out_channels", ",", "attn_chs", ",", "1", ",", "groups", "=", "groups", ")", "\n", "self", ".", "bn1", "=", "norm_layer", "(", "attn_chs", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act1", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "attn_chs", ",", "mid_chs", ",", "1", ",", "groups", "=", "groups", ")", "\n", "self", ".", "rsoftmax", "=", "RadixSoftmax", "(", "radix", ",", "groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.split_attn.SplitAttn.forward": [[61, 86], ["split_attn.SplitAttn.conv", "split_attn.SplitAttn.bn0", "split_attn.SplitAttn.act0", "x.reshape.sum.mean", "split_attn.SplitAttn.fc1", "split_attn.SplitAttn.bn1", "split_attn.SplitAttn.act1", "split_attn.SplitAttn.fc2", "split_attn.SplitAttn.rsoftmax().view", "out.contiguous", "split_attn.SplitAttn.drop_block", "x.reshape.reshape.reshape", "x.reshape.reshape.sum", "split_attn.SplitAttn.rsoftmax", "split_attn.SplitAttn.reshape"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "bn0", "(", "x", ")", "\n", "if", "self", ".", "drop_block", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "drop_block", "(", "x", ")", "\n", "", "x", "=", "self", ".", "act0", "(", "x", ")", "\n", "\n", "B", ",", "RC", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "(", "B", ",", "self", ".", "radix", ",", "RC", "//", "self", ".", "radix", ",", "H", ",", "W", ")", ")", "\n", "x_gap", "=", "x", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "x_gap", "=", "x", "\n", "", "x_gap", "=", "x_gap", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x_gap", "=", "self", ".", "fc1", "(", "x_gap", ")", "\n", "x_gap", "=", "self", ".", "bn1", "(", "x_gap", ")", "\n", "x_gap", "=", "self", ".", "act1", "(", "x_gap", ")", "\n", "x_attn", "=", "self", ".", "fc2", "(", "x_gap", ")", "\n", "\n", "x_attn", "=", "self", ".", "rsoftmax", "(", "x_attn", ")", ".", "view", "(", "B", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "if", "self", ".", "radix", ">", "1", ":", "\n", "            ", "out", "=", "(", "x", "*", "x_attn", ".", "reshape", "(", "(", "B", ",", "self", ".", "radix", ",", "RC", "//", "self", ".", "radix", ",", "1", ",", "1", ")", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "x", "*", "x_attn", "\n", "", "return", "out", ".", "contiguous", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.patch_embed.PatchEmbed.__init__": [[18, 30], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Conv2d", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ",", "norm_layer", "=", "None", ",", "flatten", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "grid_size", "=", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ",", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "\n", "self", ".", "num_patches", "=", "self", ".", "grid_size", "[", "0", "]", "*", "self", ".", "grid_size", "[", "1", "]", "\n", "self", ".", "flatten", "=", "flatten", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.patch_embed.PatchEmbed.forward": [[31, 40], ["trace_utils._assert", "trace_utils._assert", "patch_embed.PatchEmbed.proj", "patch_embed.PatchEmbed.norm", "x.flatten().transpose.flatten().transpose.flatten().transpose", "x.flatten().transpose.flatten().transpose.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "_assert", "(", "H", "==", "self", ".", "img_size", "[", "0", "]", ",", "f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"", ")", "\n", "_assert", "(", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "if", "self", ".", "flatten", ":", "\n", "            ", "x", "=", "x", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "# BCHW -> BNC", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.FastAdaptiveAvgPool2d.__init__": [[53, 56], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "flatten", "=", "False", ")", ":", "\n", "        ", "super", "(", "FastAdaptiveAvgPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "flatten", "=", "flatten", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.FastAdaptiveAvgPool2d.forward": [[57, 59], ["x.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "not", "self", ".", "flatten", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.AdaptiveAvgMaxPool2d.__init__": [[62, 65], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "AdaptiveAvgMaxPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.AdaptiveAvgMaxPool2d.forward": [[66, 68], ["adaptive_avgmax_pool.adaptive_avgmax_pool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "adaptive_avgmax_pool2d", "(", "x", ",", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d.__init__": [[71, 74], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "output_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "AdaptiveCatAvgMaxPool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_size", "=", "output_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d.forward": [[75, 77], ["adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "adaptive_catavgmax_pool2d", "(", "x", ",", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.__init__": [[82, 102], ["torch.Module.__init__", "torch.Flatten", "torch.Flatten", "torch.Flatten", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "adaptive_avgmax_pool.FastAdaptiveAvgPool2d", "torch.Identity", "torch.Identity", "torch.Identity", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "adaptive_avgmax_pool.AdaptiveAvgMaxPool2d", "adaptive_avgmax_pool.AdaptiveCatAvgMaxPool2d", "torch.AdaptiveMaxPool2d", "torch.AdaptiveMaxPool2d", "torch.AdaptiveMaxPool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "output_size", "=", "1", ",", "pool_type", "=", "'fast'", ",", "flatten", "=", "False", ")", ":", "\n", "        ", "super", "(", "SelectAdaptivePool2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pool_type", "=", "pool_type", "or", "''", "# convert other falsy values to empty string for consistent TS typing", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "flatten", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "pool_type", "==", "''", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "Identity", "(", ")", "# pass through", "\n", "", "elif", "pool_type", "==", "'fast'", ":", "\n", "            ", "assert", "output_size", "==", "1", "\n", "self", ".", "pool", "=", "FastAdaptiveAvgPool2d", "(", "flatten", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Identity", "(", ")", "\n", "", "elif", "pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'avgmax'", ":", "\n", "            ", "self", ".", "pool", "=", "AdaptiveAvgMaxPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'catavgmax'", ":", "\n", "            ", "self", ".", "pool", "=", "AdaptiveCatAvgMaxPool2d", "(", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveMaxPool2d", "(", "output_size", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Invalid pool type: %s'", "%", "pool_type", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.is_identity": [[103, 105], ["None"], "methods", ["None"], ["", "", "def", "is_identity", "(", "self", ")", ":", "\n", "        ", "return", "not", "self", ".", "pool_type", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.forward": [[106, 110], ["adaptive_avgmax_pool.SelectAdaptivePool2d.pool", "adaptive_avgmax_pool.SelectAdaptivePool2d.flatten"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult": [[111, 113], ["adaptive_avgmax_pool.adaptive_pool_feat_mult"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_pool_feat_mult"], ["", "def", "feat_mult", "(", "self", ")", ":", "\n", "        ", "return", "adaptive_pool_feat_mult", "(", "self", ".", "pool_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.__repr__": [[114, 118], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "' ('", "+", "'pool_type='", "+", "self", ".", "pool_type", "+", "', flatten='", "+", "str", "(", "self", ".", "flatten", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_pool_feat_mult": [[17, 22], ["None"], "function", ["None"], ["def", "adaptive_pool_feat_mult", "(", "pool_type", "=", "'avg'", ")", ":", "\n", "    ", "if", "pool_type", "==", "'catavgmax'", ":", "\n", "        ", "return", "2", "\n", "", "else", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d": [[24, 28], ["torch.adaptive_avg_pool2d", "torch.adaptive_max_pool2d"], "function", ["None"], ["", "", "def", "adaptive_avgmax_pool2d", "(", "x", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "x_avg", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "x_max", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "return", "0.5", "*", "(", "x_avg", "+", "x_max", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d": [[30, 34], ["torch.adaptive_avg_pool2d", "torch.adaptive_max_pool2d", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "adaptive_catavgmax_pool2d", "(", "x", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "x_avg", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "x_max", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "return", "torch", ".", "cat", "(", "(", "x_avg", ",", "x_max", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.select_adaptive_pool2d": [[36, 50], ["torch.adaptive_avg_pool2d", "adaptive_avgmax_pool.adaptive_avgmax_pool2d", "adaptive_avgmax_pool.adaptive_catavgmax_pool2d", "torch.adaptive_max_pool2d"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_avgmax_pool2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.adaptive_catavgmax_pool2d"], ["", "def", "select_adaptive_pool2d", "(", "x", ",", "pool_type", "=", "'avg'", ",", "output_size", "=", "1", ")", ":", "\n", "    ", "\"\"\"Selectable global pooling function with dynamic input kernel size\n    \"\"\"", "\n", "if", "pool_type", "==", "'avg'", ":", "\n", "        ", "x", "=", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'avgmax'", ":", "\n", "        ", "x", "=", "adaptive_avgmax_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'catavgmax'", ":", "\n", "        ", "x", "=", "adaptive_catavgmax_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "elif", "pool_type", "==", "'max'", ":", "\n", "        ", "x", "=", "F", ".", "adaptive_max_pool2d", "(", "x", ",", "output_size", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "'Invalid pool type: %s'", "%", "pool_type", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple": [[10, 16], ["isinstance", "tuple", "itertools.repeat"], "function", ["None"], ["from", "typing", "import", "Any", ",", "Callable", ",", "Optional", ",", "Tuple", "\n", "\n", "import", "torch", "\n", "import", "torch", ".", "nn", "as", "nn", "\n", "\n", "\n", "from", ".", "features", "import", "FeatureListNet", ",", "FeatureDictNet", ",", "FeatureHookNet", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers.make_divisible": [[25, 32], ["max", "int"], "function", ["None"], ["    ", "if", "checkpoint_path", "and", "os", ".", "path", ".", "isfile", "(", "checkpoint_path", ")", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "checkpoint_path", ",", "map_location", "=", "'cpu'", ")", "\n", "state_dict_key", "=", "''", "\n", "if", "isinstance", "(", "checkpoint", ",", "dict", ")", ":", "\n", "            ", "if", "use_ema", "and", "checkpoint", ".", "get", "(", "'state_dict_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'state_dict_ema'", "\n", "", "elif", "use_ema", "and", "checkpoint", ".", "get", "(", "'model_ema'", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "state_dict_key", "=", "'model_ema'", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConvBnAct.__init__": [[17, 31], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_conv2d.create_conv2d.create_conv2d", "create_norm_act.convert_norm_act", "create_norm_act.convert_norm_act.", "int", "int"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.convert_norm_act"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "bias", "=", "False", ",", "\n", "channel_multiplier", "=", "1.0", ",", "pw_kernel_size", "=", "1", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "\n", "apply_act", "=", "True", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "SeparableConvBnAct", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_channels", ",", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "padding", "=", "padding", ",", "depthwise", "=", "True", ")", "\n", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "\n", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "out_channels", ",", "pw_kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "bias", ")", "\n", "\n", "norm_act_layer", "=", "convert_norm_act", "(", "norm_layer", ",", "act_layer", ")", "\n", "self", ".", "bn", "=", "norm_act_layer", "(", "out_channels", ",", "apply_act", "=", "apply_act", ",", "drop_block", "=", "drop_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConvBnAct.in_channels": [[32, 35], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_dw", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConvBnAct.out_channels": [[36, 39], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_pw", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConvBnAct.forward": [[40, 46], ["separable_conv.SeparableConvBnAct.conv_dw", "separable_conv.SeparableConvBnAct.conv_pw", "separable_conv.SeparableConvBnAct.bn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "if", "self", ".", "bn", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConv2d.__init__": [[51, 61], ["torch.nn.Module.__init__", "create_conv2d.create_conv2d.create_conv2d", "create_conv2d.create_conv2d.create_conv2d", "int", "int"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "padding", "=", "''", ",", "bias", "=", "False", ",", "\n", "channel_multiplier", "=", "1.0", ",", "pw_kernel_size", "=", "1", ")", ":", "\n", "        ", "super", "(", "SeparableConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv_dw", "=", "create_conv2d", "(", "\n", "in_channels", ",", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "dilation", "=", "dilation", ",", "padding", "=", "padding", ",", "depthwise", "=", "True", ")", "\n", "\n", "self", ".", "conv_pw", "=", "create_conv2d", "(", "\n", "int", "(", "in_channels", "*", "channel_multiplier", ")", ",", "out_channels", ",", "pw_kernel_size", ",", "padding", "=", "padding", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConv2d.in_channels": [[62, 65], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_dw", ".", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConv2d.out_channels": [[66, 69], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_channels", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "conv_pw", ".", "out_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.separable_conv.SeparableConv2d.forward": [[70, 74], ["separable_conv.SeparableConv2d.conv_dw", "separable_conv.SeparableConv2d.conv_pw"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv_dw", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_pw", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init._no_grad_trunc_normal_": [[8, 42], ["warnings.warn", "torch.no_grad", "weight_init._no_grad_trunc_normal_.norm_cdf"], "function", ["None"], ["def", "_no_grad_trunc_normal_", "(", "tensor", ",", "mean", ",", "std", ",", "a", ",", "b", ")", ":", "\n", "# Cut & paste from PyTorch official master until it's in a few official releases - RW", "\n", "# Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf", "\n", "    ", "def", "norm_cdf", "(", "x", ")", ":", "\n", "# Computes standard normal cumulative distribution function", "\n", "        ", "return", "(", "1.", "+", "math", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.", ")", ")", ")", "/", "2.", "\n", "\n", "", "if", "(", "mean", "<", "a", "-", "2", "*", "std", ")", "or", "(", "mean", ">", "b", "+", "2", "*", "std", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"", "\n", "\"The distribution of values may be incorrect.\"", ",", "\n", "stacklevel", "=", "2", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Values are generated by using a truncated uniform distribution and", "\n", "# then using the inverse CDF for the normal distribution.", "\n", "# Get upper and lower cdf values", "\n", "        ", "l", "=", "norm_cdf", "(", "(", "a", "-", "mean", ")", "/", "std", ")", "\n", "u", "=", "norm_cdf", "(", "(", "b", "-", "mean", ")", "/", "std", ")", "\n", "\n", "# Uniformly fill tensor with values from [l, u], then translate to", "\n", "# [2l-1, 2u-1].", "\n", "tensor", ".", "uniform_", "(", "2", "*", "l", "-", "1", ",", "2", "*", "u", "-", "1", ")", "\n", "\n", "# Use inverse cdf transform for normal distribution to get truncated", "\n", "# standard normal", "\n", "tensor", ".", "erfinv_", "(", ")", "\n", "\n", "# Transform to proper mean, std", "\n", "tensor", ".", "mul_", "(", "std", "*", "math", ".", "sqrt", "(", "2.", ")", ")", "\n", "tensor", ".", "add_", "(", "mean", ")", "\n", "\n", "# Clamp to ensure it's in the proper range", "\n", "tensor", ".", "clamp_", "(", "min", "=", "a", ",", "max", "=", "b", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_": [[44, 63], ["weight_init._no_grad_trunc_normal_"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init._no_grad_trunc_normal_"], ["", "", "def", "trunc_normal_", "(", "tensor", ",", "mean", "=", "0.", ",", "std", "=", "1.", ",", "a", "=", "-", "2.", ",", "b", "=", "2.", ")", ":", "\n", "# type: (Tensor, float, float, float, float) -> Tensor", "\n", "    ", "r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"", "\n", "return", "_no_grad_trunc_normal_", "(", "tensor", ",", "mean", ",", "std", ",", "a", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.variance_scaling_": [[65, 86], ["torch.nn.init._calculate_fan_in_and_fan_out", "weight_init.trunc_normal_", "tensor.normal_", "math.sqrt", "tensor.uniform_", "ValueError", "math.sqrt", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["", "def", "variance_scaling_", "(", "tensor", ",", "scale", "=", "1.0", ",", "mode", "=", "'fan_in'", ",", "distribution", "=", "'normal'", ")", ":", "\n", "    ", "fan_in", ",", "fan_out", "=", "_calculate_fan_in_and_fan_out", "(", "tensor", ")", "\n", "if", "mode", "==", "'fan_in'", ":", "\n", "        ", "denom", "=", "fan_in", "\n", "", "elif", "mode", "==", "'fan_out'", ":", "\n", "        ", "denom", "=", "fan_out", "\n", "", "elif", "mode", "==", "'fan_avg'", ":", "\n", "        ", "denom", "=", "(", "fan_in", "+", "fan_out", ")", "/", "2", "\n", "\n", "", "variance", "=", "scale", "/", "denom", "\n", "\n", "if", "distribution", "==", "\"truncated_normal\"", ":", "\n", "# constant is stddev of standard normal truncated to (-2, 2)", "\n", "        ", "trunc_normal_", "(", "tensor", ",", "std", "=", "math", ".", "sqrt", "(", "variance", ")", "/", ".87962566103423978", ")", "\n", "", "elif", "distribution", "==", "\"normal\"", ":", "\n", "        ", "tensor", ".", "normal_", "(", "std", "=", "math", ".", "sqrt", "(", "variance", ")", ")", "\n", "", "elif", "distribution", "==", "\"uniform\"", ":", "\n", "        ", "bound", "=", "math", ".", "sqrt", "(", "3", "*", "variance", ")", "\n", "tensor", ".", "uniform_", "(", "-", "bound", ",", "bound", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"invalid distribution {distribution}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.lecun_normal_": [[88, 90], ["weight_init.variance_scaling_"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.variance_scaling_"], ["", "", "def", "lecun_normal_", "(", "tensor", ")", ":", "\n", "    ", "variance_scaling_", "(", "tensor", ",", "mode", "=", "'fan_in'", ",", "distribution", "=", "'truncated_normal'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.lambda_layer.LambdaLayer.__init__": [[67, 101], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "torch.nn.BatchNorm2d", "lambda_layer.LambdaLayer.reset_parameters", "torch.nn.Conv3d", "torch.nn.Conv3d", "helpers.to_2tuple", "torch.nn.Parameter", "torch.nn.Parameter", "lambda_layer.LambdaLayer.register_buffer", "torch.nn.AvgPool2d", "torch.nn.AvgPool2d", "torch.nn.Identity", "torch.nn.Identity", "helpers.make_divisible", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "lambda_layer.rel_pos_indices"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.lambda_layer.rel_pos_indices"], ["def", "__init__", "(", "\n", "self", ",", "dim", ",", "dim_out", "=", "None", ",", "feat_size", "=", "None", ",", "stride", "=", "1", ",", "num_heads", "=", "4", ",", "dim_head", "=", "16", ",", "r", "=", "9", ",", "\n", "qk_ratio", "=", "1.0", ",", "qkv_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "dim_out", "=", "dim_out", "or", "dim", "\n", "assert", "dim_out", "%", "num_heads", "==", "0", ",", "' should be divided by num_heads'", "\n", "self", ".", "dim_qk", "=", "dim_head", "or", "make_divisible", "(", "dim_out", "*", "qk_ratio", ",", "divisor", "=", "8", ")", "//", "num_heads", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dim_v", "=", "dim_out", "//", "num_heads", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Conv2d", "(", "\n", "dim", ",", "\n", "num_heads", "*", "self", ".", "dim_qk", "+", "self", ".", "dim_qk", "+", "self", ".", "dim_v", ",", "\n", "kernel_size", "=", "1", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "norm_q", "=", "nn", ".", "BatchNorm2d", "(", "num_heads", "*", "self", ".", "dim_qk", ")", "\n", "self", ".", "norm_v", "=", "nn", ".", "BatchNorm2d", "(", "self", ".", "dim_v", ")", "\n", "\n", "if", "r", "is", "not", "None", ":", "\n", "# local lambda convolution for pos", "\n", "            ", "self", ".", "conv_lambda", "=", "nn", ".", "Conv3d", "(", "1", ",", "self", ".", "dim_qk", ",", "(", "r", ",", "r", ",", "1", ")", ",", "padding", "=", "(", "r", "//", "2", ",", "r", "//", "2", ",", "0", ")", ")", "\n", "self", ".", "pos_emb", "=", "None", "\n", "self", ".", "rel_pos_indices", "=", "None", "\n", "", "else", ":", "\n", "# relative pos embedding", "\n", "            ", "assert", "feat_size", "is", "not", "None", "\n", "feat_size", "=", "to_2tuple", "(", "feat_size", ")", "\n", "rel_size", "=", "[", "2", "*", "s", "-", "1", "for", "s", "in", "feat_size", "]", "\n", "self", ".", "conv_lambda", "=", "None", "\n", "self", ".", "pos_emb", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "rel_size", "[", "0", "]", ",", "rel_size", "[", "1", "]", ",", "self", ".", "dim_qk", ")", ")", "\n", "self", ".", "register_buffer", "(", "'rel_pos_indices'", ",", "rel_pos_indices", "(", "feat_size", ")", ",", "persistent", "=", "False", ")", "\n", "\n", "", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "2", ",", "2", ")", "if", "stride", "==", "2", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.lambda_layer.LambdaLayer.reset_parameters": [[102, 108], ["weight_init.trunc_normal_", "weight_init.trunc_normal_", "weight_init.trunc_normal_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "self", ".", "qkv", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "# fan-in", "\n", "if", "self", ".", "conv_lambda", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "conv_lambda", ".", "weight", ",", "std", "=", "self", ".", "dim_qk", "**", "-", "0.5", ")", "\n", "", "if", "self", ".", "pos_emb", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "pos_emb", ",", "std", "=", ".02", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.lambda_layer.LambdaLayer.forward": [[109, 134], ["lambda_layer.LambdaLayer.qkv", "torch.split", "torch.split", "torch.split", "torch.split", "lambda_layer.LambdaLayer.norm_q().reshape().transpose", "lambda_layer.LambdaLayer.norm_v().reshape().transpose", "torch.softmax", "torch.softmax", "lambda_layer.LambdaLayer.pool", "torch.softmax.reshape", "content_lam.unsqueeze", "lambda_layer.LambdaLayer.conv_lambda", "position_lam.reshape().transpose.reshape().transpose.reshape().transpose", "lambda_layer.LambdaLayer.pos_emb[].expand", "lambda_layer.LambdaLayer.norm_q().reshape", "lambda_layer.LambdaLayer.norm_v().reshape", "lambda_layer.LambdaLayer.reshape", "position_lam.reshape().transpose.reshape().transpose.reshape", "lambda_layer.LambdaLayer.unsqueeze", "lambda_layer.LambdaLayer.norm_q", "lambda_layer.LambdaLayer.norm_v", "lambda_layer.LambdaLayer.transpose", "lambda_layer.LambdaLayer.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "M", "=", "H", "*", "W", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", "\n", "q", ",", "k", ",", "v", "=", "torch", ".", "split", "(", "qkv", ",", "[", "\n", "self", ".", "num_heads", "*", "self", ".", "dim_qk", ",", "self", ".", "dim_qk", ",", "self", ".", "dim_v", "]", ",", "dim", "=", "1", ")", "\n", "q", "=", "self", ".", "norm_q", "(", "q", ")", ".", "reshape", "(", "B", ",", "self", ".", "num_heads", ",", "self", ".", "dim_qk", ",", "M", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "# B, num_heads, M, K", "\n", "v", "=", "self", ".", "norm_v", "(", "v", ")", ".", "reshape", "(", "B", ",", "self", ".", "dim_v", ",", "M", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "# B, M, V", "\n", "k", "=", "F", ".", "softmax", "(", "k", ".", "reshape", "(", "B", ",", "self", ".", "dim_qk", ",", "M", ")", ",", "dim", "=", "-", "1", ")", "# B, K, M", "\n", "\n", "content_lam", "=", "k", "@", "v", "# B, K, V", "\n", "content_out", "=", "q", "@", "content_lam", ".", "unsqueeze", "(", "1", ")", "# B, num_heads, M, V", "\n", "\n", "if", "self", ".", "pos_emb", "is", "None", ":", "\n", "            ", "position_lam", "=", "self", ".", "conv_lambda", "(", "v", ".", "reshape", "(", "B", ",", "1", ",", "H", ",", "W", ",", "self", ".", "dim_v", ")", ")", "# B, H, W, V, K", "\n", "position_lam", "=", "position_lam", ".", "reshape", "(", "B", ",", "1", ",", "self", ".", "dim_qk", ",", "H", "*", "W", ",", "self", ".", "dim_v", ")", ".", "transpose", "(", "2", ",", "3", ")", "# B, 1, M, K, V", "\n", "", "else", ":", "\n", "# FIXME relative pos embedding path not fully verified", "\n", "            ", "pos_emb", "=", "self", ".", "pos_emb", "[", "self", ".", "rel_pos_indices", "[", "0", "]", ",", "self", ".", "rel_pos_indices", "[", "1", "]", "]", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "position_lam", "=", "(", "pos_emb", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "@", "v", ".", "unsqueeze", "(", "1", ")", ")", ".", "unsqueeze", "(", "1", ")", "# B, 1, M, K, V", "\n", "", "position_out", "=", "(", "q", ".", "unsqueeze", "(", "-", "2", ")", "@", "position_lam", ")", ".", "squeeze", "(", "-", "2", ")", "# B, num_heads, M, V", "\n", "\n", "out", "=", "(", "content_out", "+", "position_out", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "# B, C (num_heads * V), H, W", "\n", "out", "=", "self", ".", "pool", "(", "out", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.lambda_layer.rel_pos_indices": [[31, 38], ["helpers.to_2tuple", "torch.stack().flatten", "torch.stack().flatten", "torch.stack", "torch.stack", "torch.meshgrid", "torch.meshgrid", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["def", "rel_pos_indices", "(", "size", ")", ":", "\n", "    ", "size", "=", "to_2tuple", "(", "size", ")", "\n", "pos", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "torch", ".", "arange", "(", "size", "[", "0", "]", ")", ",", "torch", ".", "arange", "(", "size", "[", "1", "]", ")", ")", ")", ".", "flatten", "(", "1", ")", "\n", "rel_pos", "=", "pos", "[", ":", ",", "None", ",", ":", "]", "-", "pos", "[", ":", ",", ":", ",", "None", "]", "\n", "rel_pos", "[", "0", "]", "+=", "size", "[", "0", "]", "-", "1", "\n", "rel_pos", "[", "1", "]", "+=", "size", "[", "1", "]", "-", "1", "\n", "return", "rel_pos", "# 2, H * W, H * W", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotaryEmbedding.__init__": [[44, 48], ["torch.Module.__init__", "attention_pool2d.RotaryEmbedding.register_buffer", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "dim", ",", "max_freq", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "register_buffer", "(", "'bands'", ",", "2", "**", "torch", ".", "linspace", "(", "0.", ",", "max_freq", "-", "1", ",", "self", ".", "dim", "//", "4", ")", ",", "persistent", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotaryEmbedding.get_embed": [[49, 64], ["torch.Size.numel", "torch.Size.numel", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "emb.sin().reshape().repeat_interleave", "emb.cos().reshape().repeat_interleave", "isinstance", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "emb.sin().reshape", "emb.cos().reshape", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "torch.meshgrid", "emb.sin", "emb.cos", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["None"], ["", "def", "get_embed", "(", "self", ",", "shape", ":", "torch", ".", "Size", ",", "device", ":", "torch", ".", "device", "=", "None", ",", "dtype", ":", "torch", ".", "dtype", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        NOTE: shape arg should include spatial dim only\n        \"\"\"", "\n", "device", "=", "device", "or", "self", ".", "bands", ".", "device", "\n", "dtype", "=", "dtype", "or", "self", ".", "bands", ".", "dtype", "\n", "if", "not", "isinstance", "(", "shape", ",", "torch", ".", "Size", ")", ":", "\n", "            ", "shape", "=", "torch", ".", "Size", "(", "shape", ")", "\n", "", "N", "=", "shape", ".", "numel", "(", ")", "\n", "grid", "=", "torch", ".", "stack", "(", "torch", ".", "meshgrid", "(", "\n", "[", "torch", ".", "linspace", "(", "-", "1.", ",", "1.", ",", "steps", "=", "s", ",", "device", "=", "device", ",", "dtype", "=", "dtype", ")", "for", "s", "in", "shape", "]", ")", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "emb", "=", "grid", "*", "math", ".", "pi", "*", "self", ".", "bands", "\n", "sin", "=", "emb", ".", "sin", "(", ")", ".", "reshape", "(", "N", ",", "-", "1", ")", ".", "repeat_interleave", "(", "2", ",", "-", "1", ")", "\n", "cos", "=", "emb", ".", "cos", "(", ")", ".", "reshape", "(", "N", ",", "-", "1", ")", ".", "repeat_interleave", "(", "2", ",", "-", "1", ")", "\n", "return", "sin", ",", "cos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotaryEmbedding.forward": [[65, 69], ["attention_pool2d.RotaryEmbedding.get_embed", "attention_pool2d.apply_rot_embed"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotaryEmbedding.get_embed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.apply_rot_embed"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# assuming channel-first tensor where spatial dim are >= 2", "\n", "        ", "sin_emb", ",", "cos_emb", "=", "self", ".", "get_embed", "(", "x", ".", "shape", "[", "2", ":", "]", ")", "\n", "return", "apply_rot_embed", "(", "x", ",", "sin_emb", ",", "cos_emb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotAttentionPool2d.__init__": [[81, 102], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "attention_pool2d.RotaryEmbedding", "weight_init.trunc_normal_", "torch.init.zeros_", "torch.init.zeros_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "out_features", ":", "int", "=", "None", ",", "\n", "embed_dim", ":", "int", "=", "None", ",", "\n", "num_heads", ":", "int", "=", "4", ",", "\n", "qkv_bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "embed_dim", "=", "embed_dim", "or", "in_features", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "in_features", ",", "embed_dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "out_features", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "assert", "embed_dim", "%", "num_heads", "==", "0", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "self", ".", "pos_embed", "=", "RotaryEmbedding", "(", "self", ".", "head_dim", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "qkv", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotAttentionPool2d.forward": [[103, 128], ["attention_pool2d.RotAttentionPool2d.pos_embed.get_embed", "attention_pool2d.RotAttentionPool2d.reshape().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention_pool2d.RotAttentionPool2d.qkv().reshape().permute", "attention_pool2d.apply_rot_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention_pool2d.apply_rot_embed", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn.softmax.softmax.softmax", "attention_pool2d.RotAttentionPool2d.proj", "attention_pool2d.RotAttentionPool2d.reshape", "attention_pool2d.RotAttentionPool2d.mean", "attention_pool2d.RotAttentionPool2d.qkv().reshape", "torch.cat.transpose", "torch.cat.transpose", "attention_pool2d.RotAttentionPool2d.qkv"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.RotaryEmbedding.get_embed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.apply_rot_embed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.apply_rot_embed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "N", "=", "H", "*", "W", "\n", "sin_emb", ",", "cos_emb", "=", "self", ".", "pos_embed", ".", "get_embed", "(", "x", ".", "shape", "[", "2", ":", "]", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "N", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "3", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "\n", "\n", "qc", ",", "q", "=", "q", "[", ":", ",", ":", ",", ":", "1", "]", ",", "q", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "q", "=", "apply_rot_embed", "(", "q", ",", "sin_emb", ",", "cos_emb", ")", "\n", "q", "=", "torch", ".", "cat", "(", "[", "qc", ",", "q", "]", ",", "dim", "=", "2", ")", "\n", "\n", "kc", ",", "k", "=", "k", "[", ":", ",", ":", ",", ":", "1", "]", ",", "k", "[", ":", ",", ":", ",", "1", ":", "]", "\n", "k", "=", "apply_rot_embed", "(", "k", ",", "sin_emb", ",", "cos_emb", ")", "\n", "k", "=", "torch", ".", "cat", "(", "[", "kc", ",", "k", "]", ",", "dim", "=", "2", ")", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.AttentionPool2d.__init__": [[139, 165], ["torch.Module.__init__", "helpers.to_2tuple", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "weight_init.trunc_normal_", "weight_init.trunc_normal_", "torch.init.zeros_", "torch.init.zeros_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_features", ":", "int", ",", "\n", "feat_size", ":", "Union", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "out_features", ":", "int", "=", "None", ",", "\n", "embed_dim", ":", "int", "=", "None", ",", "\n", "num_heads", ":", "int", "=", "4", ",", "\n", "qkv_bias", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "embed_dim", "=", "embed_dim", "or", "in_features", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "assert", "embed_dim", "%", "num_heads", "==", "0", "\n", "self", ".", "feat_size", "=", "to_2tuple", "(", "feat_size", ")", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "in_features", ",", "embed_dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "out_features", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "spatial_dim", "=", "self", ".", "feat_size", "[", "0", "]", "*", "self", ".", "feat_size", "[", "1", "]", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "spatial_dim", "+", "1", ",", "in_features", ")", ")", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "trunc_normal_", "(", "self", ".", "qkv", ".", "weight", ",", "std", "=", "in_features", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "qkv", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.AttentionPool2d.forward": [[166, 183], ["attention_pool2d.AttentionPool2d.reshape().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attention_pool2d.AttentionPool2d.qkv().reshape().permute", "attn.softmax.softmax.softmax", "attention_pool2d.AttentionPool2d.proj", "attention_pool2d.AttentionPool2d.pos_embed.unsqueeze().to", "attention_pool2d.AttentionPool2d.reshape", "attention_pool2d.AttentionPool2d.mean", "attention_pool2d.AttentionPool2d.qkv().reshape", "k.transpose", "attention_pool2d.AttentionPool2d.pos_embed.unsqueeze", "attention_pool2d.AttentionPool2d.qkv"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "N", "=", "H", "*", "W", "\n", "assert", "self", ".", "feat_size", "[", "0", "]", "==", "H", "\n", "assert", "self", ".", "feat_size", "[", "1", "]", "==", "W", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "-", "1", ",", "N", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", ",", "x", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", "+", "self", ".", "pos_embed", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "x", ".", "dtype", ")", "\n", "\n", "x", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "3", ",", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", ",", "x", "[", "2", "]", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", "+", "1", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "return", "x", "[", ":", ",", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.rot": [[20, 22], ["torch.stack().reshape", "torch.stack().reshape", "torch.stack", "torch.stack"], "function", ["None"], ["def", "rot", "(", "x", ")", ":", "\n", "    ", "return", "torch", ".", "stack", "(", "[", "-", "x", "[", "...", ",", "1", ":", ":", "2", "]", ",", "x", "[", "...", ",", ":", ":", "2", "]", "]", ",", "-", "1", ")", ".", "reshape", "(", "x", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.apply_rot_embed": [[24, 26], ["attention_pool2d.rot"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.rot"], ["", "def", "apply_rot_embed", "(", "x", ":", "torch", ".", "Tensor", ",", "sin_emb", ",", "cos_emb", ")", ":", "\n", "    ", "return", "x", "*", "cos_emb", "+", "rot", "(", "x", ")", "*", "sin_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.apply_rot_embed_list": [[28, 32], ["isinstance", "attention_pool2d.rot"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.attention_pool2d.rot"], ["", "def", "apply_rot_embed_list", "(", "x", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "sin_emb", ",", "cos_emb", ")", ":", "\n", "    ", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "[", "x", "]", "\n", "", "return", "[", "t", "*", "cos_emb", "+", "rot", "(", "t", ")", "*", "sin_emb", "for", "t", "in", "x", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding": [[12, 15], ["None"], "function", ["None"], ["def", "get_padding", "(", "kernel_size", ":", "int", ",", "stride", ":", "int", "=", "1", ",", "dilation", ":", "int", "=", "1", ",", "**", "_", ")", "->", "int", ":", "\n", "    ", "padding", "=", "(", "(", "stride", "-", "1", ")", "+", "dilation", "*", "(", "kernel_size", "-", "1", ")", ")", "//", "2", "\n", "return", "padding", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_same_padding": [[18, 20], ["max", "math.ceil"], "function", ["None"], ["", "def", "get_same_padding", "(", "x", ":", "int", ",", "k", ":", "int", ",", "s", ":", "int", ",", "d", ":", "int", ")", ":", "\n", "    ", "return", "max", "(", "(", "math", ".", "ceil", "(", "x", "/", "s", ")", "-", "1", ")", "*", "s", "+", "(", "k", "-", "1", ")", "*", "d", "+", "1", "-", "x", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.is_static_pad": [[23, 25], ["None"], "function", ["None"], ["", "def", "is_static_pad", "(", "kernel_size", ":", "int", ",", "stride", ":", "int", "=", "1", ",", "dilation", ":", "int", "=", "1", ",", "**", "_", ")", ":", "\n", "    ", "return", "stride", "==", "1", "and", "(", "dilation", "*", "(", "kernel_size", "-", "1", ")", ")", "%", "2", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.pad_same": [[28, 34], ["F.pad.size", "padding.get_same_padding", "padding.get_same_padding", "torch.pad"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_same_padding", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_same_padding"], ["", "def", "pad_same", "(", "x", ",", "k", ":", "List", "[", "int", "]", ",", "s", ":", "List", "[", "int", "]", ",", "d", ":", "List", "[", "int", "]", "=", "(", "1", ",", "1", ")", ",", "value", ":", "float", "=", "0", ")", ":", "\n", "    ", "ih", ",", "iw", "=", "x", ".", "size", "(", ")", "[", "-", "2", ":", "]", "\n", "pad_h", ",", "pad_w", "=", "get_same_padding", "(", "ih", ",", "k", "[", "0", "]", ",", "s", "[", "0", "]", ",", "d", "[", "0", "]", ")", ",", "get_same_padding", "(", "iw", ",", "k", "[", "1", "]", ",", "s", "[", "1", "]", ",", "d", "[", "1", "]", ")", "\n", "if", "pad_h", ">", "0", "or", "pad_w", ">", "0", ":", "\n", "        ", "x", "=", "F", ".", "pad", "(", "x", ",", "[", "pad_w", "//", "2", ",", "pad_w", "-", "pad_w", "//", "2", ",", "pad_h", "//", "2", ",", "pad_h", "-", "pad_h", "//", "2", "]", ",", "value", "=", "value", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value": [[36, 57], ["isinstance", "get_padding.lower", "padding.is_static_pad", "padding.get_padding", "padding.get_padding"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.is_static_pad", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding"], ["", "def", "get_padding_value", "(", "padding", ",", "kernel_size", ",", "**", "kwargs", ")", "->", "Tuple", "[", "Tuple", ",", "bool", "]", ":", "\n", "    ", "dynamic", "=", "False", "\n", "if", "isinstance", "(", "padding", ",", "str", ")", ":", "\n", "# for any string padding, the padding will be calculated for you, one of three ways", "\n", "        ", "padding", "=", "padding", ".", "lower", "(", ")", "\n", "if", "padding", "==", "'same'", ":", "\n", "# TF compatible 'SAME' padding, has a performance and GPU memory allocation impact", "\n", "            ", "if", "is_static_pad", "(", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "# static case, no extra overhead", "\n", "                ", "padding", "=", "get_padding", "(", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# dynamic 'SAME' padding, has runtime/GPU memory overhead", "\n", "                ", "padding", "=", "0", "\n", "dynamic", "=", "True", "\n", "", "", "elif", "padding", "==", "'valid'", ":", "\n", "# 'VALID' padding, same as padding=0", "\n", "            ", "padding", "=", "0", "\n", "", "else", ":", "\n", "# Default to PyTorch style 'same'-ish symmetric padding", "\n", "            ", "padding", "=", "get_padding", "(", "kernel_size", ",", "**", "kwargs", ")", "\n", "", "", "return", "padding", ",", "dynamic", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.NonLocalAttn.__init__": [[22, 33], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.BatchNorm2d", "non_local_attn.NonLocalAttn.reset_parameters", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "use_scale", "=", "True", ",", "rd_ratio", "=", "1", "/", "8", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "NonLocalAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "in_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "self", ".", "scale", "=", "in_channels", "**", "-", "0.5", "if", "use_scale", "else", "1.0", "\n", "self", ".", "t", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "p", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "g", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "z", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "in_channels", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "norm", "=", "nn", ".", "BatchNorm2d", "(", "in_channels", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.NonLocalAttn.forward": [[34, 55], ["non_local_attn.NonLocalAttn.t", "non_local_attn.NonLocalAttn.p", "non_local_attn.NonLocalAttn.g", "t.view().permute.view().permute.size", "t.view().permute.view().permute.view().permute", "p.view.view.view", "g.view().permute.view().permute.view().permute", "torch.nn.functional.softmax", "torch.bmm", "non_local_attn.NonLocalAttn.permute().reshape", "non_local_attn.NonLocalAttn.z", "torch.bmm", "non_local_attn.NonLocalAttn.norm", "t.view().permute.view().permute.view", "g.view().permute.view().permute.view", "non_local_attn.NonLocalAttn.permute"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "shortcut", "=", "x", "\n", "\n", "t", "=", "self", ".", "t", "(", "x", ")", "\n", "p", "=", "self", ".", "p", "(", "x", ")", "\n", "g", "=", "self", ".", "g", "(", "x", ")", "\n", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "t", ".", "size", "(", ")", "\n", "t", "=", "t", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", "\n", "g", "=", "g", ".", "view", "(", "B", ",", "C", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "att", "=", "torch", ".", "bmm", "(", "t", ",", "p", ")", "*", "self", ".", "scale", "\n", "att", "=", "F", ".", "softmax", "(", "att", ",", "dim", "=", "2", ")", "\n", "x", "=", "torch", ".", "bmm", "(", "att", ",", "g", ")", "\n", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "x", "=", "self", ".", "z", "(", "x", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "+", "shortcut", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.NonLocalAttn.reset_parameters": [[56, 69], ["non_local_attn.NonLocalAttn.named_modules", "isinstance", "torch.nn.init.kaiming_normal_", "isinstance", "len", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "isinstance", "list", "torch.nn.init.constant_", "torch.nn.init.constant_", "m.parameters"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "name", ",", "m", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "kaiming_normal_", "(", "\n", "m", ".", "weight", ",", "mode", "=", "'fan_out'", ",", "nonlinearity", "=", "'relu'", ")", "\n", "if", "len", "(", "list", "(", "m", ".", "parameters", "(", ")", ")", ")", ">", "1", ":", "\n", "                    ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "GroupNorm", ")", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BilinearAttnTransform.__init__": [[73, 83], ["torch.nn.Module.__init__", "conv_bn_act.ConvBnAct", "torch.nn.Conv2d", "torch.nn.Conv2d", "conv_bn_act.ConvBnAct"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "block_size", ",", "groups", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "super", "(", "BilinearAttnTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv1", "=", "ConvBnAct", "(", "in_channels", ",", "groups", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv_p", "=", "nn", ".", "Conv2d", "(", "groups", ",", "block_size", "*", "block_size", "*", "groups", ",", "kernel_size", "=", "(", "block_size", ",", "1", ")", ")", "\n", "self", ".", "conv_q", "=", "nn", ".", "Conv2d", "(", "groups", ",", "block_size", "*", "block_size", "*", "groups", ",", "kernel_size", "=", "(", "1", ",", "block_size", ")", ")", "\n", "self", ".", "conv2", "=", "ConvBnAct", "(", "in_channels", ",", "in_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "block_size", "=", "block_size", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BilinearAttnTransform.resize_mat": [[84, 96], ["x.view.view.view", "x.view.view.view", "torch.cat", "torch.cat", "x.view.view.view", "torch.eye", "torch.split", "torch.split"], "methods", ["None"], ["", "def", "resize_mat", "(", "self", ",", "x", ",", "t", ":", "int", ")", ":", "\n", "        ", "B", ",", "C", ",", "block_size", ",", "block_size1", "=", "x", ".", "shape", "\n", "assert", "block_size", "==", "block_size1", "\n", "if", "t", "<=", "1", ":", "\n", "            ", "return", "x", "\n", "", "x", "=", "x", ".", "view", "(", "B", "*", "C", ",", "-", "1", ",", "1", ",", "1", ")", "\n", "x", "=", "x", "*", "torch", ".", "eye", "(", "t", ",", "t", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "x", "=", "x", ".", "view", "(", "B", "*", "C", ",", "block_size", ",", "block_size", ",", "t", ",", "t", ")", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "split", "(", "x", ",", "1", ",", "dim", "=", "1", ")", ",", "dim", "=", "3", ")", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "split", "(", "x", ",", "1", ",", "dim", "=", "2", ")", ",", "dim", "=", "4", ")", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "C", ",", "block_size", "*", "t", ",", "block_size", "*", "t", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BilinearAttnTransform.forward": [[97, 120], ["non_local_attn.BilinearAttnTransform.conv1", "torch.nn.functional.adaptive_max_pool2d", "torch.nn.functional.adaptive_max_pool2d", "non_local_attn.BilinearAttnTransform.conv_p().view().sigmoid", "non_local_attn.BilinearAttnTransform.conv_q().view().sigmoid", "non_local_attn.BilinearAttnTransform.view().expand().contiguous", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.view().expand().contiguous", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.resize_mat", "non_local_attn.BilinearAttnTransform.resize_mat", "non_local_attn.BilinearAttnTransform.matmul", "non_local_attn.BilinearAttnTransform.matmul", "non_local_attn.BilinearAttnTransform.conv2", "non_local_attn.BilinearAttnTransform.sum", "non_local_attn.BilinearAttnTransform.sum", "non_local_attn.BilinearAttnTransform.conv_p().view", "non_local_attn.BilinearAttnTransform.conv_q().view", "non_local_attn.BilinearAttnTransform.view().expand", "non_local_attn.BilinearAttnTransform.view().expand", "x.size", "x.size", "non_local_attn.BilinearAttnTransform.conv_p", "non_local_attn.BilinearAttnTransform.conv_q", "non_local_attn.BilinearAttnTransform.view", "non_local_attn.BilinearAttnTransform.view"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BilinearAttnTransform.resize_mat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BilinearAttnTransform.resize_mat"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "shape", "[", "-", "1", "]", "%", "self", ".", "block_size", "==", "0", "and", "x", ".", "shape", "[", "-", "2", "]", "%", "self", ".", "block_size", "==", "0", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "rp", "=", "F", ".", "adaptive_max_pool2d", "(", "out", ",", "(", "self", ".", "block_size", ",", "1", ")", ")", "\n", "cp", "=", "F", ".", "adaptive_max_pool2d", "(", "out", ",", "(", "1", ",", "self", ".", "block_size", ")", ")", "\n", "p", "=", "self", ".", "conv_p", "(", "rp", ")", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "sigmoid", "(", ")", "\n", "q", "=", "self", ".", "conv_q", "(", "cp", ")", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "sigmoid", "(", ")", "\n", "p", "=", "p", "/", "p", ".", "sum", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "q", "=", "q", "/", "q", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "expand", "(", "x", ".", "size", "(", "\n", "0", ")", ",", "self", ".", "groups", ",", "C", "//", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "contiguous", "(", ")", "\n", "p", "=", "p", ".", "view", "(", "B", ",", "C", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", "\n", "q", "=", "q", ".", "view", "(", "B", ",", "self", ".", "groups", ",", "1", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "expand", "(", "x", ".", "size", "(", "\n", "0", ")", ",", "self", ".", "groups", ",", "C", "//", "self", ".", "groups", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", ".", "contiguous", "(", ")", "\n", "q", "=", "q", ".", "view", "(", "B", ",", "C", ",", "self", ".", "block_size", ",", "self", ".", "block_size", ")", "\n", "p", "=", "self", ".", "resize_mat", "(", "p", ",", "H", "//", "self", ".", "block_size", ")", "\n", "q", "=", "self", ".", "resize_mat", "(", "q", ",", "W", "//", "self", ".", "block_size", ")", "\n", "y", "=", "p", ".", "matmul", "(", "x", ")", "\n", "y", "=", "y", ".", "matmul", "(", "q", ")", "\n", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BatNonLocalAttn.__init__": [[127, 137], ["torch.nn.Module.__init__", "conv_bn_act.ConvBnAct", "non_local_attn.BilinearAttnTransform", "conv_bn_act.ConvBnAct", "torch.nn.Dropout2d", "helpers.make_divisible"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "in_channels", ",", "block_size", "=", "7", ",", "groups", "=", "2", ",", "rd_ratio", "=", "0.25", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "\n", "drop_rate", "=", "0.2", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "rd_channels", "is", "None", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "in_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "", "self", ".", "conv1", "=", "ConvBnAct", "(", "in_channels", ",", "rd_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "ba", "=", "BilinearAttnTransform", "(", "rd_channels", ",", "block_size", ",", "groups", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "conv2", "=", "ConvBnAct", "(", "rd_channels", ",", "in_channels", ",", "1", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout2d", "(", "p", "=", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.non_local_attn.BatNonLocalAttn.forward": [[138, 144], ["non_local_attn.BatNonLocalAttn.conv1", "non_local_attn.BatNonLocalAttn.ba", "non_local_attn.BatNonLocalAttn.conv2", "non_local_attn.BatNonLocalAttn.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "xl", "=", "self", ".", "conv1", "(", "x", ")", "\n", "y", "=", "self", ".", "ba", "(", "xl", ")", "\n", "y", "=", "self", ".", "conv2", "(", "y", ")", "\n", "y", "=", "self", ".", "dropout", "(", "y", ")", "\n", "return", "y", "+", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Swish.__init__": [[21, 24], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Swish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Swish.forward": [[25, 27], ["activations.swish"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.swish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "swish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Mish.__init__": [[39, 41], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Mish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Mish.forward": [[42, 44], ["activations.mish"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.mish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "mish", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Sigmoid.__init__": [[52, 55], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Sigmoid", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Sigmoid.forward": [[56, 58], ["x.sigmoid_", "x.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "sigmoid_", "(", ")", "if", "self", ".", "inplace", "else", "x", ".", "sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Tanh.__init__": [[66, 69], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "Tanh", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.Tanh.forward": [[70, 72], ["x.tanh_", "x.tanh"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "tanh_", "(", ")", "if", "self", ".", "inplace", "else", "x", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardSwish.__init__": [[80, 83], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSwish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardSwish.forward": [[84, 86], ["activations.hard_swish"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_swish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_swish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardSigmoid.__init__": [[96, 99], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardSigmoid", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardSigmoid.forward": [[100, 102], ["activations.hard_sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_sigmoid", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardMish.__init__": [[116, 119], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "HardMish", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inplace", "=", "inplace", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.HardMish.forward": [[120, 122], ["activations.hard_mish"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_mish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "hard_mish", "(", "x", ",", "self", ".", "inplace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.PReLU.__init__": [[127, 129], ["torch.nn.PReLU.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_parameters", ":", "int", "=", "1", ",", "init", ":", "float", "=", "0.25", ",", "inplace", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", "PReLU", ",", "self", ")", ".", "__init__", "(", "num_parameters", "=", "num_parameters", ",", "init", "=", "init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.PReLU.forward": [[130, 132], ["torch.nn.functional.prelu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "prelu", "(", "input", ",", "self", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.GELU.__init__": [[141, 143], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "super", "(", "GELU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.GELU.forward": [[144, 146], ["torch.nn.functional.gelu"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.gelu"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "F", ".", "gelu", "(", "input", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.swish": [[14, 18], ["x.mul_", "x.mul", "x.sigmoid", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["def", "swish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Swish - Described in: https://arxiv.org/abs/1710.05941\n    \"\"\"", "\n", "return", "x", ".", "mul_", "(", "x", ".", "sigmoid", "(", ")", ")", "if", "inplace", "else", "x", ".", "mul", "(", "x", ".", "sigmoid", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.mish": [[29, 34], ["x.mul", "torch.nn.functional.softplus().tanh", "torch.nn.functional.softplus"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "", "def", "mish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    NOTE: I don't have a working inplace variant\n    \"\"\"", "\n", "return", "x", ".", "mul", "(", "F", ".", "softplus", "(", "x", ")", ".", "tanh", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid": [[46, 48], ["x.sigmoid_", "x.sigmoid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "", "def", "sigmoid", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "x", ".", "sigmoid_", "(", ")", "if", "inplace", "else", "x", ".", "sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh": [[60, 62], ["x.tanh_", "x.tanh"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.tanh"], ["", "", "def", "tanh", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "return", "x", ".", "tanh_", "(", ")", "if", "inplace", "else", "x", ".", "tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_swish": [[74, 77], ["torch.nn.functional.relu6().div_", "x.mul_", "x.mul", "torch.nn.functional.relu6"], "function", ["None"], ["", "", "def", "hard_swish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "inner", "=", "F", ".", "relu6", "(", "x", "+", "3.", ")", ".", "div_", "(", "6.", ")", "\n", "return", "x", ".", "mul_", "(", "inner", ")", "if", "inplace", "else", "x", ".", "mul", "(", "inner", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_sigmoid": [[88, 93], ["x.add_().clamp_().div_", "torch.nn.functional.relu6", "x.add_().clamp_", "x.add_"], "function", ["None"], ["", "", "def", "hard_sigmoid", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "if", "inplace", ":", "\n", "        ", "return", "x", ".", "add_", "(", "3.", ")", ".", "clamp_", "(", "0.", ",", "6.", ")", ".", "div_", "(", "6.", ")", "\n", "", "else", ":", "\n", "        ", "return", "F", ".", "relu6", "(", "x", "+", "3.", ")", "/", "6.", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.hard_mish": [[104, 113], ["x.mul_"], "function", ["None"], ["", "", "def", "hard_mish", "(", "x", ",", "inplace", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" Hard Mish\n    Experimental, based on notes by Mish author Diganta Misra at\n      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md\n    \"\"\"", "\n", "if", "inplace", ":", "\n", "        ", "return", "x", ".", "mul_", "(", "0.5", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "0.5", "*", "x", "*", "(", "x", "+", "2", ")", ".", "clamp", "(", "min", "=", "0", ",", "max", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.gelu": [[134, 136], ["torch.nn.functional.gelu"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.gelu"], ["", "", "def", "gelu", "(", "x", ":", "torch", ".", "Tensor", ",", "inplace", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "return", "F", ".", "gelu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.linear.Linear.forward": [[14, 20], ["torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "linear.Linear.bias.to", "linear.Linear.weight.to"], "methods", ["None"], ["def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "bias", "=", "self", ".", "bias", ".", "to", "(", "dtype", "=", "input", ".", "dtype", ")", "if", "self", ".", "bias", "is", "not", "None", "else", "None", "\n", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ".", "to", "(", "dtype", "=", "input", ".", "dtype", ")", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ",", "self", ".", "bias", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormBatch2d.__init__": [[17, 29], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNormBatch2d.register_buffer", "evo_norm.EvoNormBatch2d.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "momentum", "=", "0.1", ",", "eps", "=", "1e-5", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "EvoNormBatch2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "momentum", "=", "momentum", "\n", "self", ".", "eps", "=", "eps", "\n", "param_shape", "=", "(", "1", ",", "num_features", ",", "1", ",", "1", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "if", "apply_act", ":", "\n", "            ", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "1", ",", "num_features", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormBatch2d.reset_parameters": [[30, 35], ["torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.ones_", "torch.init.ones_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormBatch2d.forward": [[36, 53], ["x.dim", "x.var", "evo_norm.EvoNormBatch2d.running_var.copy_", "evo_norm.EvoNormBatch2d.v.to", "d.max.max.max", "x.numel", "x.var.detach", "x.var"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", "\n", "x_type", "=", "x", ".", "dtype", "\n", "if", "self", ".", "training", ":", "\n", "            ", "var", "=", "x", ".", "var", "(", "dim", "=", "(", "0", ",", "2", ",", "3", ")", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "\n", "n", "=", "x", ".", "numel", "(", ")", "/", "x", ".", "shape", "[", "1", "]", "\n", "self", ".", "running_var", ".", "copy_", "(", "\n", "var", ".", "detach", "(", ")", "*", "self", ".", "momentum", "*", "(", "n", "/", "(", "n", "-", "1", ")", ")", "+", "self", ".", "running_var", "*", "(", "1", "-", "self", ".", "momentum", ")", ")", "\n", "", "else", ":", "\n", "            ", "var", "=", "self", ".", "running_var", "\n", "\n", "", "if", "self", ".", "apply_act", ":", "\n", "            ", "v", "=", "self", ".", "v", ".", "to", "(", "dtype", "=", "x_type", ")", "\n", "d", "=", "x", "*", "v", "+", "(", "x", ".", "var", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "+", "self", ".", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "dtype", "=", "x_type", ")", "\n", "d", "=", "d", ".", "max", "(", "(", "var", "+", "self", ".", "eps", ")", ".", "sqrt", "(", ")", ".", "to", "(", "dtype", "=", "x_type", ")", ")", "\n", "x", "=", "x", "/", "d", "\n", "", "return", "x", "*", "self", ".", "weight", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormSample2d.__init__": [[56, 67], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "evo_norm.EvoNormSample2d.reset_parameters", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ",", "apply_act", "=", "True", ",", "groups", "=", "8", ",", "eps", "=", "1e-5", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "EvoNormSample2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "apply_act", "=", "apply_act", "# apply activation (non-linearity)", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "eps", "=", "eps", "\n", "param_shape", "=", "(", "1", ",", "num_features", ",", "1", ",", "1", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "if", "apply_act", ":", "\n", "            ", "self", ".", "v", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "param_shape", ")", ",", "requires_grad", "=", "True", ")", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormSample2d.reset_parameters": [[68, 73], ["torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.ones_", "torch.init.ones_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "nn", ".", "init", ".", "ones_", "(", "self", ".", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.evo_norm.EvoNormSample2d.forward": [[74, 84], ["x.reshape.reshape.dim", "x.reshape.reshape.reshape", "x.reshape.reshape.reshape", "n.reshape", "x.reshape.reshape.var"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "dim", "(", ")", "==", "4", ",", "'expected 4D input'", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "assert", "C", "%", "self", ".", "groups", "==", "0", "\n", "if", "self", ".", "apply_act", ":", "\n", "            ", "n", "=", "x", "*", "(", "x", "*", "self", ".", "v", ")", ".", "sigmoid", "(", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "self", ".", "groups", ",", "-", "1", ")", "\n", "x", "=", "n", ".", "reshape", "(", "B", ",", "self", ".", "groups", ",", "-", "1", ")", "/", "(", "x", ".", "var", "(", "dim", "=", "-", "1", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "+", "self", ".", "eps", ")", ".", "sqrt", "(", ")", "\n", "x", "=", "x", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "", "return", "x", "*", "self", ".", "weight", "+", "self", ".", "bias", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.squeeze_excite.SEModule.__init__": [[28, 40], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "create_act.create_act_layer", "torch.nn.Conv2d", "create_act.create_act_layer", "helpers.make_divisible", "norm_layer", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "add_maxpool", "=", "False", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "None", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "SEModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "fc1", "=", "nn", ".", "Conv2d", "(", "channels", ",", "rd_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "rd_channels", ")", "if", "norm_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "act", "=", "create_act_layer", "(", "act_layer", ",", "inplace", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv2d", "(", "rd_channels", ",", "channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "True", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.squeeze_excite.SEModule.forward": [[41, 50], ["x.mean", "squeeze_excite.SEModule.fc1", "squeeze_excite.SEModule.act", "squeeze_excite.SEModule.fc2", "squeeze_excite.SEModule.bn", "squeeze_excite.SEModule.gate", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_se", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "            ", "x_se", "=", "0.5", "*", "x_se", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "x_se", "=", "self", ".", "fc1", "(", "x_se", ")", "\n", "x_se", "=", "self", ".", "act", "(", "self", ".", "bn", "(", "x_se", ")", ")", "\n", "x_se", "=", "self", ".", "fc2", "(", "x_se", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_se", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.squeeze_excite.EffectiveSEModule.__init__": [[59, 64], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "create_act.create_act_layer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer"], ["def", "__init__", "(", "self", ",", "channels", ",", "add_maxpool", "=", "False", ",", "gate_layer", "=", "'hard_sigmoid'", ",", "**", "_", ")", ":", "\n", "        ", "super", "(", "EffectiveSEModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "self", ".", "fc", "=", "nn", ".", "Conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.squeeze_excite.EffectiveSEModule.forward": [[65, 72], ["x.mean", "squeeze_excite.EffectiveSEModule.fc", "squeeze_excite.EffectiveSEModule.gate", "x.amax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x_se", "=", "x", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "            ", "x_se", "=", "0.5", "*", "x_se", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "x_se", "=", "self", ".", "fc", "(", "x_se", ")", "\n", "return", "x", "*", "self", ".", "gate", "(", "x_se", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.get_norm_act_layer": [[23, 38], ["layer_class.replace().lower.replace().lower", "layer_class.replace().lower.startswith", "layer_class.replace().lower.startswith", "layer_class.replace().lower.replace"], "function", ["None"], ["def", "get_norm_act_layer", "(", "layer_class", ")", ":", "\n", "    ", "layer_class", "=", "layer_class", ".", "replace", "(", "'_'", ",", "''", ")", ".", "lower", "(", ")", "\n", "if", "layer_class", ".", "startswith", "(", "\"batchnorm\"", ")", ":", "\n", "        ", "layer", "=", "BatchNormAct2d", "\n", "", "elif", "layer_class", ".", "startswith", "(", "\"groupnorm\"", ")", ":", "\n", "        ", "layer", "=", "GroupNormAct", "\n", "", "elif", "layer_class", "==", "\"evonormbatch\"", ":", "\n", "        ", "layer", "=", "EvoNormBatch2d", "\n", "", "elif", "layer_class", "==", "\"evonormsample\"", ":", "\n", "        ", "layer", "=", "EvoNormSample2d", "\n", "", "elif", "layer_class", "==", "\"iabn\"", "or", "layer_class", "==", "\"inplaceabn\"", ":", "\n", "        ", "layer", "=", "InplaceAbn", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"Invalid norm_act layer (%s)\"", "%", "layer_class", "\n", "", "return", "layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.create_norm_act": [[40, 49], ["layer_type.split", "create_norm_act.get_norm_act_layer", "get_norm_act_layer.", "len", "torch.jit.script", "torch.jit.script"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.get_norm_act_layer"], ["", "def", "create_norm_act", "(", "layer_type", ",", "num_features", ",", "apply_act", "=", "True", ",", "jit", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "layer_parts", "=", "layer_type", ".", "split", "(", "'-'", ")", "# e.g. batchnorm-leaky_relu", "\n", "assert", "len", "(", "layer_parts", ")", "in", "(", "1", ",", "2", ")", "\n", "layer", "=", "get_norm_act_layer", "(", "layer_parts", "[", "0", "]", ")", "\n", "#activation_class = layer_parts[1].lower() if len(layer_parts) > 1 else ''   # FIXME support string act selection?", "\n", "layer_instance", "=", "layer", "(", "num_features", ",", "apply_act", "=", "apply_act", ",", "**", "kwargs", ")", "\n", "if", "jit", ":", "\n", "        ", "layer_instance", "=", "torch", ".", "jit", ".", "script", "(", "layer_instance", ")", "\n", "", "return", "layer_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.convert_norm_act": [[51, 84], ["isinstance", "isinstance", "isinstance", "isinstance", "norm_act_kwargs.update", "create_norm_act.get_norm_act_layer", "norm_act_kwargs.setdefault", "functools.partial", "isinstance", "norm_layer.__name__.lower", "norm_layer.__name__.lower.startswith", "norm_layer.__name__.lower.startswith"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_norm_act.get_norm_act_layer"], ["", "def", "convert_norm_act", "(", "norm_layer", ",", "act_layer", ")", ":", "\n", "    ", "assert", "isinstance", "(", "norm_layer", ",", "(", "type", ",", "str", ",", "types", ".", "FunctionType", ",", "functools", ".", "partial", ")", ")", "\n", "assert", "act_layer", "is", "None", "or", "isinstance", "(", "act_layer", ",", "(", "type", ",", "str", ",", "types", ".", "FunctionType", ",", "functools", ".", "partial", ")", ")", "\n", "norm_act_kwargs", "=", "{", "}", "\n", "\n", "# unbind partial fn, so args can be rebound later", "\n", "if", "isinstance", "(", "norm_layer", ",", "functools", ".", "partial", ")", ":", "\n", "        ", "norm_act_kwargs", ".", "update", "(", "norm_layer", ".", "keywords", ")", "\n", "norm_layer", "=", "norm_layer", ".", "func", "\n", "\n", "", "if", "isinstance", "(", "norm_layer", ",", "str", ")", ":", "\n", "        ", "norm_act_layer", "=", "get_norm_act_layer", "(", "norm_layer", ")", "\n", "", "elif", "norm_layer", "in", "_NORM_ACT_TYPES", ":", "\n", "        ", "norm_act_layer", "=", "norm_layer", "\n", "", "elif", "isinstance", "(", "norm_layer", ",", "types", ".", "FunctionType", ")", ":", "\n", "# if function type, must be a lambda/fn that creates a norm_act layer", "\n", "        ", "norm_act_layer", "=", "norm_layer", "\n", "", "else", ":", "\n", "        ", "type_name", "=", "norm_layer", ".", "__name__", ".", "lower", "(", ")", "\n", "if", "type_name", ".", "startswith", "(", "'batchnorm'", ")", ":", "\n", "            ", "norm_act_layer", "=", "BatchNormAct2d", "\n", "", "elif", "type_name", ".", "startswith", "(", "'groupnorm'", ")", ":", "\n", "            ", "norm_act_layer", "=", "GroupNormAct", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"No equivalent norm_act layer for {type_name}\"", "\n", "\n", "", "", "if", "norm_act_layer", "in", "_NORM_ACT_REQUIRES_ARG", ":", "\n", "# pass `act_layer` through for backwards compat where `act_layer=None` implies no activation.", "\n", "# In the future, may force use of `apply_act` with `act_layer` arg bound to relevant NormAct types", "\n", "        ", "norm_act_kwargs", ".", "setdefault", "(", "'act_layer'", ",", "act_layer", ")", "\n", "", "if", "norm_act_kwargs", ":", "\n", "        ", "norm_act_layer", "=", "functools", ".", "partial", "(", "norm_act_layer", ",", "**", "norm_act_kwargs", ")", "# bind/rebind args", "\n", "", "return", "norm_act_layer", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.gather_excite.GatherExcite.__init__": [[28, 69], ["torch.nn.Module.__init__", "create_act.get_act_layer", "create_act.create_act_layer", "torch.nn.Sequential", "helpers.make_divisible", "mlp.ConvMlp", "torch.nn.Identity", "gather_excite.GatherExcite.gather.add_module", "int", "range", "create_conv2d.create_conv2d.create_conv2d", "gather_excite.GatherExcite.gather.add_module", "math.log2", "gather_excite.GatherExcite.gather.add_module", "torch.nn.BatchNorm2d", "create_conv2d.create_conv2d.create_conv2d", "gather_excite.GatherExcite.gather.add_module", "gather_excite.GatherExcite.gather.add_module", "torch.nn.BatchNorm2d", "create_act.get_act_layer."], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.create_act_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_conv2d.create_conv2d"], ["def", "__init__", "(", "\n", "self", ",", "channels", ",", "feat_size", "=", "None", ",", "extra_params", "=", "False", ",", "extent", "=", "0", ",", "use_mlp", "=", "True", ",", "\n", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "1", ",", "add_maxpool", "=", "False", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "gate_layer", "=", "'sigmoid'", ")", ":", "\n", "        ", "super", "(", "GatherExcite", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "add_maxpool", "=", "add_maxpool", "\n", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "self", ".", "extent", "=", "extent", "\n", "if", "extra_params", ":", "\n", "            ", "self", ".", "gather", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "extent", "==", "0", ":", "\n", "                ", "assert", "feat_size", "is", "not", "None", ",", "'spatial feature size must be specified for global extent w/ params'", "\n", "self", ".", "gather", ".", "add_module", "(", "\n", "'conv1'", ",", "create_conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "feat_size", ",", "stride", "=", "1", ",", "depthwise", "=", "True", ")", ")", "\n", "if", "norm_layer", ":", "\n", "                    ", "self", ".", "gather", ".", "add_module", "(", "f'norm1'", ",", "nn", ".", "BatchNorm2d", "(", "channels", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "assert", "extent", "%", "2", "==", "0", "\n", "num_conv", "=", "int", "(", "math", ".", "log2", "(", "extent", ")", ")", "\n", "for", "i", "in", "range", "(", "num_conv", ")", ":", "\n", "                    ", "self", ".", "gather", ".", "add_module", "(", "\n", "f'conv{i + 1}'", ",", "\n", "create_conv2d", "(", "channels", ",", "channels", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "depthwise", "=", "True", ")", ")", "\n", "if", "norm_layer", ":", "\n", "                        ", "self", ".", "gather", ".", "add_module", "(", "f'norm{i + 1}'", ",", "nn", ".", "BatchNorm2d", "(", "channels", ")", ")", "\n", "", "if", "i", "!=", "num_conv", "-", "1", ":", "\n", "                        ", "self", ".", "gather", ".", "add_module", "(", "f'act{i + 1}'", ",", "act_layer", "(", "inplace", "=", "True", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "self", ".", "gather", "=", "None", "\n", "if", "self", ".", "extent", "==", "0", ":", "\n", "                ", "self", ".", "gk", "=", "0", "\n", "self", ".", "gs", "=", "0", "\n", "", "else", ":", "\n", "                ", "assert", "extent", "%", "2", "==", "0", "\n", "self", ".", "gk", "=", "self", ".", "extent", "*", "2", "-", "1", "\n", "self", ".", "gs", "=", "self", ".", "extent", "\n", "\n", "", "", "if", "not", "rd_channels", ":", "\n", "            ", "rd_channels", "=", "make_divisible", "(", "channels", "*", "rd_ratio", ",", "rd_divisor", ",", "round_limit", "=", "0.", ")", "\n", "", "self", ".", "mlp", "=", "ConvMlp", "(", "channels", ",", "rd_channels", ",", "act_layer", "=", "act_layer", ")", "if", "use_mlp", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "gate", "=", "create_act_layer", "(", "gate_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.gather_excite.GatherExcite.forward": [[70, 91], ["gather_excite.GatherExcite.mlp", "gather_excite.GatherExcite.gather", "torch.interpolate", "gather_excite.GatherExcite.gate", "x.mean", "torch.avg_pool2d", "x.amax", "torch.max_pool2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size", "=", "x", ".", "shape", "[", "-", "2", ":", "]", "\n", "if", "self", ".", "gather", "is", "not", "None", ":", "\n", "            ", "x_ge", "=", "self", ".", "gather", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "extent", "==", "0", ":", "\n", "# global extent", "\n", "                ", "x_ge", "=", "x", ".", "mean", "(", "dim", "=", "(", "2", ",", "3", ")", ",", "keepdims", "=", "True", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "                    ", "x_ge", "=", "0.5", "*", "x_ge", "+", "0.5", "*", "x", ".", "amax", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "                ", "x_ge", "=", "F", ".", "avg_pool2d", "(", "\n", "x", ",", "kernel_size", "=", "self", ".", "gk", ",", "stride", "=", "self", ".", "gs", ",", "padding", "=", "self", ".", "gk", "//", "2", ",", "count_include_pad", "=", "False", ")", "\n", "if", "self", ".", "add_maxpool", ":", "\n", "# experimental codepath, may remove or change", "\n", "                    ", "x_ge", "=", "0.5", "*", "x_ge", "+", "0.5", "*", "F", ".", "max_pool2d", "(", "x", ",", "kernel_size", "=", "self", ".", "gk", ",", "stride", "=", "self", ".", "gs", ",", "padding", "=", "self", ".", "gk", "//", "2", ")", "\n", "", "", "", "x_ge", "=", "self", ".", "mlp", "(", "x_ge", ")", "\n", "if", "x_ge", ".", "shape", "[", "-", "1", "]", "!=", "1", "or", "x_ge", ".", "shape", "[", "-", "2", "]", "!=", "1", ":", "\n", "            ", "x_ge", "=", "F", ".", "interpolate", "(", "x_ge", ",", "size", "=", "size", ")", "\n", "", "return", "x", "*", "self", ".", "gate", "(", "x_ge", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d.__init__": [[17, 28], ["torch.nn.BatchNorm2d.__init__", "isinstance", "create_act.get_act_layer", "create_act.get_act_layer.", "torch.nn.Identity", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer"], ["def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "track_running_stats", "=", "True", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "True", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "BatchNormAct2d", ",", "self", ")", ".", "__init__", "(", "\n", "num_features", ",", "eps", "=", "eps", ",", "momentum", "=", "momentum", ",", "affine", "=", "affine", ",", "track_running_stats", "=", "track_running_stats", ")", "\n", "if", "isinstance", "(", "act_layer", ",", "str", ")", ":", "\n", "            ", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d._forward_jit": [[29, 54], ["torch.nn.functional.batch_norm", "float"], "methods", ["None"], ["", "", "def", "_forward_jit", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" A cut & paste of the contents of the PyTorch BatchNorm2d forward function\n        \"\"\"", "\n", "# exponential_average_factor is self.momentum set to", "\n", "# (when it is available) only so that if gets updated", "\n", "# in ONNX graph when this node is exported to ONNX.", "\n", "if", "self", ".", "momentum", "is", "None", ":", "\n", "            ", "exponential_average_factor", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "exponential_average_factor", "=", "self", ".", "momentum", "\n", "\n", "", "if", "self", ".", "training", "and", "self", ".", "track_running_stats", ":", "\n", "# TODO: if statement only here to tell the jit to skip emitting this when it is None", "\n", "            ", "if", "self", ".", "num_batches_tracked", "is", "not", "None", ":", "\n", "                ", "self", ".", "num_batches_tracked", "+=", "1", "\n", "if", "self", ".", "momentum", "is", "None", ":", "# use cumulative moving average", "\n", "                    ", "exponential_average_factor", "=", "1.0", "/", "float", "(", "self", ".", "num_batches_tracked", ")", "\n", "", "else", ":", "# use exponential moving average", "\n", "                    ", "exponential_average_factor", "=", "self", ".", "momentum", "\n", "\n", "", "", "", "x", "=", "F", ".", "batch_norm", "(", "\n", "x", ",", "self", ".", "running_mean", ",", "self", ".", "running_var", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "\n", "self", ".", "training", "or", "not", "self", ".", "track_running_stats", ",", "\n", "exponential_average_factor", ",", "self", ".", "eps", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d._forward_python": [[55, 58], ["super().forward"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "_forward_python", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "super", "(", "BatchNormAct2d", ",", "self", ")", ".", "forward", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d.forward": [[59, 67], ["torch.jit.is_scripting", "norm_act.BatchNormAct2d.act", "norm_act.BatchNormAct2d._forward_jit", "norm_act.BatchNormAct2d._forward_python"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d._forward_jit", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.BatchNormAct2d._forward_python"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# FIXME cannot call parent forward() and maintain jit.script compatibility?", "\n", "        ", "if", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "            ", "x", "=", "self", ".", "_forward_jit", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "_forward_python", "(", "x", ")", "\n", "", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.GroupNormAct.__init__": [[71, 81], ["torch.nn.GroupNorm.__init__", "isinstance", "create_act.get_act_layer", "create_act.get_act_layer.", "torch.nn.Identity", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_act.get_act_layer"], ["    ", "def", "__init__", "(", "self", ",", "num_channels", ",", "num_groups", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ",", "\n", "apply_act", "=", "True", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "inplace", "=", "True", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "GroupNormAct", ",", "self", ")", ".", "__init__", "(", "num_groups", ",", "num_channels", ",", "eps", "=", "eps", ",", "affine", "=", "affine", ")", "\n", "if", "isinstance", "(", "act_layer", ",", "str", ")", ":", "\n", "            ", "act_layer", "=", "get_act_layer", "(", "act_layer", ")", "\n", "", "if", "act_layer", "is", "not", "None", "and", "apply_act", ":", "\n", "            ", "act_args", "=", "dict", "(", "inplace", "=", "True", ")", "if", "inplace", "else", "{", "}", "\n", "self", ".", "act", "=", "act_layer", "(", "**", "act_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "act", "=", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.norm_act.GroupNormAct.forward": [[82, 86], ["torch.nn.functional.group_norm", "norm_act.GroupNormAct.act"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "group_norm", "(", "x", ",", "self", ".", "num_groups", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "eps", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_attn.get_attn": [[21, 82], ["isinstance", "isinstance", "attn_type.lower.lower", "isinstance", "functools.partial", "functools.partial"], "function", ["None"], ["def", "get_attn", "(", "attn_type", ")", ":", "\n", "    ", "if", "isinstance", "(", "attn_type", ",", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "        ", "return", "attn_type", "\n", "", "module_cls", "=", "None", "\n", "if", "attn_type", "is", "not", "None", ":", "\n", "        ", "if", "isinstance", "(", "attn_type", ",", "str", ")", ":", "\n", "            ", "attn_type", "=", "attn_type", ".", "lower", "(", ")", "\n", "# Lightweight attention modules (channel and/or coarse spatial).", "\n", "# Typically added to existing network architecture blocks in addition to existing convolutions.", "\n", "if", "attn_type", "==", "'se'", ":", "\n", "                ", "module_cls", "=", "SEModule", "\n", "", "elif", "attn_type", "==", "'ese'", ":", "\n", "                ", "module_cls", "=", "EffectiveSEModule", "\n", "", "elif", "attn_type", "==", "'eca'", ":", "\n", "                ", "module_cls", "=", "EcaModule", "\n", "", "elif", "attn_type", "==", "'ecam'", ":", "\n", "                ", "module_cls", "=", "partial", "(", "EcaModule", ",", "use_mlp", "=", "True", ")", "\n", "", "elif", "attn_type", "==", "'ceca'", ":", "\n", "                ", "module_cls", "=", "CecaModule", "\n", "", "elif", "attn_type", "==", "'ge'", ":", "\n", "                ", "module_cls", "=", "GatherExcite", "\n", "", "elif", "attn_type", "==", "'gc'", ":", "\n", "                ", "module_cls", "=", "GlobalContext", "\n", "", "elif", "attn_type", "==", "'gca'", ":", "\n", "                ", "module_cls", "=", "partial", "(", "GlobalContext", ",", "fuse_add", "=", "True", ",", "fuse_scale", "=", "False", ")", "\n", "", "elif", "attn_type", "==", "'cbam'", ":", "\n", "                ", "module_cls", "=", "CbamModule", "\n", "", "elif", "attn_type", "==", "'lcbam'", ":", "\n", "                ", "module_cls", "=", "LightCbamModule", "\n", "\n", "# Attention / attention-like modules w/ significant params", "\n", "# Typically replace some of the existing workhorse convs in a network architecture.", "\n", "# All of these accept a stride argument and can spatially downsample the input.", "\n", "", "elif", "attn_type", "==", "'sk'", ":", "\n", "                ", "module_cls", "=", "SelectiveKernel", "\n", "", "elif", "attn_type", "==", "'splat'", ":", "\n", "                ", "module_cls", "=", "SplitAttn", "\n", "\n", "# Self-attention / attention-like modules w/ significant compute and/or params", "\n", "# Typically replace some of the existing workhorse convs in a network architecture.", "\n", "# All of these accept a stride argument and can spatially downsample the input.", "\n", "", "elif", "attn_type", "==", "'lambda'", ":", "\n", "                ", "return", "LambdaLayer", "\n", "", "elif", "attn_type", "==", "'bottleneck'", ":", "\n", "                ", "return", "BottleneckAttn", "\n", "", "elif", "attn_type", "==", "'halo'", ":", "\n", "                ", "return", "HaloAttn", "\n", "", "elif", "attn_type", "==", "'nl'", ":", "\n", "                ", "module_cls", "=", "NonLocalAttn", "\n", "", "elif", "attn_type", "==", "'bat'", ":", "\n", "                ", "module_cls", "=", "BatNonLocalAttn", "\n", "\n", "# Woops!", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"Invalid attn module (%s)\"", "%", "attn_type", "\n", "", "", "elif", "isinstance", "(", "attn_type", ",", "bool", ")", ":", "\n", "            ", "if", "attn_type", ":", "\n", "                ", "module_cls", "=", "SEModule", "\n", "", "", "else", ":", "\n", "            ", "module_cls", "=", "attn_type", "\n", "", "", "return", "module_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_attn.create_attn": [[84, 90], ["create_attn.get_attn", "get_attn."], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.create_attn.get_attn"], ["", "def", "create_attn", "(", "attn_type", ",", "channels", ",", "**", "kwargs", ")", ":", "\n", "    ", "module_cls", "=", "get_attn", "(", "attn_type", ")", "\n", "if", "module_cls", "is", "not", "None", ":", "\n", "# NOTE: it's expected the first (positional) argument of all attention layers is the # input channels", "\n", "        ", "return", "module_cls", "(", "channels", ",", "**", "kwargs", ")", "\n", "", "return", "None", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.CondConv2d.__init__": [[43, 72], ["torch.nn.Module.__init__", "helpers.to_2tuple", "helpers.to_2tuple", "padding.get_padding_value", "helpers.to_2tuple", "helpers.to_2tuple", "torch.nn.Parameter", "cond_conv2d.CondConv2d.reset_parameters", "torch.Tensor", "torch.nn.Parameter", "cond_conv2d.CondConv2d.register_parameter", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.padding.get_padding_value", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "padding", "=", "''", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "bias", "=", "False", ",", "num_experts", "=", "4", ")", ":", "\n", "        ", "super", "(", "CondConv2d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "to_2tuple", "(", "kernel_size", ")", "\n", "self", ".", "stride", "=", "to_2tuple", "(", "stride", ")", "\n", "padding_val", ",", "is_padding_dynamic", "=", "get_padding_value", "(", "\n", "padding", ",", "kernel_size", ",", "stride", "=", "stride", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "dynamic_padding", "=", "is_padding_dynamic", "# if in forward to work with torchscript", "\n", "self", ".", "padding", "=", "to_2tuple", "(", "padding_val", ")", "\n", "self", ".", "dilation", "=", "to_2tuple", "(", "dilation", ")", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "num_experts", "=", "num_experts", "\n", "\n", "self", ".", "weight_shape", "=", "(", "self", ".", "out_channels", ",", "self", ".", "in_channels", "//", "self", ".", "groups", ")", "+", "self", ".", "kernel_size", "\n", "weight_num_param", "=", "1", "\n", "for", "wd", "in", "self", ".", "weight_shape", ":", "\n", "            ", "weight_num_param", "*=", "wd", "\n", "", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_experts", ",", "weight_num_param", ")", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias_shape", "=", "(", "self", ".", "out_channels", ",", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "self", ".", "num_experts", ",", "self", ".", "out_channels", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.CondConv2d.reset_parameters": [[73, 83], ["cond_conv2d.get_condconv_initializer", "get_condconv_initializer.", "functools.partial", "numpy.prod", "cond_conv2d.get_condconv_initializer", "get_condconv_initializer.", "math.sqrt", "functools.partial", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.get_condconv_initializer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.get_condconv_initializer"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "init_weight", "=", "get_condconv_initializer", "(", "\n", "partial", "(", "nn", ".", "init", ".", "kaiming_uniform_", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", ",", "self", ".", "num_experts", ",", "self", ".", "weight_shape", ")", "\n", "init_weight", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "fan_in", "=", "np", ".", "prod", "(", "self", ".", "weight_shape", "[", "1", ":", "]", ")", "\n", "bound", "=", "1", "/", "math", ".", "sqrt", "(", "fan_in", ")", "\n", "init_bias", "=", "get_condconv_initializer", "(", "\n", "partial", "(", "nn", ".", "init", ".", "uniform_", ",", "a", "=", "-", "bound", ",", "b", "=", "bound", ")", ",", "self", ".", "num_experts", ",", "self", ".", "bias_shape", ")", "\n", "init_bias", "(", "self", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.CondConv2d.forward": [[84, 123], ["torch.matmul", "weight.view.view.view", "x.view.view.view", "torch.nn.functional.conv2d.permute().view", "torch.matmul", "bias.view.view.view", "conv2d_same.conv2d_same.conv2d_same", "torch.nn.functional.conv2d", "torch.nn.functional.conv2d.permute"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.conv2d_same.conv2d_same"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "routing_weights", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "weight", "=", "torch", ".", "matmul", "(", "routing_weights", ",", "self", ".", "weight", ")", "\n", "new_weight_shape", "=", "(", "B", "*", "self", ".", "out_channels", ",", "self", ".", "in_channels", "//", "self", ".", "groups", ")", "+", "self", ".", "kernel_size", "\n", "weight", "=", "weight", ".", "view", "(", "new_weight_shape", ")", "\n", "bias", "=", "None", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "torch", ".", "matmul", "(", "routing_weights", ",", "self", ".", "bias", ")", "\n", "bias", "=", "bias", ".", "view", "(", "B", "*", "self", ".", "out_channels", ")", "\n", "# move batch elements with channels so each batch element can be efficiently convolved with separate kernel", "\n", "", "x", "=", "x", ".", "view", "(", "1", ",", "B", "*", "C", ",", "H", ",", "W", ")", "\n", "if", "self", ".", "dynamic_padding", ":", "\n", "            ", "out", "=", "conv2d_same", "(", "\n", "x", ",", "weight", ",", "bias", ",", "stride", "=", "self", ".", "stride", ",", "padding", "=", "self", ".", "padding", ",", "\n", "dilation", "=", "self", ".", "dilation", ",", "groups", "=", "self", ".", "groups", "*", "B", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "F", ".", "conv2d", "(", "\n", "x", ",", "weight", ",", "bias", ",", "stride", "=", "self", ".", "stride", ",", "padding", "=", "self", ".", "padding", ",", "\n", "dilation", "=", "self", ".", "dilation", ",", "groups", "=", "self", ".", "groups", "*", "B", ")", "\n", "", "out", "=", "out", ".", "permute", "(", "[", "1", ",", "0", ",", "2", ",", "3", "]", ")", ".", "view", "(", "B", ",", "self", ".", "out_channels", ",", "out", ".", "shape", "[", "-", "2", "]", ",", "out", ".", "shape", "[", "-", "1", "]", ")", "\n", "\n", "# Literal port (from TF definition)", "\n", "# x = torch.split(x, 1, 0)", "\n", "# weight = torch.split(weight, 1, 0)", "\n", "# if self.bias is not None:", "\n", "#     bias = torch.matmul(routing_weights, self.bias)", "\n", "#     bias = torch.split(bias, 1, 0)", "\n", "# else:", "\n", "#     bias = [None] * B", "\n", "# out = []", "\n", "# for xi, wi, bi in zip(x, weight, bias):", "\n", "#     wi = wi.view(*self.weight_shape)", "\n", "#     if bi is not None:", "\n", "#         bi = bi.view(*self.bias_shape)", "\n", "#     out.append(self.conv_fn(", "\n", "#         xi, wi, bi, stride=self.stride, padding=self.padding,", "\n", "#         dilation=self.dilation, groups=self.groups))", "\n", "# out = torch.cat(out, 0)", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.cond_conv2d.get_condconv_initializer": [[21, 32], ["numpy.prod", "range", "ValueError", "initializer", "len", "weight[].view"], "function", ["None"], ["def", "get_condconv_initializer", "(", "initializer", ",", "num_experts", ",", "expert_shape", ")", ":", "\n", "    ", "def", "condconv_initializer", "(", "weight", ")", ":", "\n", "        ", "\"\"\"CondConv initializer function.\"\"\"", "\n", "num_params", "=", "np", ".", "prod", "(", "expert_shape", ")", "\n", "if", "(", "len", "(", "weight", ".", "shape", ")", "!=", "2", "or", "weight", ".", "shape", "[", "0", "]", "!=", "num_experts", "or", "\n", "weight", ".", "shape", "[", "1", "]", "!=", "num_params", ")", ":", "\n", "            ", "raise", "(", "ValueError", "(", "\n", "'CondConv variables must have shape [num_experts, num_params]'", ")", ")", "\n", "", "for", "i", "in", "range", "(", "num_experts", ")", ":", "\n", "            ", "initializer", "(", "weight", "[", "i", "]", ".", "view", "(", "expert_shape", ")", ")", "\n", "", "", "return", "condconv_initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.SpaceToDepth.__init__": [[6, 10], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "block_size", "=", "4", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "block_size", "==", "4", "\n", "self", ".", "bs", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.SpaceToDepth.forward": [[11, 17], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", ",", "H", "//", "self", ".", "bs", ",", "self", ".", "bs", ",", "W", "//", "self", ".", "bs", ",", "self", ".", "bs", ")", "# (N, C, H//bs, bs, W//bs, bs)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "5", ",", "1", ",", "2", ",", "4", ")", ".", "contiguous", "(", ")", "# (N, bs, bs, C, H//bs, W//bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "*", "(", "self", ".", "bs", "**", "2", ")", ",", "H", "//", "self", ".", "bs", ",", "W", "//", "self", ".", "bs", ")", "# (N, C*bs^2, H//bs, W//bs)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.SpaceToDepthJit.__call__": [[21, 28], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "# assuming hard-coded that block_size==4 for acceleration", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", ",", "H", "//", "4", ",", "4", ",", "W", "//", "4", ",", "4", ")", "# (N, C, H//bs, bs, W//bs, bs)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "5", ",", "1", ",", "2", ",", "4", ")", ".", "contiguous", "(", ")", "# (N, bs, bs, C, H//bs, W//bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "*", "16", ",", "H", "//", "4", ",", "W", "//", "4", ")", "# (N, C*bs^2, H//bs, W//bs)", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.SpaceToDepthModule.__init__": [[31, 37], ["torch.Module.__init__", "space_to_depth.SpaceToDepthJit", "space_to_depth.SpaceToDepth"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "no_jit", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "no_jit", ":", "\n", "            ", "self", ".", "op", "=", "SpaceToDepthJit", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "op", "=", "SpaceToDepth", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.SpaceToDepthModule.forward": [[38, 40], ["space_to_depth.SpaceToDepthModule.op"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "op", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.DepthToSpace.__init__": [[44, 47], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "block_size", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bs", "=", "block_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.space_to_depth.DepthToSpace.forward": [[48, 54], ["x.view.view.size", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "x.view.view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "N", ",", "C", ",", "H", ",", "W", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "self", ".", "bs", ",", "self", ".", "bs", ",", "C", "//", "(", "self", ".", "bs", "**", "2", ")", ",", "H", ",", "W", ")", "# (N, bs, bs, C//bs^2, H, W)", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "4", ",", "1", ",", "5", ",", "2", ")", ".", "contiguous", "(", ")", "# (N, C//bs^2, H, bs, W, bs)", "\n", "x", "=", "x", ".", "view", "(", "N", ",", "C", "//", "(", "self", ".", "bs", "**", "2", ")", ",", "H", "*", "self", ".", "bs", ",", "W", "*", "self", ".", "bs", ")", "# (N, C//bs^2, H * bs, W * bs)", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel.SelectiveKernelAttn.__init__": [[22, 35], ["torch.nn.Module.__init__", "torch.nn.Conv2d", "norm_layer", "act_layer", "torch.nn.Conv2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "num_paths", "=", "2", ",", "attn_channels", "=", "32", ",", "\n", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "\"\"\" Selective Kernel Attention Module\n\n        Selective Kernel attention mechanism factored out into its own module.\n\n        \"\"\"", "\n", "super", "(", "SelectiveKernelAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_paths", "=", "num_paths", "\n", "self", ".", "fc_reduce", "=", "nn", ".", "Conv2d", "(", "channels", ",", "attn_channels", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "bn", "=", "norm_layer", "(", "attn_channels", ")", "\n", "self", ".", "act", "=", "act_layer", "(", "inplace", "=", "True", ")", "\n", "self", ".", "fc_select", "=", "nn", ".", "Conv2d", "(", "attn_channels", ",", "channels", "*", "num_paths", ",", "kernel_size", "=", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel.SelectiveKernelAttn.forward": [[36, 47], ["torch.softmax.sum().mean", "selective_kernel.SelectiveKernelAttn.fc_reduce", "selective_kernel.SelectiveKernelAttn.bn", "selective_kernel.SelectiveKernelAttn.act", "selective_kernel.SelectiveKernelAttn.fc_select", "torch.softmax.view", "torch.softmax", "torch.softmax.sum"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "x", ".", "shape", "[", "1", "]", "==", "self", ".", "num_paths", "\n", "x", "=", "x", ".", "sum", "(", "1", ")", ".", "mean", "(", "(", "2", ",", "3", ")", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "self", ".", "fc_reduce", "(", "x", ")", "\n", "x", "=", "self", ".", "bn", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "fc_select", "(", "x", ")", "\n", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "view", "(", "B", ",", "self", ".", "num_paths", ",", "C", "//", "self", ".", "num_paths", ",", "H", ",", "W", ")", "\n", "x", "=", "torch", ".", "softmax", "(", "x", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel.SelectiveKernel.__init__": [[51, 108], ["torch.nn.Module.__init__", "selective_kernel._kernel_valid", "len", "min", "dict", "torch.nn.ModuleList", "selective_kernel.SelectiveKernelAttn", "isinstance", "helpers.make_divisible", "len", "len", "conv_bn_act.ConvBnAct", "zip"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel._kernel_valid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", "=", "None", ",", "kernel_size", "=", "None", ",", "stride", "=", "1", ",", "dilation", "=", "1", ",", "groups", "=", "1", ",", "\n", "rd_ratio", "=", "1.", "/", "16", ",", "rd_channels", "=", "None", ",", "rd_divisor", "=", "8", ",", "keep_3x3", "=", "True", ",", "split_input", "=", "True", ",", "\n", "drop_block", "=", "None", ",", "act_layer", "=", "nn", ".", "ReLU", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "aa_layer", "=", "None", ")", ":", "\n", "        ", "\"\"\" Selective Kernel Convolution Module\n\n        As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n\n        Largest change is the input split, which divides the input channels across each convolution path, this can\n        be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n        the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n        a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n\n        Args:\n            in_channels (int):  module input (feature) channel count\n            out_channels (int):  module output (feature) channel count\n            kernel_size (int, list): kernel size for each convolution branch\n            stride (int): stride for convolutions\n            dilation (int): dilation for module as a whole, impacts dilation of each branch\n            groups (int): number of groups for each branch\n            rd_ratio (int, float): reduction factor for attention features\n            keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n            split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n                can be viewed as grouping by path, output expands to module out_channels count\n            drop_block (nn.Module): drop block module\n            act_layer (nn.Module): activation layer to use\n            norm_layer (nn.Module): batchnorm/norm layer to use\n        \"\"\"", "\n", "super", "(", "SelectiveKernel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "out_channels", "=", "out_channels", "or", "in_channels", "\n", "kernel_size", "=", "kernel_size", "or", "[", "3", ",", "5", "]", "# default to one 3x3 and one 5x5 branch. 5x5 -> 3x3 + dilation", "\n", "_kernel_valid", "(", "kernel_size", ")", "\n", "if", "not", "isinstance", "(", "kernel_size", ",", "list", ")", ":", "\n", "            ", "kernel_size", "=", "[", "kernel_size", "]", "*", "2", "\n", "", "if", "keep_3x3", ":", "\n", "            ", "dilation", "=", "[", "dilation", "*", "(", "k", "-", "1", ")", "//", "2", "for", "k", "in", "kernel_size", "]", "\n", "kernel_size", "=", "[", "3", "]", "*", "len", "(", "kernel_size", ")", "\n", "", "else", ":", "\n", "            ", "dilation", "=", "[", "dilation", "]", "*", "len", "(", "kernel_size", ")", "\n", "", "self", ".", "num_paths", "=", "len", "(", "kernel_size", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "split_input", "=", "split_input", "\n", "if", "self", ".", "split_input", ":", "\n", "            ", "assert", "in_channels", "%", "self", ".", "num_paths", "==", "0", "\n", "in_channels", "=", "in_channels", "//", "self", ".", "num_paths", "\n", "", "groups", "=", "min", "(", "out_channels", ",", "groups", ")", "\n", "\n", "conv_kwargs", "=", "dict", "(", "\n", "stride", "=", "stride", ",", "groups", "=", "groups", ",", "drop_block", "=", "drop_block", ",", "act_layer", "=", "act_layer", ",", "norm_layer", "=", "norm_layer", ",", "\n", "aa_layer", "=", "aa_layer", ")", "\n", "self", ".", "paths", "=", "nn", ".", "ModuleList", "(", "[", "\n", "ConvBnAct", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "k", ",", "dilation", "=", "d", ",", "**", "conv_kwargs", ")", "\n", "for", "k", ",", "d", "in", "zip", "(", "kernel_size", ",", "dilation", ")", "]", ")", "\n", "\n", "attn_channels", "=", "rd_channels", "or", "make_divisible", "(", "out_channels", "*", "rd_ratio", ",", "divisor", "=", "rd_divisor", ")", "\n", "self", ".", "attn", "=", "SelectiveKernelAttn", "(", "out_channels", ",", "self", ".", "num_paths", ",", "attn_channels", ")", "\n", "self", ".", "drop_block", "=", "drop_block", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel.SelectiveKernel.forward": [[109, 120], ["torch.stack", "selective_kernel.SelectiveKernel.attn", "torch.sum", "torch.split", "op", "op", "enumerate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "split_input", ":", "\n", "            ", "x_split", "=", "torch", ".", "split", "(", "x", ",", "self", ".", "in_channels", "//", "self", ".", "num_paths", ",", "1", ")", "\n", "x_paths", "=", "[", "op", "(", "x_split", "[", "i", "]", ")", "for", "i", ",", "op", "in", "enumerate", "(", "self", ".", "paths", ")", "]", "\n", "", "else", ":", "\n", "            ", "x_paths", "=", "[", "op", "(", "x", ")", "for", "op", "in", "self", ".", "paths", "]", "\n", "", "x", "=", "torch", ".", "stack", "(", "x_paths", ",", "dim", "=", "1", ")", "\n", "x_attn", "=", "self", ".", "attn", "(", "x", ")", "\n", "x", "=", "x", "*", "x_attn", "\n", "x", "=", "torch", ".", "sum", "(", "x", ",", "dim", "=", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel._kernel_valid": [[14, 19], ["isinstance", "selective_kernel._kernel_valid"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.selective_kernel._kernel_valid"], ["def", "_kernel_valid", "(", "k", ")", ":", "\n", "    ", "if", "isinstance", "(", "k", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "for", "ki", "in", "k", ":", "\n", "            ", "return", "_kernel_valid", "(", "ki", ")", "\n", "", "", "assert", "k", ">=", "3", "and", "k", "%", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.ClassifierHead.__init__": [[43, 49], ["torch.nn.Module.__init__", "classifier._create_pool", "classifier._create_fc", "torch.nn.Flatten", "torch.nn.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_pool", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_fc"], ["def", "__init__", "(", "self", ",", "in_chs", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "drop_rate", "=", "0.", ",", "use_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", "ClassifierHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_rate", "=", "drop_rate", "\n", "self", ".", "global_pool", ",", "num_pooled_features", "=", "_create_pool", "(", "in_chs", ",", "num_classes", ",", "pool_type", ",", "use_conv", "=", "use_conv", ")", "\n", "self", ".", "fc", "=", "_create_fc", "(", "num_pooled_features", ",", "num_classes", ",", "use_conv", "=", "use_conv", ")", "\n", "self", ".", "flatten", "=", "nn", ".", "Flatten", "(", "1", ")", "if", "use_conv", "and", "pool_type", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.ClassifierHead.forward": [[50, 57], ["classifier.ClassifierHead.global_pool", "classifier.ClassifierHead.fc", "classifier.ClassifierHead.flatten", "torch.nn.functional.dropout", "float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "global_pool", "(", "x", ")", "\n", "if", "self", ".", "drop_rate", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "float", "(", "self", ".", "drop_rate", ")", ",", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_pool": [[12, 21], ["adaptive_avgmax_pool.SelectAdaptivePool2d", "adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.adaptive_avgmax_pool.SelectAdaptivePool2d.feat_mult"], ["def", "_create_pool", "(", "num_features", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "flatten_in_pool", "=", "not", "use_conv", "# flatten when we use a Linear layer after pooling", "\n", "if", "not", "pool_type", ":", "\n", "        ", "assert", "num_classes", "==", "0", "or", "use_conv", ",", "'Pooling can only be disabled if classifier is also removed or conv classifier is used'", "\n", "flatten_in_pool", "=", "False", "# disable flattening if pooling is pass-through (no pooling)", "\n", "", "global_pool", "=", "SelectAdaptivePool2d", "(", "pool_type", "=", "pool_type", ",", "flatten", "=", "flatten_in_pool", ")", "\n", "num_pooled_features", "=", "num_features", "*", "global_pool", ".", "feat_mult", "(", ")", "\n", "return", "global_pool", ",", "num_pooled_features", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_fc": [[23, 32], ["torch.nn.Identity", "torch.nn.Conv2d", "linear.Linear"], "function", ["None"], ["", "def", "_create_fc", "(", "num_features", ",", "num_classes", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "if", "num_classes", "<=", "0", ":", "\n", "        ", "fc", "=", "nn", ".", "Identity", "(", ")", "# pass-through (no classifier)", "\n", "", "elif", "use_conv", ":", "\n", "        ", "fc", "=", "nn", ".", "Conv2d", "(", "num_features", ",", "num_classes", ",", "1", ",", "bias", "=", "True", ")", "\n", "", "else", ":", "\n", "# NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue", "\n", "        ", "fc", "=", "Linear", "(", "num_features", ",", "num_classes", ",", "bias", "=", "True", ")", "\n", "", "return", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier.create_classifier": [[34, 38], ["classifier._create_pool", "classifier._create_fc"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_pool", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.classifier._create_fc"], ["", "def", "create_classifier", "(", "num_features", ",", "num_classes", ",", "pool_type", "=", "'avg'", ",", "use_conv", "=", "False", ")", ":", "\n", "    ", "global_pool", ",", "num_pooled_features", "=", "_create_pool", "(", "num_features", ",", "num_classes", ",", "pool_type", ",", "use_conv", "=", "use_conv", ")", "\n", "fc", "=", "_create_fc", "(", "num_pooled_features", ",", "num_classes", ",", "use_conv", "=", "use_conv", ")", "\n", "return", "global_pool", ",", "fc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.__init__": [[40, 73], ["torch.nn.Module.__init__", "inplace_abn.InplaceAbn.register_buffer", "inplace_abn.InplaceAbn.register_buffer", "inplace_abn.InplaceAbn.reset_parameters", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "inplace_abn.InplaceAbn.register_parameter", "inplace_abn.InplaceAbn.register_parameter", "torch.zeros", "torch.ones", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters"], ["def", "__init__", "(", "self", ",", "num_features", ",", "eps", "=", "1e-5", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "apply_act", "=", "True", ",", "\n", "act_layer", "=", "\"leaky_relu\"", ",", "act_param", "=", "0.01", ",", "drop_block", "=", "None", ")", ":", "\n", "        ", "super", "(", "InplaceAbn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "affine", "=", "affine", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "momentum", "=", "momentum", "\n", "if", "apply_act", ":", "\n", "            ", "if", "isinstance", "(", "act_layer", ",", "str", ")", ":", "\n", "                ", "assert", "act_layer", "in", "(", "'leaky_relu'", ",", "'elu'", ",", "'identity'", ",", "''", ")", "\n", "self", ".", "act_name", "=", "act_layer", "if", "act_layer", "else", "'identity'", "\n", "", "else", ":", "\n", "# convert act layer passed as type to string", "\n", "                ", "if", "act_layer", "==", "nn", ".", "ELU", ":", "\n", "                    ", "self", ".", "act_name", "=", "'elu'", "\n", "", "elif", "act_layer", "==", "nn", ".", "LeakyReLU", ":", "\n", "                    ", "self", ".", "act_name", "=", "'leaky_relu'", "\n", "", "elif", "act_layer", "==", "nn", ".", "Identity", ":", "\n", "                    ", "self", ".", "act_name", "=", "'identity'", "\n", "", "else", ":", "\n", "                    ", "assert", "False", ",", "f'Invalid act layer {act_layer.__name__} for IABN'", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "act_name", "=", "'identity'", "\n", "", "self", ".", "act_param", "=", "act_param", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'weight'", ",", "None", ")", "\n", "self", ".", "register_parameter", "(", "'bias'", ",", "None", ")", "\n", "", "self", ".", "register_buffer", "(", "'running_mean'", ",", "torch", ".", "zeros", "(", "num_features", ")", ")", "\n", "self", ".", "register_buffer", "(", "'running_var'", ",", "torch", ".", "ones", "(", "num_features", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.reset_parameters": [[74, 80], ["torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "running_mean", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "running_var", ",", "1", ")", "\n", "if", "self", ".", "affine", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "weight", ",", "1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.inplace_abn.InplaceAbn.forward": [[81, 88], ["inplace_abn", "isinstance"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "inplace_abn", "(", "\n", "x", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "running_mean", ",", "self", ".", "running_var", ",", "\n", "self", ".", "training", ",", "self", ".", "momentum", ",", "self", ".", "eps", ",", "self", ".", "act_name", ",", "self", ".", "act_param", ")", "\n", "if", "isinstance", "(", "output", ",", "tuple", ")", ":", "\n", "            ", "output", "=", "output", "[", "0", "]", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d.__init__": [[27, 87], ["dict", "torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "int", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "mmcv.cnn.build_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "resnet2plus1d.Conv2plus1d.init_weights", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "len", "(", "stride", ")", "==", "len", "(", "padding", ")", "==", "3", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "output_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "self", ".", "transposed", "=", "False", "\n", "\n", "# The middle-plane is calculated according to:", "\n", "# M_i = \\floor{\\frac{t * d^2 N_i-1 * N_i}", "\n", "#   {d^2 * N_i-1 + t * N_i}}", "\n", "# where d, t are spatial and temporal kernel, and", "\n", "# N_i, N_i-1 are planes", "\n", "# and inplanes. https://arxiv.org/pdf/1711.11248.pdf", "\n", "mid_channels", "=", "3", "*", "(", "\n", "in_channels", "*", "out_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", ")", "\n", "mid_channels", "/=", "(", "\n", "in_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "+", "3", "*", "out_channels", ")", "\n", "mid_channels", "=", "int", "(", "mid_channels", ")", "\n", "\n", "\n", "self", ".", "conv_s", "=", "nn", ".", "Conv3d", "(", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "_", ",", "self", ".", "bn_s", "=", "build_norm_layer", "(", "self", ".", "norm_cfg", ",", "mid_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv_t", "=", "nn", ".", "Conv3d", "(", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "self", ".", "mid_channel", "=", "mid_channels", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d.forward": [[88, 100], ["resnet2plus1d.Conv2plus1d.conv_s", "resnet2plus1d.Conv2plus1d.bn_s", "resnet2plus1d.Conv2plus1d.relu", "resnet2plus1d.Conv2plus1d.conv_t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv_s", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_s", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_t", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d.init_weights": [[101, 106], ["mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "kaiming_init", "(", "self", ".", "conv_s", ")", "\n", "kaiming_init", "(", "self", ".", "conv_t", ")", "\n", "constant_init", "(", "self", ".", "bn_s", ",", "1", ",", "bias", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d_reshape.__init__": [[125, 185], ["dict", "torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "int", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "mmcv.cnn.build_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "resnet2plus1d.Conv2plus1d_reshape.init_weights", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "len", "(", "stride", ")", "==", "len", "(", "padding", ")", "==", "3", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "output_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "self", ".", "transposed", "=", "False", "\n", "\n", "# The middle-plane is calculated according to:", "\n", "# M_i = \\floor{\\frac{t * d^2 N_i-1 * N_i}", "\n", "#   {d^2 * N_i-1 + t * N_i}}", "\n", "# where d, t are spatial and temporal kernel, and", "\n", "# N_i, N_i-1 are planes", "\n", "# and inplanes. https://arxiv.org/pdf/1711.11248.pdf", "\n", "mid_channels", "=", "3", "*", "(", "\n", "in_channels", "*", "out_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", ")", "\n", "mid_channels", "/=", "(", "\n", "in_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "+", "3", "*", "out_channels", ")", "\n", "mid_channels", "=", "int", "(", "mid_channels", ")", "\n", "\n", "\n", "self", ".", "conv_s", "=", "nn", ".", "Conv3d", "(", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "_", ",", "self", ".", "bn_s", "=", "build_norm_layer", "(", "self", ".", "norm_cfg", ",", "mid_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv_t", "=", "nn", ".", "Conv3d", "(", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "self", ".", "mid_channel", "=", "mid_channels", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d_reshape.forward": [[186, 198], ["resnet2plus1d.Conv2plus1d_reshape.conv_s", "resnet2plus1d.Conv2plus1d_reshape.bn_s", "resnet2plus1d.Conv2plus1d_reshape.relu", "resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv_s", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_s", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "reshape_temporal_conv", "(", "self", ".", "conv_t", ",", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d_reshape.init_weights": [[199, 204], ["mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "kaiming_init", "(", "self", ".", "conv_s", ")", "\n", "kaiming_init", "(", "self", ".", "conv_t", ")", "\n", "constant_init", "(", "self", ".", "bn_s", ",", "1", ",", "bias", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv": [[207, 224], ["conv3d_para[].view", "resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal", "torch.conv3d", "torch.conv3d", "torch.conv3d", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "conv3d_para[].view.size", "motion_para.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal"], ["", "def", "reshape_temporal_conv", "(", "self", ",", "conv_3d", ",", "x", ",", "stride", "=", "1", ",", "groups", "=", "1", ")", ":", "\n", "\n", "        ", "conv3d_para", "=", "conv_3d", ".", "weight", "\n", "outplanes", ",", "inplanes", ",", "_", ",", "_", ",", "_", "=", "conv3d_para", ".", "shape", "\n", "stride", "=", "conv_3d", ".", "stride", "\n", "\n", "# reshape 3dconv", "\n", "app_para", "=", "conv3d_para", "[", "0", ":", "outplanes", "//", "2", ",", ":", ",", ":", ",", ":", ",", ":", "]", ".", "view", "(", "outplanes", "//", "2", ",", "inplanes", ",", "1", ",", "-", "1", ")", "\n", "motion_para", "=", "conv3d_para", "[", "outplanes", "//", "2", ":", ",", ":", ",", ":", ",", ":", ",", ":", "]", "\n", "padding_t", "=", "(", "app_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", "\n", "padding_s", "=", "(", "motion_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", "\n", "\n", "out_app", "=", "self", ".", "apperance_pretrain_temporal", "(", "x", ",", "app_para", ",", "padding_t", ",", "stride", "=", "stride", ")", "\n", "out_motion", "=", "F", ".", "conv3d", "(", "x", ",", "weight", "=", "motion_para", ",", "stride", "=", "stride", ",", "groups", "=", "groups", ",", "padding", "=", "(", "padding_t", ",", "padding_s", ",", "padding_s", ")", ")", "\n", "\n", "out", "=", "torch", ".", "cat", "(", "[", "out_app", ",", "out_motion", "]", ",", "dim", "=", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal": [[225, 245], ["x.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "einops.rearrange", "einops.rearrange", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "einops.rearrange", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "apperance_pretrain_temporal", "(", "self", ",", "x", ",", "app_para", ",", "padding_t", ",", "groups", "=", "1", ",", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ")", ":", "\n", "\n", "        ", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "app_kernel_chunks", "=", "torch", ".", "chunk", "(", "app_para", ",", "2", ",", "dim", "=", "0", ")", "\n", "\n", "#reshape to column or row", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> b c t  ( h w)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> b  c t  ( w h)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "\n", "# convolve", "\n", "out_column", "=", "F", ".", "conv2d", "(", "out_column", ",", "weight", "=", "app_kernel_chunks", "[", "0", "]", ",", "stride", "=", "(", "stride", "[", "0", "]", ",", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "0", ",", "padding_t", ")", ")", "\n", "out_row", "=", "F", ".", "conv2d", "(", "out_row", ",", "weight", "=", "app_kernel_chunks", "[", "1", "]", ",", "stride", "=", "(", "stride", "[", "0", "]", ",", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "0", ",", "padding_t", ")", ")", "\n", "\n", "# reshape back to original shape", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "out_column", ",", "'b  c  t ( h w)  -> b c t h w'", ",", "t", "=", "t", "//", "stride", "[", "0", "]", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "out_row", ",", "'b  c  t ( w h)  -> b c t h w'", ",", "t", "=", "t", "//", "stride", "[", "0", "]", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "\n", "# concat along the channel dimension", "\n", "out_app", "=", "torch", ".", "cat", "(", "[", "out_column", ",", "out_row", "]", ",", "dim", "=", "1", ")", "\n", "return", "out_app", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.ResNet2Plus1d.__init__": [[255, 259], ["resnet3d.ResNet3d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "assert", "self", ".", "pretrained2d", "is", "False", "\n", "assert", "self", ".", "conv_cfg", "[", "'type'", "]", "==", "'Conv2plus1d_reshape'", "or", "self", ".", "conv_cfg", "[", "'type'", "]", "==", "'Conv2plus1d'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.ResNet2Plus1d._freeze_stages": [[260, 273], ["range", "resnet2plus1d.ResNet2Plus1d.conv1.eval", "resnet2plus1d.ResNet2Plus1d.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet2plus1d.ResNet2Plus1d.forward": [[274, 292], ["resnet2plus1d.ResNet2Plus1d.conv1", "resnet2plus1d.ResNet2Plus1d.maxpool", "getattr", "getattr."], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The feature of the input\n            samples extracted by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "for", "layer_name", "in", "self", ".", "res_layers", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "# no pool2 in R(2+1)d", "\n", "x", "=", "res_layer", "(", "x", ")", "\n", "\n", "", "return", "x", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.SEModule.__init__": [[19, 29], ["torch.Module.__init__", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "x3d.SEModule._round_width", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_width"], ["    ", "def", "__init__", "(", "self", ",", "channels", ",", "reduction", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "self", ".", "bottleneck", "=", "self", ".", "_round_width", "(", "channels", ",", "reduction", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv3d", "(", "\n", "channels", ",", "self", ".", "bottleneck", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "bottleneck", ",", "channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.SEModule._round_width": [[30, 39], ["max", "int", "int"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_round_width", "(", "width", ",", "multiplier", ",", "min_width", "=", "8", ",", "divisor", "=", "8", ")", ":", "\n", "        ", "width", "*=", "multiplier", "\n", "min_width", "=", "min_width", "or", "divisor", "\n", "width_out", "=", "max", "(", "min_width", ",", "\n", "int", "(", "width", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "if", "width_out", "<", "0.9", "*", "width", ":", "\n", "            ", "width_out", "+=", "divisor", "\n", "", "return", "int", "(", "width_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.SEModule.forward": [[40, 48], ["x3d.SEModule.avg_pool", "x3d.SEModule.fc1", "x3d.SEModule.relu", "x3d.SEModule.fc2", "x3d.SEModule.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "module_input", "=", "x", "\n", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "sigmoid", "(", "x", ")", "\n", "return", "module_input", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.BlockX3D.__init__": [[73, 140], ["dict", "dict", "dict", "torch.Module.__init__", "dict", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.Swish", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "x3d.SEModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "outplanes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "se_ratio", "=", "None", ",", "\n", "use_swish", "=", "True", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "outplanes", "=", "outplanes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "se_ratio", "=", "se_ratio", "\n", "self", ".", "use_swish", "=", "use_swish", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "act_cfg_swish", "=", "dict", "(", "type", "=", "'Swish'", ")", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "in_channels", "=", "inplanes", ",", "\n", "out_channels", "=", "planes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "# Here we use the channel-wise conv", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "in_channels", "=", "planes", ",", "\n", "out_channels", "=", "planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "(", "1", ",", "self", ".", "spatial_stride", ",", "self", ".", "spatial_stride", ")", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "planes", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "swish", "=", "Swish", "(", ")", "\n", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "in_channels", "=", "planes", ",", "\n", "out_channels", "=", "outplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "if", "self", ".", "se_ratio", "is", "not", "None", ":", "\n", "            ", "self", ".", "se_module", "=", "SEModule", "(", "planes", ",", "self", ".", "se_ratio", ")", "\n", "\n", "", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.BlockX3D.forward": [[141, 170], ["x3d.BlockX3D.relu", "x3d.BlockX3D.conv1", "x3d.BlockX3D.conv2", "x3d.BlockX3D.swish", "x3d.BlockX3D.conv3", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "x3d.BlockX3D.forward._inner_forward"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.swish"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "if", "self", ".", "se_ratio", "is", "not", "None", ":", "\n", "                ", "out", "=", "self", ".", "se_module", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "swish", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D.__init__": [[214, 314], ["dict", "dict", "dict", "torch.Module.__init__", "x3d.X3D._round_width", "x3d.X3D._make_stem_layer", "enumerate", "mmcv.cnn.ConvModule", "int", "x3d.X3D._round_repeats", "len", "int", "x3d.X3D.make_res_layer", "x3d.X3D.add_module", "x3d.X3D.res_layers.append", "int", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_width", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_repeats", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["def", "__init__", "(", "self", ",", "\n", "gamma_w", "=", "1.0", ",", "\n", "gamma_b", "=", "1.0", ",", "\n", "gamma_d", "=", "1.0", ",", "\n", "pretrained", "=", "None", ",", "\n", "in_channels", "=", "3", ",", "\n", "num_stages", "=", "4", ",", "\n", "spatial_strides", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "se_style", "=", "'half'", ",", "\n", "se_ratio", "=", "1", "/", "16", ",", "\n", "use_swish", "=", "True", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma_w", "=", "gamma_w", "\n", "self", ".", "gamma_b", "=", "gamma_b", "\n", "self", ".", "gamma_d", "=", "gamma_d", "\n", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "# Hard coded, can be changed by gamma_w", "\n", "self", ".", "base_channels", "=", "24", "\n", "self", ".", "stage_blocks", "=", "[", "1", ",", "2", ",", "5", ",", "3", "]", "\n", "\n", "# apply parameters gamma_w and gamma_d", "\n", "self", ".", "base_channels", "=", "self", ".", "_round_width", "(", "self", ".", "base_channels", ",", "\n", "self", ".", "gamma_w", ")", "\n", "\n", "self", ".", "stage_blocks", "=", "[", "\n", "self", ".", "_round_repeats", "(", "x", ",", "self", ".", "gamma_d", ")", "for", "x", "in", "self", ".", "stage_blocks", "\n", "]", "\n", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "spatial_strides", "=", "spatial_strides", "\n", "assert", "len", "(", "spatial_strides", ")", "==", "num_stages", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "\n", "self", ".", "se_style", "=", "se_style", "\n", "assert", "self", ".", "se_style", "in", "[", "'all'", ",", "'half'", "]", "\n", "self", ".", "se_ratio", "=", "se_ratio", "\n", "assert", "(", "self", ".", "se_ratio", "is", "None", ")", "or", "(", "self", ".", "se_ratio", ">", "0", ")", "\n", "self", ".", "use_swish", "=", "use_swish", "\n", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "self", ".", "block", "=", "BlockX3D", "\n", "self", ".", "stage_blocks", "=", "self", ".", "stage_blocks", "[", ":", "num_stages", "]", "\n", "self", ".", "layer_inplanes", "=", "self", ".", "base_channels", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "spatial_stride", "=", "spatial_strides", "[", "i", "]", "\n", "inplanes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "planes", "=", "int", "(", "inplanes", "*", "self", ".", "gamma_b", ")", "\n", "\n", "res_layer", "=", "self", ".", "make_res_layer", "(", "\n", "self", ".", "block", ",", "\n", "self", ".", "layer_inplanes", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "se_style", "=", "self", ".", "se_style", ",", "\n", "se_ratio", "=", "self", ".", "se_ratio", ",", "\n", "use_swish", "=", "self", ".", "use_swish", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "layer_inplanes", "=", "inplanes", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "base_channels", "*", "2", "**", "(", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "self", ".", "conv5", "=", "ConvModule", "(", "\n", "self", ".", "feat_dim", ",", "\n", "int", "(", "self", ".", "feat_dim", "*", "self", ".", "gamma_b", ")", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "self", ".", "feat_dim", "=", "int", "(", "self", ".", "feat_dim", "*", "self", ".", "gamma_b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D._round_width": [[316, 329], ["max", "int", "int"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_round_width", "(", "width", ",", "multiplier", ",", "min_depth", "=", "8", ",", "divisor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Round width of filters based on width multiplier.\"\"\"", "\n", "if", "not", "multiplier", ":", "\n", "            ", "return", "width", "\n", "\n", "", "width", "*=", "multiplier", "\n", "min_depth", "=", "min_depth", "or", "divisor", "\n", "new_filters", "=", "max", "(", "min_depth", ",", "\n", "int", "(", "width", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "if", "new_filters", "<", "0.9", "*", "width", ":", "\n", "            ", "new_filters", "+=", "divisor", "\n", "", "return", "int", "(", "new_filters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D._round_repeats": [[330, 336], ["int", "math.ceil"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_round_repeats", "(", "repeats", ",", "multiplier", ")", ":", "\n", "        ", "\"\"\"Round number of layers based on depth multiplier.\"\"\"", "\n", "if", "not", "multiplier", ":", "\n", "            ", "return", "repeats", "\n", "", "return", "int", "(", "math", ".", "ceil", "(", "multiplier", "*", "repeats", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D.make_res_layer": [[339, 438], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule", "block", "layers.append", "block", "range"], "methods", ["None"], ["", "def", "make_res_layer", "(", "self", ",", "\n", "block", ",", "\n", "layer_inplanes", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "se_style", "=", "'half'", ",", "\n", "se_ratio", "=", "None", ",", "\n", "use_swish", "=", "True", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Build residual layer for ResNet3D.\n\n        Args:\n            block (nn.Module): Residual module to be built.\n            layer_inplanes (int): Number of channels for the input feature\n                of the res layer.\n            inplanes (int): Number of channels for the input feature in each\n                block, which equals to base_channels * gamma_w.\n            planes (int): Number of channels for the output feature in each\n                block, which equals to base_channel * gamma_w * gamma_b.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int): Spatial strides in residual and conv layers.\n                Default: 1.\n            se_style (str): The style of inserting SE modules into BlockX3D,\n                'half' denotes insert into half of the blocks, while 'all'\n                denotes insert into all blocks. Default: 'half'.\n            se_ratio (float | None): The reduction ratio of squeeze and\n                excitation unit. If set as None, it means not using SE unit.\n                Default: None.\n            use_swish (bool): Whether to use swish as the activation function\n                before and after the 3x3x3 conv. Default: True.\n            conv_cfg (dict | None): Config for norm layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool | None): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n\n        Returns:\n            nn.Module: A residual layer for the given config.\n        \"\"\"", "\n", "downsample", "=", "None", "\n", "if", "spatial_stride", "!=", "1", "or", "layer_inplanes", "!=", "inplanes", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "layer_inplanes", ",", "\n", "inplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "1", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "use_se", "=", "[", "False", "]", "*", "blocks", "\n", "if", "self", ".", "se_style", "==", "'all'", ":", "\n", "            ", "use_se", "=", "[", "True", "]", "*", "blocks", "\n", "", "elif", "self", ".", "se_style", "==", "'half'", ":", "\n", "            ", "use_se", "=", "[", "i", "%", "2", "==", "0", "for", "i", "in", "range", "(", "blocks", ")", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "layer_inplanes", ",", "\n", "planes", ",", "\n", "inplanes", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "downsample", "=", "downsample", ",", "\n", "se_ratio", "=", "se_ratio", "if", "use_se", "[", "0", "]", "else", "None", ",", "\n", "use_swish", "=", "use_swish", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "inplanes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "se_ratio", "=", "se_ratio", "if", "use_se", "[", "i", "]", "else", "None", ",", "\n", "use_swish", "=", "use_swish", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D._make_stem_layer": [[439, 463], ["mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule"], "methods", ["None"], ["", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1_s", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "conv1_t", "=", "ConvModule", "(", "\n", "self", ".", "base_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "(", "5", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "2", ",", "0", ",", "0", ")", ",", "\n", "groups", "=", "self", ".", "base_channels", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D._freeze_stages": [[464, 480], ["range", "x3d.X3D.conv1_s.eval", "x3d.X3D.conv1_t.eval", "x3d.X3D.conv1_s.parameters", "x3d.X3D.conv1_t.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1_s", ".", "eval", "(", ")", "\n", "self", ".", "conv1_t", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1_s", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "param", "in", "self", ".", "conv1_t", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D._init_weights": [[481, 502], ["isinstance", "logging.getLogger", "logging.getLogger.info", "logging.getLogger.info", "mmcv.runner.load_checkpoint", "TypeError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\n        Args:\n            pretrained (str | None): The path of the pretrained weight. Will\n                override the original `pretrained` if set. The arg is added to\n                be compatible with mmdet. Default: None.\n        \"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n", "# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ",", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D.forward": [[503, 520], ["x3d.X3D.conv1_s", "x3d.X3D.conv1_t", "x3d.X3D.conv5", "getattr", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The feature of the input\n            samples extracted by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1_s", "(", "x", ")", "\n", "x", "=", "self", ".", "conv1_t", "(", "x", ")", "\n", "for", "layer_name", "in", "self", ".", "res_layers", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "x", "=", "res_layer", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.x3d.X3D.train": [[522, 530], ["super().train", "x3d.X3D._freeze_stages", "x3d.X3D.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet_csn.CSNBottleneck3d.__init__": [[34, 74], ["resnet3d.Bottleneck3d.__init__", "bool", "mmcv.cnn.ConvModule", "conv2.append", "torch.Sequential", "torch.Sequential", "conv2.append", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "*", "args", ",", "\n", "bottleneck_mode", "=", "'ir'", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "CSNBottleneck3d", ",", "self", ")", ".", "__init__", "(", "inplanes", ",", "planes", ",", "*", "args", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "bottleneck_mode", "=", "bottleneck_mode", "\n", "conv2", "=", "[", "]", "\n", "if", "self", ".", "bottleneck_mode", "==", "'ip'", ":", "\n", "            ", "conv2", ".", "append", "(", "\n", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "stride", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", ")", "\n", "", "conv2_kernel_size", "=", "self", ".", "conv2", ".", "conv", ".", "kernel_size", "\n", "conv2_stride", "=", "self", ".", "conv2", ".", "conv", ".", "stride", "\n", "conv2_padding", "=", "self", ".", "conv2", ".", "conv", ".", "padding", "\n", "conv2_dilation", "=", "self", ".", "conv2", ".", "conv", ".", "dilation", "\n", "conv2_bias", "=", "bool", "(", "self", ".", "conv2", ".", "conv", ".", "bias", ")", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "conv2_stride", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "dilation", "=", "conv2_dilation", ",", "\n", "bias", "=", "conv2_bias", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "groups", "=", "planes", ")", "\n", "conv2", ".", "append", "(", "self", ".", "conv2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "*", "conv2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet_csn.ResNet3dCSN.__init__": [[111, 145], ["dict", "resnet3d.ResNet3d.__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "temporal_strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_kernel", "=", "(", "3", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ",", "eps", "=", "1e-3", ")", ",", "\n", "inflate_style", "=", "'3x3x3'", ",", "\n", "bottleneck_mode", "=", "'ir'", ",", "\n", "bn_frozen", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "arch_settings", "=", "{", "\n", "# 18: (BasicBlock3d, (2, 2, 2, 2)),", "\n", "# 34: (BasicBlock3d, (3, 4, 6, 3)),", "\n", "50", ":", "(", "[", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", "]", ",", "(", "3", ",", "4", ",", "6", ",", "3", ")", ")", ",", "\n", "101", ":", "(", "[", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", "]", ",", "(", "3", ",", "4", ",", "23", ",", "3", ")", ")", ",", "\n", "152", ":", "(", "[", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", ",", "CSNBottleneck3d", "]", ",", "(", "3", ",", "8", ",", "36", ",", "3", ")", ")", "\n", "}", "\n", "self", ".", "bn_frozen", "=", "bn_frozen", "\n", "if", "bottleneck_mode", "not", "in", "[", "'ip'", ",", "'ir'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "f'Bottleneck mode must be \"ip\" or \"ir\",'", "\n", "f'but got {bottleneck_mode}.'", ")", "\n", "", "super", "(", "ResNet3dCSN", ",", "self", ")", ".", "__init__", "(", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "temporal_strides", "=", "temporal_strides", ",", "\n", "conv1_kernel", "=", "conv1_kernel", ",", "\n", "conv1_stride_t", "=", "conv1_stride_t", ",", "\n", "pool1_stride_t", "=", "pool1_stride_t", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "bottleneck_mode", "=", "bottleneck_mode", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet_csn.ResNet3dCSN.train": [[146, 156], ["super().train", "resnet_csn.ResNet3dCSN._freeze_stages", "resnet_csn.ResNet3dCSN.modules", "isinstance", "m.eval", "m.parameters"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", "ResNet3d", ",", "self", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "if", "self", ".", "bn_frozen", ":", "\n", "                        ", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                            ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.BasicBlock3d.__init__": [[54, 141], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "print", "set().issubset", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "mmcv.cnn.NonLocal3d", "set"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "True", ",", "\n", "non_local", "=", "False", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "with_cp", "=", "False", ",", "\n", "drop_path", "=", "None", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "# make sure that only ``inflate_style`` is passed into kwargs", "\n", "print", "(", "kwargs", ")", "\n", "assert", "set", "(", "kwargs", ")", ".", "issubset", "(", "[", "'inflate_style'", "]", ")", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "inflate", "=", "inflate", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "non_local", "=", "non_local", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "self", ".", "conv1_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv2_stride_s", "=", "1", "\n", "self", ".", "conv1_stride_t", "=", "temporal_stride", "\n", "self", ".", "conv2_stride_t", "=", "1", "\n", "\n", "if", "self", ".", "inflate", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv1_padding", "=", "(", "1", ",", "dilation", ",", "dilation", ")", "\n", "conv2_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv1_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "0", ",", "1", ",", "1", ")", "\n", "\n", "", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "conv1_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "conv1_padding", ",", "\n", "dilation", "=", "(", "1", ",", "dilation", ",", "dilation", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv2_stride_t", ",", "self", ".", "conv2_stride_s", ",", "\n", "self", ".", "conv2_stride_s", ")", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "self", ".", "non_local_block", "=", "NonLocal3d", "(", "self", ".", "conv2", ".", "norm", ".", "num_features", ",", "\n", "**", "self", ".", "non_local_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.BasicBlock3d.forward": [[142, 171], ["resnet3d.BasicBlock3d.relu", "resnet3d.BasicBlock3d.conv1", "resnet3d.BasicBlock3d.conv2", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "resnet3d.BasicBlock3d.forward._inner_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ",", "block_id", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "\n", "if", "drop_path", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ",", "drop_path", "=", "self", ".", "drop_path", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "out", "=", "self", ".", "non_local_block", "(", "out", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.Bottleneck3d.__init__": [[203, 315], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "mmcv.cnn.NonLocal3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "True", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "drop_path", "=", "None", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "assert", "inflate_style", "in", "[", "'3x1x1'", ",", "'3x3x3'", "]", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "inflate", "=", "inflate", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "non_local", "=", "non_local", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "\n", "if", "self", ".", "style", "==", "'pytorch'", ":", "\n", "            ", "self", ".", "conv1_stride_s", "=", "1", "\n", "self", ".", "conv2_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv1_stride_t", "=", "1", "\n", "self", ".", "conv2_stride_t", "=", "temporal_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv2_stride_s", "=", "1", "\n", "self", ".", "conv1_stride_t", "=", "temporal_stride", "\n", "self", ".", "conv2_stride_t", "=", "1", "\n", "\n", "", "if", "self", ".", "inflate", ":", "\n", "            ", "if", "inflate_style", "==", "'3x1x1'", ":", "\n", "                ", "conv1_kernel_size", "=", "(", "3", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "1", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "self", ".", "conv1_stride_t", "=", "1", "\n", "conv2_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "", "else", ":", "\n", "                ", "conv1_kernel_size", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "1", ",", "dilation", ",", "dilation", ")", "\n", "", "", "else", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "\n", "", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "conv1_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "conv1_padding", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv2_stride_t", ",", "self", ".", "conv2_stride_s", ",", "\n", "self", ".", "conv2_stride_s", ")", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "dilation", "=", "(", "1", ",", "dilation", ",", "dilation", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "# No activation in the third ConvModule for bottleneck", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "reshape_t", "=", "reshape_t", "\n", "self", ".", "reshape_st", "=", "reshape_st", "\n", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "self", ".", "non_local_block", "=", "NonLocal3d", "(", "self", ".", "conv3", ".", "norm", ".", "num_features", ",", "\n", "**", "self", ".", "non_local_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.Bottleneck3d.forward": [[317, 358], ["resnet3d.Bottleneck3d.relu", "resnet3d.Bottleneck3d.conv3", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "resnet3d.Bottleneck3d.forward._inner_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ",", "drop_path", "=", "None", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "if", "self", ".", "inflate", ":", "\n", "                ", "if", "self", ".", "reshape_t", ":", "\n", "\n", "                    ", "out", "=", "STS_3dconv", "(", "self", ".", "conv2", ",", "x", ")", "\n", "", "else", ":", "\n", "                    ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "\n", "", "", "else", ":", "\n", "                    ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "", "if", "self", ".", "reshape_st", ":", "\n", "                    ", "out", "=", "STS_3dconv", "(", "self", ".", "conv2", ",", "out", ")", "\n", "\n", "", "else", ":", "\n", "                     ", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "if", "drop_path", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ",", "self", ".", "drop_path", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "out", "=", "self", ".", "non_local_block", "(", "out", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.__init__": [[435, 555], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "resnet3d.ResNet3d._make_stem_layer", "enumerate", "KeyError", "max", "len", "len", "len", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "resnet3d.ResNet3d.make_res_layer", "resnet3d.ResNet3d.add_module", "resnet3d.ResNet3d.res_layers.append", "len", "sum", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "stage_blocks", "=", "None", ",", "\n", "pretrained2d", "=", "False", ",", "\n", "in_channels", "=", "3", ",", "\n", "num_stages", "=", "4", ",", "\n", "base_channels", "=", "64", ",", "\n", "out_indices", "=", "(", "3", ",", ")", ",", "\n", "spatial_strides", "=", "(", "1", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "temporal_strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_kernel", "=", "(", "3", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_s", "=", "2", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_s", "=", "2", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "with_pool1", "=", "True", ",", "\n", "with_pool2", "=", "True", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "inflate", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "non_local", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "drop_path_rate", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "depth", "not", "in", "self", ".", "arch_settings", ":", "\n", "            ", "raise", "KeyError", "(", "f'invalid depth {depth} for resnet'", ")", "\n", "", "self", ".", "depth", "=", "depth", "\n", "self", ".", "pretrained2d", "=", "pretrained2d", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "base_channels", "=", "base_channels", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "stage_blocks", "=", "stage_blocks", "\n", "self", ".", "out_indices", "=", "out_indices", "\n", "assert", "max", "(", "out_indices", ")", "<", "num_stages", "\n", "self", ".", "spatial_strides", "=", "spatial_strides", "\n", "self", ".", "temporal_strides", "=", "temporal_strides", "\n", "self", ".", "dilations", "=", "dilations", "\n", "assert", "len", "(", "spatial_strides", ")", "==", "len", "(", "temporal_strides", ")", "==", "len", "(", "\n", "dilations", ")", "==", "num_stages", "\n", "if", "self", ".", "stage_blocks", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "self", ".", "stage_blocks", ")", "==", "num_stages", "\n", "\n", "", "self", ".", "conv1_kernel", "=", "conv1_kernel", "\n", "self", ".", "conv1_stride_s", "=", "conv1_stride_s", "\n", "self", ".", "conv1_stride_t", "=", "conv1_stride_t", "\n", "self", ".", "pool1_stride_s", "=", "pool1_stride_s", "\n", "self", ".", "pool1_stride_t", "=", "pool1_stride_t", "\n", "self", ".", "with_pool1", "=", "with_pool1", "\n", "self", ".", "with_pool2", "=", "with_pool2", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "self", ".", "stage_inflations", "=", "_ntuple", "(", "num_stages", ")", "(", "inflate", ")", "\n", "self", ".", "non_local_stages", "=", "_ntuple", "(", "num_stages", ")", "(", "non_local", ")", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "self", ".", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "\n", "if", "self", ".", "stage_blocks", "is", "None", ":", "\n", "            ", "self", ".", "stage_blocks", "=", "stage_blocks", "[", ":", "num_stages", "]", "\n", "\n", "", "self", ".", "inplanes", "=", "self", ".", "base_channels", "\n", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "net_block_idx", "=", "0", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "spatial_stride", "=", "spatial_strides", "[", "i", "]", "\n", "temporal_stride", "=", "temporal_strides", "[", "i", "]", "\n", "dilation", "=", "dilations", "[", "i", "]", "\n", "planes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "res_layer", ",", "net_block_idx", "=", "self", ".", "make_res_layer", "(", "\n", "self", ".", "block", "[", "i", "]", ",", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "self", ".", "style", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "non_local", "=", "self", ".", "non_local_stages", "[", "i", "]", ",", "\n", "non_local_cfg", "=", "self", ".", "non_local_cfg", ",", "\n", "inflate", "=", "self", ".", "stage_inflations", "[", "i", "]", ",", "\n", "inflate_style", "=", "self", ".", "inflate_style", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path_rate", "=", "drop_path_rate", ",", "\n", "net_num_blocks", "=", "sum", "(", "self", ".", "stage_blocks", ")", ",", "\n", "net_block_idx", "=", "net_block_idx", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", "[", "i", "]", ".", "expansion", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "block", "[", "i", "]", ".", "expansion", "*", "self", ".", "base_channels", "*", "2", "**", "(", "\n", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "", "@", "staticmethod", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.make_res_layer": [[555, 683], ["dict", "layers.append", "range", "mmcv.cnn.ConvModule", "block", "layers.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "isinstance", "isinstance", "len", "len", "block", "layers.DropPath", "layers.DropPath"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "make_res_layer", "(", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "0", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "net_num_blocks", "=", "16", ",", "\n", "net_block_idx", "=", "0", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "pretrained", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Build residual layer for ResNet3D.\n        Args:\n            block (nn.Module): Residual module to be built.\n            inplanes (int): Number of channels for the input feature\n                in each block.\n            planes (int): Number of channels for the output feature\n                in each block.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int | Sequence[int]): Spatial strides in\n                residual and conv layers. Default: 1.\n            temporal_stride (int | Sequence[int]): Temporal strides in\n                residual and conv layers. Default: 1.\n            dilation (int): Spacing between kernel elements. Default: 1.\n            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,\n                the stride-two layer is the 3x3 conv layer, otherwise\n                the stride-two layer is the first 1x1 conv layer.\n                Default: ``pytorch``.\n            inflate (int | Sequence[int]): Determine whether to inflate\n                for each block. Default: 1.\n            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines\n                the kernel sizes and padding strides for conv1 and conv2\n                in each block. Default: '3x1x1'.\n            non_local (int | Sequence[int]): Determine whether to apply\n                non-local module in the corresponding block of each stages.\n                Default: 0.\n            non_local_cfg (dict): Config for non-local module.\n                Default: ``dict()``.\n            conv_cfg (dict | None): Config for norm layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool | None): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n        Returns:\n            nn.Module: A residual layer for the given config.\n\n        \"\"\"", "\n", "inflate", "=", "inflate", "if", "not", "isinstance", "(", "inflate", ",", "\n", "int", ")", "else", "(", "inflate", ",", ")", "*", "blocks", "\n", "non_local", "=", "non_local", "if", "not", "isinstance", "(", "\n", "non_local", ",", "int", ")", "else", "(", "non_local", ",", ")", "*", "blocks", "\n", "assert", "len", "(", "inflate", ")", "==", "blocks", "and", "len", "(", "non_local", ")", "==", "blocks", "\n", "downsample", "=", "None", "\n", "if", "spatial_stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "temporal_stride", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "downsample", "=", "downsample", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "0", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "0", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", "**", "kwargs", ")", ")", "\n", "net_block_idx", "+=", "1", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "i", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "i", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", "**", "kwargs", ")", ")", "\n", "net_block_idx", "+=", "1", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", ",", "net_block_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._inflate_conv_params": [[686, 712], ["conv3d.weight.data.copy_", "inflated_param_names.append", "conv2d_weight.data.unsqueeze().expand_as", "getattr", "conv3d.bias.data.copy_", "inflated_param_names.append", "conv2d_weight.data.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_inflate_conv_params", "(", "conv3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        ", "\"\"\"Inflate a conv module from 2d to 3d.\n        Args:\n            conv3d (nn.Module): The destination conv3d module.\n            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n            module_name_2d (str): The name of corresponding conv module in the\n                2d model.\n            inflated_param_names (list[str]): List of parameters that have been\n                inflated.\n        \"\"\"", "\n", "weight_2d_name", "=", "module_name_2d", "+", "'.weight'", "\n", "\n", "conv2d_weight", "=", "state_dict_2d", "[", "weight_2d_name", "]", "\n", "kernel_t", "=", "conv3d", ".", "weight", ".", "data", ".", "shape", "[", "2", "]", "\n", "\n", "new_weight", "=", "conv2d_weight", ".", "data", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "conv3d", ".", "weight", ")", "/", "kernel_t", "\n", "conv3d", ".", "weight", ".", "data", ".", "copy_", "(", "new_weight", ")", "\n", "inflated_param_names", ".", "append", "(", "weight_2d_name", ")", "\n", "\n", "if", "getattr", "(", "conv3d", ",", "'bias'", ")", "is", "not", "None", ":", "\n", "            ", "bias_2d_name", "=", "module_name_2d", "+", "'.bias'", "\n", "conv3d", ".", "bias", ".", "data", ".", "copy_", "(", "state_dict_2d", "[", "bias_2d_name", "]", ")", "\n", "inflated_param_names", ".", "append", "(", "bias_2d_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._inflate_bn_params": [[713, 744], ["bn3d.named_parameters", "bn3d.named_buffers", "param.data.copy_", "inflated_param_names.append", "warnings.warn", "param.data.copy_", "inflated_param_names.append"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_inflate_bn_params", "(", "bn3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        ", "\"\"\"Inflate a norm module from 2d to 3d.\n        Args:\n            bn3d (nn.Module): The destination bn3d module.\n            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n            module_name_2d (str): The name of corresponding bn module in the\n                2d model.\n            inflated_param_names (list[str]): List of parameters that have been\n                inflated.\n        \"\"\"", "\n", "for", "param_name", ",", "param", "in", "bn3d", ".", "named_parameters", "(", ")", ":", "\n", "            ", "param_2d_name", "=", "f'{module_name_2d}.{param_name}'", "\n", "param_2d", "=", "state_dict_2d", "[", "param_2d_name", "]", "\n", "if", "param", ".", "data", ".", "shape", "!=", "param_2d", ".", "shape", ":", "\n", "                ", "warnings", ".", "warn", "(", "f'The parameter of {module_name_2d} is not'", "\n", "'loaded due to incompatible shapes. '", ")", "\n", "return", "\n", "\n", "", "param", ".", "data", ".", "copy_", "(", "param_2d", ")", "\n", "inflated_param_names", ".", "append", "(", "param_2d_name", ")", "\n", "\n", "", "for", "param_name", ",", "param", "in", "bn3d", ".", "named_buffers", "(", ")", ":", "\n", "            ", "param_2d_name", "=", "f'{module_name_2d}.{param_name}'", "\n", "# some buffers like num_batches_tracked may not exist in old", "\n", "# checkpoints", "\n", "if", "param_2d_name", "in", "state_dict_2d", ":", "\n", "                ", "param_2d", "=", "state_dict_2d", "[", "param_2d_name", "]", "\n", "param", ".", "data", ".", "copy_", "(", "param_2d", ")", "\n", "inflated_param_names", ".", "append", "(", "param_2d_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._inflate_weights": [[745, 806], ["mmcv.runner._load_checkpoint", "resnet3d.ResNet3d.named_modules", "isinstance", "set", "set", "logger.info", "mmcv.runner._load_checkpoint.keys", "name.replace", "logger.warning", "logger.warning", "resnet3d.ResNet3d._inflate_bn_params", "logger.warning", "resnet3d.ResNet3d._inflate_conv_params"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_bn_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params"], ["", "", "", "@", "staticmethod", "\n", "def", "_inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "\"\"\"Inflate the resnet2d parameters to resnet3d.\n        The differences between resnet3d and resnet2d mainly lie in an extra\n        axis of conv kernel. To utilize the pretrained parameters in 2d model,\n        the weight of conv2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging information.\n        \"\"\"", "\n", "\n", "state_dict_r2d", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict_r2d", ":", "\n", "            ", "state_dict_r2d", "=", "state_dict_r2d", "[", "'state_dict'", "]", "\n", "\n", "", "inflated_param_names", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "ConvModule", ")", ":", "\n", "# we use a ConvModule to wrap conv+bn+relu layers, thus the", "\n", "# name mapping is needed", "\n", "                ", "if", "'downsample'", "in", "name", ":", "\n", "# layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0", "\n", "                    ", "original_conv_name", "=", "name", "+", "'.0'", "\n", "# layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1", "\n", "original_bn_name", "=", "name", "+", "'.1'", "\n", "", "else", ":", "\n", "# layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}", "\n", "                    ", "original_conv_name", "=", "name", "\n", "# layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}", "\n", "original_bn_name", "=", "name", ".", "replace", "(", "'conv'", ",", "'bn'", ")", "\n", "", "if", "original_conv_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_conv_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "shape_2d", "=", "state_dict_r2d", "[", "original_conv_name", "+", "\n", "'.weight'", "]", ".", "shape", "\n", "shape_3d", "=", "module", ".", "conv", ".", "weight", ".", "data", ".", "shape", "\n", "if", "shape_2d", "!=", "shape_3d", "[", ":", "2", "]", "+", "shape_3d", "[", "3", ":", "]", ":", "\n", "                        ", "logger", ".", "warning", "(", "f'Weight shape mismatch for '", "\n", "f': {original_conv_name} : '", "\n", "f'3d weight shape: {shape_3d}; '", "\n", "f'2d weight shape: {shape_2d}. '", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_inflate_conv_params", "(", "module", ".", "conv", ",", "state_dict_r2d", ",", "\n", "original_conv_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "", "", "if", "original_bn_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_bn_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_bn_params", "(", "module", ".", "bn", ",", "state_dict_r2d", ",", "\n", "original_bn_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "# check if any parameters in the 2d checkpoint are not loaded", "\n", "", "", "", "remaining_names", "=", "set", "(", "\n", "state_dict_r2d", ".", "keys", "(", ")", ")", "-", "set", "(", "inflated_param_names", ")", "\n", "if", "remaining_names", ":", "\n", "            ", "logger", ".", "info", "(", "f'These parameters in the 2d checkpoint are not loaded'", "\n", "f': {remaining_names}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.inflate_weights": [[808, 810], ["resnet3d.ResNet3d._inflate_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_weights"], ["", "", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "self", ".", "_inflate_weights", "(", "self", ",", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._make_stem_layer": [[811, 833], ["mmcv.cnn.ConvModule", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "tuple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple"], "methods", ["None"], ["", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "self", ".", "conv1_kernel", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "tuple", "(", "[", "(", "k", "-", "1", ")", "//", "2", "for", "k", "in", "_triple", "(", "self", ".", "conv1_kernel", ")", "]", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "self", ".", "pool1_stride_t", ",", "self", ".", "pool1_stride_s", ",", "\n", "self", ".", "pool1_stride_s", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "1", ",", "1", ")", ",", "stride", "=", "(", "2", ",", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._freeze_stages": [[834, 847], ["range", "resnet3d.ResNet3d.conv1.eval", "resnet3d.ResNet3d.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d._init_weights": [[848, 893], ["isinstance", "logging.getLogger", "logging.getLogger.info", "logging.getLogger.info", "resnet3d.ResNet3d.inflate_weights", "logging.getLogger.info", "mmcv.runner.load_checkpoint", "resnet3d.ResNet3d.modules", "TypeError", "isinstance", "resnet3d.ResNet3d.modules", "mmcv.cnn.kaiming_init", "isinstance", "isinstance", "mmcv.cnn.constant_init", "mmcv.cnn.constant_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.inflate_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\n        Args:\n            pretrained (str | None): The path of the pretrained weight. Will\n                override the original `pretrained` if set. The arg is added to\n                be compatible with mmdet. Default: None.\n        \"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "if", "self", ".", "pretrained2d", ":", "\n", "                ", "logger", ".", "info", "(", "f'load 2D model using inflate weights'", ")", "\n", "# Inflate 2D model into 3D model.", "\n", "self", ".", "inflate_weights", "(", "logger", ")", "\n", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n", "# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "\n", "strict", "=", "False", ",", "\n", "logger", "=", "logger", ",", "\n", "map_location", "=", "'cpu'", ",", "\n", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", ",", "[", "'get_feature.'", ",", "''", "]", "]", ")", "\n", "\n", "", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n", "", "", "if", "self", ".", "zero_init_residual", ":", "\n", "                ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "m", ",", "Bottleneck3d", ")", ":", "\n", "                        ", "constant_init", "(", "m", ".", "conv3", ".", "bn", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock3d", ")", ":", "\n", "                        ", "constant_init", "(", "m", ".", "conv2", ".", "bn", ",", "0", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.init_weights": [[894, 896], ["resnet3d.ResNet3d._init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights"], ["", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "self", ".", "_init_weights", "(", "self", ",", "pretrained", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.forward": [[897, 923], ["resnet3d.ResNet3d.conv1", "enumerate", "tuple", "resnet3d.ResNet3d.maxpool", "getattr", "len", "block", "resnet3d.ResNet3d.pool2", "outs.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            torch.Tensor: The feature of the input\n            samples extracted by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "if", "self", ".", "with_pool1", ":", "\n", "            ", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "", "outs", "=", "[", "]", "\n", "idx", "=", "1", "\n", "for", "i", ",", "layer_name", "in", "enumerate", "(", "self", ".", "res_layers", ")", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "for", "block", "in", "res_layer", ":", "\n", "                ", "x", "=", "block", "(", "x", ",", ")", "\n", "idx", "+=", "1", "\n", "", "if", "i", "==", "0", "and", "self", ".", "with_pool2", ":", "\n", "                ", "x", "=", "self", ".", "pool2", "(", "x", ")", "\n", "", "if", "i", "in", "self", ".", "out_indices", ":", "\n", "                ", "outs", ".", "append", "(", "x", ")", "\n", "", "", "if", "len", "(", "outs", ")", "==", "1", ":", "\n", "            ", "return", "outs", "[", "0", "]", "\n", "\n", "", "return", "tuple", "(", "outs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3d.train": [[924, 932], ["super().train", "resnet3d.ResNet3d._freeze_stages", "resnet3d.ResNet3d.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer.__init__": [[972, 1051], ["dict", "dict", "dict", "torch.Module.__init__", "resnet3d.ResNet3dLayer.make_res_layer", "resnet3d.ResNet3dLayer.add_module"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "pretrained2d", "=", "True", ",", "\n", "stage", "=", "3", ",", "\n", "base_channels", "=", "64", ",", "\n", "spatial_stride", "=", "2", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "all_frozen", "=", "False", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "arch_settings", "=", "ResNet3d", ".", "arch_settings", "\n", "assert", "depth", "in", "self", ".", "arch_settings", "\n", "\n", "self", ".", "make_res_layer", "=", "ResNet3d", ".", "make_res_layer", "\n", "self", ".", "_inflate_conv_params", "=", "ResNet3d", ".", "_inflate_conv_params", "\n", "self", ".", "_inflate_bn_params", "=", "ResNet3d", ".", "_inflate_bn_params", "\n", "self", ".", "_inflate_weights", "=", "ResNet3d", ".", "_inflate_weights", "\n", "self", ".", "_init_weights", "=", "ResNet3d", ".", "_init_weights", "\n", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "pretrained2d", "=", "pretrained2d", "\n", "self", ".", "stage", "=", "stage", "\n", "# stage index is 0 based", "\n", "assert", "0", "<=", "stage", "<=", "3", "\n", "self", ".", "base_channels", "=", "base_channels", "\n", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "all_frozen", "=", "all_frozen", "\n", "\n", "self", ".", "stage_inflation", "=", "inflate", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "stage_block", "=", "stage_blocks", "[", "stage", "]", "\n", "planes", "=", "64", "*", "2", "**", "stage", "\n", "inplanes", "=", "64", "*", "2", "**", "(", "stage", "-", "1", ")", "*", "block", ".", "expansion", "\n", "\n", "res_layer", "=", "self", ".", "make_res_layer", "(", "\n", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stage_block", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "self", ".", "style", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "inflate", "=", "self", ".", "stage_inflation", ",", "\n", "inflate_style", "=", "self", ".", "inflate_style", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", "\n", "\n", "self", ".", "layer_name", "=", "f'layer{stage + 1}'", "\n", "self", ".", "add_module", "(", "self", ".", "layer_name", ",", "res_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer.inflate_weights": [[1052, 1054], ["resnet3d.ResNet3dLayer._inflate_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_weights"], ["", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "self", ".", "_inflate_weights", "(", "self", ",", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer._freeze_stages": [[1055, 1063], ["getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "all_frozen", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "self", ".", "layer_name", ")", "\n", "layer", ".", "eval", "(", ")", "\n", "for", "param", "in", "layer", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer.init_weights": [[1064, 1066], ["resnet3d.ResNet3dLayer._init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights"], ["", "", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "self", ".", "_init_weights", "(", "self", ",", "pretrained", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer.forward": [[1067, 1078], ["getattr", "getattr."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            torch.Tensor: The feature of the input\n            samples extracted by the backbone.\n        \"\"\"", "\n", "res_layer", "=", "getattr", "(", "self", ",", "self", ".", "layer_name", ")", "\n", "out", "=", "res_layer", "(", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d.ResNet3dLayer.train": [[1079, 1087], ["super().train", "resnet3d.ResNet3dLayer._freeze_stages", "resnet3d.ResNet3dLayer.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway.__init__": [[32, 81], ["resnet3d.ResNet3d.__init__", "range", "mmcv.cnn.ConvModule", "len", "setattr", "resnet3d_slowfast.ResNet3dPathway.lateral_connections.append", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "*", "args", ",", "\n", "lateral", "=", "False", ",", "\n", "speed_ratio", "=", "8", ",", "\n", "channel_ratio", "=", "8", ",", "\n", "fusion_kernel", "=", "5", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "lateral", "=", "lateral", "\n", "self", ".", "speed_ratio", "=", "speed_ratio", "\n", "self", ".", "channel_ratio", "=", "channel_ratio", "\n", "self", ".", "fusion_kernel", "=", "fusion_kernel", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "inplanes", "=", "self", ".", "base_channels", "\n", "if", "self", ".", "lateral", ":", "\n", "            ", "self", ".", "conv1_lateral", "=", "ConvModule", "(", "\n", "self", ".", "inplanes", "//", "self", ".", "channel_ratio", ",", "\n", "# https://arxiv.org/abs/1812.03982, the", "\n", "# third type of lateral connection has out_channel:", "\n", "# 2 * \\beta * C", "\n", "self", ".", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", ",", "\n", "kernel_size", "=", "(", "fusion_kernel", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "self", ".", "speed_ratio", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "(", "fusion_kernel", "-", "1", ")", "//", "2", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "self", ".", "lateral_connections", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "stage_blocks", ")", ")", ":", "\n", "            ", "planes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", "[", "i", "]", ".", "expansion", "\n", "\n", "if", "lateral", "and", "i", "!=", "self", ".", "num_stages", "-", "1", ":", "\n", "# no lateral connection needed in final stage", "\n", "                ", "lateral_name", "=", "f'layer{(i + 1)}_lateral'", "\n", "setattr", "(", "\n", "self", ",", "lateral_name", ",", "\n", "ConvModule", "(", "\n", "self", ".", "inplanes", "//", "self", ".", "channel_ratio", ",", "\n", "self", ".", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", ",", "\n", "kernel_size", "=", "(", "fusion_kernel", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "(", "fusion_kernel", "-", "1", ")", "//", "2", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", ")", "\n", "self", ".", "lateral_connections", ".", "append", "(", "lateral_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway.make_res_layer": [[82, 214], ["dict", "layers.append", "range", "mmcv.cnn.ConvModule", "block", "layers.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "isinstance", "isinstance", "len", "len", "block", "layers.DropPath", "layers.DropPath"], "methods", ["None"], ["", "", "", "def", "make_res_layer", "(", "self", ",", "\n", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "0", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "net_num_blocks", "=", "16", ",", "\n", "net_block_idx", "=", "0", ",", "\n", "pretrained", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"Build residual layer for Slowfast.\n        Args:\n            block (nn.Module): Residual module to be built.\n            inplanes (int): Number of channels for the input\n                feature in each block.\n            planes (int): Number of channels for the output\n                feature in each block.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int | Sequence[int]): Spatial strides\n                in residual and conv layers. Default: 1.\n            temporal_stride (int | Sequence[int]): Temporal strides in\n                residual and conv layers. Default: 1.\n            dilation (int): Spacing between kernel elements. Default: 1.\n            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,\n                the stride-two layer is the 3x3 conv layer,\n                otherwise the stride-two layer is the first 1x1 conv layer.\n                Default: ``pytorch``.\n            inflate (int | Sequence[int]): Determine whether to inflate\n                for each block. Default: 1.\n            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines\n                the kernel sizes and padding strides for conv1 and\n                conv2 in each block. Default: ``3x1x1``.\n            non_local (int | Sequence[int]): Determine whether to apply\n                non-local module in the corresponding block of each stages.\n                Default: 0.\n            non_local_cfg (dict): Config for non-local module.\n                Default: ``dict()``.\n            conv_cfg (dict | None): Config for conv layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool): Use checkpoint or not. Using checkpoint will save\n                some memory while slowing down the training speed.\n                Default: False.\n        Returns:\n            nn.Module: A residual layer for the given config.\n        \"\"\"", "\n", "inflate", "=", "inflate", "if", "not", "isinstance", "(", "inflate", ",", "\n", "int", ")", "else", "(", "inflate", ",", ")", "*", "blocks", "\n", "non_local", "=", "non_local", "if", "not", "isinstance", "(", "\n", "non_local", ",", "int", ")", "else", "(", "non_local", ",", ")", "*", "blocks", "\n", "assert", "len", "(", "inflate", ")", "==", "blocks", "and", "len", "(", "non_local", ")", "==", "blocks", "\n", "if", "self", ".", "lateral", ":", "\n", "            ", "lateral_inplanes", "=", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", "\n", "", "else", ":", "\n", "            ", "lateral_inplanes", "=", "0", "\n", "", "if", "(", "spatial_stride", "!=", "1", "\n", "or", "(", "inplanes", "+", "lateral_inplanes", ")", "!=", "planes", "*", "block", ".", "expansion", ")", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", "+", "lateral_inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "temporal_stride", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "", "else", ":", "\n", "            ", "downsample", "=", "None", "\n", "\n", "", "layers", "=", "[", "]", "\n", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", "+", "lateral_inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", ",", "\n", "temporal_stride", ",", "\n", "dilation", ",", "\n", "downsample", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "0", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "0", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", ")", ")", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "net_block_idx", "+=", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "1", ",", "\n", "dilation", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "i", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "i", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", ")", ")", "\n", "net_block_idx", "+=", "1", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", ",", "net_block_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway.inflate_weights": [[215, 268], ["mmcv.runner._load_checkpoint", "resnet3d_slowfast.ResNet3dPathway.named_modules", "isinstance", "set", "set", "logger.info", "mmcv.runner._load_checkpoint.keys", "name.replace", "logger.warning", "resnet3d_slowfast.ResNet3dPathway._inflate_conv_params", "logger.warning", "resnet3d_slowfast.ResNet3dPathway._inflate_bn_params"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_bn_params"], ["", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "\"\"\"Inflate the resnet2d parameters to resnet3d pathway.\n        The differences between resnet3d and resnet2d mainly lie in an extra\n        axis of conv kernel. To utilize the pretrained parameters in 2d model,\n        the weight of conv2d models should be inflated to fit in the shapes of\n        the 3d counterpart. For pathway the ``lateral_connection`` part should\n        not be inflated from 2d weights.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging information.\n        \"\"\"", "\n", "\n", "state_dict_r2d", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict_r2d", ":", "\n", "            ", "state_dict_r2d", "=", "state_dict_r2d", "[", "'state_dict'", "]", "\n", "\n", "", "inflated_param_names", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "'lateral'", "in", "name", ":", "\n", "                ", "continue", "\n", "", "if", "isinstance", "(", "module", ",", "ConvModule", ")", ":", "\n", "# we use a ConvModule to wrap conv+bn+relu layers, thus the", "\n", "# name mapping is needed", "\n", "                ", "if", "'downsample'", "in", "name", ":", "\n", "# layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0", "\n", "                    ", "original_conv_name", "=", "name", "+", "'.0'", "\n", "# layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1", "\n", "original_bn_name", "=", "name", "+", "'.1'", "\n", "", "else", ":", "\n", "# layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}", "\n", "                    ", "original_conv_name", "=", "name", "\n", "# layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}", "\n", "original_bn_name", "=", "name", ".", "replace", "(", "'conv'", ",", "'bn'", ")", "\n", "", "if", "original_conv_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_conv_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_conv_params", "(", "module", ".", "conv", ",", "state_dict_r2d", ",", "\n", "original_conv_name", ",", "\n", "inflated_param_names", ")", "\n", "", "if", "original_bn_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_bn_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_bn_params", "(", "module", ".", "bn", ",", "state_dict_r2d", ",", "\n", "original_bn_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "# check if any parameters in the 2d checkpoint are not loaded", "\n", "", "", "", "remaining_names", "=", "set", "(", "\n", "state_dict_r2d", ".", "keys", "(", ")", ")", "-", "set", "(", "inflated_param_names", ")", "\n", "if", "remaining_names", ":", "\n", "            ", "logger", ".", "info", "(", "f'These parameters in the 2d checkpoint are not loaded'", "\n", "f': {remaining_names}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params": [[270, 316], ["conv3d.weight.data.copy_", "inflated_param_names.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.data.unsqueeze().expand_as", "torch.cat.data.unsqueeze().expand_as", "torch.cat.data.unsqueeze().expand_as", "getattr", "conv3d.bias.data.copy_", "inflated_param_names.append", "warnings.warn", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.cat.data.unsqueeze", "torch.cat.data.unsqueeze", "torch.cat.data.unsqueeze", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "", "def", "_inflate_conv_params", "(", "self", ",", "conv3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        ", "\"\"\"Inflate a conv module from 2d to 3d.\n        The differences of conv modules betweene 2d and 3d in Pathway\n        mainly lie in the inplanes due to lateral connections. To fit the\n        shapes of the lateral connection counterpart, it will expand\n        parameters by concatting conv2d parameters and extra zero paddings.\n        Args:\n            conv3d (nn.Module): The destination conv3d module.\n            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n            module_name_2d (str): The name of corresponding conv module in the\n                2d model.\n            inflated_param_names (list[str]): List of parameters that have been\n                inflated.\n        \"\"\"", "\n", "weight_2d_name", "=", "module_name_2d", "+", "'.weight'", "\n", "conv2d_weight", "=", "state_dict_2d", "[", "weight_2d_name", "]", "\n", "old_shape", "=", "conv2d_weight", ".", "shape", "\n", "new_shape", "=", "conv3d", ".", "weight", ".", "data", ".", "shape", "\n", "kernel_t", "=", "new_shape", "[", "2", "]", "\n", "\n", "if", "new_shape", "[", "1", "]", "!=", "old_shape", "[", "1", "]", ":", "\n", "            ", "if", "new_shape", "[", "1", "]", "<", "old_shape", "[", "1", "]", ":", "\n", "                ", "warnings", ".", "warn", "(", "f'The parameter of {module_name_2d} is not'", "\n", "'loaded due to incompatible shapes. '", ")", "\n", "return", "\n", "# Inplanes may be different due to lateral connections", "\n", "", "new_channels", "=", "new_shape", "[", "1", "]", "-", "old_shape", "[", "1", "]", "\n", "pad_shape", "=", "old_shape", "\n", "pad_shape", "=", "pad_shape", "[", ":", "1", "]", "+", "(", "new_channels", ",", ")", "+", "pad_shape", "[", "2", ":", "]", "\n", "# Expand parameters by concat extra channels", "\n", "conv2d_weight", "=", "torch", ".", "cat", "(", "\n", "(", "conv2d_weight", ",", "\n", "torch", ".", "zeros", "(", "pad_shape", ")", ".", "type_as", "(", "conv2d_weight", ")", ".", "to", "(", "\n", "conv2d_weight", ".", "device", ")", ")", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "", "new_weight", "=", "conv2d_weight", ".", "data", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "conv3d", ".", "weight", ")", "/", "kernel_t", "\n", "conv3d", ".", "weight", ".", "data", ".", "copy_", "(", "new_weight", ")", "\n", "inflated_param_names", ".", "append", "(", "weight_2d_name", ")", "\n", "\n", "if", "getattr", "(", "conv3d", ",", "'bias'", ")", "is", "not", "None", ":", "\n", "            ", "bias_2d_name", "=", "module_name_2d", "+", "'.bias'", "\n", "conv3d", ".", "bias", ".", "data", ".", "copy_", "(", "state_dict_2d", "[", "bias_2d_name", "]", ")", "\n", "inflated_param_names", ".", "append", "(", "bias_2d_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway._freeze_stages": [[317, 338], ["range", "resnet3d_slowfast.ResNet3dPathway.conv1.eval", "resnet3d_slowfast.ResNet3dPathway.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters", "getattr", "getattr.eval", "getattr.parameters", "len"], "methods", ["None"], ["", "", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        `self.frozen_stages`.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "if", "i", "!=", "len", "(", "self", ".", "res_layers", ")", "and", "self", ".", "lateral", ":", "\n", "# No fusion needed in the final stage", "\n", "                ", "lateral_name", "=", "self", ".", "lateral_connections", "[", "i", "-", "1", "]", "\n", "conv_lateral", "=", "getattr", "(", "self", ",", "lateral_name", ")", "\n", "conv_lateral", ".", "eval", "(", ")", "\n", "for", "param", "in", "conv_lateral", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dPathway.init_weights": [[339, 352], ["super().init_weights", "getattr", "getattr.modules", "isinstance", "mmcv.cnn.kaiming_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "", "", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "\n", "# Override the init_weights of i3d", "\n", "", "super", "(", ")", ".", "init_weights", "(", ")", "\n", "for", "module_name", "in", "self", ".", "lateral_connections", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "module_name", ")", "\n", "for", "m", "in", "layer", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "(", "nn", ".", "Conv3d", ",", "nn", ".", "Conv2d", ")", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dSlowFast.__init__": [[419, 461], ["dict", "dict", "torch.Module.__init__", "resnet3d_slowfast.build_pathway", "resnet3d_slowfast.build_pathway"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.build_pathway", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.build_pathway"], ["def", "__init__", "(", "self", ",", "\n", "pretrained", "=", "False", ",", "\n", "resample_rate", "=", "8", ",", "\n", "speed_ratio", "=", "8", ",", "\n", "channel_ratio", "=", "8", ",", "\n", "slow_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "True", ",", "\n", "reshape_t", "=", "True", ",", "\n", "reshape_st", "=", "False", ",", "\n", "conv1_kernel", "=", "(", "1", ",", "7", ",", "7", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "inflate", "=", "(", "0", ",", "0", ",", "1", ",", "1", ")", ",", "\n", "drop_path_rate", "=", "0.", ")", ",", "\n", "fast_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "False", ",", "\n", "reshape_t", "=", "True", ",", "\n", "reshape_st", "=", "False", ",", "\n", "base_channels", "=", "8", ",", "\n", "conv1_kernel", "=", "(", "5", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "drop_path_rate", "=", "0.", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "resample_rate", "=", "resample_rate", "\n", "self", ".", "speed_ratio", "=", "speed_ratio", "\n", "self", ".", "channel_ratio", "=", "channel_ratio", "\n", "\n", "if", "slow_pathway", "[", "'lateral'", "]", ":", "\n", "            ", "slow_pathway", "[", "'speed_ratio'", "]", "=", "speed_ratio", "\n", "slow_pathway", "[", "'channel_ratio'", "]", "=", "channel_ratio", "\n", "\n", "", "self", ".", "slow_path", "=", "build_pathway", "(", "slow_pathway", ")", "\n", "self", ".", "fast_path", "=", "build_pathway", "(", "fast_pathway", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dSlowFast.init_weights": [[462, 474], ["resnet3d_slowfast.ResNet3dSlowFast.fast_path.init_weights", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.init_weights", "TypeError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "\n", "", "if", "self", ".", "pretrained", "is", "None", ":", "\n", "# Init two branch separately.", "\n", "            ", "self", ".", "fast_path", ".", "init_weights", "(", ")", "\n", "self", ".", "slow_path", ".", "init_weights", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dSlowFast._init_weights": [[475, 495], ["isinstance", "logging.getLogger", "logging.getLogger.info", "logging.getLogger.info", "mmcv.runner.load_checkpoint", "TypeError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\n        Args:\n            pretrained (str | None): The path of the pretrained weight. Will\n                override the original `pretrained` if set. The arg is added to\n                be compatible with mmdet. Default: None.\n        \"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n", "# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ",", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", ",", "(", "'get_feature.'", ",", "''", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.ResNet3dSlowFast.forward": [[496, 548], ["resnet3d_slowfast.ResNet3dSlowFast.slow_path.conv1", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.maxpool", "torch.functional.interpolate", "torch.functional.interpolate", "torch.functional.interpolate", "resnet3d_slowfast.ResNet3dSlowFast.fast_path.conv1", "resnet3d_slowfast.ResNet3dSlowFast.fast_path.maxpool", "enumerate", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.conv1_lateral", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "getattr", "getattr", "block", "block", "getattr", "getattr.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            tuple[torch.Tensor]: The feature of the input samples extracted\n                by the backbone.\n        \"\"\"", "\n", "# x_slow = nn.functional.interpolate(", "\n", "#     x,", "\n", "#     mode='nearest',", "\n", "#     scale_factor=(1.0 / self.resample_rate, 1.0, 1.0))", "\n", "x_slow", "=", "self", ".", "slow_path", ".", "conv1", "(", "x", ")", "\n", "x_slow", "=", "self", ".", "slow_path", ".", "maxpool", "(", "x_slow", ")", "\n", "\n", "x_fast", "=", "nn", ".", "functional", ".", "interpolate", "(", "\n", "x", ",", "\n", "mode", "=", "'nearest'", ",", "\n", "scale_factor", "=", "(", "1.0", "/", "(", "self", ".", "resample_rate", "//", "self", ".", "speed_ratio", ")", ",", "1.0", ",", "\n", "1.0", ")", ")", "\n", "x_fast", "=", "self", ".", "fast_path", ".", "conv1", "(", "x_fast", ")", "\n", "x_fast", "=", "self", ".", "fast_path", ".", "maxpool", "(", "x_fast", ")", "\n", "\n", "if", "self", ".", "slow_path", ".", "lateral", ":", "\n", "            ", "x_fast_lateral", "=", "self", ".", "slow_path", ".", "conv1_lateral", "(", "x_fast", ")", "\n", "x_slow", "=", "torch", ".", "cat", "(", "(", "x_slow", ",", "x_fast_lateral", ")", ",", "dim", "=", "1", ")", "\n", "", "ids", "=", "1", "\n", "idf", "=", "1", "\n", "for", "i", ",", "layer_name", "in", "enumerate", "(", "self", ".", "slow_path", ".", "res_layers", ")", ":", "\n", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ".", "slow_path", ",", "layer_name", ")", "\n", "for", "block", "in", "res_layer", ":", "\n", "                ", "x_slow", "=", "block", "(", "x_slow", ")", "\n", "ids", "+=", "1", "\n", "\n", "", "res_layer_fast", "=", "getattr", "(", "self", ".", "fast_path", ",", "layer_name", ")", "\n", "\n", "for", "block", "in", "res_layer_fast", ":", "\n", "                ", "x_fast", "=", "block", "(", "x_fast", ")", "\n", "idf", "+=", "1", "\n", "\n", "", "if", "(", "i", "!=", "len", "(", "self", ".", "slow_path", ".", "res_layers", ")", "-", "1", "\n", "and", "self", ".", "slow_path", ".", "lateral", ")", ":", "\n", "# No fusion needed in the final stage", "\n", "                ", "lateral_name", "=", "self", ".", "slow_path", ".", "lateral_connections", "[", "i", "]", "\n", "conv_lateral", "=", "getattr", "(", "self", ".", "slow_path", ",", "lateral_name", ")", "\n", "x_fast_lateral", "=", "conv_lateral", "(", "x_fast", ")", "\n", "x_slow", "=", "torch", ".", "cat", "(", "(", "x_slow", ",", "x_fast_lateral", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "", "out", "=", "(", "x_slow", ",", "x_fast", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowfast.build_pathway": [[360, 380], ["cfg.copy", "cfg.copy.pop", "pathway_cls", "TypeError", "KeyError", "isinstance"], "function", ["None"], ["def", "build_pathway", "(", "cfg", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Build pathway.\n    Args:\n        cfg (None or dict): cfg should contain:\n            - type (str): identify conv layer type.\n    Returns:\n        nn.Module: Created pathway.\n    \"\"\"", "\n", "if", "not", "(", "isinstance", "(", "cfg", ",", "dict", ")", "and", "'type'", "in", "cfg", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'cfg must be a dict containing the key \"type\"'", ")", "\n", "", "cfg_", "=", "cfg", ".", "copy", "(", ")", "\n", "\n", "pathway_type", "=", "cfg_", ".", "pop", "(", "'type'", ")", "\n", "if", "pathway_type", "not", "in", "pathway_cfg", ":", "\n", "        ", "raise", "KeyError", "(", "f'Unrecognized pathway type {pathway_type}'", ")", "\n", "\n", "", "pathway_cls", "=", "pathway_cfg", "[", "pathway_type", "]", "\n", "pathway", "=", "pathway_cls", "(", "*", "args", ",", "**", "kwargs", ",", "**", "cfg_", ")", "\n", "\n", "return", "pathway", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.STS_Conv.STS_3dconv": [[16, 66], ["isinstance", "torch.chunk", "torch.chunk", "torch.chunk", "torch.cat", "torch.cat", "torch.cat", "torch.chunk", "torch.chunk", "torch.chunk", "STS_Conv.apperance_st", "torch.conv3d", "STS_Conv.apperance_t", "torch.conv3d", "conv3d.norm", "conv3d.activate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_st", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_t"], ["", "def", "STS_3dconv", "(", "conv3d", ",", "x", ")", ":", "\n", "    ", "\"\"\".\n\n    Args:\n        conv3d (ConvModule): Number of channels in the input feature map.\n            Same as that in ``nn._ConvNd``.\n        x (tensor): Number of channels in the input feature map.\n            Same as that in ``nn._ConvNd``.\n    \"\"\"", "\n", "\n", "assert", "isinstance", "(", "conv3d", ",", "ConvModule", ")", ",", "'conv3d should be a ConvModule'", "\n", "\n", "# extract the parameters of conv3d", "\n", "conv3d_para", "=", "conv3d", ".", "conv", ".", "weight", "\n", "outplanes", ",", "inplanes", ",", "k_t", ",", "k_s", ",", "_", "=", "conv3d_para", ".", "shape", "\n", "stride_t", ",", "stride_s", ",", "_", "=", "conv3d", ".", "stride", "\n", "assert", "stride_t", "==", "1", ",", "'We dont support temporal downsampling currently'", "\n", "padding_t", ",", "padding_s", "=", "(", "k_t", "-", "1", ")", "//", "2", ",", "(", "k_s", "-", "1", ")", "//", "2", "\n", "groups", "=", "conv3d", ".", "groups", "\n", "\n", "# separate 3d conv into dynamic and static channels", "\n", "app_para", ",", "motion_para", "=", "torch", ".", "chunk", "(", "conv3d_para", ",", "2", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "if", "k_s", ">", "1", ":", "# this is not a temporal 3d conv", "\n", "        ", "assert", "groups", ">", "1", ",", "'We only support depthwise 3D conv currently (3x3x3)'", "\n", "x_app", ",", "x_motion", "=", "torch", ".", "chunk", "(", "x", ",", "2", ",", "dim", "=", "1", ")", "\n", "\n", "# static appearance modeling by splitting 3D kernel", "\n", "out_app", "=", "apperance_st", "(", "x_app", ",", "app_para", ",", "stride", "=", "(", "stride_t", ",", "stride_s", ")", ",", "groups", "=", "groups", "//", "2", ")", "\n", "\n", "# dynamic motion modeling by normal 3D conv", "\n", "out_motion", "=", "F", ".", "conv3d", "(", "x_motion", ",", "weight", "=", "motion_para", ",", "stride", "=", "(", "stride_t", ",", "stride_s", ",", "stride_s", ")", ",", "groups", "=", "groups", "//", "2", ",", "padding", "=", "(", "padding_t", ",", "padding_s", ",", "padding_s", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "assert", "groups", "==", "1", ",", "'We only support fully-connected temporal convolution (3x1x1)'", "\n", "\n", "# static appearance modeling by splitting 3D kernel", "\n", "out_app", "=", "apperance_t", "(", "x", ",", "app_para", ",", "padding_t", "=", "padding_t", ",", "groups", "=", "groups", ")", "\n", "\n", "# dynamic motion modeling by normal 3D conv", "\n", "out_motion", "=", "F", ".", "conv3d", "(", "x", ",", "weight", "=", "motion_para", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "(", "padding_t", ",", "padding_s", ",", "padding_s", ")", ")", "\n", "\n", "# concat the feature map", "\n", "", "out", "=", "torch", ".", "cat", "(", "[", "out_app", ",", "out_motion", "]", ",", "dim", "=", "1", ")", "\n", "if", "conv3d", ".", "norm_cfg", "is", "not", "None", ":", "\n", "        ", "out", "=", "conv3d", ".", "norm", "(", "out", ")", "\n", "", "if", "conv3d", ".", "act_cfg", "is", "not", "None", ":", "\n", "        ", "out", "=", "conv3d", ".", "activate", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.STS_Conv.apperance_st": [[69, 102], ["x.size", "torch.conv3d", "app_para[].view", "einops.rearrange", "torch.conv1d", "einops.rearrange", "app_para[].view", "einops.rearrange", "torch.conv1d", "einops.rearrange", "app_para[].view.size", "app_para[].view.size", "spatial_para.size", "spatial_para.size"], "function", ["None"], ["", "def", "apperance_st", "(", "x", ",", "app_para", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\".\n\n        Args:\n            x (tensor): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n            app_para (weight): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n        \"\"\"", "\n", "\n", "# prepare parameters", "\n", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "outplanes", ",", "inplanes", ",", "_", ",", "_", ",", "_", "=", "app_para", ".", "shape", "\n", "\n", "# middle 2D kernel 1x3x3", "\n", "spatial_para", "=", "app_para", "[", ":", ",", ":", ",", "1", ":", "2", ",", ":", ",", ":", "]", "\n", "out_s", "=", "F", ".", "conv3d", "(", "x", ",", "weight", "=", "spatial_para", ",", "stride", "=", "(", "stride", "[", "0", "]", ",", "stride", "[", "1", "]", ",", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "0", ",", "(", "spatial_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ",", "(", "spatial_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "\n", "# t1 kernel reshapes into 1D (1x9), then convolve along the row direction", "\n", "# note that some backbones use temporal dowsampling, we don't support temporal downsampling by using 1D conv", "\n", "row_para", "=", "app_para", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", ".", "view", "(", "outplanes", ",", "inplanes", ",", "-", "1", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "x", ",", "'b c t h w  -> (b t) c (h w)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "F", ".", "conv1d", "(", "out_row", ",", "weight", "=", "row_para", ",", "stride", "=", "(", "stride", "[", "1", "]", "*", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "(", "row_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "out_row", ",", "'(b t) c (h w)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "\n", "# t2 kernel reshapes into 1D (9x1), then convolve along the column direction", "\n", "column_para", "=", "app_para", "[", ":", ",", ":", ",", "2", ",", ":", ",", ":", "]", ".", "view", "(", "outplanes", ",", "inplanes", ",", "-", "1", ")", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "x", ",", "'b c t h w  -> (b t) c (w h)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_column", "=", "F", ".", "conv1d", "(", "out_column", ",", "weight", "=", "column_para", ",", "stride", "=", "(", "stride", "[", "1", "]", "*", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "(", "column_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "out_column", ",", "'(b t) c (w h)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "\n", "out", "=", "out_s", "+", "out_row", "+", "out_column", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.STS_Conv.apperance_t": [[107, 138], ["x.size", "app_para.view.view", "torch.chunk", "torch.chunk", "torch.chunk", "einops.rearrange", "einops.rearrange", "torch.conv1d", "torch.conv1d", "einops.rearrange", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "app_para.view.size", "app_para.view.size"], "function", ["None"], ["", "def", "apperance_t", "(", "x", ",", "app_para", ",", "padding_t", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\".\n\n        Args:\n            conv3d (int): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n            conv3d (int): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n        \"\"\"", "\n", "\n", "# prepare parameters", "\n", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "app_para", "=", "app_para", ".", "view", "(", "app_para", ".", "size", "(", "0", ")", ",", "app_para", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "app_kernel_chunks", "=", "torch", ".", "chunk", "(", "app_para", ",", "2", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "# reshape to column or row", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> (b t) c ( h w)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> (b t) c ( w h)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "\n", "# convolve", "\n", "out_column", "=", "F", ".", "conv1d", "(", "out_column", ",", "weight", "=", "app_kernel_chunks", "[", "0", "]", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "padding_t", ")", "\n", "out_row", "=", "F", ".", "conv1d", "(", "out_row", ",", "weight", "=", "app_kernel_chunks", "[", "1", "]", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "padding_t", ")", "\n", "\n", "# reshape back to original shape", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "out_column", ",", "'(b t) c ( h w)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "out_row", ",", "'(b t) c ( w h)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "\n", "# concat along the channel dimension", "\n", "out_app", "=", "torch", ".", "cat", "(", "[", "out_column", ",", "out_row", "]", ",", "dim", "=", "1", ")", "\n", "return", "out_app", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.video_models.resnet3d_slowonly.ResNet3dSlowOnly.__init__": [[22, 42], ["resnet3d_slowfast.ResNet3dPathway.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "*", "args", ",", "\n", "lateral", "=", "False", ",", "\n", "conv1_kernel", "=", "(", "1", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "inflate", "=", "(", "0", ",", "0", ",", "1", ",", "1", ")", ",", "\n", "with_pool2", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "*", "args", ",", "\n", "lateral", "=", "lateral", ",", "\n", "conv1_kernel", "=", "conv1_kernel", ",", "\n", "conv1_stride_t", "=", "conv1_stride_t", ",", "\n", "pool1_stride_t", "=", "pool1_stride_t", ",", "\n", "inflate", "=", "inflate", ",", "\n", "with_pool2", "=", "with_pool2", ",", "\n", "**", "kwargs", ")", "\n", "\n", "assert", "not", "self", ".", "lateral", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.ToNumpy.__call__": [[17, 23], ["numpy.array", "numpy.rollaxis", "numpy.expand_dims"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "np_img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.ToTensor.__init__": [[27, 29], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dtype", "=", "torch", ".", "float32", ")", ":", "\n", "        ", "self", ".", "dtype", "=", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.ToTensor.__call__": [[30, 36], ["numpy.array", "numpy.rollaxis", "torch.from_numpy().to", "numpy.expand_dims", "torch.from_numpy"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "torch", ".", "from_numpy", "(", "np_img", ")", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.__init__": [[100, 115], ["isinstance", "tuple", "warnings.warn", "transforms.str_to_interp_mode"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_interp_mode"], ["def", "__init__", "(", "self", ",", "size", ",", "scale", "=", "(", "0.08", ",", "1.0", ")", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "\n", "interpolation", "=", "'bilinear'", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "size", "=", "tuple", "(", "size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "size", "=", "(", "size", ",", "size", ")", "\n", "", "if", "(", "scale", "[", "0", "]", ">", "scale", "[", "1", "]", ")", "or", "(", "ratio", "[", "0", "]", ">", "ratio", "[", "1", "]", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"range should be of kind (min, max)\"", ")", "\n", "\n", "", "if", "interpolation", "==", "'random'", ":", "\n", "            ", "self", ".", "interpolation", "=", "_RANDOM_INTERPOLATION", "\n", "", "else", ":", "\n", "            ", "self", ".", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", "\n", "", "self", ".", "scale", "=", "scale", "\n", "self", ".", "ratio", "=", "ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.get_params": [[116, 158], ["range", "math.exp", "int", "int", "min", "int", "random.uniform", "math.log", "math.log", "random.uniform", "round", "round", "random.randint", "random.randint", "round", "max", "int", "math.sqrt", "math.sqrt", "round", "min", "max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_params", "(", "img", ",", "scale", ",", "ratio", ")", ":", "\n", "        ", "\"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"", "\n", "area", "=", "img", ".", "size", "[", "0", "]", "*", "img", ".", "size", "[", "1", "]", "\n", "\n", "for", "attempt", "in", "range", "(", "10", ")", ":", "\n", "            ", "target_area", "=", "random", ".", "uniform", "(", "*", "scale", ")", "*", "area", "\n", "log_ratio", "=", "(", "math", ".", "log", "(", "ratio", "[", "0", "]", ")", ",", "math", ".", "log", "(", "ratio", "[", "1", "]", ")", ")", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "log_ratio", ")", ")", "\n", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "\n", "if", "w", "<=", "img", ".", "size", "[", "0", "]", "and", "h", "<=", "img", ".", "size", "[", "1", "]", ":", "\n", "                ", "i", "=", "random", ".", "randint", "(", "0", ",", "img", ".", "size", "[", "1", "]", "-", "h", ")", "\n", "j", "=", "random", ".", "randint", "(", "0", ",", "img", ".", "size", "[", "0", "]", "-", "w", ")", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n", "# Fallback to central crop", "\n", "", "", "in_ratio", "=", "img", ".", "size", "[", "0", "]", "/", "img", ".", "size", "[", "1", "]", "\n", "if", "in_ratio", "<", "min", "(", "ratio", ")", ":", "\n", "            ", "w", "=", "img", ".", "size", "[", "0", "]", "\n", "h", "=", "int", "(", "round", "(", "w", "/", "min", "(", "ratio", ")", ")", ")", "\n", "", "elif", "in_ratio", ">", "max", "(", "ratio", ")", ":", "\n", "            ", "h", "=", "img", ".", "size", "[", "1", "]", "\n", "w", "=", "int", "(", "round", "(", "h", "*", "max", "(", "ratio", ")", ")", ")", "\n", "", "else", ":", "# whole image", "\n", "            ", "w", "=", "img", ".", "size", "[", "0", "]", "\n", "h", "=", "img", ".", "size", "[", "1", "]", "\n", "", "i", "=", "(", "img", ".", "size", "[", "1", "]", "-", "h", ")", "//", "2", "\n", "j", "=", "(", "img", ".", "size", "[", "0", "]", "-", "w", ")", "//", "2", "\n", "return", "i", ",", "j", ",", "h", ",", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.__call__": [[159, 173], ["transforms.RandomResizedCropAndInterpolation.get_params", "isinstance", "torchvision.resized_crop", "random.choice"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.get_params"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"", "\n", "i", ",", "j", ",", "h", ",", "w", "=", "self", ".", "get_params", "(", "img", ",", "self", ".", "scale", ",", "self", ".", "ratio", ")", "\n", "if", "isinstance", "(", "self", ".", "interpolation", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "interpolation", "=", "random", ".", "choice", "(", "self", ".", "interpolation", ")", "\n", "", "else", ":", "\n", "            ", "interpolation", "=", "self", ".", "interpolation", "\n", "", "return", "F", ".", "resized_crop", "(", "img", ",", "i", ",", "j", ",", "h", ",", "w", ",", "self", ".", "size", ",", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.RandomResizedCropAndInterpolation.__repr__": [[174, 184], ["isinstance", "transforms.interp_mode_to_str", "tuple", "tuple", "transforms.interp_mode_to_str", "round", "round"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.interp_mode_to_str", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.interp_mode_to_str"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "interpolation", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "interpolate_str", "=", "' '", ".", "join", "(", "[", "interp_mode_to_str", "(", "x", ")", "for", "x", "in", "self", ".", "interpolation", "]", ")", "\n", "", "else", ":", "\n", "            ", "interpolate_str", "=", "interp_mode_to_str", "(", "self", ".", "interpolation", ")", "\n", "", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'(size={0}'", ".", "format", "(", "self", ".", "size", ")", "\n", "format_string", "+=", "', scale={0}'", ".", "format", "(", "tuple", "(", "round", "(", "s", ",", "4", ")", "for", "s", "in", "self", ".", "scale", ")", ")", "\n", "format_string", "+=", "', ratio={0}'", ".", "format", "(", "tuple", "(", "round", "(", "r", ",", "4", ")", "for", "r", "in", "self", ".", "ratio", ")", ")", "\n", "format_string", "+=", "', interpolation={0})'", ".", "format", "(", "interpolate_str", ")", "\n", "return", "format_string", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_pil_interp": [[64, 66], ["None"], "function", ["None"], ["", "def", "str_to_pil_interp", "(", "mode_str", ")", ":", "\n", "    ", "return", "_str_to_pil_interpolation", "[", "mode_str", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_interp_mode": [[68, 73], ["None"], "function", ["None"], ["", "def", "str_to_interp_mode", "(", "mode_str", ")", ":", "\n", "    ", "if", "has_interpolation_mode", ":", "\n", "        ", "return", "_str_to_torch_interpolation", "[", "mode_str", "]", "\n", "", "else", ":", "\n", "        ", "return", "_str_to_pil_interpolation", "[", "mode_str", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.interp_mode_to_str": [[75, 80], ["None"], "function", ["None"], ["", "", "def", "interp_mode_to_str", "(", "mode", ")", ":", "\n", "    ", "if", "has_interpolation_mode", ":", "\n", "        ", "return", "_torch_interpolation_to_str", "[", "mode", "]", "\n", "", "else", ":", "\n", "        ", "return", "_pil_interpolation_to_str", "[", "mode", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.config.resolve_data_config": [[8, 79], ["hasattr", "isinstance", "tuple", "tuple", "tuple", "_logger.info", "new_config.items", "len", "isinstance", "len", "tuple", "len", "tuple", "_logger.info", "len", "len", "list", "list", "str"], "function", ["None"], ["]", "\n", "\n", "# Set to True if prefer to have layers with no jit optimization (includes activations)", "\n", "_NO_JIT", "=", "False", "\n", "\n", "# Set to True if prefer to have activation layers with no jit optimization", "\n", "# NOTE not currently used as no difference between no_jit and no_activation jit as only layers obeying", "\n", "# the jit flags so far are activations. This will change as more layers are updated and/or added.", "\n", "_NO_ACTIVATION_JIT", "=", "False", "\n", "\n", "# Set to True if exporting a model with Same padding via ONNX", "\n", "_EXPORTABLE", "=", "False", "\n", "\n", "# Set to True if wanting to use torch.jit.script on a model", "\n", "_SCRIPTABLE", "=", "False", "\n", "\n", "\n", "def", "is_no_jit", "(", ")", ":", "\n", "    ", "return", "_NO_JIT", "\n", "\n", "\n", "", "class", "set_no_jit", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_NO_JIT", "\n", "self", ".", "prev", "=", "_NO_JIT", "\n", "_NO_JIT", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_NO_JIT", "\n", "_NO_JIT", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n", "\n", "", "", "def", "is_exportable", "(", ")", ":", "\n", "    ", "return", "_EXPORTABLE", "\n", "\n", "\n", "", "class", "set_exportable", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "self", ".", "prev", "=", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_EXPORTABLE", "\n", "_EXPORTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n", "\n", "\n", "", "", "def", "is_scriptable", "(", ")", ":", "\n", "    ", "return", "_SCRIPTABLE", "\n", "\n", "\n", "", "class", "set_scriptable", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "mode", ":", "bool", ")", "->", "None", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "self", ".", "prev", "=", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "mode", "\n", "\n", "", "def", "__enter__", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n", "", "def", "__exit__", "(", "self", ",", "*", "args", ":", "Any", ")", "->", "bool", ":", "\n", "        ", "global", "_SCRIPTABLE", "\n", "_SCRIPTABLE", "=", "self", ".", "prev", "\n", "return", "False", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_noaug_train": [[17, 42], ["torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "timm.data.transforms.str_to_interp_mode", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_interp_mode"], ["def", "transforms_noaug_train", "(", "\n", "img_size", "=", "224", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", ")", ":", "\n", "    ", "if", "interpolation", "==", "'random'", ":", "\n", "# random interpolation not supported with no-aug", "\n", "        ", "interpolation", "=", "'bilinear'", "\n", "", "tfl", "=", "[", "\n", "transforms", ".", "Resize", "(", "img_size", ",", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "img_size", ")", "\n", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "", "return", "transforms", ".", "Compose", "(", "tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_imagenet_train": [[44, 128], ["tuple", "tuple", "timm.data.transforms.RandomResizedCropAndInterpolation", "isinstance", "isinstance", "dict", "auto_augment.startswith", "torchvision.transforms.Compose", "torchvision.transforms.RandomHorizontalFlip", "torchvision.transforms.RandomVerticalFlip", "min", "timm.data.transforms.str_to_pil_interp", "auto_augment.startswith", "isinstance", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "final_tfl.append", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "int", "tuple", "timm.data.auto_augment.rand_augment_transform", "torchvision.transforms.ColorJitter", "timm.data.random_erasing.RandomErasing", "timm.data.auto_augment.augment_and_mix_transform", "timm.data.auto_augment.auto_augment_transform", "len", "torch.tensor", "torch.tensor", "min", "float", "round"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_pil_interp", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rand_augment_transform", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.augment_and_mix_transform", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_transform"], ["", "def", "transforms_imagenet_train", "(", "\n", "img_size", "=", "224", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "interpolation", "=", "'random'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", ",", "\n", "separate", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the 'clean' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    \"\"\"", "\n", "scale", "=", "tuple", "(", "scale", "or", "(", "0.08", ",", "1.0", ")", ")", "# default imagenet scale range", "\n", "ratio", "=", "tuple", "(", "ratio", "or", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ")", "# default imagenet ratio range", "\n", "primary_tfl", "=", "[", "\n", "RandomResizedCropAndInterpolation", "(", "img_size", ",", "scale", "=", "scale", ",", "ratio", "=", "ratio", ",", "interpolation", "=", "interpolation", ")", "]", "\n", "if", "hflip", ">", "0.", ":", "\n", "        ", "primary_tfl", "+=", "[", "transforms", ".", "RandomHorizontalFlip", "(", "p", "=", "hflip", ")", "]", "\n", "", "if", "vflip", ">", "0.", ":", "\n", "        ", "primary_tfl", "+=", "[", "transforms", ".", "RandomVerticalFlip", "(", "p", "=", "vflip", ")", "]", "\n", "\n", "", "secondary_tfl", "=", "[", "]", "\n", "if", "auto_augment", ":", "\n", "        ", "assert", "isinstance", "(", "auto_augment", ",", "str", ")", "\n", "if", "isinstance", "(", "img_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "            ", "img_size_min", "=", "min", "(", "img_size", ")", "\n", "", "else", ":", "\n", "            ", "img_size_min", "=", "img_size", "\n", "", "aa_params", "=", "dict", "(", "\n", "translate_const", "=", "int", "(", "img_size_min", "*", "0.45", ")", ",", "\n", "img_mean", "=", "tuple", "(", "[", "min", "(", "255", ",", "round", "(", "255", "*", "x", ")", ")", "for", "x", "in", "mean", "]", ")", ",", "\n", ")", "\n", "if", "interpolation", "and", "interpolation", "!=", "'random'", ":", "\n", "            ", "aa_params", "[", "'interpolation'", "]", "=", "str_to_pil_interp", "(", "interpolation", ")", "\n", "", "if", "auto_augment", ".", "startswith", "(", "'rand'", ")", ":", "\n", "            ", "secondary_tfl", "+=", "[", "rand_augment_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "elif", "auto_augment", ".", "startswith", "(", "'augmix'", ")", ":", "\n", "            ", "aa_params", "[", "'translate_pct'", "]", "=", "0.3", "\n", "secondary_tfl", "+=", "[", "augment_and_mix_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "else", ":", "\n", "            ", "secondary_tfl", "+=", "[", "auto_augment_transform", "(", "auto_augment", ",", "aa_params", ")", "]", "\n", "", "", "elif", "color_jitter", "is", "not", "None", ":", "\n", "# color jitter is enabled when not using AA", "\n", "        ", "if", "isinstance", "(", "color_jitter", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "# color jitter should be a 3-tuple/list if spec brightness/contrast/saturation", "\n", "# or 4 if also augmenting hue", "\n", "            ", "assert", "len", "(", "color_jitter", ")", "in", "(", "3", ",", "4", ")", "\n", "", "else", ":", "\n", "# if it's a scalar, duplicate for brightness, contrast, and saturation, no hue", "\n", "            ", "color_jitter", "=", "(", "float", "(", "color_jitter", ")", ",", ")", "*", "3", "\n", "", "secondary_tfl", "+=", "[", "transforms", ".", "ColorJitter", "(", "*", "color_jitter", ")", "]", "\n", "\n", "", "final_tfl", "=", "[", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "final_tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "final_tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "if", "re_prob", ">", "0.", ":", "\n", "            ", "final_tfl", ".", "append", "(", "\n", "RandomErasing", "(", "re_prob", ",", "mode", "=", "re_mode", ",", "max_count", "=", "re_count", ",", "num_splits", "=", "re_num_splits", ",", "device", "=", "'cpu'", ")", ")", "\n", "\n", "", "", "if", "separate", ":", "\n", "        ", "return", "transforms", ".", "Compose", "(", "primary_tfl", ")", ",", "transforms", ".", "Compose", "(", "secondary_tfl", ")", ",", "transforms", ".", "Compose", "(", "final_tfl", ")", "\n", "", "else", ":", "\n", "        ", "return", "transforms", ".", "Compose", "(", "primary_tfl", "+", "secondary_tfl", "+", "final_tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_imagenet_eval": [[130, 165], ["isinstance", "torchvision.transforms.Compose", "int", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "len", "int", "tuple", "math.floor", "timm.data.transforms.ToNumpy", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "math.floor", "timm.data.transforms.str_to_interp_mode", "int", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms.str_to_interp_mode"], ["", "", "def", "transforms_imagenet_eval", "(", "\n", "img_size", "=", "224", ",", "\n", "crop_pct", "=", "None", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ")", ":", "\n", "    ", "crop_pct", "=", "crop_pct", "or", "DEFAULT_CROP_PCT", "\n", "\n", "if", "isinstance", "(", "img_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "assert", "len", "(", "img_size", ")", "==", "2", "\n", "if", "img_size", "[", "-", "1", "]", "==", "img_size", "[", "-", "2", "]", ":", "\n", "# fall-back to older behaviour so Resize scales to shortest edge if target is square", "\n", "            ", "scale_size", "=", "int", "(", "math", ".", "floor", "(", "img_size", "[", "0", "]", "/", "crop_pct", ")", ")", "\n", "", "else", ":", "\n", "            ", "scale_size", "=", "tuple", "(", "[", "int", "(", "x", "/", "crop_pct", ")", "for", "x", "in", "img_size", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "scale_size", "=", "int", "(", "math", ".", "floor", "(", "img_size", "/", "crop_pct", ")", ")", "\n", "\n", "", "tfl", "=", "[", "\n", "transforms", ".", "Resize", "(", "scale_size", ",", "interpolation", "=", "str_to_interp_mode", "(", "interpolation", ")", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "img_size", ")", ",", "\n", "]", "\n", "if", "use_prefetcher", ":", "\n", "# prefetcher and collate will handle tensor conversion and norm", "\n", "        ", "tfl", "+=", "[", "ToNumpy", "(", ")", "]", "\n", "", "else", ":", "\n", "        ", "tfl", "+=", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "\n", "mean", "=", "torch", ".", "tensor", "(", "mean", ")", ",", "\n", "std", "=", "torch", ".", "tensor", "(", "std", ")", ")", "\n", "]", "\n", "\n", "", "return", "transforms", ".", "Compose", "(", "tfl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.create_transform": [[167, 237], ["isinstance", "TfPreprocessTransform", "transforms_factory.transforms_noaug_train", "transforms_factory.transforms_imagenet_train", "transforms_factory.transforms_imagenet_eval"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_noaug_train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_imagenet_train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.transforms_imagenet_eval"], ["", "def", "create_transform", "(", "\n", "input_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "False", ",", "\n", "no_aug", "=", "False", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", ",", "\n", "crop_pct", "=", "None", ",", "\n", "tf_preprocessing", "=", "False", ",", "\n", "separate", "=", "False", ")", ":", "\n", "\n", "    ", "if", "isinstance", "(", "input_size", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "img_size", "=", "input_size", "[", "-", "2", ":", "]", "\n", "", "else", ":", "\n", "        ", "img_size", "=", "input_size", "\n", "\n", "", "if", "tf_preprocessing", "and", "use_prefetcher", ":", "\n", "        ", "assert", "not", "separate", ",", "\"Separate transforms not supported for TF preprocessing\"", "\n", "from", "timm", ".", "data", ".", "tf_preprocessing", "import", "TfPreprocessTransform", "\n", "transform", "=", "TfPreprocessTransform", "(", "\n", "is_training", "=", "is_training", ",", "size", "=", "img_size", ",", "interpolation", "=", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "if", "is_training", "and", "no_aug", ":", "\n", "            ", "assert", "not", "separate", ",", "\"Cannot perform split augmentation with no_aug\"", "\n", "transform", "=", "transforms_noaug_train", "(", "\n", "img_size", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ")", "\n", "", "elif", "is_training", ":", "\n", "            ", "transform", "=", "transforms_imagenet_train", "(", "\n", "img_size", ",", "\n", "scale", "=", "scale", ",", "\n", "ratio", "=", "ratio", ",", "\n", "hflip", "=", "hflip", ",", "\n", "vflip", "=", "vflip", ",", "\n", "color_jitter", "=", "color_jitter", ",", "\n", "auto_augment", "=", "auto_augment", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "re_prob", "=", "re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", ",", "\n", "separate", "=", "separate", ")", "\n", "", "else", ":", "\n", "            ", "assert", "not", "separate", ",", "\"Separate transforms not supported for validation preprocessing\"", "\n", "transform", "=", "transforms_imagenet_eval", "(", "\n", "img_size", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "crop_pct", "=", "crop_pct", ")", "\n", "\n", "", "", "return", "transform", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.real_labels.RealLabelsImagenet.__init__": [[14, 24], ["open", "json.load", "len", "len", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "filenames", ",", "real_json", "=", "'real.json'", ",", "topk", "=", "(", "1", ",", "5", ")", ")", ":", "\n", "        ", "with", "open", "(", "real_json", ")", "as", "real_labels", ":", "\n", "            ", "real_labels", "=", "json", ".", "load", "(", "real_labels", ")", "\n", "real_labels", "=", "{", "f'ILSVRC2012_val_{i + 1:08d}.JPEG'", ":", "labels", "for", "i", ",", "labels", "in", "enumerate", "(", "real_labels", ")", "}", "\n", "", "self", ".", "real_labels", "=", "real_labels", "\n", "self", ".", "filenames", "=", "filenames", "\n", "assert", "len", "(", "self", ".", "filenames", ")", "==", "len", "(", "self", ".", "real_labels", ")", "\n", "self", ".", "topk", "=", "topk", "\n", "self", ".", "is_correct", "=", "{", "k", ":", "[", "]", "for", "k", "in", "topk", "}", "\n", "self", ".", "sample_idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.real_labels.RealLabelsImagenet.add_result": [[25, 37], ["max", "output.topk", "pred_batch.cpu().numpy.cpu().numpy.cpu().numpy", "os.path.basename", "pred_batch.cpu().numpy.cpu().numpy.cpu", "real_labels.RealLabelsImagenet.is_correct[].append", "any"], "methods", ["None"], ["", "def", "add_result", "(", "self", ",", "output", ")", ":", "\n", "        ", "maxk", "=", "max", "(", "self", ".", "topk", ")", "\n", "_", ",", "pred_batch", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred_batch", "=", "pred_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "pred", "in", "pred_batch", ":", "\n", "            ", "filename", "=", "self", ".", "filenames", "[", "self", ".", "sample_idx", "]", "\n", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "if", "self", ".", "real_labels", "[", "filename", "]", ":", "\n", "                ", "for", "k", "in", "self", ".", "topk", ":", "\n", "                    ", "self", ".", "is_correct", "[", "k", "]", ".", "append", "(", "\n", "any", "(", "[", "p", "in", "self", ".", "real_labels", "[", "filename", "]", "for", "p", "in", "pred", "[", ":", "k", "]", "]", ")", ")", "\n", "", "", "self", ".", "sample_idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.real_labels.RealLabelsImagenet.get_accuracy": [[38, 43], ["float", "float", "numpy.mean", "numpy.mean"], "methods", ["None"], ["", "", "def", "get_accuracy", "(", "self", ",", "k", "=", "None", ")", ":", "\n", "        ", "if", "k", "is", "None", ":", "\n", "            ", "return", "{", "k", ":", "float", "(", "np", ".", "mean", "(", "self", ".", "is_correct", "[", "k", "]", ")", ")", "*", "100", "for", "k", "in", "self", ".", "topk", "}", "\n", "", "else", ":", "\n", "            ", "return", "float", "(", "np", ".", "mean", "(", "self", ".", "is_correct", "[", "k", "]", ")", ")", "*", "100", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing.__init__": [[45, 67], ["mode.lower", "math.log", "math.log"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "probability", "=", "0.5", ",", "min_area", "=", "0.02", ",", "max_area", "=", "1", "/", "3", ",", "min_aspect", "=", "0.3", ",", "max_aspect", "=", "None", ",", "\n", "mode", "=", "'const'", ",", "min_count", "=", "1", ",", "max_count", "=", "None", ",", "num_splits", "=", "0", ",", "device", "=", "'cuda'", ")", ":", "\n", "        ", "self", ".", "probability", "=", "probability", "\n", "self", ".", "min_area", "=", "min_area", "\n", "self", ".", "max_area", "=", "max_area", "\n", "max_aspect", "=", "max_aspect", "or", "1", "/", "min_aspect", "\n", "self", ".", "log_aspect_ratio", "=", "(", "math", ".", "log", "(", "min_aspect", ")", ",", "math", ".", "log", "(", "max_aspect", ")", ")", "\n", "self", ".", "min_count", "=", "min_count", "\n", "self", ".", "max_count", "=", "max_count", "or", "min_count", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "mode", "=", "mode", ".", "lower", "(", ")", "\n", "self", ".", "rand_color", "=", "False", "\n", "self", ".", "per_pixel", "=", "False", "\n", "if", "self", ".", "mode", "==", "'rand'", ":", "\n", "            ", "self", ".", "rand_color", "=", "True", "# per block random normal", "\n", "", "elif", "self", ".", "mode", "==", "'pixel'", ":", "\n", "            ", "self", ".", "per_pixel", "=", "True", "# per pixel random normal", "\n", "", "else", ":", "\n", "            ", "assert", "not", "self", ".", "mode", "or", "self", ".", "mode", "==", "'const'", "\n", "", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing._erase": [[68, 87], ["range", "random.random", "random.randint", "range", "math.exp", "int", "int", "random.uniform", "round", "round", "random.randint", "random.randint", "random_erasing._get_pixels", "random.uniform", "math.sqrt", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing._get_pixels"], ["", "def", "_erase", "(", "self", ",", "img", ",", "chan", ",", "img_h", ",", "img_w", ",", "dtype", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", ">", "self", ".", "probability", ":", "\n", "            ", "return", "\n", "", "area", "=", "img_h", "*", "img_w", "\n", "count", "=", "self", ".", "min_count", "if", "self", ".", "min_count", "==", "self", ".", "max_count", "else", "random", ".", "randint", "(", "self", ".", "min_count", ",", "self", ".", "max_count", ")", "\n", "for", "_", "in", "range", "(", "count", ")", ":", "\n", "            ", "for", "attempt", "in", "range", "(", "10", ")", ":", "\n", "                ", "target_area", "=", "random", ".", "uniform", "(", "self", ".", "min_area", ",", "self", ".", "max_area", ")", "*", "area", "/", "count", "\n", "aspect_ratio", "=", "math", ".", "exp", "(", "random", ".", "uniform", "(", "*", "self", ".", "log_aspect_ratio", ")", ")", "\n", "h", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "*", "aspect_ratio", ")", ")", ")", "\n", "w", "=", "int", "(", "round", "(", "math", ".", "sqrt", "(", "target_area", "/", "aspect_ratio", ")", ")", ")", "\n", "if", "w", "<", "img_w", "and", "h", "<", "img_h", ":", "\n", "                    ", "top", "=", "random", ".", "randint", "(", "0", ",", "img_h", "-", "h", ")", "\n", "left", "=", "random", ".", "randint", "(", "0", ",", "img_w", "-", "w", ")", "\n", "img", "[", ":", ",", "top", ":", "top", "+", "h", ",", "left", ":", "left", "+", "w", "]", "=", "_get_pixels", "(", "\n", "self", ".", "per_pixel", ",", "self", ".", "rand_color", ",", "(", "chan", ",", "h", ",", "w", ")", ",", "\n", "dtype", "=", "dtype", ",", "device", "=", "self", ".", "device", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing.__call__": [[88, 98], ["len", "random_erasing.RandomErasing._erase", "input.size", "range", "input.size", "random_erasing.RandomErasing._erase", "input.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing._erase", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing._erase"], ["", "", "", "", "def", "__call__", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "len", "(", "input", ".", "size", "(", ")", ")", "==", "3", ":", "\n", "            ", "self", ".", "_erase", "(", "input", ",", "*", "input", ".", "size", "(", ")", ",", "input", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", ",", "chan", ",", "img_h", ",", "img_w", "=", "input", ".", "size", "(", ")", "\n", "# skip first slice of batch if num_splits is set (for clean portion of samples)", "\n", "batch_start", "=", "batch_size", "//", "self", ".", "num_splits", "if", "self", ".", "num_splits", ">", "1", "else", "0", "\n", "for", "i", "in", "range", "(", "batch_start", ",", "batch_size", ")", ":", "\n", "                ", "self", ".", "_erase", "(", "input", "[", "i", "]", ",", "chan", ",", "img_h", ",", "img_w", ",", "input", ".", "dtype", ")", "\n", "", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing.RandomErasing.__repr__": [[99, 104], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "# NOTE simplified state for repr", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(p={self.probability}, mode={self.mode}'", "\n", "fs", "+=", "f', count=({self.min_count}, {self.max_count}))'", "\n", "return", "fs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.random_erasing._get_pixels": [[13, 23], ["torch.empty().normal_", "torch.empty().normal_", "torch.zeros", "torch.empty", "torch.empty"], "function", ["None"], ["def", "_get_pixels", "(", "per_pixel", ",", "rand_color", ",", "patch_size", ",", "dtype", "=", "torch", ".", "float32", ",", "device", "=", "'cuda'", ")", ":", "\n", "# NOTE I've seen CUDA illegal memory access errors being caused by the normal_()", "\n", "# paths, flip the order so normal is run on CPU if this becomes a problem", "\n", "# Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508", "\n", "    ", "if", "per_pixel", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "patch_size", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", ".", "normal_", "(", ")", "\n", "", "elif", "rand_color", ":", "\n", "        ", "return", "torch", ".", "empty", "(", "(", "patch_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", ".", "normal_", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "zeros", "(", "(", "patch_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugmentOp.__init__": [[317, 337], ["hparams.copy", "dict", "auto_augment.AugmentOp.hparams.get", "auto_augment.AugmentOp.hparams.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "prob", "=", "0.5", ",", "magnitude", "=", "10", ",", "hparams", "=", "None", ")", ":", "\n", "        ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "aug_fn", "=", "NAME_TO_OP", "[", "name", "]", "\n", "self", ".", "level_fn", "=", "LEVEL_TO_ARG", "[", "name", "]", "\n", "self", ".", "prob", "=", "prob", "\n", "self", ".", "magnitude", "=", "magnitude", "\n", "self", ".", "hparams", "=", "hparams", ".", "copy", "(", ")", "\n", "self", ".", "kwargs", "=", "dict", "(", "\n", "fillcolor", "=", "hparams", "[", "'img_mean'", "]", "if", "'img_mean'", "in", "hparams", "else", "_FILL", ",", "\n", "resample", "=", "hparams", "[", "'interpolation'", "]", "if", "'interpolation'", "in", "hparams", "else", "_RANDOM_INTERPOLATION", ",", "\n", ")", "\n", "\n", "# If magnitude_std is > 0, we introduce some randomness", "\n", "# in the usually fixed policy and sample magnitude from a normal distribution", "\n", "# with mean `magnitude` and std-dev of `magnitude_std`.", "\n", "# NOTE This is my own hack, being tested, not in papers or reference impls.", "\n", "# If magnitude_std is inf, we sample magnitude from a uniform distribution", "\n", "self", ".", "magnitude_std", "=", "self", ".", "hparams", ".", "get", "(", "'magnitude_std'", ",", "0", ")", "\n", "self", ".", "magnitude_max", "=", "self", ".", "hparams", ".", "get", "(", "'magnitude_max'", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugmentOp.__call__": [[338, 354], ["max", "auto_augment.AugmentOp.aug_fn", "min", "auto_augment.AugmentOp.level_fn", "tuple", "random.random", "float", "random.uniform", "random.gauss"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "if", "self", ".", "prob", "<", "1.0", "and", "random", ".", "random", "(", ")", ">", "self", ".", "prob", ":", "\n", "            ", "return", "img", "\n", "", "magnitude", "=", "self", ".", "magnitude", "\n", "if", "self", ".", "magnitude_std", ">", "0", ":", "\n", "# magnitude randomization enabled", "\n", "            ", "if", "self", ".", "magnitude_std", "==", "float", "(", "'inf'", ")", ":", "\n", "                ", "magnitude", "=", "random", ".", "uniform", "(", "0", ",", "magnitude", ")", "\n", "", "elif", "self", ".", "magnitude_std", ">", "0", ":", "\n", "                ", "magnitude", "=", "random", ".", "gauss", "(", "magnitude", ",", "self", ".", "magnitude_std", ")", "\n", "# default upper_bound for the timm RA impl is _LEVEL_DENOM (10)", "\n", "# setting magnitude_max overrides this to allow M > 10 (behaviour closer to Google TF RA impl)", "\n", "", "", "upper_bound", "=", "self", ".", "magnitude_max", "or", "_LEVEL_DENOM", "\n", "magnitude", "=", "max", "(", "0.", ",", "min", "(", "magnitude", ",", "upper_bound", ")", ")", "\n", "level_args", "=", "self", ".", "level_fn", "(", "magnitude", ",", "self", ".", "hparams", ")", "if", "self", ".", "level_fn", "is", "not", "None", "else", "tuple", "(", ")", "\n", "return", "self", ".", "aug_fn", "(", "img", ",", "*", "level_args", ",", "**", "self", ".", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugmentOp.__repr__": [[355, 362], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(name={self.name}, p={self.prob}'", "\n", "fs", "+=", "f', m={self.magnitude}, mstd={self.magnitude_std}'", "\n", "if", "self", ".", "magnitude_max", "is", "not", "None", ":", "\n", "            ", "fs", "+=", "f', mmax={self.magnitude_max}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AutoAugment.__init__": [[513, 515], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "policy", ")", ":", "\n", "        ", "self", ".", "policy", "=", "policy", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AutoAugment.__call__": [[516, 521], ["random.choice", "op"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "sub_policy", "=", "random", ".", "choice", "(", "self", ".", "policy", ")", "\n", "for", "op", "in", "sub_policy", ":", "\n", "            ", "img", "=", "op", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AutoAugment.__repr__": [[522, 530], ["str"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(policy='", "\n", "for", "p", "in", "self", ".", "policy", ":", "\n", "            ", "fs", "+=", "'\\n\\t['", "\n", "fs", "+=", "', '", ".", "join", "(", "[", "str", "(", "op", ")", "for", "op", "in", "p", "]", ")", "\n", "fs", "+=", "']'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.RandAugment.__init__": [[642, 646], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "ops", ",", "num_layers", "=", "2", ",", "choice_weights", "=", "None", ")", ":", "\n", "        ", "self", ".", "ops", "=", "ops", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "choice_weights", "=", "choice_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.RandAugment.__call__": [[647, 654], ["numpy.random.choice", "op"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "# no replacement when using weighted choice", "\n", "        ", "ops", "=", "np", ".", "random", ".", "choice", "(", "\n", "self", ".", "ops", ",", "self", ".", "num_layers", ",", "replace", "=", "self", ".", "choice_weights", "is", "None", ",", "p", "=", "self", ".", "choice_weights", ")", "\n", "for", "op", "in", "ops", ":", "\n", "            ", "img", "=", "op", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.RandAugment.__repr__": [[655, 661], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(n={self.num_layers}, ops='", "\n", "for", "op", "in", "self", ".", "ops", ":", "\n", "            ", "fs", "+=", "f'\\n\\t{op}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment.__init__": [[751, 757], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ops", ",", "alpha", "=", "1.", ",", "width", "=", "3", ",", "depth", "=", "-", "1", ",", "blended", "=", "False", ")", ":", "\n", "        ", "self", ".", "ops", "=", "ops", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "blended", "=", "blended", "# blended mode is faster but not well tested", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._calc_blended_weights": [[758, 767], ["numpy.array", "rws.append"], "methods", ["None"], ["", "def", "_calc_blended_weights", "(", "self", ",", "ws", ",", "m", ")", ":", "\n", "        ", "ws", "=", "ws", "*", "m", "\n", "cump", "=", "1.", "\n", "rws", "=", "[", "]", "\n", "for", "w", "in", "ws", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "alpha", "=", "w", "/", "cump", "\n", "cump", "*=", "(", "1", "-", "alpha", ")", "\n", "rws", ".", "append", "(", "alpha", ")", "\n", "", "return", "np", ".", "array", "(", "rws", "[", ":", ":", "-", "1", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._apply_blended": [[768, 783], ["PIL.Image.blend.copy", "auto_augment.AugMixAugment._calc_blended_weights", "numpy.random.choice", "PIL.Image.blend", "numpy.random.randint", "op"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._calc_blended_weights"], ["", "def", "_apply_blended", "(", "self", ",", "img", ",", "mixing_weights", ",", "m", ")", ":", "\n", "# This is my first crack and implementing a slightly faster mixed augmentation. Instead", "\n", "# of accumulating the mix for each chain in a Numpy array and then blending with original,", "\n", "# it recomputes the blending coefficients and applies one PIL image blend per chain.", "\n", "# TODO the results appear in the right ballpark but they differ by more than rounding.", "\n", "        ", "img_orig", "=", "img", ".", "copy", "(", ")", "\n", "ws", "=", "self", ".", "_calc_blended_weights", "(", "mixing_weights", ",", "m", ")", "\n", "for", "w", "in", "ws", ":", "\n", "            ", "depth", "=", "self", ".", "depth", "if", "self", ".", "depth", ">", "0", "else", "np", ".", "random", ".", "randint", "(", "1", ",", "4", ")", "\n", "ops", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "ops", ",", "depth", ",", "replace", "=", "True", ")", "\n", "img_aug", "=", "img_orig", "# no ops are in-place, deep copy not necessary", "\n", "for", "op", "in", "ops", ":", "\n", "                ", "img_aug", "=", "op", "(", "img_aug", ")", "\n", "", "img", "=", "Image", ".", "blend", "(", "img", ",", "img_aug", ",", "w", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._apply_basic": [[784, 800], ["numpy.zeros", "numpy.clip", "PIL.Image.fromarray", "PIL.Image.blend", "len", "numpy.random.choice", "PIL.Image.fromarray.astype", "img.getbands", "numpy.random.randint", "op", "numpy.asarray"], "methods", ["None"], ["", "def", "_apply_basic", "(", "self", ",", "img", ",", "mixing_weights", ",", "m", ")", ":", "\n", "# This is a literal adaptation of the paper/official implementation without normalizations and", "\n", "# PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the", "\n", "# typical augmentation transforms, could use a GPU / Kornia implementation.", "\n", "        ", "img_shape", "=", "img", ".", "size", "[", "0", "]", ",", "img", ".", "size", "[", "1", "]", ",", "len", "(", "img", ".", "getbands", "(", ")", ")", "\n", "mixed", "=", "np", ".", "zeros", "(", "img_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "mw", "in", "mixing_weights", ":", "\n", "            ", "depth", "=", "self", ".", "depth", "if", "self", ".", "depth", ">", "0", "else", "np", ".", "random", ".", "randint", "(", "1", ",", "4", ")", "\n", "ops", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "ops", ",", "depth", ",", "replace", "=", "True", ")", "\n", "img_aug", "=", "img", "# no ops are in-place, deep copy not necessary", "\n", "for", "op", "in", "ops", ":", "\n", "                ", "img_aug", "=", "op", "(", "img_aug", ")", "\n", "", "mixed", "+=", "mw", "*", "np", ".", "asarray", "(", "img_aug", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "np", ".", "clip", "(", "mixed", ",", "0", ",", "255.", ",", "out", "=", "mixed", ")", "\n", "mixed", "=", "Image", ".", "fromarray", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "return", "Image", ".", "blend", "(", "img", ",", "mixed", ",", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment.__call__": [[801, 809], ["numpy.float32", "numpy.float32", "numpy.random.dirichlet", "numpy.random.beta", "auto_augment.AugMixAugment._apply_blended", "auto_augment.AugMixAugment._apply_basic"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._apply_blended", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment._apply_basic"], ["", "def", "__call__", "(", "self", ",", "img", ")", ":", "\n", "        ", "mixing_weights", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "dirichlet", "(", "[", "self", ".", "alpha", "]", "*", "self", ".", "width", ")", ")", "\n", "m", "=", "np", ".", "float32", "(", "np", ".", "random", ".", "beta", "(", "self", ".", "alpha", ",", "self", ".", "alpha", ")", ")", "\n", "if", "self", ".", "blended", ":", "\n", "            ", "mixed", "=", "self", ".", "_apply_blended", "(", "img", ",", "mixing_weights", ",", "m", ")", "\n", "", "else", ":", "\n", "            ", "mixed", "=", "self", ".", "_apply_basic", "(", "img", ",", "mixing_weights", ",", "m", ")", "\n", "", "return", "mixed", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.AugMixAugment.__repr__": [[810, 816], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "fs", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(alpha={self.alpha}, width={self.width}, depth={self.depth}, ops='", "\n", "for", "op", "in", "self", ".", "ops", ":", "\n", "            ", "fs", "+=", "f'\\n\\t{op}'", "\n", "", "fs", "+=", "')'", "\n", "return", "fs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._interpolation": [[42, 48], ["kwargs.pop", "isinstance", "random.choice"], "function", ["None"], ["def", "_interpolation", "(", "kwargs", ")", ":", "\n", "    ", "interpolation", "=", "kwargs", ".", "pop", "(", "'resample'", ",", "Image", ".", "BILINEAR", ")", "\n", "if", "isinstance", "(", "interpolation", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "return", "random", ".", "choice", "(", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "return", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf": [[50, 54], ["auto_augment._interpolation", "kwargs.pop"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._interpolation"], ["", "", "def", "_check_args_tf", "(", "kwargs", ")", ":", "\n", "    ", "if", "'fillcolor'", "in", "kwargs", "and", "_PIL_VER", "<", "(", "5", ",", "0", ")", ":", "\n", "        ", "kwargs", ".", "pop", "(", "'fillcolor'", ")", "\n", "", "kwargs", "[", "'resample'", "]", "=", "_interpolation", "(", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.shear_x": [[56, 59], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "shear_x", "(", "img", ",", "factor", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "factor", ",", "0", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.shear_y": [[61, 64], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "shear_y", "(", "img", ",", "factor", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "factor", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.translate_x_rel": [[66, 70], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "translate_x_rel", "(", "img", ",", "pct", ",", "**", "kwargs", ")", ":", "\n", "    ", "pixels", "=", "pct", "*", "img", ".", "size", "[", "0", "]", "\n", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "pixels", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.translate_y_rel": [[72, 76], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "translate_y_rel", "(", "img", ",", "pct", ",", "**", "kwargs", ")", ":", "\n", "    ", "pixels", "=", "pct", "*", "img", ".", "size", "[", "1", "]", "\n", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "pixels", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.translate_x_abs": [[78, 81], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "translate_x_abs", "(", "img", ",", "pixels", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "pixels", ",", "0", ",", "1", ",", "0", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.translate_y_abs": [[83, 86], ["auto_augment._check_args_tf", "img.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "translate_y_abs", "(", "img", ",", "pixels", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "(", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "pixels", ")", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rotate": [[88, 118], ["auto_augment._check_args_tf", "img.rotate", "auto_augment.rotate.transform"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._check_args_tf", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rotate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "rotate", "(", "img", ",", "degrees", ",", "**", "kwargs", ")", ":", "\n", "    ", "_check_args_tf", "(", "kwargs", ")", "\n", "if", "_PIL_VER", ">=", "(", "5", ",", "2", ")", ":", "\n", "        ", "return", "img", ".", "rotate", "(", "degrees", ",", "**", "kwargs", ")", "\n", "", "elif", "_PIL_VER", ">=", "(", "5", ",", "0", ")", ":", "\n", "        ", "w", ",", "h", "=", "img", ".", "size", "\n", "post_trans", "=", "(", "0", ",", "0", ")", "\n", "rotn_center", "=", "(", "w", "/", "2.0", ",", "h", "/", "2.0", ")", "\n", "angle", "=", "-", "math", ".", "radians", "(", "degrees", ")", "\n", "matrix", "=", "[", "\n", "round", "(", "math", ".", "cos", "(", "angle", ")", ",", "15", ")", ",", "\n", "round", "(", "math", ".", "sin", "(", "angle", ")", ",", "15", ")", ",", "\n", "0.0", ",", "\n", "round", "(", "-", "math", ".", "sin", "(", "angle", ")", ",", "15", ")", ",", "\n", "round", "(", "math", ".", "cos", "(", "angle", ")", ",", "15", ")", ",", "\n", "0.0", ",", "\n", "]", "\n", "\n", "def", "transform", "(", "x", ",", "y", ",", "matrix", ")", ":", "\n", "            ", "(", "a", ",", "b", ",", "c", ",", "d", ",", "e", ",", "f", ")", "=", "matrix", "\n", "return", "a", "*", "x", "+", "b", "*", "y", "+", "c", ",", "d", "*", "x", "+", "e", "*", "y", "+", "f", "\n", "\n", "", "matrix", "[", "2", "]", ",", "matrix", "[", "5", "]", "=", "transform", "(", "\n", "-", "rotn_center", "[", "0", "]", "-", "post_trans", "[", "0", "]", ",", "-", "rotn_center", "[", "1", "]", "-", "post_trans", "[", "1", "]", ",", "matrix", "\n", ")", "\n", "matrix", "[", "2", "]", "+=", "rotn_center", "[", "0", "]", "\n", "matrix", "[", "5", "]", "+=", "rotn_center", "[", "1", "]", "\n", "return", "img", ".", "transform", "(", "img", ".", "size", ",", "Image", ".", "AFFINE", ",", "matrix", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "return", "img", ".", "rotate", "(", "degrees", ",", "resample", "=", "kwargs", "[", "'resample'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_contrast": [[120, 122], ["PIL.ImageOps.autocontrast"], "function", ["None"], ["", "", "def", "auto_contrast", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "autocontrast", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.invert": [[124, 126], ["PIL.ImageOps.invert"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.invert"], ["", "def", "invert", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "invert", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.equalize": [[128, 130], ["PIL.ImageOps.equalize"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.equalize"], ["", "def", "equalize", "(", "img", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "equalize", "(", "img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.solarize": [[132, 134], ["PIL.ImageOps.solarize"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.solarize"], ["", "def", "solarize", "(", "img", ",", "thresh", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageOps", ".", "solarize", "(", "img", ",", "thresh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.solarize_add": [[136, 149], ["range", "img.point", "lut.append", "lut.append", "min", "len"], "function", ["None"], ["", "def", "solarize_add", "(", "img", ",", "add", ",", "thresh", "=", "128", ",", "**", "__", ")", ":", "\n", "    ", "lut", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "256", ")", ":", "\n", "        ", "if", "i", "<", "thresh", ":", "\n", "            ", "lut", ".", "append", "(", "min", "(", "255", ",", "i", "+", "add", ")", ")", "\n", "", "else", ":", "\n", "            ", "lut", ".", "append", "(", "i", ")", "\n", "", "", "if", "img", ".", "mode", "in", "(", "\"L\"", ",", "\"RGB\"", ")", ":", "\n", "        ", "if", "img", ".", "mode", "==", "\"RGB\"", "and", "len", "(", "lut", ")", "==", "256", ":", "\n", "            ", "lut", "=", "lut", "+", "lut", "+", "lut", "\n", "", "return", "img", ".", "point", "(", "lut", ")", "\n", "", "else", ":", "\n", "        ", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.posterize": [[151, 155], ["PIL.ImageOps.posterize"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.posterize"], ["", "", "def", "posterize", "(", "img", ",", "bits_to_keep", ",", "**", "__", ")", ":", "\n", "    ", "if", "bits_to_keep", ">=", "8", ":", "\n", "        ", "return", "img", "\n", "", "return", "ImageOps", ".", "posterize", "(", "img", ",", "bits_to_keep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.contrast": [[157, 159], ["PIL.ImageEnhance.Contrast().enhance", "PIL.ImageEnhance.Contrast"], "function", ["None"], ["", "def", "contrast", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Contrast", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.color": [[161, 163], ["PIL.ImageEnhance.Color().enhance", "PIL.ImageEnhance.Color"], "function", ["None"], ["", "def", "color", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Color", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.brightness": [[165, 167], ["PIL.ImageEnhance.Brightness().enhance", "PIL.ImageEnhance.Brightness"], "function", ["None"], ["", "def", "brightness", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Brightness", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.sharpness": [[169, 171], ["PIL.ImageEnhance.Sharpness().enhance", "PIL.ImageEnhance.Sharpness"], "function", ["None"], ["", "def", "sharpness", "(", "img", ",", "factor", ",", "**", "__", ")", ":", "\n", "    ", "return", "ImageEnhance", ".", "Sharpness", "(", "img", ")", ".", "enhance", "(", "factor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate": [[173, 176], ["random.random"], "function", ["None"], ["", "def", "_randomly_negate", "(", "v", ")", ":", "\n", "    ", "\"\"\"With 50% prob, negate the value\"\"\"", "\n", "return", "-", "v", "if", "random", ".", "random", "(", ")", ">", "0.5", "else", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._rotate_level_to_arg": [[178, 183], ["auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate"], ["", "def", "_rotate_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [-30, 30]", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "30.", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._enhance_level_to_arg": [[185, 188], ["None"], "function", ["None"], ["", "def", "_enhance_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0.1, 1.9]", "\n", "    ", "return", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "1.8", "+", "0.1", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._enhance_increasing_level_to_arg": [[190, 196], ["max", "auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate"], ["", "def", "_enhance_increasing_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend", "\n", "# range [0.1, 1.9] if level <= _LEVEL_DENOM", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", ".9", "\n", "level", "=", "max", "(", "0.1", ",", "1.0", "+", "_randomly_negate", "(", "level", ")", ")", "# keep it >= 0.1", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._shear_level_to_arg": [[198, 203], ["auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate"], ["", "def", "_shear_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [-0.3, 0.3]", "\n", "    ", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "0.3", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._translate_abs_level_to_arg": [[205, 210], ["auto_augment._randomly_negate", "float"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate"], ["", "def", "_translate_abs_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "    ", "translate_const", "=", "hparams", "[", "'translate_const'", "]", "\n", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "float", "(", "translate_const", ")", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._translate_rel_level_to_arg": [[212, 218], ["hparams.get", "auto_augment._randomly_negate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._randomly_negate"], ["", "def", "_translate_rel_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "# default range [-0.45, 0.45]", "\n", "    ", "translate_pct", "=", "hparams", ".", "get", "(", "'translate_pct'", ",", "0.45", ")", "\n", "level", "=", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "translate_pct", "\n", "level", "=", "_randomly_negate", "(", "level", ")", "\n", "return", "level", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._posterize_level_to_arg": [[220, 225], ["int"], "function", ["None"], ["", "def", "_posterize_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# As per Tensorflow TPU EfficientNet impl", "\n", "# range [0, 4], 'keep 0 up to 4 MSB of original image'", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "4", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._posterize_increasing_level_to_arg": [[227, 232], ["auto_augment._posterize_level_to_arg"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._posterize_level_to_arg"], ["", "def", "_posterize_increasing_level_to_arg", "(", "level", ",", "hparams", ")", ":", "\n", "# As per Tensorflow models research and UDA impl", "\n", "# range [4, 0], 'keep 4 down to 0 MSB of original image',", "\n", "# intensity/severity of augmentation increases with level", "\n", "    ", "return", "4", "-", "_posterize_level_to_arg", "(", "level", ",", "hparams", ")", "[", "0", "]", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._posterize_original_level_to_arg": [[234, 239], ["int"], "function", ["None"], ["", "def", "_posterize_original_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# As per original AutoAugment paper description", "\n", "# range [4, 8], 'keep 4 up to 8 MSB of image'", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "4", ")", "+", "4", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._solarize_level_to_arg": [[241, 245], ["int"], "function", ["None"], ["", "def", "_solarize_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 256]", "\n", "# intensity/severity of augmentation decreases with level", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "256", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._solarize_increasing_level_to_arg": [[247, 251], ["auto_augment._solarize_level_to_arg"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._solarize_level_to_arg"], ["", "def", "_solarize_increasing_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 256]", "\n", "# intensity/severity of augmentation increases with level", "\n", "    ", "return", "256", "-", "_solarize_level_to_arg", "(", "level", ",", "_hparams", ")", "[", "0", "]", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._solarize_add_level_to_arg": [[253, 256], ["int"], "function", ["None"], ["", "def", "_solarize_add_level_to_arg", "(", "level", ",", "_hparams", ")", ":", "\n", "# range [0, 110]", "\n", "    ", "return", "int", "(", "(", "level", "/", "_LEVEL_DENOM", ")", "*", "110", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_v0": [[364, 395], ["auto_augment.AugmentOp"], "function", ["None"], ["", "", "def", "auto_augment_policy_v0", "(", "hparams", ")", ":", "\n", "# ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "1", ")", ",", "(", "'ShearY'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "1", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.8", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.4", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.2", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "8", ")", ",", "(", "'SolarizeAdd'", ",", "0.8", ",", "3", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.2", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "1", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.4", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "9", ")", ",", "(", "'ShearY'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Posterize'", ",", "0.4", ",", "6", ")", ",", "(", "'AutoContrast'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Color'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.2", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "1.0", ",", "7", ")", ",", "(", "'TranslateYRel'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.0", ",", "0", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.8", ",", "0", ")", ",", "(", "'Color'", ",", "0.6", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "1.0", ",", "0", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "4", ")", ",", "(", "'Equalize'", ",", "0.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "4", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.4", ",", "7", ")", ",", "(", "'SolarizeAdd'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'Posterize'", ",", "0.8", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "10", ")", "]", ",", "# This results in black image with Tpu posterize", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "1", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "6", ")", ",", "(", "'Rotate'", ",", "0.4", ",", "5", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_v0r": [[397, 429], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_v0r", "(", "hparams", ")", ":", "\n", "# ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used", "\n", "# in Google research implementation (number of bits discarded increases with magnitude)", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "1", ")", ",", "(", "'ShearY'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "1", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.8", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.4", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.2", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "8", ")", ",", "(", "'SolarizeAdd'", ",", "0.8", ",", "3", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.2", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "1", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.4", ",", "9", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "9", ")", ",", "(", "'ShearY'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "0", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "6", ")", ",", "(", "'AutoContrast'", ",", "0.4", ",", "7", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Color'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.2", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "1.0", ",", "7", ")", ",", "(", "'TranslateYRel'", ",", "0.8", ",", "9", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.0", ",", "0", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "4", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.8", ",", "0", ")", ",", "(", "'Color'", ",", "0.6", ",", "4", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "1.0", ",", "0", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "4", ")", ",", "(", "'Equalize'", ",", "0.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "1.0", ",", "4", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'ShearY'", ",", "0.4", ",", "7", ")", ",", "(", "'SolarizeAdd'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.8", ",", "2", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "10", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "1", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "6", ")", ",", "(", "'Rotate'", ",", "0.4", ",", "5", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_original": [[431, 462], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_original", "(", "hparams", ")", ":", "\n", "# ImageNet policy from https://arxiv.org/abs/1805.09501", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.4", ",", "8", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.6", ",", "7", ")", ",", "(", "'PosterizeOriginal'", ",", "0.6", ",", "6", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeOriginal'", ",", "0.8", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.2", ",", "3", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.6", ",", "8", ")", ",", "(", "'PosterizeOriginal'", ",", "0.4", ",", "6", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "0.4", ",", "0", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.0", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "8", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "7", ")", "]", ",", "\n", "[", "(", "'Sharpness'", ",", "0.4", ",", "7", ")", ",", "(", "'Invert'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.6", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "9", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_originalr": [[464, 495], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "auto_augment_policy_originalr", "(", "hparams", ")", ":", "\n", "# ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation", "\n", "    ", "policy", "=", "[", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "8", ")", ",", "(", "'Rotate'", ",", "0.6", ",", "9", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.6", ",", "7", ")", ",", "(", "'PosterizeIncreasing'", ",", "0.6", ",", "6", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "4", ")", ",", "(", "'Rotate'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "3", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "7", ")", "]", ",", "\n", "[", "(", "'PosterizeIncreasing'", ",", "0.8", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.2", ",", "3", ")", ",", "(", "'Solarize'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.6", ",", "8", ")", ",", "(", "'PosterizeIncreasing'", ",", "0.4", ",", "6", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "0.4", ",", "0", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.4", ",", "9", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "2", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.0", ",", "7", ")", ",", "(", "'Equalize'", ",", "0.8", ",", "8", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Rotate'", ",", "0.8", ",", "8", ")", ",", "(", "'Color'", ",", "1.0", ",", "2", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.8", ",", "8", ")", ",", "(", "'Solarize'", ",", "0.8", ",", "7", ")", "]", ",", "\n", "[", "(", "'Sharpness'", ",", "0.4", ",", "7", ")", ",", "(", "'Invert'", ",", "0.6", ",", "8", ")", "]", ",", "\n", "[", "(", "'ShearX'", ",", "0.6", ",", "5", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "9", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.4", ",", "0", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.4", ",", "7", ")", ",", "(", "'Solarize'", ",", "0.2", ",", "4", ")", "]", ",", "\n", "[", "(", "'Solarize'", ",", "0.6", ",", "5", ")", ",", "(", "'AutoContrast'", ",", "0.6", ",", "5", ")", "]", ",", "\n", "[", "(", "'Invert'", ",", "0.6", ",", "4", ")", ",", "(", "'Equalize'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Color'", ",", "0.6", ",", "4", ")", ",", "(", "'Contrast'", ",", "1.0", ",", "8", ")", "]", ",", "\n", "[", "(", "'Equalize'", ",", "0.8", ",", "8", ")", ",", "(", "'Equalize'", ",", "0.6", ",", "3", ")", "]", ",", "\n", "]", "\n", "pc", "=", "[", "[", "AugmentOp", "(", "*", "a", ",", "hparams", "=", "hparams", ")", "for", "a", "in", "sp", "]", "for", "sp", "in", "policy", "]", "\n", "return", "pc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy": [[497, 509], ["auto_augment.auto_augment_policy_original", "auto_augment.auto_augment_policy_originalr", "auto_augment.auto_augment_policy_v0", "auto_augment.auto_augment_policy_v0r"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_original", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_originalr", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_v0", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy_v0r"], ["", "def", "auto_augment_policy", "(", "name", "=", "'v0'", ",", "hparams", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "if", "name", "==", "'original'", ":", "\n", "        ", "return", "auto_augment_policy_original", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'originalr'", ":", "\n", "        ", "return", "auto_augment_policy_originalr", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'v0'", ":", "\n", "        ", "return", "auto_augment_policy_v0", "(", "hparams", ")", "\n", "", "elif", "name", "==", "'v0r'", ":", "\n", "        ", "return", "auto_augment_policy_v0r", "(", "hparams", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "'Unknown AA policy (%s)'", "%", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_transform": [[532, 561], ["config_str.split", "auto_augment.auto_augment_policy", "auto_augment.AutoAugment", "re.split", "len", "hparams.setdefault", "float"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.auto_augment_policy"], ["", "", "def", "auto_augment_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"\n    Create a AutoAugment transform\n\n    :param config_str: String defining configuration of auto augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the AutoAugment policy (one of 'v0', 'v0r', 'original', 'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "policy_name", "=", "config", "[", "0", "]", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param injected via hparams for now", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "val", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown AutoAugment config section'", "\n", "", "", "aa_policy", "=", "auto_augment_policy", "(", "policy_name", ",", "hparams", "=", "hparams", ")", "\n", "return", "AutoAugment", "(", "aa_policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._select_rand_weights": [[625, 632], ["numpy.sum"], "function", ["None"], ["def", "_select_rand_weights", "(", "weight_idx", "=", "0", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "transforms", "=", "transforms", "or", "_RAND_TRANSFORMS", "\n", "assert", "weight_idx", "==", "0", "# only one set of weights currently", "\n", "rand_weights", "=", "_RAND_CHOICE_WEIGHTS_0", "\n", "probs", "=", "[", "rand_weights", "[", "k", "]", "for", "k", "in", "transforms", "]", "\n", "probs", "/=", "np", ".", "sum", "(", "probs", ")", "\n", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rand_augment_ops": [[634, 639], ["auto_augment.AugmentOp"], "function", ["None"], ["", "def", "rand_augment_ops", "(", "magnitude", "=", "10", ",", "hparams", "=", "None", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "transforms", "=", "transforms", "or", "_RAND_TRANSFORMS", "\n", "return", "[", "AugmentOp", "(", "\n", "name", ",", "prob", "=", "0.5", ",", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "for", "name", "in", "transforms", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rand_augment_transform": [[663, 719], ["config_str.split", "auto_augment.rand_augment_ops", "auto_augment.RandAugment", "re.split", "auto_augment._select_rand_weights", "len", "float", "hparams.setdefault", "float", "hparams.setdefault", "int", "bool", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.rand_augment_ops", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment._select_rand_weights"], ["", "", "def", "rand_augment_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\"\n    Create a RandAugment transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        'mstd' -  float std deviation of magnitude noise applied, or uniform sampling if infinity (or > 100)\n        'mmax' - set upper bound for magnitude to something other than default of  _LEVEL_DENOM (10)\n        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n\n    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "magnitude", "=", "_LEVEL_DENOM", "# default to _LEVEL_DENOM for magnitude (currently 10)", "\n", "num_layers", "=", "2", "# default to 2 ops per image", "\n", "weight_idx", "=", "None", "# default to no probability weights for op choice", "\n", "transforms", "=", "_RAND_TRANSFORMS", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "assert", "config", "[", "0", "]", "==", "'rand'", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param / randomization of magnitude values", "\n", "            ", "mstd", "=", "float", "(", "val", ")", "\n", "if", "mstd", ">", "100", ":", "\n", "# use uniform sampling in 0 to magnitude if mstd is > 100", "\n", "                ", "mstd", "=", "float", "(", "'inf'", ")", "\n", "", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "mstd", ")", "\n", "", "elif", "key", "==", "'mmax'", ":", "\n", "# clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_max'", ",", "int", "(", "val", ")", ")", "\n", "", "elif", "key", "==", "'inc'", ":", "\n", "            ", "if", "bool", "(", "val", ")", ":", "\n", "                ", "transforms", "=", "_RAND_INCREASING_TRANSFORMS", "\n", "", "", "elif", "key", "==", "'m'", ":", "\n", "            ", "magnitude", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'n'", ":", "\n", "            ", "num_layers", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'w'", ":", "\n", "            ", "weight_idx", "=", "int", "(", "val", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown RandAugment config section'", "\n", "", "", "ra_ops", "=", "rand_augment_ops", "(", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ",", "transforms", "=", "transforms", ")", "\n", "choice_weights", "=", "None", "if", "weight_idx", "is", "None", "else", "_select_rand_weights", "(", "weight_idx", ")", "\n", "return", "RandAugment", "(", "ra_ops", ",", "num_layers", ",", "choice_weights", "=", "choice_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.augmix_ops": [[738, 743], ["auto_augment.AugmentOp"], "function", ["None"], ["def", "augmix_ops", "(", "magnitude", "=", "10", ",", "hparams", "=", "None", ",", "transforms", "=", "None", ")", ":", "\n", "    ", "hparams", "=", "hparams", "or", "_HPARAMS_DEFAULT", "\n", "transforms", "=", "transforms", "or", "_AUGMIX_TRANSFORMS", "\n", "return", "[", "AugmentOp", "(", "\n", "name", ",", "prob", "=", "1.0", ",", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "for", "name", "in", "transforms", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.augment_and_mix_transform": [[818, 866], ["config_str.split", "hparams.setdefault", "auto_augment.augmix_ops", "auto_augment.AugMixAugment", "re.split", "float", "len", "hparams.setdefault", "float", "int", "int", "int", "float", "bool"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.auto_augment.augmix_ops"], ["", "", "def", "augment_and_mix_transform", "(", "config_str", ",", "hparams", ")", ":", "\n", "    ", "\"\"\" Create AugMix PyTorch transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude (severity) of augmentation mix (default: 3)\n        'w' - integer width of augmentation chain (default: 3)\n        'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n        'b' - integer (bool), blend each branch of chain into end result without a final blend, less CPU (default: 0)\n        'mstd' -  float std deviation of magnitude noise applied (default: 0)\n    Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n    :param hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    :return: A PyTorch compatible Transform\n    \"\"\"", "\n", "magnitude", "=", "3", "\n", "width", "=", "3", "\n", "depth", "=", "-", "1", "\n", "alpha", "=", "1.", "\n", "blended", "=", "False", "\n", "config", "=", "config_str", ".", "split", "(", "'-'", ")", "\n", "assert", "config", "[", "0", "]", "==", "'augmix'", "\n", "config", "=", "config", "[", "1", ":", "]", "\n", "for", "c", "in", "config", ":", "\n", "        ", "cs", "=", "re", ".", "split", "(", "r'(\\d.*)'", ",", "c", ")", "\n", "if", "len", "(", "cs", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "key", ",", "val", "=", "cs", "[", ":", "2", "]", "\n", "if", "key", "==", "'mstd'", ":", "\n", "# noise param injected via hparams for now", "\n", "            ", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "val", ")", ")", "\n", "", "elif", "key", "==", "'m'", ":", "\n", "            ", "magnitude", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'w'", ":", "\n", "            ", "width", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'d'", ":", "\n", "            ", "depth", "=", "int", "(", "val", ")", "\n", "", "elif", "key", "==", "'a'", ":", "\n", "            ", "alpha", "=", "float", "(", "val", ")", "\n", "", "elif", "key", "==", "'b'", ":", "\n", "            ", "blended", "=", "bool", "(", "val", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "'Unknown AugMix config section'", "\n", "", "", "hparams", ".", "setdefault", "(", "'magnitude_std'", ",", "float", "(", "'inf'", ")", ")", "# default to uniform sampling (if not set via mstd arg)", "\n", "ops", "=", "augmix_ops", "(", "magnitude", "=", "magnitude", ",", "hparams", "=", "hparams", ")", "\n", "return", "AugMixAugment", "(", "ops", ",", "alpha", "=", "alpha", ",", "width", "=", "width", ",", "depth", "=", "depth", ",", "blended", "=", "blended", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.ImageDataset.__init__": [[22, 39], ["isinstance", "parsers.create_parser"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_factory.create_parser"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "parser", "=", "None", ",", "\n", "class_map", "=", "None", ",", "\n", "load_bytes", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", "target_transform", "=", "None", ",", "\n", "train_val_list", "=", "None", "\n", ")", ":", "\n", "        ", "if", "parser", "is", "None", "or", "isinstance", "(", "parser", ",", "str", ")", ":", "\n", "            ", "parser", "=", "create_parser", "(", "parser", "or", "''", ",", "root", "=", "root", ",", "class_map", "=", "class_map", ",", "train_val_list", "=", "train_val_list", ")", "\n", "", "self", ".", "parser", "=", "parser", "\n", "self", ".", "load_bytes", "=", "load_bytes", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "_consecutive_errors", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.ImageDataset.__getitem__": [[40, 59], ["dataset.ImageDataset.transform", "dataset.ImageDataset.read", "PIL.Image.open().convert", "_logger.warning", "dataset.ImageDataset.target_transform", "dataset.ImageDataset.__getitem__", "PIL.Image.open", "dataset.ImageDataset.parser.filename", "str", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.__getitem__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filename"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "img", ",", "target", "=", "self", ".", "parser", "[", "index", "]", "\n", "try", ":", "\n", "            ", "img", "=", "img", ".", "read", "(", ")", "if", "self", ".", "load_bytes", "else", "Image", ".", "open", "(", "img", ")", ".", "convert", "(", "'RGB'", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "_logger", ".", "warning", "(", "f'Skipped sample (index {index}, file {self.parser.filename(index)}). {str(e)}'", ")", "\n", "self", ".", "_consecutive_errors", "+=", "1", "\n", "if", "self", ".", "_consecutive_errors", "<", "_ERROR_RETRY", ":", "\n", "                ", "return", "self", ".", "__getitem__", "(", "(", "index", "+", "1", ")", "%", "len", "(", "self", ".", "parser", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "e", "\n", "", "", "self", ".", "_consecutive_errors", "=", "0", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "if", "target", "is", "None", ":", "\n", "            ", "target", "=", "-", "1", "\n", "", "elif", "self", ".", "target_transform", "is", "not", "None", ":", "\n", "            ", "target", "=", "self", ".", "target_transform", "(", "target", ")", "\n", "", "return", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.ImageDataset.__len__": [[60, 62], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "parser", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.ImageDataset.filename": [[63, 65], ["dataset.ImageDataset.parser.filename"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filename"], ["", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filename", "(", "index", ",", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.ImageDataset.filenames": [[66, 68], ["dataset.ImageDataset.parser.filenames"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filenames"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filenames", "(", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.IterableImageDataset.__init__": [[72, 94], ["isinstance", "parsers.create_parser"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_factory.create_parser"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "parser", "=", "None", ",", "\n", "split", "=", "'train'", ",", "\n", "is_training", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "repeats", "=", "0", ",", "\n", "download", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", "target_transform", "=", "None", ",", "\n", ")", ":", "\n", "        ", "assert", "parser", "is", "not", "None", "\n", "if", "isinstance", "(", "parser", ",", "str", ")", ":", "\n", "            ", "self", ".", "parser", "=", "create_parser", "(", "\n", "parser", ",", "root", "=", "root", ",", "split", "=", "split", ",", "is_training", "=", "is_training", ",", "\n", "batch_size", "=", "batch_size", ",", "repeats", "=", "repeats", ",", "download", "=", "download", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "parser", "=", "parser", "\n", "", "self", ".", "transform", "=", "transform", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "_consecutive_errors", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.IterableImageDataset.__iter__": [[95, 102], ["dataset.IterableImageDataset.transform", "dataset.IterableImageDataset.target_transform"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "img", ",", "target", "in", "self", ".", "parser", ":", "\n", "            ", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "                ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "if", "self", ".", "target_transform", "is", "not", "None", ":", "\n", "                ", "target", "=", "self", ".", "target_transform", "(", "target", ")", "\n", "", "yield", "img", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.IterableImageDataset.__len__": [[103, 108], ["hasattr", "len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ".", "parser", ",", "'__len__'", ")", ":", "\n", "            ", "return", "len", "(", "self", ".", "parser", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.IterableImageDataset.filename": [[109, 111], ["None"], "methods", ["None"], ["", "", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "assert", "False", ",", "'Filename lookup by index not supported, use filenames().'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.IterableImageDataset.filenames": [[112, 114], ["dataset.IterableImageDataset.parser.filenames"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filenames"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "parser", ".", "filenames", "(", "basename", ",", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.__init__": [[119, 126], ["dataset.AugMixDataset._set_transforms"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._set_transforms"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_splits", "=", "2", ")", ":", "\n", "        ", "self", ".", "augmentation", "=", "None", "\n", "self", ".", "normalize", "=", "None", "\n", "self", ".", "dataset", "=", "dataset", "\n", "if", "self", ".", "dataset", ".", "transform", "is", "not", "None", ":", "\n", "            ", "self", ".", "_set_transforms", "(", "self", ".", "dataset", ".", "transform", ")", "\n", "", "self", ".", "num_splits", "=", "num_splits", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._set_transforms": [[127, 132], ["isinstance", "len"], "methods", ["None"], ["", "def", "_set_transforms", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "isinstance", "(", "x", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "x", ")", "==", "3", ",", "'Expecting a tuple/list of 3 transforms'", "\n", "self", ".", "dataset", ".", "transform", "=", "x", "[", "0", "]", "\n", "self", ".", "augmentation", "=", "x", "[", "1", "]", "\n", "self", ".", "normalize", "=", "x", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.transform": [[137, 140], ["dataset.AugMixDataset._set_transforms"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._set_transforms"], ["", "@", "transform", ".", "setter", "\n", "def", "transform", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "_set_transforms", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._normalize": [[141, 143], ["dataset.AugMixDataset.normalize"], "methods", ["None"], ["", "def", "_normalize", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "if", "self", ".", "normalize", "is", "None", "else", "self", ".", "normalize", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.__getitem__": [[144, 151], ["range", "dataset.AugMixDataset._normalize", "x_list.append", "tuple", "dataset.AugMixDataset._normalize", "dataset.AugMixDataset.augmentation"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._normalize", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset._normalize"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "dataset", "[", "i", "]", "# all splits share the same dataset base transform", "\n", "x_list", "=", "[", "self", ".", "_normalize", "(", "x", ")", "]", "# first split only normalizes (this is the 'clean' split)", "\n", "# run the full augmentation on the remaining splits", "\n", "for", "_", "in", "range", "(", "self", ".", "num_splits", "-", "1", ")", ":", "\n", "            ", "x_list", ".", "append", "(", "self", ".", "_normalize", "(", "self", ".", "augmentation", "(", "x", ")", ")", ")", "\n", "", "return", "tuple", "(", "x_list", ")", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset.AugMixDataset.__len__": [[152, 154], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup.__init__": [[104, 120], ["len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mixup_alpha", "=", "1.", ",", "cutmix_alpha", "=", "0.", ",", "cutmix_minmax", "=", "None", ",", "prob", "=", "1.0", ",", "switch_prob", "=", "0.5", ",", "\n", "mode", "=", "'batch'", ",", "correct_lam", "=", "True", ",", "label_smoothing", "=", "0.1", ",", "num_classes", "=", "1000", ")", ":", "\n", "        ", "self", ".", "mixup_alpha", "=", "mixup_alpha", "\n", "self", ".", "cutmix_alpha", "=", "cutmix_alpha", "\n", "self", ".", "cutmix_minmax", "=", "cutmix_minmax", "\n", "if", "self", ".", "cutmix_minmax", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "self", ".", "cutmix_minmax", ")", "==", "2", "\n", "# force cutmix alpha == 1.0 when minmax active to keep logic simple & safe", "\n", "self", ".", "cutmix_alpha", "=", "1.0", "\n", "", "self", ".", "mix_prob", "=", "prob", "\n", "self", ".", "switch_prob", "=", "switch_prob", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "correct_lam", "=", "correct_lam", "# correct lambda based on clipped area for cutmix", "\n", "self", ".", "mixup_enabled", "=", "True", "# set to false to disable mixing (intended tp be set by train loop)", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_elem": [[122, 141], ["numpy.ones", "numpy.zeros", "numpy.where", "numpy.where", "numpy.random.beta.astype", "numpy.random.rand", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta", "numpy.random.rand", "numpy.ones", "numpy.random.beta"], "methods", ["None"], ["", "def", "_params_per_elem", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "lam", "=", "np", ".", "ones", "(", "batch_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "use_cutmix", "=", "np", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "if", "self", ".", "mixup_enabled", ":", "\n", "            ", "if", "self", ".", "mixup_alpha", ">", "0.", "and", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "<", "self", ".", "switch_prob", "\n", "lam_mix", "=", "np", ".", "where", "(", "\n", "use_cutmix", ",", "\n", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ",", "size", "=", "batch_size", ")", ",", "\n", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ",", "size", "=", "batch_size", ")", ")", "\n", "", "elif", "self", ".", "mixup_alpha", ">", "0.", ":", "\n", "                ", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ",", "size", "=", "batch_size", ")", "\n", "", "elif", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "ones", "(", "batch_size", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ",", "size", "=", "batch_size", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"", "\n", "", "lam", "=", "np", ".", "where", "(", "np", ".", "random", ".", "rand", "(", "batch_size", ")", "<", "self", ".", "mix_prob", ",", "lam_mix", ".", "astype", "(", "np", ".", "float32", ")", ",", "lam", ")", "\n", "", "return", "lam", ",", "use_cutmix", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_batch": [[142, 159], ["float", "numpy.random.rand", "numpy.random.rand", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta", "numpy.random.beta"], "methods", ["None"], ["", "def", "_params_per_batch", "(", "self", ")", ":", "\n", "        ", "lam", "=", "1.", "\n", "use_cutmix", "=", "False", "\n", "if", "self", ".", "mixup_enabled", "and", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "mix_prob", ":", "\n", "            ", "if", "self", ".", "mixup_alpha", ">", "0.", "and", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "switch_prob", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ")", "if", "use_cutmix", "else", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ")", "\n", "", "elif", "self", ".", "mixup_alpha", ">", "0.", ":", "\n", "                ", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "mixup_alpha", ",", "self", ".", "mixup_alpha", ")", "\n", "", "elif", "self", ".", "cutmix_alpha", ">", "0.", ":", "\n", "                ", "use_cutmix", "=", "True", "\n", "lam_mix", "=", "np", ".", "random", ".", "beta", "(", "self", ".", "cutmix_alpha", ",", "self", ".", "cutmix_alpha", ")", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"", "\n", "", "lam", "=", "float", "(", "lam_mix", ")", "\n", "", "return", "lam", ",", "use_cutmix", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_elem": [[160, 176], ["len", "mixup.Mixup._params_per_elem", "x.clone", "range", "torch.tensor().unsqueeze", "torch.tensor", "mixup.cutmix_bbox_and_lam"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_elem", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "x", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", ")", "\n", "x_orig", "=", "x", ".", "clone", "(", ")", "# need to keep an unmodified original for mixing source", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", "[", "i", "]", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "x", "[", "i", "]", "=", "x", "[", "i", "]", "*", "lam", "+", "x_orig", "[", "j", "]", "*", "(", "1", "-", "lam", ")", "\n", "", "", "", "return", "torch", ".", "tensor", "(", "lam_batch", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_pair": [[177, 196], ["len", "mixup.Mixup._params_per_elem", "x.clone", "range", "numpy.concatenate", "torch.tensor().unsqueeze", "torch.tensor", "mixup.cutmix_bbox_and_lam"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_pair", "(", "self", ",", "x", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "x", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", "//", "2", ")", "\n", "x_orig", "=", "x", ".", "clone", "(", ")", "# need to keep an unmodified original for mixing source", "\n", "for", "i", "in", "range", "(", "batch_size", "//", "2", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", "[", "i", "]", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "x", "[", "j", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x_orig", "[", "i", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "x", "[", "i", "]", "=", "x", "[", "i", "]", "*", "lam", "+", "x_orig", "[", "j", "]", "*", "(", "1", "-", "lam", ")", "\n", "x", "[", "j", "]", "=", "x", "[", "j", "]", "*", "lam", "+", "x_orig", "[", "i", "]", "*", "(", "1", "-", "lam", ")", "\n", "", "", "", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "lam_batch", "[", ":", ":", "-", "1", "]", ")", ")", "\n", "return", "torch", ".", "tensor", "(", "lam_batch", ",", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_batch": [[197, 209], ["mixup.Mixup._params_per_batch", "mixup.cutmix_bbox_and_lam", "x.flip().mul_", "x.mul_().add_", "x.flip", "x.flip", "x.mul_"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_batch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_batch", "(", "self", ",", "x", ")", ":", "\n", "        ", "lam", ",", "use_cutmix", "=", "self", ".", "_params_per_batch", "(", ")", "\n", "if", "lam", "==", "1.", ":", "\n", "            ", "return", "1.", "\n", "", "if", "use_cutmix", ":", "\n", "            ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x", ".", "flip", "(", "0", ")", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "", "else", ":", "\n", "            ", "x_flipped", "=", "x", ".", "flip", "(", "0", ")", ".", "mul_", "(", "1.", "-", "lam", ")", "\n", "x", ".", "mul_", "(", "lam", ")", ".", "add_", "(", "x_flipped", ")", "\n", "", "return", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup.__call__": [[210, 220], ["mixup.mixup_target", "mixup.Mixup._mix_elem", "len", "mixup.Mixup._mix_pair", "mixup.Mixup._mix_batch"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.mixup_target", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_elem", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_pair", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._mix_batch"], ["", "def", "__call__", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "assert", "len", "(", "x", ")", "%", "2", "==", "0", ",", "'Batch size should be even when using this'", "\n", "if", "self", ".", "mode", "==", "'elem'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_elem", "(", "x", ")", "\n", "", "elif", "self", ".", "mode", "==", "'pair'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_pair", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_batch", "(", "x", ")", "\n", "", "target", "=", "mixup_target", "(", "target", ",", "self", ".", "num_classes", ",", "lam", ",", "self", ".", "label_smoothing", ",", "x", ".", "device", ")", "\n", "return", "x", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup.__init__": [[228, 233], ["mixup.Mixup.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "mixup_alpha", "=", "1.", ",", "cutmix_alpha", "=", "0.", ",", "cutmix_minmax", "=", "None", ",", "prob", "=", "1.0", ",", "switch_prob", "=", "0.5", ",", "\n", "mode", "=", "'batch'", ",", "correct_lam", "=", "True", ",", "label_smoothing", "=", "0.1", ",", "num_classes", "=", "1000", ")", ":", "\n", "        ", "super", "(", "FastCollateMixup", ",", "self", ")", ".", "__init__", "(", "\n", "mixup_alpha", "=", "mixup_alpha", ",", "cutmix_alpha", "=", "cutmix_alpha", ",", "cutmix_minmax", "=", "cutmix_minmax", ",", "prob", "=", "prob", ",", "switch_prob", "=", "switch_prob", ",", "\n", "mode", "=", "mode", ",", "correct_lam", "=", "correct_lam", ",", "label_smoothing", "=", "label_smoothing", ",", "num_classes", "=", "num_classes", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_elem_collate": [[234, 258], ["len", "mixup.FastCollateMixup._params_per_elem", "range", "torch.tensor().unsqueeze", "len", "torch.from_numpy", "numpy.concatenate", "mixed.copy.copy.astype", "torch.tensor", "mixup.cutmix_bbox_and_lam", "numpy.rint", "numpy.ones", "mixed.copy.copy.copy", "mixed.copy.copy.astype", "[].astype"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_elem_collate", "(", "self", ",", "output", ",", "batch", ",", "half", "=", "False", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "num_elem", "=", "batch_size", "//", "2", "if", "half", "else", "batch_size", "\n", "assert", "len", "(", "output", ")", "==", "num_elem", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "num_elem", ")", "\n", "for", "i", "in", "range", "(", "num_elem", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "mixed", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "if", "not", "half", ":", "\n", "                        ", "mixed", "=", "mixed", ".", "copy", "(", ")", "\n", "", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "mixed", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "batch", "[", "j", "]", "[", "0", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "batch", "[", "j", "]", "[", "0", "]", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "np", ".", "rint", "(", "mixed", ",", "out", "=", "mixed", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "if", "half", ":", "\n", "            ", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "np", ".", "ones", "(", "num_elem", ")", ")", ")", "\n", "", "return", "torch", ".", "tensor", "(", "lam_batch", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_pair_collate": [[259, 286], ["len", "mixup.FastCollateMixup._params_per_elem", "range", "numpy.concatenate", "torch.tensor().unsqueeze", "torch.from_numpy", "torch.from_numpy", "mixed_i.astype", "mixed_j.astype", "torch.tensor", "mixup.cutmix_bbox_and_lam", "mixed_i[].copy", "numpy.rint", "numpy.rint", "mixed_i.astype", "mixed_j.astype", "mixed_j.astype", "mixed_i.astype"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_elem", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_pair_collate", "(", "self", ",", "output", ",", "batch", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "lam_batch", ",", "use_cutmix", "=", "self", ".", "_params_per_elem", "(", "batch_size", "//", "2", ")", "\n", "for", "i", "in", "range", "(", "batch_size", "//", "2", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "lam", "=", "lam_batch", "[", "i", "]", "\n", "mixed_i", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "mixed_j", "=", "batch", "[", "j", "]", "[", "0", "]", "\n", "assert", "0", "<=", "lam", "<=", "1.0", "\n", "if", "lam", "<", "1.", ":", "\n", "                ", "if", "use_cutmix", "[", "i", "]", ":", "\n", "                    ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "patch_i", "=", "mixed_i", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", ".", "copy", "(", ")", "\n", "mixed_i", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "mixed_j", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "mixed_j", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "patch_i", "\n", "lam_batch", "[", "i", "]", "=", "lam", "\n", "", "else", ":", "\n", "                    ", "mixed_temp", "=", "mixed_i", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "mixed_j", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "mixed_j", "=", "mixed_j", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "mixed_i", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "mixed_i", "=", "mixed_temp", "\n", "np", ".", "rint", "(", "mixed_j", ",", "out", "=", "mixed_j", ")", "\n", "np", ".", "rint", "(", "mixed_i", ",", "out", "=", "mixed_i", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed_i", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "output", "[", "j", "]", "+=", "torch", ".", "from_numpy", "(", "mixed_j", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "lam_batch", "=", "np", ".", "concatenate", "(", "(", "lam_batch", ",", "lam_batch", "[", ":", ":", "-", "1", "]", ")", ")", "\n", "return", "torch", ".", "tensor", "(", "lam_batch", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_batch_collate": [[287, 305], ["len", "mixup.FastCollateMixup._params_per_batch", "range", "mixup.cutmix_bbox_and_lam", "torch.from_numpy", "mixed.copy.copy.astype", "mixed.copy.copy.copy", "numpy.rint", "mixed.copy.copy.astype", "[].astype"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.Mixup._params_per_batch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam"], ["", "def", "_mix_batch_collate", "(", "self", ",", "output", ",", "batch", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "lam", ",", "use_cutmix", "=", "self", ".", "_params_per_batch", "(", ")", "\n", "if", "use_cutmix", ":", "\n", "            ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "output", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "j", "=", "batch_size", "-", "i", "-", "1", "\n", "mixed", "=", "batch", "[", "i", "]", "[", "0", "]", "\n", "if", "lam", "!=", "1.", ":", "\n", "                ", "if", "use_cutmix", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "copy", "(", ")", "# don't want to modify the original while iterating", "\n", "mixed", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "batch", "[", "j", "]", "[", "0", "]", "[", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "\n", "", "else", ":", "\n", "                    ", "mixed", "=", "mixed", ".", "astype", "(", "np", ".", "float32", ")", "*", "lam", "+", "batch", "[", "j", "]", "[", "0", "]", ".", "astype", "(", "np", ".", "float32", ")", "*", "(", "1", "-", "lam", ")", "\n", "np", ".", "rint", "(", "mixed", ",", "out", "=", "mixed", ")", "\n", "", "", "output", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "mixed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "", "return", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup.__call__": [[306, 323], ["len", "torch.zeros", "torch.tensor", "mixup.mixup_target", "mixup.FastCollateMixup._mix_elem_collate", "mixup.FastCollateMixup._mix_pair_collate", "mixup.FastCollateMixup._mix_batch_collate"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.mixup_target", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_elem_collate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_pair_collate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.FastCollateMixup._mix_batch_collate"], ["", "def", "__call__", "(", "self", ",", "batch", ",", "_", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "batch", ")", "\n", "assert", "batch_size", "%", "2", "==", "0", ",", "'Batch size should be even when using this'", "\n", "half", "=", "'half'", "in", "self", ".", "mode", "\n", "if", "half", ":", "\n", "            ", "batch_size", "//=", "2", "\n", "", "output", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "if", "self", ".", "mode", "==", "'elem'", "or", "self", ".", "mode", "==", "'half'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_elem_collate", "(", "output", ",", "batch", ",", "half", "=", "half", ")", "\n", "", "elif", "self", ".", "mode", "==", "'pair'", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_pair_collate", "(", "output", ",", "batch", ")", "\n", "", "else", ":", "\n", "            ", "lam", "=", "self", ".", "_mix_batch_collate", "(", "output", ",", "batch", ")", "\n", "", "target", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "target", "=", "mixup_target", "(", "target", ",", "self", ".", "num_classes", ",", "lam", ",", "self", ".", "label_smoothing", ",", "device", "=", "'cpu'", ")", "\n", "target", "=", "target", "[", ":", "batch_size", "]", "\n", "return", "output", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.one_hot": [[17, 20], ["x.long().view.long().view", "torch.full().scatter_", "x.long().view.long", "torch.full", "x.long().view.size"], "function", ["None"], ["def", "one_hot", "(", "x", ",", "num_classes", ",", "on_value", "=", "1.", ",", "off_value", "=", "0.", ",", "device", "=", "'cuda'", ")", ":", "\n", "    ", "x", "=", "x", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "return", "torch", ".", "full", "(", "(", "x", ".", "size", "(", ")", "[", "0", "]", ",", "num_classes", ")", ",", "off_value", ",", "device", "=", "device", ")", ".", "scatter_", "(", "1", ",", "x", ",", "on_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.mixup_target": [[22, 28], ["mixup.one_hot", "mixup.one_hot", "target.flip"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.one_hot", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.one_hot"], ["", "def", "mixup_target", "(", "target", ",", "num_classes", ",", "lam", "=", "1.", ",", "smoothing", "=", "0.0", ",", "device", "=", "'cuda'", ")", ":", "\n", "    ", "off_value", "=", "smoothing", "/", "num_classes", "\n", "on_value", "=", "1.", "-", "smoothing", "+", "off_value", "\n", "y1", "=", "one_hot", "(", "target", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "y2", "=", "one_hot", "(", "target", ".", "flip", "(", "0", ")", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "return", "y1", "*", "lam", "+", "y2", "*", "(", "1.", "-", "lam", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.rand_bbox": [[30, 52], ["numpy.sqrt", "numpy.random.randint", "numpy.random.randint", "numpy.clip", "numpy.clip", "numpy.clip", "numpy.clip", "int", "int", "int", "int"], "function", ["None"], ["", "def", "rand_bbox", "(", "img_shape", ",", "lam", ",", "margin", "=", "0.", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"", "\n", "ratio", "=", "np", ".", "sqrt", "(", "1", "-", "lam", ")", "\n", "img_h", ",", "img_w", "=", "img_shape", "[", "-", "2", ":", "]", "\n", "cut_h", ",", "cut_w", "=", "int", "(", "img_h", "*", "ratio", ")", ",", "int", "(", "img_w", "*", "ratio", ")", "\n", "margin_y", ",", "margin_x", "=", "int", "(", "margin", "*", "cut_h", ")", ",", "int", "(", "margin", "*", "cut_w", ")", "\n", "cy", "=", "np", ".", "random", ".", "randint", "(", "0", "+", "margin_y", ",", "img_h", "-", "margin_y", ",", "size", "=", "count", ")", "\n", "cx", "=", "np", ".", "random", ".", "randint", "(", "0", "+", "margin_x", ",", "img_w", "-", "margin_x", ",", "size", "=", "count", ")", "\n", "yl", "=", "np", ".", "clip", "(", "cy", "-", "cut_h", "//", "2", ",", "0", ",", "img_h", ")", "\n", "yh", "=", "np", ".", "clip", "(", "cy", "+", "cut_h", "//", "2", ",", "0", ",", "img_h", ")", "\n", "xl", "=", "np", ".", "clip", "(", "cx", "-", "cut_w", "//", "2", ",", "0", ",", "img_w", ")", "\n", "xh", "=", "np", ".", "clip", "(", "cx", "+", "cut_w", "//", "2", ",", "0", ",", "img_w", ")", "\n", "return", "yl", ",", "yh", ",", "xl", ",", "xh", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.rand_bbox_minmax": [[54, 75], ["numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "len", "int", "int", "int", "int"], "function", ["None"], ["", "def", "rand_bbox_minmax", "(", "img_shape", ",", "minmax", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image size)\n        count (int): Number of bbox to generate\n    \"\"\"", "\n", "assert", "len", "(", "minmax", ")", "==", "2", "\n", "img_h", ",", "img_w", "=", "img_shape", "[", "-", "2", ":", "]", "\n", "cut_h", "=", "np", ".", "random", ".", "randint", "(", "int", "(", "img_h", "*", "minmax", "[", "0", "]", ")", ",", "int", "(", "img_h", "*", "minmax", "[", "1", "]", ")", ",", "size", "=", "count", ")", "\n", "cut_w", "=", "np", ".", "random", ".", "randint", "(", "int", "(", "img_w", "*", "minmax", "[", "0", "]", ")", ",", "int", "(", "img_w", "*", "minmax", "[", "1", "]", ")", ",", "size", "=", "count", ")", "\n", "yl", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "img_h", "-", "cut_h", ",", "size", "=", "count", ")", "\n", "xl", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "img_w", "-", "cut_w", ",", "size", "=", "count", ")", "\n", "yu", "=", "yl", "+", "cut_h", "\n", "xu", "=", "xl", "+", "cut_w", "\n", "return", "yl", ",", "yu", ",", "xl", ",", "xu", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.cutmix_bbox_and_lam": [[77, 88], ["mixup.rand_bbox_minmax", "mixup.rand_bbox", "float"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.rand_bbox_minmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.rand_bbox"], ["", "def", "cutmix_bbox_and_lam", "(", "img_shape", ",", "lam", ",", "ratio_minmax", "=", "None", ",", "correct_lam", "=", "True", ",", "count", "=", "None", ")", ":", "\n", "    ", "\"\"\" Generate bbox and apply lambda correction.\n    \"\"\"", "\n", "if", "ratio_minmax", "is", "not", "None", ":", "\n", "        ", "yl", ",", "yu", ",", "xl", ",", "xu", "=", "rand_bbox_minmax", "(", "img_shape", ",", "ratio_minmax", ",", "count", "=", "count", ")", "\n", "", "else", ":", "\n", "        ", "yl", ",", "yu", ",", "xl", ",", "xu", "=", "rand_bbox", "(", "img_shape", ",", "lam", ",", "count", "=", "count", ")", "\n", "", "if", "correct_lam", "or", "ratio_minmax", "is", "not", "None", ":", "\n", "        ", "bbox_area", "=", "(", "yu", "-", "yl", ")", "*", "(", "xu", "-", "xl", ")", "\n", "lam", "=", "1.", "-", "bbox_area", "/", "float", "(", "img_shape", "[", "-", "2", "]", "*", "img_shape", "[", "-", "1", "]", ")", "\n", "", "return", "(", "yl", ",", "yu", ",", "xl", ",", "xu", ")", ",", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.TfPreprocessTransform.__init__": [[206, 213], ["tf_preprocessing.TfPreprocessTransform._build_tf_graph", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.TfPreprocessTransform._build_tf_graph"], ["    ", "def", "__init__", "(", "self", ",", "is_training", "=", "False", ",", "size", "=", "224", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "        ", "self", ".", "is_training", "=", "is_training", "\n", "self", ".", "size", "=", "size", "[", "0", "]", "if", "isinstance", "(", "size", ",", "tuple", ")", "else", "size", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "_image_bytes", "=", "None", "\n", "self", ".", "process_image", "=", "self", ".", "_build_tf_graph", "(", ")", "\n", "self", ".", "sess", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.TfPreprocessTransform._build_tf_graph": [[214, 223], ["tensorflow.device", "tensorflow.placeholder", "tf_preprocessing.preprocess_image"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_image"], ["", "def", "_build_tf_graph", "(", "self", ")", ":", "\n", "        ", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "            ", "self", ".", "_image_bytes", "=", "tf", ".", "placeholder", "(", "\n", "shape", "=", "[", "]", ",", "\n", "dtype", "=", "tf", ".", "string", ",", "\n", ")", "\n", "img", "=", "preprocess_image", "(", "\n", "self", ".", "_image_bytes", ",", "self", ".", "is_training", ",", "False", ",", "self", ".", "size", ",", "self", ".", "interpolation", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.TfPreprocessTransform.__call__": [[224, 233], ["tf_preprocessing.TfPreprocessTransform.sess.run", "numpy.expand_dims.round().clip().astype", "numpy.rollaxis", "tensorflow.Session", "numpy.expand_dims", "numpy.expand_dims.round().clip", "numpy.expand_dims.round"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run"], ["", "def", "__call__", "(", "self", ",", "image_bytes", ")", ":", "\n", "        ", "if", "self", ".", "sess", "is", "None", ":", "\n", "            ", "self", ".", "sess", "=", "tf", ".", "Session", "(", ")", "\n", "", "img", "=", "self", ".", "sess", ".", "run", "(", "self", ".", "process_image", ",", "feed_dict", "=", "{", "self", ".", "_image_bytes", ":", "image_bytes", "}", ")", "\n", "img", "=", "img", ".", "round", "(", ")", ".", "clip", "(", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "if", "img", ".", "ndim", "<", "3", ":", "\n", "            ", "img", "=", "np", ".", "expand_dims", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "", "img", "=", "np", ".", "rollaxis", "(", "img", ",", "2", ")", "# HWC to CHW", "\n", "return", "img", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.distorted_bounding_box_crop": [[32, 82], ["tensorflow.name_scope", "tensorflow.image.extract_jpeg_shape", "tensorflow.image.sample_distorted_bounding_box", "tensorflow.unstack", "tensorflow.unstack", "tensorflow.stack", "tensorflow.image.decode_and_crop_jpeg"], "function", ["None"], ["def", "distorted_bounding_box_crop", "(", "image_bytes", ",", "\n", "bbox", ",", "\n", "min_object_covered", "=", "0.1", ",", "\n", "aspect_ratio_range", "=", "(", "0.75", ",", "1.33", ")", ",", "\n", "area_range", "=", "(", "0.05", ",", "1.0", ")", ",", "\n", "max_attempts", "=", "100", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "scope", ",", "'distorted_bounding_box_crop'", ",", "[", "image_bytes", ",", "bbox", "]", ")", ":", "\n", "        ", "shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "sample_distorted_bounding_box", "=", "tf", ".", "image", ".", "sample_distorted_bounding_box", "(", "\n", "shape", ",", "\n", "bounding_boxes", "=", "bbox", ",", "\n", "min_object_covered", "=", "min_object_covered", ",", "\n", "aspect_ratio_range", "=", "aspect_ratio_range", ",", "\n", "area_range", "=", "area_range", ",", "\n", "max_attempts", "=", "max_attempts", ",", "\n", "use_image_if_no_bounding_boxes", "=", "True", ")", "\n", "bbox_begin", ",", "bbox_size", ",", "_", "=", "sample_distorted_bounding_box", "\n", "\n", "# Crop the image to the specified bounding box.", "\n", "offset_y", ",", "offset_x", ",", "_", "=", "tf", ".", "unstack", "(", "bbox_begin", ")", "\n", "target_height", ",", "target_width", ",", "_", "=", "tf", ".", "unstack", "(", "bbox_size", ")", "\n", "crop_window", "=", "tf", ".", "stack", "(", "[", "offset_y", ",", "offset_x", ",", "target_height", ",", "target_width", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "decode_and_crop_jpeg", "(", "image_bytes", ",", "crop_window", ",", "channels", "=", "3", ")", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._at_least_x_are_equal": [[84, 89], ["tensorflow.equal", "tensorflow.cast", "tensorflow.greater_equal", "tensorflow.reduce_sum"], "function", ["None"], ["", "", "def", "_at_least_x_are_equal", "(", "a", ",", "b", ",", "x", ")", ":", "\n", "    ", "\"\"\"At least `x` of `a` and `b` `Tensors` are equal.\"\"\"", "\n", "match", "=", "tf", ".", "equal", "(", "a", ",", "b", ")", "\n", "match", "=", "tf", ".", "cast", "(", "match", ",", "tf", ".", "int32", ")", "\n", "return", "tf", ".", "greater_equal", "(", "tf", ".", "reduce_sum", "(", "match", ")", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._decode_and_random_crop": [[91, 111], ["tensorflow.constant", "tf_preprocessing.distorted_bounding_box_crop", "tensorflow.image.extract_jpeg_shape", "tf_preprocessing._at_least_x_are_equal", "tensorflow.cond", "tensorflow.shape", "tf_preprocessing._decode_and_center_crop", "tensorflow.image.resize"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.distorted_bounding_box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._at_least_x_are_equal", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._decode_and_center_crop"], ["", "def", "_decode_and_random_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", ":", "\n", "    ", "\"\"\"Make a random crop of image_size.\"\"\"", "\n", "bbox", "=", "tf", ".", "constant", "(", "[", "0.0", ",", "0.0", ",", "1.0", ",", "1.0", "]", ",", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "1", ",", "1", ",", "4", "]", ")", "\n", "image", "=", "distorted_bounding_box_crop", "(", "\n", "image_bytes", ",", "\n", "bbox", ",", "\n", "min_object_covered", "=", "0.1", ",", "\n", "aspect_ratio_range", "=", "(", "3.", "/", "4", ",", "4.", "/", "3.", ")", ",", "\n", "area_range", "=", "(", "0.08", ",", "1.0", ")", ",", "\n", "max_attempts", "=", "10", ",", "\n", "scope", "=", "None", ")", "\n", "original_shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "bad", "=", "_at_least_x_are_equal", "(", "original_shape", ",", "tf", ".", "shape", "(", "image", ")", ",", "3", ")", "\n", "\n", "image", "=", "tf", ".", "cond", "(", "\n", "bad", ",", "\n", "lambda", ":", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ")", ",", "\n", "lambda", ":", "tf", ".", "image", ".", "resize", "(", "[", "image", "]", ",", "[", "image_size", ",", "image_size", "]", ",", "resize_method", ")", "[", "0", "]", ")", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._decode_and_center_crop": [[113, 132], ["tensorflow.image.extract_jpeg_shape", "tensorflow.cast", "tensorflow.stack", "tensorflow.image.decode_and_crop_jpeg", "tensorflow.image.resize", "tensorflow.cast", "tensorflow.minimum"], "function", ["None"], ["", "def", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", ":", "\n", "    ", "\"\"\"Crops to center of image with padding then scales image_size.\"\"\"", "\n", "shape", "=", "tf", ".", "image", ".", "extract_jpeg_shape", "(", "image_bytes", ")", "\n", "image_height", "=", "shape", "[", "0", "]", "\n", "image_width", "=", "shape", "[", "1", "]", "\n", "\n", "padded_center_crop_size", "=", "tf", ".", "cast", "(", "\n", "(", "(", "image_size", "/", "(", "image_size", "+", "CROP_PADDING", ")", ")", "*", "\n", "tf", ".", "cast", "(", "tf", ".", "minimum", "(", "image_height", ",", "image_width", ")", ",", "tf", ".", "float32", ")", ")", ",", "\n", "tf", ".", "int32", ")", "\n", "\n", "offset_height", "=", "(", "(", "image_height", "-", "padded_center_crop_size", ")", "+", "1", ")", "//", "2", "\n", "offset_width", "=", "(", "(", "image_width", "-", "padded_center_crop_size", ")", "+", "1", ")", "//", "2", "\n", "crop_window", "=", "tf", ".", "stack", "(", "[", "offset_height", ",", "offset_width", ",", "\n", "padded_center_crop_size", ",", "padded_center_crop_size", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "decode_and_crop_jpeg", "(", "image_bytes", ",", "crop_window", ",", "channels", "=", "3", ")", "\n", "image", "=", "tf", ".", "image", ".", "resize", "(", "[", "image", "]", ",", "[", "image_size", ",", "image_size", "]", ",", "resize_method", ")", "[", "0", "]", "\n", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._flip": [[134, 138], ["tensorflow.image.random_flip_left_right"], "function", ["None"], ["", "def", "_flip", "(", "image", ")", ":", "\n", "    ", "\"\"\"Random horizontal image flip.\"\"\"", "\n", "image", "=", "tf", ".", "image", ".", "random_flip_left_right", "(", "image", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_for_train": [[140, 159], ["tf_preprocessing._decode_and_random_crop", "tf_preprocessing._flip", "tensorflow.reshape", "tensorflow.image.convert_image_dtype"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._decode_and_random_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._flip"], ["", "def", "preprocess_for_train", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", "=", "IMAGE_SIZE", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"", "\n", "resize_method", "=", "tf", ".", "image", ".", "ResizeMethod", ".", "BICUBIC", "if", "interpolation", "==", "'bicubic'", "else", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", "\n", "image", "=", "_decode_and_random_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", "\n", "image", "=", "_flip", "(", "image", ")", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "image_size", ",", "image_size", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "\n", "image", ",", "dtype", "=", "tf", ".", "bfloat16", "if", "use_bfloat16", "else", "tf", ".", "float32", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_for_eval": [[161, 179], ["tf_preprocessing._decode_and_center_crop", "tensorflow.reshape", "tensorflow.image.convert_image_dtype"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing._decode_and_center_crop"], ["", "def", "preprocess_for_eval", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", "=", "IMAGE_SIZE", ",", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"", "\n", "resize_method", "=", "tf", ".", "image", ".", "ResizeMethod", ".", "BICUBIC", "if", "interpolation", "==", "'bicubic'", "else", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", "\n", "image", "=", "_decode_and_center_crop", "(", "image_bytes", ",", "image_size", ",", "resize_method", ")", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "image_size", ",", "image_size", ",", "3", "]", ")", "\n", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "\n", "image", ",", "dtype", "=", "tf", ".", "bfloat16", "if", "use_bfloat16", "else", "tf", ".", "float32", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_image": [[181, 202], ["tf_preprocessing.preprocess_for_train", "tf_preprocessing.preprocess_for_eval"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_for_train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.tf_preprocessing.preprocess_for_eval"], ["", "def", "preprocess_image", "(", "image_bytes", ",", "\n", "is_training", "=", "False", ",", "\n", "use_bfloat16", "=", "False", ",", "\n", "image_size", "=", "IMAGE_SIZE", ",", "\n", "interpolation", "=", "'bicubic'", ")", ":", "\n", "    ", "\"\"\"Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    \"\"\"", "\n", "if", "is_training", ":", "\n", "        ", "return", "preprocess_for_train", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", ",", "interpolation", ")", "\n", "", "else", ":", "\n", "        ", "return", "preprocess_for_eval", "(", "image_bytes", ",", "use_bfloat16", ",", "image_size", ",", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory._search_split": [[25, 43], ["os.path.join", "os.path.exists", "split.split", "dataset_factory._search_split._try"], "function", ["None"], ["def", "_search_split", "(", "root", ",", "split", ")", ":", "\n", "# look for sub-folder with name of split in root and use that if it exists", "\n", "    ", "split_name", "=", "split", ".", "split", "(", "'['", ")", "[", "0", "]", "\n", "try_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "split_name", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "try_root", ")", ":", "\n", "        ", "return", "try_root", "\n", "\n", "", "def", "_try", "(", "syn", ")", ":", "\n", "        ", "for", "s", "in", "syn", ":", "\n", "            ", "try_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "s", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "try_root", ")", ":", "\n", "                ", "return", "try_root", "\n", "", "", "return", "root", "\n", "", "if", "split_name", "in", "_TRAIN_SYNONYM", ":", "\n", "        ", "root", "=", "_try", "(", "_TRAIN_SYNONYM", ")", "\n", "", "elif", "split_name", "in", "_EVAL_SYNONYM", ":", "\n", "        ", "root", "=", "_try", "(", "_EVAL_SYNONYM", ")", "\n", "", "return", "root", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory.create_dataset": [[45, 135], ["name.lower.lower", "name.lower.startswith", "dict", "name.lower.startswith", "name.lower.split", "ds_class", "dataset.IterableImageDataset", "dataset.ImageDataset", "split.split", "INaturalist", "os.path.isdir", "dataset_factory._search_split", "len", "split_split[].split", "torchvision.datasets.Places365", "len", "torchvision.datasets.ImageNet", "torchvision.datasets.ImageFolder", "os.path.isdir", "dataset_factory._search_split"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory._search_split", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.dataset_factory._search_split"], ["", "def", "create_dataset", "(", "\n", "name", ",", "\n", "root", ",", "\n", "split", "=", "'validation'", ",", "\n", "search_split", "=", "True", ",", "\n", "class_map", "=", "None", ",", "\n", "load_bytes", "=", "False", ",", "\n", "is_training", "=", "False", ",", "\n", "download", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "repeats", "=", "0", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "\"\"\" Dataset factory method\n\n    In parenthesis after each arg are the type of dataset supported for each arg, one of:\n      * folder - default, timm folder (or tar) based ImageDataset\n      * torch - torchvision based datasets\n      * TFDS - Tensorflow-datasets wrapper in IterabeDataset interface via IterableImageDataset\n      * all - any of the above\n\n    Args:\n        name: dataset name, empty is okay for folder based datasets\n        root: root folder of dataset (all)\n        split: dataset split (all)\n        search_split: search for split specific child fold from root so one can specify\n            `imagenet/` instead of `/imagenet/val`, etc on cmd line / config. (folder, torch/folder)\n        class_map: specify class -> index mapping via text file or dict (folder)\n        load_bytes: load data, return images as undecoded bytes (folder)\n        download: download dataset if not present and supported (TFDS, torch)\n        is_training: create dataset in train mode, this is different from the split.\n            For Iterable / TDFS it enables shuffle, ignored for other datasets. (TFDS)\n        batch_size: batch size hint for (TFDS)\n        repeats: dataset repeats per iteration i.e. epoch (TFDS)\n        **kwargs: other args to pass to dataset\n\n    Returns:\n        Dataset object\n    \"\"\"", "\n", "name", "=", "name", ".", "lower", "(", ")", "\n", "if", "name", ".", "startswith", "(", "'torch/'", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ",", "2", ")", "[", "-", "1", "]", "\n", "torch_kwargs", "=", "dict", "(", "root", "=", "root", ",", "download", "=", "download", ",", "**", "kwargs", ")", "\n", "if", "name", "in", "_TORCH_BASIC_DS", ":", "\n", "            ", "ds_class", "=", "_TORCH_BASIC_DS", "[", "name", "]", "\n", "use_train", "=", "split", "in", "_TRAIN_SYNONYM", "\n", "ds", "=", "ds_class", "(", "train", "=", "use_train", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'inaturalist'", "or", "name", "==", "'inat'", ":", "\n", "            ", "assert", "has_inaturalist", ",", "'Please update to PyTorch 1.10, torchvision 0.11+ for Inaturalist'", "\n", "target_type", "=", "'full'", "\n", "split_split", "=", "split", ".", "split", "(", "'/'", ")", "\n", "if", "len", "(", "split_split", ")", ">", "1", ":", "\n", "                ", "target_type", "=", "split_split", "[", "0", "]", ".", "split", "(", "'_'", ")", "\n", "if", "len", "(", "target_type", ")", "==", "1", ":", "\n", "                    ", "target_type", "=", "target_type", "[", "0", "]", "\n", "", "split", "=", "split_split", "[", "-", "1", "]", "\n", "", "if", "split", "in", "_TRAIN_SYNONYM", ":", "\n", "                ", "split", "=", "'2021_train'", "\n", "", "elif", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'2021_valid'", "\n", "", "ds", "=", "INaturalist", "(", "version", "=", "split", ",", "target_type", "=", "target_type", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'places365'", ":", "\n", "            ", "if", "split", "in", "_TRAIN_SYNONYM", ":", "\n", "                ", "split", "=", "'train-standard'", "\n", "", "elif", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'val'", "\n", "", "ds", "=", "Places365", "(", "split", "=", "split", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'imagenet'", ":", "\n", "            ", "if", "split", "in", "_EVAL_SYNONYM", ":", "\n", "                ", "split", "=", "'val'", "\n", "", "ds", "=", "ImageNet", "(", "split", "=", "split", ",", "**", "torch_kwargs", ")", "\n", "", "elif", "name", "==", "'image_folder'", "or", "name", "==", "'folder'", ":", "\n", "# in case torchvision ImageFolder is preferred over timm ImageDataset for some reason", "\n", "            ", "if", "search_split", "and", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "# look for split specific sub-folder in root", "\n", "                ", "root", "=", "_search_split", "(", "root", ",", "split", ")", "\n", "", "ds", "=", "ImageFolder", "(", "root", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "f\"Unknown torchvision dataset {name}\"", "\n", "", "", "elif", "name", ".", "startswith", "(", "'tfds/'", ")", ":", "\n", "        ", "ds", "=", "IterableImageDataset", "(", "\n", "root", ",", "parser", "=", "name", ",", "split", "=", "split", ",", "is_training", "=", "is_training", ",", "\n", "download", "=", "download", ",", "batch_size", "=", "batch_size", ",", "repeats", "=", "repeats", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# FIXME support more advance split cfg for ImageFolder/Tar datasets in the future", "\n", "        ", "if", "search_split", "and", "os", ".", "path", ".", "isdir", "(", "root", ")", ":", "\n", "# look for split specific sub-folder in root", "\n", "            ", "root", "=", "_search_split", "(", "root", ",", "split", ")", "\n", "", "ds", "=", "ImageDataset", "(", "root", ",", "parser", "=", "name", ",", "class_map", "=", "class_map", ",", "load_bytes", "=", "load_bytes", ",", "**", "kwargs", ")", "\n", "", "return", "ds", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.__init__": [[64, 86], ["torch.tensor().cuda().view", "torch.tensor().cuda().view", "loader.PrefetchLoader.mean.half", "loader.PrefetchLoader.std.half", "random_erasing.RandomErasing", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor", "torch.tensor"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "loader", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "fp16", "=", "False", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_num_splits", "=", "0", "\n", ")", ":", "\n", "        ", "self", ".", "loader", "=", "loader", "\n", "self", ".", "mean", "=", "torch", ".", "tensor", "(", "[", "x", "*", "255", "for", "x", "in", "mean", "]", ")", ".", "cuda", "(", ")", ".", "view", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "self", ".", "std", "=", "torch", ".", "tensor", "(", "[", "x", "*", "255", "for", "x", "in", "std", "]", ")", ".", "cuda", "(", ")", ".", "view", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "self", ".", "fp16", "=", "fp16", "\n", "if", "fp16", ":", "\n", "            ", "self", ".", "mean", "=", "self", ".", "mean", ".", "half", "(", ")", "\n", "self", ".", "std", "=", "self", ".", "std", ".", "half", "(", ")", "\n", "", "if", "re_prob", ">", "0.", ":", "\n", "            ", "self", ".", "random_erasing", "=", "RandomErasing", "(", "\n", "probability", "=", "re_prob", ",", "mode", "=", "re_mode", ",", "max_count", "=", "re_count", ",", "num_splits", "=", "re_num_splits", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "random_erasing", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.__iter__": [[87, 115], ["torch.cuda.Stream", "torch.cuda.current_stream().wait_stream", "torch.cuda.stream", "loader.PrefetchLoader.cuda", "next_target.cuda.cuda.cuda", "loader.PrefetchLoader.half().sub_().div_", "loader.PrefetchLoader.float().sub_().div_", "loader.PrefetchLoader.random_erasing", "torch.cuda.current_stream", "loader.PrefetchLoader.half().sub_", "loader.PrefetchLoader.float().sub_", "loader.PrefetchLoader.half", "loader.PrefetchLoader.float"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "stream", "=", "torch", ".", "cuda", ".", "Stream", "(", ")", "\n", "first", "=", "True", "\n", "\n", "for", "next_input", ",", "next_target", "in", "self", ".", "loader", ":", "\n", "            ", "mean", "=", "self", ".", "mean", "\n", "std", "=", "self", ".", "std", "\n", "\n", "with", "torch", ".", "cuda", ".", "stream", "(", "stream", ")", ":", "\n", "                ", "next_input", "=", "next_input", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "next_target", "=", "next_target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "if", "self", ".", "fp16", ":", "\n", "                    ", "next_input", "=", "next_input", ".", "half", "(", ")", ".", "sub_", "(", "mean", ")", ".", "div_", "(", "std", ")", "\n", "", "else", ":", "\n", "                    ", "next_input", "=", "next_input", ".", "float", "(", ")", ".", "sub_", "(", "mean", ")", ".", "div_", "(", "std", ")", "\n", "", "if", "self", ".", "random_erasing", "is", "not", "None", ":", "\n", "                    ", "next_input", "=", "self", ".", "random_erasing", "(", "next_input", ")", "\n", "\n", "", "", "if", "not", "first", ":", "\n", "                ", "yield", "input", ",", "target", "\n", "", "else", ":", "\n", "                ", "first", "=", "False", "\n", "\n", "", "torch", ".", "cuda", ".", "current_stream", "(", ")", ".", "wait_stream", "(", "stream", ")", "\n", "input", "=", "next_input", "\n", "target", "=", "next_target", "\n", "\n", "", "yield", "input", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.__len__": [[116, 118], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "loader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.sampler": [[119, 122], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sampler", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "loader", ".", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.dataset": [[123, 126], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dataset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "loader", ".", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.PrefetchLoader.mixup_enabled": [[134, 138], ["isinstance"], "methods", ["None"], ["", "", "@", "mixup_enabled", ".", "setter", "\n", "def", "mixup_enabled", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "loader", ".", "collate_fn", ",", "FastCollateMixup", ")", ":", "\n", "            ", "self", ".", "loader", ".", "collate_fn", ".", "mixup_enabled", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.MultiEpochsDataLoader.__init__": [[271, 277], ["super().__init__", "loader._RepeatSampler", "super().__iter__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.samplers.distributed_sampler.ClassSpecificDistributedSampler.__iter__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "_DataLoader__initialized", "=", "False", "\n", "self", ".", "batch_sampler", "=", "_RepeatSampler", "(", "self", ".", "batch_sampler", ")", "\n", "self", ".", "_DataLoader__initialized", "=", "True", "\n", "self", ".", "iterator", "=", "super", "(", ")", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.MultiEpochsDataLoader.__len__": [[278, 280], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "batch_sampler", ".", "sampler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.MultiEpochsDataLoader.__iter__": [[281, 284], ["range", "len", "next"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "self", ")", ")", ":", "\n", "            ", "yield", "next", "(", "self", ".", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader._RepeatSampler.__init__": [[293, 295], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "sampler", ")", ":", "\n", "        ", "self", ".", "sampler", "=", "sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader._RepeatSampler.__iter__": [[296, 299], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "yield", "from", "iter", "(", "self", ".", "sampler", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.fast_collate": [[27, 60], ["isinstance", "len", "isinstance", "len", "torch.zeros", "torch.zeros", "range", "isinstance", "range", "torch.tensor", "torch.zeros", "range", "isinstance", "len", "torch.from_numpy", "len", "torch.from_numpy", "torch.tensor", "torch.zeros", "range", "len", "tensor[].copy_"], "function", ["None"], ["", "def", "fast_collate", "(", "batch", ")", ":", "\n", "    ", "\"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"", "\n", "assert", "isinstance", "(", "batch", "[", "0", "]", ",", "tuple", ")", "\n", "batch_size", "=", "len", "(", "batch", ")", "\n", "if", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "tuple", ")", ":", "\n", "# This branch 'deinterleaves' and flattens tuples of input tensors into one tensor ordered by position", "\n", "# such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position", "\n", "        ", "inner_tuple_size", "=", "len", "(", "batch", "[", "0", "]", "[", "0", "]", ")", "\n", "flattened_batch_size", "=", "batch_size", "*", "inner_tuple_size", "\n", "targets", "=", "torch", ".", "zeros", "(", "flattened_batch_size", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "flattened_batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "assert", "len", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "==", "inner_tuple_size", "# all input tensor tuples must be same length", "\n", "for", "j", "in", "range", "(", "inner_tuple_size", ")", ":", "\n", "                ", "targets", "[", "i", "+", "j", "*", "batch_size", "]", "=", "batch", "[", "i", "]", "[", "1", "]", "\n", "tensor", "[", "i", "+", "j", "*", "batch_size", "]", "+=", "torch", ".", "from_numpy", "(", "batch", "[", "i", "]", "[", "0", "]", "[", "j", "]", ")", "\n", "", "", "return", "tensor", ",", "targets", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "targets", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "assert", "len", "(", "targets", ")", "==", "batch_size", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "tensor", "[", "i", "]", "+=", "torch", ".", "from_numpy", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "tensor", ",", "targets", "\n", "", "elif", "isinstance", "(", "batch", "[", "0", "]", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "targets", "=", "torch", ".", "tensor", "(", "[", "b", "[", "1", "]", "for", "b", "in", "batch", "]", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "assert", "len", "(", "targets", ")", "==", "batch_size", "\n", "tensor", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "*", "batch", "[", "0", "]", "[", "0", "]", ".", "shape", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "tensor", "[", "i", "]", ".", "copy_", "(", "batch", "[", "i", "]", "[", "0", "]", ")", "\n", "", "return", "tensor", ",", "targets", "\n", "", "else", ":", "\n", "        ", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader._worker_init": [[140, 154], ["torch.utils.data.get_worker_info", "isinstance", "worker_seeding", "random.seed", "torch.manual_seed", "numpy.random.seed", "numpy.random.seed"], "function", ["None"], ["", "", "", "def", "_worker_init", "(", "worker_id", ",", "worker_seeding", "=", "'all'", ")", ":", "\n", "    ", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "assert", "worker_info", ".", "id", "==", "worker_id", "\n", "if", "isinstance", "(", "worker_seeding", ",", "Callable", ")", ":", "\n", "        ", "seed", "=", "worker_seeding", "(", "worker_info", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", "%", "(", "2", "**", "32", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "worker_seeding", "in", "(", "'all'", ",", "'part'", ")", "\n", "# random / torch seed already called in dataloader iter class w/ worker_info.seed", "\n", "# to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)", "\n", "if", "worker_seeding", "==", "'all'", ":", "\n", "            ", "np", ".", "random", ".", "seed", "(", "worker_info", ".", "seed", "%", "(", "2", "**", "32", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.loader.create_loader": [[156, 267], ["transforms_factory.create_transform", "dict", "loader_class", "loader.PrefetchLoader", "isinstance", "distributed_sampler.OrderedDistributedSampler", "functools.partial", "dict.pop", "loader_class", "distributed_sampler.RepeatAugSampler", "torch.utils.data.distributed.DistributedSampler", "isinstance"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.transforms_factory.create_transform"], ["", "", "", "def", "create_loader", "(", "\n", "dataset", ",", "\n", "input_size", ",", "\n", "batch_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "True", ",", "\n", "no_aug", "=", "False", ",", "\n", "re_prob", "=", "0.", ",", "\n", "re_mode", "=", "'const'", ",", "\n", "re_count", "=", "1", ",", "\n", "re_split", "=", "False", ",", "\n", "scale", "=", "None", ",", "\n", "ratio", "=", "None", ",", "\n", "hflip", "=", "0.5", ",", "\n", "vflip", "=", "0.", ",", "\n", "color_jitter", "=", "0.4", ",", "\n", "auto_augment", "=", "None", ",", "\n", "num_aug_repeats", "=", "0", ",", "\n", "num_aug_splits", "=", "0", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "mean", "=", "IMAGENET_DEFAULT_MEAN", ",", "\n", "std", "=", "IMAGENET_DEFAULT_STD", ",", "\n", "num_workers", "=", "1", ",", "\n", "distributed", "=", "False", ",", "\n", "crop_pct", "=", "None", ",", "\n", "collate_fn", "=", "None", ",", "\n", "pin_memory", "=", "False", ",", "\n", "fp16", "=", "False", ",", "\n", "tf_preprocessing", "=", "False", ",", "\n", "use_multi_epochs_loader", "=", "False", ",", "\n", "persistent_workers", "=", "True", ",", "\n", "worker_seeding", "=", "'all'", ",", "\n", ")", ":", "\n", "    ", "re_num_splits", "=", "0", "\n", "if", "re_split", ":", "\n", "# apply RE to second half of batch if no aug split otherwise line up with aug split", "\n", "        ", "re_num_splits", "=", "num_aug_splits", "or", "2", "\n", "", "dataset", ".", "transform", "=", "create_transform", "(", "\n", "input_size", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_prefetcher", "=", "use_prefetcher", ",", "\n", "no_aug", "=", "no_aug", ",", "\n", "scale", "=", "scale", ",", "\n", "ratio", "=", "ratio", ",", "\n", "hflip", "=", "hflip", ",", "\n", "vflip", "=", "vflip", ",", "\n", "color_jitter", "=", "color_jitter", ",", "\n", "auto_augment", "=", "auto_augment", ",", "\n", "interpolation", "=", "interpolation", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "crop_pct", "=", "crop_pct", ",", "\n", "tf_preprocessing", "=", "tf_preprocessing", ",", "\n", "re_prob", "=", "re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", ",", "\n", "separate", "=", "num_aug_splits", ">", "0", ",", "\n", ")", "\n", "\n", "sampler", "=", "None", "\n", "if", "distributed", "and", "not", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "IterableDataset", ")", ":", "\n", "        ", "if", "is_training", ":", "\n", "            ", "if", "num_aug_repeats", ":", "\n", "                ", "sampler", "=", "RepeatAugSampler", "(", "dataset", ",", "num_repeats", "=", "num_aug_repeats", ")", "\n", "", "else", ":", "\n", "                ", "sampler", "=", "torch", ".", "utils", ".", "data", ".", "distributed", ".", "DistributedSampler", "(", "dataset", ")", "\n", "", "", "else", ":", "\n", "# This will add extra duplicate entries to result in equal num", "\n", "# of samples per-process, will slightly alter validation results", "\n", "            ", "sampler", "=", "OrderedDistributedSampler", "(", "dataset", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "num_aug_repeats", "==", "0", ",", "\"RepeatAugment not currently supported in non-distributed or IterableDataset use\"", "\n", "\n", "", "if", "collate_fn", "is", "None", ":", "\n", "        ", "collate_fn", "=", "fast_collate", "if", "use_prefetcher", "else", "torch", ".", "utils", ".", "data", ".", "dataloader", ".", "default_collate", "\n", "\n", "", "loader_class", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "\n", "if", "use_multi_epochs_loader", ":", "\n", "        ", "loader_class", "=", "MultiEpochsDataLoader", "\n", "\n", "", "loader_args", "=", "dict", "(", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "not", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "IterableDataset", ")", "and", "sampler", "is", "None", "and", "is_training", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "sampler", "=", "sampler", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "pin_memory", "=", "pin_memory", ",", "\n", "drop_last", "=", "is_training", ",", "\n", "worker_init_fn", "=", "partial", "(", "_worker_init", ",", "worker_seeding", "=", "worker_seeding", ")", ",", "\n", "persistent_workers", "=", "persistent_workers", "\n", ")", "\n", "try", ":", "\n", "        ", "loader", "=", "loader_class", "(", "dataset", ",", "**", "loader_args", ")", "\n", "", "except", "TypeError", "as", "e", ":", "\n", "        ", "loader_args", ".", "pop", "(", "'persistent_workers'", ")", "# only in Pytorch 1.7+", "\n", "loader", "=", "loader_class", "(", "dataset", ",", "**", "loader_args", ")", "\n", "", "if", "use_prefetcher", ":", "\n", "        ", "prefetch_re_prob", "=", "re_prob", "if", "is_training", "and", "not", "no_aug", "else", "0.", "\n", "loader", "=", "PrefetchLoader", "(", "\n", "loader", ",", "\n", "mean", "=", "mean", ",", "\n", "std", "=", "std", ",", "\n", "fp16", "=", "fp16", ",", "\n", "re_prob", "=", "prefetch_re_prob", ",", "\n", "re_mode", "=", "re_mode", ",", "\n", "re_count", "=", "re_count", ",", "\n", "re_num_splits", "=", "re_num_splits", ",", "\n", ")", "\n", "\n", "", "return", "loader", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.OrderedDistributedSampler.__init__": [[22, 36], ["int", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "math.ceil", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.OrderedDistributedSampler.__iter__": [[37, 49], ["list", "iter", "range", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.OrderedDistributedSampler.__len__": [[50, 52], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.__init__": [[65, 100], ["int", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "math.ceil", "int", "int", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "math.floor", "math.ceil", "len", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "num_replicas", "=", "None", ",", "\n", "rank", "=", "None", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_repeats", "=", "3", ",", "\n", "selected_round", "=", "256", ",", "\n", "selected_ratio", "=", "0", ",", "\n", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "num_repeats", "=", "num_repeats", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "num_repeats", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "# Determine the number of samples to select per epoch for each rank.", "\n", "# num_selected logic defaults to be the same as original RASampler impl, but this one can be tweaked", "\n", "# via selected_ratio and selected_round args.", "\n", "selected_ratio", "=", "selected_ratio", "or", "num_replicas", "# ratio to reduce selected samples by, num_replicas if 0", "\n", "if", "selected_round", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "floor", "(", "\n", "len", "(", "self", ".", "dataset", ")", "//", "selected_round", "*", "selected_round", "/", "selected_ratio", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "/", "selected_ratio", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.__iter__": [[101, 123], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "iter", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "list", "len", "len", "len", "range", "range", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "len", "len"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "# deterministically shuffle based on epoch", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "torch", ".", "randperm", "(", "len", "(", "self", ".", "dataset", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]", "\n", "", "indices", "=", "[", "x", "for", "x", "in", "indices", "for", "_", "in", "range", "(", "self", ".", "num_repeats", ")", "]", "\n", "# add extra samples to make it evenly divisible", "\n", "padding_size", "=", "self", ".", "total_size", "-", "len", "(", "indices", ")", "\n", "indices", "+=", "indices", "[", ":", "padding_size", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample per rank", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "# return up to num selected samples", "\n", "return", "iter", "(", "indices", "[", ":", "self", ".", "num_selected_samples", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.__len__": [[124, 126], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_selected_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.set_epoch": [[127, 129], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.ParserTxtFile.__init__": [[33, 49], ["parser.Parser.__init__", "isinstance", "parser_txt_list.read_list", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.read_list"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "train_val_list", ",", "\n", "class_map", "=", "''", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "samples", "=", "[", "]", "\n", "if", "isinstance", "(", "train_val_list", ",", "str", ")", ":", "\n", "            ", "self", ".", "samples", "=", "read_list", "(", "train_val_list", ")", "\n", "", "elif", "isinstance", "(", "train_val_list", ",", "list", ")", ":", "\n", "            ", "self", ".", "samples", "=", "train_val_list", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'the input must be the path of your train/val list'", ")", "\n", "#random.shuffle(self.samples)", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.ParserTxtFile.__getitem__": [[52, 55], ["open"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", ",", "target", "=", "self", ".", "samples", "[", "index", "]", "\n", "return", "open", "(", "path", ",", "'rb'", ")", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.ParserTxtFile.__len__": [[56, 58], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.ParserTxtFile._filename": [[59, 66], ["os.path.basename", "os.path.relpath"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "elif", "not", "absolute", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "relpath", "(", "filename", ",", "self", ".", "root", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_txt_list.read_list": [[22, 29], ["open", "line.split", "imgs.append", "int"], "function", ["None"], ["def", "read_list", "(", "path", ")", ":", "\n", "    ", "imgs", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "name", ",", "target", "=", "line", ".", "split", "(", "' '", ")", "\n", "imgs", ".", "append", "(", "(", "name", ",", "int", "(", "target", ")", ")", ")", "\n", "", "", "return", "imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.ParserImageFolder.__init__": [[41, 55], ["parser.Parser.__init__", "parser_image_folder.find_images_and_targets", "class_map.load_class_map", "len", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.find_images_and_targets", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.class_map.load_class_map"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "class_map", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "root", "=", "root", "\n", "class_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "self", ".", "samples", ",", "self", ".", "class_to_idx", "=", "find_images_and_targets", "(", "root", ",", "class_to_idx", "=", "class_to_idx", ")", "\n", "if", "len", "(", "self", ".", "samples", ")", "==", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "f'Found 0 images in subfolders of {root}. Supported image extensions are {\", \".join(IMG_EXTENSIONS)}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.ParserImageFolder.__getitem__": [[56, 59], ["open"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "path", ",", "target", "=", "self", ".", "samples", "[", "index", "]", "\n", "return", "open", "(", "path", ",", "'rb'", ")", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.ParserImageFolder.__len__": [[60, 62], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.ParserImageFolder._filename": [[63, 70], ["os.path.basename", "os.path.relpath"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "elif", "not", "absolute", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "relpath", "(", "filename", ",", "self", ".", "root", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_folder.find_images_and_targets": [[17, 37], ["os.walk", "set", "list", "sorted", "os.path.relpath", "os.path.basename", "rel_path.replace", "os.path.splitext", "sorted", "zip", "ext.lower", "filenames.append", "labels.append", "enumerate", "os.path.join", "timm.utils.misc.natural_key"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.natural_key"], ["def", "find_images_and_targets", "(", "folder", ",", "types", "=", "IMG_EXTENSIONS", ",", "class_to_idx", "=", "None", ",", "leaf_name_only", "=", "True", ",", "sort", "=", "True", ")", ":", "\n", "    ", "labels", "=", "[", "]", "\n", "filenames", "=", "[", "]", "\n", "for", "root", ",", "subdirs", ",", "files", "in", "os", ".", "walk", "(", "folder", ",", "topdown", "=", "False", ",", "followlinks", "=", "True", ")", ":", "\n", "        ", "rel_path", "=", "os", ".", "path", ".", "relpath", "(", "root", ",", "folder", ")", "if", "(", "root", "!=", "folder", ")", "else", "''", "\n", "label", "=", "os", ".", "path", ".", "basename", "(", "rel_path", ")", "if", "leaf_name_only", "else", "rel_path", ".", "replace", "(", "os", ".", "path", ".", "sep", ",", "'_'", ")", "\n", "for", "f", "in", "files", ":", "\n", "            ", "base", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "if", "ext", ".", "lower", "(", ")", "in", "types", ":", "\n", "                ", "filenames", ".", "append", "(", "os", ".", "path", ".", "join", "(", "root", ",", "f", ")", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "", "", "if", "class_to_idx", "is", "None", ":", "\n", "# building class index", "\n", "        ", "unique_labels", "=", "set", "(", "labels", ")", "\n", "sorted_labels", "=", "list", "(", "sorted", "(", "unique_labels", ",", "key", "=", "natural_key", ")", ")", "\n", "class_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "", "images_and_targets", "=", "[", "(", "f", ",", "class_to_idx", "[", "l", "]", ")", "for", "f", ",", "l", "in", "zip", "(", "filenames", ",", "labels", ")", "if", "l", "in", "class_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "images_and_targets", "=", "sorted", "(", "images_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", ")", ")", "\n", "", "return", "images_and_targets", ",", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds.__init__": [[65, 129], ["parser.Parser.__init__", "tfds.builder", "parser_tfds.ParserTfds.builder.download_and_prepare", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.get_rank", "torch.get_rank", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "root", ",", "\n", "name", ",", "\n", "split", "=", "'train'", ",", "\n", "is_training", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "download", "=", "False", ",", "\n", "repeats", "=", "0", ",", "\n", "seed", "=", "42", ",", "\n", "prefetch_size", "=", "None", ",", "\n", "shuffle_size", "=", "None", ",", "\n", "max_threadpool_size", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\" Tensorflow-datasets Wrapper\n\n        Args:\n            root: root data dir (ie your TFDS_DATA_DIR. not dataset specific sub-dir)\n            name: tfds dataset name (eg `imagenet2012`)\n            split: tfds dataset split (can use all TFDS split strings eg `train[:10%]`)\n            is_training: training mode, shuffle enabled, dataset len rounded by batch_size\n            batch_size: batch_size to use to unsure total samples % batch_size == 0 in training across all dis nodes\n            download: download and build TFDS dataset if set, otherwise must use tfds CLI\n            repeats: iterate through (repeat) the dataset this many times per iteration (once if 0 or 1)\n            seed: common seed for shard shuffle across all distributed/worker instances\n            prefetch_size: override default tf.data prefetch buffer size\n            shuffle_size: override default tf.data shuffle buffer size\n            max_threadpool_size: override default threadpool size for tf.data\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "is_training", "=", "is_training", "\n", "if", "self", ".", "is_training", ":", "\n", "            ", "assert", "batch_size", "is", "not", "None", ",", "\"Must specify batch_size in training mode for reasonable behaviour w/ TFDS wrapper\"", "\n", "", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "repeats", "=", "repeats", "\n", "self", ".", "common_seed", "=", "seed", "# a seed that's fixed across all worker / distributed instances", "\n", "self", ".", "prefetch_size", "=", "prefetch_size", "or", "PREFETCH_SIZE", "\n", "self", ".", "shuffle_size", "=", "shuffle_size", "or", "SHUFFLE_SIZE", "\n", "self", ".", "max_threadpool_size", "=", "max_threadpool_size", "or", "MAX_TP_SIZE", "\n", "\n", "# TFDS builder and split information", "\n", "self", ".", "builder", "=", "tfds", ".", "builder", "(", "name", ",", "data_dir", "=", "root", ")", "\n", "# NOTE: the tfds command line app can be used download & prepare datasets if you don't enable download flag", "\n", "if", "download", ":", "\n", "            ", "self", ".", "builder", ".", "download_and_prepare", "(", ")", "\n", "", "self", ".", "split_info", "=", "self", ".", "builder", ".", "info", ".", "splits", "[", "split", "]", "\n", "self", ".", "num_samples", "=", "self", ".", "split_info", ".", "num_examples", "\n", "\n", "# Distributed world state", "\n", "self", ".", "dist_rank", "=", "0", "\n", "self", ".", "dist_num_replicas", "=", "1", "\n", "if", "dist", ".", "is_available", "(", ")", "and", "dist", ".", "is_initialized", "(", ")", "and", "dist", ".", "get_world_size", "(", ")", ">", "1", ":", "\n", "            ", "self", ".", "dist_rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "self", ".", "dist_num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "\n", "# Attributes that are updated in _lazy_init, including the tf.data pipeline itself", "\n", "", "self", ".", "global_num_workers", "=", "1", "\n", "self", ".", "worker_info", "=", "None", "\n", "self", ".", "worker_seed", "=", "0", "# seed unique to each work instance", "\n", "self", ".", "subsplit", "=", "None", "# set when data is distributed across workers using sub-splits", "\n", "self", ".", "ds", "=", "None", "# initialized lazily on each dataloader worker process", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds._lazy_init": [[130, 205], ["torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "tfds.ReadConfig", "parser_tfds.ParserTfds.builder.as_dataset", "tf.data.Options", "max", "ds.shuffle.shuffle.with_options", "ds.shuffle.shuffle.prefetch", "tfds.as_numpy", "tf.distribute.InputContext", "hasattr", "getattr", "getattr", "ds.shuffle.shuffle.repeat", "ds.shuffle.shuffle.shuffle", "min", "tfds.even_splits", "min", "isinstance", "parser_tfds.even_split_indices"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.even_split_indices"], ["", "def", "_lazy_init", "(", "self", ")", ":", "\n", "        ", "\"\"\" Lazily initialize the dataset.\n\n        This is necessary to init the Tensorflow dataset pipeline in the (dataloader) process that\n        will be using the dataset instance. The __init__ method is called on the main process,\n        this will be called in a dataloader worker process.\n\n        NOTE: There will be problems if you try to re-use this dataset across different loader/worker\n        instances once it has been initialized. Do not call any dataset methods that can call _lazy_init\n        before it is passed to dataloader.\n        \"\"\"", "\n", "worker_info", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "\n", "# setup input context to split dataset across distributed processes", "\n", "num_workers", "=", "1", "\n", "global_worker_id", "=", "0", "\n", "if", "worker_info", "is", "not", "None", ":", "\n", "            ", "self", ".", "worker_info", "=", "worker_info", "\n", "self", ".", "worker_seed", "=", "worker_info", ".", "seed", "\n", "num_workers", "=", "worker_info", ".", "num_workers", "\n", "self", ".", "global_num_workers", "=", "self", ".", "dist_num_replicas", "*", "num_workers", "\n", "global_worker_id", "=", "self", ".", "dist_rank", "*", "num_workers", "+", "worker_info", ".", "id", "\n", "\n", "\"\"\" Data sharding\n            InputContext will assign subset of underlying TFRecord files to each 'pipeline' if used.\n            My understanding is that using split, the underling TFRecord files will shuffle (shuffle_files=True)\n            between the splits each iteration, but that understanding could be wrong.\n            \n            I am currently using a mix of InputContext shard assignment and fine-grained sub-splits for distributing\n            the data across workers. For training InputContext is used to assign shards to nodes unless num_shards\n            in dataset < total number of workers. Otherwise sub-split API is used for datasets without enough shards or\n            for validation where we can't drop samples and need to avoid minimize uneven splits to avoid padding.\n            \"\"\"", "\n", "should_subsplit", "=", "self", ".", "global_num_workers", ">", "1", "and", "(", "\n", "self", ".", "split_info", ".", "num_shards", "<", "self", ".", "global_num_workers", "or", "not", "self", ".", "is_training", ")", "\n", "if", "should_subsplit", ":", "\n", "# split the dataset w/o using sharding for more even samples / worker, can result in less optimal", "\n", "# read patterns for distributed training (overlap across shards) so better to use InputContext there", "\n", "                ", "if", "has_buggy_even_splits", ":", "\n", "# my even_split workaround doesn't work on subsplits, upgrade tfds!", "\n", "                    ", "if", "not", "isinstance", "(", "self", ".", "split_info", ",", "tfds", ".", "core", ".", "splits", ".", "SubSplitInfo", ")", ":", "\n", "                        ", "subsplits", "=", "even_split_indices", "(", "self", ".", "split", ",", "self", ".", "global_num_workers", ",", "self", ".", "num_samples", ")", "\n", "self", ".", "subsplit", "=", "subsplits", "[", "global_worker_id", "]", "\n", "", "", "else", ":", "\n", "                    ", "subsplits", "=", "tfds", ".", "even_splits", "(", "self", ".", "split", ",", "self", ".", "global_num_workers", ")", "\n", "self", ".", "subsplit", "=", "subsplits", "[", "global_worker_id", "]", "\n", "\n", "", "", "", "input_context", "=", "None", "\n", "if", "self", ".", "global_num_workers", ">", "1", "and", "self", ".", "subsplit", "is", "None", ":", "\n", "# set input context to divide shards among distributed replicas", "\n", "            ", "input_context", "=", "tf", ".", "distribute", ".", "InputContext", "(", "\n", "num_input_pipelines", "=", "self", ".", "global_num_workers", ",", "\n", "input_pipeline_id", "=", "global_worker_id", ",", "\n", "num_replicas_in_sync", "=", "self", ".", "dist_num_replicas", "# FIXME does this arg have any impact?", "\n", ")", "\n", "", "read_config", "=", "tfds", ".", "ReadConfig", "(", "\n", "shuffle_seed", "=", "self", ".", "common_seed", ",", "\n", "shuffle_reshuffle_each_iteration", "=", "True", ",", "\n", "input_context", "=", "input_context", ")", "\n", "ds", "=", "self", ".", "builder", ".", "as_dataset", "(", "\n", "split", "=", "self", ".", "subsplit", "or", "self", ".", "split", ",", "shuffle_files", "=", "self", ".", "is_training", ",", "read_config", "=", "read_config", ")", "\n", "# avoid overloading threading w/ combo of TF ds threads + PyTorch workers", "\n", "options", "=", "tf", ".", "data", ".", "Options", "(", ")", "\n", "thread_member", "=", "'threading'", "if", "hasattr", "(", "options", ",", "'threading'", ")", "else", "'experimental_threading'", "\n", "getattr", "(", "options", ",", "thread_member", ")", ".", "private_threadpool_size", "=", "max", "(", "1", ",", "self", ".", "max_threadpool_size", "//", "num_workers", ")", "\n", "getattr", "(", "options", ",", "thread_member", ")", ".", "max_intra_op_parallelism", "=", "1", "\n", "ds", "=", "ds", ".", "with_options", "(", "options", ")", "\n", "if", "self", ".", "is_training", "or", "self", ".", "repeats", ">", "1", ":", "\n", "# to prevent excessive drop_last batch behaviour w/ IterableDatasets", "\n", "# see warnings at https://pytorch.org/docs/stable/data.html#multi-process-data-loading", "\n", "            ", "ds", "=", "ds", ".", "repeat", "(", ")", "# allow wrap around and break iteration manually", "\n", "", "if", "self", ".", "is_training", ":", "\n", "            ", "ds", "=", "ds", ".", "shuffle", "(", "min", "(", "self", ".", "num_samples", ",", "self", ".", "shuffle_size", ")", "//", "self", ".", "global_num_workers", ",", "seed", "=", "self", ".", "worker_seed", ")", "\n", "", "ds", "=", "ds", ".", "prefetch", "(", "min", "(", "self", ".", "num_samples", "//", "self", ".", "global_num_workers", ",", "self", ".", "prefetch_size", ")", ")", "\n", "self", ".", "ds", "=", "tfds", ".", "as_numpy", "(", "ds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds.__iter__": [[206, 242], ["math.ceil", "parser_tfds.ParserTfds._lazy_init", "PIL.Image.fromarray", "math.ceil", "max"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds._lazy_init"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "ds", "is", "None", ":", "\n", "            ", "self", ".", "_lazy_init", "(", ")", "\n", "\n", "# Compute a rounded up sample count that is used to:", "\n", "#   1. make batches even cross workers & replicas in distributed validation.", "\n", "#     This adds extra samples and will slightly alter validation results.", "\n", "#   2. determine loop ending condition in training w/ repeat enabled so that only full batch_size", "\n", "#     batches are produced (underlying tfds iter wraps around)", "\n", "", "target_sample_count", "=", "math", ".", "ceil", "(", "max", "(", "1", ",", "self", ".", "repeats", ")", "*", "self", ".", "num_samples", "/", "self", ".", "global_num_workers", ")", "\n", "if", "self", ".", "is_training", ":", "\n", "# round up to nearest batch_size per worker-replica", "\n", "            ", "target_sample_count", "=", "math", ".", "ceil", "(", "target_sample_count", "/", "self", ".", "batch_size", ")", "*", "self", ".", "batch_size", "\n", "\n", "# Iterate until exhausted or sample count hits target when training (ds.repeat enabled)", "\n", "", "sample_count", "=", "0", "\n", "for", "sample", "in", "self", ".", "ds", ":", "\n", "            ", "img", "=", "Image", ".", "fromarray", "(", "sample", "[", "'image'", "]", ",", "mode", "=", "'RGB'", ")", "\n", "yield", "img", ",", "sample", "[", "'label'", "]", "\n", "sample_count", "+=", "1", "\n", "if", "self", ".", "is_training", "and", "sample_count", ">=", "target_sample_count", ":", "\n", "# Need to break out of loop when repeat() is enabled for training w/ oversampling", "\n", "# this results in extra samples per epoch but seems more desirable than dropping", "\n", "# up to N*J batches per epoch (where N = num distributed processes, and J = num worker processes)", "\n", "                ", "break", "\n", "\n", "# Pad across distributed nodes (make counts equal by adding samples)", "\n", "", "", "if", "not", "self", ".", "is_training", "and", "self", ".", "dist_num_replicas", ">", "1", "and", "self", ".", "subsplit", "is", "not", "None", "and", "0", "<", "sample_count", "<", "target_sample_count", ":", "\n", "# Validation batch padding only done for distributed training where results are reduced across nodes.", "\n", "# For single process case, it won't matter if workers return different batch sizes.", "\n", "# If using input_context or % based splits, sample count can vary significantly across workers and this", "\n", "# approach should not be used (hence disabled if self.subsplit isn't set).", "\n", "            ", "while", "sample_count", "<", "target_sample_count", ":", "\n", "                ", "yield", "img", ",", "sample", "[", "'label'", "]", "# yield prev sample again", "\n", "sample_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds.__len__": [[243, 247], ["math.ceil", "max"], "methods", ["None"], ["", "", "", "def", "__len__", "(", "self", ")", ":", "\n", "# this is just an estimate and does not factor in extra samples added to pad batches based on", "\n", "# complete worker & replica info (not available until init in dataloader).", "\n", "        ", "return", "math", ".", "ceil", "(", "max", "(", "1", ",", "self", ".", "repeats", ")", "*", "self", ".", "num_samples", "/", "self", ".", "dist_num_replicas", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds._filename": [[248, 250], ["None"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "assert", "False", ",", "\"Not supported\"", "# no random access to samples", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds.filenames": [[251, 269], ["parser_tfds.ParserTfds._lazy_init", "names.append", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.ParserTfds._lazy_init"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "\"\"\" Return all filenames in dataset, overrides base\"\"\"", "\n", "if", "self", ".", "ds", "is", "None", ":", "\n", "            ", "self", ".", "_lazy_init", "(", ")", "\n", "", "names", "=", "[", "]", "\n", "for", "sample", "in", "self", ".", "ds", ":", "\n", "            ", "if", "len", "(", "names", ")", ">", "self", ".", "num_samples", ":", "\n", "                ", "break", "# safety for ds.repeat() case", "\n", "", "if", "'file_name'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'file_name'", "]", "\n", "", "elif", "'filename'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'filename'", "]", "\n", "", "elif", "'id'", "in", "sample", ":", "\n", "                ", "name", "=", "sample", "[", "'id'", "]", "\n", "", "else", ":", "\n", "                ", "assert", "False", ",", "\"No supported name field present\"", "\n", "", "names", ".", "append", "(", "name", ")", "\n", "", "return", "names", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_tfds.even_split_indices": [[37, 40], ["round", "range", "range"], "function", ["None"], ["def", "even_split_indices", "(", "split", ",", "n", ",", "num_samples", ")", ":", "\n", "    ", "partitions", "=", "[", "round", "(", "i", "*", "num_samples", "/", "n", ")", "for", "i", "in", "range", "(", "n", "+", "1", ")", "]", "\n", "return", "[", "f\"{split}[{partitions[i]}:{partitions[i+1]}]\"", "for", "i", "in", "range", "(", "n", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.TarState.__init__": [[33, 37], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tf", ":", "tarfile", ".", "TarFile", "=", "None", ",", "ti", ":", "tarfile", ".", "TarInfo", "=", "None", ")", ":", "\n", "        ", "self", ".", "tf", ":", "tarfile", ".", "TarFile", "=", "tf", "\n", "self", ".", "ti", ":", "tarfile", ".", "TarInfo", "=", "ti", "\n", "self", ".", "children", ":", "Dict", "[", "str", ",", "TarState", "]", "=", "{", "}", "# child states (tars within tars)", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.TarState.reset": [[38, 40], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "tf", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.ParserImageInTar.__init__": [[169, 189], ["parser.Parser.__init__", "parser_image_in_tar.extract_tarinfos", "class_map.load_class_map", "dict", "parser_image_in_tar.ParserImageInTar.class_name_to_idx.items", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.extract_tarinfos", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.class_map.load_class_map"], ["def", "__init__", "(", "self", ",", "root", ",", "class_map", "=", "''", ",", "cache_tarfiles", "=", "True", ",", "cache_tarinfo", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "class_name_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_name_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "self", ".", "root", "=", "root", "\n", "self", ".", "samples", ",", "self", ".", "targets", ",", "self", ".", "class_name_to_idx", ",", "tarfiles", "=", "extract_tarinfos", "(", "\n", "self", ".", "root", ",", "\n", "class_name_to_idx", "=", "class_name_to_idx", ",", "\n", "cache_tarinfo", "=", "cache_tarinfo", ",", "\n", "extensions", "=", "IMG_EXTENSIONS", ")", "\n", "self", ".", "class_idx_to_name", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "class_name_to_idx", ".", "items", "(", ")", "}", "\n", "if", "len", "(", "tarfiles", ")", "==", "1", "and", "tarfiles", "[", "0", "]", "[", "0", "]", "is", "None", ":", "\n", "            ", "self", ".", "root_is_tar", "=", "True", "\n", "self", ".", "tar_state", "=", "tarfiles", "[", "0", "]", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "root_is_tar", "=", "False", "\n", "self", ".", "tar_state", "=", "dict", "(", "tarfiles", ")", "\n", "", "self", ".", "cache_tarfiles", "=", "cache_tarfiles", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.ParserImageInTar.__len__": [[190, 192], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.ParserImageInTar.__getitem__": [[193, 217], ["os.path.join", "tarfile.open", "tarfile.open.extractfile", "tarfile.open", "tarfile.open.extractfile"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "sample", "=", "self", ".", "samples", "[", "index", "]", "\n", "target", "=", "self", ".", "targets", "[", "index", "]", "\n", "sample_ti", ",", "parent_fn", ",", "child_ti", "=", "sample", "\n", "parent_abs", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root", ",", "parent_fn", ")", "if", "parent_fn", "else", "self", ".", "root", "\n", "\n", "tf", "=", "None", "\n", "cache_state", "=", "None", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "            ", "cache_state", "=", "self", ".", "tar_state", "if", "self", ".", "root_is_tar", "else", "self", ".", "tar_state", "[", "parent_fn", "]", "\n", "tf", "=", "cache_state", ".", "tf", "\n", "", "if", "tf", "is", "None", ":", "\n", "            ", "tf", "=", "tarfile", ".", "open", "(", "parent_abs", ")", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "                ", "cache_state", ".", "tf", "=", "tf", "\n", "", "", "if", "child_ti", "is", "not", "None", ":", "\n", "            ", "ctf", "=", "cache_state", ".", "children", "[", "child_ti", ".", "name", "]", ".", "tf", "if", "self", ".", "cache_tarfiles", "else", "None", "\n", "if", "ctf", "is", "None", ":", "\n", "                ", "ctf", "=", "tarfile", ".", "open", "(", "fileobj", "=", "tf", ".", "extractfile", "(", "child_ti", ")", ")", "\n", "if", "self", ".", "cache_tarfiles", ":", "\n", "                    ", "cache_state", ".", "children", "[", "child_ti", ".", "name", "]", ".", "tf", "=", "ctf", "\n", "", "", "tf", "=", "ctf", "\n", "\n", "", "return", "tf", ".", "extractfile", "(", "sample_ti", ")", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.ParserImageInTar._filename": [[218, 223], ["os.path.basename"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", ".", "name", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar._extract_tarinfo": [[42, 61], ["enumerate", "os.path.split", "os.path.splitext", "ext.lower.lower", "ti.isfile", "tarfile.open", "dict", "parser_image_in_tar._extract_tarinfo", "_logger.debug", "parent_info[].append", "parent_info[].append", "tf.extractfile", "os.path.join", "len"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar._extract_tarinfo"], ["", "", "def", "_extract_tarinfo", "(", "tf", ":", "tarfile", ".", "TarFile", ",", "parent_info", ":", "Dict", ",", "extensions", "=", "IMG_EXTENSIONS", ")", ":", "\n", "    ", "sample_count", "=", "0", "\n", "for", "i", ",", "ti", "in", "enumerate", "(", "tf", ")", ":", "\n", "        ", "if", "not", "ti", ".", "isfile", "(", ")", ":", "\n", "            ", "continue", "\n", "", "dirname", ",", "basename", "=", "os", ".", "path", ".", "split", "(", "ti", ".", "path", ")", "\n", "name", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "basename", ")", "\n", "ext", "=", "ext", ".", "lower", "(", ")", "\n", "if", "ext", "==", "'.tar'", ":", "\n", "            ", "with", "tarfile", ".", "open", "(", "fileobj", "=", "tf", ".", "extractfile", "(", "ti", ")", ",", "mode", "=", "'r|'", ")", "as", "ctf", ":", "\n", "                ", "child_info", "=", "dict", "(", "\n", "name", "=", "ti", ".", "name", ",", "path", "=", "os", ".", "path", ".", "join", "(", "parent_info", "[", "'path'", "]", ",", "name", ")", ",", "ti", "=", "ti", ",", "children", "=", "[", "]", ",", "samples", "=", "[", "]", ")", "\n", "sample_count", "+=", "_extract_tarinfo", "(", "ctf", ",", "child_info", ",", "extensions", "=", "extensions", ")", "\n", "_logger", ".", "debug", "(", "f'{i}/?. Extracted child tarinfos from {ti.name}. {len(child_info[\"samples\"])} images.'", ")", "\n", "parent_info", "[", "'children'", "]", ".", "append", "(", "child_info", ")", "\n", "", "", "elif", "ext", "in", "extensions", ":", "\n", "            ", "parent_info", "[", "'samples'", "]", ".", "append", "(", "ti", ")", "\n", "sample_count", "+=", "1", "\n", "", "", "return", "sample_count", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_in_tar.extract_tarinfos": [[63, 163], ["os.path.isfile", "len", "sum", "_logger.info", "dict", "os.path.exists", "_logger.info", "_logger.info", "zip", "numpy.array", "numpy.array", "_logger.info", "os.path.split", "glob.glob", "os.path.join", "_logger.info", "enumerate", "os.path.join().strip", "parser_image_in_tar.TarState", "parser_image_in_tar.extract_tarinfos._add_samples"], "function", ["None"], ["", "def", "extract_tarinfos", "(", "root", ",", "class_name_to_idx", "=", "None", ",", "cache_tarinfo", "=", "None", ",", "extensions", "=", "IMG_EXTENSIONS", ",", "sort", "=", "True", ")", ":", "\n", "    ", "root_is_tar", "=", "False", "\n", "if", "os", ".", "path", ".", "isfile", "(", "root", ")", ":", "\n", "        ", "assert", "os", ".", "path", ".", "splitext", "(", "root", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "==", "'.tar'", "\n", "tar_filenames", "=", "[", "root", "]", "\n", "root", ",", "root_name", "=", "os", ".", "path", ".", "split", "(", "root", ")", "\n", "root_name", "=", "os", ".", "path", ".", "splitext", "(", "root_name", ")", "[", "0", "]", "\n", "root_is_tar", "=", "True", "\n", "", "else", ":", "\n", "        ", "root_name", "=", "root", ".", "strip", "(", "os", ".", "path", ".", "sep", ")", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "[", "-", "1", "]", "\n", "tar_filenames", "=", "glob", "(", "os", ".", "path", ".", "join", "(", "root", ",", "'*.tar'", ")", ",", "recursive", "=", "True", ")", "\n", "", "num_tars", "=", "len", "(", "tar_filenames", ")", "\n", "tar_bytes", "=", "sum", "(", "[", "os", ".", "path", ".", "getsize", "(", "f", ")", "for", "f", "in", "tar_filenames", "]", ")", "\n", "assert", "num_tars", ",", "f'No .tar files found at specified path ({root}).'", "\n", "\n", "_logger", ".", "info", "(", "f'Scanning {tar_bytes/1024**2:.2f}MB of tar files...'", ")", "\n", "info", "=", "dict", "(", "tartrees", "=", "[", "]", ")", "\n", "cache_path", "=", "''", "\n", "if", "cache_tarinfo", "is", "None", ":", "\n", "        ", "cache_tarinfo", "=", "True", "if", "tar_bytes", ">", "10", "*", "1024", "**", "3", "else", "False", "# FIXME magic number, 10GB", "\n", "", "if", "cache_tarinfo", ":", "\n", "        ", "cache_filename", "=", "'_'", "+", "root_name", "+", "CACHE_FILENAME_SUFFIX", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cache_filename", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "_logger", ".", "info", "(", "f'Reading tar info from cache file {cache_path}.'", ")", "\n", "with", "open", "(", "cache_path", ",", "'rb'", ")", "as", "pf", ":", "\n", "            ", "info", "=", "pickle", ".", "load", "(", "pf", ")", "\n", "", "assert", "len", "(", "info", "[", "'tartrees'", "]", ")", "==", "num_tars", ",", "\"Cached tartree len doesn't match number of tarfiles\"", "\n", "", "else", ":", "\n", "        ", "for", "i", ",", "fn", "in", "enumerate", "(", "tar_filenames", ")", ":", "\n", "            ", "path", "=", "''", "if", "root_is_tar", "else", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "fn", ")", ")", "[", "0", "]", "\n", "with", "tarfile", ".", "open", "(", "fn", ",", "mode", "=", "'r|'", ")", "as", "tf", ":", "# tarinfo scans done in streaming mode", "\n", "                ", "parent_info", "=", "dict", "(", "name", "=", "os", ".", "path", ".", "relpath", "(", "fn", ",", "root", ")", ",", "path", "=", "path", ",", "ti", "=", "None", ",", "children", "=", "[", "]", ",", "samples", "=", "[", "]", ")", "\n", "num_samples", "=", "_extract_tarinfo", "(", "tf", ",", "parent_info", ",", "extensions", "=", "extensions", ")", "\n", "num_children", "=", "len", "(", "parent_info", "[", "\"children\"", "]", ")", "\n", "_logger", ".", "debug", "(", "\n", "f'{i}/{num_tars}. Extracted tarinfos from {fn}. {num_children} children, {num_samples} samples.'", ")", "\n", "", "info", "[", "'tartrees'", "]", ".", "append", "(", "parent_info", ")", "\n", "", "if", "cache_path", ":", "\n", "            ", "_logger", ".", "info", "(", "f'Writing tar info to cache file {cache_path}.'", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "pf", ":", "\n", "                ", "pickle", ".", "dump", "(", "info", ",", "pf", ")", "\n", "\n", "", "", "", "samples", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "build_class_map", "=", "False", "\n", "if", "class_name_to_idx", "is", "None", ":", "\n", "        ", "build_class_map", "=", "True", "\n", "\n", "# Flatten tartree info into lists of samples and targets w/ targets based on label id via", "\n", "# class map arg or from unique paths.", "\n", "# NOTE: currently only flattening up to two-levels, filesystem .tars and then one level of sub-tar children", "\n", "# this covers my current use cases and keeps things a little easier to test for now.", "\n", "", "tarfiles", "=", "[", "]", "\n", "\n", "def", "_label_from_paths", "(", "*", "path", ",", "leaf_only", "=", "True", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "*", "path", ")", ".", "strip", "(", "os", ".", "path", ".", "sep", ")", "\n", "return", "path", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "[", "-", "1", "]", "if", "leaf_only", "else", "path", ".", "replace", "(", "os", ".", "path", ".", "sep", ",", "'_'", ")", "\n", "\n", "", "def", "_add_samples", "(", "info", ",", "fn", ")", ":", "\n", "        ", "added", "=", "0", "\n", "for", "s", "in", "info", "[", "'samples'", "]", ":", "\n", "            ", "label", "=", "_label_from_paths", "(", "info", "[", "'path'", "]", ",", "os", ".", "path", ".", "dirname", "(", "s", ".", "path", ")", ")", "\n", "if", "not", "build_class_map", "and", "label", "not", "in", "class_name_to_idx", ":", "\n", "                ", "continue", "\n", "", "samples", ".", "append", "(", "(", "s", ",", "fn", ",", "info", "[", "'ti'", "]", ")", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "added", "+=", "1", "\n", "", "return", "added", "\n", "\n", "", "_logger", ".", "info", "(", "f'Collecting samples and building tar states.'", ")", "\n", "for", "parent_info", "in", "info", "[", "'tartrees'", "]", ":", "\n", "# if tartree has children, we assume all samples are at the child level", "\n", "        ", "tar_name", "=", "None", "if", "root_is_tar", "else", "parent_info", "[", "'name'", "]", "\n", "tar_state", "=", "TarState", "(", ")", "\n", "parent_added", "=", "0", "\n", "for", "child_info", "in", "parent_info", "[", "'children'", "]", ":", "\n", "            ", "child_added", "=", "_add_samples", "(", "child_info", ",", "fn", "=", "tar_name", ")", "\n", "if", "child_added", ":", "\n", "                ", "tar_state", ".", "children", "[", "child_info", "[", "'name'", "]", "]", "=", "TarState", "(", "ti", "=", "child_info", "[", "'ti'", "]", ")", "\n", "", "parent_added", "+=", "child_added", "\n", "", "parent_added", "+=", "_add_samples", "(", "parent_info", ",", "fn", "=", "tar_name", ")", "\n", "if", "parent_added", ":", "\n", "            ", "tarfiles", ".", "append", "(", "(", "tar_name", ",", "tar_state", ")", ")", "\n", "", "", "del", "info", "\n", "\n", "if", "build_class_map", ":", "\n", "# build class index", "\n", "        ", "sorted_labels", "=", "list", "(", "sorted", "(", "set", "(", "labels", ")", ",", "key", "=", "natural_key", ")", ")", "\n", "class_name_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "\n", "", "_logger", ".", "info", "(", "f'Mapping targets and sorting samples.'", ")", "\n", "samples_and_targets", "=", "[", "(", "s", ",", "class_name_to_idx", "[", "l", "]", ")", "for", "s", ",", "l", "in", "zip", "(", "samples", ",", "labels", ")", "if", "l", "in", "class_name_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "samples_and_targets", "=", "sorted", "(", "samples_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", "[", "0", "]", ".", "path", ")", ")", "\n", "", "samples", ",", "targets", "=", "zip", "(", "*", "samples_and_targets", ")", "\n", "samples", "=", "np", ".", "array", "(", "samples", ")", "\n", "targets", "=", "np", ".", "array", "(", "targets", ")", "\n", "_logger", ".", "info", "(", "f'Finished processing {len(samples)} samples across {len(tarfiles)} tar files.'", ")", "\n", "return", "samples", ",", "targets", ",", "class_name_to_idx", ",", "tarfiles", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_factory.create_parser": [[9, 33], ["name.split.lower", "name.split.split", "len", "ParserTfds", "os.path.exists", "os.path.isfile", "parser_image_in_tar.ParserImageInTar", "parser_txt_list.ParserTxtFile", "parser_image_folder.ParserImageFolder", "os.path.splitext"], "function", ["None"], ["def", "create_parser", "(", "name", ",", "root", ",", "split", "=", "'train'", ",", "train_val_list", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "name", "=", "name", ".", "lower", "(", ")", "\n", "name", "=", "name", ".", "split", "(", "'/'", ",", "2", ")", "\n", "prefix", "=", "''", "\n", "if", "len", "(", "name", ")", ">", "1", ":", "\n", "        ", "prefix", "=", "name", "[", "0", "]", "\n", "", "name", "=", "name", "[", "-", "1", "]", "\n", "\n", "# FIXME improve the selection right now just tfds prefix or fallback path, will need options to", "\n", "# explicitly select other options shortly", "\n", "if", "prefix", "==", "'tfds'", ":", "\n", "        ", "from", ".", "parser_tfds", "import", "ParserTfds", "# defer tensorflow import", "\n", "parser", "=", "ParserTfds", "(", "root", ",", "name", ",", "split", "=", "split", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "root", ")", "\n", "# default fallback path (backwards compat), use image tar if root is a .tar file, otherwise image folder", "\n", "# FIXME support split here, in parser?", "\n", "if", "os", ".", "path", ".", "isfile", "(", "root", ")", "and", "os", ".", "path", ".", "splitext", "(", "root", ")", "[", "1", "]", "==", "'.tar'", ":", "\n", "            ", "parser", "=", "ParserImageInTar", "(", "root", ",", "**", "kwargs", ")", "\n", "", "elif", "train_val_list", ":", "\n", "            ", "parser", "=", "ParserTxtFile", "(", "root", ",", "train_val_list", ")", "\n", "", "else", ":", "\n", "            ", "parser", "=", "ParserImageFolder", "(", "root", ",", "**", "kwargs", ")", "\n", "", "", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.__init__": [[5, 7], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser._filename": [[8, 11], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filename": [[12, 14], ["parser.Parser._filename"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar._filename"], ["", "def", "filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "_filename", "(", "index", ",", "basename", "=", "basename", ",", "absolute", "=", "absolute", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser.Parser.filenames": [[15, 17], ["parser.Parser._filename", "range", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar._filename"], ["", "def", "filenames", "(", "self", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "return", "[", "self", ".", "_filename", "(", "index", ",", "basename", "=", "basename", ",", "absolute", "=", "absolute", ")", "for", "index", "in", "range", "(", "len", "(", "self", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar.__init__": [[44, 57], ["parser.Parser.__init__", "os.path.isfile", "class_map.load_class_map", "tarfile.open", "parser_image_tar.extract_tarinfo"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.class_map.load_class_map", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.extract_tarinfo"], ["def", "__init__", "(", "self", ",", "root", ",", "class_map", "=", "''", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "class_to_idx", "=", "None", "\n", "if", "class_map", ":", "\n", "            ", "class_to_idx", "=", "load_class_map", "(", "class_map", ",", "root", ")", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "root", ")", "\n", "self", ".", "root", "=", "root", "\n", "\n", "with", "tarfile", ".", "open", "(", "root", ")", "as", "tf", ":", "# cannot keep this open across processes, reopen later", "\n", "            ", "self", ".", "samples", ",", "self", ".", "class_to_idx", "=", "extract_tarinfo", "(", "tf", ",", "class_to_idx", ")", "\n", "", "self", ".", "imgs", "=", "self", ".", "samples", "\n", "self", ".", "tarfile", "=", "None", "# lazy init in __getitem__", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar.__getitem__": [[58, 64], ["parser_image_tar.ParserImageTar.tarfile.extractfile", "tarfile.open"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "tarfile", "is", "None", ":", "\n", "            ", "self", ".", "tarfile", "=", "tarfile", ".", "open", "(", "self", ".", "root", ")", "\n", "", "tarinfo", ",", "target", "=", "self", ".", "samples", "[", "index", "]", "\n", "fileobj", "=", "self", ".", "tarfile", ".", "extractfile", "(", "tarinfo", ")", "\n", "return", "fileobj", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar.__len__": [[65, 67], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.ParserImageTar._filename": [[68, 73], ["os.path.basename"], "methods", ["None"], ["", "def", "_filename", "(", "self", ",", "index", ",", "basename", "=", "False", ",", "absolute", "=", "False", ")", ":", "\n", "        ", "filename", "=", "self", ".", "samples", "[", "index", "]", "[", "0", "]", ".", "name", "\n", "if", "basename", ":", "\n", "            ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filename", ")", "\n", "", "return", "filename", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.parser_image_tar.extract_tarinfo": [[17, 37], ["tarfile.getmembers", "os.path.split", "os.path.basename", "set", "list", "sorted", "ti.isfile", "os.path.splitext", "ext.lower", "files.append", "labels.append", "sorted", "zip", "enumerate", "timm.utils.misc.natural_key"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.natural_key"], ["def", "extract_tarinfo", "(", "tarfile", ",", "class_to_idx", "=", "None", ",", "sort", "=", "True", ")", ":", "\n", "    ", "files", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "ti", "in", "tarfile", ".", "getmembers", "(", ")", ":", "\n", "        ", "if", "not", "ti", ".", "isfile", "(", ")", ":", "\n", "            ", "continue", "\n", "", "dirname", ",", "basename", "=", "os", ".", "path", ".", "split", "(", "ti", ".", "path", ")", "\n", "label", "=", "os", ".", "path", ".", "basename", "(", "dirname", ")", "\n", "ext", "=", "os", ".", "path", ".", "splitext", "(", "basename", ")", "[", "1", "]", "\n", "if", "ext", ".", "lower", "(", ")", "in", "IMG_EXTENSIONS", ":", "\n", "            ", "files", ".", "append", "(", "ti", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "", "if", "class_to_idx", "is", "None", ":", "\n", "        ", "unique_labels", "=", "set", "(", "labels", ")", "\n", "sorted_labels", "=", "list", "(", "sorted", "(", "unique_labels", ",", "key", "=", "natural_key", ")", ")", "\n", "class_to_idx", "=", "{", "c", ":", "idx", "for", "idx", ",", "c", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "", "tarinfo_and_targets", "=", "[", "(", "f", ",", "class_to_idx", "[", "l", "]", ")", "for", "f", ",", "l", "in", "zip", "(", "files", ",", "labels", ")", "if", "l", "in", "class_to_idx", "]", "\n", "if", "sort", ":", "\n", "        ", "tarinfo_and_targets", "=", "sorted", "(", "tarinfo_and_targets", ",", "key", "=", "lambda", "k", ":", "natural_key", "(", "k", "[", "0", "]", ".", "path", ")", ")", "\n", "", "return", "tarinfo_and_targets", ",", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.parsers.class_map.load_class_map": [[4, 19], ["isinstance", "[].lower", "os.path.exists", "os.path.join", "os.path.exists", "open", "os.path.splitext", "v.strip", "enumerate"], "function", ["None"], ["def", "load_class_map", "(", "map_or_filename", ",", "root", "=", "''", ")", ":", "\n", "    ", "if", "isinstance", "(", "map_or_filename", ",", "dict", ")", ":", "\n", "        ", "assert", "dict", ",", "'class_map dict must be non-empty'", "\n", "return", "map_or_filename", "\n", "", "class_map_path", "=", "map_or_filename", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "class_map_path", ")", ":", "\n", "        ", "class_map_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "class_map_path", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "class_map_path", ")", ",", "'Cannot locate specified class map file (%s)'", "%", "map_or_filename", "\n", "", "class_map_ext", "=", "os", ".", "path", ".", "splitext", "(", "map_or_filename", ")", "[", "-", "1", "]", ".", "lower", "(", ")", "\n", "if", "class_map_ext", "==", "'.txt'", ":", "\n", "        ", "with", "open", "(", "class_map_path", ")", "as", "f", ":", "\n", "            ", "class_to_idx", "=", "{", "v", ".", "strip", "(", ")", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "f", ")", "}", "\n", "", "", "else", ":", "\n", "        ", "assert", "False", ",", "f'Unsupported class map file extension ({class_map_ext}).'", "\n", "", "return", "class_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.jsd.JsdCrossEntropy.__init__": [[17, 25], ["torch.Module.__init__", "cross_entropy.LabelSmoothingCrossEntropy", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_splits", "=", "3", ",", "alpha", "=", "12", ",", "smoothing", "=", "0.1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_splits", "=", "num_splits", "\n", "self", ".", "alpha", "=", "alpha", "\n", "if", "smoothing", "is", "not", "None", "and", "smoothing", ">", "0", ":", "\n", "            ", "self", ".", "cross_entropy_loss", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cross_entropy_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.jsd.JsdCrossEntropy.__call__": [[26, 40], ["torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "jsd.JsdCrossEntropy.cross_entropy_loss", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.clamp().log", "torch.softmax", "torch.softmax", "torch.softmax", "len", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "sum", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.kl_div", "torch.kl_div", "torch.kl_div", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "", "def", "__call__", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "split_size", "=", "output", ".", "shape", "[", "0", "]", "//", "self", ".", "num_splits", "\n", "assert", "split_size", "*", "self", ".", "num_splits", "==", "output", ".", "shape", "[", "0", "]", "\n", "logits_split", "=", "torch", ".", "split", "(", "output", ",", "split_size", ")", "\n", "\n", "# Cross-entropy is only computed on clean images", "\n", "loss", "=", "self", ".", "cross_entropy_loss", "(", "logits_split", "[", "0", "]", ",", "target", "[", ":", "split_size", "]", ")", "\n", "probs", "=", "[", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "for", "logits", "in", "logits_split", "]", "\n", "\n", "# Clamp mixture distribution to avoid exploding KL divergence", "\n", "logp_mixture", "=", "torch", ".", "clamp", "(", "torch", ".", "stack", "(", "probs", ")", ".", "mean", "(", "axis", "=", "0", ")", ",", "1e-7", ",", "1", ")", ".", "log", "(", ")", "\n", "loss", "+=", "self", ".", "alpha", "*", "sum", "(", "[", "F", ".", "kl_div", "(", "\n", "logp_mixture", ",", "p_split", ",", "reduction", "=", "'batchmean'", ")", "for", "p_split", "in", "probs", "]", ")", "/", "len", "(", "probs", ")", "\n", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.cross_entropy.LabelSmoothingCrossEntropy.__init__": [[14, 19], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "smoothing", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "LabelSmoothingCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "smoothing", "<", "1.0", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "confidence", "=", "1.", "-", "smoothing", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.cross_entropy.LabelSmoothingCrossEntropy.forward": [[20, 27], ["torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "nll_loss.squeeze.squeeze.squeeze", "loss.mean", "torch.log_softmax.gather", "torch.log_softmax.mean", "target.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "logprobs", "=", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ")", "\n", "nll_loss", "=", "-", "logprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "target", ".", "unsqueeze", "(", "1", ")", ")", "\n", "nll_loss", "=", "nll_loss", ".", "squeeze", "(", "1", ")", "\n", "smooth_loss", "=", "-", "logprobs", ".", "mean", "(", "dim", "=", "-", "1", ")", "\n", "loss", "=", "self", ".", "confidence", "*", "nll_loss", "+", "self", ".", "smoothing", "*", "smooth_loss", "\n", "return", "loss", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.cross_entropy.SoftTargetCrossEntropy.__init__": [[31, 33], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SoftTargetCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.cross_entropy.SoftTargetCrossEntropy.forward": [[34, 37], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.mean", "torch.sum.mean", "torch.sum.mean", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "loss", "=", "torch", ".", "sum", "(", "-", "target", "*", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "loss", ".", "mean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.asymmetric_loss.AsymmetricLossMultiLabel.__init__": [[6, 14], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gamma_neg", "=", "4", ",", "gamma_pos", "=", "1", ",", "clip", "=", "0.05", ",", "eps", "=", "1e-8", ",", "disable_torch_grad_focal_loss", "=", "False", ")", ":", "\n", "        ", "super", "(", "AsymmetricLossMultiLabel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "gamma_neg", "=", "gamma_neg", "\n", "self", ".", "gamma_pos", "=", "gamma_pos", "\n", "self", ".", "clip", "=", "clip", "\n", "self", ".", "disable_torch_grad_focal_loss", "=", "disable_torch_grad_focal_loss", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.asymmetric_loss.AsymmetricLossMultiLabel.forward": [[15, 51], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "loss.sum", "xs_pos.clamp", "xs_neg.clamp", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled", "torch._C.set_grad_enabled"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (multi-label binarized vector)\n        \"\"\"", "\n", "\n", "# Calculating Probabilities", "\n", "x_sigmoid", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "xs_pos", "=", "x_sigmoid", "\n", "xs_neg", "=", "1", "-", "x_sigmoid", "\n", "\n", "# Asymmetric Clipping", "\n", "if", "self", ".", "clip", "is", "not", "None", "and", "self", ".", "clip", ">", "0", ":", "\n", "            ", "xs_neg", "=", "(", "xs_neg", "+", "self", ".", "clip", ")", ".", "clamp", "(", "max", "=", "1", ")", "\n", "\n", "# Basic CE calculation", "\n", "", "los_pos", "=", "y", "*", "torch", ".", "log", "(", "xs_pos", ".", "clamp", "(", "min", "=", "self", ".", "eps", ")", ")", "\n", "los_neg", "=", "(", "1", "-", "y", ")", "*", "torch", ".", "log", "(", "xs_neg", ".", "clamp", "(", "min", "=", "self", ".", "eps", ")", ")", "\n", "loss", "=", "los_pos", "+", "los_neg", "\n", "\n", "# Asymmetric Focusing", "\n", "if", "self", ".", "gamma_neg", ">", "0", "or", "self", ".", "gamma_pos", ">", "0", ":", "\n", "            ", "if", "self", ".", "disable_torch_grad_focal_loss", ":", "\n", "                ", "torch", ".", "_C", ".", "set_grad_enabled", "(", "False", ")", "\n", "", "pt0", "=", "xs_pos", "*", "y", "\n", "pt1", "=", "xs_neg", "*", "(", "1", "-", "y", ")", "# pt = p if t > 0 else 1-p", "\n", "pt", "=", "pt0", "+", "pt1", "\n", "one_sided_gamma", "=", "self", ".", "gamma_pos", "*", "y", "+", "self", ".", "gamma_neg", "*", "(", "1", "-", "y", ")", "\n", "one_sided_w", "=", "torch", ".", "pow", "(", "1", "-", "pt", ",", "one_sided_gamma", ")", "\n", "if", "self", ".", "disable_torch_grad_focal_loss", ":", "\n", "                ", "torch", ".", "_C", ".", "set_grad_enabled", "(", "True", ")", "\n", "", "loss", "*=", "one_sided_w", "\n", "\n", "", "return", "-", "loss", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.asymmetric_loss.AsymmetricLossSingleLabel.__init__": [[54, 63], ["torch.Module.__init__", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gamma_pos", "=", "1", ",", "gamma_neg", "=", "4", ",", "eps", ":", "float", "=", "0.1", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "super", "(", "AsymmetricLossSingleLabel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "targets_classes", "=", "[", "]", "# prevent gpu repeated memory allocation", "\n", "self", ".", "gamma_pos", "=", "gamma_pos", "\n", "self", ".", "gamma_neg", "=", "gamma_neg", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.asymmetric_loss.AsymmetricLossSingleLabel.forward": [[64, 98], ["asymmetric_loss.AsymmetricLossSingleLabel.logsoftmax", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "loss.mean.mean.sum", "inputs.size", "target.long().unsqueeze", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul_().add_", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul", "loss.mean.mean.mean", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "target.long", "asymmetric_loss.AsymmetricLossSingleLabel.targets_classes.mul_"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "target", ",", "reduction", "=", "None", ")", ":", "\n", "        ", "\"\"\"\"\n        Parameters\n        ----------\n        x: input logits\n        y: targets (1-hot vector)\n        \"\"\"", "\n", "\n", "num_classes", "=", "inputs", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "log_preds", "=", "self", ".", "logsoftmax", "(", "inputs", ")", "\n", "self", ".", "targets_classes", "=", "torch", ".", "zeros_like", "(", "inputs", ")", ".", "scatter_", "(", "1", ",", "target", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "1", ")", "\n", "\n", "# ASL weights", "\n", "targets", "=", "self", ".", "targets_classes", "\n", "anti_targets", "=", "1", "-", "targets", "\n", "xs_pos", "=", "torch", ".", "exp", "(", "log_preds", ")", "\n", "xs_neg", "=", "1", "-", "xs_pos", "\n", "xs_pos", "=", "xs_pos", "*", "targets", "\n", "xs_neg", "=", "xs_neg", "*", "anti_targets", "\n", "asymmetric_w", "=", "torch", ".", "pow", "(", "1", "-", "xs_pos", "-", "xs_neg", ",", "\n", "self", ".", "gamma_pos", "*", "targets", "+", "self", ".", "gamma_neg", "*", "anti_targets", ")", "\n", "log_preds", "=", "log_preds", "*", "asymmetric_w", "\n", "\n", "if", "self", ".", "eps", ">", "0", ":", "# label smoothing", "\n", "            ", "self", ".", "targets_classes", ".", "mul_", "(", "1", "-", "self", ".", "eps", ")", ".", "add_", "(", "self", ".", "eps", "/", "num_classes", ")", "\n", "\n", "# loss calculation", "\n", "", "loss", "=", "-", "self", ".", "targets_classes", ".", "mul", "(", "log_preds", ")", "\n", "\n", "loss", "=", "loss", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "            ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.binary_cross_entropy.BinaryCrossEntropy.__init__": [[16, 26], ["torch.Module.__init__", "binary_cross_entropy.BinaryCrossEntropy.register_buffer", "binary_cross_entropy.BinaryCrossEntropy.register_buffer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "smoothing", "=", "0.1", ",", "target_threshold", ":", "Optional", "[", "float", "]", "=", "None", ",", "weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "reduction", ":", "str", "=", "'mean'", ",", "pos_weight", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", ":", "\n", "        ", "super", "(", "BinaryCrossEntropy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "0.", "<=", "smoothing", "<", "1.0", "\n", "self", ".", "smoothing", "=", "smoothing", "\n", "self", ".", "target_threshold", "=", "target_threshold", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "register_buffer", "(", "'weight'", ",", "weight", ")", "\n", "self", ".", "register_buffer", "(", "'pos_weight'", ",", "pos_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.loss.binary_cross_entropy.BinaryCrossEntropy.forward": [[27, 48], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "target.gt().to.gt().to.long().view", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "torch.full().scatter_", "target.gt().to.gt().to.gt().to", "target.gt().to.gt().to.long", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "torch.full", "target.gt().to.gt().to.gt", "target.gt().to.gt().to.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "assert", "x", ".", "shape", "[", "0", "]", "==", "target", ".", "shape", "[", "0", "]", "\n", "if", "target", ".", "shape", "!=", "x", ".", "shape", ":", "\n", "# NOTE currently assume smoothing or other label softening is applied upstream if targets are already sparse", "\n", "            ", "num_classes", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "# FIXME should off/on be different for smoothing w/ BCE? Other impl out there differ", "\n", "off_value", "=", "self", ".", "smoothing", "/", "num_classes", "\n", "on_value", "=", "1.", "-", "self", ".", "smoothing", "+", "off_value", "\n", "target", "=", "target", ".", "long", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "target", "=", "torch", ".", "full", "(", "\n", "(", "target", ".", "size", "(", ")", "[", "0", "]", ",", "num_classes", ")", ",", "\n", "off_value", ",", "\n", "device", "=", "x", ".", "device", ",", "dtype", "=", "x", ".", "dtype", ")", ".", "scatter_", "(", "1", ",", "target", ",", "on_value", ")", "\n", "", "if", "self", ".", "target_threshold", "is", "not", "None", ":", "\n", "# Make target 0, or 1 if threshold set", "\n", "            ", "target", "=", "target", ".", "gt", "(", "self", ".", "target_threshold", ")", ".", "to", "(", "dtype", "=", "target", ".", "dtype", ")", "\n", "", "return", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "x", ",", "target", ",", "\n", "self", ".", "weight", ",", "\n", "pos_weight", "=", "self", ".", "pos_weight", ",", "\n", "reduction", "=", "self", ".", "reduction", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.train.parse_args": [[33, 88], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "str"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["from", "timm", ".", "data", "import", "create_dataset", ",", "create_loader", ",", "resolve_data_config", ",", "Mixup", ",", "FastCollateMixup", ",", "AugMixDataset", "\n", "from", "timm", ".", "models", "import", "create_model", ",", "safe_model_name", ",", "resume_checkpoint", ",", "load_checkpoint", ",", "convert_splitbn_model", ",", "model_parameters", "\n", "from", "timm", ".", "utils", "import", "*", "\n", "from", "timm", ".", "loss", "import", "*", "\n", "from", "timm", ".", "optim", "import", "create_optimizer_v2", ",", "optimizer_kwargs", "\n", "from", "timm", ".", "scheduler", "import", "create_scheduler", "\n", "from", "timm", ".", "utils", "import", "ApexScaler", ",", "NativeScaler", "\n", "\n", "try", ":", "\n", "    ", "from", "apex", "import", "amp", "\n", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "ApexDDP", "\n", "from", "apex", ".", "parallel", "import", "convert_syncbn_model", "\n", "has_apex", "=", "True", "\n", "", "except", "ImportError", ":", "\n", "    ", "has_apex", "=", "False", "\n", "\n", "", "has_native_amp", "=", "False", "\n", "try", ":", "\n", "    ", "if", "getattr", "(", "torch", ".", "cuda", ".", "amp", ",", "'autocast'", ")", "is", "not", "None", ":", "\n", "        ", "has_native_amp", "=", "True", "\n", "", "", "except", "AttributeError", ":", "\n", "    ", "pass", "\n", "\n", "", "try", ":", "\n", "    ", "import", "wandb", "\n", "has_wandb", "=", "True", "\n", "# os.environ[\"WANDB_API_KEY\"] = YOUR_KEY_HERE", "\n", "", "except", "ImportError", ":", "\n", "    ", "has_wandb", "=", "False", "\n", "\n", "", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "_logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "\n", "# The first arg parser parses out only the --config argument, this argument is used to", "\n", "# load a yaml file containing key-values that override the defaults for the main parser below", "\n", "config_parser", "=", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Training Config'", ",", "add_help", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-c'", ",", "'--config'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'YAML config file specifying default arguments'", ")", "\n", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch ImageNet Training'", ")", "\n", "\n", "# Dataset parameters", "\n", "parser", ".", "add_argument", "(", "'data_dir'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "'-d'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "''", ",", "\n", "help", "=", "'dataset type (default: ImageFolder/ImageTar if empty)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-split'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "'train'", ",", "\n", "help", "=", "'dataset train split (default: train)'", ")", "\n", "parser", ".", "add_argument", "(", "'--val-split'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "'validation'", ",", "\n", "help", "=", "'dataset validation split (default: validation)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset-download'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Allow download of dataset for torch/ and tfds/ datasets that support it.'", ")", "\n", "parser", ".", "add_argument", "(", "'--class-map'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILENAME'", ",", "\n", "help", "=", "'path to class to idx mapping file (default: \"\")'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.train.main": [[90, 209], ["train.parse_args", "mmcv.Config.fromfile", "Config.fromfile.merge_from_dict", "Config.fromfile.get", "Config.fromfile.setdefault", "Config.fromfile.setdefault", "mmcv.mkdir_or_exist", "Config.fromfile.dump", "time.strftime", "os.join", "mmaction.utils.get_root_logger", "dict", "mmaction.utils.collect_env", "mmaction.utils.get_root_logger.info", "mmaction.utils.get_root_logger.info", "mmaction.utils.get_root_logger.info", "mmaction.apis.init_random_seed", "mmaction.utils.get_root_logger.info", "mmcv.runner.set_random_seed", "os.basename", "os.basename", "mmaction.models.build_model", "dict", "mmaction.apis.train_model", "mmcv.runner.init_dist", "mmcv.runner.get_dist_info", "range", "os.abspath", "os.join", "time.localtime", "Config.fromfile.work_dir.rstrip", "len", "mmaction.utils.register_module_hooks", "isinstance", "len", "copy.deepcopy", "datasets.append", "dict", "Config.fromfile.get", "os.join", "range", "range", "os.basename", "Config.fromfile.get", "Config.fromfile.get", "mmaction.datasets.build_dataset", "mmaction.datasets.build_dataset", "warnings.warn", "mmaction.datasets.build_dataset", "mmaction.utils.collect_env.items", "os.splitext", "mmcv.utils.get_git_hash", "os.basename"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.collect_env.collect_env", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.init_random_seed", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.train_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.register_module_hooks", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset"], ["# Model parameters", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "default", "=", "'resnet50'", ",", "type", "=", "str", ",", "metavar", "=", "'MODEL'", ",", "\n", "help", "=", "'Name of model to train (default: \"resnet50\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Start with pretrained version of specified network (if avail)'", ")", "\n", "parser", ".", "add_argument", "(", "'--initial-checkpoint'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'Initialize model from this checkpoint (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'Resume full model and optimizer state from checkpoint (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-resume-opt'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'prevent resume of optimizer state when resuming model'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of label classes (Model default if None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gp'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "metavar", "=", "'POOL'", ",", "\n", "help", "=", "'Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.'", ")", "\n", "parser", ".", "add_argument", "(", "'--img-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Image patch size (default: None => model default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "None", ",", "nargs", "=", "3", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N N N'", ",", "help", "=", "'Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty'", ")", "\n", "parser", ".", "add_argument", "(", "'--crop-pct'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Input image center crop percent (for validation only)'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'MEAN'", ",", "\n", "help", "=", "'Override mean pixel value of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--std'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'STD'", ",", "\n", "help", "=", "'Override std deviation of of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--interpolation'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'Image resize interpolation type (overrides model)'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'input batch size for training (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'-vb'", ",", "'--validation-batch-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'validation batch size override (default: None)'", ")", "\n", "\n", "# Optimizer parameters", "\n", "parser", ".", "add_argument", "(", "'--opt'", ",", "default", "=", "'sgd'", ",", "type", "=", "str", ",", "metavar", "=", "'OPTIMIZER'", ",", "\n", "help", "=", "'Optimizer (default: \"sgd\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-eps'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "metavar", "=", "'EPSILON'", ",", "\n", "help", "=", "'Optimizer Epsilon (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-betas'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "metavar", "=", "'BETA'", ",", "\n", "help", "=", "'Optimizer Betas (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'Optimizer momentum (default: 0.9)'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "2e-5", ",", "\n", "help", "=", "'weight decay (default: 2e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-grad'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'Clip gradient norm (default: None, no clipping)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-mode'", ",", "type", "=", "str", ",", "default", "=", "'norm'", ",", "\n", "help", "=", "'Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")'", ")", "\n", "\n", "\n", "# Learning rate schedule parameters", "\n", "parser", ".", "add_argument", "(", "'--sched'", ",", "default", "=", "'cosine'", ",", "type", "=", "str", ",", "metavar", "=", "'SCHEDULER'", ",", "\n", "help", "=", "'LR scheduler (default: \"step\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'learning rate (default: 0.05)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'pct, pct'", ",", "\n", "help", "=", "'learning rate noise on/off epoch percentages'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-pct'", ",", "type", "=", "float", ",", "default", "=", "0.67", ",", "metavar", "=", "'PERCENT'", ",", "\n", "help", "=", "'learning rate noise limit percent (default: 0.67)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'STDDEV'", ",", "\n", "help", "=", "'learning rate noise std-dev (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-mul'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'MULT'", ",", "\n", "help", "=", "'learning rate cycle len multiplier (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-decay'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "metavar", "=", "'MULT'", ",", "\n", "help", "=", "'amount to decay each learning rate cycle (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-limit'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'learning rate cycle limit, cycles enabled if > 1'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-k-decay'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'learning rate k-decay for cosine/poly (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-lr'", ",", "type", "=", "float", ",", "default", "=", "0.0001", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'warmup learning rate (default: 0.0001)'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'lower lr bound for cyclic schedulers that hit 0 (1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of epochs to train (default: 300)'", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch-repeats'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).'", ")", "\n", "parser", ".", "add_argument", "(", "'--start-epoch'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'manual epoch number (useful on restarts)'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-epochs'", ",", "type", "=", "float", ",", "default", "=", "100", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch interval to decay LR'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epochs'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to warmup LR, if scheduler supports'", ")", "\n", "parser", ".", "add_argument", "(", "'--cooldown-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to cooldown LR at min_lr, after cyclic schedule ends'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'patience epochs for Plateau LR scheduler (default: 10'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "'--dr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'RATE'", ",", "\n", "help", "=", "'LR decay rate (default: 0.1)'", ")", "\n", "\n", "# Augmentation & regularization parameters", "\n", "parser", ".", "add_argument", "(", "'--no-aug'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable all training augmentation, override other train aug args'", ")", "\n", "parser", ".", "add_argument", "(", "'--scale'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "0.08", ",", "1.0", "]", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Random resize scale (default: 0.08 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "3.", "/", "4.", ",", "4.", "/", "3.", "]", ",", "metavar", "=", "'RATIO'", ",", "\n", "help", "=", "'Random resize aspect ratio (default: 0.75 1.33)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hflip'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "'Horizontal flip training aug probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--vflip'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'Vertical flip training aug probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--color-jitter'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Color jitter factor (default: 0.4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aa'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'Use AutoAugment policy. \"v0\" or \"original\". (default: None)'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--aug-repeats'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Number of augmentation repetitions (distributed training only) (default: 0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aug-splits'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Number of augmentation splits (default: 0, valid: 0 or >=2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--jsd-loss'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.'", ")", "\n", "parser", ".", "add_argument", "(", "'--bce-loss'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable BCE loss w/ Mixup/CutMix use.'", ")", "\n", "parser", ".", "add_argument", "(", "'--bce-target-thresh'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "\n", "help", "=", "'Threshold for binarizing softened BCE targets (default: None, disabled)'", ")", "\n", "parser", ".", "add_argument", "(", "'--reprob'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Random erase prob (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--remode'", ",", "type", "=", "str", ",", "default", "=", "'pixel'", ",", "\n", "help", "=", "'Random erase mode (default: \"pixel\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--recount'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random erase count (default: 1)'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.parse_args": [[45, 127], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "str", "ValueError", "warnings.warn"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'MMAction2 test (and eval) a model'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'test config file path'", ")", "\n", "parser", ".", "add_argument", "(", "'checkpoint'", ",", "help", "=", "'checkpoint file'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--out'", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'output result file in pkl/yaml/json format'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--fuse-conv-bn'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to fuse conv and bn, this will slightly increase'", "\n", "'the inference speed'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--eval'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "help", "=", "'evaluation metrics, which depends on the dataset, e.g.,'", "\n", "' \"top_k_accuracy\", \"mean_class_accuracy\" for video dataset'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--gpu-collect'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'whether to use gpu to collect results'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--tmpdir'", ",", "\n", "help", "=", "'tmp directory used for collecting results from multiple '", "\n", "'workers, available when gpu-collect is not specified'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'custom options for evaluation, the key-value pair in xxx=yyy '", "\n", "'format will be kwargs for dataset.evaluate() function (deprecate), '", "\n", "'change to --eval-options instead.'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--eval-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'custom options for evaluation, the key-value pair in xxx=yyy '", "\n", "'format will be kwargs for dataset.evaluate() function'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cfg-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'override some settings in the used config, the key-value pair '", "\n", "'in xxx=yyy format will be merged into config file. For example, '", "\n", "\"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--average-clips'", ",", "\n", "choices", "=", "[", "'score'", ",", "'prob'", ",", "None", "]", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'average type when averaging test clips'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--launcher'", ",", "\n", "choices", "=", "[", "'none'", ",", "'pytorch'", ",", "'slurm'", ",", "'mpi'", "]", ",", "\n", "default", "=", "'none'", ",", "\n", "help", "=", "'job launcher'", ")", "\n", "parser", ".", "add_argument", "(", "'--local_rank'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--onnx'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to test with onnx model or not'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--tensorrt'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to test with TensorRT engine or not'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "'LOCAL_RANK'", "not", "in", "os", ".", "environ", ":", "\n", "        ", "os", ".", "environ", "[", "'LOCAL_RANK'", "]", "=", "str", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "if", "args", ".", "options", "and", "args", ".", "eval_options", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'--options and --eval-options cannot be both '", "\n", "'specified, --options is deprecated in favor of --eval-options'", ")", "\n", "", "if", "args", ".", "options", ":", "\n", "        ", "warnings", ".", "warn", "(", "'--options is deprecated in favor of --eval-options'", ")", "\n", "args", ".", "eval_options", "=", "args", ".", "options", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.turn_off_pretrained": [[129, 139], ["cfg.values", "isinstance", "test.turn_off_pretrained"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.turn_off_pretrained"], ["", "def", "turn_off_pretrained", "(", "cfg", ")", ":", "\n", "# recursively find all pretrained in the model config,", "\n", "# and set them None to avoid redundant pretrain steps for testing", "\n", "    ", "if", "'pretrained'", "in", "cfg", ":", "\n", "        ", "cfg", ".", "pretrained", "=", "None", "\n", "\n", "# recursively turn off pretrained value", "\n", "", "for", "sub_cfg", "in", "cfg", ".", "values", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "sub_cfg", ",", "dict", ")", ":", "\n", "            ", "turn_off_pretrained", "(", "sub_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.inference_pytorch": [[141, 185], ["test.turn_off_pretrained", "mmaction.models.build_model", "cfg.get", "mmcv.runner.load_checkpoint", "len", "mmaction.utils.register_module_hooks", "mmcv.runner.fp16_utils.wrap_fp16_model", "mmcv.cnn.fuse_conv_bn", "mmcv.parallel.MMDataParallel", "single_gpu_test", "mmcv.parallel.MMDistributedDataParallel", "multi_gpu_test", "cfg.model.setdefault", "cfg.get", "mmcv.parallel.MMDistributedDataParallel.cuda", "cfg.model.get", "cfg.get", "dict", "cfg.model.get", "torch.cuda.current_device"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.turn_off_pretrained", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.register_module_hooks", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "", "", "def", "inference_pytorch", "(", "args", ",", "cfg", ",", "distributed", ",", "data_loader", ")", ":", "\n", "    ", "\"\"\"Get predictions by pytorch models.\"\"\"", "\n", "if", "args", ".", "average_clips", "is", "not", "None", ":", "\n", "# You can set average_clips during testing, it will override the", "\n", "# original setting", "\n", "        ", "if", "cfg", ".", "model", ".", "get", "(", "'test_cfg'", ")", "is", "None", "and", "cfg", ".", "get", "(", "'test_cfg'", ")", "is", "None", ":", "\n", "            ", "cfg", ".", "model", ".", "setdefault", "(", "'test_cfg'", ",", "\n", "dict", "(", "average_clips", "=", "args", ".", "average_clips", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "cfg", ".", "model", ".", "get", "(", "'test_cfg'", ")", "is", "not", "None", ":", "\n", "                ", "cfg", ".", "model", ".", "test_cfg", ".", "average_clips", "=", "args", ".", "average_clips", "\n", "", "else", ":", "\n", "                ", "cfg", ".", "test_cfg", ".", "average_clips", "=", "args", ".", "average_clips", "\n", "\n", "# remove redundant pretrain steps for testing", "\n", "", "", "", "turn_off_pretrained", "(", "cfg", ".", "model", ")", "\n", "\n", "# build the model and load checkpoint", "\n", "model", "=", "build_model", "(", "\n", "cfg", ".", "model", ",", "train_cfg", "=", "None", ",", "test_cfg", "=", "cfg", ".", "get", "(", "'test_cfg'", ")", ")", "\n", "\n", "if", "len", "(", "cfg", ".", "module_hooks", ")", ">", "0", ":", "\n", "        ", "register_module_hooks", "(", "model", ",", "cfg", ".", "module_hooks", ")", "\n", "\n", "", "fp16_cfg", "=", "cfg", ".", "get", "(", "'fp16'", ",", "None", ")", "\n", "if", "fp16_cfg", "is", "not", "None", ":", "\n", "        ", "wrap_fp16_model", "(", "model", ")", "\n", "", "load_checkpoint", "(", "model", ",", "args", ".", "checkpoint", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "if", "args", ".", "fuse_conv_bn", ":", "\n", "        ", "model", "=", "fuse_conv_bn", "(", "model", ")", "\n", "\n", "", "if", "not", "distributed", ":", "\n", "        ", "model", "=", "MMDataParallel", "(", "model", ",", "device_ids", "=", "[", "0", "]", ")", "\n", "outputs", "=", "single_gpu_test", "(", "model", ",", "data_loader", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "MMDistributedDataParallel", "(", "\n", "model", ".", "cuda", "(", ")", ",", "\n", "device_ids", "=", "[", "torch", ".", "cuda", ".", "current_device", "(", ")", "]", ",", "\n", "broadcast_buffers", "=", "False", ")", "\n", "outputs", "=", "multi_gpu_test", "(", "model", ",", "data_loader", ",", "args", ".", "tmpdir", ",", "\n", "args", ".", "gpu_collect", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.inference_tensorrt": [[187, 235], ["runtime.deserialize_cuda_engine.create_execution_context", "torch_dtype_from_trt", "tuple", "torch_device_from_trt", "torch.empty", "mmcv.ProgressBar", "trt.Logger", "trt.Runtime", "runtime.deserialize_cuda_engine", "runtime.deserialize_cuda_engine.get_binding_shape", "runtime.deserialize_cuda_engine.get_binding_dtype", "engine.create_execution_context.get_binding_shape", "runtime.deserialize_cuda_engine.get_location", "len", "engine.create_execution_context.execute_async_v2", "results.extend", "len", "range", "open", "f.read", "data[].contiguous().data_ptr", "torch.empty.contiguous().data_ptr", "torch.empty.cpu().numpy", "next", "mmcv.ProgressBar.update", "torch.cuda.current_stream", "iter", "data[].contiguous", "torch.empty.contiguous", "torch.empty.cpu", "data.values"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "inference_tensorrt", "(", "ckpt_path", ",", "distributed", ",", "data_loader", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Get predictions by TensorRT engine.\n\n    For now, multi-gpu mode and dynamic tensor shape are not supported.\n    \"\"\"", "\n", "assert", "not", "distributed", ",", "'TensorRT engine inference only supports single gpu mode.'", "\n", "import", "tensorrt", "as", "trt", "\n", "from", "mmcv", ".", "tensorrt", ".", "tensorrt_utils", "import", "(", "torch_dtype_from_trt", ",", "\n", "torch_device_from_trt", ")", "\n", "\n", "# load engine", "\n", "with", "trt", ".", "Logger", "(", ")", "as", "logger", ",", "trt", ".", "Runtime", "(", "logger", ")", "as", "runtime", ":", "\n", "        ", "with", "open", "(", "ckpt_path", ",", "mode", "=", "'rb'", ")", "as", "f", ":", "\n", "            ", "engine_bytes", "=", "f", ".", "read", "(", ")", "\n", "", "engine", "=", "runtime", ".", "deserialize_cuda_engine", "(", "engine_bytes", ")", "\n", "\n", "# For now, only support fixed input tensor", "\n", "", "cur_batch_size", "=", "engine", ".", "get_binding_shape", "(", "0", ")", "[", "0", "]", "\n", "assert", "batch_size", "==", "cur_batch_size", ",", "(", "'Dataset and TensorRT model should share the same batch size, '", "\n", "f'but get {batch_size} and {cur_batch_size}'", ")", "\n", "\n", "context", "=", "engine", ".", "create_execution_context", "(", ")", "\n", "\n", "# get output tensor", "\n", "dtype", "=", "torch_dtype_from_trt", "(", "engine", ".", "get_binding_dtype", "(", "1", ")", ")", "\n", "shape", "=", "tuple", "(", "context", ".", "get_binding_shape", "(", "1", ")", ")", "\n", "device", "=", "torch_device_from_trt", "(", "engine", ".", "get_location", "(", "1", ")", ")", "\n", "output", "=", "torch", ".", "empty", "(", "\n", "size", "=", "shape", ",", "dtype", "=", "dtype", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "\n", "# get predictions", "\n", "results", "=", "[", "]", "\n", "dataset", "=", "data_loader", ".", "dataset", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "len", "(", "dataset", ")", ")", "\n", "for", "data", "in", "data_loader", ":", "\n", "        ", "bindings", "=", "[", "\n", "data", "[", "'imgs'", "]", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", ",", "\n", "output", ".", "contiguous", "(", ")", ".", "data_ptr", "(", ")", "\n", "]", "\n", "context", ".", "execute_async_v2", "(", "bindings", ",", "\n", "torch", ".", "cuda", ".", "current_stream", "(", ")", ".", "cuda_stream", ")", "\n", "results", ".", "extend", "(", "output", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "batch_size", "=", "len", "(", "next", "(", "iter", "(", "data", ".", "values", "(", ")", ")", ")", ")", "\n", "for", "_", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "prog_bar", ".", "update", "(", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.inference_onnx": [[237, 278], ["onnx.load", "list", "rt.InferenceSession", "mmcv.ProgressBar", "len", "len", "data[].cpu().numpy", "results.extend", "len", "range", "set", "set", "rt.InferenceSession.run", "next", "mmcv.ProgressBar.update", "data[].cpu", "iter", "data.values"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "inference_onnx", "(", "ckpt_path", ",", "distributed", ",", "data_loader", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Get predictions by ONNX.\n\n    For now, multi-gpu mode and dynamic tensor shape are not supported.\n    \"\"\"", "\n", "assert", "not", "distributed", ",", "'ONNX inference only supports single gpu mode.'", "\n", "\n", "import", "onnx", "\n", "import", "onnxruntime", "as", "rt", "\n", "\n", "# get input tensor name", "\n", "onnx_model", "=", "onnx", ".", "load", "(", "ckpt_path", ")", "\n", "input_all", "=", "[", "node", ".", "name", "for", "node", "in", "onnx_model", ".", "graph", ".", "input", "]", "\n", "input_initializer", "=", "[", "node", ".", "name", "for", "node", "in", "onnx_model", ".", "graph", ".", "initializer", "]", "\n", "net_feed_input", "=", "list", "(", "set", "(", "input_all", ")", "-", "set", "(", "input_initializer", ")", ")", "\n", "assert", "len", "(", "net_feed_input", ")", "==", "1", "\n", "\n", "# For now, only support fixed tensor shape", "\n", "input_tensor", "=", "None", "\n", "for", "tensor", "in", "onnx_model", ".", "graph", ".", "input", ":", "\n", "        ", "if", "tensor", ".", "name", "==", "net_feed_input", "[", "0", "]", ":", "\n", "            ", "input_tensor", "=", "tensor", "\n", "break", "\n", "", "", "cur_batch_size", "=", "input_tensor", ".", "type", ".", "tensor_type", ".", "shape", ".", "dim", "[", "0", "]", ".", "dim_value", "\n", "assert", "batch_size", "==", "cur_batch_size", ",", "(", "'Dataset and ONNX model should share the same batch size, '", "\n", "f'but get {batch_size} and {cur_batch_size}'", ")", "\n", "\n", "# get predictions", "\n", "sess", "=", "rt", ".", "InferenceSession", "(", "ckpt_path", ")", "\n", "results", "=", "[", "]", "\n", "dataset", "=", "data_loader", ".", "dataset", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "len", "(", "dataset", ")", ")", "\n", "for", "data", "in", "data_loader", ":", "\n", "        ", "imgs", "=", "data", "[", "'imgs'", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "onnx_result", "=", "sess", ".", "run", "(", "None", ",", "{", "net_feed_input", "[", "0", "]", ":", "imgs", "}", ")", "[", "0", "]", "\n", "results", ".", "extend", "(", "onnx_result", ")", "\n", "batch_size", "=", "len", "(", "next", "(", "iter", "(", "data", ".", "values", "(", ")", ")", ")", ")", "\n", "for", "_", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "prog_bar", ".", "update", "(", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.main": [[280, 376], ["test.parse_args", "mmcv.Config.fromfile", "Config.fromfile.merge_from_dict", "Config.fromfile.get", "Config.fromfile.get", "Config._merge_a_into_b.get", "Config.fromfile.get", "Config.fromfile.setdefault", "mmaction.datasets.build_dataset", "dict", "dict", "mmaction.datasets.build_dataloader", "mmcv.runner.get_dist_info", "ValueError", "mmcv.Config._merge_a_into_b", "mmcv.Config._merge_a_into_b", "mmcv.Config._merge_a_into_b", "mmcv.runner.init_dist", "dict", "test.inference_tensorrt", "Config._merge_a_into_b.get", "dict", "dict", "warnings.warn", "mmcv.mkdir_or_exist", "os.splitext", "Config.fromfile.data.get", "Config.fromfile.data.get", "Config.fromfile.data.get", "test.inference_onnx", "test.inference_pytorch", "print", "mmaction.datasets.build_dataset.dump_results", "mmaction.datasets.build_dataset.evaluate", "dataset.evaluate.items", "os.dirname", "print"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.inference_tensorrt", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.tools.test.inference_onnx", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.inference_pytorch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.dump_results", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "tensorrt", "and", "args", ".", "onnx", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'Cannot set onnx mode and tensorrt mode at the same time.'", ")", "\n", "\n", "", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "\n", "cfg", ".", "merge_from_dict", "(", "args", ".", "cfg_options", ")", "\n", "\n", "# Load output_config from cfg", "\n", "output_config", "=", "cfg", ".", "get", "(", "'output_config'", ",", "{", "}", ")", "\n", "if", "args", ".", "out", ":", "\n", "# Overwrite output_config from args.out", "\n", "        ", "output_config", "=", "Config", ".", "_merge_a_into_b", "(", "\n", "dict", "(", "out", "=", "args", ".", "out", ")", ",", "output_config", ")", "\n", "\n", "# Load eval_config from cfg", "\n", "", "eval_config", "=", "cfg", ".", "get", "(", "'eval_config'", ",", "{", "}", ")", "\n", "if", "args", ".", "eval", ":", "\n", "# Overwrite eval_config from args.eval", "\n", "        ", "eval_config", "=", "Config", ".", "_merge_a_into_b", "(", "\n", "dict", "(", "metrics", "=", "args", ".", "eval", ")", ",", "eval_config", ")", "\n", "", "if", "args", ".", "eval_options", ":", "\n", "# Add options from args.eval_options", "\n", "        ", "eval_config", "=", "Config", ".", "_merge_a_into_b", "(", "args", ".", "eval_options", ",", "eval_config", ")", "\n", "\n", "", "assert", "output_config", "or", "eval_config", ",", "(", "'Please specify at least one operation (save or eval the '", "\n", "'results) with the argument \"--out\" or \"--eval\"'", ")", "\n", "\n", "dataset_type", "=", "cfg", ".", "data", ".", "test", ".", "type", "\n", "if", "output_config", ".", "get", "(", "'out'", ",", "None", ")", ":", "\n", "        ", "if", "'output_format'", "in", "output_config", ":", "\n", "# ugly workround to make recognition and localization the same", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Skip checking `output_format` in localization task.'", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "output_config", "[", "'out'", "]", "\n", "# make sure the dirname of the output path exists", "\n", "mmcv", ".", "mkdir_or_exist", "(", "osp", ".", "dirname", "(", "out", ")", ")", "\n", "_", ",", "suffix", "=", "osp", ".", "splitext", "(", "out", ")", "\n", "if", "dataset_type", "==", "'AVADataset'", ":", "\n", "                ", "assert", "suffix", "[", "1", ":", "]", "==", "'csv'", ",", "(", "'For AVADataset, the format of '", "\n", "'the output file should be csv'", ")", "\n", "", "else", ":", "\n", "                ", "assert", "suffix", "[", "1", ":", "]", "in", "file_handlers", ",", "(", "\n", "'The format of the output '", "\n", "'file should be json, pickle or yaml'", ")", "\n", "\n", "# set cudnn benchmark", "\n", "", "", "", "if", "cfg", ".", "get", "(", "'cudnn_benchmark'", ",", "False", ")", ":", "\n", "        ", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "", "cfg", ".", "data", ".", "test", ".", "test_mode", "=", "True", "\n", "\n", "# init distributed env first, since logger depends on the dist info.", "\n", "if", "args", ".", "launcher", "==", "'none'", ":", "\n", "        ", "distributed", "=", "False", "\n", "", "else", ":", "\n", "        ", "distributed", "=", "True", "\n", "init_dist", "(", "args", ".", "launcher", ",", "**", "cfg", ".", "dist_params", ")", "\n", "\n", "# The flag is used to register module's hooks", "\n", "", "cfg", ".", "setdefault", "(", "'module_hooks'", ",", "[", "]", ")", "\n", "\n", "# build the dataloader", "\n", "dataset", "=", "build_dataset", "(", "cfg", ".", "data", ".", "test", ",", "dict", "(", "test_mode", "=", "True", ")", ")", "\n", "dataloader_setting", "=", "dict", "(", "\n", "videos_per_gpu", "=", "cfg", ".", "data", ".", "get", "(", "'videos_per_gpu'", ",", "1", ")", ",", "\n", "workers_per_gpu", "=", "cfg", ".", "data", ".", "get", "(", "'workers_per_gpu'", ",", "1", ")", ",", "\n", "dist", "=", "distributed", ",", "\n", "shuffle", "=", "False", ")", "\n", "dataloader_setting", "=", "dict", "(", "dataloader_setting", ",", "\n", "**", "cfg", ".", "data", ".", "get", "(", "'test_dataloader'", ",", "{", "}", ")", ")", "\n", "data_loader", "=", "build_dataloader", "(", "dataset", ",", "**", "dataloader_setting", ")", "\n", "\n", "if", "args", ".", "tensorrt", ":", "\n", "        ", "outputs", "=", "inference_tensorrt", "(", "args", ".", "checkpoint", ",", "distributed", ",", "data_loader", ",", "\n", "dataloader_setting", "[", "'videos_per_gpu'", "]", ")", "\n", "", "elif", "args", ".", "onnx", ":", "\n", "        ", "outputs", "=", "inference_onnx", "(", "args", ".", "checkpoint", ",", "distributed", ",", "data_loader", ",", "\n", "dataloader_setting", "[", "'videos_per_gpu'", "]", ")", "\n", "", "else", ":", "\n", "        ", "outputs", "=", "inference_pytorch", "(", "args", ",", "cfg", ",", "distributed", ",", "data_loader", ")", "\n", "\n", "", "rank", ",", "_", "=", "get_dist_info", "(", ")", "\n", "if", "rank", "==", "0", ":", "\n", "        ", "if", "output_config", ".", "get", "(", "'out'", ",", "None", ")", ":", "\n", "            ", "out", "=", "output_config", "[", "'out'", "]", "\n", "print", "(", "f'\\nwriting results to {out}'", ")", "\n", "dataset", ".", "dump_results", "(", "outputs", ",", "**", "output_config", ")", "\n", "", "if", "eval_config", ":", "\n", "            ", "eval_res", "=", "dataset", ".", "evaluate", "(", "outputs", ",", "**", "eval_config", ")", "\n", "for", "name", ",", "val", "in", "eval_res", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "f'{name}: {val:.04f}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.print_config.parse_args": [[7, 15], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Print the whole config'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'config file path'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--options'", ",", "nargs", "=", "'+'", ",", "action", "=", "DictAction", ",", "help", "=", "'arguments in dict'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.print_config.main": [[17, 24], ["print_config.parse_args", "mmcv.Config.fromfile", "print", "Config.fromfile.merge_from_dict"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "if", "args", ".", "options", "is", "not", "None", ":", "\n", "        ", "cfg", ".", "merge_from_dict", "(", "args", ".", "options", ")", "\n", "", "print", "(", "f'Config:\\n{cfg.pretty_text}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.bench_processing.main": [[23, 62], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "mmcv.Config.fromfile", "mmaction.utils.get_root_logger", "mmaction.utils.get_root_logger.info", "mmaction.utils.get_root_logger.info", "mmaction.datasets.build_dataset", "mmaction.datasets.build_dataloader", "mmcv.ProgressBar", "enumerate", "os.path.exists", "open", "len", "mmcv.ProgressBar.start", "mmcv.ProgressBar.update", "f.readlines", "open", "f1.writelines"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Benchmark dataloading'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'train config file path'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "\n", "# init logger before other steps", "\n", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'MMAction2 Version: {__version__}'", ")", "\n", "logger", ".", "info", "(", "f'Config: {cfg.text}'", ")", "\n", "\n", "# create bench data list", "\n", "ann_file_bench", "=", "'benchlist.txt'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "ann_file_bench", ")", ":", "\n", "        ", "with", "open", "(", "cfg", ".", "ann_file_train", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "[", ":", "256", "]", "\n", "with", "open", "(", "ann_file_bench", ",", "'w'", ")", "as", "f1", ":", "\n", "                ", "f1", ".", "writelines", "(", "lines", ")", "\n", "", "", "", "cfg", ".", "data", ".", "train", ".", "ann_file", "=", "ann_file_bench", "\n", "\n", "dataset", "=", "build_dataset", "(", "cfg", ".", "data", ".", "train", ")", "\n", "data_loader", "=", "build_dataloader", "(", "\n", "dataset", ",", "\n", "videos_per_gpu", "=", "cfg", ".", "data", ".", "videos_per_gpu", ",", "\n", "workers_per_gpu", "=", "0", ",", "\n", "persistent_workers", "=", "False", ",", "\n", "num_gpus", "=", "1", ",", "\n", "dist", "=", "False", ")", "\n", "\n", "# Start progress bar after first 5 batches", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "\n", "len", "(", "dataset", ")", "-", "5", "*", "cfg", ".", "data", ".", "videos_per_gpu", ",", "start", "=", "False", ")", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "        ", "if", "i", "==", "5", ":", "\n", "            ", "prog_bar", ".", "start", "(", ")", "\n", "", "for", "_", "in", "data", "[", "'imgs'", "]", ":", "\n", "            ", "if", "i", "<", "5", ":", "\n", "                ", "continue", "\n", "", "prog_bar", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.get_flops.parse_args": [[21, 32], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Train a recognizer'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'train config file path'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--shape'", ",", "\n", "type", "=", "int", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "340", ",", "256", "]", ",", "\n", "help", "=", "'input image size'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.get_flops.main": [[34, 72], ["get_flops.parse_args", "mmcv.Config.fromfile", "mmaction.models.build_recognizer", "model.cuda.cuda", "model.cuda.eval", "fvcore.nn.FlopCountAnalysis", "print", "print", "hasattr", "len", "fvcore.nn.flop_count_str", "NotImplementedError", "len", "Config.fromfile.get", "Config.fromfile.get", "torch.ones", "torch.ones", "fvcore.nn.FlopCountAnalysis.total", "tuple", "len", "tuple", "len", "tuple", "ValueError"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_recognizer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "main", "(", ")", ":", "\n", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "if", "len", "(", "args", ".", "shape", ")", "==", "1", ":", "\n", "        ", "input_shape", "=", "(", "1", ",", "3", ",", "args", ".", "shape", "[", "0", "]", ",", "args", ".", "shape", "[", "0", "]", ")", "\n", "", "elif", "len", "(", "args", ".", "shape", ")", "==", "2", ":", "\n", "        ", "input_shape", "=", "(", "\n", "1", ",", "\n", "3", ",", "\n", ")", "+", "tuple", "(", "args", ".", "shape", ")", "\n", "", "elif", "len", "(", "args", ".", "shape", ")", "==", "4", ":", "\n", "# n, c, h, w = args.shape", "\n", "        ", "input_shape", "=", "tuple", "(", "args", ".", "shape", ")", "\n", "", "elif", "len", "(", "args", ".", "shape", ")", "==", "5", ":", "\n", "# n, c, t, h, w = args.shape", "\n", "        ", "input_shape", "=", "tuple", "(", "args", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'invalid input shape'", ")", "\n", "\n", "", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "model", "=", "build_recognizer", "(", "\n", "cfg", ".", "model", ",", "\n", "train_cfg", "=", "cfg", ".", "get", "(", "'train_cfg'", ")", ",", "\n", "test_cfg", "=", "cfg", ".", "get", "(", "'test_cfg'", ")", ")", "\n", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "flops", "=", "FlopCountAnalysis", "(", "model", ",", "(", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "'cuda'", ")", ",", "\n", "torch", ".", "ones", "(", "(", "400", ")", ",", "device", "=", "'cuda'", ")", ")", ")", "\n", "print", "(", "flop_count_str", "(", "flops", ")", ")", "\n", "print", "(", "flops", ".", "total", "(", ")", "/", "(", "1e+9", ")", ")", "\n", "if", "hasattr", "(", "model", ",", "'forward_dummy'", ")", ":", "\n", "        ", "model", ".", "forward", "=", "model", ".", "forward_dummy", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "'FLOPs counter is currently not currently supported with {}'", ".", "\n", "format", "(", "model", ".", "__class__", ".", "__name__", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.cal_train_time": [[11, 32], ["enumerate", "print", "log_dict.keys", "numpy.array", "np.array.mean", "all_times.mean.argmax", "all_times.mean.argmin", "all_times.mean.std", "print", "print", "print", "print", "print", "np.array.append", "np.array.append", "numpy.mean"], "function", ["None"], ["def", "cal_train_time", "(", "log_dicts", ",", "args", ")", ":", "\n", "    ", "for", "i", ",", "log_dict", "in", "enumerate", "(", "log_dicts", ")", ":", "\n", "        ", "print", "(", "f'{\"-\" * 5}Analyze train time of {args.json_logs[i]}{\"-\" * 5}'", ")", "\n", "all_times", "=", "[", "]", "\n", "for", "epoch", "in", "log_dict", ".", "keys", "(", ")", ":", "\n", "            ", "if", "args", ".", "include_outliers", ":", "\n", "                ", "all_times", ".", "append", "(", "log_dict", "[", "epoch", "]", "[", "'time'", "]", ")", "\n", "", "else", ":", "\n", "                ", "all_times", ".", "append", "(", "log_dict", "[", "epoch", "]", "[", "'time'", "]", "[", "1", ":", "]", ")", "\n", "", "", "all_times", "=", "np", ".", "array", "(", "all_times", ")", "\n", "epoch_ave_time", "=", "all_times", ".", "mean", "(", "-", "1", ")", "\n", "slowest_epoch", "=", "epoch_ave_time", ".", "argmax", "(", ")", "\n", "fastest_epoch", "=", "epoch_ave_time", ".", "argmin", "(", ")", "\n", "std_over_epoch", "=", "epoch_ave_time", ".", "std", "(", ")", "\n", "print", "(", "f'slowest epoch {slowest_epoch + 1}, '", "\n", "f'average time is {epoch_ave_time[slowest_epoch]:.4f}'", ")", "\n", "print", "(", "f'fastest epoch {fastest_epoch + 1}, '", "\n", "f'average time is {epoch_ave_time[fastest_epoch]:.4f}'", ")", "\n", "print", "(", "f'time std over epochs is {std_over_epoch:.4f}'", ")", "\n", "print", "(", "f'average iter time: {np.mean(all_times):.4f} s/iter'", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.plot_curve": [[34, 78], ["seaborn.set_style", "len", "enumerate", "matplotlib.switch_backend", "len", "list", "enumerate", "matplotlib.show", "print", "matplotlib.savefig", "matplotlib.cla", "len", "len", "log_dict.keys", "print", "numpy.concatenate", "numpy.concatenate", "matplotlib.xlabel", "matplotlib.plot", "matplotlib.legend", "matplotlib.title", "legend.append", "KeyError", "np.concatenate.append", "np.concatenate.append", "numpy.array", "numpy.array", "len"], "function", ["None"], ["", "", "def", "plot_curve", "(", "log_dicts", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "backend", "is", "not", "None", ":", "\n", "        ", "plt", ".", "switch_backend", "(", "args", ".", "backend", ")", "\n", "", "sns", ".", "set_style", "(", "args", ".", "style", ")", "\n", "# if legend is None, use {filename}_{key} as legend", "\n", "legend", "=", "args", ".", "legend", "\n", "if", "legend", "is", "None", ":", "\n", "        ", "legend", "=", "[", "]", "\n", "for", "json_log", "in", "args", ".", "json_logs", ":", "\n", "            ", "for", "metric", "in", "args", ".", "keys", ":", "\n", "                ", "legend", ".", "append", "(", "f'{json_log}_{metric}'", ")", "\n", "", "", "", "assert", "len", "(", "legend", ")", "==", "(", "len", "(", "args", ".", "json_logs", ")", "*", "len", "(", "args", ".", "keys", ")", ")", "\n", "metrics", "=", "args", ".", "keys", "\n", "\n", "num_metrics", "=", "len", "(", "metrics", ")", "\n", "for", "i", ",", "log_dict", "in", "enumerate", "(", "log_dicts", ")", ":", "\n", "        ", "epochs", "=", "list", "(", "log_dict", ".", "keys", "(", ")", ")", "\n", "for", "j", ",", "metric", "in", "enumerate", "(", "metrics", ")", ":", "\n", "            ", "print", "(", "f'plot curve of {args.json_logs[i]}, metric is {metric}'", ")", "\n", "if", "metric", "not", "in", "log_dict", "[", "epochs", "[", "0", "]", "]", ":", "\n", "                ", "raise", "KeyError", "(", "\n", "f'{args.json_logs[i]} does not contain metric {metric}'", ")", "\n", "", "xs", "=", "[", "]", "\n", "ys", "=", "[", "]", "\n", "num_iters_per_epoch", "=", "log_dict", "[", "epochs", "[", "0", "]", "]", "[", "'iter'", "]", "[", "-", "1", "]", "\n", "for", "epoch", "in", "epochs", ":", "\n", "                ", "iters", "=", "log_dict", "[", "epoch", "]", "[", "'iter'", "]", "\n", "if", "log_dict", "[", "epoch", "]", "[", "'mode'", "]", "[", "-", "1", "]", "==", "'val'", ":", "\n", "                    ", "iters", "=", "iters", "[", ":", "-", "1", "]", "\n", "", "xs", ".", "append", "(", "np", ".", "array", "(", "iters", ")", "+", "(", "epoch", "-", "1", ")", "*", "num_iters_per_epoch", ")", "\n", "ys", ".", "append", "(", "np", ".", "array", "(", "log_dict", "[", "epoch", "]", "[", "metric", "]", "[", ":", "len", "(", "iters", ")", "]", ")", ")", "\n", "", "xs", "=", "np", ".", "concatenate", "(", "xs", ")", "\n", "ys", "=", "np", ".", "concatenate", "(", "ys", ")", "\n", "plt", ".", "xlabel", "(", "'iter'", ")", "\n", "plt", ".", "plot", "(", "xs", ",", "ys", ",", "label", "=", "legend", "[", "i", "*", "num_metrics", "+", "j", "]", ",", "linewidth", "=", "0.5", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "", "if", "args", ".", "title", "is", "not", "None", ":", "\n", "            ", "plt", ".", "title", "(", "args", ".", "title", ")", "\n", "", "", "if", "args", ".", "out", "is", "None", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f'save curve to: {args.out}'", ")", "\n", "plt", ".", "savefig", "(", "args", ".", "out", ")", "\n", "plt", ".", "cla", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.add_plot_parser": [[80, 106], ["subparsers.add_parser", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument"], "function", ["None"], ["", "", "def", "add_plot_parser", "(", "subparsers", ")", ":", "\n", "    ", "parser_plt", "=", "subparsers", ".", "add_parser", "(", "\n", "'plot_curve'", ",", "help", "=", "'parser for plotting curves'", ")", "\n", "parser_plt", ".", "add_argument", "(", "\n", "'json_logs'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "help", "=", "'path of train log in json format'", ")", "\n", "parser_plt", ".", "add_argument", "(", "\n", "'--keys'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "'top1_acc'", "]", ",", "\n", "help", "=", "'the metric that you want to plot'", ")", "\n", "parser_plt", ".", "add_argument", "(", "'--title'", ",", "type", "=", "str", ",", "help", "=", "'title of figure'", ")", "\n", "parser_plt", ".", "add_argument", "(", "\n", "'--legend'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'legend of each plot'", ")", "\n", "parser_plt", ".", "add_argument", "(", "\n", "'--backend'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "'backend of plt'", ")", "\n", "parser_plt", ".", "add_argument", "(", "\n", "'--style'", ",", "type", "=", "str", ",", "default", "=", "'dark'", ",", "help", "=", "'style of plt'", ")", "\n", "parser_plt", ".", "add_argument", "(", "'--out'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.add_time_parser": [[108, 121], ["subparsers.add_parser", "subparsers.add_parser.add_argument", "subparsers.add_parser.add_argument"], "function", ["None"], ["", "def", "add_time_parser", "(", "subparsers", ")", ":", "\n", "    ", "parser_time", "=", "subparsers", ".", "add_parser", "(", "\n", "'cal_train_time'", ",", "\n", "help", "=", "'parser for computing the average time per training iteration'", ")", "\n", "parser_time", ".", "add_argument", "(", "\n", "'json_logs'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "help", "=", "'path of train log in json format'", ")", "\n", "parser_time", ".", "add_argument", "(", "\n", "'--include-outliers'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include the first value of every epoch when computing '", "\n", "'the average time'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.parse_args": [[124, 132], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_subparsers", "analyze_logs.add_plot_parser", "analyze_logs.add_time_parser", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.add_plot_parser", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.add_time_parser", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Analyze Json Log'", ")", "\n", "# currently only support plot curve and calculate average train time", "\n", "subparsers", "=", "parser", ".", "add_subparsers", "(", "dest", "=", "'task'", ",", "help", "=", "'task parser'", ")", "\n", "add_plot_parser", "(", "subparsers", ")", "\n", "add_time_parser", "(", "subparsers", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.load_json_logs": [[134, 152], ["zip", "dict", "open", "json.loads", "json.loads.pop", "json.loads.items", "line.strip", "collections.defaultdict", "[].append"], "function", ["None"], ["", "def", "load_json_logs", "(", "json_logs", ")", ":", "\n", "# load and convert json_logs to log_dict, key is epoch, value is a sub dict", "\n", "# keys of sub dict is different metrics, e.g. memory, top1_acc", "\n", "# value of sub dict is a list of corresponding values of all iterations", "\n", "    ", "log_dicts", "=", "[", "dict", "(", ")", "for", "_", "in", "json_logs", "]", "\n", "for", "json_log", ",", "log_dict", "in", "zip", "(", "json_logs", ",", "log_dicts", ")", ":", "\n", "        ", "with", "open", "(", "json_log", ",", "'r'", ")", "as", "log_file", ":", "\n", "            ", "for", "line", "in", "log_file", ":", "\n", "                ", "log", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "# skip lines without `epoch` field", "\n", "if", "'epoch'", "not", "in", "log", ":", "\n", "                    ", "continue", "\n", "", "epoch", "=", "log", ".", "pop", "(", "'epoch'", ")", "\n", "if", "epoch", "not", "in", "log_dict", ":", "\n", "                    ", "log_dict", "[", "epoch", "]", "=", "defaultdict", "(", "list", ")", "\n", "", "for", "k", ",", "v", "in", "log", ".", "items", "(", ")", ":", "\n", "                    ", "log_dict", "[", "epoch", "]", "[", "k", "]", ".", "append", "(", "v", ")", "\n", "", "", "", "", "return", "log_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.main": [[154, 164], ["analyze_logs.parse_args", "analyze_logs.load_json_logs", "json_log.endswith", "eval"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.analyze_logs.load_json_logs"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "json_logs", "=", "args", ".", "json_logs", "\n", "for", "json_log", "in", "json_logs", ":", "\n", "        ", "assert", "json_log", ".", "endswith", "(", "'.json'", ")", "\n", "\n", "", "log_dicts", "=", "load_json_logs", "(", "json_logs", ")", "\n", "\n", "eval", "(", "args", ".", "task", ")", "(", "log_dicts", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.report_accuracy.parse_args": [[11, 31], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Fusing multiple scores'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--scores'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "help", "=", "'list of scores'", ",", "\n", "default", "=", "[", "'demo/fuse/rgb.pkl'", ",", "'demo/fuse/flow.pkl'", "]", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--coefficients'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "'coefficients of each score file'", ",", "\n", "default", "=", "[", "1.0", ",", "1.0", "]", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--datalist'", ",", "\n", "help", "=", "'list of testing data'", ",", "\n", "default", "=", "'demo/fuse/data_list.txt'", ")", "\n", "parser", ".", "add_argument", "(", "'--apply-softmax'", ",", "action", "=", "'store_true'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.report_accuracy.main": [[33, 54], ["report_accuracy.parse_args", "mmaction.core.evaluation.get_weighted_score", "open().readlines", "mmaction.core.evaluation.mean_class_accuracy", "mmaction.core.evaluation.top_k_accuracy", "print", "print", "print", "len", "len", "mmcv.load", "int", "report_accuracy.main.apply_softmax"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.get_weighted_score", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_class_accuracy", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.top_k_accuracy"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "assert", "len", "(", "args", ".", "scores", ")", "==", "len", "(", "args", ".", "coefficients", ")", "\n", "score_list", "=", "args", ".", "scores", "\n", "score_list", "=", "[", "load", "(", "f", ")", "for", "f", "in", "score_list", "]", "\n", "if", "args", ".", "apply_softmax", ":", "\n", "\n", "        ", "def", "apply_softmax", "(", "scores", ")", ":", "\n", "            ", "return", "[", "softmax", "(", "score", ")", "for", "score", "in", "scores", "]", "\n", "\n", "", "score_list", "=", "[", "apply_softmax", "(", "scores", ")", "for", "scores", "in", "score_list", "]", "\n", "\n", "", "weighted_scores", "=", "get_weighted_score", "(", "score_list", ",", "args", ".", "coefficients", ")", "\n", "data", "=", "open", "(", "args", ".", "datalist", ")", ".", "readlines", "(", ")", "\n", "labels", "=", "[", "int", "(", "x", ".", "strip", "(", ")", ".", "split", "(", ")", "[", "-", "1", "]", ")", "for", "x", "in", "data", "]", "\n", "\n", "mean_class_acc", "=", "mean_class_accuracy", "(", "weighted_scores", ",", "labels", ")", "\n", "top_1_acc", ",", "top_5_acc", "=", "top_k_accuracy", "(", "weighted_scores", ",", "labels", ",", "(", "1", ",", "5", ")", ")", "\n", "print", "(", "f'Mean Class Accuracy: {mean_class_acc:.04f}'", ")", "\n", "print", "(", "f'Top 1 Accuracy: {top_1_acc:.04f}'", ")", "\n", "print", "(", "f'Top 5 Accuracy: {top_5_acc:.04f}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.benchmark.parse_args": [[15, 28], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["import", "torch", ".", "nn", "as", "nn", "\n", "import", "torch", ".", "nn", ".", "parallel", "\n", "from", "collections", "import", "OrderedDict", "\n", "from", "contextlib", "import", "suppress", "\n", "from", "functools", "import", "partial", "\n", "\n", "from", "timm", ".", "models", "import", "create_model", ",", "is_model", ",", "list_models", "\n", "from", "timm", ".", "optim", "import", "create_optimizer_v2", "\n", "from", "timm", ".", "data", "import", "resolve_data_config", "\n", "from", "timm", ".", "utils", "import", "AverageMeter", ",", "setup_default_logging", "\n", "\n", "\n", "has_apex", "=", "False", "\n", "try", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.benchmark.main": [[30, 91], ["benchmark.parse_args", "mmcv.Config.fromfile", "Config.fromfile.get", "mmaction.datasets.build_dataset", "mmaction.datasets.build_dataloader", "mmaction.models.build_model", "Config.fromfile.get", "mmcv.parallel.MMDataParallel", "mmcv.cnn.fuse_conv_bn.eval", "enumerate", "dict", "mmcv.runner.fp16_utils.wrap_fp16_model", "mmcv.cnn.fuse_conv_bn", "torch.cuda.synchronize", "time.perf_counter", "torch.cuda.synchronize", "Config.fromfile.data.get", "Config.fromfile.get", "torch.no_grad", "mmcv.cnn.fuse_conv_bn.", "time.perf_counter", "print", "print"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["has_apex", "=", "True", "\n", "", "except", "ImportError", ":", "\n", "    ", "pass", "\n", "\n", "", "has_native_amp", "=", "False", "\n", "try", ":", "\n", "    ", "if", "getattr", "(", "torch", ".", "cuda", ".", "amp", ",", "'autocast'", ")", "is", "not", "None", ":", "\n", "        ", "has_native_amp", "=", "True", "\n", "", "", "except", "AttributeError", ":", "\n", "    ", "pass", "\n", "\n", "", "try", ":", "\n", "    ", "from", "deepspeed", ".", "profiling", ".", "flops_profiler", "import", "get_model_profile", "\n", "has_deepspeed_profiling", "=", "True", "\n", "", "except", "ImportError", "as", "e", ":", "\n", "    ", "has_deepspeed_profiling", "=", "False", "\n", "\n", "", "try", ":", "\n", "    ", "from", "fvcore", ".", "nn", "import", "FlopCountAnalysis", ",", "flop_count_str", ",", "ActivationCountAnalysis", "\n", "has_fvcore_profiling", "=", "True", "\n", "", "except", "ImportError", "as", "e", ":", "\n", "    ", "FlopCountAnalysis", "=", "None", "\n", "has_fvcore_profiling", "=", "False", "\n", "\n", "\n", "", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "_logger", "=", "logging", ".", "getLogger", "(", "'validate'", ")", "\n", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch Benchmark'", ")", "\n", "\n", "# benchmark specific args", "\n", "parser", ".", "add_argument", "(", "'--model-list'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "''", ",", "\n", "help", "=", "'txt file based list of model names to benchmark'", ")", "\n", "parser", ".", "add_argument", "(", "'--bench'", ",", "default", "=", "'both'", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Benchmark mode. One of 'inference', 'train', 'both'. Defaults to 'both'\"", ")", "\n", "parser", ".", "add_argument", "(", "'--detail'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Provide train fwd/bwd/opt breakdown detail if True. Defaults to False'", ")", "\n", "parser", ".", "add_argument", "(", "'--results-file'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILENAME'", ",", "\n", "help", "=", "'Output csv file for validation results (summary)'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-warm-iter'", ",", "default", "=", "10", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Number of warmup iterations (default: 10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-bench-iter'", ",", "default", "=", "40", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Number of benchmark iterations (default: 40)'", ")", "\n", "\n", "# common inference / train args", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "'-m'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "'resnet50'", ",", "\n", "help", "=", "'model architecture (default: resnet50)'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch-size'", ",", "default", "=", "256", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'mini-batch size (default: 256)'", ")", "\n", "parser", ".", "add_argument", "(", "'--img-size'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Input image dimension, uses model default if empty'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "None", ",", "nargs", "=", "3", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N N N'", ",", "help", "=", "'Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty'", ")", "\n", "parser", ".", "add_argument", "(", "'--use-train-size'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Run inference at train size, not test-input-size if it exists.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'Number classes in dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--gp'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "metavar", "=", "'POOL'", ",", "\n", "help", "=", "'Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.'", ")", "\n", "parser", ".", "add_argument", "(", "'--channels-last'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use channels_last memory layout'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.eval_metric.parse_args": [[10, 37], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Evaluate metric of the '", "\n", "'results saved in pkl/yaml/json format'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'Config of the model'", ")", "\n", "parser", ".", "add_argument", "(", "'results'", ",", "help", "=", "'Results in pkl/yaml/json format'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--eval'", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "'+'", ",", "\n", "help", "=", "'evaluation metrics, which depends on the dataset, e.g.,'", "\n", "' \"top_k_accuracy\", \"mean_class_accuracy\" for video dataset'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cfg-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'override some settings in the used config, the key-value pair '", "\n", "'in xxx=yyy format will be merged into config file. For example, '", "\n", "\"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--eval-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "help", "=", "'custom options for evaluation, the key-value pair in xxx=yyy '", "\n", "'format will be kwargs for dataset.evaluate() function'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.eval_metric.main": [[39, 63], ["eval_metric.parse_args", "mmcv.Config.fromfile", "mmaction.datasets.build_dataset", "mmcv.load", "Config.fromfile.get().copy", "cfg.get().copy.update", "print", "Config.fromfile.merge_from_dict", "cfg.get().copy.pop", "dict", "mmaction.datasets.build_dataset.evaluate", "Config.fromfile.get"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "\n", "assert", "args", ".", "eval", "is", "not", "None", "\n", "\n", "if", "args", ".", "cfg_options", "is", "not", "None", ":", "\n", "        ", "cfg", ".", "merge_from_dict", "(", "args", ".", "cfg_options", ")", "\n", "", "cfg", ".", "data", ".", "test", ".", "test_mode", "=", "True", "\n", "\n", "dataset", "=", "build_dataset", "(", "cfg", ".", "data", ".", "test", ")", "\n", "outputs", "=", "mmcv", ".", "load", "(", "args", ".", "results", ")", "\n", "\n", "kwargs", "=", "{", "}", "if", "args", ".", "eval_options", "is", "None", "else", "args", ".", "eval_options", "\n", "eval_kwargs", "=", "cfg", ".", "get", "(", "'evaluation'", ",", "{", "}", ")", ".", "copy", "(", ")", "\n", "# hard-code way to remove EvalHook args", "\n", "for", "key", "in", "[", "\n", "'interval'", ",", "'tmpdir'", ",", "'start'", ",", "'gpu_collect'", ",", "'save_best'", ",", "'rule'", ",", "\n", "'by_epoch'", "\n", "]", ":", "\n", "        ", "eval_kwargs", ".", "pop", "(", "key", ",", "None", ")", "\n", "", "eval_kwargs", ".", "update", "(", "dict", "(", "metrics", "=", "args", ".", "eval", ",", "**", "kwargs", ")", ")", "\n", "print", "(", "dataset", ".", "evaluate", "(", "outputs", ",", "**", "eval_kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.check_videos.RandomSampleFrames.__call__": [[72, 94], ["numpy.array", "numpy.concatenate", "numpy.random.randint"], "methods", ["None"], ["    ", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Select frames to verify.\n\n        Select the first, last and three random frames, Required key is\n        \"total_frames\", added or modified key is \"frame_inds\".\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "assert", "results", "[", "'total_frames'", "]", ">", "0", "\n", "\n", "# first and last frames", "\n", "results", "[", "'frame_inds'", "]", "=", "np", ".", "array", "(", "[", "0", ",", "results", "[", "'total_frames'", "]", "-", "1", "]", ")", "\n", "\n", "# choose 3 random frames", "\n", "if", "results", "[", "'total_frames'", "]", ">", "2", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "concatenate", "(", "[", "\n", "results", "[", "'frame_inds'", "]", ",", "\n", "np", ".", "random", ".", "randint", "(", "1", ",", "results", "[", "'total_frames'", "]", "-", "1", ",", "3", ")", "\n", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.check_videos.parse_args": [[15, 67], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "ValueError", "warnings.warn", "multiprocessing.cpu_count"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'MMAction2 check datasets'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'test config file path'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'custom options for evaluation, the key-value pair in xxx=yyy '", "\n", "'format will be kwargs for dataset.evaluate() function (deprecate), '", "\n", "'change to --eval-options instead.'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cfg-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'override some settings in the used config, the key-value pair '", "\n", "'in xxx=yyy format will be merged into config file. For example, '", "\n", "\"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--output-file'", ",", "\n", "default", "=", "'invalid-video.txt'", ",", "\n", "help", "=", "'Output file path which keeps corrupted/missing video file paths'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--split'", ",", "\n", "default", "=", "'train'", ",", "\n", "choices", "=", "[", "'train'", ",", "'val'", ",", "'test'", "]", ",", "\n", "help", "=", "'Dataset split'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--decoder'", ",", "\n", "default", "=", "'decord'", ",", "\n", "choices", "=", "[", "'decord'", ",", "'opencv'", ",", "'pyav'", "]", ",", "\n", "help", "=", "'Video decoder type, should be one of [decord, opencv, pyav]'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--num-processes'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "(", "cpu_count", "(", ")", "-", "1", "or", "1", ")", ",", "\n", "help", "=", "'Number of processes to check videos'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--remove-corrupted-videos'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to delete all corrupted videos'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "options", "and", "args", ".", "eval_options", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "'--options and --eval-options cannot be both '", "\n", "'specified, --options is deprecated in favor of --eval-options'", ")", "\n", "", "if", "args", ".", "options", ":", "\n", "        ", "warnings", ".", "warn", "(", "'--options is deprecated in favor of --eval-options'", ")", "\n", "args", ".", "eval_options", "=", "args", ".", "options", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.check_videos._do_check_videos": [[96, 105], ["lock.acquire", "lock.release", "open", "f.write"], "function", ["None"], ["", "", "def", "_do_check_videos", "(", "lock", ",", "dataset", ",", "output_file", ",", "idx", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "dataset", "[", "idx", "]", "\n", "", "except", ":", "# noqa", "\n", "# save invalid video path to output file", "\n", "        ", "lock", ".", "acquire", "(", ")", "\n", "with", "open", "(", "output_file", ",", "'a'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "dataset", ".", "video_infos", "[", "idx", "]", "[", "'filename'", "]", "+", "'\\n'", ")", "\n", "", "lock", ".", "release", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.report_map.cuhk17_top1": [[14, 40], ["mmcv.load", "results.items", "mmcv.dump", "os.exists", "os.system", "os.system", "mmcv.load", "preds.sort", "report_map.cuhk17_top1.get_topk"], "function", ["None"], ["def", "cuhk17_top1", "(", ")", ":", "\n", "    ", "\"\"\"Assign label for each proposal with the cuhk17 result, which is the #2\n    entry in http://activity-net.org/challenges/2017/evaluation.html.\"\"\"", "\n", "if", "not", "osp", ".", "exists", "(", "'cuhk_anet17_pred.json'", ")", ":", "\n", "        ", "os", ".", "system", "(", "'wget https://download.openmmlab.com/'", "\n", "'mmaction/localization/cuhk_anet17_pred.json'", ")", "\n", "", "proposal", "=", "mmcv", ".", "load", "(", "args", ".", "proposal", ")", "\n", "results", "=", "proposal", "[", "'results'", "]", "\n", "cuhk_pred", "=", "mmcv", ".", "load", "(", "'cuhk_anet17_pred.json'", ")", "[", "'results'", "]", "\n", "\n", "def", "get_topk", "(", "preds", ",", "k", ")", ":", "\n", "        ", "preds", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "'score'", "]", ")", "\n", "return", "preds", "[", "-", "k", ":", "]", "\n", "\n", "", "for", "k", ",", "v", "in", "results", ".", "items", "(", ")", ":", "\n", "        ", "action_pred", "=", "cuhk_pred", "[", "k", "]", "\n", "top1", "=", "get_topk", "(", "action_pred", ",", "1", ")", "\n", "top1_label", "=", "top1", "[", "0", "]", "[", "'label'", "]", "\n", "new_value", "=", "[", "]", "\n", "for", "item", "in", "v", ":", "\n", "            ", "x", "=", "dict", "(", "label", "=", "top1_label", ")", "\n", "x", ".", "update", "(", "item", ")", "\n", "new_value", ".", "append", "(", "x", ")", "\n", "", "results", "[", "k", "]", "=", "new_value", "\n", "", "proposal", "[", "'results'", "]", "=", "results", "\n", "mmcv", ".", "dump", "(", "proposal", ",", "args", ".", "det_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.report_map.parse_args": [[45, 69], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Report detection mAP for'", "\n", "'ActivityNet proposal file'", ")", "\n", "parser", ".", "add_argument", "(", "'--proposal'", ",", "type", "=", "str", ",", "help", "=", "'proposal file'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--gt'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'data/ActivityNet/'", "\n", "'anet_anno_val.json'", ",", "\n", "help", "=", "'groundtruth file'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cls'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'cuhk17_top1'", ",", "\n", "choices", "=", "[", "'cuhk17_top1'", "]", ",", "\n", "help", "=", "'the way to assign label for each '", "\n", "'proposal'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--det-output'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'det_result.json'", ",", "\n", "help", "=", "'the path to store detection results'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.analysis.report_map.main": [[71, 83], ["report_map.parse_args", "func", "mmaction.core.ActivityNetLocalization", "mmaction.core.ActivityNetLocalization.evaluate", "print", "numpy.linspace"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "global", "args", ",", "cls_funcs", "\n", "args", "=", "parse_args", "(", ")", "\n", "func", "=", "cls_funcs", "[", "args", ".", "cls", "]", "\n", "func", "(", ")", "\n", "anet_detection", "=", "ActivityNetLocalization", "(", "\n", "args", ".", "gt", ",", "\n", "args", ".", "det_output", ",", "\n", "tiou_thresholds", "=", "np", ".", "linspace", "(", "0.5", ",", "0.95", ",", "10", ")", ",", "\n", "verbose", "=", "True", ")", "\n", "mAP", ",", "average_mAP", "=", "anet_detection", ".", "evaluate", "(", ")", "\n", "print", "(", "'[RESULTS] Performance on ActivityNet detection task.\\n'", "\n", "f'mAP: {mAP}\\nAverage-mAP: {average_mAP}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.flow_to_img": [[10, 26], ["numpy.clip", "flow.astype.astype", "float"], "function", ["None"], ["def", "flow_to_img", "(", "raw_flow", ",", "bound", "=", "20.", ")", ":", "\n", "    ", "\"\"\"Convert flow to gray image.\n\n    Args:\n        raw_flow (np.ndarray[float]): Estimated flow with the shape (w, h).\n        bound (float): Bound for the flow-to-image normalization. Default: 20.\n\n    Returns:\n        np.ndarray[uint8]: The result list of np.ndarray[uint8], with shape\n                        (w, h).\n    \"\"\"", "\n", "flow", "=", "np", ".", "clip", "(", "raw_flow", ",", "-", "bound", ",", "bound", ")", "\n", "flow", "+=", "bound", "\n", "flow", "*=", "(", "255", "/", "float", "(", "2", "*", "bound", ")", ")", "\n", "flow", "=", "flow", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "return", "flow", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.generate_flow": [[28, 60], ["cv2.cvtColor", "cv2.optflow.DualTVL1OpticalFlow_create", "flow_extraction.generate_flow.op"], "function", ["None"], ["", "def", "generate_flow", "(", "frames", ",", "method", "=", "'tvl1'", ")", ":", "\n", "    ", "\"\"\"Estimate flow with given frames.\n\n    Args:\n        frames (list[np.ndarray[uint8]]): List of rgb frames, with shape\n                                        (w, h, 3).\n        method (str): Use which method to generate flow. Options are 'tvl1'\n                    and 'farneback'. Default: 'tvl1'.\n\n    Returns:\n        list[np.ndarray[float]]: The result list of np.ndarray[float], with\n                                shape (w, h, 2).\n    \"\"\"", "\n", "assert", "method", "in", "[", "'tvl1'", ",", "'farneback'", "]", "\n", "gray_frames", "=", "[", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", "for", "frame", "in", "frames", "]", "\n", "\n", "if", "method", "==", "'tvl1'", ":", "\n", "        ", "tvl1", "=", "cv2", ".", "optflow", ".", "DualTVL1OpticalFlow_create", "(", ")", "\n", "\n", "def", "op", "(", "x", ",", "y", ")", ":", "\n", "            ", "return", "tvl1", ".", "calc", "(", "x", ",", "y", ",", "None", ")", "\n", "", "", "elif", "method", "==", "'farneback'", ":", "\n", "\n", "        ", "def", "op", "(", "x", ",", "y", ")", ":", "\n", "            ", "return", "cv2", ".", "calcOpticalFlowFarneback", "(", "x", ",", "y", ",", "None", ",", "0.5", ",", "3", ",", "15", ",", "3", ",", "5", ",", "\n", "1.2", ",", "0", ")", "\n", "\n", "", "", "gray_st", "=", "gray_frames", "[", ":", "-", "1", "]", "\n", "gray_ed", "=", "gray_frames", "[", "1", ":", "]", "\n", "\n", "flow", "=", "[", "op", "(", "x", ",", "y", ")", "for", "x", ",", "y", "in", "zip", "(", "gray_st", ",", "gray_ed", ")", "]", "\n", "return", "flow", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.extract_dense_flow": [[62, 124], ["os.exists", "cv2.VideoCapture", "cv2.VideoCapture.read", "flow_extraction.generate_flow", "len", "range", "frames.append", "cv2.VideoCapture.read", "flow_extraction.flow_to_img", "flow_extraction.flow_to_img", "os.exists", "os.system", "os.system", "os.join", "os.join", "cv2.imwrite", "cv2.imwrite", "zip", "flow_tmpl.format", "range", "flow_tmpl.format", "range", "os.join", "cv2.imwrite", "len", "len", "rgb_tmpl.format", "range", "len"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.generate_flow", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.flow_to_img", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.flow_to_img", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format"], ["", "def", "extract_dense_flow", "(", "path", ",", "\n", "dest", ",", "\n", "bound", "=", "20.", ",", "\n", "save_rgb", "=", "False", ",", "\n", "start_idx", "=", "0", ",", "\n", "rgb_tmpl", "=", "'img_{:05d}.jpg'", ",", "\n", "flow_tmpl", "=", "'{}_{:05d}.jpg'", ",", "\n", "method", "=", "'tvl1'", ")", ":", "\n", "    ", "\"\"\"Extract dense flow given video or frames, save them as gray-scale\n    images.\n\n    Args:\n        path (str): Location of the input video.\n        dest (str): The directory to store the extracted flow images.\n        bound (float): Bound for the flow-to-image normalization. Default: 20.\n        save_rgb (bool): Save extracted RGB frames. Default: False.\n        start_idx (int): The starting frame index if use frames as input, the\n            first image is path.format(start_idx). Default: 0.\n        rgb_tmpl (str): The template of RGB frame names, Default:\n            'img_{:05d}.jpg'.\n        flow_tmpl (str): The template of Flow frame names, Default:\n            '{}_{:05d}.jpg'.\n        method (str): Use which method to generate flow. Options are 'tvl1'\n            and 'farneback'. Default: 'tvl1'.\n    \"\"\"", "\n", "\n", "frames", "=", "[", "]", "\n", "assert", "osp", ".", "exists", "(", "path", ")", "\n", "video", "=", "cv2", ".", "VideoCapture", "(", "path", ")", "\n", "flag", ",", "f", "=", "video", ".", "read", "(", ")", "\n", "while", "flag", ":", "\n", "        ", "frames", ".", "append", "(", "f", ")", "\n", "flag", ",", "f", "=", "video", ".", "read", "(", ")", "\n", "\n", "", "flow", "=", "generate_flow", "(", "frames", ",", "method", "=", "method", ")", "\n", "\n", "flow_x", "=", "[", "flow_to_img", "(", "x", "[", ":", ",", ":", ",", "0", "]", ",", "bound", ")", "for", "x", "in", "flow", "]", "\n", "flow_y", "=", "[", "flow_to_img", "(", "x", "[", ":", ",", ":", ",", "1", "]", ",", "bound", ")", "for", "x", "in", "flow", "]", "\n", "\n", "if", "not", "osp", ".", "exists", "(", "dest", ")", ":", "\n", "        ", "os", ".", "system", "(", "'mkdir -p '", "+", "dest", ")", "\n", "", "flow_x_names", "=", "[", "\n", "osp", ".", "join", "(", "dest", ",", "flow_tmpl", ".", "format", "(", "'x'", ",", "ind", "+", "start_idx", ")", ")", "\n", "for", "ind", "in", "range", "(", "len", "(", "flow_x", ")", ")", "\n", "]", "\n", "flow_y_names", "=", "[", "\n", "osp", ".", "join", "(", "dest", ",", "flow_tmpl", ".", "format", "(", "'y'", ",", "ind", "+", "start_idx", ")", ")", "\n", "for", "ind", "in", "range", "(", "len", "(", "flow_y", ")", ")", "\n", "]", "\n", "\n", "num_frames", "=", "len", "(", "flow", ")", "\n", "for", "i", "in", "range", "(", "num_frames", ")", ":", "\n", "        ", "cv2", ".", "imwrite", "(", "flow_x_names", "[", "i", "]", ",", "flow_x", "[", "i", "]", ")", "\n", "cv2", ".", "imwrite", "(", "flow_y_names", "[", "i", "]", ",", "flow_y", "[", "i", "]", ")", "\n", "\n", "", "if", "save_rgb", ":", "\n", "        ", "img_names", "=", "[", "\n", "osp", ".", "join", "(", "dest", ",", "rgb_tmpl", ".", "format", "(", "ind", "+", "start_idx", ")", ")", "\n", "for", "ind", "in", "range", "(", "len", "(", "frames", ")", ")", "\n", "]", "\n", "for", "frame", ",", "name", "in", "zip", "(", "frames", ",", "img_names", ")", ":", "\n", "            ", "cv2", ".", "imwrite", "(", "name", ",", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.flow_extraction.parse_args": [[126, 171], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Extract flow and RGB images'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--input'", ",", "\n", "help", "=", "'videos for frame extraction, can be'", "\n", "'single video or a video list, the video list should be a txt file '", "\n", "'and just consists of filenames without directories'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--prefix'", ",", "\n", "default", "=", "''", ",", "\n", "help", "=", "'the prefix of input '", "\n", "'videos, used when input is a video list'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--dest'", ",", "\n", "default", "=", "''", ",", "\n", "help", "=", "'the destination to save '", "\n", "'extracted frames'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--save-rgb'", ",", "action", "=", "'store_true'", ",", "help", "=", "'also save '", "\n", "'rgb frames'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--rgb-tmpl'", ",", "\n", "default", "=", "'img_{:05d}.jpg'", ",", "\n", "help", "=", "'template filename of rgb frames'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--flow-tmpl'", ",", "\n", "default", "=", "'{}_{:05d}.jpg'", ",", "\n", "help", "=", "'template filename of flow frames'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--start-idx'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "'the start '", "\n", "'index of extracted frames'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--method'", ",", "\n", "default", "=", "'tvl1'", ",", "\n", "help", "=", "'use which method to '", "\n", "'generate flow'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--bound'", ",", "type", "=", "float", ",", "default", "=", "20", ",", "help", "=", "'maximum of '", "\n", "'optical flow'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.load_video_infos": [[14, 30], ["mmcv.load", "video_infos.append"], "function", ["None"], ["def", "load_video_infos", "(", "ann_file", ")", ":", "\n", "    ", "\"\"\"Load the video annotations.\n\n    Args:\n        ann_file (str): A json file path of the annotation file.\n\n    Returns:\n        list[dict]: A list containing annotations for videos.\n    \"\"\"", "\n", "video_infos", "=", "[", "]", "\n", "anno_database", "=", "mmcv", ".", "load", "(", "ann_file", ")", "\n", "for", "video_name", "in", "anno_database", ":", "\n", "        ", "video_info", "=", "anno_database", "[", "video_name", "]", "\n", "video_info", "[", "'video_name'", "]", "=", "video_name", "\n", "video_infos", ".", "append", "(", "video_info", ")", "\n", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_proposals": [[32, 95], ["bsn_proposal_generation.load_video_infos", "len", "torch.Manager", "mp.Manager.dict", "range", "range", "torch.Process", "mp.Process.start", "processes.append", "os.makedirs", "os.makedirs", "mmcv.ProgressBar", "range", "torch.Process", "mp.Process.start", "processes.append", "mp.Process.join", "os.join", "numpy.savetxt", "mmcv.ProgressBar.update"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.load_video_infos", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "generate_proposals", "(", "ann_file", ",", "tem_results_dir", ",", "pgm_proposals_dir", ",", "\n", "pgm_proposals_thread", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Generate proposals using multi-process.\n\n    Args:\n        ann_file (str): A json file path of the annotation file for\n            all videos to be processed.\n        tem_results_dir (str): Directory to read tem results\n        pgm_proposals_dir (str): Directory to save generated proposals.\n        pgm_proposals_thread (int): Total number of threads.\n        kwargs (dict): Keyword arguments for \"generate_candidate_proposals\".\n    \"\"\"", "\n", "video_infos", "=", "load_video_infos", "(", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "num_videos_per_thread", "=", "num_videos", "//", "pgm_proposals_thread", "\n", "processes", "=", "[", "]", "\n", "manager", "=", "mp", ".", "Manager", "(", ")", "\n", "result_dict", "=", "manager", ".", "dict", "(", ")", "\n", "kwargs", "[", "'result_dict'", "]", "=", "result_dict", "\n", "for", "tid", "in", "range", "(", "pgm_proposals_thread", "-", "1", ")", ":", "\n", "        ", "tmp_video_list", "=", "range", "(", "tid", "*", "num_videos_per_thread", ",", "\n", "(", "tid", "+", "1", ")", "*", "num_videos_per_thread", ")", "\n", "p", "=", "mp", ".", "Process", "(", "\n", "target", "=", "generate_candidate_proposals", ",", "\n", "args", "=", "(", "\n", "tmp_video_list", ",", "\n", "video_infos", ",", "\n", "tem_results_dir", ",", "\n", ")", ",", "\n", "kwargs", "=", "kwargs", ")", "\n", "p", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "\n", "", "tmp_video_list", "=", "range", "(", "(", "pgm_proposals_thread", "-", "1", ")", "*", "num_videos_per_thread", ",", "\n", "num_videos", ")", "\n", "p", "=", "mp", ".", "Process", "(", "\n", "target", "=", "generate_candidate_proposals", ",", "\n", "args", "=", "(", "\n", "tmp_video_list", ",", "\n", "video_infos", ",", "\n", "tem_results_dir", ",", "\n", ")", ",", "\n", "kwargs", "=", "kwargs", ")", "\n", "p", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "\n", "for", "p", "in", "processes", ":", "\n", "        ", "p", ".", "join", "(", ")", "\n", "\n", "# save results", "\n", "", "os", ".", "makedirs", "(", "pgm_proposals_dir", ",", "exist_ok", "=", "True", ")", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "num_videos", ")", "\n", "header", "=", "'tmin,tmax,tmin_score,tmax_score,score,match_iou,match_ioa'", "\n", "for", "video_name", "in", "result_dict", ":", "\n", "        ", "proposals", "=", "result_dict", "[", "video_name", "]", "\n", "proposal_path", "=", "osp", ".", "join", "(", "pgm_proposals_dir", ",", "video_name", "+", "'.csv'", ")", "\n", "np", ".", "savetxt", "(", "\n", "proposal_path", ",", "\n", "proposals", ",", "\n", "header", "=", "header", ",", "\n", "delimiter", "=", "','", ",", "\n", "comments", "=", "''", ")", "\n", "prog_bar", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_features": [[97, 156], ["bsn_proposal_generation.load_video_infos", "len", "torch.Manager", "mp.Manager.dict", "range", "range", "torch.Process", "mp.Process.start", "processes.append", "os.makedirs", "os.makedirs", "mmcv.ProgressBar", "manager.dict.keys", "range", "torch.Process", "mp.Process.start", "processes.append", "mp.Process.join", "os.join", "numpy.save", "mmcv.ProgressBar.update"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.load_video_infos", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "", "def", "generate_features", "(", "ann_file", ",", "tem_results_dir", ",", "pgm_proposals_dir", ",", "\n", "pgm_features_dir", ",", "pgm_features_thread", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Generate proposals features using multi-process.\n\n    Args:\n        ann_file (str): A json file path of the annotation file for\n            all videos to be processed.\n        tem_results_dir (str): Directory to read tem results.\n        pgm_proposals_dir (str): Directory to read generated proposals.\n        pgm_features_dir (str): Directory to save generated features.\n        pgm_features_thread (int): Total number of threads.\n        kwargs (dict): Keyword arguments for \"generate_bsp_feature\".\n    \"\"\"", "\n", "video_infos", "=", "load_video_infos", "(", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "num_videos_per_thread", "=", "num_videos", "//", "pgm_features_thread", "\n", "processes", "=", "[", "]", "\n", "manager", "=", "mp", ".", "Manager", "(", ")", "\n", "feature_return_dict", "=", "manager", ".", "dict", "(", ")", "\n", "kwargs", "[", "'result_dict'", "]", "=", "feature_return_dict", "\n", "for", "tid", "in", "range", "(", "pgm_features_thread", "-", "1", ")", ":", "\n", "        ", "tmp_video_list", "=", "range", "(", "tid", "*", "num_videos_per_thread", ",", "\n", "(", "tid", "+", "1", ")", "*", "num_videos_per_thread", ")", "\n", "p", "=", "mp", ".", "Process", "(", "\n", "target", "=", "generate_bsp_feature", ",", "\n", "args", "=", "(", "\n", "tmp_video_list", ",", "\n", "video_infos", ",", "\n", "tem_results_dir", ",", "\n", "pgm_proposals_dir", ",", "\n", ")", ",", "\n", "kwargs", "=", "kwargs", ")", "\n", "p", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "", "tmp_video_list", "=", "range", "(", "(", "pgm_features_thread", "-", "1", ")", "*", "num_videos_per_thread", ",", "\n", "num_videos", ")", "\n", "p", "=", "mp", ".", "Process", "(", "\n", "target", "=", "generate_bsp_feature", ",", "\n", "args", "=", "(", "\n", "tmp_video_list", ",", "\n", "video_infos", ",", "\n", "tem_results_dir", ",", "\n", "pgm_proposals_dir", ",", "\n", ")", ",", "\n", "kwargs", "=", "kwargs", ")", "\n", "p", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "p", ")", "\n", "\n", "for", "p", "in", "processes", ":", "\n", "        ", "p", ".", "join", "(", ")", "\n", "\n", "# save results", "\n", "", "os", ".", "makedirs", "(", "pgm_features_dir", ",", "exist_ok", "=", "True", ")", "\n", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "num_videos", ")", "\n", "for", "video_name", "in", "feature_return_dict", ".", "keys", "(", ")", ":", "\n", "        ", "bsp_feature", "=", "feature_return_dict", "[", "video_name", "]", "\n", "feature_path", "=", "osp", ".", "join", "(", "pgm_features_dir", ",", "video_name", "+", "'.npy'", ")", "\n", "np", ".", "save", "(", "feature_path", ",", "bsp_feature", ")", "\n", "prog_bar", ".", "update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.parse_args": [[158, 168], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["", "", "def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Proposal generation module'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'test config file path'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--mode'", ",", "\n", "choices", "=", "[", "'train'", ",", "'test'", "]", ",", "\n", "default", "=", "'test'", ",", "\n", "help", "=", "'train or test'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.main": [[170, 195], ["print", "bsn_proposal_generation.parse_args", "mmcv.Config.fromfile", "print", "bsn_proposal_generation.generate_proposals", "print", "bsn_proposal_generation.generate_features", "print", "bsn_proposal_generation.generate_proposals", "print", "bsn_proposal_generation.generate_features", "print"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_proposals", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_features", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_proposals", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.bsn_proposal_generation.generate_features"], ["", "def", "main", "(", ")", ":", "\n", "    ", "print", "(", "'Begin Proposal Generation Module'", ")", "\n", "args", "=", "parse_args", "(", ")", "\n", "cfg", "=", "mmcv", ".", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "tem_results_dir", "=", "cfg", ".", "tem_results_dir", "\n", "pgm_proposals_dir", "=", "cfg", ".", "pgm_proposals_dir", "\n", "pgm_features_dir", "=", "cfg", ".", "pgm_features_dir", "\n", "if", "args", ".", "mode", "==", "'test'", ":", "\n", "        ", "generate_proposals", "(", "cfg", ".", "ann_file_val", ",", "tem_results_dir", ",", "\n", "pgm_proposals_dir", ",", "**", "cfg", ".", "pgm_proposals_cfg", ")", "\n", "print", "(", "'\\nFinish proposal generation'", ")", "\n", "generate_features", "(", "cfg", ".", "ann_file_val", ",", "tem_results_dir", ",", "pgm_proposals_dir", ",", "\n", "pgm_features_dir", ",", "**", "cfg", ".", "pgm_features_test_cfg", ")", "\n", "print", "(", "'\\nFinish feature generation'", ")", "\n", "\n", "", "elif", "args", ".", "mode", "==", "'train'", ":", "\n", "        ", "generate_proposals", "(", "cfg", ".", "ann_file_train", ",", "tem_results_dir", ",", "\n", "pgm_proposals_dir", ",", "**", "cfg", ".", "pgm_proposals_cfg", ")", "\n", "print", "(", "'\\nFinish proposal generation'", ")", "\n", "generate_features", "(", "cfg", ".", "ann_file_train", ",", "tem_results_dir", ",", "\n", "pgm_proposals_dir", ",", "pgm_features_dir", ",", "\n", "**", "cfg", ".", "pgm_features_train_cfg", ")", "\n", "print", "(", "'\\nFinish feature generation'", ")", "\n", "\n", "", "print", "(", "'Finish Proposal Generation Module'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args": [[25, 68], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "str"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'MMAction2 clip-level feature extraction'", ")", "\n", "parser", ".", "add_argument", "(", "'config'", ",", "help", "=", "'test config file path'", ")", "\n", "parser", ".", "add_argument", "(", "'checkpoint'", ",", "help", "=", "'checkpoint file'", ")", "\n", "parser", ".", "add_argument", "(", "'--video-list'", ",", "help", "=", "'video file list'", ")", "\n", "parser", ".", "add_argument", "(", "'--video-root'", ",", "help", "=", "'video root directory'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--out'", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "'output result file in pkl/yaml/json format'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--fuse-conv-bn'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to fuse conv and bn, this will slightly increase'", "\n", "'the inference speed'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--gpu-collect'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'whether to use gpu to collect results'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--tmpdir'", ",", "\n", "help", "=", "'tmp directory used for collecting results from multiple '", "\n", "'workers, available when gpu-collect is not specified'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cfg-options'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "action", "=", "DictAction", ",", "\n", "default", "=", "{", "}", ",", "\n", "help", "=", "'override some settings in the used config, the key-value pair '", "\n", "'in xxx=yyy format will be merged into config file. For example, '", "\n", "\"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--launcher'", ",", "\n", "choices", "=", "[", "'none'", ",", "'pytorch'", ",", "'slurm'", ",", "'mpi'", "]", ",", "\n", "default", "=", "'none'", ",", "\n", "help", "=", "'job launcher'", ")", "\n", "parser", ".", "add_argument", "(", "'--local_rank'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "'LOCAL_RANK'", "not", "in", "os", ".", "environ", ":", "\n", "        ", "os", ".", "environ", "[", "'LOCAL_RANK'", "]", "=", "str", "(", "args", ".", "local_rank", ")", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.turn_off_pretrained": [[70, 80], ["cfg.values", "isinstance", "clip_feature_extraction.turn_off_pretrained"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.turn_off_pretrained"], ["", "def", "turn_off_pretrained", "(", "cfg", ")", ":", "\n", "# recursively find all pretrained in the model config,", "\n", "# and set them None to avoid redundant pretrain steps for testing", "\n", "    ", "if", "'pretrained'", "in", "cfg", ":", "\n", "        ", "cfg", ".", "pretrained", "=", "None", "\n", "\n", "# recursively turn off pretrained value", "\n", "", "for", "sub_cfg", "in", "cfg", ".", "values", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "sub_cfg", ",", "dict", ")", ":", "\n", "            ", "turn_off_pretrained", "(", "sub_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.text2tensor": [[82, 88], ["np.array.extend", "numpy.array", "torch.from_numpy", "torch.from_numpy", "ord", "len", "len"], "function", ["None"], ["", "", "", "def", "text2tensor", "(", "text", ",", "size", "=", "256", ")", ":", "\n", "    ", "nums", "=", "[", "ord", "(", "x", ")", "for", "x", "in", "text", "]", "\n", "assert", "len", "(", "nums", ")", "<", "size", "\n", "nums", ".", "extend", "(", "[", "0", "]", "*", "(", "size", "-", "len", "(", "nums", ")", ")", ")", "\n", "nums", "=", "np", ".", "array", "(", "nums", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "return", "torch", ".", "from_numpy", "(", "nums", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.tensor2text": [[90, 94], ["chr"], "function", ["None"], ["", "def", "tensor2text", "(", "tensor", ")", ":", "\n", "# 0 may not occur in a string", "\n", "    ", "chars", "=", "[", "chr", "(", "x", ")", "for", "x", "in", "tensor", "if", "x", "!=", "0", "]", "\n", "return", "''", ".", "join", "(", "chars", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.inference_pytorch": [[96, 128], ["clip_feature_extraction.turn_off_pretrained", "mmaction.models.build_model", "cfg.get", "mmcv.runner.load_checkpoint", "len", "mmaction.utils.register_module_hooks", "mmcv.runner.fp16_utils.wrap_fp16_model", "mmcv.cnn.fuse_conv_bn", "mmcv.parallel.MMDataParallel", "mmaction.apis.single_gpu_test", "mmcv.parallel.MMDistributedDataParallel", "mmaction.apis.multi_gpu_test", "cfg.get", "mmcv.parallel.MMDistributedDataParallel.cuda", "torch.cuda.current_device", "torch.cuda.current_device"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.turn_off_pretrained", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.module_hooks.register_module_hooks", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "inference_pytorch", "(", "args", ",", "cfg", ",", "distributed", ",", "data_loader", ")", ":", "\n", "    ", "\"\"\"Get predictions by pytorch models.\"\"\"", "\n", "# remove redundant pretrain steps for testing", "\n", "turn_off_pretrained", "(", "cfg", ".", "model", ")", "\n", "\n", "# build the model and load checkpoint", "\n", "model", "=", "build_model", "(", "\n", "cfg", ".", "model", ",", "train_cfg", "=", "None", ",", "test_cfg", "=", "cfg", ".", "get", "(", "'test_cfg'", ")", ")", "\n", "\n", "if", "len", "(", "cfg", ".", "module_hooks", ")", ">", "0", ":", "\n", "        ", "register_module_hooks", "(", "model", ",", "cfg", ".", "module_hooks", ")", "\n", "\n", "", "fp16_cfg", "=", "cfg", ".", "get", "(", "'fp16'", ",", "None", ")", "\n", "if", "fp16_cfg", "is", "not", "None", ":", "\n", "        ", "wrap_fp16_model", "(", "model", ")", "\n", "", "load_checkpoint", "(", "model", ",", "args", ".", "checkpoint", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "if", "args", ".", "fuse_conv_bn", ":", "\n", "        ", "model", "=", "fuse_conv_bn", "(", "model", ")", "\n", "\n", "", "if", "not", "distributed", ":", "\n", "        ", "model", "=", "MMDataParallel", "(", "model", ",", "device_ids", "=", "[", "0", "]", ")", "\n", "outputs", "=", "single_gpu_test", "(", "model", ",", "data_loader", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "MMDistributedDataParallel", "(", "\n", "model", ".", "cuda", "(", ")", ",", "\n", "device_ids", "=", "[", "torch", ".", "cuda", ".", "current_device", "(", ")", "]", ",", "\n", "broadcast_buffers", "=", "False", ")", "\n", "outputs", "=", "multi_gpu_test", "(", "model", ",", "data_loader", ",", "args", ".", "tmpdir", ",", "\n", "args", ".", "gpu_collect", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.main": [[130, 226], ["clip_feature_extraction.parse_args", "mmcv.Config.fromfile", "Config.fromfile.merge_from_dict", "Config.fromfile.get", "Config._merge_a_into_b.get", "Config.fromfile.get", "mmcv.runner.get_dist_info", "torch.zeros().cuda", "torch.zeros().cuda", "clip_feature_extraction.tensor2text", "Config.fromfile.setdefault", "mmaction.datasets.build_dataset", "dict", "dict", "mmaction.datasets.build_dataloader", "clip_feature_extraction.inference_pytorch", "dict", "mmcv.Config._merge_a_into_b", "mmcv.runner.init_dist", "open().readlines", "datetime.datetime.now().strftime", "text2tensor().cuda", "torch.broadcast", "dict", "Config._merge_a_into_b.get", "os.remove", "os.remove", "dict", "warnings.warn", "mmcv.mkdir_or_exist", "os.splitext", "torch.zeros", "torch.zeros", "x.strip", "open", "fout.write", "text2tensor().cuda.cuda", "Config.fromfile.data.get", "Config.fromfile.data.get", "Config.fromfile.data.get", "print", "mmaction.datasets.build_dataset.dump_results", "os.dirname", "open", "datetime.datetime.now", "clip_feature_extraction.text2tensor"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.parse_args", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.tensor2text", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.inference_pytorch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.dump_results", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.misc.clip_feature_extraction.text2tensor"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "cfg", "=", "Config", ".", "fromfile", "(", "args", ".", "config", ")", "\n", "\n", "cfg", ".", "merge_from_dict", "(", "args", ".", "cfg_options", ")", "\n", "\n", "if", "cfg", ".", "model", "[", "'test_cfg'", "]", "is", "None", ":", "\n", "        ", "cfg", ".", "model", "[", "'test_cfg'", "]", "=", "dict", "(", "feature_extraction", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "cfg", ".", "model", "[", "'test_cfg'", "]", "[", "'feature_extraction'", "]", "=", "True", "\n", "\n", "# Load output_config from cfg", "\n", "", "output_config", "=", "cfg", ".", "get", "(", "'output_config'", ",", "{", "}", ")", "\n", "if", "args", ".", "out", ":", "\n", "# Overwrite output_config from args.out", "\n", "        ", "output_config", "=", "Config", ".", "_merge_a_into_b", "(", "\n", "dict", "(", "out", "=", "args", ".", "out", ")", ",", "output_config", ")", "\n", "\n", "", "assert", "output_config", ",", "'Please specify output filename with --out.'", "\n", "\n", "dataset_type", "=", "cfg", ".", "data", ".", "test", ".", "type", "\n", "if", "output_config", ".", "get", "(", "'out'", ",", "None", ")", ":", "\n", "        ", "if", "'output_format'", "in", "output_config", ":", "\n", "# ugly workround to make recognition and localization the same", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Skip checking `output_format` in localization task.'", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "output_config", "[", "'out'", "]", "\n", "# make sure the dirname of the output path exists", "\n", "mmcv", ".", "mkdir_or_exist", "(", "osp", ".", "dirname", "(", "out", ")", ")", "\n", "_", ",", "suffix", "=", "osp", ".", "splitext", "(", "out", ")", "\n", "assert", "dataset_type", "==", "'VideoDataset'", "\n", "\n", "assert", "suffix", "[", "1", ":", "]", "in", "file_handlers", ",", "(", "\n", "'The format of the output '", "\n", "'file should be json, pickle or yaml'", ")", "\n", "\n", "# set cudnn benchmark", "\n", "", "", "if", "cfg", ".", "get", "(", "'cudnn_benchmark'", ",", "False", ")", ":", "\n", "        ", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "", "cfg", ".", "data", ".", "test", ".", "test_mode", "=", "True", "\n", "cfg", ".", "data", ".", "test", ".", "data_prefix", "=", "args", ".", "video_root", "\n", "\n", "# init distributed env first, since logger depends on the dist info.", "\n", "if", "args", ".", "launcher", "==", "'none'", ":", "\n", "        ", "distributed", "=", "False", "\n", "", "else", ":", "\n", "        ", "distributed", "=", "True", "\n", "init_dist", "(", "args", ".", "launcher", ",", "**", "cfg", ".", "dist_params", ")", "\n", "\n", "", "rank", ",", "_", "=", "get_dist_info", "(", ")", "\n", "\n", "size", "=", "256", "\n", "fname_tensor", "=", "torch", ".", "zeros", "(", "size", ",", "dtype", "=", "torch", ".", "uint8", ")", ".", "cuda", "(", ")", "\n", "if", "rank", "==", "0", ":", "\n", "        ", "videos", "=", "open", "(", "args", ".", "video_list", ")", ".", "readlines", "(", ")", "\n", "videos", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "videos", "]", "\n", "\n", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "fake_anno", "=", "f'fake_anno_{timestamp}.txt'", "\n", "with", "open", "(", "fake_anno", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "lines", "=", "[", "x", "+", "' 0'", "for", "x", "in", "videos", "]", "\n", "fout", ".", "write", "(", "'\\n'", ".", "join", "(", "lines", ")", ")", "\n", "", "fname_tensor", "=", "text2tensor", "(", "fake_anno", ",", "size", ")", ".", "cuda", "(", ")", "\n", "\n", "", "if", "distributed", ":", "\n", "        ", "dist", ".", "broadcast", "(", "fname_tensor", ".", "cuda", "(", ")", ",", "src", "=", "0", ")", "\n", "\n", "", "fname", "=", "tensor2text", "(", "fname_tensor", ")", "\n", "cfg", ".", "data", ".", "test", ".", "ann_file", "=", "fname", "\n", "\n", "# The flag is used to register module's hooks", "\n", "cfg", ".", "setdefault", "(", "'module_hooks'", ",", "[", "]", ")", "\n", "\n", "# build the dataloader", "\n", "dataset", "=", "build_dataset", "(", "cfg", ".", "data", ".", "test", ",", "dict", "(", "test_mode", "=", "True", ")", ")", "\n", "dataloader_setting", "=", "dict", "(", "\n", "videos_per_gpu", "=", "cfg", ".", "data", ".", "get", "(", "'videos_per_gpu'", ",", "1", ")", ",", "\n", "workers_per_gpu", "=", "cfg", ".", "data", ".", "get", "(", "'workers_per_gpu'", ",", "1", ")", ",", "\n", "dist", "=", "distributed", ",", "\n", "shuffle", "=", "False", ")", "\n", "\n", "dataloader_setting", "=", "dict", "(", "dataloader_setting", ",", "\n", "**", "cfg", ".", "data", ".", "get", "(", "'test_dataloader'", ",", "{", "}", ")", ")", "\n", "data_loader", "=", "build_dataloader", "(", "dataset", ",", "**", "dataloader_setting", ")", "\n", "\n", "outputs", "=", "inference_pytorch", "(", "args", ",", "cfg", ",", "distributed", ",", "data_loader", ")", "\n", "\n", "if", "rank", "==", "0", ":", "\n", "        ", "if", "output_config", ".", "get", "(", "'out'", ",", "None", ")", ":", "\n", "            ", "out", "=", "output_config", "[", "'out'", "]", "\n", "print", "(", "f'\\nwriting results to {out}'", ")", "\n", "dataset", ".", "dump_results", "(", "outputs", ",", "**", "output_config", ")", "\n", "# remove the temporary file", "\n", "", "os", ".", "remove", "(", "fake_anno", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.mmaction.version.parse_version_info": [[6, 16], ["version_str.split", "tuple", "x.isdigit", "version_info.append", "int", "x.find", "x.split", "version_info.append", "version_info.append", "int"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.image_dataset.ImageDataset.__init__": [[44, 46], ["video_dataset.VideoDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "start_index", "=", "None", ",", "**", "kwargs", ")", "\n", "# use `start_index=None` to indicate it is for `ImageDataset`", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_visual_dataset.AudioVisualDataset.__init__": [[31, 36], ["kwargs.pop", "kwargs.get", "rawframe_dataset.RawframeDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "audio_prefix", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "audio_prefix", "=", "audio_prefix", "\n", "self", ".", "video_prefix", "=", "kwargs", ".", "pop", "(", "'video_prefix'", ",", "None", ")", "\n", "self", ".", "data_prefix", "=", "kwargs", ".", "get", "(", "'data_prefix'", ",", "None", ")", "\n", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_visual_dataset.AudioVisualDataset.load_annotations": [[37, 78], ["open", "line.strip().split", "video_infos.append", "os.join", "os.join", "os.join", "int", "int", "int", "int", "len", "line.strip", "len"], "methods", ["None"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "video_info", "=", "{", "}", "\n", "idx", "=", "0", "\n", "# idx for frame_dir", "\n", "frame_dir", "=", "line_split", "[", "idx", "]", "\n", "if", "self", ".", "audio_prefix", "is", "not", "None", ":", "\n", "                    ", "audio_path", "=", "osp", ".", "join", "(", "self", ".", "audio_prefix", ",", "\n", "frame_dir", "+", "'.npy'", ")", "\n", "video_info", "[", "'audio_path'", "]", "=", "audio_path", "\n", "", "if", "self", ".", "video_prefix", ":", "\n", "                    ", "video_path", "=", "osp", ".", "join", "(", "self", ".", "video_prefix", ",", "\n", "frame_dir", "+", "'.mp4'", ")", "\n", "video_info", "[", "'filename'", "]", "=", "video_path", "\n", "", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "frame_dir", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "frame_dir", ")", "\n", "video_info", "[", "'frame_dir'", "]", "=", "frame_dir", "\n", "", "idx", "+=", "1", "\n", "if", "self", ".", "with_offset", ":", "\n", "# idx for offset and total_frames", "\n", "                    ", "video_info", "[", "'offset'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "+", "1", "]", ")", "\n", "idx", "+=", "2", "\n", "", "else", ":", "\n", "# idx for total_frames", "\n", "                    ", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "idx", "+=", "1", "\n", "# idx for label[s]", "\n", "", "label", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line_split", "[", "idx", ":", "]", "]", "\n", "assert", "len", "(", "label", ")", "!=", "0", ",", "f'missing label in line: {line}'", "\n", "if", "self", ".", "multi_class", ":", "\n", "                    ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "video_info", "[", "'label'", "]", "=", "label", "\n", "", "else", ":", "\n", "                    ", "assert", "len", "(", "label", ")", "==", "1", "\n", "video_info", "[", "'label'", "]", "=", "label", "[", "0", "]", "\n", "", "video_infos", ".", "append", "(", "video_info", ")", "\n", "", "", "return", "video_infos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_feature_dataset.AudioFeatureDataset.__init__": [[32, 35], ["base.BaseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "suffix", "=", "'.npy'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "suffix", "=", "suffix", "\n", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "modality", "=", "'Audio'", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_feature_dataset.AudioFeatureDataset.load_annotations": [[36, 72], ["audio_feature_dataset.AudioFeatureDataset.ann_file.endswith", "audio_feature_dataset.AudioFeatureDataset.load_json_annotations", "open", "line.strip().split", "int", "video_infos.append", "int", "torch.zeros", "line.strip", "os.join.endswith", "os.join", "len", "os.join"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "if", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", ":", "\n", "            ", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "video_info", "=", "{", "}", "\n", "idx", "=", "0", "\n", "filename", "=", "line_split", "[", "idx", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "if", "not", "filename", ".", "endswith", "(", "self", ".", "suffix", ")", ":", "\n", "                        ", "filename", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "\n", "filename", ")", "+", "self", ".", "suffix", "\n", "", "else", ":", "\n", "                        ", "filename", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "filename", ")", "\n", "", "", "video_info", "[", "'audio_path'", "]", "=", "filename", "\n", "idx", "+=", "1", "\n", "# idx for total_frames", "\n", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "idx", "+=", "1", "\n", "# idx for label[s]", "\n", "label", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line_split", "[", "idx", ":", "]", "]", "\n", "assert", "label", ",", "f'missing label in line: {line}'", "\n", "if", "self", ".", "multi_class", ":", "\n", "                    ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "label", "]", "=", "1.0", "\n", "video_info", "[", "'label'", "]", "=", "onehot", "\n", "", "else", ":", "\n", "                    ", "assert", "len", "(", "label", ")", "==", "1", "\n", "video_info", "[", "'label'", "]", "=", "label", "[", "0", "]", "\n", "", "video_infos", ".", "append", "(", "video_info", ")", "\n", "\n", "", "", "return", "video_infos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.__init__": [[59, 102], ["torch.utils.data.Dataset.__init__", "pipelines.Compose", "base.BaseDataset.load_annotations", "os.realpath", "base.BaseDataset.parse_by_class", "base.BaseDataset.video_infos_by_class.items", "sum", "dict", "os.isdir", "class_prob.append", "zip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.load_annotations", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.parse_by_class"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "multi_class", "=", "False", ",", "\n", "num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n", "power", "=", "0", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ann_file", "=", "ann_file", "\n", "self", ".", "data_prefix", "=", "osp", ".", "realpath", "(", "\n", "data_prefix", ")", "if", "data_prefix", "is", "not", "None", "and", "osp", ".", "isdir", "(", "\n", "data_prefix", ")", "else", "data_prefix", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "multi_class", "=", "multi_class", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "start_index", "=", "start_index", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "sample_by_class", "=", "sample_by_class", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "dynamic_length", "=", "dynamic_length", "\n", "\n", "assert", "not", "(", "self", ".", "multi_class", "and", "self", ".", "sample_by_class", ")", "\n", "\n", "self", ".", "pipeline", "=", "Compose", "(", "pipeline", ")", "\n", "self", ".", "video_infos", "=", "self", ".", "load_annotations", "(", ")", "\n", "if", "self", ".", "sample_by_class", ":", "\n", "            ", "self", ".", "video_infos_by_class", "=", "self", ".", "parse_by_class", "(", ")", "\n", "\n", "class_prob", "=", "[", "]", "\n", "for", "_", ",", "samples", "in", "self", ".", "video_infos_by_class", ".", "items", "(", ")", ":", "\n", "                ", "class_prob", ".", "append", "(", "len", "(", "samples", ")", "/", "len", "(", "self", ".", "video_infos", ")", ")", "\n", "", "class_prob", "=", "[", "x", "**", "self", ".", "power", "for", "x", "in", "class_prob", "]", "\n", "\n", "summ", "=", "sum", "(", "class_prob", ")", "\n", "class_prob", "=", "[", "x", "/", "summ", "for", "x", "in", "class_prob", "]", "\n", "\n", "self", ".", "class_prob", "=", "dict", "(", "zip", "(", "self", ".", "video_infos_by_class", ",", "class_prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.load_annotations": [[103, 106], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.load_json_annotations": [[109, 125], ["mmcv.load", "len", "range", "os.join", "len"], "methods", ["None"], ["", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "path_key", "=", "'frame_dir'", "if", "'frame_dir'", "in", "video_infos", "[", "0", "]", "else", "'filename'", "\n", "for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n", "if", "self", ".", "multi_class", ":", "\n", "                ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "", "else", ":", "\n", "                ", "assert", "len", "(", "video_infos", "[", "i", "]", "[", "'label'", "]", ")", "==", "1", "\n", "video_infos", "[", "i", "]", "[", "'label'", "]", "=", "video_infos", "[", "i", "]", "[", "'label'", "]", "[", "0", "]", "\n", "", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.parse_by_class": [[126, 132], ["collections.defaultdict", "video_infos_by_class[].append"], "methods", ["None"], ["", "def", "parse_by_class", "(", "self", ")", ":", "\n", "        ", "video_infos_by_class", "=", "defaultdict", "(", "list", ")", "\n", "for", "item", "in", "self", ".", "video_infos", ":", "\n", "            ", "label", "=", "item", "[", "'label'", "]", "\n", "video_infos_by_class", "[", "label", "]", ".", "append", "(", "item", ")", "\n", "", "return", "video_infos_by_class", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.label2array": [[133, 138], ["numpy.zeros"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "label2array", "(", "num", ",", "label", ")", ":", "\n", "        ", "arr", "=", "np", ".", "zeros", "(", "num", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "arr", "[", "label", "]", "=", "1.", "\n", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.evaluate": [[139, 244], ["dict", "copy.deepcopy", "collections.OrderedDict", "warnings.warn", "dict", "isinstance", "TypeError", "len", "len", "isinstance", "mmcv.utils.print_log", "dict", "len", "len", "KeyError", "copy.deepcopy.setdefault().setdefault", "isinstance", "core.top_k_accuracy", "zip", "mmcv.utils.print_log", "core.mean_class_accuracy", "mmcv.utils.print_log", "mmcv.utils.print_log", "isinstance", "TypeError", "log_msg.append", "base.BaseDataset.label2array", "core.mean_average_precision", "type", "copy.deepcopy.setdefault", "core.mmit_mean_average_precision", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.top_k_accuracy", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_class_accuracy", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.label2array", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_average_precision", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mmit_mean_average_precision"], ["", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'top_k_accuracy'", ",", "\n", "metric_options", "=", "dict", "(", "top_k_accuracy", "=", "dict", "(", "topk", "=", "(", "1", ",", "5", ")", ")", ")", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        ", "\"\"\"Perform evaluation for common datasets.\n\n        Args:\n            results (list): Output results.\n            metrics (str | sequence[str]): Metrics to be performed.\n                Defaults: 'top_k_accuracy'.\n            metric_options (dict): Dict for metric options. Options are\n                ``topk`` for ``top_k_accuracy``.\n                Default: ``dict(top_k_accuracy=dict(topk=(1, 5)))``.\n            logger (logging.Logger | None): Logger for recording.\n                Default: None.\n            deprecated_kwargs (dict): Used for containing deprecated arguments.\n                See 'https://github.com/open-mmlab/mmaction2/pull/286'.\n\n        Returns:\n            dict: Evaluation results dict.\n        \"\"\"", "\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n", "\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'top_k_accuracy'", "]", "=", "dict", "(", "\n", "metric_options", "[", "'top_k_accuracy'", "]", ",", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "allowed_metrics", "=", "[", "\n", "'top_k_accuracy'", ",", "'mean_class_accuracy'", ",", "'mean_average_precision'", ",", "\n", "'mmit_mean_average_precision'", "\n", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "allowed_metrics", ":", "\n", "                ", "raise", "KeyError", "(", "f'metric {metric} is not supported'", ")", "\n", "\n", "", "", "eval_results", "=", "OrderedDict", "(", ")", "\n", "gt_labels", "=", "[", "ann", "[", "'label'", "]", "for", "ann", "in", "self", ".", "video_infos", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "msg", "=", "f'Evaluating {metric} ...'", "\n", "if", "logger", "is", "None", ":", "\n", "                ", "msg", "=", "'\\n'", "+", "msg", "\n", "", "print_log", "(", "msg", ",", "logger", "=", "logger", ")", "\n", "\n", "if", "metric", "==", "'top_k_accuracy'", ":", "\n", "                ", "topk", "=", "metric_options", ".", "setdefault", "(", "'top_k_accuracy'", ",", "\n", "{", "}", ")", ".", "setdefault", "(", "\n", "'topk'", ",", "(", "1", ",", "5", ")", ")", "\n", "if", "not", "isinstance", "(", "topk", ",", "(", "int", ",", "tuple", ")", ")", ":", "\n", "                    ", "raise", "TypeError", "(", "'topk must be int or tuple of int, '", "\n", "f'but got {type(topk)}'", ")", "\n", "", "if", "isinstance", "(", "topk", ",", "int", ")", ":", "\n", "                    ", "topk", "=", "(", "topk", ",", ")", "\n", "\n", "", "top_k_acc", "=", "top_k_accuracy", "(", "results", ",", "gt_labels", ",", "topk", ")", "\n", "log_msg", "=", "[", "]", "\n", "for", "k", ",", "acc", "in", "zip", "(", "topk", ",", "top_k_acc", ")", ":", "\n", "                    ", "eval_results", "[", "f'top{k}_acc'", "]", "=", "acc", "\n", "log_msg", ".", "append", "(", "f'\\ntop{k}_acc\\t{acc:.4f}'", ")", "\n", "", "log_msg", "=", "''", ".", "join", "(", "log_msg", ")", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "if", "metric", "==", "'mean_class_accuracy'", ":", "\n", "                ", "mean_acc", "=", "mean_class_accuracy", "(", "results", ",", "gt_labels", ")", "\n", "eval_results", "[", "'mean_class_accuracy'", "]", "=", "mean_acc", "\n", "log_msg", "=", "f'\\nmean_acc\\t{mean_acc:.4f}'", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "if", "metric", "in", "[", "\n", "'mean_average_precision'", ",", "'mmit_mean_average_precision'", "\n", "]", ":", "\n", "                ", "gt_labels_arrays", "=", "[", "\n", "self", ".", "label2array", "(", "self", ".", "num_classes", ",", "label", ")", "\n", "for", "label", "in", "gt_labels", "\n", "]", "\n", "if", "metric", "==", "'mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mean_average_precision", "(", "results", ",", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmean_average_precision\\t{mAP:.4f}'", "\n", "", "elif", "metric", "==", "'mmit_mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mmit_mean_average_precision", "(", "results", ",", "\n", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mmit_mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmmit_mean_average_precision\\t{mAP:.4f}'", "\n", "", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "", "return", "eval_results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.dump_results": [[245, 249], ["mmcv.dump"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "dump_results", "(", "results", ",", "out", ")", ":", "\n", "        ", "\"\"\"Dump data to json/yaml/pickle strings or files.\"\"\"", "\n", "return", "mmcv", ".", "dump", "(", "results", ",", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.prepare_train_frames": [[250, 264], ["copy.deepcopy", "base.BaseDataset.pipeline", "isinstance", "torch.zeros"], "methods", ["None"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "# If HVU, type(results['label']) is dict", "\n", "if", "self", ".", "multi_class", "and", "isinstance", "(", "results", "[", "'label'", "]", ",", "list", ")", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.prepare_test_frames": [[265, 279], ["copy.deepcopy", "base.BaseDataset.pipeline", "isinstance", "torch.zeros"], "methods", ["None"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "# If HVU, type(results['label']) is dict", "\n", "if", "self", ".", "multi_class", "and", "isinstance", "(", "results", "[", "'label'", "]", ",", "list", ")", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.__len__": [[280, 283], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the size of the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "video_infos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.base.BaseDataset.__getitem__": [[284, 290], ["base.BaseDataset.prepare_train_frames", "base.BaseDataset.prepare_test_frames"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.prepare_train_frames", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.prepare_test_frames"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Get the sample for either training or testing given index.\"\"\"", "\n", "if", "self", ".", "test_mode", ":", "\n", "            ", "return", "self", ".", "prepare_test_frames", "(", "idx", ")", "\n", "\n", "", "return", "self", ".", "prepare_train_frames", "(", "idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.video_dataset.VideoDataset.__init__": [[39, 41], ["base.BaseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "start_index", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "start_index", "=", "start_index", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.video_dataset.VideoDataset.load_annotations": [[42, 62], ["video_dataset.VideoDataset.ann_file.endswith", "video_dataset.VideoDataset.load_json_annotations", "open", "line.strip().split", "video_infos.append", "list", "int", "os.join", "dict", "line.strip", "map"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "if", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", ":", "\n", "            ", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "\n", "", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "self", ".", "multi_class", ":", "\n", "                    ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "filename", ",", "label", "=", "line_split", "[", "0", "]", ",", "line_split", "[", "1", ":", "]", "\n", "label", "=", "list", "(", "map", "(", "int", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "                    ", "filename", ",", "label", "=", "line_split", "\n", "label", "=", "int", "(", "label", ")", "\n", "", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "filename", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "filename", ")", "\n", "", "video_infos", ".", "append", "(", "dict", "(", "filename", "=", "filename", ",", "label", "=", "label", ")", ")", "\n", "", "", "return", "video_infos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.__init__": [[67, 90], ["len", "sum", "dict", "range", "dict", "kwargs.pop", "base.BaseDataset.__init__", "len", "len", "zip", "hvu_dataset.HVUDataset.start_idx.append", "zip"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "tag_categories", ",", "\n", "tag_category_nums", ",", "\n", "filename_tmpl", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "assert", "len", "(", "tag_categories", ")", "==", "len", "(", "tag_category_nums", ")", "\n", "self", ".", "tag_categories", "=", "tag_categories", "\n", "self", ".", "tag_category_nums", "=", "tag_category_nums", "\n", "self", ".", "filename_tmpl", "=", "filename_tmpl", "\n", "self", ".", "num_categories", "=", "len", "(", "self", ".", "tag_categories", ")", "\n", "self", ".", "num_tags", "=", "sum", "(", "self", ".", "tag_category_nums", ")", "\n", "self", ".", "category2num", "=", "dict", "(", "zip", "(", "tag_categories", ",", "tag_category_nums", ")", ")", "\n", "self", ".", "start_idx", "=", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_categories", "-", "1", ")", ":", "\n", "            ", "self", ".", "start_idx", ".", "append", "(", "self", ".", "start_idx", "[", "-", "1", "]", "+", "\n", "self", ".", "tag_category_nums", "[", "i", "]", ")", "\n", "", "self", ".", "category2startidx", "=", "dict", "(", "zip", "(", "tag_categories", ",", "self", ".", "start_idx", ")", ")", "\n", "self", ".", "start_index", "=", "kwargs", ".", "pop", "(", "'start_index'", ",", "0", ")", "\n", "self", ".", "dataset_type", "=", "None", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "ann_file", ",", "pipeline", ",", "start_index", "=", "self", ".", "start_index", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.load_annotations": [[91, 95], ["hvu_dataset.HVUDataset.ann_file.endswith", "hvu_dataset.HVUDataset.load_json_annotations"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "assert", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", "\n", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.load_json_annotations": [[96, 122], ["mmcv.load", "len", "range", "os.join"], "methods", ["None"], ["", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "\n", "video_info0", "=", "video_infos", "[", "0", "]", "\n", "assert", "(", "'filename'", "in", "video_info0", ")", "!=", "(", "'frame_dir'", "in", "video_info0", ")", "\n", "path_key", "=", "'filename'", "if", "'filename'", "in", "video_info0", "else", "'frame_dir'", "\n", "self", ".", "dataset_type", "=", "'video'", "if", "path_key", "==", "'filename'", "else", "'rawframe'", "\n", "if", "self", ".", "dataset_type", "==", "'rawframe'", ":", "\n", "            ", "assert", "self", ".", "filename_tmpl", "is", "not", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n", "\n", "# We will convert label to torch tensors in the pipeline", "\n", "video_infos", "[", "i", "]", "[", "'categories'", "]", "=", "self", ".", "tag_categories", "\n", "video_infos", "[", "i", "]", "[", "'category_nums'", "]", "=", "self", ".", "tag_category_nums", "\n", "if", "self", ".", "dataset_type", "==", "'rawframe'", ":", "\n", "                ", "video_infos", "[", "i", "]", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "video_infos", "[", "i", "]", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "video_infos", "[", "i", "]", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "\n", "", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.label2array": [[123, 128], ["numpy.zeros"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "label2array", "(", "num", ",", "label", ")", ":", "\n", "        ", "arr", "=", "np", ".", "zeros", "(", "num", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "arr", "[", "label", "]", "=", "1.", "\n", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.evaluate": [[129, 193], ["copy.deepcopy", "collections.OrderedDict", "isinstance", "TypeError", "len", "len", "isinstance", "len", "core.mean_average_precision", "mmcv.utils.print_log", "len", "len", "hvu_dataset.HVUDataset.label2array", "enumerate", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_average_precision", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.hvu_dataset.HVUDataset.label2array"], ["", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'mean_average_precision'", ",", "\n", "metric_options", "=", "None", ",", "\n", "logger", "=", "None", ")", ":", "\n", "        ", "\"\"\"Evaluation in HVU Video Dataset. We only support evaluating mAP for\n        each tag categories. Since some tag categories are missing for some\n        videos, we can not evaluate mAP for all tags.\n\n        Args:\n            results (list): Output results.\n            metrics (str | sequence[str]): Metrics to be performed.\n                Defaults: 'mean_average_precision'.\n            metric_options (dict | None): Dict for metric options.\n                Default: None.\n            logger (logging.Logger | None): Logger for recording.\n                Default: None.\n\n        Returns:\n            dict: Evaluation results dict.\n        \"\"\"", "\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "\n", "# There should be only one metric in the metrics list:", "\n", "# 'mean_average_precision'", "\n", "assert", "len", "(", "metrics", ")", "==", "1", "\n", "metric", "=", "metrics", "[", "0", "]", "\n", "assert", "metric", "==", "'mean_average_precision'", "\n", "\n", "gt_labels", "=", "[", "ann", "[", "'label'", "]", "for", "ann", "in", "self", ".", "video_infos", "]", "\n", "\n", "eval_results", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "category", "in", "self", ".", "tag_categories", ":", "\n", "\n", "            ", "start_idx", "=", "self", ".", "category2startidx", "[", "category", "]", "\n", "num", "=", "self", ".", "category2num", "[", "category", "]", "\n", "preds", "=", "[", "\n", "result", "[", "start_idx", ":", "start_idx", "+", "num", "]", "\n", "for", "video_idx", ",", "result", "in", "enumerate", "(", "results", ")", "\n", "if", "category", "in", "gt_labels", "[", "video_idx", "]", "\n", "]", "\n", "gts", "=", "[", "\n", "gt_label", "[", "category", "]", "for", "gt_label", "in", "gt_labels", "\n", "if", "category", "in", "gt_label", "\n", "]", "\n", "\n", "gts", "=", "[", "self", ".", "label2array", "(", "num", ",", "item", ")", "for", "item", "in", "gts", "]", "\n", "\n", "mAP", "=", "mean_average_precision", "(", "preds", ",", "gts", ")", "\n", "eval_results", "[", "f'{category}_mAP'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\n{category}_mAP\\t{mAP:.4f}'", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "\n", "", "return", "eval_results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.BaseMiniBatchBlending.__init__": [[16, 18], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_classes", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.BaseMiniBatchBlending.do_blending": [[19, 22], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "do_blending", "(", "self", ",", "imgs", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.BaseMiniBatchBlending.__call__": [[23, 57], ["torch.one_hot", "torch.one_hot", "blending_utils.BaseMiniBatchBlending.do_blending"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.one_hot", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.mixup.one_hot", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.do_blending"], ["", "def", "__call__", "(", "self", ",", "imgs", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Blending data in a mini-batch.\n\n        Images are float tensors with the shape of (B, N, C, H, W) for 2D\n        recognizers or (B, N, C, T, H, W) for 3D recognizers.\n\n        Besides, labels are converted from hard labels to soft labels.\n        Hard labels are integer tensors with the shape of (B, 1) and all of the\n        elements are in the range [0, num_classes - 1].\n        Soft labels (probablity distribution over classes) are float tensors\n        with the shape of (B, 1, num_classes) and all of the elements are in\n        the range [0, 1].\n\n        Args:\n            imgs (torch.Tensor): Model input images, float tensor with the\n                shape of (B, N, C, H, W) or (B, N, C, T, H, W).\n            label (torch.Tensor): Hard labels, integer tensor with the shape\n                of (B, 1) and all elements are in range [0, num_classes).\n            kwargs (dict, optional): Other keyword argument to be used to\n                blending imgs and labels in a mini-batch.\n\n        Returns:\n            mixed_imgs (torch.Tensor): Blending images, float tensor with the\n                same shape of the input imgs.\n            mixed_label (torch.Tensor): Blended soft labels, float tensor with\n                the shape of (B, 1, num_classes) and all elements are in range\n                [0, 1].\n        \"\"\"", "\n", "one_hot_label", "=", "F", ".", "one_hot", "(", "label", ",", "num_classes", "=", "self", ".", "num_classes", ")", "\n", "\n", "mixed_imgs", ",", "mixed_label", "=", "self", ".", "do_blending", "(", "imgs", ",", "one_hot_label", ",", "\n", "**", "kwargs", ")", "\n", "\n", "return", "mixed_imgs", ",", "mixed_label", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.MixupBlending.__init__": [[72, 75], ["blending_utils.BaseMiniBatchBlending.__init__", "torch.distributions.beta.Beta", "torch.distributions.beta.Beta"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "alpha", "=", ".2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", "=", "num_classes", ")", "\n", "self", ".", "beta", "=", "Beta", "(", "alpha", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.MixupBlending.do_blending": [[76, 88], ["blending_utils.MixupBlending.beta.sample", "imgs.size", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "len"], "methods", ["None"], ["", "def", "do_blending", "(", "self", ",", "imgs", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Blending images with mixup.\"\"\"", "\n", "assert", "len", "(", "kwargs", ")", "==", "0", ",", "f'unexpected kwargs for mixup {kwargs}'", "\n", "\n", "lam", "=", "self", ".", "beta", ".", "sample", "(", ")", "\n", "batch_size", "=", "imgs", ".", "size", "(", "0", ")", "\n", "rand_index", "=", "torch", ".", "randperm", "(", "batch_size", ")", "\n", "\n", "mixed_imgs", "=", "lam", "*", "imgs", "+", "(", "1", "-", "lam", ")", "*", "imgs", "[", "rand_index", ",", ":", "]", "\n", "mixed_label", "=", "lam", "*", "label", "+", "(", "1", "-", "lam", ")", "*", "label", "[", "rand_index", ",", ":", "]", "\n", "\n", "return", "mixed_imgs", ",", "mixed_label", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.__init__": [[103, 106], ["blending_utils.BaseMiniBatchBlending.__init__", "torch.distributions.beta.Beta", "torch.distributions.beta.Beta"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "alpha", "=", ".2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", "=", "num_classes", ")", "\n", "self", ".", "beta", "=", "Beta", "(", "alpha", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.rand_bbox": [[107, 126], ["torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "int", "int", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "rand_bbox", "(", "img_size", ",", "lam", ")", ":", "\n", "        ", "\"\"\"Generate a random boudning box.\"\"\"", "\n", "w", "=", "img_size", "[", "-", "1", "]", "\n", "h", "=", "img_size", "[", "-", "2", "]", "\n", "cut_rat", "=", "torch", ".", "sqrt", "(", "1.", "-", "lam", ")", "\n", "cut_w", "=", "torch", ".", "tensor", "(", "int", "(", "w", "*", "cut_rat", ")", ")", "\n", "cut_h", "=", "torch", ".", "tensor", "(", "int", "(", "h", "*", "cut_rat", ")", ")", "\n", "\n", "# uniform", "\n", "cx", "=", "torch", ".", "randint", "(", "w", ",", "(", "1", ",", ")", ")", "[", "0", "]", "\n", "cy", "=", "torch", ".", "randint", "(", "h", ",", "(", "1", ",", ")", ")", "[", "0", "]", "\n", "\n", "bbx1", "=", "torch", ".", "clamp", "(", "cx", "-", "cut_w", "//", "2", ",", "0", ",", "w", ")", "\n", "bby1", "=", "torch", ".", "clamp", "(", "cy", "-", "cut_h", "//", "2", ",", "0", ",", "h", ")", "\n", "bbx2", "=", "torch", ".", "clamp", "(", "cx", "+", "cut_w", "//", "2", ",", "0", ",", "w", ")", "\n", "bby2", "=", "torch", ".", "clamp", "(", "cy", "+", "cut_h", "//", "2", ",", "0", ",", "h", ")", "\n", "\n", "return", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.do_blending": [[127, 144], ["imgs.size", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "blending_utils.CutmixBlending.beta.sample", "blending_utils.CutmixBlending.rand_bbox", "len", "imgs.size", "imgs.size", "imgs.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.blending_utils.CutmixBlending.rand_bbox"], ["", "def", "do_blending", "(", "self", ",", "imgs", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Blending images with cutmix.\"\"\"", "\n", "assert", "len", "(", "kwargs", ")", "==", "0", ",", "f'unexpected kwargs for cutmix {kwargs}'", "\n", "\n", "batch_size", "=", "imgs", ".", "size", "(", "0", ")", "\n", "rand_index", "=", "torch", ".", "randperm", "(", "batch_size", ")", "\n", "lam", "=", "self", ".", "beta", ".", "sample", "(", ")", "\n", "\n", "bbx1", ",", "bby1", ",", "bbx2", ",", "bby2", "=", "self", ".", "rand_bbox", "(", "imgs", ".", "size", "(", ")", ",", "lam", ")", "\n", "imgs", "[", ":", ",", "...", ",", "bby1", ":", "bby2", ",", "bbx1", ":", "bbx2", "]", "=", "imgs", "[", "rand_index", ",", "...", ",", "bby1", ":", "bby2", ",", "\n", "bbx1", ":", "bbx2", "]", "\n", "lam", "=", "1", "-", "(", "1.0", "*", "(", "bbx2", "-", "bbx1", ")", "*", "(", "bby2", "-", "bby1", ")", "/", "\n", "(", "imgs", ".", "size", "(", ")", "[", "-", "1", "]", "*", "imgs", ".", "size", "(", ")", "[", "-", "2", "]", ")", ")", "\n", "\n", "label", "=", "lam", "*", "label", "+", "(", "1", "-", "lam", ")", "*", "label", "[", "rand_index", ",", ":", "]", "\n", "\n", "return", "imgs", ",", "label", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset": [[28, 41], ["mmcv.utils.build_from_cfg"], "function", ["None"], ["def", "build_dataset", "(", "cfg", ",", "default_args", "=", "None", ")", ":", "\n", "    ", "\"\"\"Build a dataset from config dict.\n\n    Args:\n        cfg (dict): Config dict. It should at least contain the key \"type\".\n        default_args (dict | None, optional): Default initialization arguments.\n            Default: None.\n\n    Returns:\n        Dataset: The constructed dataset.\n    \"\"\"", "\n", "dataset", "=", "build_from_cfg", "(", "cfg", ",", "DATASETS", ",", "default_args", ")", "\n", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader": [[43, 130], ["mmcv.runner.get_dist_info", "getattr", "torch.utils.data.DataLoader", "functools.partial", "mmcv.utils.digit_version", "mmcv.utils.digit_version", "getattr", "samplers.ClassSpecificDistributedSampler", "samplers.DistributedSampler", "functools.partial"], "function", ["None"], ["", "def", "build_dataloader", "(", "dataset", ",", "\n", "videos_per_gpu", ",", "\n", "workers_per_gpu", ",", "\n", "num_gpus", "=", "1", ",", "\n", "dist", "=", "True", ",", "\n", "shuffle", "=", "True", ",", "\n", "seed", "=", "None", ",", "\n", "drop_last", "=", "False", ",", "\n", "pin_memory", "=", "True", ",", "\n", "persistent_workers", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Build PyTorch DataLoader.\n\n    In distributed training, each GPU/process has a dataloader.\n    In non-distributed training, there is only one dataloader for all GPUs.\n\n    Args:\n        dataset (:obj:`Dataset`): A PyTorch dataset.\n        videos_per_gpu (int): Number of videos on each GPU, i.e.,\n            batch size of each GPU.\n        workers_per_gpu (int): How many subprocesses to use for data\n            loading for each GPU.\n        num_gpus (int): Number of GPUs. Only used in non-distributed\n            training. Default: 1.\n        dist (bool): Distributed training/test or not. Default: True.\n        shuffle (bool): Whether to shuffle the data at every epoch.\n            Default: True.\n        seed (int | None): Seed to be used. Default: None.\n        drop_last (bool): Whether to drop the last incomplete batch in epoch.\n            Default: False\n        pin_memory (bool): Whether to use pin_memory in DataLoader.\n            Default: True\n        persistent_workers (bool): If True, the data loader will not shutdown\n            the worker processes after a dataset has been consumed once.\n            This allows to maintain the workers Dataset instances alive.\n            The argument also has effect in PyTorch>=1.8.0.\n            Default: False\n        kwargs (dict, optional): Any keyword argument to be used to initialize\n            DataLoader.\n\n    Returns:\n        DataLoader: A PyTorch dataloader.\n    \"\"\"", "\n", "rank", ",", "world_size", "=", "get_dist_info", "(", ")", "\n", "sample_by_class", "=", "getattr", "(", "dataset", ",", "'sample_by_class'", ",", "False", ")", "\n", "\n", "if", "dist", ":", "\n", "        ", "if", "sample_by_class", ":", "\n", "            ", "dynamic_length", "=", "getattr", "(", "dataset", ",", "'dynamic_length'", ",", "True", ")", "\n", "sampler", "=", "ClassSpecificDistributedSampler", "(", "\n", "dataset", ",", "\n", "world_size", ",", "\n", "rank", ",", "\n", "dynamic_length", "=", "dynamic_length", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "seed", "=", "seed", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "DistributedSampler", "(", "\n", "dataset", ",", "world_size", ",", "rank", ",", "shuffle", "=", "shuffle", ",", "seed", "=", "seed", ")", "\n", "", "shuffle", "=", "False", "\n", "batch_size", "=", "videos_per_gpu", "\n", "num_workers", "=", "workers_per_gpu", "\n", "", "else", ":", "\n", "        ", "sampler", "=", "None", "\n", "batch_size", "=", "num_gpus", "*", "videos_per_gpu", "\n", "num_workers", "=", "num_gpus", "*", "workers_per_gpu", "\n", "\n", "", "init_fn", "=", "partial", "(", "\n", "worker_init_fn", ",", "num_workers", "=", "num_workers", ",", "rank", "=", "rank", ",", "\n", "seed", "=", "seed", ")", "if", "seed", "is", "not", "None", "else", "None", "\n", "\n", "if", "digit_version", "(", "torch", ".", "__version__", ")", ">=", "digit_version", "(", "'1.8.0'", ")", ":", "\n", "        ", "kwargs", "[", "'persistent_workers'", "]", "=", "persistent_workers", "\n", "\n", "", "data_loader", "=", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "sampler", "=", "sampler", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "partial", "(", "collate", ",", "samples_per_gpu", "=", "videos_per_gpu", ")", ",", "\n", "pin_memory", "=", "pin_memory", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "worker_init_fn", "=", "init_fn", ",", "\n", "drop_last", "=", "drop_last", ",", "\n", "**", "kwargs", ")", "\n", "\n", "return", "data_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.worker_init_fn": [[132, 139], ["numpy.random.seed", "random.seed"], "function", ["None"], ["", "def", "worker_init_fn", "(", "worker_id", ",", "num_workers", ",", "rank", ",", "seed", ")", ":", "\n", "    ", "\"\"\"Init the random seed for various workers.\"\"\"", "\n", "# The seed of each worker equals to", "\n", "# num_worker * rank + worker_id + user_seed", "\n", "worker_seed", "=", "num_workers", "*", "rank", "+", "worker_id", "+", "seed", "\n", "np", ".", "random", ".", "seed", "(", "worker_seed", ")", "\n", "random", ".", "seed", "(", "worker_seed", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.pose_dataset.PoseDataset.__init__": [[43, 92], ["base.BaseDataset.__init__", "utils.get_root_logger", "utils.get_root_logger.info", "isinstance", "float", "len", "numpy.array", "enumerate"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "split", "=", "None", ",", "\n", "valid_ratio", "=", "None", ",", "\n", "box_thr", "=", "None", ",", "\n", "class_prob", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "modality", "=", "'Pose'", "\n", "# split, applicable to ucf or hmdb", "\n", "self", ".", "split", "=", "split", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "ann_file", ",", "pipeline", ",", "start_index", "=", "0", ",", "modality", "=", "modality", ",", "**", "kwargs", ")", "\n", "\n", "# box_thr, which should be a string", "\n", "self", ".", "box_thr", "=", "box_thr", "\n", "if", "self", ".", "box_thr", "is", "not", "None", ":", "\n", "            ", "assert", "box_thr", "in", "[", "'0.5'", ",", "'0.6'", ",", "'0.7'", ",", "'0.8'", ",", "'0.9'", "]", "\n", "\n", "# Thresholding Training Examples", "\n", "", "self", ".", "valid_ratio", "=", "valid_ratio", "\n", "if", "self", ".", "valid_ratio", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "valid_ratio", ",", "float", ")", "\n", "if", "self", ".", "box_thr", "is", "None", ":", "\n", "                ", "self", ".", "video_infos", "=", "self", ".", "video_infos", "=", "[", "\n", "x", "for", "x", "in", "self", ".", "video_infos", "\n", "if", "x", "[", "'valid_frames'", "]", "/", "x", "[", "'total_frames'", "]", ">=", "valid_ratio", "\n", "]", "\n", "", "else", ":", "\n", "                ", "key", "=", "f'valid@{self.box_thr}'", "\n", "self", ".", "video_infos", "=", "[", "\n", "x", "for", "x", "in", "self", ".", "video_infos", "\n", "if", "x", "[", "key", "]", "/", "x", "[", "'total_frames'", "]", ">=", "valid_ratio", "\n", "]", "\n", "if", "self", ".", "box_thr", "!=", "'0.5'", ":", "\n", "                    ", "box_thr", "=", "float", "(", "self", ".", "box_thr", ")", "\n", "for", "item", "in", "self", ".", "video_infos", ":", "\n", "                        ", "inds", "=", "[", "\n", "i", "for", "i", ",", "score", "in", "enumerate", "(", "item", "[", "'box_score'", "]", ")", "\n", "if", "score", ">=", "box_thr", "\n", "]", "\n", "item", "[", "'anno_inds'", "]", "=", "np", ".", "array", "(", "inds", ")", "\n", "\n", "", "", "", "", "if", "class_prob", "is", "not", "None", ":", "\n", "            ", "self", ".", "class_prob", "=", "class_prob", "\n", "\n", "", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'{len(self)} videos remain after valid thresholding'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.pose_dataset.PoseDataset.load_annotations": [[93, 97], ["pose_dataset.PoseDataset.ann_file.endswith", "pose_dataset.PoseDataset.load_pkl_annotations"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.pose_dataset.PoseDataset.load_pkl_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "assert", "self", ".", "ann_file", ".", "endswith", "(", "'.pkl'", ")", "\n", "return", "self", ".", "load_pkl_annotations", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.pose_dataset.PoseDataset.load_pkl_annotations": [[98, 114], ["mmcv.load", "os.join", "os.join"], "methods", ["None"], ["", "def", "load_pkl_annotations", "(", "self", ")", ":", "\n", "        ", "data", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "\n", "if", "self", ".", "split", ":", "\n", "            ", "split", ",", "data", "=", "data", "[", "'split'", "]", ",", "data", "[", "'annotations'", "]", "\n", "identifier", "=", "'filename'", "if", "'filename'", "in", "data", "[", "0", "]", "else", "'frame_dir'", "\n", "data", "=", "[", "x", "for", "x", "in", "data", "if", "x", "[", "identifier", "]", "in", "split", "[", "self", ".", "split", "]", "]", "\n", "\n", "", "for", "item", "in", "data", ":", "\n", "# Sometimes we may need to load anno from the file", "\n", "            ", "if", "'filename'", "in", "item", ":", "\n", "                ", "item", "[", "'filename'", "]", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "item", "[", "'filename'", "]", ")", "\n", "", "if", "'frame_dir'", "in", "item", ":", "\n", "                ", "item", "[", "'frame_dir'", "]", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "\n", "item", "[", "'frame_dir'", "]", ")", "\n", "", "", "return", "data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.RepeatDataset.__init__": [[23, 29], ["builder.build_dataset", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset"], ["def", "__init__", "(", "self", ",", "dataset", ",", "times", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "dataset", "[", "'test_mode'", "]", "=", "test_mode", "\n", "self", ".", "dataset", "=", "build_dataset", "(", "dataset", ")", "\n", "self", ".", "times", "=", "times", "\n", "\n", "self", ".", "_ori_len", "=", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.RepeatDataset.__getitem__": [[30, 33], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Get data.\"\"\"", "\n", "return", "self", ".", "dataset", "[", "idx", "%", "self", ".", "_ori_len", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.RepeatDataset.__len__": [[34, 37], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Length after repetition.\"\"\"", "\n", "return", "self", ".", "times", "*", "self", ".", "_ori_len", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.ConcatDataset.__init__": [[53, 62], ["numpy.cumsum", "builder.build_dataset", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset"], ["def", "__init__", "(", "self", ",", "datasets", ",", "test_mode", "=", "False", ")", ":", "\n", "\n", "        ", "for", "item", "in", "datasets", ":", "\n", "            ", "item", "[", "'test_mode'", "]", "=", "test_mode", "\n", "\n", "", "datasets", "=", "[", "build_dataset", "(", "cfg", ")", "for", "cfg", "in", "datasets", "]", "\n", "self", ".", "datasets", "=", "datasets", "\n", "self", ".", "lens", "=", "[", "len", "(", "x", ")", "for", "x", "in", "self", ".", "datasets", "]", "\n", "self", ".", "cumsum", "=", "np", ".", "cumsum", "(", "self", ".", "lens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.ConcatDataset.__getitem__": [[63, 68], ["numpy.searchsorted"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Get data.\"\"\"", "\n", "dataset_idx", "=", "np", ".", "searchsorted", "(", "self", ".", "cumsum", ",", "idx", ",", "side", "=", "'right'", ")", "\n", "item_idx", "=", "idx", "if", "dataset_idx", "==", "0", "else", "idx", "-", "self", ".", "cumsum", "[", "dataset_idx", "]", "\n", "return", "self", ".", "datasets", "[", "dataset_idx", "]", "[", "item_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.dataset_wrappers.ConcatDataset.__len__": [[69, 72], ["sum"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Length after repetition.\"\"\"", "\n", "return", "sum", "(", "self", ".", "lens", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.__init__": [[77, 79], ["base.BaseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "data_prefix", "=", "None", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "data_prefix", ",", "test_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.load_annotations": [[80, 89], ["mmcv.load", "video_infos.append"], "methods", ["None"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n", "video_infos", "=", "[", "]", "\n", "anno_database", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "for", "video_name", "in", "anno_database", ":", "\n", "            ", "video_info", "=", "anno_database", "[", "video_name", "]", "\n", "video_info", "[", "'video_name'", "]", "=", "video_name", "\n", "video_infos", ".", "append", "(", "video_info", ")", "\n", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.prepare_test_frames": [[90, 95], ["copy.deepcopy", "activitynet_dataset.ActivityNetDataset.pipeline"], "methods", ["None"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'data_prefix'", "]", "=", "self", ".", "data_prefix", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.prepare_train_frames": [[96, 101], ["copy.deepcopy", "activitynet_dataset.ActivityNetDataset.pipeline"], "methods", ["None"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'data_prefix'", "]", "=", "self", ".", "data_prefix", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.__len__": [[102, 105], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the size of the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "video_infos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset._import_ground_truth": [[106, 118], ["numpy.array", "this_video_ground_truths.append"], "methods", ["None"], ["", "def", "_import_ground_truth", "(", "self", ")", ":", "\n", "        ", "\"\"\"Read ground truth data from video_infos.\"\"\"", "\n", "ground_truth", "=", "{", "}", "\n", "for", "video_info", "in", "self", ".", "video_infos", ":", "\n", "            ", "video_id", "=", "video_info", "[", "'video_name'", "]", "[", "2", ":", "]", "\n", "this_video_ground_truths", "=", "[", "]", "\n", "for", "ann", "in", "video_info", "[", "'annotations'", "]", ":", "\n", "                ", "t_start", ",", "t_end", "=", "ann", "[", "'segment'", "]", "\n", "label", "=", "ann", "[", "'label'", "]", "\n", "this_video_ground_truths", ".", "append", "(", "[", "t_start", ",", "t_end", ",", "label", "]", ")", "\n", "", "ground_truth", "[", "video_id", "]", "=", "np", ".", "array", "(", "this_video_ground_truths", ")", "\n", "", "return", "ground_truth", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.proposals2json": [[119, 146], ["print", "mmcv.ProgressBar", "len", "mmcv.ProgressBar.update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "@", "staticmethod", "\n", "def", "proposals2json", "(", "results", ",", "show_progress", "=", "False", ")", ":", "\n", "        ", "\"\"\"Convert all proposals to a final dict(json) format.\n\n        Args:\n            results (list[dict]): All proposals.\n            show_progress (bool): Whether to show the progress bar.\n                Defaults: False.\n\n        Returns:\n            dict: The final result dict. E.g.\n\n            .. code-block:: Python\n\n                dict(video-1=[dict(segment=[1.1,2.0]. score=0.9),\n                              dict(segment=[50.1, 129.3], score=0.6)])\n        \"\"\"", "\n", "result_dict", "=", "{", "}", "\n", "print", "(", "'Convert proposals to json format'", ")", "\n", "if", "show_progress", ":", "\n", "            ", "prog_bar", "=", "mmcv", ".", "ProgressBar", "(", "len", "(", "results", ")", ")", "\n", "", "for", "result", "in", "results", ":", "\n", "            ", "video_name", "=", "result", "[", "'video_name'", "]", "\n", "result_dict", "[", "video_name", "[", "2", ":", "]", "]", "=", "result", "[", "'proposal_list'", "]", "\n", "if", "show_progress", ":", "\n", "                ", "prog_bar", ".", "update", "(", ")", "\n", "", "", "return", "result_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset._import_proposals": [[147, 162], ["numpy.array", "this_video_proposals.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_import_proposals", "(", "results", ")", ":", "\n", "        ", "\"\"\"Read predictions from results.\"\"\"", "\n", "proposals", "=", "{", "}", "\n", "num_proposals", "=", "0", "\n", "for", "result", "in", "results", ":", "\n", "            ", "video_id", "=", "result", "[", "'video_name'", "]", "[", "2", ":", "]", "\n", "this_video_proposals", "=", "[", "]", "\n", "for", "proposal", "in", "result", "[", "'proposal_list'", "]", ":", "\n", "                ", "t_start", ",", "t_end", "=", "proposal", "[", "'segment'", "]", "\n", "score", "=", "proposal", "[", "'score'", "]", "\n", "this_video_proposals", ".", "append", "(", "[", "t_start", ",", "t_end", ",", "score", "]", ")", "\n", "num_proposals", "+=", "1", "\n", "", "proposals", "[", "video_id", "]", "=", "np", ".", "array", "(", "this_video_proposals", ")", "\n", "", "return", "proposals", ",", "num_proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.dump_results": [[163, 189], ["activitynet_dataset.ActivityNetDataset.proposals2json", "mmcv.dump", "os.makedirs", "os.makedirs", "os.makedirs", "os.makedirs", "ValueError", "os.join", "os.join", "numpy.savetxt"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.proposals2json"], ["", "def", "dump_results", "(", "self", ",", "results", ",", "out", ",", "output_format", ",", "version", "=", "'VERSION 1.3'", ")", ":", "\n", "        ", "\"\"\"Dump data to json/csv files.\"\"\"", "\n", "if", "output_format", "==", "'json'", ":", "\n", "            ", "result_dict", "=", "self", ".", "proposals2json", "(", "results", ")", "\n", "output_dict", "=", "{", "\n", "'version'", ":", "version", ",", "\n", "'results'", ":", "result_dict", ",", "\n", "'external_data'", ":", "{", "}", "\n", "}", "\n", "mmcv", ".", "dump", "(", "output_dict", ",", "out", ")", "\n", "", "elif", "output_format", "==", "'csv'", ":", "\n", "# TODO: add csv handler to mmcv and use mmcv.dump", "\n", "            ", "os", ".", "makedirs", "(", "out", ",", "exist_ok", "=", "True", ")", "\n", "header", "=", "'action,start,end,tmin,tmax'", "\n", "for", "result", "in", "results", ":", "\n", "                ", "video_name", ",", "outputs", "=", "result", "\n", "output_path", "=", "osp", ".", "join", "(", "out", ",", "video_name", "+", "'.csv'", ")", "\n", "np", ".", "savetxt", "(", "\n", "output_path", ",", "\n", "outputs", ",", "\n", "header", "=", "header", ",", "\n", "delimiter", "=", "','", ",", "\n", "comments", "=", "''", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The output format {output_format} is not supported.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset.evaluate": [[190, 271], ["copy.deepcopy", "collections.OrderedDict", "activitynet_dataset.ActivityNetDataset._import_ground_truth", "activitynet_dataset.ActivityNetDataset._import_proposals", "dict", "warnings.warn", "dict", "isinstance", "TypeError", "len", "len", "isinstance", "len", "len", "KeyError", "copy.deepcopy.setdefault().setdefault", "copy.deepcopy.setdefault().setdefault", "isinstance", "core.average_recall_at_avg_proposals", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.linspace", "numpy.linspace", "numpy.array", "type", "copy.deepcopy.setdefault", "copy.deepcopy.setdefault"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization._import_ground_truth", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.activitynet_dataset.ActivityNetDataset._import_proposals", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.average_recall_at_avg_proposals"], ["", "", "def", "evaluate", "(", "\n", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'AR@AN'", ",", "\n", "metric_options", "=", "{", "\n", "'AR@AN'", ":", "\n", "dict", "(", "\n", "max_avg_proposals", "=", "100", ",", "\n", "temporal_iou_thresholds", "=", "np", ".", "linspace", "(", "0.5", ",", "0.95", ",", "10", ")", ")", "\n", "}", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        ", "\"\"\"Evaluation in feature dataset.\n\n        Args:\n            results (list[dict]): Output results.\n            metrics (str | sequence[str]): Metrics to be performed.\n                Defaults: 'AR@AN'.\n            metric_options (dict): Dict for metric options. Options are\n                ``max_avg_proposals``, ``temporal_iou_thresholds`` for\n                ``AR@AN``.\n                default: ``{'AR@AN': dict(max_avg_proposals=100,\n                temporal_iou_thresholds=np.linspace(0.5, 0.95, 10))}``.\n            logger (logging.Logger | None): Training logger. Defaults: None.\n            deprecated_kwargs (dict): Used for containing deprecated arguments.\n                See 'https://github.com/open-mmlab/mmaction2/pull/286'.\n\n        Returns:\n            dict: Evaluation results for evaluation metrics.\n        \"\"\"", "\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n", "\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'AR@AN'", "]", "=", "dict", "(", "metric_options", "[", "'AR@AN'", "]", ",", "\n", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "allowed_metrics", "=", "[", "'AR@AN'", "]", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "allowed_metrics", ":", "\n", "                ", "raise", "KeyError", "(", "f'metric {metric} is not supported'", ")", "\n", "\n", "", "", "eval_results", "=", "OrderedDict", "(", ")", "\n", "ground_truth", "=", "self", ".", "_import_ground_truth", "(", ")", "\n", "proposal", ",", "num_proposals", "=", "self", ".", "_import_proposals", "(", "results", ")", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "==", "'AR@AN'", ":", "\n", "                ", "temporal_iou_thresholds", "=", "metric_options", ".", "setdefault", "(", "\n", "'AR@AN'", ",", "{", "}", ")", ".", "setdefault", "(", "'temporal_iou_thresholds'", ",", "\n", "np", ".", "linspace", "(", "0.5", ",", "0.95", ",", "10", ")", ")", "\n", "max_avg_proposals", "=", "metric_options", ".", "setdefault", "(", "\n", "'AR@AN'", ",", "{", "}", ")", ".", "setdefault", "(", "'max_avg_proposals'", ",", "100", ")", "\n", "if", "isinstance", "(", "temporal_iou_thresholds", ",", "list", ")", ":", "\n", "                    ", "temporal_iou_thresholds", "=", "np", ".", "array", "(", "temporal_iou_thresholds", ")", "\n", "\n", "", "recall", ",", "_", ",", "_", ",", "auc", "=", "(", "\n", "average_recall_at_avg_proposals", "(", "\n", "ground_truth", ",", "\n", "proposal", ",", "\n", "num_proposals", ",", "\n", "max_avg_proposals", "=", "max_avg_proposals", ",", "\n", "temporal_iou_thresholds", "=", "temporal_iou_thresholds", ")", ")", "\n", "eval_results", "[", "'auc'", "]", "=", "auc", "\n", "eval_results", "[", "'AR@1'", "]", "=", "np", ".", "mean", "(", "recall", "[", ":", ",", "0", "]", ")", "\n", "eval_results", "[", "'AR@5'", "]", "=", "np", ".", "mean", "(", "recall", "[", ":", ",", "4", "]", ")", "\n", "eval_results", "[", "'AR@10'", "]", "=", "np", ".", "mean", "(", "recall", "[", ":", ",", "9", "]", ")", "\n", "eval_results", "[", "'AR@100'", "]", "=", "np", ".", "mean", "(", "recall", "[", ":", ",", "99", "]", ")", "\n", "\n", "", "", "return", "eval_results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNInstance.__init__": [[33, 50], ["min"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "start_frame", ",", "\n", "end_frame", ",", "\n", "num_video_frames", ",", "\n", "label", "=", "None", ",", "\n", "best_iou", "=", "0", ",", "\n", "overlap_self", "=", "0", ")", ":", "\n", "        ", "self", ".", "start_frame", "=", "start_frame", "\n", "self", ".", "end_frame", "=", "min", "(", "end_frame", ",", "num_video_frames", ")", "\n", "self", ".", "num_video_frames", "=", "num_video_frames", "\n", "self", ".", "label", "=", "label", "if", "label", "is", "not", "None", "else", "-", "1", "\n", "self", ".", "coverage", "=", "(", "end_frame", "-", "start_frame", ")", "/", "num_video_frames", "\n", "self", ".", "best_iou", "=", "best_iou", "\n", "self", ".", "overlap_self", "=", "overlap_self", "\n", "self", ".", "loc_reg", "=", "None", "\n", "self", ".", "size_reg", "=", "None", "\n", "self", ".", "regression_targets", "=", "[", "0.", ",", "0.", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNInstance.compute_regression_targets": [[51, 80], ["numpy.log", "temporal_iou", "numpy.argmax"], "methods", ["None"], ["", "def", "compute_regression_targets", "(", "self", ",", "gt_list", ")", ":", "\n", "        ", "\"\"\"Compute regression targets of positive proposals.\n\n        Args:\n            gt_list (list): The list of groundtruth instances.\n        \"\"\"", "\n", "# Find the groundtruth instance with the highest IOU.", "\n", "ious", "=", "[", "\n", "temporal_iou", "(", "self", ".", "start_frame", ",", "self", ".", "end_frame", ",", "gt", ".", "start_frame", ",", "\n", "gt", ".", "end_frame", ")", "for", "gt", "in", "gt_list", "\n", "]", "\n", "best_gt", "=", "gt_list", "[", "np", ".", "argmax", "(", "ious", ")", "]", "\n", "\n", "# interval: [start_frame, end_frame)", "\n", "proposal_center", "=", "(", "self", ".", "start_frame", "+", "self", ".", "end_frame", "-", "1", ")", "/", "2", "\n", "gt_center", "=", "(", "best_gt", ".", "start_frame", "+", "best_gt", ".", "end_frame", "-", "1", ")", "/", "2", "\n", "proposal_size", "=", "self", ".", "end_frame", "-", "self", ".", "start_frame", "\n", "gt_size", "=", "best_gt", ".", "end_frame", "-", "best_gt", ".", "start_frame", "\n", "\n", "# Get regression targets:", "\n", "# (1). Localization regression target:", "\n", "#     center shift proportional to the proposal duration", "\n", "# (2). Duration/Size regression target:", "\n", "#     logarithm of the groundtruth duration over proposal duration", "\n", "\n", "self", ".", "loc_reg", "=", "(", "gt_center", "-", "proposal_center", ")", "/", "proposal_size", "\n", "self", ".", "size_reg", "=", "np", ".", "log", "(", "gt_size", "/", "proposal_size", ")", "\n", "self", ".", "regression_targets", "=", "(", "[", "self", ".", "loc_reg", ",", "self", ".", "size_reg", "]", "\n", "if", "self", ".", "loc_reg", "is", "not", "None", "else", "[", "0.", ",", "0.", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.__init__": [[163, 278], ["utils.get_root_logger", "base.BaseDataset.__init__", "ssn_dataset.SSNDataset.logger.info", "ssn_dataset.SSNDataset.construct_proposal_pools", "torch.nn.modules.utils._pair", "int", "int", "ssn_dataset.SSNDataset._compute_reg_normalize_constants", "mmcv.is_tuple_of", "TypeError", "len", "ssn_dataset.SSNDataset.logger.info", "ssn_dataset.SSNDataset.logger.info", "enumerate", "len", "len", "len", "type", "len", "len", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.construct_proposal_pools", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._compute_reg_normalize_constants"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "train_cfg", ",", "\n", "test_cfg", ",", "\n", "data_prefix", ",", "\n", "test_mode", "=", "False", ",", "\n", "filename_tmpl", "=", "'img_{:05d}.jpg'", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "video_centric", "=", "True", ",", "\n", "reg_normalize_constants", "=", "None", ",", "\n", "body_segments", "=", "5", ",", "\n", "aug_segments", "=", "(", "2", ",", "2", ")", ",", "\n", "aug_ratio", "=", "(", "0.5", ",", "0.5", ")", ",", "\n", "clip_len", "=", "1", ",", "\n", "frame_interval", "=", "1", ",", "\n", "filter_gt", "=", "True", ",", "\n", "use_regression", "=", "True", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "        ", "self", ".", "logger", "=", "get_root_logger", "(", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "data_prefix", ",", "\n", "test_mode", "=", "test_mode", ",", "\n", "start_index", "=", "start_index", ",", "\n", "modality", "=", "modality", ")", "\n", "self", ".", "train_cfg", "=", "train_cfg", "\n", "self", ".", "test_cfg", "=", "test_cfg", "\n", "self", ".", "assigner", "=", "train_cfg", ".", "ssn", ".", "assigner", "\n", "self", ".", "sampler", "=", "train_cfg", ".", "ssn", ".", "sampler", "\n", "self", ".", "evaluater", "=", "test_cfg", ".", "ssn", ".", "evaluater", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "filename_tmpl", "=", "filename_tmpl", "\n", "\n", "if", "filter_gt", "or", "not", "test_mode", ":", "\n", "            ", "valid_inds", "=", "[", "\n", "i", "for", "i", ",", "video_info", "in", "enumerate", "(", "self", ".", "video_infos", ")", "\n", "if", "len", "(", "video_info", "[", "'gts'", "]", ")", ">", "0", "\n", "]", "\n", "", "self", ".", "logger", ".", "info", "(", "f'{len(valid_inds)} out of {len(self.video_infos)} '", "\n", "f'videos are valid.'", ")", "\n", "self", ".", "video_infos", "=", "[", "self", ".", "video_infos", "[", "i", "]", "for", "i", "in", "valid_inds", "]", "\n", "\n", "# construct three pools:", "\n", "# 1. Positive(Foreground)", "\n", "# 2. Background", "\n", "# 3. Incomplete", "\n", "self", ".", "positive_pool", "=", "[", "]", "\n", "self", ".", "background_pool", "=", "[", "]", "\n", "self", ".", "incomplete_pool", "=", "[", "]", "\n", "self", ".", "construct_proposal_pools", "(", ")", "\n", "\n", "if", "reg_normalize_constants", "is", "None", ":", "\n", "            ", "self", ".", "reg_norm_consts", "=", "self", ".", "_compute_reg_normalize_constants", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reg_norm_consts", "=", "reg_normalize_constants", "\n", "", "self", ".", "video_centric", "=", "video_centric", "\n", "self", ".", "body_segments", "=", "body_segments", "\n", "self", ".", "aug_segments", "=", "aug_segments", "\n", "self", ".", "aug_ratio", "=", "_pair", "(", "aug_ratio", ")", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "aug_ratio", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'aug_ratio should be int, float'", "\n", "f'or tuple of int and float, '", "\n", "f'but got {type(aug_ratio)}'", ")", "\n", "", "assert", "len", "(", "self", ".", "aug_ratio", ")", "==", "2", "\n", "\n", "total_ratio", "=", "(", "\n", "self", ".", "sampler", ".", "positive_ratio", "+", "self", ".", "sampler", ".", "background_ratio", "+", "\n", "self", ".", "sampler", ".", "incomplete_ratio", ")", "\n", "self", ".", "positive_per_video", "=", "int", "(", "\n", "self", ".", "sampler", ".", "num_per_video", "*", "\n", "(", "self", ".", "sampler", ".", "positive_ratio", "/", "total_ratio", ")", ")", "\n", "self", ".", "background_per_video", "=", "int", "(", "\n", "self", ".", "sampler", ".", "num_per_video", "*", "\n", "(", "self", ".", "sampler", ".", "background_ratio", "/", "total_ratio", ")", ")", "\n", "self", ".", "incomplete_per_video", "=", "(", "\n", "self", ".", "sampler", ".", "num_per_video", "-", "self", ".", "positive_per_video", "-", "\n", "self", ".", "background_per_video", ")", "\n", "\n", "self", ".", "test_interval", "=", "self", ".", "test_cfg", ".", "ssn", ".", "sampler", ".", "test_interval", "\n", "# number of consecutive frames", "\n", "self", ".", "clip_len", "=", "clip_len", "\n", "# number of steps (sparse sampling for efficiency of io)", "\n", "self", ".", "frame_interval", "=", "frame_interval", "\n", "\n", "# test mode or not", "\n", "self", ".", "filter_gt", "=", "filter_gt", "\n", "self", ".", "use_regression", "=", "use_regression", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "\n", "# yapf: disable", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "f\"\"\"\n            SSNDataset: proposal file {self.proposal_file} parsed.\n\n            There are {len(self.positive_pool) + len(self.background_pool) +\n                len(self.incomplete_pool)} usable proposals from {len(self.video_infos)} videos.\n            {len(self.positive_pool)} positive proposals\n            {len(self.incomplete_pool)} incomplete proposals\n            {len(self.background_pool)} background proposals\n\n            Sample config:\n            FG/BG/INCOMP: {self.positive_per_video}/{self.background_per_video}/{self.incomplete_per_video}  # noqa:E501\n            Video Centric: {self.video_centric}\n\n            Regression Normalization Constants:\n            Location: mean {self.reg_norm_consts[0][0]:.05f} std {self.reg_norm_consts[1][0]:.05f} # noqa: E501\n            Duration: mean {self.reg_norm_consts[0][1]:.05f} std {self.reg_norm_consts[1][1]:.05f} # noqa: E501\n            \"\"\"", ")", "\n", "# yapf: enable", "\n", "", "else", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "\n", "f'SSNDataset: proposal file {self.proposal_file} parsed.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.load_annotations": [[279, 330], ["load_localize_proposal_file", "ssn_dataset.SSNDataset.ann_file.replace", "int", "video_infos.append", "os.exists", "Exception", "os.join", "dict", "ssn_dataset.SSNInstance", "gts.append", "ssn_dataset.SSNInstance", "proposals.append", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "int", "float", "float"], "methods", ["None"], ["", "", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "[", "]", "\n", "if", "'normalized_'", "in", "self", ".", "ann_file", ":", "\n", "            ", "self", ".", "proposal_file", "=", "self", ".", "ann_file", ".", "replace", "(", "'normalized_'", ",", "''", ")", "\n", "if", "not", "osp", ".", "exists", "(", "self", ".", "proposal_file", ")", ":", "\n", "                ", "raise", "Exception", "(", "f'Please refer to `$MMACTION2/tools/data` to'", "\n", "f'denormalize {self.ann_file}.'", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "proposal_file", "=", "self", ".", "ann_file", "\n", "", "proposal_infos", "=", "load_localize_proposal_file", "(", "self", ".", "proposal_file", ")", "\n", "# proposal_info:[video_id, num_frames, gt_list, proposal_list]", "\n", "# gt_list member: [label, start_frame, end_frame]", "\n", "# proposal_list member: [label, best_iou, overlap_self,", "\n", "#                        start_frame, end_frame]", "\n", "for", "proposal_info", "in", "proposal_infos", ":", "\n", "            ", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "frame_dir", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "proposal_info", "[", "0", "]", ")", "\n", "", "num_frames", "=", "int", "(", "proposal_info", "[", "1", "]", ")", "\n", "# gts:start, end, num_frames, class_label, tIoU=1", "\n", "gts", "=", "[", "]", "\n", "for", "x", "in", "proposal_info", "[", "2", "]", ":", "\n", "                ", "if", "int", "(", "x", "[", "2", "]", ")", ">", "int", "(", "x", "[", "1", "]", ")", "and", "int", "(", "x", "[", "1", "]", ")", "<", "num_frames", ":", "\n", "                    ", "ssn_instance", "=", "SSNInstance", "(", "\n", "int", "(", "x", "[", "1", "]", ")", ",", "\n", "int", "(", "x", "[", "2", "]", ")", ",", "\n", "num_frames", ",", "\n", "label", "=", "int", "(", "x", "[", "0", "]", ")", ",", "\n", "best_iou", "=", "1.0", ")", "\n", "gts", ".", "append", "(", "ssn_instance", ")", "\n", "# proposals:start, end, num_frames, class_label", "\n", "# tIoU=best_iou, overlap_self", "\n", "", "", "proposals", "=", "[", "]", "\n", "for", "x", "in", "proposal_info", "[", "3", "]", ":", "\n", "                ", "if", "int", "(", "x", "[", "4", "]", ")", ">", "int", "(", "x", "[", "3", "]", ")", "and", "int", "(", "x", "[", "3", "]", ")", "<", "num_frames", ":", "\n", "                    ", "ssn_instance", "=", "SSNInstance", "(", "\n", "int", "(", "x", "[", "3", "]", ")", ",", "\n", "int", "(", "x", "[", "4", "]", ")", ",", "\n", "num_frames", ",", "\n", "label", "=", "int", "(", "x", "[", "0", "]", ")", ",", "\n", "best_iou", "=", "float", "(", "x", "[", "1", "]", ")", ",", "\n", "overlap_self", "=", "float", "(", "x", "[", "2", "]", ")", ")", "\n", "proposals", ".", "append", "(", "ssn_instance", ")", "\n", "", "", "video_infos", ".", "append", "(", "\n", "dict", "(", "\n", "frame_dir", "=", "frame_dir", ",", "\n", "video_id", "=", "proposal_info", "[", "0", "]", ",", "\n", "total_frames", "=", "num_frames", ",", "\n", "gts", "=", "gts", ",", "\n", "proposals", "=", "proposals", ")", ")", "\n", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.results_to_detections": [[331, 394], ["range", "dict", "len", "numpy.zeros.reshape", "range", "len", "numpy.squeeze", "numpy.zeros", "range", "core.softmax", "numpy.exp", "numpy.concatenate", "core.softmax", "numpy.exp", "numpy.argsort", "len", "combined_scores.ravel", "numpy.array", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "results_to_detections", "(", "self", ",", "results", ",", "top_k", "=", "2000", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Convert prediction results into detections.\n\n        Args:\n            results (list): Prediction results.\n            top_k (int): Number of top results. Default: 2000.\n\n        Returns:\n            list: Detection results.\n        \"\"\"", "\n", "num_classes", "=", "results", "[", "0", "]", "[", "'activity_scores'", "]", ".", "shape", "[", "1", "]", "-", "1", "\n", "detections", "=", "[", "dict", "(", ")", "for", "_", "in", "range", "(", "num_classes", ")", "]", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ")", ")", ":", "\n", "            ", "video_id", "=", "self", ".", "video_infos", "[", "idx", "]", "[", "'video_id'", "]", "\n", "relative_proposals", "=", "results", "[", "idx", "]", "[", "'relative_proposal_list'", "]", "\n", "if", "len", "(", "relative_proposals", "[", "0", "]", ".", "shape", ")", "==", "3", ":", "\n", "                ", "relative_proposals", "=", "np", ".", "squeeze", "(", "relative_proposals", ",", "0", ")", "\n", "\n", "", "activity_scores", "=", "results", "[", "idx", "]", "[", "'activity_scores'", "]", "\n", "completeness_scores", "=", "results", "[", "idx", "]", "[", "'completeness_scores'", "]", "\n", "regression_scores", "=", "results", "[", "idx", "]", "[", "'bbox_preds'", "]", "\n", "if", "regression_scores", "is", "None", ":", "\n", "                ", "regression_scores", "=", "np", ".", "zeros", "(", "\n", "(", "len", "(", "relative_proposals", ")", ",", "num_classes", ",", "2", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "", "regression_scores", "=", "regression_scores", ".", "reshape", "(", "(", "-", "1", ",", "num_classes", ",", "2", ")", ")", "\n", "\n", "if", "top_k", "<=", "0", ":", "\n", "                ", "combined_scores", "=", "(", "\n", "softmax", "(", "activity_scores", "[", ":", ",", "1", ":", "]", ",", "dim", "=", "1", ")", "*", "\n", "np", ".", "exp", "(", "completeness_scores", ")", ")", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "                    ", "center_scores", "=", "regression_scores", "[", ":", ",", "i", ",", "0", "]", "[", ":", ",", "None", "]", "\n", "duration_scores", "=", "regression_scores", "[", ":", ",", "i", ",", "1", "]", "[", ":", ",", "None", "]", "\n", "detections", "[", "i", "]", "[", "video_id", "]", "=", "np", ".", "concatenate", "(", "\n", "(", "relative_proposals", ",", "combined_scores", "[", ":", ",", "i", "]", "[", ":", ",", "None", "]", ",", "\n", "center_scores", ",", "duration_scores", ")", ",", "\n", "axis", "=", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "combined_scores", "=", "(", "\n", "softmax", "(", "activity_scores", "[", ":", ",", "1", ":", "]", ",", "dim", "=", "1", ")", "*", "\n", "np", ".", "exp", "(", "completeness_scores", ")", ")", "\n", "keep_idx", "=", "np", ".", "argsort", "(", "combined_scores", ".", "ravel", "(", ")", ")", "[", "-", "top_k", ":", "]", "\n", "for", "k", "in", "keep_idx", ":", "\n", "                    ", "class_idx", "=", "k", "%", "num_classes", "\n", "proposal_idx", "=", "k", "//", "num_classes", "\n", "new_item", "=", "[", "\n", "relative_proposals", "[", "proposal_idx", ",", "0", "]", ",", "\n", "relative_proposals", "[", "proposal_idx", ",", "\n", "1", "]", ",", "combined_scores", "[", "proposal_idx", ",", "\n", "class_idx", "]", ",", "\n", "regression_scores", "[", "proposal_idx", ",", "class_idx", ",", "\n", "0", "]", ",", "regression_scores", "[", "proposal_idx", ",", "\n", "class_idx", ",", "1", "]", "\n", "]", "\n", "if", "video_id", "not", "in", "detections", "[", "class_idx", "]", ":", "\n", "                        ", "detections", "[", "class_idx", "]", "[", "video_id", "]", "=", "np", ".", "array", "(", "[", "new_item", "]", ")", "\n", "", "else", ":", "\n", "                        ", "detections", "[", "class_idx", "]", "[", "video_id", "]", "=", "np", ".", "vstack", "(", "\n", "[", "detections", "[", "class_idx", "]", "[", "video_id", "]", ",", "new_item", "]", ")", "\n", "\n", "", "", "", "", "return", "detections", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.evaluate": [[395, 490], ["dict", "copy.deepcopy", "ssn_dataset.SSNDataset.results_to_detections", "ssn_dataset.SSNDataset.logger.info", "enumerate", "ssn_dataset.SSNDataset.logger.info", "ssn_dataset.SSNDataset.get_all_gts", "enumerate", "enumerate", "collections.OrderedDict", "warnings.warn", "dict", "isinstance", "TypeError", "len", "len", "isinstance", "ssn_dataset.SSNDataset.logger.info", "enumerate", "ssn_dataset.SSNDataset.logger.info", "detections[].items", "dict", "len", "len", "KeyError", "temporal_nms", "dict", "detection_list.extend", "copy.deepcopy.setdefault().setdefault", "perform_regression", "detections[].items", "numpy.arange", "eval_ap", "eval_ap.mean", "ssn_dataset.SSNDataset.logger.info", "zip", "type", "detections[].items", "copy.deepcopy.setdefault", "dets.tolist"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.results_to_detections", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_all_gts"], ["", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'mAP'", ",", "\n", "metric_options", "=", "dict", "(", "mAP", "=", "dict", "(", "eval_dataset", "=", "'thumos14'", ")", ")", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        ", "\"\"\"Evaluation in SSN proposal dataset.\n\n        Args:\n            results (list[dict]): Output results.\n            metrics (str | sequence[str]): Metrics to be performed.\n                Defaults: 'mAP'.\n            metric_options (dict): Dict for metric options. Options are\n                ``eval_dataset`` for ``mAP``.\n                Default: ``dict(mAP=dict(eval_dataset='thumos14'))``.\n            logger (logging.Logger | None): Logger for recording.\n                Default: None.\n            deprecated_kwargs (dict): Used for containing deprecated arguments.\n                See 'https://github.com/open-mmlab/mmaction2/pull/286'.\n\n        Returns:\n            dict: Evaluation results for evaluation metrics.\n        \"\"\"", "\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n", "\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'mAP'", "]", "=", "dict", "(", "metric_options", "[", "'mAP'", "]", ",", "\n", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "allowed_metrics", "=", "[", "'mAP'", "]", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "allowed_metrics", ":", "\n", "                ", "raise", "KeyError", "(", "f'metric {metric} is not supported'", ")", "\n", "\n", "", "", "detections", "=", "self", ".", "results_to_detections", "(", "results", ",", "**", "self", ".", "evaluater", ")", "\n", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Performing location regression'", ")", "\n", "for", "class_idx", ",", "_", "in", "enumerate", "(", "detections", ")", ":", "\n", "                ", "detections", "[", "class_idx", "]", "=", "{", "\n", "k", ":", "perform_regression", "(", "v", ")", "\n", "for", "k", ",", "v", "in", "detections", "[", "class_idx", "]", ".", "items", "(", ")", "\n", "}", "\n", "", "self", ".", "logger", ".", "info", "(", "'Regression finished'", ")", "\n", "\n", "", "self", ".", "logger", ".", "info", "(", "'Performing NMS'", ")", "\n", "for", "class_idx", ",", "_", "in", "enumerate", "(", "detections", ")", ":", "\n", "            ", "detections", "[", "class_idx", "]", "=", "{", "\n", "k", ":", "temporal_nms", "(", "v", ",", "self", ".", "evaluater", ".", "nms", ")", "\n", "for", "k", ",", "v", "in", "detections", "[", "class_idx", "]", ".", "items", "(", ")", "\n", "}", "\n", "", "self", ".", "logger", ".", "info", "(", "'NMS finished'", ")", "\n", "\n", "# get gts", "\n", "all_gts", "=", "self", ".", "get_all_gts", "(", ")", "\n", "for", "class_idx", ",", "_", "in", "enumerate", "(", "detections", ")", ":", "\n", "            ", "if", "class_idx", "not", "in", "all_gts", ":", "\n", "                ", "all_gts", "[", "class_idx", "]", "=", "dict", "(", ")", "\n", "\n", "# get predictions", "\n", "", "", "plain_detections", "=", "{", "}", "\n", "for", "class_idx", ",", "_", "in", "enumerate", "(", "detections", ")", ":", "\n", "            ", "detection_list", "=", "[", "]", "\n", "for", "video", ",", "dets", "in", "detections", "[", "class_idx", "]", ".", "items", "(", ")", ":", "\n", "                ", "detection_list", ".", "extend", "(", "[", "[", "video", ",", "class_idx", "]", "+", "x", "[", ":", "3", "]", "\n", "for", "x", "in", "dets", ".", "tolist", "(", ")", "]", ")", "\n", "", "plain_detections", "[", "class_idx", "]", "=", "detection_list", "\n", "\n", "", "eval_results", "=", "OrderedDict", "(", ")", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "==", "'mAP'", ":", "\n", "                ", "eval_dataset", "=", "metric_options", ".", "setdefault", "(", "'mAP'", ",", "{", "}", ")", ".", "setdefault", "(", "\n", "'eval_dataset'", ",", "'thumos14'", ")", "\n", "if", "eval_dataset", "==", "'thumos14'", ":", "\n", "                    ", "iou_range", "=", "np", ".", "arange", "(", "0.1", ",", "1.0", ",", ".1", ")", "\n", "ap_values", "=", "eval_ap", "(", "plain_detections", ",", "all_gts", ",", "iou_range", ")", "\n", "map_ious", "=", "ap_values", ".", "mean", "(", "axis", "=", "0", ")", "\n", "self", ".", "logger", ".", "info", "(", "'Evaluation finished'", ")", "\n", "\n", "for", "iou", ",", "map_iou", "in", "zip", "(", "iou_range", ",", "map_ious", ")", ":", "\n", "                        ", "eval_results", "[", "f'mAP@{iou:.02f}'", "]", "=", "map_iou", "\n", "\n", "", "", "", "", "return", "eval_results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.construct_proposal_pools": [[491, 512], ["ssn_dataset.SSNDataset.get_positives", "ssn_dataset.SSNDataset.positive_pool.extend", "ssn_dataset.SSNDataset.get_negatives", "ssn_dataset.SSNDataset.incomplete_pool.extend", "ssn_dataset.SSNDataset.background_pool.extend"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_positives", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_negatives"], ["", "def", "construct_proposal_pools", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct positive proposal pool, incomplete proposal pool and\n        background proposal pool of the entire dataset.\"\"\"", "\n", "for", "video_info", "in", "self", ".", "video_infos", ":", "\n", "            ", "positives", "=", "self", ".", "get_positives", "(", "\n", "video_info", "[", "'gts'", "]", ",", "video_info", "[", "'proposals'", "]", ",", "\n", "self", ".", "assigner", ".", "positive_iou_threshold", ",", "\n", "self", ".", "sampler", ".", "add_gt_as_proposals", ")", "\n", "self", ".", "positive_pool", ".", "extend", "(", "[", "(", "video_info", "[", "'video_id'", "]", ",", "proposal", ")", "\n", "for", "proposal", "in", "positives", "]", ")", "\n", "\n", "incompletes", ",", "backgrounds", "=", "self", ".", "get_negatives", "(", "\n", "video_info", "[", "'proposals'", "]", ",", "\n", "self", ".", "assigner", ".", "incomplete_iou_threshold", ",", "\n", "self", ".", "assigner", ".", "background_iou_threshold", ",", "\n", "self", ".", "assigner", ".", "background_coverage_threshold", ",", "\n", "self", ".", "assigner", ".", "incomplete_overlap_threshold", ")", "\n", "self", ".", "incomplete_pool", ".", "extend", "(", "[", "(", "video_info", "[", "'video_id'", "]", ",", "proposal", ")", "\n", "for", "proposal", "in", "incompletes", "]", ")", "\n", "self", ".", "background_pool", ".", "extend", "(", "[", "video_info", "[", "'video_id'", "]", ",", "proposal", "]", "\n", "for", "proposal", "in", "backgrounds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_all_gts": [[513, 529], ["gts.setdefault().setdefault().append", "gts.setdefault().setdefault", "gts.setdefault"], "methods", ["None"], ["", "", "def", "get_all_gts", "(", "self", ")", ":", "\n", "        ", "\"\"\"Fetch groundtruth instances of the entire dataset.\"\"\"", "\n", "gts", "=", "{", "}", "\n", "for", "video_info", "in", "self", ".", "video_infos", ":", "\n", "            ", "video", "=", "video_info", "[", "'video_id'", "]", "\n", "for", "gt", "in", "video_info", "[", "'gts'", "]", ":", "\n", "                ", "class_idx", "=", "gt", ".", "label", "-", "1", "\n", "# gt_info: [relative_start, relative_end]", "\n", "gt_info", "=", "[", "\n", "gt", ".", "start_frame", "/", "video_info", "[", "'total_frames'", "]", ",", "\n", "gt", ".", "end_frame", "/", "video_info", "[", "'total_frames'", "]", "\n", "]", "\n", "gts", ".", "setdefault", "(", "class_idx", ",", "{", "}", ")", ".", "setdefault", "(", "video", ",", "\n", "[", "]", ")", ".", "append", "(", "gt_info", ")", "\n", "\n", "", "", "return", "gts", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_positives": [[530, 558], ["positives.extend", "proposal.compute_regression_targets"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNInstance.compute_regression_targets"], ["", "@", "staticmethod", "\n", "def", "get_positives", "(", "gts", ",", "proposals", ",", "positive_threshold", ",", "with_gt", "=", "True", ")", ":", "\n", "        ", "\"\"\"Get positive/foreground proposals.\n\n        Args:\n            gts (list): List of groundtruth instances(:obj:`SSNInstance`).\n            proposals (list): List of proposal instances(:obj:`SSNInstance`).\n            positive_threshold (float): Minimum threshold of overlap of\n                positive/foreground proposals and groundtruths.\n            with_gt (bool): Whether to include groundtruth instances in\n                positive proposals. Default: True.\n\n        Returns:\n            list[:obj:`SSNInstance`]: (positives), positives is a list\n                comprised of positive proposal instances.\n        \"\"\"", "\n", "positives", "=", "[", "\n", "proposal", "for", "proposal", "in", "proposals", "\n", "if", "proposal", ".", "best_iou", ">", "positive_threshold", "\n", "]", "\n", "\n", "if", "with_gt", ":", "\n", "            ", "positives", ".", "extend", "(", "gts", ")", "\n", "\n", "", "for", "proposal", "in", "positives", ":", "\n", "            ", "proposal", ".", "compute_regression_targets", "(", "gts", ")", "\n", "\n", "", "return", "positives", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_negatives": [[559, 597], ["incompletes.append", "backgrounds.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_negatives", "(", "proposals", ",", "\n", "incomplete_iou_threshold", ",", "\n", "background_iou_threshold", ",", "\n", "background_coverage_threshold", "=", "0.01", ",", "\n", "incomplete_overlap_threshold", "=", "0.7", ")", ":", "\n", "        ", "\"\"\"Get negative proposals, including incomplete proposals and\n        background proposals.\n\n        Args:\n            proposals (list): List of proposal instances(:obj:`SSNInstance`).\n            incomplete_iou_threshold (float): Maximum threshold of overlap\n                of incomplete proposals and groundtruths.\n            background_iou_threshold (float): Maximum threshold of overlap\n                of background proposals and groundtruths.\n            background_coverage_threshold (float): Minimum coverage\n                of background proposals in video duration. Default: 0.01.\n            incomplete_overlap_threshold (float): Minimum percent of incomplete\n                proposals' own span contained in a groundtruth instance.\n                Default: 0.7.\n\n        Returns:\n            list[:obj:`SSNInstance`]: (incompletes, backgrounds), incompletes\n                and backgrounds are lists comprised of incomplete\n                proposal instances and background proposal instances.\n        \"\"\"", "\n", "incompletes", "=", "[", "]", "\n", "backgrounds", "=", "[", "]", "\n", "\n", "for", "proposal", "in", "proposals", ":", "\n", "            ", "if", "(", "proposal", ".", "best_iou", "<", "incomplete_iou_threshold", "\n", "and", "proposal", ".", "overlap_self", ">", "incomplete_overlap_threshold", ")", ":", "\n", "                ", "incompletes", ".", "append", "(", "proposal", ")", "\n", "", "elif", "(", "proposal", ".", "best_iou", "<", "background_iou_threshold", "\n", "and", "proposal", ".", "coverage", ">", "background_coverage_threshold", ")", ":", "\n", "                ", "backgrounds", ".", "append", "(", "proposal", ")", "\n", "\n", "", "", "return", "incompletes", ",", "backgrounds", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._video_centric_sampling": [[598, 666], ["ssn_dataset.SSNDataset.get_positives", "ssn_dataset.SSNDataset.get_negatives", "out_proposals.extend", "out_proposals.extend", "out_proposals.extend", "numpy.random.choice", "ssn_dataset.SSNDataset._video_centric_sampling.sample_video_proposals"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_positives", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_negatives"], ["", "def", "_video_centric_sampling", "(", "self", ",", "record", ")", ":", "\n", "        ", "\"\"\"Sample proposals from the this video instance.\n\n        Args:\n            record (dict): Information of the video instance(video_info[idx]).\n                key: frame_dir, video_id, total_frames,\n                gts: List of groundtruth instances(:obj:`SSNInstance`).\n                proposals: List of proposal instances(:obj:`SSNInstance`).\n        \"\"\"", "\n", "positives", "=", "self", ".", "get_positives", "(", "record", "[", "'gts'", "]", ",", "record", "[", "'proposals'", "]", ",", "\n", "self", ".", "assigner", ".", "positive_iou_threshold", ",", "\n", "self", ".", "sampler", ".", "add_gt_as_proposals", ")", "\n", "incompletes", ",", "backgrounds", "=", "self", ".", "get_negatives", "(", "\n", "record", "[", "'proposals'", "]", ",", "self", ".", "assigner", ".", "incomplete_iou_threshold", ",", "\n", "self", ".", "assigner", ".", "background_iou_threshold", ",", "\n", "self", ".", "assigner", ".", "background_coverage_threshold", ",", "\n", "self", ".", "assigner", ".", "incomplete_overlap_threshold", ")", "\n", "\n", "def", "sample_video_proposals", "(", "proposal_type", ",", "video_id", ",", "video_pool", ",", "\n", "num_requested_proposals", ",", "dataset_pool", ")", ":", "\n", "            ", "\"\"\"This method will sample proposals from the this video pool. If\n            the video pool is empty, it will fetch from the dataset pool\n            (collect proposal of the entire dataset).\n\n            Args:\n                proposal_type (int): Type id of proposal.\n                    Positive/Foreground: 0\n                    Negative:\n                        Incomplete: 1\n                        Background: 2\n                video_id (str): Name of the video.\n                video_pool (list): Pool comprised of proposals in this video.\n                num_requested_proposals (int): Number of proposals\n                    to be sampled.\n                dataset_pool (list): Proposals of the entire dataset.\n\n            Returns:\n                list[(str, :obj:`SSNInstance`), int]:\n                    video_id (str): Name of the video.\n                    :obj:`SSNInstance`: Instance of class SSNInstance.\n                    proposal_type (int): Type of proposal.\n            \"\"\"", "\n", "\n", "if", "len", "(", "video_pool", ")", "==", "0", ":", "\n", "                ", "idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "dataset_pool", ")", ",", "num_requested_proposals", ",", "replace", "=", "False", ")", "\n", "return", "[", "(", "dataset_pool", "[", "x", "]", ",", "proposal_type", ")", "for", "x", "in", "idx", "]", "\n", "\n", "", "replicate", "=", "len", "(", "video_pool", ")", "<", "num_requested_proposals", "\n", "idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "video_pool", ")", ",", "num_requested_proposals", ",", "replace", "=", "replicate", ")", "\n", "return", "[", "(", "(", "video_id", ",", "video_pool", "[", "x", "]", ")", ",", "proposal_type", ")", "for", "x", "in", "idx", "]", "\n", "\n", "", "out_proposals", "=", "[", "]", "\n", "out_proposals", ".", "extend", "(", "\n", "sample_video_proposals", "(", "0", ",", "record", "[", "'video_id'", "]", ",", "positives", ",", "\n", "self", ".", "positive_per_video", ",", "\n", "self", ".", "positive_pool", ")", ")", "\n", "out_proposals", ".", "extend", "(", "\n", "sample_video_proposals", "(", "1", ",", "record", "[", "'video_id'", "]", ",", "incompletes", ",", "\n", "self", ".", "incomplete_per_video", ",", "\n", "self", ".", "incomplete_pool", ")", ")", "\n", "out_proposals", ".", "extend", "(", "\n", "sample_video_proposals", "(", "2", ",", "record", "[", "'video_id'", "]", ",", "backgrounds", ",", "\n", "self", ".", "background_per_video", ",", "\n", "self", ".", "background_pool", ")", ")", "\n", "\n", "return", "out_proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._random_sampling": [[667, 691], ["numpy.random.choice", "out_proposals.extend", "numpy.random.choice", "out_proposals.extend", "numpy.random.choice", "out_proposals.extend", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_random_sampling", "(", "self", ")", ":", "\n", "        ", "\"\"\"Randomly sample proposals from the entire dataset.\"\"\"", "\n", "out_proposals", "=", "[", "]", "\n", "\n", "positive_idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "positive_pool", ")", ",", "\n", "self", ".", "positive_per_video", ",", "\n", "replace", "=", "len", "(", "self", ".", "positive_pool", ")", "<", "self", ".", "positive_per_video", ")", "\n", "out_proposals", ".", "extend", "(", "[", "(", "self", ".", "positive_pool", "[", "x", "]", ",", "0", ")", "\n", "for", "x", "in", "positive_idx", "]", ")", "\n", "incomplete_idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "incomplete_pool", ")", ",", "\n", "self", ".", "incomplete_per_video", ",", "\n", "replace", "=", "len", "(", "self", ".", "incomplete_pool", ")", "<", "self", ".", "incomplete_per_video", ")", "\n", "out_proposals", ".", "extend", "(", "[", "(", "self", ".", "incomplete_pool", "[", "x", "]", ",", "1", ")", "\n", "for", "x", "in", "incomplete_idx", "]", ")", "\n", "background_idx", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "background_pool", ")", ",", "\n", "self", ".", "background_per_video", ",", "\n", "replace", "=", "len", "(", "self", ".", "background_pool", ")", "<", "self", ".", "background_per_video", ")", "\n", "out_proposals", ".", "extend", "(", "[", "(", "self", ".", "background_pool", "[", "x", "]", ",", "2", ")", "\n", "for", "x", "in", "background_idx", "]", ")", "\n", "\n", "return", "out_proposals", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._get_stage": [[692, 737], ["max", "min", "int", "int"], "methods", ["None"], ["", "def", "_get_stage", "(", "self", ",", "proposal", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Fetch the scale factor of starting and ending stage and get the\n        stage split.\n\n        Args:\n            proposal (:obj:`SSNInstance`): Proposal instance.\n            num_frames (int): Total frames of the video.\n\n        Returns:\n            tuple[float, float, list]: (starting_scale_factor,\n                ending_scale_factor, stage_split), starting_scale_factor is\n                the ratio of the effective sampling length to augment length\n                in starting stage, ending_scale_factor is the ratio of the\n                effective sampling length to augment length in ending stage,\n                stage_split is  ending segment id of starting, course and\n                ending stage.\n        \"\"\"", "\n", "# proposal interval: [start_frame, end_frame)", "\n", "start_frame", "=", "proposal", ".", "start_frame", "\n", "end_frame", "=", "proposal", ".", "end_frame", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "\n", "duration", "=", "end_frame", "-", "start_frame", "\n", "assert", "duration", "!=", "0", "\n", "\n", "valid_starting", "=", "max", "(", "0", ",", "\n", "start_frame", "-", "int", "(", "duration", "*", "self", ".", "aug_ratio", "[", "0", "]", ")", ")", "\n", "valid_ending", "=", "min", "(", "num_frames", "-", "ori_clip_len", "+", "1", ",", "\n", "end_frame", "-", "1", "+", "int", "(", "duration", "*", "self", ".", "aug_ratio", "[", "1", "]", ")", ")", "\n", "\n", "valid_starting_length", "=", "start_frame", "-", "valid_starting", "-", "ori_clip_len", "\n", "valid_ending_length", "=", "(", "valid_ending", "-", "end_frame", "+", "1", ")", "-", "ori_clip_len", "\n", "\n", "starting_scale_factor", "=", "(", "(", "valid_starting_length", "+", "ori_clip_len", "+", "1", ")", "/", "\n", "(", "duration", "*", "self", ".", "aug_ratio", "[", "0", "]", ")", ")", "\n", "ending_scale_factor", "=", "(", "valid_ending_length", "+", "ori_clip_len", "+", "1", ")", "/", "(", "\n", "duration", "*", "self", ".", "aug_ratio", "[", "1", "]", ")", "\n", "\n", "aug_start", ",", "aug_end", "=", "self", ".", "aug_segments", "\n", "stage_split", "=", "[", "\n", "aug_start", ",", "aug_start", "+", "self", ".", "body_segments", ",", "\n", "aug_start", "+", "self", ".", "body_segments", "+", "aug_end", "\n", "]", "\n", "\n", "return", "starting_scale_factor", ",", "ending_scale_factor", ",", "stage_split", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._compute_reg_normalize_constants": [[738, 751], ["numpy.array", "ssn_dataset.SSNDataset.logger.info", "ssn_dataset.SSNDataset.get_positives", "targets.append", "numpy.mean", "numpy.std", "list"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.get_positives"], ["", "def", "_compute_reg_normalize_constants", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute regression target normalized constants.\"\"\"", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Compute regression target normalized constants'", ")", "\n", "", "targets", "=", "[", "]", "\n", "for", "video_info", "in", "self", ".", "video_infos", ":", "\n", "            ", "positives", "=", "self", ".", "get_positives", "(", "\n", "video_info", "[", "'gts'", "]", ",", "video_info", "[", "'proposals'", "]", ",", "\n", "self", ".", "assigner", ".", "positive_iou_threshold", ",", "False", ")", "\n", "for", "positive", "in", "positives", ":", "\n", "                ", "targets", ".", "append", "(", "list", "(", "positive", ".", "regression_targets", ")", ")", "\n", "\n", "", "", "return", "np", ".", "array", "(", "(", "np", ".", "mean", "(", "targets", ",", "axis", "=", "0", ")", ",", "np", ".", "std", "(", "targets", ",", "axis", "=", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.prepare_train_frames": [[752, 821], ["copy.deepcopy", "enumerate", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "ssn_dataset.SSNDataset.pipeline", "ssn_dataset.SSNDataset._video_centric_sampling", "ssn_dataset.SSNDataset._random_sampling", "ssn_dataset.SSNDataset._get_stage", "out_proposal_scale_factor.append", "out_proposal_labels.append", "out_proposal_type.append", "out_proposal_reg_targets.append", "isinstance", "TypeError", "ValueError", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._video_centric_sampling", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._random_sampling", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset._get_stage"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "if", "self", ".", "video_centric", ":", "\n", "# yapf: disable", "\n", "            ", "results", "[", "'out_proposals'", "]", "=", "self", ".", "_video_centric_sampling", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "# noqa: E501", "\n", "# yapf: enable", "\n", "", "else", ":", "\n", "            ", "results", "[", "'out_proposals'", "]", "=", "self", ".", "_random_sampling", "(", ")", "\n", "\n", "", "out_proposal_scale_factor", "=", "[", "]", "\n", "out_proposal_type", "=", "[", "]", "\n", "out_proposal_labels", "=", "[", "]", "\n", "out_proposal_reg_targets", "=", "[", "]", "\n", "\n", "for", "_", ",", "proposal", "in", "enumerate", "(", "results", "[", "'out_proposals'", "]", ")", ":", "\n", "# proposal: [(video_id, SSNInstance), proposal_type]", "\n", "            ", "num_frames", "=", "proposal", "[", "0", "]", "[", "1", "]", ".", "num_video_frames", "\n", "\n", "(", "starting_scale_factor", ",", "ending_scale_factor", ",", "\n", "_", ")", "=", "self", ".", "_get_stage", "(", "proposal", "[", "0", "]", "[", "1", "]", ",", "num_frames", ")", "\n", "\n", "# proposal[1]: Type id of proposal.", "\n", "# Positive/Foreground: 0", "\n", "# Negative:", "\n", "#   Incomplete: 1", "\n", "#   Background: 2", "\n", "\n", "# Positivte/Foreground proposal", "\n", "if", "proposal", "[", "1", "]", "==", "0", ":", "\n", "                ", "label", "=", "proposal", "[", "0", "]", "[", "1", "]", ".", "label", "\n", "# Incomplete proposal", "\n", "", "elif", "proposal", "[", "1", "]", "==", "1", ":", "\n", "                ", "label", "=", "proposal", "[", "0", "]", "[", "1", "]", ".", "label", "\n", "# Background proposal", "\n", "", "elif", "proposal", "[", "1", "]", "==", "2", ":", "\n", "                ", "label", "=", "0", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f'Proposal type should be 0, 1, or 2,'", "\n", "f'but got {proposal[1]}'", ")", "\n", "", "out_proposal_scale_factor", ".", "append", "(", "\n", "[", "starting_scale_factor", ",", "ending_scale_factor", "]", ")", "\n", "if", "not", "isinstance", "(", "label", ",", "int", ")", ":", "\n", "                ", "raise", "TypeError", "(", "f'proposal_label must be an int,'", "\n", "f'but got {type(label)}'", ")", "\n", "", "out_proposal_labels", ".", "append", "(", "label", ")", "\n", "out_proposal_type", ".", "append", "(", "proposal", "[", "1", "]", ")", "\n", "\n", "reg_targets", "=", "proposal", "[", "0", "]", "[", "1", "]", ".", "regression_targets", "\n", "if", "proposal", "[", "1", "]", "==", "0", ":", "\n", "# Normalize regression targets of positive proposals.", "\n", "                ", "reg_targets", "=", "(", "(", "reg_targets", "[", "0", "]", "-", "self", ".", "reg_norm_consts", "[", "0", "]", "[", "0", "]", ")", "/", "\n", "self", ".", "reg_norm_consts", "[", "1", "]", "[", "0", "]", ",", "\n", "(", "reg_targets", "[", "1", "]", "-", "self", ".", "reg_norm_consts", "[", "0", "]", "[", "1", "]", ")", "/", "\n", "self", ".", "reg_norm_consts", "[", "1", "]", "[", "1", "]", ")", "\n", "", "out_proposal_reg_targets", ".", "append", "(", "reg_targets", ")", "\n", "\n", "", "results", "[", "'reg_targets'", "]", "=", "np", ".", "array", "(", "\n", "out_proposal_reg_targets", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "results", "[", "'proposal_scale_factor'", "]", "=", "np", ".", "array", "(", "\n", "out_proposal_scale_factor", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "results", "[", "'proposal_labels'", "]", "=", "np", ".", "array", "(", "out_proposal_labels", ")", "\n", "results", "[", "'proposal_type'", "]", "=", "np", ".", "array", "(", "out_proposal_type", ")", "\n", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ssn_dataset.SSNDataset.prepare_test_frames": [[822, 883], ["copy.deepcopy", "len", "numpy.array", "numpy.array", "numpy.array", "ssn_dataset.SSNDataset.pipeline", "numpy.arange", "len", "proposals.append", "max", "min", "relative_proposal_list.append", "proposal_tick_list.append", "scale_factor_list.append", "ssn_dataset.SSNInstance", "numpy.array"], "methods", ["None"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "proposals", "=", "results", "[", "'proposals'", "]", "\n", "num_frames", "=", "results", "[", "'total_frames'", "]", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "frame_ticks", "=", "np", ".", "arange", "(", "\n", "0", ",", "num_frames", "-", "ori_clip_len", ",", "self", ".", "test_interval", ",", "dtype", "=", "int", ")", "+", "1", "\n", "\n", "num_sampled_frames", "=", "len", "(", "frame_ticks", ")", "\n", "\n", "if", "len", "(", "proposals", ")", "==", "0", ":", "\n", "            ", "proposals", ".", "append", "(", "SSNInstance", "(", "0", ",", "num_frames", "-", "1", ",", "num_frames", ")", ")", "\n", "\n", "", "relative_proposal_list", "=", "[", "]", "\n", "proposal_tick_list", "=", "[", "]", "\n", "scale_factor_list", "=", "[", "]", "\n", "\n", "for", "proposal", "in", "proposals", ":", "\n", "            ", "relative_proposal", "=", "(", "proposal", ".", "start_frame", "/", "num_frames", ",", "\n", "proposal", ".", "end_frame", "/", "num_frames", ")", "\n", "relative_duration", "=", "relative_proposal", "[", "1", "]", "-", "relative_proposal", "[", "0", "]", "\n", "relative_starting_duration", "=", "relative_duration", "*", "self", ".", "aug_ratio", "[", "0", "]", "\n", "relative_ending_duration", "=", "relative_duration", "*", "self", ".", "aug_ratio", "[", "1", "]", "\n", "relative_starting", "=", "(", "\n", "relative_proposal", "[", "0", "]", "-", "relative_starting_duration", ")", "\n", "relative_ending", "=", "relative_proposal", "[", "1", "]", "+", "relative_ending_duration", "\n", "\n", "real_relative_starting", "=", "max", "(", "0.0", ",", "relative_starting", ")", "\n", "real_relative_ending", "=", "min", "(", "1.0", ",", "relative_ending", ")", "\n", "\n", "starting_scale_factor", "=", "(", "\n", "(", "relative_proposal", "[", "0", "]", "-", "real_relative_starting", ")", "/", "\n", "relative_starting_duration", ")", "\n", "ending_scale_factor", "=", "(", "\n", "(", "real_relative_ending", "-", "relative_proposal", "[", "1", "]", ")", "/", "\n", "relative_ending_duration", ")", "\n", "\n", "proposal_ranges", "=", "(", "real_relative_starting", ",", "*", "relative_proposal", ",", "\n", "real_relative_ending", ")", "\n", "proposal_ticks", "=", "(", "np", ".", "array", "(", "proposal_ranges", ")", "*", "\n", "num_sampled_frames", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "\n", "relative_proposal_list", ".", "append", "(", "relative_proposal", ")", "\n", "proposal_tick_list", ".", "append", "(", "proposal_ticks", ")", "\n", "scale_factor_list", ".", "append", "(", "\n", "(", "starting_scale_factor", ",", "ending_scale_factor", ")", ")", "\n", "\n", "", "results", "[", "'relative_proposal_list'", "]", "=", "np", ".", "array", "(", "\n", "relative_proposal_list", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "results", "[", "'scale_factor_list'", "]", "=", "np", ".", "array", "(", "\n", "scale_factor_list", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "results", "[", "'proposal_tick_list'", "]", "=", "np", ".", "array", "(", "\n", "proposal_tick_list", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "results", "[", "'reg_norm_consts'", "]", "=", "self", ".", "reg_norm_consts", "\n", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.__init__": [[59, 74], ["base.BaseDataset.__init__", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "clipname_tmpl", "=", "'part_{}.mp4'", ",", "\n", "sampling_strategy", "=", "'positive'", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "start_index", "=", "0", ",", "**", "kwargs", ")", "\n", "assert", "self", ".", "multi_class", "is", "False", "\n", "self", ".", "sampling_strategy", "=", "sampling_strategy", "\n", "self", ".", "clipname_tmpl", "=", "clipname_tmpl", "\n", "# If positive, we should only keep those raw videos with positive", "\n", "# clips", "\n", "if", "self", ".", "sampling_strategy", "==", "'positive'", ":", "\n", "            ", "self", ".", "video_infos", "=", "[", "\n", "x", "for", "x", "in", "self", ".", "video_infos", "if", "len", "(", "x", "[", "'positive_clip_inds'", "]", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_annotations": [[77, 100], ["rawvideo_dataset.RawVideoDataset.ann_file.endswith", "rawvideo_dataset.RawVideoDataset.load_json_annotations", "open", "line.strip().split", "int", "int", "video_infos.append", "int", "os.join", "dict", "line.strip"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "if", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", ":", "\n", "            ", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "\n", "", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "video_dir", "=", "line_split", "[", "0", "]", "\n", "label", "=", "int", "(", "line_split", "[", "1", "]", ")", "\n", "num_clips", "=", "int", "(", "line_split", "[", "2", "]", ")", "\n", "positive_clip_inds", "=", "[", "int", "(", "ind", ")", "for", "ind", "in", "line_split", "[", "3", ":", "]", "]", "\n", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "video_dir", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "video_dir", ")", "\n", "", "video_infos", ".", "append", "(", "\n", "dict", "(", "\n", "video_dir", "=", "video_dir", ",", "\n", "label", "=", "label", ",", "\n", "num_clips", "=", "num_clips", ",", "\n", "positive_clip_inds", "=", "positive_clip_inds", ")", ")", "\n", "", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations": [[102, 113], ["mmcv.load", "len", "range", "os.join"], "methods", ["None"], ["", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "path_key", "=", "'video_dir'", "\n", "for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n", "", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.sample_clip": [[114, 132], ["rawvideo_dataset.RawVideoDataset.clipname_tmpl.format", "rawvideo_dataset.RawVideoDataset.clipname_tmpl[].isalpha", "random.choice", "random.randint", "os.join"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format"], ["", "def", "sample_clip", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Sample a clip from the raw video given the sampling strategy.\"\"\"", "\n", "assert", "self", ".", "sampling_strategy", "in", "[", "'positive'", ",", "'random'", "]", "\n", "if", "self", ".", "sampling_strategy", "==", "'positive'", ":", "\n", "            ", "assert", "results", "[", "'positive_clip_inds'", "]", "\n", "ind", "=", "random", ".", "choice", "(", "results", "[", "'positive_clip_inds'", "]", ")", "\n", "", "else", ":", "\n", "            ", "ind", "=", "random", ".", "randint", "(", "0", ",", "results", "[", "'num_clips'", "]", "-", "1", ")", "\n", "", "clipname", "=", "self", ".", "clipname_tmpl", ".", "format", "(", "ind", ")", "\n", "\n", "# if the first char of self.clipname_tmpl is a letter, use osp.join;", "\n", "# otherwise, directly concat them", "\n", "if", "self", ".", "clipname_tmpl", "[", "0", "]", ".", "isalpha", "(", ")", ":", "\n", "            ", "filename", "=", "osp", ".", "join", "(", "results", "[", "'video_dir'", "]", ",", "clipname", ")", "\n", "", "else", ":", "\n", "            ", "filename", "=", "results", "[", "'video_dir'", "]", "+", "clipname", "\n", "", "results", "[", "'filename'", "]", "=", "filename", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.prepare_train_frames": [[133, 140], ["copy.deepcopy", "rawvideo_dataset.RawVideoDataset.sample_clip", "rawvideo_dataset.RawVideoDataset.pipeline"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.sample_clip"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "=", "self", ".", "sample_clip", "(", "results", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.prepare_test_frames": [[141, 148], ["copy.deepcopy", "rawvideo_dataset.RawVideoDataset.sample_clip", "rawvideo_dataset.RawVideoDataset.pipeline"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.sample_clip"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "=", "self", ".", "sample_clip", "(", "results", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawframe_dataset.RawframeDataset.__init__": [[89, 117], ["base.BaseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "filename_tmpl", "=", "'img_{:05}.jpg'", ",", "\n", "with_offset", "=", "False", ",", "\n", "multi_class", "=", "False", ",", "\n", "num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n", "power", "=", "0.", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "self", ".", "filename_tmpl", "=", "filename_tmpl", "\n", "self", ".", "with_offset", "=", "with_offset", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", ",", "\n", "test_mode", ",", "\n", "multi_class", ",", "\n", "num_classes", ",", "\n", "start_index", ",", "\n", "modality", ",", "\n", "sample_by_class", "=", "sample_by_class", ",", "\n", "power", "=", "power", ",", "\n", "dynamic_length", "=", "dynamic_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawframe_dataset.RawframeDataset.load_annotations": [[118, 155], ["rawframe_dataset.RawframeDataset.ann_file.endswith", "rawframe_dataset.RawframeDataset.load_json_annotations", "open", "line.strip().split", "line_split[].replace", "video_infos.append", "os.join", "int", "int", "int", "int", "line.strip", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "if", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", ":", "\n", "            ", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "video_info", "=", "{", "}", "\n", "idx", "=", "0", "\n", "# idx for frame_dir", "\n", "frame_dir", "=", "line_split", "[", "idx", "]", ".", "replace", "(", "'.mp4'", ",", "''", ")", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "frame_dir", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "frame_dir", ")", "\n", "", "video_info", "[", "'frame_dir'", "]", "=", "frame_dir", "\n", "idx", "+=", "1", "\n", "if", "self", ".", "with_offset", ":", "\n", "# idx for offset and total_frames", "\n", "                    ", "video_info", "[", "'offset'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "+", "1", "]", ")", "\n", "idx", "+=", "2", "\n", "", "else", ":", "\n", "# idx for total_frames", "\n", "                    ", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "idx", "+=", "1", "\n", "# idx for label[s]", "\n", "", "label", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line_split", "[", "idx", ":", "]", "]", "\n", "assert", "label", ",", "f'missing label in line: {line}'", "\n", "if", "self", ".", "multi_class", ":", "\n", "                    ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "video_info", "[", "'label'", "]", "=", "label", "\n", "", "else", ":", "\n", "                    ", "assert", "len", "(", "label", ")", "==", "1", "\n", "video_info", "[", "'label'", "]", "=", "label", "[", "0", "]", "\n", "", "video_infos", ".", "append", "(", "video_info", ")", "\n", "\n", "", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawframe_dataset.RawframeDataset.prepare_train_frames": [[156, 170], ["copy.deepcopy", "rawframe_dataset.RawframeDataset.pipeline", "torch.zeros"], "methods", ["None"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "if", "self", ".", "multi_class", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawframe_dataset.RawframeDataset.prepare_test_frames": [[171, 185], ["copy.deepcopy", "rawframe_dataset.RawframeDataset.pipeline", "torch.zeros"], "methods", ["None"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "if", "self", ".", "multi_class", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_dataset.AudioDataset.__init__": [[31, 34], ["base.BaseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "ann_file", ",", "pipeline", ",", "suffix", "=", "'.wav'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "suffix", "=", "suffix", "\n", "super", "(", ")", ".", "__init__", "(", "ann_file", ",", "pipeline", ",", "modality", "=", "'Audio'", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.audio_dataset.AudioDataset.load_annotations": [[35, 71], ["audio_dataset.AudioDataset.ann_file.endswith", "audio_dataset.AudioDataset.load_json_annotations", "open", "line.strip().split", "int", "video_infos.append", "int", "torch.zeros", "line.strip", "os.join.endswith", "os.join", "os.join", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.rawvideo_dataset.RawVideoDataset.load_json_annotations"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load annotation file to get video information.\"\"\"", "\n", "if", "self", ".", "ann_file", ".", "endswith", "(", "'.json'", ")", ":", "\n", "            ", "return", "self", ".", "load_json_annotations", "(", ")", "\n", "", "video_infos", "=", "[", "]", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "video_info", "=", "{", "}", "\n", "idx", "=", "0", "\n", "filename", "=", "line_split", "[", "idx", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                    ", "if", "not", "filename", ".", "endswith", "(", "self", ".", "suffix", ")", ":", "\n", "                        ", "filename", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "\n", "filename", "+", "self", ".", "suffix", ")", "\n", "", "else", ":", "\n", "                        ", "filename", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "filename", ")", "\n", "", "", "video_info", "[", "'audio_path'", "]", "=", "filename", "\n", "idx", "+=", "1", "\n", "# idx for total_frames", "\n", "video_info", "[", "'total_frames'", "]", "=", "int", "(", "line_split", "[", "idx", "]", ")", "\n", "idx", "+=", "1", "\n", "# idx for label[s]", "\n", "label", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line_split", "[", "idx", ":", "]", "]", "\n", "assert", "label", ",", "f'missing label in line: {line}'", "\n", "if", "self", ".", "multi_class", ":", "\n", "                    ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "label", "]", "=", "1.0", "\n", "video_info", "[", "'label'", "]", "=", "onehot", "\n", "", "else", ":", "\n", "                    ", "assert", "len", "(", "label", ")", "==", "1", "\n", "video_info", "[", "'label'", "]", "=", "label", "[", "0", "]", "\n", "", "video_infos", ".", "append", "(", "video_info", ")", "\n", "\n", "", "", "return", "video_infos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.__init__": [[98, 158], ["utils.get_root_logger", "base.BaseDataset.__init__", "core.evaluation.ava_utils.read_labelmap", "set().issubset", "tuple", "mmcv.load", "ava_dataset.AVADataset.filter_exclude_file", "ava_dataset.AVADataset.logger.info", "open", "len", "set", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_labelmap", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.filter_exclude_file", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "exclude_file", ",", "\n", "pipeline", ",", "\n", "label_file", "=", "None", ",", "\n", "filename_tmpl", "=", "'img_{:05}.jpg'", ",", "\n", "start_index", "=", "0", ",", "\n", "proposal_file", "=", "None", ",", "\n", "person_det_score_thr", "=", "0.9", ",", "\n", "num_classes", "=", "81", ",", "\n", "custom_classes", "=", "None", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "num_max_proposals", "=", "1000", ",", "\n", "timestamp_start", "=", "900", ",", "\n", "timestamp_end", "=", "1800", ")", ":", "\n", "# since it inherits from `BaseDataset`, some arguments", "\n", "# should be assigned before performing `load_annotations()`", "\n", "        ", "self", ".", "custom_classes", "=", "custom_classes", "\n", "if", "custom_classes", "is", "not", "None", ":", "\n", "            ", "assert", "num_classes", "==", "len", "(", "custom_classes", ")", "+", "1", "\n", "assert", "0", "not", "in", "custom_classes", "\n", "_", ",", "class_whitelist", "=", "read_labelmap", "(", "open", "(", "label_file", ")", ")", "\n", "assert", "set", "(", "custom_classes", ")", ".", "issubset", "(", "class_whitelist", ")", "\n", "\n", "self", ".", "custom_classes", "=", "tuple", "(", "[", "0", "]", "+", "custom_classes", ")", "\n", "", "self", ".", "exclude_file", "=", "exclude_file", "\n", "self", ".", "label_file", "=", "label_file", "\n", "self", ".", "proposal_file", "=", "proposal_file", "\n", "assert", "0", "<=", "person_det_score_thr", "<=", "1", ",", "(", "\n", "'The value of '", "\n", "'person_det_score_thr should in [0, 1]. '", ")", "\n", "self", ".", "person_det_score_thr", "=", "person_det_score_thr", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "filename_tmpl", "=", "filename_tmpl", "\n", "self", ".", "num_max_proposals", "=", "num_max_proposals", "\n", "self", ".", "timestamp_start", "=", "timestamp_start", "\n", "self", ".", "timestamp_end", "=", "timestamp_end", "\n", "self", ".", "logger", "=", "get_root_logger", "(", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", ",", "\n", "test_mode", ",", "\n", "start_index", "=", "start_index", ",", "\n", "modality", "=", "modality", ",", "\n", "num_classes", "=", "num_classes", ")", "\n", "\n", "if", "self", ".", "proposal_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "proposals", "=", "mmcv", ".", "load", "(", "self", ".", "proposal_file", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "proposals", "=", "None", "\n", "\n", "", "if", "not", "test_mode", ":", "\n", "            ", "valid_indexes", "=", "self", ".", "filter_exclude_file", "(", ")", "\n", "self", ".", "logger", ".", "info", "(", "\n", "f'{len(valid_indexes)} out of {len(self.video_infos)} '", "\n", "f'frames are valid.'", ")", "\n", "self", ".", "video_infos", "=", "[", "self", ".", "video_infos", "[", "i", "]", "for", "i", "in", "valid_indexes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.parse_img_record": [[159, 205], ["numpy.stack", "numpy.stack", "numpy.stack", "len", "len", "len", "numpy.stack.append", "numpy.array", "numpy.zeros", "numpy.stack.append", "numpy.stack.append", "numpy.array_equal", "len", "numpy.array_equal"], "methods", ["None"], ["", "", "def", "parse_img_record", "(", "self", ",", "img_records", ")", ":", "\n", "        ", "\"\"\"Merge image records of the same entity at the same time.\n\n        Args:\n            img_records (list[dict]): List of img_records (lines in AVA\n                annotations).\n\n        Returns:\n            tuple(list): A tuple consists of lists of bboxes, action labels and\n                entity_ids\n        \"\"\"", "\n", "bboxes", ",", "labels", ",", "entity_ids", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "while", "len", "(", "img_records", ")", ">", "0", ":", "\n", "            ", "img_record", "=", "img_records", "[", "0", "]", "\n", "num_img_records", "=", "len", "(", "img_records", ")", "\n", "\n", "selected_records", "=", "[", "\n", "x", "for", "x", "in", "img_records", "\n", "if", "np", ".", "array_equal", "(", "x", "[", "'entity_box'", "]", ",", "img_record", "[", "'entity_box'", "]", ")", "\n", "]", "\n", "\n", "num_selected_records", "=", "len", "(", "selected_records", ")", "\n", "img_records", "=", "[", "\n", "x", "for", "x", "in", "img_records", "if", "\n", "not", "np", ".", "array_equal", "(", "x", "[", "'entity_box'", "]", ",", "img_record", "[", "'entity_box'", "]", ")", "\n", "]", "\n", "\n", "assert", "len", "(", "img_records", ")", "+", "num_selected_records", "==", "num_img_records", "\n", "\n", "bboxes", ".", "append", "(", "img_record", "[", "'entity_box'", "]", ")", "\n", "valid_labels", "=", "np", ".", "array", "(", "[", "\n", "selected_record", "[", "'label'", "]", "\n", "for", "selected_record", "in", "selected_records", "\n", "]", ")", "\n", "\n", "# The format can be directly used by BCELossWithLogits", "\n", "label", "=", "np", ".", "zeros", "(", "self", ".", "num_classes", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "label", "[", "valid_labels", "]", "=", "1.", "\n", "\n", "labels", ".", "append", "(", "label", ")", "\n", "entity_ids", ".", "append", "(", "img_record", "[", "'entity_id'", "]", ")", "\n", "\n", "", "bboxes", "=", "np", ".", "stack", "(", "bboxes", ")", "\n", "labels", "=", "np", ".", "stack", "(", "labels", ")", "\n", "entity_ids", "=", "np", ".", "stack", "(", "entity_ids", ")", "\n", "return", "bboxes", ",", "labels", ",", "entity_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.filter_exclude_file": [[206, 223], ["list", "enumerate", "range", "x.strip().split", "list.append", "len", "open", "x.strip", "list.pop", "int"], "methods", ["None"], ["", "def", "filter_exclude_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"Filter out records in the exclude_file.\"\"\"", "\n", "valid_indexes", "=", "[", "]", "\n", "if", "self", ".", "exclude_file", "is", "None", ":", "\n", "            ", "valid_indexes", "=", "list", "(", "range", "(", "len", "(", "self", ".", "video_infos", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "exclude_video_infos", "=", "[", "\n", "x", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "for", "x", "in", "open", "(", "self", ".", "exclude_file", ")", "\n", "]", "\n", "for", "i", ",", "video_info", "in", "enumerate", "(", "self", ".", "video_infos", ")", ":", "\n", "                ", "valid_indexes", ".", "append", "(", "i", ")", "\n", "for", "video_id", ",", "timestamp", "in", "exclude_video_infos", ":", "\n", "                    ", "if", "(", "video_info", "[", "'video_id'", "]", "==", "video_id", "\n", "and", "video_info", "[", "'timestamp'", "]", "==", "int", "(", "timestamp", ")", ")", ":", "\n", "                        ", "valid_indexes", ".", "pop", "(", ")", "\n", "break", "\n", "", "", "", "", "return", "valid_indexes", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.load_annotations": [[224, 276], ["collections.defaultdict", "open", "img_key.split", "ava_dataset.AVADataset.parse_img_record", "dict", "dict", "video_infos.append", "line.strip().split", "int", "int", "numpy.array", "int", "dict", "records_dict_by_img[].append", "os.join", "os.join", "ava_dataset.AVADataset.custom_classes.index", "list", "int", "line.strip", "map"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.parse_img_record"], ["", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load AVA annotations.\"\"\"", "\n", "video_infos", "=", "[", "]", "\n", "records_dict_by_img", "=", "defaultdict", "(", "list", ")", "\n", "with", "open", "(", "self", ".", "ann_file", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "line_split", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "','", ")", "\n", "\n", "label", "=", "int", "(", "line_split", "[", "6", "]", ")", "\n", "if", "self", ".", "custom_classes", "is", "not", "None", ":", "\n", "                    ", "if", "label", "not", "in", "self", ".", "custom_classes", ":", "\n", "                        ", "continue", "\n", "", "label", "=", "self", ".", "custom_classes", ".", "index", "(", "label", ")", "\n", "\n", "", "video_id", "=", "line_split", "[", "0", "]", "\n", "timestamp", "=", "int", "(", "line_split", "[", "1", "]", ")", "\n", "img_key", "=", "f'{video_id},{timestamp:04d}'", "\n", "\n", "entity_box", "=", "np", ".", "array", "(", "list", "(", "map", "(", "float", ",", "line_split", "[", "2", ":", "6", "]", ")", ")", ")", "\n", "entity_id", "=", "int", "(", "line_split", "[", "7", "]", ")", "\n", "shot_info", "=", "(", "0", ",", "(", "self", ".", "timestamp_end", "-", "self", ".", "timestamp_start", ")", "*", "\n", "self", ".", "_FPS", ")", "\n", "\n", "video_info", "=", "dict", "(", "\n", "video_id", "=", "video_id", ",", "\n", "timestamp", "=", "timestamp", ",", "\n", "entity_box", "=", "entity_box", ",", "\n", "label", "=", "label", ",", "\n", "entity_id", "=", "entity_id", ",", "\n", "shot_info", "=", "shot_info", ")", "\n", "records_dict_by_img", "[", "img_key", "]", ".", "append", "(", "video_info", ")", "\n", "\n", "", "", "for", "img_key", "in", "records_dict_by_img", ":", "\n", "            ", "video_id", ",", "timestamp", "=", "img_key", ".", "split", "(", "','", ")", "\n", "bboxes", ",", "labels", ",", "entity_ids", "=", "self", ".", "parse_img_record", "(", "\n", "records_dict_by_img", "[", "img_key", "]", ")", "\n", "ann", "=", "dict", "(", "\n", "gt_bboxes", "=", "bboxes", ",", "gt_labels", "=", "labels", ",", "entity_ids", "=", "entity_ids", ")", "\n", "frame_dir", "=", "video_id", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "frame_dir", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "frame_dir", ")", "\n", "", "video_info", "=", "dict", "(", "\n", "frame_dir", "=", "frame_dir", ",", "\n", "video_id", "=", "video_id", ",", "\n", "timestamp", "=", "int", "(", "timestamp", ")", ",", "\n", "img_key", "=", "img_key", ",", "\n", "shot_info", "=", "shot_info", ",", "\n", "fps", "=", "self", ".", "_FPS", ",", "\n", "ann", "=", "ann", ")", "\n", "video_infos", ".", "append", "(", "video_info", ")", "\n", "\n", "", "return", "video_infos", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.prepare_train_frames": [[277, 312], ["copy.deepcopy", "copy.deepcopy.pop", "ava_dataset.AVADataset.pipeline", "numpy.array", "numpy.array", "min", "max"], "methods", ["None"], ["", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "img_key", "=", "results", "[", "'img_key'", "]", "\n", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "results", "[", "'timestamp_start'", "]", "=", "self", ".", "timestamp_start", "\n", "results", "[", "'timestamp_end'", "]", "=", "self", ".", "timestamp_end", "\n", "\n", "if", "self", ".", "proposals", "is", "not", "None", ":", "\n", "            ", "if", "img_key", "not", "in", "self", ".", "proposals", ":", "\n", "                ", "results", "[", "'proposals'", "]", "=", "np", ".", "array", "(", "[", "[", "0", ",", "0", ",", "1", ",", "1", "]", "]", ")", "\n", "results", "[", "'scores'", "]", "=", "np", ".", "array", "(", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "proposals", "=", "self", ".", "proposals", "[", "img_key", "]", "\n", "assert", "proposals", ".", "shape", "[", "-", "1", "]", "in", "[", "4", ",", "5", "]", "\n", "if", "proposals", ".", "shape", "[", "-", "1", "]", "==", "5", ":", "\n", "                    ", "thr", "=", "min", "(", "self", ".", "person_det_score_thr", ",", "max", "(", "proposals", "[", ":", ",", "4", "]", ")", ")", "\n", "positive_inds", "=", "(", "proposals", "[", ":", ",", "4", "]", ">=", "thr", ")", "\n", "proposals", "=", "proposals", "[", "positive_inds", "]", "\n", "proposals", "=", "proposals", "[", ":", "self", ".", "num_max_proposals", "]", "\n", "results", "[", "'proposals'", "]", "=", "proposals", "[", ":", ",", ":", "4", "]", "\n", "results", "[", "'scores'", "]", "=", "proposals", "[", ":", ",", "4", "]", "\n", "", "else", ":", "\n", "                    ", "proposals", "=", "proposals", "[", ":", "self", ".", "num_max_proposals", "]", "\n", "results", "[", "'proposals'", "]", "=", "proposals", "\n", "\n", "", "", "", "ann", "=", "results", ".", "pop", "(", "'ann'", ")", "\n", "results", "[", "'gt_bboxes'", "]", "=", "ann", "[", "'gt_bboxes'", "]", "\n", "results", "[", "'gt_labels'", "]", "=", "ann", "[", "'gt_labels'", "]", "\n", "results", "[", "'entity_ids'", "]", "=", "ann", "[", "'entity_ids'", "]", "\n", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.prepare_test_frames": [[313, 349], ["copy.deepcopy", "copy.deepcopy.pop", "ava_dataset.AVADataset.pipeline", "numpy.array", "numpy.array", "min", "max"], "methods", ["None"], ["", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "img_key", "=", "results", "[", "'img_key'", "]", "\n", "\n", "results", "[", "'filename_tmpl'", "]", "=", "self", ".", "filename_tmpl", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "results", "[", "'timestamp_start'", "]", "=", "self", ".", "timestamp_start", "\n", "results", "[", "'timestamp_end'", "]", "=", "self", ".", "timestamp_end", "\n", "\n", "if", "self", ".", "proposals", "is", "not", "None", ":", "\n", "            ", "if", "img_key", "not", "in", "self", ".", "proposals", ":", "\n", "                ", "results", "[", "'proposals'", "]", "=", "np", ".", "array", "(", "[", "[", "0", ",", "0", ",", "1", ",", "1", "]", "]", ")", "\n", "results", "[", "'scores'", "]", "=", "np", ".", "array", "(", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "proposals", "=", "self", ".", "proposals", "[", "img_key", "]", "\n", "assert", "proposals", ".", "shape", "[", "-", "1", "]", "in", "[", "4", ",", "5", "]", "\n", "if", "proposals", ".", "shape", "[", "-", "1", "]", "==", "5", ":", "\n", "                    ", "thr", "=", "min", "(", "self", ".", "person_det_score_thr", ",", "max", "(", "proposals", "[", ":", ",", "4", "]", ")", ")", "\n", "positive_inds", "=", "(", "proposals", "[", ":", ",", "4", "]", ">=", "thr", ")", "\n", "proposals", "=", "proposals", "[", "positive_inds", "]", "\n", "proposals", "=", "proposals", "[", ":", "self", ".", "num_max_proposals", "]", "\n", "results", "[", "'proposals'", "]", "=", "proposals", "[", ":", ",", ":", "4", "]", "\n", "results", "[", "'scores'", "]", "=", "proposals", "[", ":", ",", "4", "]", "\n", "", "else", ":", "\n", "                    ", "proposals", "=", "proposals", "[", ":", "self", ".", "num_max_proposals", "]", "\n", "results", "[", "'proposals'", "]", "=", "proposals", "\n", "\n", "", "", "", "ann", "=", "results", ".", "pop", "(", "'ann'", ")", "\n", "# Follow the mmdet variable naming style.", "\n", "results", "[", "'gt_bboxes'", "]", "=", "ann", "[", "'gt_bboxes'", "]", "\n", "results", "[", "'gt_labels'", "]", "=", "ann", "[", "'gt_labels'", "]", "\n", "results", "[", "'entity_ids'", "]", "=", "ann", "[", "'entity_ids'", "]", "\n", "\n", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.dump_results": [[350, 354], ["out.endswith", "core.evaluation.ava_utils.results2csv"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.results2csv"], ["", "def", "dump_results", "(", "self", ",", "results", ",", "out", ")", ":", "\n", "        ", "\"\"\"Dump predictions into a csv file.\"\"\"", "\n", "assert", "out", ".", "endswith", "(", "'csv'", ")", "\n", "results2csv", "(", "self", ",", "results", ",", "out", ",", "self", ".", "custom_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.evaluate": [[355, 393], ["datetime.datetime.datetime.now().strftime", "core.evaluation.ava_utils.results2csv", "os.remove", "os.remove", "os.remove", "os.remove", "mmcv.utils.print_log", "core.evaluation.ava_utils.ava_eval", "core.evaluation.ava_utils.ava_eval.items", "mmcv.utils.print_log", "ret.update", "len", "datetime.datetime.datetime.now", "log_msg.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.results2csv", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.ava_eval", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "(", "'mAP'", ",", ")", ",", "\n", "metric_options", "=", "None", ",", "\n", "logger", "=", "None", ")", ":", "\n", "        ", "\"\"\"Evaluate the prediction results and report mAP.\"\"\"", "\n", "assert", "len", "(", "metrics", ")", "==", "1", "and", "metrics", "[", "0", "]", "==", "'mAP'", ",", "(", "\n", "'For evaluation on AVADataset, you need to use metrics \"mAP\" '", "\n", "'See https://github.com/open-mmlab/mmaction2/pull/567 '", "\n", "'for more info.'", ")", "\n", "time_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "temp_file", "=", "f'AVA_{time_now}_result.csv'", "\n", "results2csv", "(", "self", ",", "results", ",", "temp_file", ",", "self", ".", "custom_classes", ")", "\n", "\n", "ret", "=", "{", "}", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "msg", "=", "f'Evaluating {metric} ...'", "\n", "if", "logger", "is", "None", ":", "\n", "                ", "msg", "=", "'\\n'", "+", "msg", "\n", "", "print_log", "(", "msg", ",", "logger", "=", "logger", ")", "\n", "\n", "eval_result", "=", "ava_eval", "(", "\n", "temp_file", ",", "\n", "metric", ",", "\n", "self", ".", "label_file", ",", "\n", "self", ".", "ann_file", ",", "\n", "self", ".", "exclude_file", ",", "\n", "custom_classes", "=", "self", ".", "custom_classes", ")", "\n", "log_msg", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "eval_result", ".", "items", "(", ")", ":", "\n", "                ", "log_msg", ".", "append", "(", "f'\\n{k}\\t{v: .4f}'", ")", "\n", "", "log_msg", "=", "''", ".", "join", "(", "log_msg", ")", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "ret", ".", "update", "(", "eval_result", ")", "\n", "\n", "", "os", ".", "remove", "(", "temp_file", ")", "\n", "\n", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.samplers.distributed_sampler.DistributedSampler.__init__": [[17, 27], ["torch.utils.data.DistributedSampler.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.samplers.distributed_sampler.DistributedSampler.__iter__": [[28, 45], ["iter", "torch.Generator", "torch.Generator.manual_seed", "torch.randperm().tolist", "torch.arange().tolist", "len", "len", "torch.randperm", "torch.arange", "len", "len", "len"], "methods", ["None"], ["            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "\n", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.samplers.distributed_sampler.ClassSpecificDistributedSampler.__init__": [[62, 81], ["torch.utils.data.DistributedSampler.__init__", "hasattr", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["\n", "\n", "def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "num_replicas", "=", "None", ",", "\n", "rank", "=", "None", ",", "\n", "shuffle", "=", "True", ",", "\n", "num_repeats", "=", "3", ",", "\n", "selected_round", "=", "256", ",", "\n", "selected_ratio", "=", "0", ",", "\n", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.samplers.distributed_sampler.ClassSpecificDistributedSampler.__iter__": [[82, 136], ["torch.Generator", "torch.Generator.manual_seed", "collections.defaultdict", "enumerate", "iter", "class_indices[].append", "distributed_sampler.ClassSpecificDistributedSampler.class_prob.items", "math.ceil", "torch.multinomial", "indices.data.numpy().tolist.data.numpy().tolist.data.numpy().tolist", "len", "len", "type", "range", "int", "indices.data.numpy().tolist.data.numpy().tolist.extend", "torch.randperm().tolist", "torch.Tensor", "int", "indices.data.numpy().tolist.data.numpy().tolist.extend", "torch.randperm().tolist", "len", "len", "indices.data.numpy().tolist.data.numpy().tolist.data.numpy", "len", "len", "torch.randperm", "torch.randperm", "len", "len"], "methods", ["None"], ["", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "num_repeats", "=", "num_repeats", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "num_repeats", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "# Determine the number of samples to select per epoch for each rank.", "\n", "# num_selected logic defaults to be the same as original RASampler impl, but this one can be tweaked", "\n", "# via selected_ratio and selected_round args.", "\n", "selected_ratio", "=", "selected_ratio", "or", "num_replicas", "# ratio to reduce selected samples by, num_replicas if 0", "\n", "if", "selected_round", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "floor", "(", "\n", "len", "(", "self", ".", "dataset", ")", "//", "selected_round", "*", "selected_round", "/", "selected_ratio", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "/", "selected_ratio", ")", ")", "\n", "\n", "", "", "def", "__iter__", "(", "self", ")", ":", "\n", "# deterministically shuffle based on epoch", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "torch", ".", "randperm", "(", "len", "(", "self", ".", "dataset", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]", "\n", "", "indices", "=", "[", "x", "for", "x", "in", "indices", "for", "_", "in", "range", "(", "self", ".", "num_repeats", ")", "]", "\n", "# add extra samples to make it evenly divisible", "\n", "padding_size", "=", "self", ".", "total_size", "-", "len", "(", "indices", ")", "\n", "indices", "+=", "indices", "[", ":", "padding_size", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample per rank", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "# return up to num selected samples", "\n", "return", "iter", "(", "indices", "[", ":", "self", ".", "num_selected_samples", "]", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_selected_samples", "\n", "\n", "", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToTensor.__init__": [[40, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "keys", ")", ":", "\n", "        ", "self", ".", "keys", "=", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToTensor.__call__": [[43, 53], ["formatting.to_tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the ToTensor formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "results", "[", "key", "]", "=", "to_tensor", "(", "results", "[", "key", "]", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToTensor.__repr__": [[54, 56], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.__class__.__name__}(keys={self.keys})'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Rename.__init__": [[69, 71], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mapping", ")", ":", "\n", "        ", "self", ".", "mapping", "=", "mapping", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Rename.__call__": [[72, 81], ["formatting.Rename.mapping.items", "results.pop", "isinstance", "isinstance"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "self", ".", "mapping", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "results", ":", "\n", "                ", "assert", "isinstance", "(", "key", ",", "str", ")", "and", "isinstance", "(", "value", ",", "str", ")", "\n", "assert", "value", "not", "in", "results", ",", "(", "'the new name already exists in '", "\n", "'results'", ")", "\n", "results", "[", "value", "]", "=", "results", "[", "key", "]", "\n", "results", ".", "pop", "(", "key", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToDataContainer.__init__": [[95, 97], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fields", ")", ":", "\n", "        ", "self", ".", "fields", "=", "fields", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToDataContainer.__call__": [[98, 114], ["field.copy", "field.copy.pop", "isinstance", "mmcv.parallel.DataContainer", "mmcv.parallel.DataContainer"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the ToDataContainer formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "for", "field", "in", "self", ".", "fields", ":", "\n", "            ", "_field", "=", "field", ".", "copy", "(", ")", "\n", "key", "=", "_field", ".", "pop", "(", "'key'", ")", "\n", "if", "isinstance", "(", "key", ",", "list", ")", ":", "\n", "                ", "for", "item", "in", "key", ":", "\n", "                    ", "results", "[", "item", "]", "=", "DC", "(", "results", "[", "item", "]", ",", "**", "_field", ")", "\n", "", "", "else", ":", "\n", "                ", "results", "[", "key", "]", "=", "DC", "(", "results", "[", "key", "]", ",", "**", "_field", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ToDataContainer.__repr__": [[115, 117], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__class__", ".", "__name__", "+", "f'(fields={self.fields})'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ImageToTensor.__init__": [[127, 129], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "keys", ")", ":", "\n", "        ", "self", ".", "keys", "=", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ImageToTensor.__call__": [[130, 140], ["formatting.to_tensor", "results[].transpose"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the ImageToTensor formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "results", "[", "key", "]", "=", "to_tensor", "(", "results", "[", "key", "]", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.ImageToTensor.__repr__": [[141, 143], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.__class__.__name__}(keys={self.keys})'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Transpose.__init__": [[154, 157], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "keys", ",", "order", ")", ":", "\n", "        ", "self", ".", "keys", "=", "keys", "\n", "self", ".", "order", "=", "order", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Transpose.__call__": [[158, 168], ["results[].transpose"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Transpose formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "results", "[", "key", "]", "=", "results", "[", "key", "]", ".", "transpose", "(", "self", ".", "order", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Transpose.__repr__": [[169, 171], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "(", "f'{self.__class__.__name__}('", "\n", "f'keys={self.keys}, order={self.order})'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Collect.__init__": [[213, 223], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "keys", ",", "\n", "meta_keys", "=", "(", "'filename'", ",", "'label'", ",", "'original_shape'", ",", "'img_shape'", ",", "\n", "'pad_shape'", ",", "'flip_direction'", ",", "'img_norm_cfg'", ")", ",", "\n", "meta_name", "=", "'img_metas'", ",", "\n", "nested", "=", "False", ")", ":", "\n", "        ", "self", ".", "keys", "=", "keys", "\n", "self", ".", "meta_keys", "=", "meta_keys", "\n", "self", ".", "meta_name", "=", "meta_name", "\n", "self", ".", "nested", "=", "nested", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Collect.__call__": [[224, 245], ["len", "mmcv.parallel.DataContainer"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Collect formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "data", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "data", "[", "key", "]", "=", "results", "[", "key", "]", "\n", "\n", "", "if", "len", "(", "self", ".", "meta_keys", ")", "!=", "0", ":", "\n", "            ", "meta", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "meta_keys", ":", "\n", "                ", "meta", "[", "key", "]", "=", "results", "[", "key", "]", "\n", "", "data", "[", "self", ".", "meta_name", "]", "=", "DC", "(", "meta", ",", "cpu_only", "=", "True", ")", "\n", "", "if", "self", ".", "nested", ":", "\n", "            ", "for", "k", "in", "data", ":", "\n", "                ", "data", "[", "k", "]", "=", "[", "data", "[", "k", "]", "]", "\n", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.Collect.__repr__": [[246, 248], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "(", "f'{self.__class__.__name__}('", "\n", "f'keys={self.keys}, meta_keys={self.meta_keys}, '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatShape.__init__": [[266, 272], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_format", ",", "collapse", "=", "False", ")", ":", "\n", "        ", "self", ".", "input_format", "=", "input_format", "\n", "self", ".", "collapse", "=", "collapse", "\n", "if", "self", ".", "input_format", "not", "in", "[", "'NCTHW'", ",", "'NCHW'", ",", "'NCHW_Flow'", ",", "'NPTCHW'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The input format {self.input_format} is invalid.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatShape.__call__": [[273, 332], ["isinstance", "numpy.array", "numpy.transpose.reshape", "numpy.transpose", "numpy.transpose.reshape", "numpy.transpose.squeeze", "numpy.transpose", "numpy.transpose.reshape", "numpy.transpose", "numpy.transpose.reshape", "numpy.transpose.reshape", "numpy.transpose"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the FormatShape formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "results", "[", "'imgs'", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "results", "[", "'imgs'", "]", "=", "np", ".", "array", "(", "results", "[", "'imgs'", "]", ")", "\n", "", "imgs", "=", "results", "[", "'imgs'", "]", "\n", "# [M x H x W x C]", "\n", "# M = 1 * N_crops * N_clips * L", "\n", "if", "self", ".", "collapse", ":", "\n", "            ", "assert", "results", "[", "'num_clips'", "]", "==", "1", "\n", "\n", "", "if", "self", ".", "input_format", "==", "'NCTHW'", ":", "\n", "            ", "num_clips", "=", "results", "[", "'num_clips'", "]", "\n", "clip_len", "=", "results", "[", "'clip_len'", "]", "\n", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", "num_clips", ",", "clip_len", ")", "+", "imgs", ".", "shape", "[", "1", ":", "]", ")", "\n", "# N_crops x N_clips x L x H x W x C", "\n", "imgs", "=", "np", ".", "transpose", "(", "imgs", ",", "(", "0", ",", "1", ",", "5", ",", "2", ",", "3", ",", "4", ")", ")", "\n", "# N_crops x N_clips x C x L x H x W", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "# M' x C x L x H x W", "\n", "# M' = N_crops x N_clips", "\n", "", "elif", "self", ".", "input_format", "==", "'NCHW'", ":", "\n", "            ", "imgs", "=", "np", ".", "transpose", "(", "imgs", ",", "(", "0", ",", "3", ",", "1", ",", "2", ")", ")", "\n", "# M x C x H x W", "\n", "", "elif", "self", ".", "input_format", "==", "'NCHW_Flow'", ":", "\n", "            ", "num_clips", "=", "results", "[", "'num_clips'", "]", "\n", "clip_len", "=", "results", "[", "'clip_len'", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", "num_clips", ",", "clip_len", ")", "+", "imgs", ".", "shape", "[", "1", ":", "]", ")", "\n", "# N_crops x N_clips x L x H x W x C", "\n", "imgs", "=", "np", ".", "transpose", "(", "imgs", ",", "(", "0", ",", "1", ",", "2", ",", "5", ",", "3", ",", "4", ")", ")", "\n", "# N_crops x N_clips x L x C x H x W", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", "imgs", ".", "shape", "[", "2", "]", "*", "imgs", ".", "shape", "[", "3", "]", ")", "+", "\n", "imgs", ".", "shape", "[", "4", ":", "]", ")", "\n", "# M' x C' x H x W", "\n", "# M' = N_crops x N_clips", "\n", "# C' = L x C", "\n", "", "elif", "self", ".", "input_format", "==", "'NPTCHW'", ":", "\n", "            ", "num_proposals", "=", "results", "[", "'num_proposals'", "]", "\n", "num_clips", "=", "results", "[", "'num_clips'", "]", "\n", "clip_len", "=", "results", "[", "'clip_len'", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "num_proposals", ",", "num_clips", "*", "clip_len", ")", "+", "\n", "imgs", ".", "shape", "[", "1", ":", "]", ")", "\n", "# P x M x H x W x C", "\n", "# M = N_clips x L", "\n", "imgs", "=", "np", ".", "transpose", "(", "imgs", ",", "(", "0", ",", "1", ",", "4", ",", "2", ",", "3", ")", ")", "\n", "# P x M x C x H x W", "\n", "\n", "", "if", "self", ".", "collapse", ":", "\n", "            ", "assert", "imgs", ".", "shape", "[", "0", "]", "==", "1", "\n", "imgs", "=", "imgs", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'input_shape'", "]", "=", "imgs", ".", "shape", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatShape.__repr__": [[333, 337], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "\n", "repr_str", "+=", "f\"(input_format='{self.input_format}')\"", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatAudioShape.__init__": [[350, 355], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_format", ")", ":", "\n", "        ", "self", ".", "input_format", "=", "input_format", "\n", "if", "self", ".", "input_format", "not", "in", "[", "'NCTF'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The input format {self.input_format} is invalid.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatAudioShape.__call__": [[356, 370], ["audios.reshape.reshape.reshape"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the FormatShape formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "audios", "=", "results", "[", "'audios'", "]", "\n", "# clip x sample x freq -> clip x channel x sample x freq", "\n", "clip", ",", "sample", ",", "freq", "=", "audios", ".", "shape", "\n", "audios", "=", "audios", ".", "reshape", "(", "clip", ",", "1", ",", "sample", ",", "freq", ")", "\n", "results", "[", "'audios'", "]", "=", "audios", "\n", "results", "[", "'input_shape'", "]", "=", "audios", ".", "shape", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatAudioShape.__repr__": [[371, 375], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "\n", "repr_str", "+=", "f\"(input_format='{self.input_format}')\"", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.JointToBone.__init__": [[389, 409], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", "=", "'nturgb+d'", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "if", "self", ".", "dataset", "not", "in", "[", "'nturgb+d'", ",", "'openpose'", ",", "'coco'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The dataset type {self.dataset} is not supported'", ")", "\n", "", "if", "self", ".", "dataset", "==", "'nturgb+d'", ":", "\n", "            ", "self", ".", "pairs", "=", "[", "(", "0", ",", "1", ")", ",", "(", "1", ",", "20", ")", ",", "(", "2", ",", "20", ")", ",", "(", "3", ",", "2", ")", ",", "(", "4", ",", "20", ")", ",", "(", "5", ",", "4", ")", ",", "\n", "(", "6", ",", "5", ")", ",", "(", "7", ",", "6", ")", ",", "(", "8", ",", "20", ")", ",", "(", "9", ",", "8", ")", ",", "(", "10", ",", "9", ")", ",", "(", "11", ",", "10", ")", ",", "\n", "(", "12", ",", "0", ")", ",", "(", "13", ",", "12", ")", ",", "(", "14", ",", "13", ")", ",", "(", "15", ",", "14", ")", ",", "(", "16", ",", "0", ")", ",", "\n", "(", "17", ",", "16", ")", ",", "(", "18", ",", "17", ")", ",", "(", "19", ",", "18", ")", ",", "(", "21", ",", "22", ")", ",", "(", "20", ",", "20", ")", ",", "\n", "(", "22", ",", "7", ")", ",", "(", "23", ",", "24", ")", ",", "(", "24", ",", "11", ")", "]", "\n", "", "elif", "self", ".", "dataset", "==", "'openpose'", ":", "\n", "            ", "self", ".", "pairs", "=", "(", "(", "0", ",", "0", ")", ",", "(", "1", ",", "0", ")", ",", "(", "2", ",", "1", ")", ",", "(", "3", ",", "2", ")", ",", "(", "4", ",", "3", ")", ",", "(", "5", ",", "1", ")", ",", "\n", "(", "6", ",", "5", ")", ",", "(", "7", ",", "6", ")", ",", "(", "8", ",", "2", ")", ",", "(", "9", ",", "8", ")", ",", "(", "10", ",", "9", ")", ",", "(", "11", ",", "5", ")", ",", "\n", "(", "12", ",", "11", ")", ",", "(", "13", ",", "12", ")", ",", "(", "14", ",", "0", ")", ",", "(", "15", ",", "0", ")", ",", "(", "16", ",", "14", ")", ",", "(", "17", ",", "\n", "15", ")", ")", "\n", "", "elif", "self", ".", "dataset", "==", "'coco'", ":", "\n", "            ", "self", ".", "pairs", "=", "(", "(", "0", ",", "0", ")", ",", "(", "1", ",", "0", ")", ",", "(", "2", ",", "0", ")", ",", "(", "3", ",", "1", ")", ",", "(", "4", ",", "2", ")", ",", "(", "5", ",", "0", ")", ",", "\n", "(", "6", ",", "0", ")", ",", "(", "7", ",", "5", ")", ",", "(", "8", ",", "6", ")", ",", "(", "9", ",", "7", ")", ",", "(", "10", ",", "8", ")", ",", "(", "11", ",", "0", ")", ",", "\n", "(", "12", ",", "0", ")", ",", "(", "13", ",", "11", ")", ",", "(", "14", ",", "12", ")", ",", "(", "15", ",", "13", ")", ",", "(", "16", ",", "14", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.JointToBone.__call__": [[410, 430], ["numpy.zeros"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Bone formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "keypoint", "=", "results", "[", "'keypoint'", "]", "\n", "M", ",", "T", ",", "V", ",", "C", "=", "keypoint", ".", "shape", "\n", "bone", "=", "np", ".", "zeros", "(", "(", "M", ",", "T", ",", "V", ",", "C", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "assert", "C", "in", "[", "2", ",", "3", "]", "\n", "for", "v1", ",", "v2", "in", "self", ".", "pairs", ":", "\n", "            ", "bone", "[", "...", ",", "v1", ",", ":", "]", "=", "keypoint", "[", "...", ",", "v1", ",", ":", "]", "-", "keypoint", "[", "...", ",", "v2", ",", ":", "]", "\n", "if", "C", "==", "3", "and", "self", ".", "dataset", "in", "[", "'openpose'", ",", "'coco'", "]", ":", "\n", "                ", "score", "=", "(", "keypoint", "[", "...", ",", "v1", ",", "2", "]", "+", "keypoint", "[", "...", ",", "v2", ",", "2", "]", ")", "/", "2", "\n", "bone", "[", "...", ",", "v1", ",", "2", "]", "=", "score", "\n", "\n", "", "", "results", "[", "'keypoint'", "]", "=", "bone", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.JointToBone.__repr__": [[431, 435], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "\n", "repr_str", "+=", "f\"(dataset_type='{self.dataset}')\"", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatGCNInput.__init__": [[448, 454], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_format", ",", "num_person", "=", "2", ")", ":", "\n", "        ", "self", ".", "input_format", "=", "input_format", "\n", "if", "self", ".", "input_format", "not", "in", "[", "'NCTVM'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'The input format {self.input_format} is invalid.'", ")", "\n", "", "self", ".", "num_person", "=", "num_person", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatGCNInput.__call__": [[455, 486], ["numpy.transpose", "numpy.expand_dims", "numpy.concatenate", "numpy.zeros", "numpy.concatenate"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the FormatShape formatting.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "keypoint", "=", "results", "[", "'keypoint'", "]", "\n", "\n", "if", "'keypoint_score'", "in", "results", ":", "\n", "            ", "keypoint_confidence", "=", "results", "[", "'keypoint_score'", "]", "\n", "keypoint_confidence", "=", "np", ".", "expand_dims", "(", "keypoint_confidence", ",", "-", "1", ")", "\n", "keypoint_3d", "=", "np", ".", "concatenate", "(", "(", "keypoint", ",", "keypoint_confidence", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "keypoint_3d", "=", "keypoint", "\n", "\n", "", "keypoint_3d", "=", "np", ".", "transpose", "(", "keypoint_3d", ",", "\n", "(", "3", ",", "1", ",", "2", ",", "0", ")", ")", "# M T V C -> C T V M", "\n", "\n", "if", "keypoint_3d", ".", "shape", "[", "-", "1", "]", "<", "self", ".", "num_person", ":", "\n", "            ", "pad_dim", "=", "self", ".", "num_person", "-", "keypoint_3d", ".", "shape", "[", "-", "1", "]", "\n", "pad", "=", "np", ".", "zeros", "(", "\n", "keypoint_3d", ".", "shape", "[", ":", "-", "1", "]", "+", "(", "pad_dim", ",", ")", ",", "dtype", "=", "keypoint_3d", ".", "dtype", ")", "\n", "keypoint_3d", "=", "np", ".", "concatenate", "(", "(", "keypoint_3d", ",", "pad", ")", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "keypoint_3d", ".", "shape", "[", "-", "1", "]", ">", "self", ".", "num_person", ":", "\n", "            ", "keypoint_3d", "=", "keypoint_3d", "[", ":", ",", ":", ",", ":", ",", ":", "self", ".", "num_person", "]", "\n", "\n", "", "results", "[", "'keypoint'", "]", "=", "keypoint_3d", "\n", "results", "[", "'input_shape'", "]", "=", "keypoint_3d", ".", "shape", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.FormatGCNInput.__repr__": [[487, 491], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "\n", "repr_str", "+=", "f\"(input_format='{self.input_format}')\"", "\n", "return", "repr_str", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor": [[12, 29], ["isinstance", "isinstance", "isinstance", "isinstance", "TypeError", "torch.from_numpy", "isinstance", "torch.tensor", "torch.LongTensor", "torch.FloatTensor", "mmcv.is_str", "type"], "function", ["None"], ["def", "to_tensor", "(", "data", ")", ":", "\n", "    ", "\"\"\"Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    \"\"\"", "\n", "if", "isinstance", "(", "data", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "data", "\n", "", "if", "isinstance", "(", "data", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "return", "torch", ".", "from_numpy", "(", "data", ")", "\n", "", "if", "isinstance", "(", "data", ",", "Sequence", ")", "and", "not", "mmcv", ".", "is_str", "(", "data", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "data", ")", "\n", "", "if", "isinstance", "(", "data", ",", "int", ")", ":", "\n", "        ", "return", "torch", ".", "LongTensor", "(", "[", "data", "]", ")", "\n", "", "if", "isinstance", "(", "data", ",", "float", ")", ":", "\n", "        ", "return", "torch", ".", "FloatTensor", "(", "[", "data", "]", ")", "\n", "", "raise", "TypeError", "(", "f'type {type(data)} cannot be converted to tensor.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadHVULabel.__init__": [[27, 30], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "hvu_initialized", "=", "False", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadHVULabel.init_hvu_info": [[31, 43], ["len", "sum", "dict", "range", "dict", "len", "len", "zip", "loading.LoadHVULabel.start_idx.append", "zip"], "methods", ["None"], ["", "def", "init_hvu_info", "(", "self", ",", "categories", ",", "category_nums", ")", ":", "\n", "        ", "assert", "len", "(", "categories", ")", "==", "len", "(", "category_nums", ")", "\n", "self", ".", "categories", "=", "categories", "\n", "self", ".", "category_nums", "=", "category_nums", "\n", "self", ".", "num_categories", "=", "len", "(", "self", ".", "categories", ")", "\n", "self", ".", "num_tags", "=", "sum", "(", "self", ".", "category_nums", ")", "\n", "self", ".", "category2num", "=", "dict", "(", "zip", "(", "categories", ",", "category_nums", ")", ")", "\n", "self", ".", "start_idx", "=", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_categories", "-", "1", ")", ":", "\n", "            ", "self", ".", "start_idx", ".", "append", "(", "self", ".", "start_idx", "[", "-", "1", "]", "+", "self", ".", "category_nums", "[", "i", "]", ")", "\n", "", "self", ".", "category2startidx", "=", "dict", "(", "zip", "(", "categories", ",", "self", ".", "start_idx", ")", ")", "\n", "self", ".", "hvu_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadHVULabel.__call__": [[44, 75], ["torch.zeros", "torch.zeros", "torch.zeros", "results[].items", "loading.LoadHVULabel.init_hvu_info", "loading.LoadHVULabel.categories.index"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadHVULabel.init_hvu_info"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Convert the label dictionary to 3 tensors: \"label\", \"mask\" and\n        \"category_mask\".\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "\n", "if", "not", "self", ".", "hvu_initialized", ":", "\n", "            ", "self", ".", "init_hvu_info", "(", "results", "[", "'categories'", "]", ",", "results", "[", "'category_nums'", "]", ")", "\n", "\n", "", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_tags", ")", "\n", "onehot_mask", "=", "torch", ".", "zeros", "(", "self", ".", "num_tags", ")", "\n", "category_mask", "=", "torch", ".", "zeros", "(", "self", ".", "num_categories", ")", "\n", "\n", "for", "category", ",", "tags", "in", "results", "[", "'label'", "]", ".", "items", "(", ")", ":", "\n", "# skip if not training on this category", "\n", "            ", "if", "category", "not", "in", "self", ".", "categories", ":", "\n", "                ", "continue", "\n", "", "category_mask", "[", "self", ".", "categories", ".", "index", "(", "category", ")", "]", "=", "1.", "\n", "start_idx", "=", "self", ".", "category2startidx", "[", "category", "]", "\n", "category_num", "=", "self", ".", "category2num", "[", "category", "]", "\n", "tags", "=", "[", "idx", "+", "start_idx", "for", "idx", "in", "tags", "]", "\n", "onehot", "[", "tags", "]", "=", "1.", "\n", "onehot_mask", "[", "start_idx", ":", "category_num", "+", "start_idx", "]", "=", "1.", "\n", "\n", "", "results", "[", "'label'", "]", "=", "onehot", "\n", "results", "[", "'mask'", "]", "=", "onehot_mask", "\n", "results", "[", "'category_mask'", "]", "=", "category_mask", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadHVULabel.__repr__": [[76, 80], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'hvu_initialized={self.hvu_initialized})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames.__init__": [[111, 134], ["warnings.warn"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "clip_len", ",", "\n", "frame_interval", "=", "1", ",", "\n", "num_clips", "=", "1", ",", "\n", "temporal_jitter", "=", "False", ",", "\n", "twice_sample", "=", "False", ",", "\n", "out_of_bound_opt", "=", "'loop'", ",", "\n", "test_mode", "=", "False", ",", "\n", "start_index", "=", "None", ",", "\n", "keep_tail_frames", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "clip_len", "=", "clip_len", "\n", "self", ".", "frame_interval", "=", "frame_interval", "\n", "self", ".", "num_clips", "=", "num_clips", "\n", "self", ".", "temporal_jitter", "=", "temporal_jitter", "\n", "self", ".", "twice_sample", "=", "twice_sample", "\n", "self", ".", "out_of_bound_opt", "=", "out_of_bound_opt", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "keep_tail_frames", "=", "keep_tail_frames", "\n", "assert", "self", ".", "out_of_bound_opt", "in", "[", "'loop'", ",", "'repeat_last'", "]", "\n", "\n", "if", "start_index", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "'No longer support \"start_index\" in \"SampleFrames\", '", "\n", "'it should be set in dataset class, see this pr: '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames._get_train_clips": [[137, 180], ["float", "numpy.zeros", "numpy.arange", "numpy.arange", "numpy.random.randint", "max", "numpy.sort", "numpy.random.randint", "numpy.around", "numpy.zeros", "numpy.random.uniform", "numpy.arange"], "methods", ["None"], ["", "", "def", "_get_train_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets in train mode.\n\n        It will calculate the average interval for selected frames,\n        and randomly shift them within offsets between [0, avg_interval].\n        If the total number of frames is smaller than clips num or origin\n        frames length, it will return all zero indices.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "\n", "if", "self", ".", "keep_tail_frames", ":", "\n", "            ", "avg_interval", "=", "(", "num_frames", "-", "ori_clip_len", "+", "1", ")", "/", "float", "(", "\n", "self", ".", "num_clips", ")", "\n", "if", "num_frames", ">", "ori_clip_len", "-", "1", ":", "\n", "                ", "base_offsets", "=", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "avg_interval", "\n", "clip_offsets", "=", "(", "base_offsets", "+", "np", ".", "random", ".", "uniform", "(", "\n", "0", ",", "avg_interval", ",", "self", ".", "num_clips", ")", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "", "else", ":", "\n", "                ", "clip_offsets", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_clips", ",", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "", "else", ":", "\n", "            ", "avg_interval", "=", "(", "num_frames", "-", "ori_clip_len", "+", "1", ")", "//", "self", ".", "num_clips", "\n", "\n", "if", "avg_interval", ">", "0", ":", "\n", "                ", "base_offsets", "=", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "avg_interval", "\n", "clip_offsets", "=", "base_offsets", "+", "np", ".", "random", ".", "randint", "(", "\n", "avg_interval", ",", "size", "=", "self", ".", "num_clips", ")", "\n", "", "elif", "num_frames", ">", "max", "(", "self", ".", "num_clips", ",", "ori_clip_len", ")", ":", "\n", "                ", "clip_offsets", "=", "np", ".", "sort", "(", "\n", "np", ".", "random", ".", "randint", "(", "\n", "num_frames", "-", "ori_clip_len", "+", "1", ",", "size", "=", "self", ".", "num_clips", ")", ")", "\n", "", "elif", "avg_interval", "==", "0", ":", "\n", "                ", "ratio", "=", "(", "num_frames", "-", "ori_clip_len", "+", "1.0", ")", "/", "self", ".", "num_clips", "\n", "clip_offsets", "=", "np", ".", "around", "(", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "ratio", ")", "\n", "", "else", ":", "\n", "                ", "clip_offsets", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_clips", ",", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "", "", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames._get_test_clips": [[181, 205], ["float", "numpy.zeros", "numpy.arange", "numpy.concatenate"], "methods", ["None"], ["", "def", "_get_test_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets in test mode.\n\n        Calculate the average interval for selected frames, and shift them\n        fixedly by avg_interval/2. If set twice_sample True, it will sample\n        frames together without fixed shift. If the total number of frames is\n        not enough, it will return all zero indices.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in test mode.\n        \"\"\"", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "avg_interval", "=", "(", "num_frames", "-", "ori_clip_len", "+", "1", ")", "/", "float", "(", "self", ".", "num_clips", ")", "\n", "if", "num_frames", ">", "ori_clip_len", "-", "1", ":", "\n", "            ", "base_offsets", "=", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "avg_interval", "\n", "clip_offsets", "=", "(", "base_offsets", "+", "avg_interval", "/", "2.0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "if", "self", ".", "twice_sample", ":", "\n", "                ", "clip_offsets", "=", "np", ".", "concatenate", "(", "[", "clip_offsets", ",", "base_offsets", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "clip_offsets", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_clips", ",", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames._sample_clips": [[206, 221], ["loading.SampleFrames._get_test_clips", "loading.SampleFrames._get_train_clips"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_test_clips", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_train_clips"], ["", "def", "_sample_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Choose clip offsets for the video in a given mode.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices.\n        \"\"\"", "\n", "if", "self", ".", "test_mode", ":", "\n", "            ", "clip_offsets", "=", "self", ".", "_get_test_clips", "(", "num_frames", ")", "\n", "", "else", ":", "\n", "            ", "clip_offsets", "=", "self", ".", "_get_train_clips", "(", "num_frames", ")", "\n", "\n", "", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames.__call__": [[222, 260], ["loading.SampleFrames._sample_clips", "numpy.concatenate", "numpy.mod.reshape", "numpy.mod.astype", "numpy.random.randint", "numpy.mod", "numpy.concatenate", "numpy.max", "ValueError", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._sample_clips"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the SampleFrames loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "total_frames", "=", "results", "[", "'total_frames'", "]", "\n", "\n", "clip_offsets", "=", "self", ".", "_sample_clips", "(", "total_frames", ")", "\n", "frame_inds", "=", "clip_offsets", "[", ":", ",", "None", "]", "+", "np", ".", "arange", "(", "\n", "self", ".", "clip_len", ")", "[", "None", ",", ":", "]", "*", "self", ".", "frame_interval", "\n", "frame_inds", "=", "np", ".", "concatenate", "(", "frame_inds", ")", "\n", "\n", "if", "self", ".", "temporal_jitter", ":", "\n", "            ", "perframe_offsets", "=", "np", ".", "random", ".", "randint", "(", "\n", "self", ".", "frame_interval", ",", "size", "=", "len", "(", "frame_inds", ")", ")", "\n", "frame_inds", "+=", "perframe_offsets", "\n", "\n", "", "frame_inds", "=", "frame_inds", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "clip_len", ")", ")", "\n", "if", "self", ".", "out_of_bound_opt", "==", "'loop'", ":", "\n", "            ", "frame_inds", "=", "np", ".", "mod", "(", "frame_inds", ",", "total_frames", ")", "\n", "", "elif", "self", ".", "out_of_bound_opt", "==", "'repeat_last'", ":", "\n", "            ", "safe_inds", "=", "frame_inds", "<", "total_frames", "\n", "unsafe_inds", "=", "1", "-", "safe_inds", "\n", "last_ind", "=", "np", ".", "max", "(", "safe_inds", "*", "frame_inds", ",", "axis", "=", "1", ")", "\n", "new_inds", "=", "(", "safe_inds", "*", "frame_inds", "+", "(", "unsafe_inds", ".", "T", "*", "last_ind", ")", ".", "T", ")", "\n", "frame_inds", "=", "new_inds", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Illegal out_of_bound option.'", ")", "\n", "\n", "", "start_index", "=", "results", "[", "'start_index'", "]", "\n", "frame_inds", "=", "np", ".", "concatenate", "(", "frame_inds", ")", "+", "start_index", "\n", "results", "[", "'frame_inds'", "]", "=", "frame_inds", ".", "astype", "(", "np", ".", "int", ")", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "self", ".", "frame_interval", "\n", "results", "[", "'num_clips'", "]", "=", "self", ".", "num_clips", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleFrames.__repr__": [[261, 271], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'frame_interval={self.frame_interval}, '", "\n", "f'num_clips={self.num_clips}, '", "\n", "f'temporal_jitter={self.temporal_jitter}, '", "\n", "f'twice_sample={self.twice_sample}, '", "\n", "f'out_of_bound_opt={self.out_of_bound_opt}, '", "\n", "f'test_mode={self.test_mode})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.UntrimmedSampleFrames.__init__": [[289, 296], ["warnings.warn"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "clip_len", "=", "1", ",", "frame_interval", "=", "16", ",", "start_index", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "clip_len", "=", "clip_len", "\n", "self", ".", "frame_interval", "=", "frame_interval", "\n", "\n", "if", "start_index", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "'No longer support \"start_index\" in \"SampleFrames\", '", "\n", "'it should be set in dataset class, see this pr: '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.UntrimmedSampleFrames.__call__": [[299, 324], ["numpy.arange", "numpy.clip", "numpy.clip.astype", "numpy.concatenate", "numpy.arange"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the SampleFrames loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "total_frames", "=", "results", "[", "'total_frames'", "]", "\n", "start_index", "=", "results", "[", "'start_index'", "]", "\n", "\n", "clip_centers", "=", "np", ".", "arange", "(", "self", ".", "frame_interval", "//", "2", ",", "total_frames", ",", "\n", "self", ".", "frame_interval", ")", "\n", "num_clips", "=", "clip_centers", ".", "shape", "[", "0", "]", "\n", "frame_inds", "=", "clip_centers", "[", ":", ",", "None", "]", "+", "np", ".", "arange", "(", "\n", "-", "(", "self", ".", "clip_len", "//", "2", ")", ",", "self", ".", "clip_len", "-", "\n", "(", "self", ".", "clip_len", "//", "2", ")", ")", "[", "None", ",", ":", "]", "\n", "# clip frame_inds to legal range", "\n", "frame_inds", "=", "np", ".", "clip", "(", "frame_inds", ",", "0", ",", "total_frames", "-", "1", ")", "\n", "\n", "frame_inds", "=", "np", ".", "concatenate", "(", "frame_inds", ")", "+", "start_index", "\n", "results", "[", "'frame_inds'", "]", "=", "frame_inds", ".", "astype", "(", "np", ".", "int", ")", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "self", ".", "frame_interval", "\n", "results", "[", "'num_clips'", "]", "=", "num_clips", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.UntrimmedSampleFrames.__repr__": [[325, 330], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'frame_interval={self.frame_interval})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DenseSampleFrames.__init__": [[355, 363], ["loading.SampleFrames.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "*", "args", ",", "\n", "sample_range", "=", "64", ",", "\n", "num_sample_positions", "=", "10", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "sample_range", "=", "sample_range", "\n", "self", ".", "num_sample_positions", "=", "num_sample_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DenseSampleFrames._get_train_clips": [[364, 385], ["max", "numpy.random.randint", "numpy.arange"], "methods", ["None"], ["", "def", "_get_train_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets by dense sample strategy in train mode.\n\n        It will calculate a sample position and sample interval and set\n        start index 0 when sample_pos == 1 or randomly choose from\n        [0, sample_pos - 1]. Then it will shift the start index by each\n        base offset.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "sample_position", "=", "max", "(", "1", ",", "1", "+", "num_frames", "-", "self", ".", "sample_range", ")", "\n", "interval", "=", "self", ".", "sample_range", "//", "self", ".", "num_clips", "\n", "start_idx", "=", "0", "if", "sample_position", "==", "1", "else", "np", ".", "random", ".", "randint", "(", "\n", "0", ",", "sample_position", "-", "1", ")", "\n", "base_offsets", "=", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "interval", "\n", "clip_offsets", "=", "(", "base_offsets", "+", "start_idx", ")", "%", "num_frames", "\n", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DenseSampleFrames._get_test_clips": [[386, 410], ["max", "numpy.linspace", "list", "numpy.array", "numpy.arange", "numpy.array.extend"], "methods", ["None"], ["", "def", "_get_test_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets by dense sample strategy in test mode.\n\n        It will calculate a sample position and sample interval and evenly\n        sample several start indexes as start positions between\n        [0, sample_position-1]. Then it will shift each start index by the\n        base offsets.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "sample_position", "=", "max", "(", "1", ",", "1", "+", "num_frames", "-", "self", ".", "sample_range", ")", "\n", "interval", "=", "self", ".", "sample_range", "//", "self", ".", "num_clips", "\n", "start_list", "=", "np", ".", "linspace", "(", "\n", "0", ",", "sample_position", "-", "1", ",", "num", "=", "self", ".", "num_sample_positions", ",", "dtype", "=", "int", ")", "\n", "base_offsets", "=", "np", ".", "arange", "(", "self", ".", "num_clips", ")", "*", "interval", "\n", "clip_offsets", "=", "list", "(", ")", "\n", "for", "start_idx", "in", "start_list", ":", "\n", "            ", "clip_offsets", ".", "extend", "(", "(", "base_offsets", "+", "start_idx", ")", "%", "num_frames", ")", "\n", "", "clip_offsets", "=", "np", ".", "array", "(", "clip_offsets", ")", "\n", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DenseSampleFrames.__repr__": [[411, 422], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'frame_interval={self.frame_interval}, '", "\n", "f'num_clips={self.num_clips}, '", "\n", "f'sample_range={self.sample_range}, '", "\n", "f'num_sample_positions={self.num_sample_positions}, '", "\n", "f'temporal_jitter={self.temporal_jitter}, '", "\n", "f'out_of_bound_opt={self.out_of_bound_opt}, '", "\n", "f'test_mode={self.test_mode})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleAVAFrames.__init__": [[427, 430], ["loading.SampleFrames.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["    ", "def", "__init__", "(", "self", ",", "clip_len", ",", "frame_interval", "=", "2", ",", "test_mode", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "clip_len", ",", "frame_interval", ",", "test_mode", "=", "test_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleAVAFrames._get_clips": [[431, 439], ["list", "numpy.clip", "range"], "methods", ["None"], ["", "def", "_get_clips", "(", "self", ",", "center_index", ",", "skip_offsets", ",", "shot_info", ")", ":", "\n", "        ", "start", "=", "center_index", "-", "(", "self", ".", "clip_len", "//", "2", ")", "*", "self", ".", "frame_interval", "\n", "end", "=", "center_index", "+", "(", "(", "self", ".", "clip_len", "+", "1", ")", "//", "2", ")", "*", "self", ".", "frame_interval", "\n", "frame_inds", "=", "list", "(", "range", "(", "start", ",", "end", ",", "self", ".", "frame_interval", ")", ")", "\n", "if", "not", "self", ".", "test_mode", ":", "\n", "            ", "frame_inds", "=", "frame_inds", "+", "skip_offsets", "\n", "", "frame_inds", "=", "np", ".", "clip", "(", "frame_inds", ",", "shot_info", "[", "0", "]", ",", "shot_info", "[", "1", "]", "-", "1", ")", "\n", "return", "frame_inds", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleAVAFrames.__call__": [[440, 461], ["numpy.random.randint", "loading.SampleAVAFrames._get_clips", "results.get", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleAVAFrames._get_clips", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "fps", "=", "results", "[", "'fps'", "]", "\n", "timestamp", "=", "results", "[", "'timestamp'", "]", "\n", "timestamp_start", "=", "results", "[", "'timestamp_start'", "]", "\n", "shot_info", "=", "results", "[", "'shot_info'", "]", "\n", "\n", "center_index", "=", "fps", "*", "(", "timestamp", "-", "timestamp_start", ")", "+", "1", "\n", "\n", "skip_offsets", "=", "np", ".", "random", ".", "randint", "(", "\n", "-", "self", ".", "frame_interval", "//", "2", ",", "(", "self", ".", "frame_interval", "+", "1", ")", "//", "2", ",", "\n", "size", "=", "self", ".", "clip_len", ")", "\n", "frame_inds", "=", "self", ".", "_get_clips", "(", "center_index", ",", "skip_offsets", ",", "shot_info", ")", "\n", "start_index", "=", "results", ".", "get", "(", "'start_index'", ",", "0", ")", "\n", "\n", "frame_inds", "=", "np", ".", "array", "(", "frame_inds", ",", "dtype", "=", "np", ".", "int", ")", "+", "start_index", "\n", "results", "[", "'frame_inds'", "]", "=", "frame_inds", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "self", ".", "frame_interval", "\n", "results", "[", "'num_clips'", "]", "=", "1", "\n", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "1", ",", "1", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleAVAFrames.__repr__": [[462, 468], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'frame_interval={self.frame_interval}, '", "\n", "f'test_mode={self.test_mode})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames.__init__": [[495, 519], ["loading.SampleFrames.__init__", "torch.nn.modules.utils._pair", "mmcv.is_tuple_of", "TypeError", "len", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "clip_len", ",", "\n", "body_segments", ",", "\n", "aug_segments", ",", "\n", "aug_ratio", ",", "\n", "frame_interval", "=", "1", ",", "\n", "test_interval", "=", "6", ",", "\n", "temporal_jitter", "=", "False", ",", "\n", "mode", "=", "'train'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "clip_len", ",", "\n", "frame_interval", "=", "frame_interval", ",", "\n", "temporal_jitter", "=", "temporal_jitter", ")", "\n", "self", ".", "body_segments", "=", "body_segments", "\n", "self", ".", "aug_segments", "=", "aug_segments", "\n", "self", ".", "aug_ratio", "=", "_pair", "(", "aug_ratio", ")", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "aug_ratio", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'aug_ratio should be int, float'", "\n", "f'or tuple of int and float, '", "\n", "f'but got {type(aug_ratio)}'", ")", "\n", "", "assert", "len", "(", "self", ".", "aug_ratio", ")", "==", "2", "\n", "assert", "mode", "in", "[", "'train'", ",", "'val'", ",", "'test'", "]", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "test_interval", "=", "test_interval", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_train_indices": [[520, 546], ["numpy.zeros", "numpy.arange", "numpy.random.randint"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_train_indices", "(", "valid_length", ",", "num_segments", ")", ":", "\n", "        ", "\"\"\"Get indices of different stages of proposals in train mode.\n\n        It will calculate the average interval for each segment,\n        and randomly shift them within offsets between [0, average_duration].\n        If the total number of frames is smaller than num segments, it will\n        return all zero indices.\n\n        Args:\n            valid_length (int): The length of the starting point's\n                valid interval.\n            num_segments (int): Total number of segments.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "avg_interval", "=", "(", "valid_length", "+", "1", ")", "//", "num_segments", "\n", "if", "avg_interval", ">", "0", ":", "\n", "            ", "base_offsets", "=", "np", ".", "arange", "(", "num_segments", ")", "*", "avg_interval", "\n", "offsets", "=", "base_offsets", "+", "np", ".", "random", ".", "randint", "(", "\n", "avg_interval", ",", "size", "=", "num_segments", ")", "\n", "", "else", ":", "\n", "            ", "offsets", "=", "np", ".", "zeros", "(", "(", "num_segments", ",", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_val_indices": [[547, 571], ["numpy.zeros", "float", "numpy.arange"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_val_indices", "(", "valid_length", ",", "num_segments", ")", ":", "\n", "        ", "\"\"\"Get indices of different stages of proposals in validation mode.\n\n        It will calculate the average interval for each segment.\n        If the total number of valid length is smaller than num segments,\n        it will return all zero indices.\n\n        Args:\n            valid_length (int): The length of the starting point's\n                valid interval.\n            num_segments (int): Total number of segments.\n\n        Returns:\n            np.ndarray: Sampled frame indices in validation mode.\n        \"\"\"", "\n", "if", "valid_length", ">=", "num_segments", ":", "\n", "            ", "avg_interval", "=", "valid_length", "/", "float", "(", "num_segments", ")", "\n", "base_offsets", "=", "np", ".", "arange", "(", "num_segments", ")", "*", "avg_interval", "\n", "offsets", "=", "(", "base_offsets", "+", "avg_interval", "/", "2.0", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "", "else", ":", "\n", "            ", "offsets", "=", "np", ".", "zeros", "(", "(", "num_segments", ",", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_proposal_clips": [[572, 623], ["max", "min", "numpy.concatenate", "loading.SampleProposalFrames._get_train_indices", "loading.SampleProposalFrames._get_train_indices", "loading.SampleProposalFrames._get_train_indices", "int", "int", "loading.SampleProposalFrames._get_val_indices", "loading.SampleProposalFrames._get_val_indices", "loading.SampleProposalFrames._get_val_indices"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_train_indices", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_train_indices", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_train_indices", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_val_indices", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_val_indices", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_val_indices"], ["", "def", "_get_proposal_clips", "(", "self", ",", "proposal", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets in train mode.\n\n        It will calculate sampled frame indices in the proposal's three\n        stages: starting, course and ending stage.\n\n        Args:\n            proposal (obj): The proposal object.\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "# proposal interval: [start_frame, end_frame)", "\n", "start_frame", "=", "proposal", ".", "start_frame", "\n", "end_frame", "=", "proposal", ".", "end_frame", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "\n", "duration", "=", "end_frame", "-", "start_frame", "\n", "assert", "duration", "!=", "0", "\n", "valid_length", "=", "duration", "-", "ori_clip_len", "\n", "\n", "valid_starting", "=", "max", "(", "0", ",", "\n", "start_frame", "-", "int", "(", "duration", "*", "self", ".", "aug_ratio", "[", "0", "]", ")", ")", "\n", "valid_ending", "=", "min", "(", "num_frames", "-", "ori_clip_len", "+", "1", ",", "\n", "end_frame", "-", "1", "+", "int", "(", "duration", "*", "self", ".", "aug_ratio", "[", "1", "]", ")", ")", "\n", "\n", "valid_starting_length", "=", "start_frame", "-", "valid_starting", "-", "ori_clip_len", "\n", "valid_ending_length", "=", "(", "valid_ending", "-", "end_frame", "+", "1", ")", "-", "ori_clip_len", "\n", "\n", "if", "self", ".", "mode", "==", "'train'", ":", "\n", "            ", "starting_offsets", "=", "self", ".", "_get_train_indices", "(", "valid_starting_length", ",", "\n", "self", ".", "aug_segments", "[", "0", "]", ")", "\n", "course_offsets", "=", "self", ".", "_get_train_indices", "(", "valid_length", ",", "\n", "self", ".", "body_segments", ")", "\n", "ending_offsets", "=", "self", ".", "_get_train_indices", "(", "valid_ending_length", ",", "\n", "self", ".", "aug_segments", "[", "1", "]", ")", "\n", "", "elif", "self", ".", "mode", "==", "'val'", ":", "\n", "            ", "starting_offsets", "=", "self", ".", "_get_val_indices", "(", "valid_starting_length", ",", "\n", "self", ".", "aug_segments", "[", "0", "]", ")", "\n", "course_offsets", "=", "self", ".", "_get_val_indices", "(", "valid_length", ",", "\n", "self", ".", "body_segments", ")", "\n", "ending_offsets", "=", "self", ".", "_get_val_indices", "(", "valid_ending_length", ",", "\n", "self", ".", "aug_segments", "[", "1", "]", ")", "\n", "", "starting_offsets", "+=", "valid_starting", "\n", "course_offsets", "+=", "start_frame", "\n", "ending_offsets", "+=", "end_frame", "\n", "\n", "offsets", "=", "np", ".", "concatenate", "(", "\n", "(", "starting_offsets", ",", "course_offsets", ",", "ending_offsets", ")", ")", "\n", "return", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_train_clips": [[624, 645], ["loading.SampleProposalFrames._get_proposal_clips", "numpy.concatenate"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_proposal_clips"], ["", "def", "_get_train_clips", "(", "self", ",", "num_frames", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"Get clip offsets in train mode.\n\n        It will calculate sampled frame indices of each proposal, and then\n        assemble them.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n            proposals (list): Proposals fetched.\n\n        Returns:\n            np.ndarray: Sampled frame indices in train mode.\n        \"\"\"", "\n", "clip_offsets", "=", "[", "]", "\n", "for", "proposal", "in", "proposals", ":", "\n", "            ", "proposal_clip_offsets", "=", "self", ".", "_get_proposal_clips", "(", "\n", "proposal", "[", "0", "]", "[", "1", "]", ",", "num_frames", ")", "\n", "clip_offsets", "=", "np", ".", "concatenate", "(", "\n", "[", "clip_offsets", ",", "proposal_clip_offsets", "]", ")", "\n", "\n", "", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._get_test_clips": [[646, 660], ["numpy.arange"], "methods", ["None"], ["", "def", "_get_test_clips", "(", "self", ",", "num_frames", ")", ":", "\n", "        ", "\"\"\"Get clip offsets in test mode.\n\n        It will calculate sampled frame indices based on test interval.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n\n        Returns:\n            np.ndarray: Sampled frame indices in test mode.\n        \"\"\"", "\n", "ori_clip_len", "=", "self", ".", "clip_len", "*", "self", ".", "frame_interval", "\n", "return", "np", ".", "arange", "(", "\n", "0", ",", "num_frames", "-", "ori_clip_len", ",", "self", ".", "test_interval", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._sample_clips": [[661, 679], ["loading.SampleProposalFrames._get_test_clips", "loading.SampleProposalFrames._get_train_clips"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_test_clips", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_train_clips"], ["", "def", "_sample_clips", "(", "self", ",", "num_frames", ",", "proposals", ")", ":", "\n", "        ", "\"\"\"Choose clip offsets for the video in a given mode.\n\n        Args:\n            num_frames (int): Total number of frame in the video.\n            proposals (list | None): Proposals fetched.\n                It is set to None in test mode.\n\n        Returns:\n            np.ndarray: Sampled frame indices.\n        \"\"\"", "\n", "if", "self", ".", "mode", "==", "'test'", ":", "\n", "            ", "clip_offsets", "=", "self", ".", "_get_test_clips", "(", "num_frames", ")", "\n", "", "else", ":", "\n", "            ", "assert", "proposals", "is", "not", "None", "\n", "clip_offsets", "=", "self", ".", "_get_train_clips", "(", "num_frames", ",", "proposals", ")", "\n", "\n", "", "return", "clip_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames.__call__": [[680, 712], ["results.get", "loading.SampleProposalFrames._sample_clips", "numpy.concatenate", "numpy.array().astype", "numpy.random.randint", "numpy.mod", "len", "numpy.array", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames._sample_clips"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the SampleFrames loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "total_frames", "=", "results", "[", "'total_frames'", "]", "\n", "\n", "out_proposals", "=", "results", ".", "get", "(", "'out_proposals'", ",", "None", ")", "\n", "clip_offsets", "=", "self", ".", "_sample_clips", "(", "total_frames", ",", "out_proposals", ")", "\n", "frame_inds", "=", "clip_offsets", "[", ":", ",", "None", "]", "+", "np", ".", "arange", "(", "\n", "self", ".", "clip_len", ")", "[", "None", ",", ":", "]", "*", "self", ".", "frame_interval", "\n", "frame_inds", "=", "np", ".", "concatenate", "(", "frame_inds", ")", "\n", "\n", "if", "self", ".", "temporal_jitter", ":", "\n", "            ", "perframe_offsets", "=", "np", ".", "random", ".", "randint", "(", "\n", "self", ".", "frame_interval", ",", "size", "=", "len", "(", "frame_inds", ")", ")", "\n", "frame_inds", "+=", "perframe_offsets", "\n", "\n", "", "start_index", "=", "results", "[", "'start_index'", "]", "\n", "frame_inds", "=", "np", ".", "mod", "(", "frame_inds", ",", "total_frames", ")", "+", "start_index", "\n", "\n", "results", "[", "'frame_inds'", "]", "=", "np", ".", "array", "(", "frame_inds", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "self", ".", "frame_interval", "\n", "results", "[", "'num_clips'", "]", "=", "(", "\n", "self", ".", "body_segments", "+", "self", ".", "aug_segments", "[", "0", "]", "+", "self", ".", "aug_segments", "[", "1", "]", ")", "\n", "if", "self", ".", "mode", "in", "[", "'train'", ",", "'val'", "]", ":", "\n", "            ", "results", "[", "'num_proposals'", "]", "=", "len", "(", "results", "[", "'out_proposals'", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.SampleProposalFrames.__repr__": [[713, 724], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'body_segments={self.body_segments}, '", "\n", "f'aug_segments={self.aug_segments}, '", "\n", "f'aug_ratio={self.aug_ratio}, '", "\n", "f'frame_interval={self.frame_interval}, '", "\n", "f'test_interval={self.test_interval}, '", "\n", "f'temporal_jitter={self.temporal_jitter}, '", "\n", "f'mode={self.mode})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVInit.__init__": [[741, 745], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVInit.__call__": [[746, 769], ["io.BytesIO", "av.open", "mmcv.fileio.FileClient", "loading.PyAVInit.file_client.get", "ImportError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the PyAV initialization.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "av", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please run \"conda install av -c conda-forge\" '", "\n", "'or \"pip install av\" to install PyAV first.'", ")", "\n", "\n", "", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "file_obj", "=", "io", ".", "BytesIO", "(", "self", ".", "file_client", ".", "get", "(", "results", "[", "'filename'", "]", ")", ")", "\n", "container", "=", "av", ".", "open", "(", "file_obj", ")", "\n", "\n", "results", "[", "'video_reader'", "]", "=", "container", "\n", "results", "[", "'total_frames'", "]", "=", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "frames", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVInit.__repr__": [[770, 773], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}(io_backend={self.io_backend})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecode.__init__": [[795, 799], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "multi_thread", "=", "False", ",", "mode", "=", "'accurate'", ")", ":", "\n", "        ", "self", ".", "multi_thread", "=", "multi_thread", "\n", "self", ".", "mode", "=", "mode", "\n", "assert", "mode", "in", "[", "'accurate'", ",", "'efficient'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecode.frame_generator": [[800, 807], ["container.demux", "packet.decode", "frame.to_rgb().to_ndarray", "frame.to_rgb"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "frame_generator", "(", "container", ",", "stream", ")", ":", "\n", "        ", "\"\"\"Frame generator for PyAV.\"\"\"", "\n", "for", "packet", "in", "container", ".", "demux", "(", "stream", ")", ":", "\n", "            ", "for", "frame", "in", "packet", ".", "decode", "(", ")", ":", "\n", "                ", "if", "frame", ":", "\n", "                    ", "return", "frame", ".", "to_rgb", "(", ")", ".", "to_ndarray", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecode.__call__": [[808, 861], ["list", "numpy.squeeze", "max", "container.decode", "list.append", "container.decode", "loading.PyAVDecode.to_rgb().to_ndarray", "int", "container.seek", "loading.PyAVDecode.frame_generator", "list.append", "list.append", "loading.PyAVDecode.to_rgb", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecode.frame_generator"], ["", "", "", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the PyAV decoding.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "container", "=", "results", "[", "'video_reader'", "]", "\n", "imgs", "=", "list", "(", ")", "\n", "\n", "if", "self", ".", "multi_thread", ":", "\n", "            ", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "thread_type", "=", "'AUTO'", "\n", "", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "if", "self", ".", "mode", "==", "'accurate'", ":", "\n", "# set max indice to make early stop", "\n", "            ", "max_inds", "=", "max", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "i", "=", "0", "\n", "for", "frame", "in", "container", ".", "decode", "(", "video", "=", "0", ")", ":", "\n", "                ", "if", "i", ">", "max_inds", "+", "1", ":", "\n", "                    ", "break", "\n", "", "imgs", ".", "append", "(", "frame", ".", "to_rgb", "(", ")", ".", "to_ndarray", "(", ")", ")", "\n", "i", "+=", "1", "\n", "\n", "# the available frame in pyav may be less than its length,", "\n", "# which may raise error", "\n", "", "results", "[", "'imgs'", "]", "=", "[", "\n", "imgs", "[", "i", "%", "len", "(", "imgs", ")", "]", "for", "i", "in", "results", "[", "'frame_inds'", "]", "\n", "]", "\n", "", "elif", "self", ".", "mode", "==", "'efficient'", ":", "\n", "            ", "for", "frame", "in", "container", ".", "decode", "(", "video", "=", "0", ")", ":", "\n", "                ", "backup_frame", "=", "frame", "\n", "break", "\n", "", "stream", "=", "container", ".", "streams", ".", "video", "[", "0", "]", "\n", "for", "idx", "in", "results", "[", "'frame_inds'", "]", ":", "\n", "                ", "pts_scale", "=", "stream", ".", "average_rate", "*", "stream", ".", "time_base", "\n", "frame_pts", "=", "int", "(", "idx", "/", "pts_scale", ")", "\n", "container", ".", "seek", "(", "\n", "frame_pts", ",", "any_frame", "=", "False", ",", "backward", "=", "True", ",", "stream", "=", "stream", ")", "\n", "frame", "=", "self", ".", "frame_generator", "(", "container", ",", "stream", ")", "\n", "if", "frame", "is", "not", "None", ":", "\n", "                    ", "imgs", ".", "append", "(", "frame", ")", "\n", "backup_frame", "=", "frame", "\n", "", "else", ":", "\n", "                    ", "imgs", ".", "append", "(", "backup_frame", ")", "\n", "", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'video_reader'", "]", "=", "None", "\n", "del", "container", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecode.__repr__": [[862, 866], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "\n", "repr_str", "+=", "f'(multi_thread={self.multi_thread}, mode={self.mode})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PIMSInit.__init__": [[886, 892], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "mode", "=", "'accurate'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "self", ".", "mode", "=", "mode", "\n", "assert", "mode", "in", "[", "'accurate'", ",", "'efficient'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PIMSInit.__call__": [[893, 913], ["io.BytesIO", "len", "mmcv.fileio.FileClient", "loading.PIMSInit.file_client.get", "pims.PyAVReaderIndexed", "pims.PyAVReaderTimed", "ImportError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "pims", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please run \"conda install pims -c conda-forge\" '", "\n", "'or \"pip install pims\" to install pims first.'", ")", "\n", "\n", "", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "file_obj", "=", "io", ".", "BytesIO", "(", "self", ".", "file_client", ".", "get", "(", "results", "[", "'filename'", "]", ")", ")", "\n", "if", "self", ".", "mode", "==", "'accurate'", ":", "\n", "            ", "container", "=", "pims", ".", "PyAVReaderIndexed", "(", "file_obj", ")", "\n", "", "else", ":", "\n", "            ", "container", "=", "pims", ".", "PyAVReaderTimed", "(", "file_obj", ")", "\n", "\n", "", "results", "[", "'video_reader'", "]", "=", "container", "\n", "results", "[", "'total_frames'", "]", "=", "len", "(", "container", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PIMSInit.__repr__": [[914, 918], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}(io_backend={self.io_backend}, '", "\n", "f'mode={self.mode})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PIMSDecode.__call__": [[930, 947], ["numpy.squeeze"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "container", "=", "results", "[", "'video_reader'", "]", "\n", "\n", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "frame_inds", "=", "results", "[", "'frame_inds'", "]", "\n", "imgs", "=", "[", "container", "[", "idx", "]", "for", "idx", "in", "frame_inds", "]", "\n", "\n", "results", "[", "'video_reader'", "]", "=", "None", "\n", "del", "container", "\n", "\n", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecodeMotionVector._parse_vectors": [[960, 978], ["zip"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "_parse_vectors", "(", "mv", ",", "vectors", ",", "height", ",", "width", ")", ":", "\n", "        ", "\"\"\"Parse the returned vectors.\"\"\"", "\n", "(", "w", ",", "h", ",", "src_x", ",", "src_y", ",", "dst_x", ",", "\n", "dst_y", ")", "=", "(", "vectors", "[", "'w'", "]", ",", "vectors", "[", "'h'", "]", ",", "vectors", "[", "'src_x'", "]", ",", "\n", "vectors", "[", "'src_y'", "]", ",", "vectors", "[", "'dst_x'", "]", ",", "vectors", "[", "'dst_y'", "]", ")", "\n", "val_x", "=", "dst_x", "-", "src_x", "\n", "val_y", "=", "dst_y", "-", "src_y", "\n", "start_x", "=", "dst_x", "-", "w", "//", "2", "\n", "start_y", "=", "dst_y", "-", "h", "//", "2", "\n", "end_x", "=", "start_x", "+", "w", "\n", "end_y", "=", "start_y", "+", "h", "\n", "for", "sx", ",", "ex", ",", "sy", ",", "ey", ",", "vx", ",", "vy", "in", "zip", "(", "start_x", ",", "end_x", ",", "start_y", ",", "end_y", ",", "\n", "val_x", ",", "val_y", ")", ":", "\n", "            ", "if", "(", "sx", ">=", "0", "and", "ex", "<", "width", "and", "sy", ">=", "0", "and", "ey", "<", "height", ")", ":", "\n", "                ", "mv", "[", "sy", ":", "ey", ",", "sx", ":", "ex", "]", "=", "(", "vx", ",", "vy", ")", "\n", "\n", "", "", "return", "mv", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecodeMotionVector.__call__": [[979, 1025], ["list", "max", "container.demux", "numpy.array", "numpy.squeeze", "packet.decode", "numpy.zeros", "frame.side_data.get", "list.append", "loading.PyAVDecodeMotionVector._parse_vectors", "len", "frame.side_data.get.to_ndarray", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.PyAVDecodeMotionVector._parse_vectors"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the PyAV motion vector decoding.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "container", "=", "results", "[", "'video_reader'", "]", "\n", "imgs", "=", "list", "(", ")", "\n", "\n", "if", "self", ".", "multi_thread", ":", "\n", "            ", "container", ".", "streams", ".", "video", "[", "0", "]", ".", "thread_type", "=", "'AUTO'", "\n", "", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "# set max index to make early stop", "\n", "", "max_idx", "=", "max", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "i", "=", "0", "\n", "stream", "=", "container", ".", "streams", ".", "video", "[", "0", "]", "\n", "codec_context", "=", "stream", ".", "codec_context", "\n", "codec_context", ".", "options", "=", "{", "'flags2'", ":", "'+export_mvs'", "}", "\n", "for", "packet", "in", "container", ".", "demux", "(", "stream", ")", ":", "\n", "            ", "for", "frame", "in", "packet", ".", "decode", "(", ")", ":", "\n", "                ", "if", "i", ">", "max_idx", "+", "1", ":", "\n", "                    ", "break", "\n", "", "i", "+=", "1", "\n", "height", "=", "frame", ".", "height", "\n", "width", "=", "frame", ".", "width", "\n", "mv", "=", "np", ".", "zeros", "(", "(", "height", ",", "width", ",", "2", ")", ",", "dtype", "=", "np", ".", "int8", ")", "\n", "vectors", "=", "frame", ".", "side_data", ".", "get", "(", "'MOTION_VECTORS'", ")", "\n", "if", "frame", ".", "key_frame", ":", "\n", "# Key frame don't have motion vectors", "\n", "                    ", "assert", "vectors", "is", "None", "\n", "", "if", "vectors", "is", "not", "None", "and", "len", "(", "vectors", ")", ">", "0", ":", "\n", "                    ", "mv", "=", "self", ".", "_parse_vectors", "(", "mv", ",", "vectors", ".", "to_ndarray", "(", ")", ",", "height", ",", "\n", "width", ")", "\n", "", "imgs", ".", "append", "(", "mv", ")", "\n", "\n", "", "", "results", "[", "'video_reader'", "]", "=", "None", "\n", "del", "container", "\n", "\n", "# the available frame in pyav may be less than its length,", "\n", "# which may raise error", "\n", "results", "[", "'motion_vectors'", "]", "=", "np", ".", "array", "(", "\n", "[", "imgs", "[", "i", "%", "len", "(", "imgs", ")", "]", "for", "i", "in", "results", "[", "'frame_inds'", "]", "]", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordInit.__init__": [[1043, 1048], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "num_threads", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "num_threads", "=", "num_threads", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordInit.__call__": [[1049, 1070], ["io.BytesIO", "decord.VideoReader", "len", "mmcv.fileio.FileClient", "loading.DecordInit.file_client.get", "ImportError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the Decord initialization.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "decord", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "'Please run \"pip install decord\" to install Decord first.'", ")", "\n", "\n", "", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "file_obj", "=", "io", ".", "BytesIO", "(", "self", ".", "file_client", ".", "get", "(", "results", "[", "'filename'", "]", ")", ")", "\n", "container", "=", "decord", ".", "VideoReader", "(", "file_obj", ",", "num_threads", "=", "self", ".", "num_threads", ")", "\n", "results", "[", "'video_reader'", "]", "=", "container", "\n", "results", "[", "'total_frames'", "]", "=", "len", "(", "container", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordInit.__repr__": [[1071, 1076], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'io_backend={self.io_backend}, '", "\n", "f'num_threads={self.num_threads})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordDecode.__init__": [[1095, 1098], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mode", "=", "'accurate'", ")", ":", "\n", "        ", "self", ".", "mode", "=", "mode", "\n", "assert", "mode", "in", "[", "'accurate'", ",", "'efficient'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordDecode.__call__": [[1099, 1133], ["numpy.squeeze", "container.get_batch().asnumpy", "list", "container.seek", "list", "container.get_batch", "container.seek", "container.next", "list.append", "container.next.asnumpy"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the Decord decoding.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "container", "=", "results", "[", "'video_reader'", "]", "\n", "\n", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "frame_inds", "=", "results", "[", "'frame_inds'", "]", "\n", "\n", "if", "self", ".", "mode", "==", "'accurate'", ":", "\n", "            ", "imgs", "=", "container", ".", "get_batch", "(", "frame_inds", ")", ".", "asnumpy", "(", ")", "\n", "imgs", "=", "list", "(", "imgs", ")", "\n", "", "elif", "self", ".", "mode", "==", "'efficient'", ":", "\n", "# This mode is faster, however it always returns I-FRAME", "\n", "            ", "container", ".", "seek", "(", "0", ")", "\n", "imgs", "=", "list", "(", ")", "\n", "for", "idx", "in", "frame_inds", ":", "\n", "                ", "container", ".", "seek", "(", "idx", ")", "\n", "frame", "=", "container", ".", "next", "(", ")", "\n", "imgs", ".", "append", "(", "frame", ".", "asnumpy", "(", ")", ")", "\n", "\n", "", "", "results", "[", "'video_reader'", "]", "=", "None", "\n", "del", "container", "\n", "\n", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.DecordDecode.__repr__": [[1134, 1137], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}(mode={self.mode})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.OpenCVInit.__init__": [[1152, 1163], ["utils.get_random_string", "utils.get_thread_id", "os.join", "os.join", "os.mkdir", "os.mkdir", "os.mkdir", "os.mkdir", "utils.get_shm_dir"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_random_string", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_thread_id", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_shm_dir"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "self", ".", "tmp_folder", "=", "None", "\n", "if", "self", ".", "io_backend", "!=", "'disk'", ":", "\n", "            ", "random_string", "=", "get_random_string", "(", ")", "\n", "thread_id", "=", "get_thread_id", "(", ")", "\n", "self", ".", "tmp_folder", "=", "osp", ".", "join", "(", "get_shm_dir", "(", ")", ",", "\n", "f'{random_string}_{thread_id}'", ")", "\n", "os", ".", "mkdir", "(", "self", ".", "tmp_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.OpenCVInit.__call__": [[1164, 1189], ["mmcv.VideoReader", "len", "utils.get_thread_id", "os.join", "os.join", "mmcv.fileio.FileClient", "open", "f.write", "loading.OpenCVInit.file_client.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.misc.get_thread_id", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the OpenCV initialization.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "if", "self", ".", "io_backend", "==", "'disk'", ":", "\n", "            ", "new_path", "=", "results", "[", "'filename'", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "file_client", "is", "None", ":", "\n", "                ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "thread_id", "=", "get_thread_id", "(", ")", "\n", "# save the file of same thread at the same place", "\n", "new_path", "=", "osp", ".", "join", "(", "self", ".", "tmp_folder", ",", "f'tmp_{thread_id}.mp4'", ")", "\n", "with", "open", "(", "new_path", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "self", ".", "file_client", ".", "get", "(", "results", "[", "'filename'", "]", ")", ")", "\n", "\n", "", "", "container", "=", "mmcv", ".", "VideoReader", "(", "new_path", ")", "\n", "results", "[", "'new_path'", "]", "=", "new_path", "\n", "results", "[", "'video_reader'", "]", "=", "container", "\n", "results", "[", "'total_frames'", "]", "=", "len", "(", "container", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.OpenCVInit.__del__": [[1190, 1193], ["os.exists", "os.exists", "shutil.rmtree"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tmp_folder", "and", "osp", ".", "exists", "(", "self", ".", "tmp_folder", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "self", ".", "tmp_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.OpenCVInit.__repr__": [[1194, 1198], ["None"], "methods", ["None"], ["", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'io_backend={self.io_backend})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.OpenCVDecode.__call__": [[1208, 1240], ["list", "numpy.array", "list", "numpy.squeeze", "isinstance", "numpy.array.append", "type"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the OpenCV decoding.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "container", "=", "results", "[", "'video_reader'", "]", "\n", "imgs", "=", "list", "(", ")", "\n", "\n", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "for", "frame_ind", "in", "results", "[", "'frame_inds'", "]", ":", "\n", "            ", "cur_frame", "=", "container", "[", "frame_ind", "]", "\n", "# last frame may be None in OpenCV", "\n", "while", "isinstance", "(", "cur_frame", ",", "type", "(", "None", ")", ")", ":", "\n", "                ", "frame_ind", "-=", "1", "\n", "cur_frame", "=", "container", "[", "frame_ind", "]", "\n", "", "imgs", ".", "append", "(", "cur_frame", ")", "\n", "\n", "", "results", "[", "'video_reader'", "]", "=", "None", "\n", "del", "container", "\n", "\n", "imgs", "=", "np", ".", "array", "(", "imgs", ")", "\n", "# The default channel order of OpenCV is BGR, thus we change it to RGB", "\n", "imgs", "=", "imgs", "[", ":", ",", ":", ",", ":", ",", ":", ":", "-", "1", "]", "\n", "results", "[", "'imgs'", "]", "=", "list", "(", "imgs", ")", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.RawFrameDecode.__init__": [[1256, 1261], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "decoding_backend", "=", "'cv2'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "decoding_backend", "=", "decoding_backend", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.RawFrameDecode.__call__": [[1262, 1335], ["mmcv.use_backend", "list", "results.get", "enumerate", "mmcv.fileio.FileClient", "numpy.squeeze", "numpy.array", "os.join", "os.join", "loading.RawFrameDecode.file_client.get", "mmcv.imfrombytes", "list.append", "list.append", "list.append", "list.append", "filename_tmpl.format", "os.join", "os.join", "os.join", "os.join", "loading.RawFrameDecode.file_client.get", "mmcv.imfrombytes", "loading.RawFrameDecode.file_client.get", "mmcv.imfrombytes", "list.extend", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "filename_tmpl.format", "filename_tmpl.format"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.log.FormatterNoInfo.format"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the ``RawFrameDecode`` to pick frames given indices.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "mmcv", ".", "use_backend", "(", "self", ".", "decoding_backend", ")", "\n", "\n", "directory", "=", "results", "[", "'frame_dir'", "]", "\n", "filename_tmpl", "=", "results", "[", "'filename_tmpl'", "]", "\n", "modality", "=", "results", "[", "'modality'", "]", "\n", "\n", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "imgs", "=", "list", "(", ")", "\n", "\n", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "offset", "=", "results", ".", "get", "(", "'offset'", ",", "0", ")", "\n", "\n", "cache", "=", "{", "}", "\n", "for", "i", ",", "frame_idx", "in", "enumerate", "(", "results", "[", "'frame_inds'", "]", ")", ":", "\n", "# Avoid loading duplicated frames", "\n", "            ", "if", "frame_idx", "in", "cache", ":", "\n", "                ", "if", "modality", "==", "'RGB'", ":", "\n", "                    ", "imgs", ".", "append", "(", "cp", ".", "deepcopy", "(", "imgs", "[", "cache", "[", "frame_idx", "]", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "imgs", ".", "append", "(", "cp", ".", "deepcopy", "(", "imgs", "[", "2", "*", "cache", "[", "frame_idx", "]", "]", ")", ")", "\n", "imgs", ".", "append", "(", "cp", ".", "deepcopy", "(", "imgs", "[", "2", "*", "cache", "[", "frame_idx", "]", "+", "1", "]", ")", ")", "\n", "", "continue", "\n", "", "else", ":", "\n", "                ", "cache", "[", "frame_idx", "]", "=", "i", "\n", "\n", "", "frame_idx", "+=", "offset", "\n", "if", "modality", "==", "'RGB'", ":", "\n", "                ", "filepath", "=", "osp", ".", "join", "(", "directory", ",", "filename_tmpl", ".", "format", "(", "frame_idx", ")", ")", "\n", "img_bytes", "=", "self", ".", "file_client", ".", "get", "(", "filepath", ")", "\n", "# Get frame with channel order RGB directly.", "\n", "cur_frame", "=", "mmcv", ".", "imfrombytes", "(", "img_bytes", ",", "channel_order", "=", "'rgb'", ")", "\n", "imgs", ".", "append", "(", "cur_frame", ")", "\n", "", "elif", "modality", "==", "'Flow'", ":", "\n", "                ", "x_filepath", "=", "osp", ".", "join", "(", "directory", ",", "\n", "filename_tmpl", ".", "format", "(", "'x'", ",", "frame_idx", ")", ")", "\n", "y_filepath", "=", "osp", ".", "join", "(", "directory", ",", "\n", "filename_tmpl", ".", "format", "(", "'y'", ",", "frame_idx", ")", ")", "\n", "x_img_bytes", "=", "self", ".", "file_client", ".", "get", "(", "x_filepath", ")", "\n", "x_frame", "=", "mmcv", ".", "imfrombytes", "(", "x_img_bytes", ",", "flag", "=", "'grayscale'", ")", "\n", "y_img_bytes", "=", "self", ".", "file_client", ".", "get", "(", "y_filepath", ")", "\n", "y_frame", "=", "mmcv", ".", "imfrombytes", "(", "y_img_bytes", ",", "flag", "=", "'grayscale'", ")", "\n", "imgs", ".", "extend", "(", "[", "x_frame", ",", "y_frame", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# we resize the gt_bboxes and proposals to their real scale", "\n", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "h", ",", "w", "=", "results", "[", "'img_shape'", "]", "\n", "scale_factor", "=", "np", ".", "array", "(", "[", "w", ",", "h", ",", "w", ",", "h", "]", ")", "\n", "gt_bboxes", "=", "results", "[", "'gt_bboxes'", "]", "\n", "gt_bboxes", "=", "(", "gt_bboxes", "*", "scale_factor", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "results", "[", "'gt_bboxes'", "]", "=", "gt_bboxes", "\n", "if", "'proposals'", "in", "results", "and", "results", "[", "'proposals'", "]", "is", "not", "None", ":", "\n", "                ", "proposals", "=", "results", "[", "'proposals'", "]", "\n", "proposals", "=", "(", "proposals", "*", "scale_factor", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "results", "[", "'proposals'", "]", "=", "proposals", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.RawFrameDecode.__repr__": [[1336, 1341], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'io_backend={self.io_backend}, '", "\n", "f'decoding_backend={self.decoding_backend})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.ArrayDecode.__call__": [[1351, 1385], ["list", "results.get", "enumerate", "numpy.squeeze", "list.append", "list.extend"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the ``RawFrameDecode`` to pick frames given indices.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "\n", "modality", "=", "results", "[", "'modality'", "]", "\n", "array", "=", "results", "[", "'array'", "]", "\n", "\n", "imgs", "=", "list", "(", ")", "\n", "\n", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "offset", "=", "results", ".", "get", "(", "'offset'", ",", "0", ")", "\n", "\n", "for", "i", ",", "frame_idx", "in", "enumerate", "(", "results", "[", "'frame_inds'", "]", ")", ":", "\n", "\n", "            ", "frame_idx", "+=", "offset", "\n", "if", "modality", "==", "'RGB'", ":", "\n", "                ", "imgs", ".", "append", "(", "array", "[", "frame_idx", "]", ")", "\n", "", "elif", "modality", "==", "'Flow'", ":", "\n", "                ", "imgs", ".", "extend", "(", "\n", "[", "array", "[", "frame_idx", ",", "...", ",", "0", "]", ",", "array", "[", "frame_idx", ",", "...", ",", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.ArrayDecode.__repr__": [[1386, 1388], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f'{self.__class__.__name__}()'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.ImageDecode.__init__": [[1404, 1409], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "io_backend", "=", "'disk'", ",", "decoding_backend", "=", "'cv2'", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "decoding_backend", "=", "decoding_backend", "\n", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.ImageDecode.__call__": [[1410, 1434], ["mmcv.use_backend", "list", "loading.ImageDecode.file_client.get", "mmcv.imfrombytes", "list.append", "mmcv.fileio.FileClient"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the ``ImageDecode`` to load image given the file path.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "mmcv", ".", "use_backend", "(", "self", ".", "decoding_backend", ")", "\n", "\n", "filename", "=", "results", "[", "'filename'", "]", "\n", "\n", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "imgs", "=", "list", "(", ")", "\n", "img_bytes", "=", "self", ".", "file_client", ".", "get", "(", "filename", ")", "\n", "\n", "img", "=", "mmcv", ".", "imfrombytes", "(", "img_bytes", ",", "channel_order", "=", "'rgb'", ")", "\n", "imgs", ".", "append", "(", "img", ")", "\n", "\n", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'original_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "results", "[", "'img_shape'", "]", "=", "imgs", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecodeInit.__init__": [[1449, 1462], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "io_backend", "=", "'disk'", ",", "\n", "sample_rate", "=", "16000", ",", "\n", "pad_method", "=", "'zero'", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "sample_rate", "=", "sample_rate", "\n", "if", "pad_method", "in", "[", "'random'", ",", "'zero'", "]", ":", "\n", "            ", "self", ".", "pad_method", "=", "pad_method", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecodeInit._zero_pad": [[1463, 1466], ["numpy.zeros"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_zero_pad", "(", "shape", ")", ":", "\n", "        ", "return", "np", ".", "zeros", "(", "shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecodeInit._random_pad": [[1467, 1471], ["numpy.random.rand().astype", "numpy.random.rand"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_random_pad", "(", "shape", ")", ":", "\n", "# librosa load raw audio file into a distribution of -1~+1", "\n", "        ", "return", "np", ".", "random", ".", "rand", "(", "shape", ")", ".", "astype", "(", "np", ".", "float32", ")", "*", "2", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecodeInit.__call__": [[1472, 1499], ["os.exists", "os.exists", "mmcv.fileio.FileClient", "io.BytesIO", "librosa.load", "getattr", "getattr.", "ImportError", "loading.AudioDecodeInit.file_client.get", "int", "round"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the librosa initialization.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "librosa", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install librosa first.'", ")", "\n", "\n", "", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "", "if", "osp", ".", "exists", "(", "results", "[", "'audio_path'", "]", ")", ":", "\n", "            ", "file_obj", "=", "io", ".", "BytesIO", "(", "self", ".", "file_client", ".", "get", "(", "results", "[", "'audio_path'", "]", ")", ")", "\n", "y", ",", "sr", "=", "librosa", ".", "load", "(", "file_obj", ",", "sr", "=", "self", ".", "sample_rate", ")", "\n", "", "else", ":", "\n", "# Generate a random dummy 10s input", "\n", "            ", "pad_func", "=", "getattr", "(", "self", ",", "f'_{self.pad_method}_pad'", ")", "\n", "y", "=", "pad_func", "(", "int", "(", "round", "(", "10.0", "*", "self", ".", "sample_rate", ")", ")", ")", "\n", "sr", "=", "self", ".", "sample_rate", "\n", "\n", "", "results", "[", "'length'", "]", "=", "y", ".", "shape", "[", "0", "]", "\n", "results", "[", "'sample_rate'", "]", "=", "sr", "\n", "results", "[", "'audios'", "]", "=", "y", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecodeInit.__repr__": [[1500, 1506], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'io_backend={self.io_backend}, '", "\n", "f'sample_rate={self.sample_rate}, '", "\n", "f'pad_method={self.pad_method})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadAudioFeature.__init__": [[1516, 1520], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "pad_method", "=", "'zero'", ")", ":", "\n", "        ", "if", "pad_method", "not", "in", "[", "'zero'", ",", "'random'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "pad_method", "=", "pad_method", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadAudioFeature._zero_pad": [[1521, 1524], ["numpy.zeros"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_zero_pad", "(", "shape", ")", ":", "\n", "        ", "return", "np", ".", "zeros", "(", "shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadAudioFeature._random_pad": [[1525, 1529], ["numpy.random.rand().astype", "numpy.random.rand"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_random_pad", "(", "shape", ")", ":", "\n", "# spectrogram is normalized into a distribution of 0~1", "\n", "        ", "return", "np", ".", "random", ".", "rand", "(", "shape", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadAudioFeature.__call__": [[1530, 1548], ["os.exists", "os.exists", "numpy.load", "getattr", "getattr."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the numpy loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "if", "osp", ".", "exists", "(", "results", "[", "'audio_path'", "]", ")", ":", "\n", "            ", "feature_map", "=", "np", ".", "load", "(", "results", "[", "'audio_path'", "]", ")", "\n", "", "else", ":", "\n", "# Generate a random dummy 10s input", "\n", "# Some videos do not have audio stream", "\n", "            ", "pad_func", "=", "getattr", "(", "self", ",", "f'_{self.pad_method}_pad'", ")", "\n", "feature_map", "=", "pad_func", "(", "(", "640", ",", "80", ")", ")", "\n", "\n", "", "results", "[", "'length'", "]", "=", "feature_map", ".", "shape", "[", "0", "]", "\n", "results", "[", "'audios'", "]", "=", "feature_map", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadAudioFeature.__repr__": [[1549, 1553], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'pad_method={self.pad_method})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecode.__init__": [[1568, 1570], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fixed_length", "=", "32000", ")", ":", "\n", "        ", "self", ".", "fixed_length", "=", "fixed_length", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioDecode.__call__": [[1571, 1604], ["list", "frame_inds.reshape.reshape.reshape", "range", "numpy.array", "max", "min", "list.append", "int", "int", "numpy.pad", "round", "round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the ``AudioDecode`` to pick audio clips.\"\"\"", "\n", "audio", "=", "results", "[", "'audios'", "]", "\n", "frame_inds", "=", "results", "[", "'frame_inds'", "]", "\n", "num_clips", "=", "results", "[", "'num_clips'", "]", "\n", "resampled_clips", "=", "list", "(", ")", "\n", "frame_inds", "=", "frame_inds", ".", "reshape", "(", "num_clips", ",", "-", "1", ")", "\n", "for", "clip_idx", "in", "range", "(", "num_clips", ")", ":", "\n", "            ", "clip_frame_inds", "=", "frame_inds", "[", "clip_idx", "]", "\n", "start_idx", "=", "max", "(", "\n", "0", ",", "\n", "int", "(", "\n", "round", "(", "(", "clip_frame_inds", "[", "0", "]", "+", "1", ")", "/", "results", "[", "'total_frames'", "]", "*", "\n", "results", "[", "'length'", "]", ")", ")", ")", "\n", "end_idx", "=", "min", "(", "\n", "results", "[", "'length'", "]", ",", "\n", "int", "(", "\n", "round", "(", "(", "clip_frame_inds", "[", "-", "1", "]", "+", "1", ")", "/", "results", "[", "'total_frames'", "]", "*", "\n", "results", "[", "'length'", "]", ")", ")", ")", "\n", "cropped_audio", "=", "audio", "[", "start_idx", ":", "end_idx", "]", "\n", "if", "cropped_audio", ".", "shape", "[", "0", "]", ">=", "self", ".", "fixed_length", ":", "\n", "                ", "truncated_audio", "=", "cropped_audio", "[", ":", "self", ".", "fixed_length", "]", "\n", "", "else", ":", "\n", "                ", "truncated_audio", "=", "np", ".", "pad", "(", "\n", "cropped_audio", ",", "\n", "(", "(", "0", ",", "self", ".", "fixed_length", "-", "cropped_audio", ".", "shape", "[", "0", "]", ")", ")", ",", "\n", "mode", "=", "'constant'", ")", "\n", "\n", "", "resampled_clips", ".", "append", "(", "truncated_audio", ")", "\n", "\n", "", "results", "[", "'audios'", "]", "=", "np", ".", "array", "(", "resampled_clips", ")", "\n", "results", "[", "'audios_shape'", "]", "=", "results", "[", "'audios'", "]", ".", "shape", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.BuildPseudoClip.__init__": [[1617, 1619], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "clip_len", ")", ":", "\n", "        ", "self", ".", "clip_len", "=", "clip_len", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.BuildPseudoClip.__call__": [[1620, 1629], ["range", "len", "results[].append", "numpy.copy"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "# the input should be one single image", "\n", "        ", "assert", "len", "(", "results", "[", "'imgs'", "]", ")", "==", "1", "\n", "im", "=", "results", "[", "'imgs'", "]", "[", "0", "]", "\n", "for", "_", "in", "range", "(", "1", ",", "self", ".", "clip_len", ")", ":", "\n", "            ", "results", "[", "'imgs'", "]", ".", "append", "(", "np", ".", "copy", "(", "im", ")", ")", "\n", "", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'num_clips'", "]", "=", "1", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.BuildPseudoClip.__repr__": [[1630, 1634], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'fix_length={self.fixed_length})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioFeatureSelector.__init__": [[1649, 1651], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fixed_length", "=", "128", ")", ":", "\n", "        ", "self", ".", "fixed_length", "=", "fixed_length", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioFeatureSelector.__call__": [[1652, 1690], ["list", "frame_inds.reshape.reshape.reshape", "range", "numpy.array", "max", "min", "list.append", "int", "int", "numpy.pad", "round", "round"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the ``AudioFeatureSelector`` to pick audio feature clips.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "audio", "=", "results", "[", "'audios'", "]", "\n", "frame_inds", "=", "results", "[", "'frame_inds'", "]", "\n", "num_clips", "=", "results", "[", "'num_clips'", "]", "\n", "resampled_clips", "=", "list", "(", ")", "\n", "\n", "frame_inds", "=", "frame_inds", ".", "reshape", "(", "num_clips", ",", "-", "1", ")", "\n", "for", "clip_idx", "in", "range", "(", "num_clips", ")", ":", "\n", "            ", "clip_frame_inds", "=", "frame_inds", "[", "clip_idx", "]", "\n", "start_idx", "=", "max", "(", "\n", "0", ",", "\n", "int", "(", "\n", "round", "(", "(", "clip_frame_inds", "[", "0", "]", "+", "1", ")", "/", "results", "[", "'total_frames'", "]", "*", "\n", "results", "[", "'length'", "]", ")", ")", ")", "\n", "end_idx", "=", "min", "(", "\n", "results", "[", "'length'", "]", ",", "\n", "int", "(", "\n", "round", "(", "(", "clip_frame_inds", "[", "-", "1", "]", "+", "1", ")", "/", "results", "[", "'total_frames'", "]", "*", "\n", "results", "[", "'length'", "]", ")", ")", ")", "\n", "cropped_audio", "=", "audio", "[", "start_idx", ":", "end_idx", ",", ":", "]", "\n", "if", "cropped_audio", ".", "shape", "[", "0", "]", ">=", "self", ".", "fixed_length", ":", "\n", "                ", "truncated_audio", "=", "cropped_audio", "[", ":", "self", ".", "fixed_length", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "truncated_audio", "=", "np", ".", "pad", "(", "\n", "cropped_audio", ",", "\n", "(", "(", "0", ",", "self", ".", "fixed_length", "-", "cropped_audio", ".", "shape", "[", "0", "]", ")", ",", "(", "0", ",", "0", ")", ")", ",", "\n", "mode", "=", "'constant'", ")", "\n", "\n", "", "resampled_clips", ".", "append", "(", "truncated_audio", ")", "\n", "", "results", "[", "'audios'", "]", "=", "np", ".", "array", "(", "resampled_clips", ")", "\n", "results", "[", "'audios_shape'", "]", "=", "results", "[", "'audios'", "]", ".", "shape", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.AudioFeatureSelector.__repr__": [[1691, 1695], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'fix_length={self.fixed_length})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadLocalizationFeature.__init__": [[1708, 1713], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "raw_feature_ext", "=", "'.csv'", ")", ":", "\n", "        ", "valid_raw_feature_ext", "=", "(", "'.csv'", ",", ")", "\n", "if", "raw_feature_ext", "not", "in", "valid_raw_feature_ext", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "raw_feature_ext", "=", "raw_feature_ext", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadLocalizationFeature.__call__": [[1714, 1731], ["os.join", "os.join", "numpy.loadtxt", "numpy.transpose"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the LoadLocalizationFeature loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "video_name", "=", "results", "[", "'video_name'", "]", "\n", "data_prefix", "=", "results", "[", "'data_prefix'", "]", "\n", "\n", "data_path", "=", "osp", ".", "join", "(", "data_prefix", ",", "video_name", "+", "self", ".", "raw_feature_ext", ")", "\n", "raw_feature", "=", "np", ".", "loadtxt", "(", "\n", "data_path", ",", "dtype", "=", "np", ".", "float32", ",", "delimiter", "=", "','", ",", "skiprows", "=", "1", ")", "\n", "\n", "results", "[", "'raw_feature'", "]", "=", "np", ".", "transpose", "(", "raw_feature", ",", "(", "1", ",", "0", ")", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadLocalizationFeature.__repr__": [[1732, 1736], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'raw_feature_ext={self.raw_feature_ext})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.GenerateLocalizationLabels.__call__": [[1746, 1771], ["numpy.array", "max", "max", "numpy.array.append", "float", "min", "min"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the GenerateLocalizationLabels loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "video_frame", "=", "results", "[", "'duration_frame'", "]", "\n", "video_second", "=", "results", "[", "'duration_second'", "]", "\n", "feature_frame", "=", "results", "[", "'feature_frame'", "]", "\n", "corrected_second", "=", "float", "(", "feature_frame", ")", "/", "video_frame", "*", "video_second", "\n", "annotations", "=", "results", "[", "'annotations'", "]", "\n", "\n", "gt_bbox", "=", "[", "]", "\n", "\n", "for", "annotation", "in", "annotations", ":", "\n", "            ", "current_start", "=", "max", "(", "\n", "min", "(", "1", ",", "annotation", "[", "'segment'", "]", "[", "0", "]", "/", "corrected_second", ")", ",", "0", ")", "\n", "current_end", "=", "max", "(", "\n", "min", "(", "1", ",", "annotation", "[", "'segment'", "]", "[", "1", "]", "/", "corrected_second", ")", ",", "0", ")", "\n", "gt_bbox", ".", "append", "(", "[", "current_start", ",", "current_end", "]", ")", "\n", "\n", "", "gt_bbox", "=", "np", ".", "array", "(", "gt_bbox", ")", "\n", "results", "[", "'gt_bbox'", "]", "=", "gt_bbox", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadProposals.__init__": [[1788, 1805], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "top_k", ",", "\n", "pgm_proposals_dir", ",", "\n", "pgm_features_dir", ",", "\n", "proposal_ext", "=", "'.csv'", ",", "\n", "feature_ext", "=", "'.npy'", ")", ":", "\n", "        ", "self", ".", "top_k", "=", "top_k", "\n", "self", ".", "pgm_proposals_dir", "=", "pgm_proposals_dir", "\n", "self", ".", "pgm_features_dir", "=", "pgm_features_dir", "\n", "valid_proposal_ext", "=", "(", "'.csv'", ",", ")", "\n", "if", "proposal_ext", "not", "in", "valid_proposal_ext", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "proposal_ext", "=", "proposal_ext", "\n", "valid_feature_ext", "=", "(", "'.npy'", ",", ")", "\n", "if", "feature_ext", "not", "in", "valid_feature_ext", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "feature_ext", "=", "feature_ext", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadProposals.__call__": [[1806, 1842], ["os.join", "os.join", "numpy.array", "os.join", "os.join", "numpy.loadtxt", "numpy.load().astype", "numpy.load"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the LoadProposals loading.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "video_name", "=", "results", "[", "'video_name'", "]", "\n", "proposal_path", "=", "osp", ".", "join", "(", "self", ".", "pgm_proposals_dir", ",", "\n", "video_name", "+", "self", ".", "proposal_ext", ")", "\n", "if", "self", ".", "proposal_ext", "==", "'.csv'", ":", "\n", "            ", "pgm_proposals", "=", "np", ".", "loadtxt", "(", "\n", "proposal_path", ",", "dtype", "=", "np", ".", "float32", ",", "delimiter", "=", "','", ",", "skiprows", "=", "1", ")", "\n", "\n", "", "pgm_proposals", "=", "np", ".", "array", "(", "pgm_proposals", "[", ":", "self", ".", "top_k", "]", ")", "\n", "tmin", "=", "pgm_proposals", "[", ":", ",", "0", "]", "\n", "tmax", "=", "pgm_proposals", "[", ":", ",", "1", "]", "\n", "tmin_score", "=", "pgm_proposals", "[", ":", ",", "2", "]", "\n", "tmax_score", "=", "pgm_proposals", "[", ":", ",", "3", "]", "\n", "reference_temporal_iou", "=", "pgm_proposals", "[", ":", ",", "5", "]", "\n", "\n", "feature_path", "=", "osp", ".", "join", "(", "self", ".", "pgm_features_dir", ",", "\n", "video_name", "+", "self", ".", "feature_ext", ")", "\n", "if", "self", ".", "feature_ext", "==", "'.npy'", ":", "\n", "            ", "bsp_feature", "=", "np", ".", "load", "(", "feature_path", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "bsp_feature", "=", "bsp_feature", "[", ":", "self", ".", "top_k", ",", ":", "]", "\n", "\n", "results", "[", "'bsp_feature'", "]", "=", "bsp_feature", "\n", "results", "[", "'tmin'", "]", "=", "tmin", "\n", "results", "[", "'tmax'", "]", "=", "tmax", "\n", "results", "[", "'tmin_score'", "]", "=", "tmin_score", "\n", "results", "[", "'tmax_score'", "]", "=", "tmax_score", "\n", "results", "[", "'reference_temporal_iou'", "]", "=", "reference_temporal_iou", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.loading.LoadProposals.__repr__": [[1843, 1851], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'top_k={self.top_k}, '", "\n", "f'pgm_proposals_dir={self.pgm_proposals_dir}, '", "\n", "f'pgm_features_dir={self.pgm_features_dir}, '", "\n", "f'proposal_ext={self.proposal_ext}, '", "\n", "f'feature_ext={self.feature_ext})'", ")", "\n", "return", "repr_str", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.TorchvisionTrans.__init__": [[66, 79], ["getattr", "getattr.", "mmcv.utils.digit_version", "mmcv.utils.digit_version", "RuntimeError", "RuntimeError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "type", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "torchvision", "\n", "import", "torchvision", ".", "transforms", "as", "tv_trans", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Install torchvision to use TorchvisionTrans'", ")", "\n", "", "if", "digit_version", "(", "torchvision", ".", "__version__", ")", "<", "digit_version", "(", "'0.8.0'", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'The version of torchvision should be at least '", "\n", "'0.8.0'", ")", "\n", "\n", "", "trans", "=", "getattr", "(", "tv_trans", ",", "type", ",", "None", ")", "\n", "assert", "trans", ",", "f'Transform {type} not in torchvision'", "\n", "self", ".", "trans", "=", "trans", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.TorchvisionTrans.__call__": [[80, 93], ["formatting.to_tensor", "augmentations.TorchvisionTrans.trans().data.numpy", "imgs.astype.astype.astype", "x.transpose", "numpy.stack", "x.transpose", "augmentations.TorchvisionTrans.trans"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "assert", "'imgs'", "in", "results", "\n", "\n", "imgs", "=", "[", "x", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "for", "x", "in", "results", "[", "'imgs'", "]", "]", "\n", "imgs", "=", "to_tensor", "(", "np", ".", "stack", "(", "imgs", ")", ")", "\n", "\n", "imgs", "=", "self", ".", "trans", "(", "imgs", ")", ".", "data", ".", "numpy", "(", ")", "\n", "imgs", "[", "imgs", ">", "255", "]", "=", "255", "\n", "imgs", "[", "imgs", "<", "0", "]", "=", "0", "\n", "imgs", "=", "imgs", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "imgs", "=", "[", "x", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", "for", "x", "in", "imgs", "]", "\n", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.PytorchVideoTrans.__init__": [[103, 124], ["getattr", "getattr.", "mmcv.utils.digit_version", "mmcv.utils.digit_version", "RuntimeError", "RuntimeError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "type", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "torch", "\n", "import", "pytorchvideo", ".", "transforms", "as", "ptv_trans", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Install pytorchvideo to use PytorchVideoTrans'", ")", "\n", "", "if", "digit_version", "(", "torch", ".", "__version__", ")", "<", "digit_version", "(", "'1.8.0'", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\n", "'The version of PyTorch should be at least 1.8.0'", ")", "\n", "\n", "", "trans", "=", "getattr", "(", "ptv_trans", ",", "type", ",", "None", ")", "\n", "assert", "trans", ",", "f'Transform {type} not in pytorchvideo'", "\n", "\n", "supported_pytorchvideo_trans", "=", "(", "'AugMix'", ",", "'RandAugment'", ",", "\n", "'RandomResizedCrop'", ",", "'ShortSideScale'", ",", "\n", "'RandomShortSideScale'", ")", "\n", "assert", "type", "in", "supported_pytorchvideo_trans", ",", "f'PytorchVideo Transform {type} is not supported in MMAction2'", "\n", "\n", "self", ".", "trans", "=", "trans", "(", "**", "kwargs", ")", "\n", "self", ".", "type", "=", "type", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.PytorchVideoTrans.__call__": [[125, 165], ["augmentations.PytorchVideoTrans.trans().data.numpy", "formatting.to_tensor", "formatting.to_tensor", "imgs.astype.astype.astype", "imgs.astype.astype.astype", "x.transpose", "numpy.stack", "x.transpose", "augmentations.PytorchVideoTrans.trans", "imgs.astype.astype.transpose", "numpy.stack().transpose", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.formatting.to_tensor"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "assert", "'imgs'", "in", "results", "\n", "\n", "assert", "'gt_bboxes'", "not", "in", "results", ",", "f'PytorchVideo {self.type} doesn\\'t support bboxes yet.'", "\n", "assert", "'proposals'", "not", "in", "results", ",", "f'PytorchVideo {self.type} doesn\\'t support bboxes yet.'", "\n", "\n", "if", "self", ".", "type", "in", "(", "'AugMix'", ",", "'RandAugment'", ")", ":", "\n", "# list[ndarray(h, w, 3)] -> torch.tensor(t, c, h, w)", "\n", "            ", "imgs", "=", "[", "x", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "for", "x", "in", "results", "[", "'imgs'", "]", "]", "\n", "imgs", "=", "to_tensor", "(", "np", ".", "stack", "(", "imgs", ")", ")", "\n", "", "else", ":", "\n", "# list[ndarray(h, w, 3)] -> torch.tensor(c, t, h, w)", "\n", "# uint8 -> float32", "\n", "            ", "imgs", "=", "to_tensor", "(", "(", "np", ".", "stack", "(", "results", "[", "'imgs'", "]", ")", ".", "transpose", "(", "3", ",", "0", ",", "1", ",", "2", ")", "/", "\n", "255.", ")", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "\n", "", "imgs", "=", "self", ".", "trans", "(", "imgs", ")", ".", "data", ".", "numpy", "(", ")", "\n", "\n", "if", "self", ".", "type", "in", "(", "'AugMix'", ",", "'RandAugment'", ")", ":", "\n", "            ", "imgs", "[", "imgs", ">", "255", "]", "=", "255", "\n", "imgs", "[", "imgs", "<", "0", "]", "=", "0", "\n", "imgs", "=", "imgs", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# torch.tensor(t, c, h, w) -> list[ndarray(h, w, 3)]", "\n", "imgs", "=", "[", "x", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", "for", "x", "in", "imgs", "]", "\n", "", "else", ":", "\n", "# float32 -> uint8", "\n", "            ", "imgs", "=", "imgs", "*", "255", "\n", "imgs", "[", "imgs", ">", "255", "]", "=", "255", "\n", "imgs", "[", "imgs", "<", "0", "]", "=", "0", "\n", "imgs", "=", "imgs", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# torch.tensor(c, t, h, w) -> list[ndarray(h, w, 3)]", "\n", "imgs", "=", "[", "x", "for", "x", "in", "imgs", ".", "transpose", "(", "1", ",", "2", ",", "3", ",", "0", ")", "]", "\n", "\n", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.PoseCompact.__init__": [[194, 209], ["torch.nn.modules.utils._pair"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "padding", "=", "0.25", ",", "\n", "threshold", "=", "10", ",", "\n", "hw_ratio", "=", "None", ",", "\n", "allow_imgpad", "=", "True", ")", ":", "\n", "\n", "        ", "self", ".", "padding", "=", "padding", "\n", "self", ".", "threshold", "=", "threshold", "\n", "if", "hw_ratio", "is", "not", "None", ":", "\n", "            ", "hw_ratio", "=", "_pair", "(", "hw_ratio", ")", "\n", "\n", "", "self", ".", "hw_ratio", "=", "hw_ratio", "\n", "\n", "self", ".", "allow_imgpad", "=", "allow_imgpad", "\n", "assert", "self", ".", "padding", ">=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.PoseCompact.__call__": [[210, 261], ["numpy.min", "numpy.min", "numpy.max", "numpy.max", "results.get", "augmentations._combine_quadruple", "max", "max", "numpy.isnan", "int", "int", "int", "int", "int", "int", "int", "int", "max", "max", "min", "min"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._combine_quadruple"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "img_shape", "=", "results", "[", "'img_shape'", "]", "\n", "h", ",", "w", "=", "img_shape", "\n", "kp", "=", "results", "[", "'keypoint'", "]", "\n", "\n", "# Make NaN zero", "\n", "kp", "[", "np", ".", "isnan", "(", "kp", ")", "]", "=", "0.", "\n", "kp_x", "=", "kp", "[", "...", ",", "0", "]", "\n", "kp_y", "=", "kp", "[", "...", ",", "1", "]", "\n", "\n", "min_x", "=", "np", ".", "min", "(", "kp_x", "[", "kp_x", "!=", "0", "]", ",", "initial", "=", "np", ".", "Inf", ")", "\n", "min_y", "=", "np", ".", "min", "(", "kp_y", "[", "kp_y", "!=", "0", "]", ",", "initial", "=", "np", ".", "Inf", ")", "\n", "max_x", "=", "np", ".", "max", "(", "kp_x", "[", "kp_x", "!=", "0", "]", ",", "initial", "=", "-", "np", ".", "Inf", ")", "\n", "max_y", "=", "np", ".", "max", "(", "kp_y", "[", "kp_y", "!=", "0", "]", ",", "initial", "=", "-", "np", ".", "Inf", ")", "\n", "\n", "# The compact area is too small", "\n", "if", "max_x", "-", "min_x", "<", "self", ".", "threshold", "or", "max_y", "-", "min_y", "<", "self", ".", "threshold", ":", "\n", "            ", "return", "results", "\n", "\n", "", "center", "=", "(", "(", "max_x", "+", "min_x", ")", "/", "2", ",", "(", "max_y", "+", "min_y", ")", "/", "2", ")", "\n", "half_width", "=", "(", "max_x", "-", "min_x", ")", "/", "2", "*", "(", "1", "+", "self", ".", "padding", ")", "\n", "half_height", "=", "(", "max_y", "-", "min_y", ")", "/", "2", "*", "(", "1", "+", "self", ".", "padding", ")", "\n", "\n", "if", "self", ".", "hw_ratio", "is", "not", "None", ":", "\n", "            ", "half_height", "=", "max", "(", "self", ".", "hw_ratio", "[", "0", "]", "*", "half_width", ",", "half_height", ")", "\n", "half_width", "=", "max", "(", "1", "/", "self", ".", "hw_ratio", "[", "1", "]", "*", "half_height", ",", "half_width", ")", "\n", "\n", "", "min_x", ",", "max_x", "=", "center", "[", "0", "]", "-", "half_width", ",", "center", "[", "0", "]", "+", "half_width", "\n", "min_y", ",", "max_y", "=", "center", "[", "1", "]", "-", "half_height", ",", "center", "[", "1", "]", "+", "half_height", "\n", "\n", "# hot update", "\n", "if", "not", "self", ".", "allow_imgpad", ":", "\n", "            ", "min_x", ",", "min_y", "=", "int", "(", "max", "(", "0", ",", "min_x", ")", ")", ",", "int", "(", "max", "(", "0", ",", "min_y", ")", ")", "\n", "max_x", ",", "max_y", "=", "int", "(", "min", "(", "w", ",", "max_x", ")", ")", ",", "int", "(", "min", "(", "h", ",", "max_y", ")", ")", "\n", "", "else", ":", "\n", "            ", "min_x", ",", "min_y", "=", "int", "(", "min_x", ")", ",", "int", "(", "min_y", ")", "\n", "max_x", ",", "max_y", "=", "int", "(", "max_x", ")", ",", "int", "(", "max_y", ")", "\n", "\n", "", "kp_x", "[", "kp_x", "!=", "0", "]", "-=", "min_x", "\n", "kp_y", "[", "kp_y", "!=", "0", "]", "-=", "min_y", "\n", "\n", "new_shape", "=", "(", "max_y", "-", "min_y", ",", "max_x", "-", "min_x", ")", "\n", "results", "[", "'img_shape'", "]", "=", "new_shape", "\n", "\n", "# the order is x, y, w, h (in [0, 1]), a tuple", "\n", "crop_quadruple", "=", "results", ".", "get", "(", "'crop_quadruple'", ",", "(", "0.", ",", "0.", ",", "1.", ",", "1.", ")", ")", "\n", "new_crop_quadruple", "=", "(", "min_x", "/", "w", ",", "min_y", "/", "h", ",", "(", "max_x", "-", "min_x", ")", "/", "w", ",", "\n", "(", "max_y", "-", "min_y", ")", "/", "h", ")", "\n", "crop_quadruple", "=", "_combine_quadruple", "(", "crop_quadruple", ",", "new_crop_quadruple", ")", "\n", "results", "[", "'crop_quadruple'", "]", "=", "crop_quadruple", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.PoseCompact.__repr__": [[262, 268], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}(padding={self.padding}, '", "\n", "f'threshold={self.threshold}, '", "\n", "f'hw_ratio={self.hw_ratio}, '", "\n", "f'allow_imgpad={self.allow_imgpad})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.__init__": [[343, 360], ["augmentations.Imgaug.default_transforms", "isinstance", "isinstance", "iaa.Sequential", "all", "isinstance", "ValueError", "augmentations.Imgaug.imgaug_builder", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.default_transforms", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.imgaug_builder"], ["def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "import", "imgaug", ".", "augmenters", "as", "iaa", "\n", "\n", "if", "transforms", "==", "'default'", ":", "\n", "            ", "self", ".", "transforms", "=", "self", ".", "default_transforms", "(", ")", "\n", "", "elif", "isinstance", "(", "transforms", ",", "list", ")", ":", "\n", "            ", "assert", "all", "(", "isinstance", "(", "trans", ",", "dict", ")", "for", "trans", "in", "transforms", ")", "\n", "self", ".", "transforms", "=", "transforms", "\n", "", "elif", "isinstance", "(", "transforms", ",", "iaa", ".", "Augmenter", ")", ":", "\n", "            ", "self", ".", "aug", "=", "self", ".", "transforms", "=", "transforms", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'transforms must be `default` or a list of dicts'", "\n", "' or iaa.Augmenter object'", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "transforms", ",", "iaa", ".", "Augmenter", ")", ":", "\n", "            ", "self", ".", "aug", "=", "iaa", ".", "Sequential", "(", "\n", "[", "self", ".", "imgaug_builder", "(", "t", ")", "for", "t", "in", "self", ".", "transforms", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.default_transforms": [[361, 416], ["dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "max", "random.choice", "random.choice", "random.choice", "random.choice", "random.choice", "int"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "default_transforms", "(", ")", ":", "\n", "        ", "\"\"\"Default transforms for imgaug.\n\n        Implement RandAugment by imgaug.\n        Please visit `https://arxiv.org/abs/1909.13719` for more information.\n\n        Augmenters and hyper parameters are borrowed from the following repo:\n        https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py # noqa\n\n        Miss one augmenter ``SolarizeAdd`` since imgaug doesn't support this.\n\n        Returns:\n            dict: The constructed RandAugment transforms.\n        \"\"\"", "\n", "# RandAugment hyper params", "\n", "num_augmenters", "=", "2", "\n", "cur_magnitude", ",", "max_magnitude", "=", "9", ",", "10", "\n", "cur_level", "=", "1.0", "*", "cur_magnitude", "/", "max_magnitude", "\n", "\n", "return", "[", "\n", "dict", "(", "\n", "type", "=", "'SomeOf'", ",", "\n", "n", "=", "num_augmenters", ",", "\n", "children", "=", "[", "\n", "dict", "(", "\n", "type", "=", "'ShearX'", ",", "\n", "shear", "=", "17.19", "*", "cur_level", "*", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ")", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'ShearY'", ",", "\n", "shear", "=", "17.19", "*", "cur_level", "*", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ")", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'TranslateX'", ",", "\n", "percent", "=", ".2", "*", "cur_level", "*", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ")", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'TranslateY'", ",", "\n", "percent", "=", ".2", "*", "cur_level", "*", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ")", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'Rotate'", ",", "\n", "rotate", "=", "30", "*", "cur_level", "*", "random", ".", "choice", "(", "[", "-", "1", ",", "1", "]", ")", ")", ",", "\n", "dict", "(", "type", "=", "'Posterize'", ",", "nb_bits", "=", "max", "(", "1", ",", "int", "(", "4", "*", "cur_level", ")", ")", ")", ",", "\n", "dict", "(", "type", "=", "'Solarize'", ",", "threshold", "=", "256", "*", "cur_level", ")", ",", "\n", "dict", "(", "type", "=", "'EnhanceColor'", ",", "factor", "=", "1.8", "*", "cur_level", "+", ".1", ")", ",", "\n", "dict", "(", "type", "=", "'EnhanceContrast'", ",", "factor", "=", "1.8", "*", "cur_level", "+", ".1", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'EnhanceBrightness'", ",", "factor", "=", "1.8", "*", "cur_level", "+", ".1", ")", ",", "\n", "dict", "(", "type", "=", "'EnhanceSharpness'", ",", "factor", "=", "1.8", "*", "cur_level", "+", ".1", ")", ",", "\n", "dict", "(", "type", "=", "'Autocontrast'", ",", "cutoff", "=", "0", ")", ",", "\n", "dict", "(", "type", "=", "'Equalize'", ")", ",", "\n", "dict", "(", "type", "=", "'Invert'", ",", "p", "=", "1.", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'Cutout'", ",", "\n", "nb_iterations", "=", "1", ",", "\n", "size", "=", "0.2", "*", "cur_level", ",", "\n", "squared", "=", "True", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.imgaug_builder": [[419, 452], ["cfg.copy", "cfg.copy.pop", "mmcv.is_str", "obj_cls", "isinstance", "issubclass", "hasattr", "getattr", "getattr", "TypeError", "augmentations.Imgaug.imgaug_builder", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.imgaug_builder"], ["", "def", "imgaug_builder", "(", "self", ",", "cfg", ")", ":", "\n", "        ", "\"\"\"Import a module from imgaug.\n\n        It follows the logic of :func:`build_from_cfg`. Use a dict object to\n        create an iaa.Augmenter object.\n\n        Args:\n            cfg (dict): Config dict. It should at least contain the key \"type\".\n\n        Returns:\n            obj:`iaa.Augmenter`: The constructed imgaug augmenter.\n        \"\"\"", "\n", "import", "imgaug", ".", "augmenters", "as", "iaa", "\n", "\n", "assert", "isinstance", "(", "cfg", ",", "dict", ")", "and", "'type'", "in", "cfg", "\n", "args", "=", "cfg", ".", "copy", "(", ")", "\n", "\n", "obj_type", "=", "args", ".", "pop", "(", "'type'", ")", "\n", "if", "mmcv", ".", "is_str", "(", "obj_type", ")", ":", "\n", "            ", "obj_cls", "=", "getattr", "(", "iaa", ",", "obj_type", ")", "if", "hasattr", "(", "iaa", ",", "obj_type", ")", "else", "getattr", "(", "iaa", ".", "pillike", ",", "obj_type", ")", "\n", "", "elif", "issubclass", "(", "obj_type", ",", "iaa", ".", "Augmenter", ")", ":", "\n", "            ", "obj_cls", "=", "obj_type", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f'type must be a str or valid type, but got {type(obj_type)}'", ")", "\n", "\n", "", "if", "'children'", "in", "args", ":", "\n", "            ", "args", "[", "'children'", "]", "=", "[", "\n", "self", ".", "imgaug_builder", "(", "child", ")", "for", "child", "in", "args", "[", "'children'", "]", "\n", "]", "\n", "\n", "", "return", "obj_cls", "(", "**", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.__repr__": [[453, 456], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "self", ".", "__class__", ".", "__name__", "+", "f'(transforms={self.aug})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Imgaug.__call__": [[457, 508], ["augmentations.Imgaug.aug.to_deterministic", "augmentations.Imgaug.augment_image", "bbs.BoundingBoxesOnImage", "augmentations.Imgaug.augment_bounding_boxes", "bbs.BoundingBox", "bbs.BoundingBoxesOnImage", "augmentations.Imgaug.augment_bounding_boxes", "max", "max", "min", "min", "bbs.BoundingBox", "max", "max", "min", "min"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "assert", "results", "[", "'modality'", "]", "==", "'RGB'", ",", "'Imgaug only support RGB images.'", "\n", "in_type", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "dtype", ".", "type", "\n", "\n", "cur_aug", "=", "self", ".", "aug", ".", "to_deterministic", "(", ")", "\n", "\n", "results", "[", "'imgs'", "]", "=", "[", "\n", "cur_aug", ".", "augment_image", "(", "frame", ")", "for", "frame", "in", "results", "[", "'imgs'", "]", "\n", "]", "\n", "img_h", ",", "img_w", ",", "_", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "\n", "\n", "out_type", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "dtype", ".", "type", "\n", "assert", "in_type", "==", "out_type", ",", "(", "'Imgaug input dtype and output dtype are not the same. '", ",", "\n", "f'Convert from {in_type} to {out_type}'", ")", "\n", "\n", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "from", "imgaug", ".", "augmentables", "import", "bbs", "\n", "bbox_list", "=", "[", "\n", "bbs", ".", "BoundingBox", "(", "\n", "x1", "=", "bbox", "[", "0", "]", ",", "y1", "=", "bbox", "[", "1", "]", ",", "x2", "=", "bbox", "[", "2", "]", ",", "y2", "=", "bbox", "[", "3", "]", ")", "\n", "for", "bbox", "in", "results", "[", "'gt_bboxes'", "]", "\n", "]", "\n", "bboxes", "=", "bbs", ".", "BoundingBoxesOnImage", "(", "\n", "bbox_list", ",", "shape", "=", "results", "[", "'img_shape'", "]", ")", "\n", "bbox_aug", ",", "*", "_", "=", "cur_aug", ".", "augment_bounding_boxes", "(", "[", "bboxes", "]", ")", "\n", "results", "[", "'gt_bboxes'", "]", "=", "[", "[", "\n", "max", "(", "bbox", ".", "x1", ",", "0", ")", ",", "\n", "max", "(", "bbox", ".", "y1", ",", "0", ")", ",", "\n", "min", "(", "bbox", ".", "x2", ",", "img_w", ")", ",", "\n", "min", "(", "bbox", ".", "y2", ",", "img_h", ")", "\n", "]", "for", "bbox", "in", "bbox_aug", ".", "items", "]", "\n", "if", "'proposals'", "in", "results", ":", "\n", "                ", "bbox_list", "=", "[", "\n", "bbs", ".", "BoundingBox", "(", "\n", "x1", "=", "bbox", "[", "0", "]", ",", "y1", "=", "bbox", "[", "1", "]", ",", "x2", "=", "bbox", "[", "2", "]", ",", "y2", "=", "bbox", "[", "3", "]", ")", "\n", "for", "bbox", "in", "results", "[", "'proposals'", "]", "\n", "]", "\n", "bboxes", "=", "bbs", ".", "BoundingBoxesOnImage", "(", "\n", "bbox_list", ",", "shape", "=", "results", "[", "'img_shape'", "]", ")", "\n", "bbox_aug", ",", "*", "_", "=", "cur_aug", ".", "augment_bounding_boxes", "(", "[", "bboxes", "]", ")", "\n", "results", "[", "'proposals'", "]", "=", "[", "[", "\n", "max", "(", "bbox", ".", "x1", ",", "0", ")", ",", "\n", "max", "(", "bbox", ".", "y1", ",", "0", ")", ",", "\n", "min", "(", "bbox", ".", "x2", ",", "img_w", ")", ",", "\n", "min", "(", "bbox", ".", "y2", ",", "img_h", ")", "\n", "]", "for", "bbox", "in", "bbox_aug", ".", "items", "]", "\n", "\n", "", "", "results", "[", "'img_shape'", "]", "=", "(", "img_h", ",", "img_w", ")", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Fuse.__call__": [[522, 552], ["lazyop[].round().astype", "ValueError", "mmcv.imresize", "lazyop[].round", "mmcv.imflip_"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "if", "'lazy'", "not", "in", "results", ":", "\n", "            ", "raise", "ValueError", "(", "'No lazy operation detected'", ")", "\n", "", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "imgs", "=", "results", "[", "'imgs'", "]", "\n", "\n", "# crop", "\n", "left", ",", "top", ",", "right", ",", "bottom", "=", "lazyop", "[", "'crop_bbox'", "]", ".", "round", "(", ")", ".", "astype", "(", "int", ")", "\n", "imgs", "=", "[", "img", "[", "top", ":", "bottom", ",", "left", ":", "right", "]", "for", "img", "in", "imgs", "]", "\n", "\n", "# resize", "\n", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "if", "lazyop", "[", "'interpolation'", "]", "is", "None", ":", "\n", "            ", "interpolation", "=", "'bilinear'", "\n", "", "else", ":", "\n", "            ", "interpolation", "=", "lazyop", "[", "'interpolation'", "]", "\n", "", "imgs", "=", "[", "\n", "mmcv", ".", "imresize", "(", "img", ",", "(", "img_w", ",", "img_h", ")", ",", "interpolation", "=", "interpolation", ")", "\n", "for", "img", "in", "imgs", "\n", "]", "\n", "\n", "# flip", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "            ", "for", "img", "in", "imgs", ":", "\n", "                ", "mmcv", ".", "imflip_", "(", "img", ",", "lazyop", "[", "'flip_direction'", "]", ")", "\n", "\n", "", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "del", "results", "[", "'lazy'", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop.__init__": [[568, 573], ["isinstance", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ",", "lazy", "=", "False", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "size", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Size must be an int, but got {type(size)}'", ")", "\n", "", "self", ".", "size", "=", "size", "\n", "self", ".", "lazy", "=", "lazy", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_kps": [[574, 577], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_crop_kps", "(", "kps", ",", "crop_bbox", ")", ":", "\n", "        ", "return", "kps", "-", "crop_bbox", "[", ":", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_imgs": [[578, 582], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_crop_imgs", "(", "imgs", ",", "crop_bbox", ")", ":", "\n", "        ", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "crop_bbox", "\n", "return", "[", "img", "[", "y1", ":", "y2", ",", "x1", ":", "x2", "]", "for", "img", "in", "imgs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._box_crop": [[583, 599], ["box.copy", "numpy.clip", "numpy.clip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_box_crop", "(", "box", ",", "crop_bbox", ")", ":", "\n", "        ", "\"\"\"Crop the bounding boxes according to the crop_bbox.\n\n        Args:\n            box (np.ndarray): The bounding boxes.\n            crop_bbox(np.ndarray): The bbox used to crop the original image.\n        \"\"\"", "\n", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "crop_bbox", "\n", "img_w", ",", "img_h", "=", "x2", "-", "x1", ",", "y2", "-", "y1", "\n", "\n", "box_", "=", "box", ".", "copy", "(", ")", "\n", "box_", "[", "...", ",", "0", ":", ":", "2", "]", "=", "np", ".", "clip", "(", "box", "[", "...", ",", "0", ":", ":", "2", "]", "-", "x1", ",", "0", ",", "img_w", "-", "1", ")", "\n", "box_", "[", "...", ",", "1", ":", ":", "2", "]", "=", "np", ".", "clip", "(", "box", "[", "...", ",", "1", ":", ":", "2", "]", "-", "y1", ",", "0", ",", "img_h", "-", "1", ")", "\n", "return", "box_", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._all_box_crop": [[600, 614], ["augmentations.RandomCrop._box_crop", "augmentations.RandomCrop._box_crop"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._box_crop"], ["", "def", "_all_box_crop", "(", "self", ",", "results", ",", "crop_bbox", ")", ":", "\n", "        ", "\"\"\"Crop the gt_bboxes and proposals in results according to crop_bbox.\n\n        Args:\n            results (dict): All information about the sample, which contain\n                'gt_bboxes' and 'proposals' (optional).\n            crop_bbox(np.ndarray): The bbox used to crop the original image.\n        \"\"\"", "\n", "results", "[", "'gt_bboxes'", "]", "=", "self", ".", "_box_crop", "(", "results", "[", "'gt_bboxes'", "]", ",", "crop_bbox", ")", "\n", "if", "'proposals'", "in", "results", "and", "results", "[", "'proposals'", "]", "is", "not", "None", ":", "\n", "            ", "assert", "results", "[", "'proposals'", "]", ".", "shape", "[", "1", "]", "==", "4", "\n", "results", "[", "'proposals'", "]", "=", "self", ".", "_box_crop", "(", "results", "[", "'proposals'", "]", ",", "\n", "crop_bbox", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop.__call__": [[615, 693], ["augmentations._init_lazy_if_proper", "numpy.array", "numpy.array", "int", "int", "numpy.array", "numpy.array", "augmentations.RandomCrop._all_box_crop", "numpy.random.randint", "numpy.random.randint", "augmentations.RandomCrop._crop_kps", "augmentations.RandomCrop._crop_imgs", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._all_box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_kps", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_imgs"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the RandomCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "assert", "self", ".", "size", "<=", "img_h", "and", "self", ".", "size", "<=", "img_w", "\n", "\n", "y_offset", "=", "0", "\n", "x_offset", "=", "0", "\n", "if", "img_h", ">", "self", ".", "size", ":", "\n", "            ", "y_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "img_h", "-", "self", ".", "size", ")", ")", "\n", "", "if", "img_w", ">", "self", ".", "size", ":", "\n", "            ", "x_offset", "=", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "img_w", "-", "self", ".", "size", ")", ")", "\n", "\n", "", "if", "'crop_quadruple'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "[", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# x, y, w, h", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "x_ratio", ",", "y_ratio", "=", "x_offset", "/", "img_w", ",", "y_offset", "/", "img_h", "\n", "w_ratio", ",", "h_ratio", "=", "self", ".", "size", "/", "img_w", ",", "self", ".", "size", "/", "img_h", "\n", "\n", "old_crop_quadruple", "=", "results", "[", "'crop_quadruple'", "]", "\n", "old_x_ratio", ",", "old_y_ratio", "=", "old_crop_quadruple", "[", "0", "]", ",", "old_crop_quadruple", "[", "1", "]", "\n", "old_w_ratio", ",", "old_h_ratio", "=", "old_crop_quadruple", "[", "2", "]", ",", "old_crop_quadruple", "[", "3", "]", "\n", "new_crop_quadruple", "=", "[", "\n", "old_x_ratio", "+", "x_ratio", "*", "old_w_ratio", ",", "\n", "old_y_ratio", "+", "y_ratio", "*", "old_h_ratio", ",", "w_ratio", "*", "old_w_ratio", ",", "\n", "h_ratio", "*", "old_h_ratio", "\n", "]", "\n", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "new_crop_quadruple", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "new_h", ",", "new_w", "=", "self", ".", "size", ",", "self", ".", "size", "\n", "\n", "crop_bbox", "=", "np", ".", "array", "(", "\n", "[", "x_offset", ",", "y_offset", ",", "x_offset", "+", "new_w", ",", "y_offset", "+", "new_h", "]", ")", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bbox", "\n", "\n", "results", "[", "'img_shape'", "]", "=", "(", "new_h", ",", "new_w", ")", "\n", "\n", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "'keypoint'", "in", "results", ":", "\n", "                ", "results", "[", "'keypoint'", "]", "=", "self", ".", "_crop_kps", "(", "results", "[", "'keypoint'", "]", ",", "\n", "crop_bbox", ")", "\n", "", "if", "'imgs'", "in", "results", ":", "\n", "                ", "results", "[", "'imgs'", "]", "=", "self", ".", "_crop_imgs", "(", "results", "[", "'imgs'", "]", ",", "crop_bbox", ")", "\n", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Put Flip at last for now'", ")", "\n", "\n", "# record crop_bbox in lazyop dict to ensure only crop once in Fuse", "\n", "", "lazy_left", ",", "lazy_top", ",", "lazy_right", ",", "lazy_bottom", "=", "lazyop", "[", "'crop_bbox'", "]", "\n", "left", "=", "x_offset", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "right", "=", "(", "x_offset", "+", "new_w", ")", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "top", "=", "y_offset", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "bottom", "=", "(", "y_offset", "+", "new_h", ")", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "lazyop", "[", "'crop_bbox'", "]", "=", "np", ".", "array", "(", "[", "(", "lazy_left", "+", "left", ")", ",", "\n", "(", "lazy_top", "+", "top", ")", ",", "\n", "(", "lazy_left", "+", "right", ")", ",", "\n", "(", "lazy_top", "+", "bottom", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Process entity boxes", "\n", "", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "\n", "results", "=", "self", ".", "_all_box_crop", "(", "results", ",", "results", "[", "'crop_bbox'", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop.__repr__": [[694, 698], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}(size={self.size}, '", "\n", "f'lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomResizedCrop.__init__": [[717, 729], ["mmcv.is_tuple_of", "TypeError", "mmcv.is_tuple_of", "TypeError", "type", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "area_range", "=", "(", "0.08", ",", "1.0", ")", ",", "\n", "aspect_ratio_range", "=", "(", "3", "/", "4", ",", "4", "/", "3", ")", ",", "\n", "lazy", "=", "False", ")", ":", "\n", "        ", "self", ".", "area_range", "=", "area_range", "\n", "self", ".", "aspect_ratio_range", "=", "aspect_ratio_range", "\n", "self", ".", "lazy", "=", "lazy", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "area_range", ",", "float", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Area_range must be a tuple of float, '", "\n", "f'but got {type(area_range)}'", ")", "\n", "", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "aspect_ratio_range", ",", "float", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Aspect_ratio_range must be a tuple of float, '", "\n", "f'but got {type(aspect_ratio_range)}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomResizedCrop.get_crop_bbox": [[731, 781], ["numpy.exp", "numpy.round().astype", "numpy.round().astype", "range", "min", "numpy.random.uniform", "numpy.random.uniform", "numpy.log", "numpy.log", "numpy.round", "numpy.round", "random.randint", "random.randint", "numpy.sqrt", "numpy.sqrt"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "get_crop_bbox", "(", "img_shape", ",", "\n", "area_range", ",", "\n", "aspect_ratio_range", ",", "\n", "max_attempts", "=", "10", ")", ":", "\n", "        ", "\"\"\"Get a crop bbox given the area range and aspect ratio range.\n\n        Args:\n            img_shape (Tuple[int]): Image shape\n            area_range (Tuple[float]): The candidate area scales range of\n                output cropped images. Default: (0.08, 1.0).\n            aspect_ratio_range (Tuple[float]): The candidate aspect\n                ratio range of output cropped images. Default: (3 / 4, 4 / 3).\n                max_attempts (int): The maximum of attempts. Default: 10.\n            max_attempts (int): Max attempts times to generate random candidate\n                bounding box. If it doesn't qualified one, the center bounding\n                box will be used.\n        Returns:\n            (list[int]) A random crop bbox within the area range and aspect\n            ratio range.\n        \"\"\"", "\n", "assert", "0", "<", "area_range", "[", "0", "]", "<=", "area_range", "[", "1", "]", "<=", "1", "\n", "assert", "0", "<", "aspect_ratio_range", "[", "0", "]", "<=", "aspect_ratio_range", "[", "1", "]", "\n", "\n", "img_h", ",", "img_w", "=", "img_shape", "\n", "area", "=", "img_h", "*", "img_w", "\n", "\n", "min_ar", ",", "max_ar", "=", "aspect_ratio_range", "\n", "aspect_ratios", "=", "np", ".", "exp", "(", "\n", "np", ".", "random", ".", "uniform", "(", "\n", "np", ".", "log", "(", "min_ar", ")", ",", "np", ".", "log", "(", "max_ar", ")", ",", "size", "=", "max_attempts", ")", ")", "\n", "target_areas", "=", "np", ".", "random", ".", "uniform", "(", "*", "area_range", ",", "size", "=", "max_attempts", ")", "*", "area", "\n", "candidate_crop_w", "=", "np", ".", "round", "(", "np", ".", "sqrt", "(", "target_areas", "*", "\n", "aspect_ratios", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "candidate_crop_h", "=", "np", ".", "round", "(", "np", ".", "sqrt", "(", "target_areas", "/", "\n", "aspect_ratios", ")", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "\n", "for", "i", "in", "range", "(", "max_attempts", ")", ":", "\n", "            ", "crop_w", "=", "candidate_crop_w", "[", "i", "]", "\n", "crop_h", "=", "candidate_crop_h", "[", "i", "]", "\n", "if", "crop_h", "<=", "img_h", "and", "crop_w", "<=", "img_w", ":", "\n", "                ", "x_offset", "=", "random", ".", "randint", "(", "0", ",", "img_w", "-", "crop_w", ")", "\n", "y_offset", "=", "random", ".", "randint", "(", "0", ",", "img_h", "-", "crop_h", ")", "\n", "return", "x_offset", ",", "y_offset", ",", "x_offset", "+", "crop_w", ",", "y_offset", "+", "crop_h", "\n", "\n", "# Fallback", "\n", "", "", "crop_size", "=", "min", "(", "img_h", ",", "img_w", ")", "\n", "x_offset", "=", "(", "img_w", "-", "crop_size", ")", "//", "2", "\n", "y_offset", "=", "(", "img_h", "-", "crop_size", ")", "//", "2", "\n", "return", "x_offset", ",", "y_offset", ",", "x_offset", "+", "crop_size", ",", "y_offset", "+", "crop_size", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomResizedCrop.__call__": [[782, 851], ["augmentations._init_lazy_if_proper", "augmentations.RandomResizedCrop.get_crop_bbox", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "augmentations.RandomResizedCrop._all_box_crop", "augmentations.RandomResizedCrop._crop_kps", "augmentations.RandomResizedCrop._crop_imgs", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomResizedCrop.get_crop_bbox", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._all_box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_kps", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_imgs"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the RandomResizeCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "\n", "left", ",", "top", ",", "right", ",", "bottom", "=", "self", ".", "get_crop_bbox", "(", "\n", "(", "img_h", ",", "img_w", ")", ",", "self", ".", "area_range", ",", "self", ".", "aspect_ratio_range", ")", "\n", "new_h", ",", "new_w", "=", "bottom", "-", "top", ",", "right", "-", "left", "\n", "\n", "if", "'crop_quadruple'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "[", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# x, y, w, h", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "x_ratio", ",", "y_ratio", "=", "left", "/", "img_w", ",", "top", "/", "img_h", "\n", "w_ratio", ",", "h_ratio", "=", "new_w", "/", "img_w", ",", "new_h", "/", "img_h", "\n", "\n", "old_crop_quadruple", "=", "results", "[", "'crop_quadruple'", "]", "\n", "old_x_ratio", ",", "old_y_ratio", "=", "old_crop_quadruple", "[", "0", "]", ",", "old_crop_quadruple", "[", "1", "]", "\n", "old_w_ratio", ",", "old_h_ratio", "=", "old_crop_quadruple", "[", "2", "]", ",", "old_crop_quadruple", "[", "3", "]", "\n", "new_crop_quadruple", "=", "[", "\n", "old_x_ratio", "+", "x_ratio", "*", "old_w_ratio", ",", "\n", "old_y_ratio", "+", "y_ratio", "*", "old_h_ratio", ",", "w_ratio", "*", "old_w_ratio", ",", "\n", "h_ratio", "*", "old_h_ratio", "\n", "]", "\n", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "new_crop_quadruple", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "crop_bbox", "=", "np", ".", "array", "(", "[", "left", ",", "top", ",", "right", ",", "bottom", "]", ")", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bbox", "\n", "results", "[", "'img_shape'", "]", "=", "(", "new_h", ",", "new_w", ")", "\n", "\n", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "'keypoint'", "in", "results", ":", "\n", "                ", "results", "[", "'keypoint'", "]", "=", "self", ".", "_crop_kps", "(", "results", "[", "'keypoint'", "]", ",", "\n", "crop_bbox", ")", "\n", "", "if", "'imgs'", "in", "results", ":", "\n", "                ", "results", "[", "'imgs'", "]", "=", "self", ".", "_crop_imgs", "(", "results", "[", "'imgs'", "]", ",", "crop_bbox", ")", "\n", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Put Flip at last for now'", ")", "\n", "\n", "# record crop_bbox in lazyop dict to ensure only crop once in Fuse", "\n", "", "lazy_left", ",", "lazy_top", ",", "lazy_right", ",", "lazy_bottom", "=", "lazyop", "[", "'crop_bbox'", "]", "\n", "left", "=", "left", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "right", "=", "right", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "top", "=", "top", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "bottom", "=", "bottom", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "lazyop", "[", "'crop_bbox'", "]", "=", "np", ".", "array", "(", "[", "(", "lazy_left", "+", "left", ")", ",", "\n", "(", "lazy_top", "+", "top", ")", ",", "\n", "(", "lazy_left", "+", "right", ")", ",", "\n", "(", "lazy_top", "+", "bottom", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "\n", "results", "=", "self", ".", "_all_box_crop", "(", "results", ",", "results", "[", "'crop_bbox'", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomResizedCrop.__repr__": [[852, 858], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'area_range={self.area_range}, '", "\n", "f'aspect_ratio_range={self.aspect_ratio_range}, '", "\n", "f'lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MultiScaleCrop.__init__": [[892, 916], ["torch.nn.modules.utils._pair", "mmcv.is_tuple_of", "TypeError", "isinstance", "TypeError", "ValueError", "type", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "input_size", ",", "\n", "scales", "=", "(", "1", ",", ")", ",", "\n", "max_wh_scale_gap", "=", "1", ",", "\n", "random_crop", "=", "False", ",", "\n", "num_fixed_crops", "=", "5", ",", "\n", "lazy", "=", "False", ")", ":", "\n", "        ", "self", ".", "input_size", "=", "_pair", "(", "input_size", ")", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "input_size", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Input_size must be int or tuple of int, '", "\n", "f'but got {type(input_size)}'", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "scales", ",", "tuple", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Scales must be tuple, but got {type(scales)}'", ")", "\n", "\n", "", "if", "num_fixed_crops", "not", "in", "[", "5", ",", "13", "]", ":", "\n", "            ", "raise", "ValueError", "(", "f'Num_fix_crops must be in {[5, 13]}, '", "\n", "f'but got {num_fixed_crops}'", ")", "\n", "\n", "", "self", ".", "scales", "=", "scales", "\n", "self", ".", "max_wh_scale_gap", "=", "max_wh_scale_gap", "\n", "self", ".", "random_crop", "=", "random_crop", "\n", "self", ".", "num_fixed_crops", "=", "num_fixed_crops", "\n", "self", ".", "lazy", "=", "lazy", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MultiScaleCrop.__call__": [[917, 1028], ["augmentations._init_lazy_if_proper", "min", "enumerate", "random.choice", "range", "numpy.array", "numpy.array", "int", "enumerate", "random.randint", "random.randint", "random.choice", "numpy.array", "numpy.array", "augmentations.MultiScaleCrop._all_box_crop", "abs", "candidate_offsets.extend", "augmentations.MultiScaleCrop._crop_kps", "augmentations.MultiScaleCrop._crop_imgs", "NotImplementedError", "abs", "candidate_sizes.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._all_box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_kps", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_imgs"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the MultiScaleCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "base_size", "=", "min", "(", "img_h", ",", "img_w", ")", "\n", "crop_sizes", "=", "[", "int", "(", "base_size", "*", "s", ")", "for", "s", "in", "self", ".", "scales", "]", "\n", "\n", "candidate_sizes", "=", "[", "]", "\n", "for", "i", ",", "h", "in", "enumerate", "(", "crop_sizes", ")", ":", "\n", "            ", "for", "j", ",", "w", "in", "enumerate", "(", "crop_sizes", ")", ":", "\n", "                ", "if", "abs", "(", "i", "-", "j", ")", "<=", "self", ".", "max_wh_scale_gap", ":", "\n", "                    ", "candidate_sizes", ".", "append", "(", "[", "w", ",", "h", "]", ")", "\n", "\n", "", "", "", "crop_size", "=", "random", ".", "choice", "(", "candidate_sizes", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "if", "abs", "(", "crop_size", "[", "i", "]", "-", "self", ".", "input_size", "[", "i", "]", ")", "<", "3", ":", "\n", "                ", "crop_size", "[", "i", "]", "=", "self", ".", "input_size", "[", "i", "]", "\n", "\n", "", "", "crop_w", ",", "crop_h", "=", "crop_size", "\n", "\n", "if", "self", ".", "random_crop", ":", "\n", "            ", "x_offset", "=", "random", ".", "randint", "(", "0", ",", "img_w", "-", "crop_w", ")", "\n", "y_offset", "=", "random", ".", "randint", "(", "0", ",", "img_h", "-", "crop_h", ")", "\n", "", "else", ":", "\n", "            ", "w_step", "=", "(", "img_w", "-", "crop_w", ")", "//", "4", "\n", "h_step", "=", "(", "img_h", "-", "crop_h", ")", "//", "4", "\n", "candidate_offsets", "=", "[", "\n", "(", "0", ",", "0", ")", ",", "# upper left", "\n", "(", "4", "*", "w_step", ",", "0", ")", ",", "# upper right", "\n", "(", "0", ",", "4", "*", "h_step", ")", ",", "# lower left", "\n", "(", "4", "*", "w_step", ",", "4", "*", "h_step", ")", ",", "# lower right", "\n", "(", "2", "*", "w_step", ",", "2", "*", "h_step", ")", ",", "# center", "\n", "]", "\n", "if", "self", ".", "num_fixed_crops", "==", "13", ":", "\n", "                ", "extra_candidate_offsets", "=", "[", "\n", "(", "0", ",", "2", "*", "h_step", ")", ",", "# center left", "\n", "(", "4", "*", "w_step", ",", "2", "*", "h_step", ")", ",", "# center right", "\n", "(", "2", "*", "w_step", ",", "4", "*", "h_step", ")", ",", "# lower center", "\n", "(", "2", "*", "w_step", ",", "0", "*", "h_step", ")", ",", "# upper center", "\n", "(", "1", "*", "w_step", ",", "1", "*", "h_step", ")", ",", "# upper left quarter", "\n", "(", "3", "*", "w_step", ",", "1", "*", "h_step", ")", ",", "# upper right quarter", "\n", "(", "1", "*", "w_step", ",", "3", "*", "h_step", ")", ",", "# lower left quarter", "\n", "(", "3", "*", "w_step", ",", "3", "*", "h_step", ")", "# lower right quarter", "\n", "]", "\n", "candidate_offsets", ".", "extend", "(", "extra_candidate_offsets", ")", "\n", "", "x_offset", ",", "y_offset", "=", "random", ".", "choice", "(", "candidate_offsets", ")", "\n", "\n", "", "new_h", ",", "new_w", "=", "crop_h", ",", "crop_w", "\n", "\n", "crop_bbox", "=", "np", ".", "array", "(", "\n", "[", "x_offset", ",", "y_offset", ",", "x_offset", "+", "new_w", ",", "y_offset", "+", "new_h", "]", ")", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bbox", "\n", "results", "[", "'img_shape'", "]", "=", "(", "new_h", ",", "new_w", ")", "\n", "results", "[", "'scales'", "]", "=", "self", ".", "scales", "\n", "\n", "if", "'crop_quadruple'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "[", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# x, y, w, h", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "x_ratio", ",", "y_ratio", "=", "x_offset", "/", "img_w", ",", "y_offset", "/", "img_h", "\n", "w_ratio", ",", "h_ratio", "=", "new_w", "/", "img_w", ",", "new_h", "/", "img_h", "\n", "\n", "old_crop_quadruple", "=", "results", "[", "'crop_quadruple'", "]", "\n", "old_x_ratio", ",", "old_y_ratio", "=", "old_crop_quadruple", "[", "0", "]", ",", "old_crop_quadruple", "[", "1", "]", "\n", "old_w_ratio", ",", "old_h_ratio", "=", "old_crop_quadruple", "[", "2", "]", ",", "old_crop_quadruple", "[", "3", "]", "\n", "new_crop_quadruple", "=", "[", "\n", "old_x_ratio", "+", "x_ratio", "*", "old_w_ratio", ",", "\n", "old_y_ratio", "+", "y_ratio", "*", "old_h_ratio", ",", "w_ratio", "*", "old_w_ratio", ",", "\n", "h_ratio", "*", "old_h_ratio", "\n", "]", "\n", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "new_crop_quadruple", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "'keypoint'", "in", "results", ":", "\n", "                ", "results", "[", "'keypoint'", "]", "=", "self", ".", "_crop_kps", "(", "results", "[", "'keypoint'", "]", ",", "\n", "crop_bbox", ")", "\n", "", "if", "'imgs'", "in", "results", ":", "\n", "                ", "results", "[", "'imgs'", "]", "=", "self", ".", "_crop_imgs", "(", "results", "[", "'imgs'", "]", ",", "crop_bbox", ")", "\n", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Put Flip at last for now'", ")", "\n", "\n", "# record crop_bbox in lazyop dict to ensure only crop once in Fuse", "\n", "", "lazy_left", ",", "lazy_top", ",", "lazy_right", ",", "lazy_bottom", "=", "lazyop", "[", "'crop_bbox'", "]", "\n", "left", "=", "x_offset", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "right", "=", "(", "x_offset", "+", "new_w", ")", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "top", "=", "y_offset", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "bottom", "=", "(", "y_offset", "+", "new_h", ")", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "lazyop", "[", "'crop_bbox'", "]", "=", "np", ".", "array", "(", "[", "(", "lazy_left", "+", "left", ")", ",", "\n", "(", "lazy_top", "+", "top", ")", ",", "\n", "(", "lazy_left", "+", "right", ")", ",", "\n", "(", "lazy_top", "+", "bottom", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "\n", "results", "=", "self", ".", "_all_box_crop", "(", "results", ",", "results", "[", "'crop_bbox'", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MultiScaleCrop.__repr__": [[1029, 1037], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'input_size={self.input_size}, scales={self.scales}, '", "\n", "f'max_wh_scale_gap={self.max_wh_scale_gap}, '", "\n", "f'random_crop={self.random_crop}, '", "\n", "f'num_fixed_crops={self.num_fixed_crops}, '", "\n", "f'lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize.__init__": [[1063, 1084], ["isinstance", "isinstance", "ValueError", "max", "min", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "scale", ",", "\n", "keep_ratio", "=", "True", ",", "\n", "interpolation", "=", "'bilinear'", ",", "\n", "lazy", "=", "False", ")", ":", "\n", "        ", "if", "isinstance", "(", "scale", ",", "float", ")", ":", "\n", "            ", "if", "scale", "<=", "0", ":", "\n", "                ", "raise", "ValueError", "(", "f'Invalid scale {scale}, must be positive.'", ")", "\n", "", "", "elif", "isinstance", "(", "scale", ",", "tuple", ")", ":", "\n", "            ", "max_long_edge", "=", "max", "(", "scale", ")", "\n", "max_short_edge", "=", "min", "(", "scale", ")", "\n", "if", "max_short_edge", "==", "-", "1", ":", "\n", "# assign np.inf to long edge for rescaling short edge later.", "\n", "                ", "scale", "=", "(", "np", ".", "inf", ",", "max_long_edge", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f'Scale must be float or tuple of int, but got {type(scale)}'", ")", "\n", "", "self", ".", "scale", "=", "scale", "\n", "self", ".", "keep_ratio", "=", "keep_ratio", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "self", ".", "lazy", "=", "lazy", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._resize_imgs": [[1085, 1090], ["mmcv.imresize"], "methods", ["None"], ["", "def", "_resize_imgs", "(", "self", ",", "imgs", ",", "new_w", ",", "new_h", ")", ":", "\n", "        ", "return", "[", "\n", "mmcv", ".", "imresize", "(", "\n", "img", ",", "(", "new_w", ",", "new_h", ")", ",", "interpolation", "=", "self", ".", "interpolation", ")", "\n", "for", "img", "in", "imgs", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._resize_kps": [[1092, 1095], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_resize_kps", "(", "kps", ",", "scale_factor", ")", ":", "\n", "        ", "return", "kps", "*", "scale_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._box_resize": [[1096, 1107], ["numpy.concatenate", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_box_resize", "(", "box", ",", "scale_factor", ")", ":", "\n", "        ", "\"\"\"Rescale the bounding boxes according to the scale_factor.\n\n        Args:\n            box (np.ndarray): The bounding boxes.\n            scale_factor (np.ndarray): The scale factor used for rescaling.\n        \"\"\"", "\n", "assert", "len", "(", "scale_factor", ")", "==", "2", "\n", "scale_factor", "=", "np", ".", "concatenate", "(", "[", "scale_factor", ",", "scale_factor", "]", ")", "\n", "return", "box", "*", "scale_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize.__call__": [[1108, 1160], ["augmentations._init_lazy_if_proper", "numpy.array", "numpy.array", "mmcv.rescale_size", "augmentations.Resize._box_resize", "augmentations.Resize._resize_imgs", "augmentations.Resize._resize_kps", "NotImplementedError", "augmentations.Resize._box_resize"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._box_resize", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._resize_imgs", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._resize_kps", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize._box_resize"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Resize augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "\n", "", "if", "'scale_factor'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'scale_factor'", "]", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "\n", "if", "self", ".", "keep_ratio", ":", "\n", "            ", "new_w", ",", "new_h", "=", "mmcv", ".", "rescale_size", "(", "(", "img_w", ",", "img_h", ")", ",", "self", ".", "scale", ")", "\n", "", "else", ":", "\n", "            ", "new_w", ",", "new_h", "=", "self", ".", "scale", "\n", "\n", "", "self", ".", "scale_factor", "=", "np", ".", "array", "(", "[", "new_w", "/", "img_w", ",", "new_h", "/", "img_h", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "results", "[", "'img_shape'", "]", "=", "(", "new_h", ",", "new_w", ")", "\n", "results", "[", "'keep_ratio'", "]", "=", "self", ".", "keep_ratio", "\n", "results", "[", "'scale_factor'", "]", "=", "results", "[", "'scale_factor'", "]", "*", "self", ".", "scale_factor", "\n", "\n", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "'imgs'", "in", "results", ":", "\n", "                ", "results", "[", "'imgs'", "]", "=", "self", ".", "_resize_imgs", "(", "results", "[", "'imgs'", "]", ",", "new_w", ",", "\n", "new_h", ")", "\n", "", "if", "'keypoint'", "in", "results", ":", "\n", "                ", "results", "[", "'keypoint'", "]", "=", "self", ".", "_resize_kps", "(", "results", "[", "'keypoint'", "]", ",", "\n", "self", ".", "scale_factor", ")", "\n", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Put Flip at last for now'", ")", "\n", "", "lazyop", "[", "'interpolation'", "]", "=", "self", ".", "interpolation", "\n", "\n", "", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "\n", "results", "[", "'gt_bboxes'", "]", "=", "self", ".", "_box_resize", "(", "results", "[", "'gt_bboxes'", "]", ",", "\n", "self", ".", "scale_factor", ")", "\n", "if", "'proposals'", "in", "results", "and", "results", "[", "'proposals'", "]", "is", "not", "None", ":", "\n", "                ", "assert", "results", "[", "'proposals'", "]", ".", "shape", "[", "1", "]", "==", "4", "\n", "results", "[", "'proposals'", "]", "=", "self", ".", "_box_resize", "(", "\n", "results", "[", "'proposals'", "]", ",", "self", ".", "scale_factor", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Resize.__repr__": [[1161, 1167], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'scale={self.scale}, keep_ratio={self.keep_ratio}, '", "\n", "f'interpolation={self.interpolation}, '", "\n", "f'lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomRescale.__init__": [[1185, 1195], ["mmcv.is_tuple_of", "numpy.all", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "scale_range", ",", "interpolation", "=", "'bilinear'", ")", ":", "\n", "        ", "self", ".", "scale_range", "=", "scale_range", "\n", "# make sure scale_range is legal, first make sure the type is OK", "\n", "assert", "mmcv", ".", "is_tuple_of", "(", "scale_range", ",", "int", ")", "\n", "assert", "len", "(", "scale_range", ")", "==", "2", "\n", "assert", "scale_range", "[", "0", "]", "<", "scale_range", "[", "1", "]", "\n", "assert", "np", ".", "all", "(", "[", "x", ">", "0", "for", "x", "in", "scale_range", "]", ")", "\n", "\n", "self", ".", "keep_ratio", "=", "True", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomRescale.__call__": [[1196, 1213], ["numpy.random.randint", "augmentations.Resize", "Resize."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Resize augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "short_edge", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "scale_range", "[", "0", "]", ",", "\n", "self", ".", "scale_range", "[", "1", "]", "+", "1", ")", "\n", "resize", "=", "Resize", "(", "(", "-", "1", ",", "short_edge", ")", ",", "\n", "keep_ratio", "=", "True", ",", "\n", "interpolation", "=", "self", ".", "interpolation", ",", "\n", "lazy", "=", "False", ")", "\n", "results", "=", "resize", "(", "results", ")", "\n", "\n", "results", "[", "'short_edge'", "]", "=", "short_edge", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomRescale.__repr__": [[1214, 1220], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "scale_range", "=", "self", ".", "scale_range", "\n", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'scale_range=({scale_range[0]}, {scale_range[1]}), '", "\n", "f'interpolation={self.interpolation})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip.__init__": [[1250, 1266], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "flip_ratio", "=", "0.5", ",", "\n", "direction", "=", "'horizontal'", ",", "\n", "flip_label_map", "=", "None", ",", "\n", "left_kp", "=", "None", ",", "\n", "right_kp", "=", "None", ",", "\n", "lazy", "=", "False", ")", ":", "\n", "        ", "if", "direction", "not", "in", "self", ".", "_directions", ":", "\n", "            ", "raise", "ValueError", "(", "f'Direction {direction} is not supported. '", "\n", "f'Currently support ones are {self._directions}'", ")", "\n", "", "self", ".", "flip_ratio", "=", "flip_ratio", "\n", "self", ".", "direction", "=", "direction", "\n", "self", ".", "flip_label_map", "=", "flip_label_map", "\n", "self", ".", "left_kp", "=", "left_kp", "\n", "self", ".", "right_kp", "=", "right_kp", "\n", "self", ".", "lazy", "=", "lazy", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._flip_imgs": [[1267, 1275], ["len", "mmcv.imflip_", "range", "mmcv.iminvert"], "methods", ["None"], ["", "def", "_flip_imgs", "(", "self", ",", "imgs", ",", "modality", ")", ":", "\n", "        ", "_", "=", "[", "mmcv", ".", "imflip_", "(", "img", ",", "self", ".", "direction", ")", "for", "img", "in", "imgs", "]", "\n", "lt", "=", "len", "(", "imgs", ")", "\n", "if", "modality", "==", "'Flow'", ":", "\n", "# The 1st frame of each 2 frames is flow-x", "\n", "            ", "for", "i", "in", "range", "(", "0", ",", "lt", ",", "2", ")", ":", "\n", "                ", "imgs", "[", "i", "]", "=", "mmcv", ".", "iminvert", "(", "imgs", "[", "i", "]", ")", "\n", "", "", "return", "imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._flip_kps": [[1276, 1288], ["list", "range", "zip"], "methods", ["None"], ["", "def", "_flip_kps", "(", "self", ",", "kps", ",", "kpscores", ",", "img_width", ")", ":", "\n", "        ", "kp_x", "=", "kps", "[", "...", ",", "0", "]", "\n", "kp_x", "[", "kp_x", "!=", "0", "]", "=", "img_width", "-", "kp_x", "[", "kp_x", "!=", "0", "]", "\n", "new_order", "=", "list", "(", "range", "(", "kps", ".", "shape", "[", "2", "]", ")", ")", "\n", "if", "self", ".", "left_kp", "is", "not", "None", "and", "self", ".", "right_kp", "is", "not", "None", ":", "\n", "            ", "for", "left", ",", "right", "in", "zip", "(", "self", ".", "left_kp", ",", "self", ".", "right_kp", ")", ":", "\n", "                ", "new_order", "[", "left", "]", "=", "right", "\n", "new_order", "[", "right", "]", "=", "left", "\n", "", "", "kps", "=", "kps", "[", ":", ",", ":", ",", "new_order", "]", "\n", "if", "kpscores", "is", "not", "None", ":", "\n", "            ", "kpscores", "=", "kpscores", "[", ":", ",", ":", ",", "new_order", "]", "\n", "", "return", "kps", ",", "kpscores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._box_flip": [[1289, 1301], ["box.copy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_box_flip", "(", "box", ",", "img_width", ")", ":", "\n", "        ", "\"\"\"Flip the bounding boxes given the width of the image.\n\n        Args:\n            box (np.ndarray): The bounding boxes.\n            img_width (int): The img width.\n        \"\"\"", "\n", "box_", "=", "box", ".", "copy", "(", ")", "\n", "box_", "[", "...", ",", "0", ":", ":", "4", "]", "=", "img_width", "-", "box", "[", "...", ",", "2", ":", ":", "4", "]", "\n", "box_", "[", "...", ",", "2", ":", ":", "4", "]", "=", "img_width", "-", "box", "[", "...", ",", "0", ":", ":", "4", "]", "\n", "return", "box_", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip.__call__": [[1302, 1360], ["augmentations._init_lazy_if_proper", "numpy.random.rand", "augmentations.Flip.flip_label_map.get", "augmentations.Flip._box_flip", "NotImplementedError", "augmentations.Flip._box_flip", "augmentations.Flip._flip_imgs", "results.get", "augmentations.Flip._flip_kps"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._box_flip", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._box_flip", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._flip_imgs", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip._flip_kps"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the Flip augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "assert", "self", ".", "direction", "==", "'horizontal'", ",", "(", "\n", "'Only horizontal flips are'", "\n", "'supported for human keypoints'", ")", "\n", "\n", "", "modality", "=", "results", "[", "'modality'", "]", "\n", "if", "modality", "==", "'Flow'", ":", "\n", "            ", "assert", "self", ".", "direction", "==", "'horizontal'", "\n", "\n", "", "flip", "=", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "flip_ratio", "\n", "\n", "results", "[", "'flip'", "]", "=", "flip", "\n", "results", "[", "'flip_direction'", "]", "=", "self", ".", "direction", "\n", "img_width", "=", "results", "[", "'img_shape'", "]", "[", "1", "]", "\n", "\n", "if", "self", ".", "flip_label_map", "is", "not", "None", "and", "flip", ":", "\n", "            ", "results", "[", "'label'", "]", "=", "self", ".", "flip_label_map", ".", "get", "(", "results", "[", "'label'", "]", ",", "\n", "results", "[", "'label'", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "flip", ":", "\n", "                ", "if", "'imgs'", "in", "results", ":", "\n", "                    ", "results", "[", "'imgs'", "]", "=", "self", ".", "_flip_imgs", "(", "results", "[", "'imgs'", "]", ",", "\n", "modality", ")", "\n", "", "if", "'keypoint'", "in", "results", ":", "\n", "                    ", "kp", "=", "results", "[", "'keypoint'", "]", "\n", "kpscore", "=", "results", ".", "get", "(", "'keypoint_score'", ",", "None", ")", "\n", "kp", ",", "kpscore", "=", "self", ".", "_flip_kps", "(", "kp", ",", "kpscore", ",", "img_width", ")", "\n", "results", "[", "'keypoint'", "]", "=", "kp", "\n", "if", "'keypoint_score'", "in", "results", ":", "\n", "                        ", "results", "[", "'keypoint_score'", "]", "=", "kpscore", "\n", "", "", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Use one Flip please'", ")", "\n", "", "lazyop", "[", "'flip'", "]", "=", "flip", "\n", "lazyop", "[", "'flip_direction'", "]", "=", "self", ".", "direction", "\n", "\n", "", "if", "'gt_bboxes'", "in", "results", "and", "flip", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "and", "self", ".", "direction", "==", "'horizontal'", "\n", "width", "=", "results", "[", "'img_shape'", "]", "[", "1", "]", "\n", "results", "[", "'gt_bboxes'", "]", "=", "self", ".", "_box_flip", "(", "results", "[", "'gt_bboxes'", "]", ",", "width", ")", "\n", "if", "'proposals'", "in", "results", "and", "results", "[", "'proposals'", "]", "is", "not", "None", ":", "\n", "                ", "assert", "results", "[", "'proposals'", "]", ".", "shape", "[", "1", "]", "==", "4", "\n", "results", "[", "'proposals'", "]", "=", "self", ".", "_box_flip", "(", "results", "[", "'proposals'", "]", ",", "\n", "width", ")", "\n", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Flip.__repr__": [[1361, 1367], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "\n", "f'{self.__class__.__name__}('", "\n", "f'flip_ratio={self.flip_ratio}, direction={self.direction}, '", "\n", "f'flip_label_map={self.flip_label_map}, lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Normalize.__init__": [[1386, 1400], ["numpy.array", "numpy.array", "isinstance", "TypeError", "isinstance", "TypeError", "type", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ",", "to_bgr", "=", "False", ",", "adjust_magnitude", "=", "False", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "mean", ",", "Sequence", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f'Mean must be list, tuple or np.ndarray, but got {type(mean)}'", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "std", ",", "Sequence", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f'Std must be list, tuple or np.ndarray, but got {type(std)}'", ")", "\n", "\n", "", "self", ".", "mean", "=", "np", ".", "array", "(", "mean", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "std", "=", "np", ".", "array", "(", "std", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "to_bgr", "=", "to_bgr", "\n", "self", ".", "adjust_magnitude", "=", "adjust_magnitude", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Normalize.__call__": [[1401, 1445], ["len", "numpy.empty", "enumerate", "dict", "len", "numpy.empty", "numpy.empty", "range", "numpy.stack", "dict", "mmcv.imnormalize_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "modality", "=", "results", "[", "'modality'", "]", "\n", "\n", "if", "modality", "==", "'RGB'", ":", "\n", "            ", "n", "=", "len", "(", "results", "[", "'imgs'", "]", ")", "\n", "h", ",", "w", ",", "c", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "\n", "imgs", "=", "np", ".", "empty", "(", "(", "n", ",", "h", ",", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "results", "[", "'imgs'", "]", ")", ":", "\n", "                ", "imgs", "[", "i", "]", "=", "img", "\n", "\n", "", "for", "img", "in", "imgs", ":", "\n", "                ", "mmcv", ".", "imnormalize_", "(", "img", ",", "self", ".", "mean", ",", "self", ".", "std", ",", "self", ".", "to_bgr", ")", "\n", "\n", "", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "results", "[", "'img_norm_cfg'", "]", "=", "dict", "(", "\n", "mean", "=", "self", ".", "mean", ",", "std", "=", "self", ".", "std", ",", "to_bgr", "=", "self", ".", "to_bgr", ")", "\n", "return", "results", "\n", "", "if", "modality", "==", "'Flow'", ":", "\n", "            ", "num_imgs", "=", "len", "(", "results", "[", "'imgs'", "]", ")", "\n", "assert", "num_imgs", "%", "2", "==", "0", "\n", "assert", "self", ".", "mean", ".", "shape", "[", "0", "]", "==", "2", "\n", "assert", "self", ".", "std", ".", "shape", "[", "0", "]", "==", "2", "\n", "n", "=", "num_imgs", "//", "2", "\n", "h", ",", "w", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "\n", "x_flow", "=", "np", ".", "empty", "(", "(", "n", ",", "h", ",", "w", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "y_flow", "=", "np", ".", "empty", "(", "(", "n", ",", "h", ",", "w", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "                ", "x_flow", "[", "i", "]", "=", "results", "[", "'imgs'", "]", "[", "2", "*", "i", "]", "\n", "y_flow", "[", "i", "]", "=", "results", "[", "'imgs'", "]", "[", "2", "*", "i", "+", "1", "]", "\n", "", "x_flow", "=", "(", "x_flow", "-", "self", ".", "mean", "[", "0", "]", ")", "/", "self", ".", "std", "[", "0", "]", "\n", "y_flow", "=", "(", "y_flow", "-", "self", ".", "mean", "[", "1", "]", ")", "/", "self", ".", "std", "[", "1", "]", "\n", "if", "self", ".", "adjust_magnitude", ":", "\n", "                ", "x_flow", "=", "x_flow", "*", "results", "[", "'scale_factor'", "]", "[", "0", "]", "\n", "y_flow", "=", "y_flow", "*", "results", "[", "'scale_factor'", "]", "[", "1", "]", "\n", "", "imgs", "=", "np", ".", "stack", "(", "[", "x_flow", ",", "y_flow", "]", ",", "axis", "=", "-", "1", ")", "\n", "results", "[", "'imgs'", "]", "=", "imgs", "\n", "args", "=", "dict", "(", "\n", "mean", "=", "self", ".", "mean", ",", "\n", "std", "=", "self", ".", "std", ",", "\n", "to_bgr", "=", "self", ".", "to_bgr", ",", "\n", "adjust_magnitude", "=", "self", ".", "adjust_magnitude", ")", "\n", "results", "[", "'img_norm_cfg'", "]", "=", "args", "\n", "return", "results", "\n", "", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.Normalize.__repr__": [[1446, 1453], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'mean={self.mean}, '", "\n", "f'std={self.std}, '", "\n", "f'to_bgr={self.to_bgr}, '", "\n", "f'adjust_magnitude={self.adjust_magnitude})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.check_input": [[1475, 1482], ["isinstance"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "check_input", "(", "val", ",", "max", ",", "base", ")", ":", "\n", "        ", "if", "isinstance", "(", "val", ",", "tuple", ")", ":", "\n", "            ", "assert", "base", "-", "max", "<=", "val", "[", "0", "]", "<=", "val", "[", "1", "]", "<=", "base", "+", "max", "\n", "return", "val", "\n", "", "assert", "val", "<=", "max", "\n", "return", "(", "base", "-", "val", ",", "base", "+", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.rgb_to_grayscale": [[1483, 1486], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "rgb_to_grayscale", "(", "img", ")", ":", "\n", "        ", "return", "0.2989", "*", "img", "[", "...", ",", "0", "]", "+", "0.587", "*", "img", "[", "...", ",", "1", "]", "+", "0.114", "*", "img", "[", "...", ",", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_contrast": [[1487, 1491], ["numpy.mean", "augmentations.ColorJitter.rgb_to_grayscale"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.rgb_to_grayscale"], ["", "@", "staticmethod", "\n", "def", "adjust_contrast", "(", "img", ",", "factor", ")", ":", "\n", "        ", "val", "=", "np", ".", "mean", "(", "ColorJitter", ".", "rgb_to_grayscale", "(", "img", ")", ")", "\n", "return", "factor", "*", "img", "+", "(", "1", "-", "factor", ")", "*", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_saturation": [[1492, 1496], ["numpy.stack", "augmentations.ColorJitter.rgb_to_grayscale"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.rgb_to_grayscale"], ["", "@", "staticmethod", "\n", "def", "adjust_saturation", "(", "img", ",", "factor", ")", ":", "\n", "        ", "gray", "=", "np", ".", "stack", "(", "[", "ColorJitter", ".", "rgb_to_grayscale", "(", "img", ")", "]", "*", "3", ",", "axis", "=", "-", "1", ")", "\n", "return", "factor", "*", "img", "+", "(", "1", "-", "factor", ")", "*", "gray", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_hue": [[1497, 1505], ["numpy.clip().astype", "cv2.cvtColor", "int", "cv2.cvtColor", "cv2.cvtColor.astype", "numpy.clip"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "adjust_hue", "(", "img", ",", "factor", ")", ":", "\n", "        ", "img", "=", "np", ".", "clip", "(", "img", ",", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "hsv", "=", "cv2", ".", "cvtColor", "(", "img", ",", "cv2", ".", "COLOR_RGB2HSV", ")", "\n", "offset", "=", "int", "(", "factor", "*", "255", ")", "\n", "hsv", "[", "...", ",", "0", "]", "=", "(", "hsv", "[", "...", ",", "0", "]", "+", "offset", ")", "%", "180", "\n", "img", "=", "cv2", ".", "cvtColor", "(", "hsv", ",", "cv2", ".", "COLOR_HSV2RGB", ")", "\n", "return", "img", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.__init__": [[1506, 1512], ["augmentations.ColorJitter.check_input", "augmentations.ColorJitter.check_input", "augmentations.ColorJitter.check_input", "augmentations.ColorJitter.check_input", "numpy.random.permutation"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.check_input", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.check_input", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.check_input", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.check_input"], ["", "def", "__init__", "(", "self", ",", "brightness", "=", "0.5", ",", "contrast", "=", "0.5", ",", "saturation", "=", "0.5", ",", "hue", "=", "0.1", ")", ":", "\n", "        ", "self", ".", "brightness", "=", "self", ".", "check_input", "(", "brightness", ",", "1", ",", "1", ")", "\n", "self", ".", "contrast", "=", "self", ".", "check_input", "(", "contrast", ",", "1", ",", "1", ")", "\n", "self", ".", "saturation", "=", "self", ".", "check_input", "(", "saturation", ",", "1", ",", "1", ")", "\n", "self", ".", "hue", "=", "self", ".", "check_input", "(", "hue", ",", "0.5", ",", "0", ")", "\n", "self", ".", "fn_idx", "=", "np", ".", "random", ".", "permutation", "(", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.__call__": [[1513, 1542], ["range", "len", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "numpy.random.uniform", "augmentations.ColorJitter.astype", "numpy.clip().astype", "new_imgs.append", "augmentations.ColorJitter.adjust_contrast", "augmentations.ColorJitter.adjust_saturation", "augmentations.ColorJitter.adjust_hue", "numpy.clip"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_contrast", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_saturation", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.adjust_hue"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "imgs", "=", "results", "[", "'imgs'", "]", "\n", "num_clips", ",", "clip_len", "=", "1", ",", "len", "(", "imgs", ")", "\n", "\n", "new_imgs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_clips", ")", ":", "\n", "            ", "b", "=", "np", ".", "random", ".", "uniform", "(", "\n", "low", "=", "self", ".", "brightness", "[", "0", "]", ",", "high", "=", "self", ".", "brightness", "[", "1", "]", ")", "\n", "c", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "self", ".", "contrast", "[", "0", "]", ",", "high", "=", "self", ".", "contrast", "[", "1", "]", ")", "\n", "s", "=", "np", ".", "random", ".", "uniform", "(", "\n", "low", "=", "self", ".", "saturation", "[", "0", "]", ",", "high", "=", "self", ".", "saturation", "[", "1", "]", ")", "\n", "h", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "self", ".", "hue", "[", "0", "]", ",", "high", "=", "self", ".", "hue", "[", "1", "]", ")", "\n", "start", ",", "end", "=", "i", "*", "clip_len", ",", "(", "i", "+", "1", ")", "*", "clip_len", "\n", "\n", "for", "img", "in", "imgs", "[", "start", ":", "end", "]", ":", "\n", "                ", "img", "=", "img", ".", "astype", "(", "np", ".", "float32", ")", "\n", "for", "fn_id", "in", "self", ".", "fn_idx", ":", "\n", "                    ", "if", "fn_id", "==", "0", "and", "b", "!=", "1", ":", "\n", "                        ", "img", "*=", "b", "\n", "", "if", "fn_id", "==", "1", "and", "c", "!=", "1", ":", "\n", "                        ", "img", "=", "self", ".", "adjust_contrast", "(", "img", ",", "c", ")", "\n", "", "if", "fn_id", "==", "2", "and", "s", "!=", "1", ":", "\n", "                        ", "img", "=", "self", ".", "adjust_saturation", "(", "img", ",", "s", ")", "\n", "", "if", "fn_id", "==", "3", "and", "h", "!=", "0", ":", "\n", "                        ", "img", "=", "self", ".", "adjust_hue", "(", "img", ",", "h", ")", "\n", "", "", "img", "=", "np", ".", "clip", "(", "img", ",", "0", ",", "255", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "new_imgs", ".", "append", "(", "img", ")", "\n", "", "", "results", "[", "'imgs'", "]", "=", "new_imgs", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ColorJitter.__repr__": [[1543, 1550], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'brightness={self.brightness}, '", "\n", "f'contrast={self.contrast}, '", "\n", "f'saturation={self.saturation}, '", "\n", "f'hue={self.hue})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.CenterCrop.__init__": [[1566, 1571], ["torch.nn.modules.utils._pair", "mmcv.is_tuple_of", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "crop_size", ",", "lazy", "=", "False", ")", ":", "\n", "        ", "self", ".", "crop_size", "=", "_pair", "(", "crop_size", ")", "\n", "self", ".", "lazy", "=", "lazy", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "crop_size", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Crop_size must be int or tuple of int, '", "\n", "f'but got {type(crop_size)}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.CenterCrop.__call__": [[1573, 1645], ["augmentations._init_lazy_if_proper", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "augmentations.CenterCrop._all_box_crop", "augmentations.CenterCrop._crop_kps", "augmentations.CenterCrop._crop_imgs", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._all_box_crop", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_kps", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.RandomCrop._crop_imgs"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the CenterCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "self", ".", "lazy", ")", "\n", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", ",", "(", "'Keypoint Augmentations are not compatible '", "\n", "'with lazy == True'", ")", "\n", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "crop_w", ",", "crop_h", "=", "self", ".", "crop_size", "\n", "\n", "left", "=", "(", "img_w", "-", "crop_w", ")", "//", "2", "\n", "top", "=", "(", "img_h", "-", "crop_h", ")", "//", "2", "\n", "right", "=", "left", "+", "crop_w", "\n", "bottom", "=", "top", "+", "crop_h", "\n", "new_h", ",", "new_w", "=", "bottom", "-", "top", ",", "right", "-", "left", "\n", "\n", "crop_bbox", "=", "np", ".", "array", "(", "[", "left", ",", "top", ",", "right", ",", "bottom", "]", ")", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bbox", "\n", "results", "[", "'img_shape'", "]", "=", "(", "new_h", ",", "new_w", ")", "\n", "\n", "if", "'crop_quadruple'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "[", "0", ",", "0", ",", "1", ",", "1", "]", ",", "# x, y, w, h", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "x_ratio", ",", "y_ratio", "=", "left", "/", "img_w", ",", "top", "/", "img_h", "\n", "w_ratio", ",", "h_ratio", "=", "new_w", "/", "img_w", ",", "new_h", "/", "img_h", "\n", "\n", "old_crop_quadruple", "=", "results", "[", "'crop_quadruple'", "]", "\n", "old_x_ratio", ",", "old_y_ratio", "=", "old_crop_quadruple", "[", "0", "]", ",", "old_crop_quadruple", "[", "1", "]", "\n", "old_w_ratio", ",", "old_h_ratio", "=", "old_crop_quadruple", "[", "2", "]", ",", "old_crop_quadruple", "[", "3", "]", "\n", "new_crop_quadruple", "=", "[", "\n", "old_x_ratio", "+", "x_ratio", "*", "old_w_ratio", ",", "\n", "old_y_ratio", "+", "y_ratio", "*", "old_h_ratio", ",", "w_ratio", "*", "old_w_ratio", ",", "\n", "h_ratio", "*", "old_h_ratio", "\n", "]", "\n", "results", "[", "'crop_quadruple'", "]", "=", "np", ".", "array", "(", "\n", "new_crop_quadruple", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "if", "not", "self", ".", "lazy", ":", "\n", "            ", "if", "'keypoint'", "in", "results", ":", "\n", "                ", "results", "[", "'keypoint'", "]", "=", "self", ".", "_crop_kps", "(", "results", "[", "'keypoint'", "]", ",", "\n", "crop_bbox", ")", "\n", "", "if", "'imgs'", "in", "results", ":", "\n", "                ", "results", "[", "'imgs'", "]", "=", "self", ".", "_crop_imgs", "(", "results", "[", "'imgs'", "]", ",", "crop_bbox", ")", "\n", "", "", "else", ":", "\n", "            ", "lazyop", "=", "results", "[", "'lazy'", "]", "\n", "if", "lazyop", "[", "'flip'", "]", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Put Flip at last for now'", ")", "\n", "\n", "# record crop_bbox in lazyop dict to ensure only crop once in Fuse", "\n", "", "lazy_left", ",", "lazy_top", ",", "lazy_right", ",", "lazy_bottom", "=", "lazyop", "[", "'crop_bbox'", "]", "\n", "left", "=", "left", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "right", "=", "right", "*", "(", "lazy_right", "-", "lazy_left", ")", "/", "img_w", "\n", "top", "=", "top", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "bottom", "=", "bottom", "*", "(", "lazy_bottom", "-", "lazy_top", ")", "/", "img_h", "\n", "lazyop", "[", "'crop_bbox'", "]", "=", "np", ".", "array", "(", "[", "(", "lazy_left", "+", "left", ")", ",", "\n", "(", "lazy_top", "+", "top", ")", ",", "\n", "(", "lazy_left", "+", "right", ")", ",", "\n", "(", "lazy_top", "+", "bottom", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "if", "'gt_bboxes'", "in", "results", ":", "\n", "            ", "assert", "not", "self", ".", "lazy", "\n", "results", "=", "self", ".", "_all_box_crop", "(", "results", ",", "results", "[", "'crop_bbox'", "]", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.CenterCrop.__repr__": [[1646, 1650], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}(crop_size={self.crop_size}, '", "\n", "f'lazy={self.lazy})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ThreeCrop.__init__": [[1665, 1669], ["torch.nn.modules.utils._pair", "mmcv.is_tuple_of", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "crop_size", ")", ":", "\n", "        ", "self", ".", "crop_size", "=", "_pair", "(", "crop_size", ")", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "crop_size", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Crop_size must be int or tuple of int, '", "\n", "f'but got {type(crop_size)}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ThreeCrop.__call__": [[1671, 1719], ["augmentations._init_lazy_if_proper", "numpy.array", "warnings.warn", "cropped.extend", "numpy.array.extend", "range", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the ThreeCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "False", ")", "\n", "if", "'gt_bboxes'", "in", "results", "or", "'proposals'", "in", "results", ":", "\n", "            ", "warnings", ".", "warn", "(", "'ThreeCrop cannot process bounding boxes'", ")", "\n", "\n", "", "imgs", "=", "results", "[", "'imgs'", "]", "\n", "img_h", ",", "img_w", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "crop_w", ",", "crop_h", "=", "self", ".", "crop_size", "\n", "assert", "crop_h", "==", "img_h", "or", "crop_w", "==", "img_w", "\n", "\n", "if", "crop_h", "==", "img_h", ":", "\n", "            ", "w_step", "=", "(", "img_w", "-", "crop_w", ")", "//", "2", "\n", "offsets", "=", "[", "\n", "(", "0", ",", "0", ")", ",", "# left", "\n", "(", "2", "*", "w_step", ",", "0", ")", ",", "# right", "\n", "(", "w_step", ",", "0", ")", ",", "# middle", "\n", "]", "\n", "", "elif", "crop_w", "==", "img_w", ":", "\n", "            ", "h_step", "=", "(", "img_h", "-", "crop_h", ")", "//", "2", "\n", "offsets", "=", "[", "\n", "(", "0", ",", "0", ")", ",", "# top", "\n", "(", "0", ",", "2", "*", "h_step", ")", ",", "# down", "\n", "(", "0", ",", "h_step", ")", ",", "# middle", "\n", "]", "\n", "\n", "", "cropped", "=", "[", "]", "\n", "crop_bboxes", "=", "[", "]", "\n", "for", "x_offset", ",", "y_offset", "in", "offsets", ":", "\n", "            ", "bbox", "=", "[", "x_offset", ",", "y_offset", ",", "x_offset", "+", "crop_w", ",", "y_offset", "+", "crop_h", "]", "\n", "crop", "=", "[", "\n", "img", "[", "y_offset", ":", "y_offset", "+", "crop_h", ",", "x_offset", ":", "x_offset", "+", "crop_w", "]", "\n", "for", "img", "in", "imgs", "\n", "]", "\n", "cropped", ".", "extend", "(", "crop", ")", "\n", "crop_bboxes", ".", "extend", "(", "[", "bbox", "for", "_", "in", "range", "(", "len", "(", "imgs", ")", ")", "]", ")", "\n", "\n", "", "crop_bboxes", "=", "np", ".", "array", "(", "crop_bboxes", ")", "\n", "results", "[", "'imgs'", "]", "=", "cropped", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bboxes", "\n", "results", "[", "'img_shape'", "]", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.ThreeCrop.__repr__": [[1720, 1723], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}(crop_size={self.crop_size})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.TenCrop.__init__": [[1738, 1742], ["torch.nn.modules.utils._pair", "mmcv.is_tuple_of", "TypeError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "crop_size", ")", ":", "\n", "        ", "self", ".", "crop_size", "=", "_pair", "(", "crop_size", ")", "\n", "if", "not", "mmcv", ".", "is_tuple_of", "(", "self", ".", "crop_size", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'Crop_size must be int or tuple of int, '", "\n", "f'but got {type(crop_size)}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.TenCrop.__call__": [[1744, 1791], ["augmentations._init_lazy_if_proper", "list", "list", "numpy.array", "warnings.warn", "list.extend", "list.extend", "numpy.array.extend", "numpy.flip().copy", "numpy.flip", "range", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Performs the TenCrop augmentation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "_init_lazy_if_proper", "(", "results", ",", "False", ")", "\n", "\n", "if", "'gt_bboxes'", "in", "results", "or", "'proposals'", "in", "results", ":", "\n", "            ", "warnings", ".", "warn", "(", "'TenCrop cannot process bounding boxes'", ")", "\n", "\n", "", "imgs", "=", "results", "[", "'imgs'", "]", "\n", "\n", "img_h", ",", "img_w", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "crop_w", ",", "crop_h", "=", "self", ".", "crop_size", "\n", "\n", "w_step", "=", "(", "img_w", "-", "crop_w", ")", "//", "4", "\n", "h_step", "=", "(", "img_h", "-", "crop_h", ")", "//", "4", "\n", "\n", "offsets", "=", "[", "\n", "(", "0", ",", "0", ")", ",", "# upper left", "\n", "(", "4", "*", "w_step", ",", "0", ")", ",", "# upper right", "\n", "(", "0", ",", "4", "*", "h_step", ")", ",", "# lower left", "\n", "(", "4", "*", "w_step", ",", "4", "*", "h_step", ")", ",", "# lower right", "\n", "(", "2", "*", "w_step", ",", "2", "*", "h_step", ")", ",", "# center", "\n", "]", "\n", "\n", "img_crops", "=", "list", "(", ")", "\n", "crop_bboxes", "=", "list", "(", ")", "\n", "for", "x_offset", ",", "y_offsets", "in", "offsets", ":", "\n", "            ", "crop", "=", "[", "\n", "img", "[", "y_offsets", ":", "y_offsets", "+", "crop_h", ",", "x_offset", ":", "x_offset", "+", "crop_w", "]", "\n", "for", "img", "in", "imgs", "\n", "]", "\n", "flip_crop", "=", "[", "np", ".", "flip", "(", "c", ",", "axis", "=", "1", ")", ".", "copy", "(", ")", "for", "c", "in", "crop", "]", "\n", "bbox", "=", "[", "x_offset", ",", "y_offsets", ",", "x_offset", "+", "crop_w", ",", "y_offsets", "+", "crop_h", "]", "\n", "img_crops", ".", "extend", "(", "crop", ")", "\n", "img_crops", ".", "extend", "(", "flip_crop", ")", "\n", "crop_bboxes", ".", "extend", "(", "[", "bbox", "for", "_", "in", "range", "(", "len", "(", "imgs", ")", "*", "2", ")", "]", ")", "\n", "\n", "", "crop_bboxes", "=", "np", ".", "array", "(", "crop_bboxes", ")", "\n", "results", "[", "'imgs'", "]", "=", "img_crops", "\n", "results", "[", "'crop_bbox'", "]", "=", "crop_bboxes", "\n", "results", "[", "'img_shape'", "]", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.TenCrop.__repr__": [[1792, 1795], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}(crop_size={self.crop_size})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.AudioAmplify.__init__": [[1808, 1813], ["isinstance", "TypeError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ratio", ")", ":", "\n", "        ", "if", "isinstance", "(", "ratio", ",", "float", ")", ":", "\n", "            ", "self", ".", "ratio", "=", "ratio", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Amplification ratio should be float.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.AudioAmplify.__call__": [[1814, 1827], ["None"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform the audio amplification.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "\n", "assert", "'audios'", "in", "results", "\n", "results", "[", "'audios'", "]", "*=", "self", ".", "ratio", "\n", "results", "[", "'amplify_ratio'", "]", "=", "self", ".", "ratio", "\n", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.AudioAmplify.__repr__": [[1828, 1831], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}(ratio={self.ratio})'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MelSpectrogram.__init__": [[1849, 1863], ["all", "TypeError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "window_size", "=", "32", ",", "\n", "step_size", "=", "16", ",", "\n", "n_mels", "=", "80", ",", "\n", "fixed_length", "=", "128", ")", ":", "\n", "        ", "if", "all", "(", "\n", "isinstance", "(", "x", ",", "int", ")", "\n", "for", "x", "in", "[", "window_size", ",", "step_size", ",", "n_mels", ",", "fixed_length", "]", ")", ":", "\n", "            ", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "step_size", "=", "step_size", "\n", "self", ".", "n_mels", "=", "n_mels", "\n", "self", ".", "fixed_length", "=", "fixed_length", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'All arguments should be int.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MelSpectrogram.__call__": [[1864, 1898], ["int", "int", "list", "range", "numpy.array", "round", "round", "librosa.feature.melspectrogram", "list.append", "ImportError", "numpy.pad"], "methods", ["None"], ["", "", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Perform MelSpectrogram transformation.\n\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "librosa", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Install librosa first.'", ")", "\n", "", "signals", "=", "results", "[", "'audios'", "]", "\n", "sample_rate", "=", "results", "[", "'sample_rate'", "]", "\n", "n_fft", "=", "int", "(", "round", "(", "sample_rate", "*", "self", ".", "window_size", "/", "1000", ")", ")", "\n", "hop_length", "=", "int", "(", "round", "(", "sample_rate", "*", "self", ".", "step_size", "/", "1000", ")", ")", "\n", "melspectrograms", "=", "list", "(", ")", "\n", "for", "clip_idx", "in", "range", "(", "results", "[", "'num_clips'", "]", ")", ":", "\n", "            ", "clip_signal", "=", "signals", "[", "clip_idx", "]", "\n", "mel", "=", "librosa", ".", "feature", ".", "melspectrogram", "(", "\n", "y", "=", "clip_signal", ",", "\n", "sr", "=", "sample_rate", ",", "\n", "n_fft", "=", "n_fft", ",", "\n", "hop_length", "=", "hop_length", ",", "\n", "n_mels", "=", "self", ".", "n_mels", ")", "\n", "if", "mel", ".", "shape", "[", "0", "]", ">=", "self", ".", "fixed_length", ":", "\n", "                ", "mel", "=", "mel", "[", ":", "self", ".", "fixed_length", ",", ":", "]", "\n", "", "else", ":", "\n", "                ", "mel", "=", "np", ".", "pad", "(", "\n", "mel", ",", "(", "(", "0", ",", "mel", ".", "shape", "[", "-", "1", "]", "-", "self", ".", "fixed_length", ")", ",", "(", "0", ",", "0", ")", ")", ",", "\n", "mode", "=", "'edge'", ")", "\n", "", "melspectrograms", ".", "append", "(", "mel", ")", "\n", "\n", "", "results", "[", "'audios'", "]", "=", "np", ".", "array", "(", "melspectrograms", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations.MelSpectrogram.__repr__": [[1899, 1906], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}'", "\n", "f'(window_size={self.window_size}), '", "\n", "f'step_size={self.step_size}, '", "\n", "f'n_mels={self.n_mels}, '", "\n", "f'fixed_length={self.fixed_length})'", ")", "\n", "return", "repr_str", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._combine_quadruple": [[16, 18], ["None"], "function", ["None"], ["def", "_combine_quadruple", "(", "a", ",", "b", ")", ":", "\n", "    ", "return", "(", "a", "[", "0", "]", "+", "a", "[", "2", "]", "*", "b", "[", "0", "]", ",", "a", "[", "1", "]", "+", "a", "[", "3", "]", "*", "b", "[", "1", "]", ",", "a", "[", "2", "]", "*", "b", "[", "2", "]", ",", "a", "[", "3", "]", "*", "b", "[", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._flip_quadruple": [[20, 22], ["None"], "function", ["None"], ["", "def", "_flip_quadruple", "(", "a", ")", ":", "\n", "    ", "return", "(", "1", "-", "a", "[", "0", "]", "-", "a", "[", "2", "]", ",", "a", "[", "1", "]", ",", "a", "[", "2", "]", ",", "a", "[", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.augmentations._init_lazy_if_proper": [[24, 56], ["dict", "numpy.array"], "function", ["None"], ["", "def", "_init_lazy_if_proper", "(", "results", ",", "lazy", ")", ":", "\n", "    ", "\"\"\"Initialize lazy operation properly.\n\n    Make sure that a lazy operation is properly initialized,\n    and avoid a non-lazy operation accidentally getting mixed in.\n\n    Required keys in results are \"imgs\" if \"img_shape\" not in results,\n    otherwise, Required keys in results are \"img_shape\", add or modified keys\n    are \"img_shape\", \"lazy\".\n    Add or modified keys in \"lazy\" are \"original_shape\", \"crop_bbox\", \"flip\",\n    \"flip_direction\", \"interpolation\".\n\n    Args:\n        results (dict): A dict stores data pipeline result.\n        lazy (bool): Determine whether to apply lazy operation. Default: False.\n    \"\"\"", "\n", "\n", "if", "'img_shape'", "not", "in", "results", ":", "\n", "        ", "results", "[", "'img_shape'", "]", "=", "results", "[", "'imgs'", "]", "[", "0", "]", ".", "shape", "[", ":", "2", "]", "\n", "", "if", "lazy", ":", "\n", "        ", "if", "'lazy'", "not", "in", "results", ":", "\n", "            ", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "lazyop", "=", "dict", "(", ")", "\n", "lazyop", "[", "'original_shape'", "]", "=", "results", "[", "'img_shape'", "]", "\n", "lazyop", "[", "'crop_bbox'", "]", "=", "np", ".", "array", "(", "[", "0", ",", "0", ",", "img_w", ",", "img_h", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "lazyop", "[", "'flip'", "]", "=", "False", "\n", "lazyop", "[", "'flip_direction'", "]", "=", "None", "\n", "lazyop", "[", "'interpolation'", "]", "=", "None", "\n", "results", "[", "'lazy'", "]", "=", "lazyop", "\n", "", "", "else", ":", "\n", "        ", "assert", "'lazy'", "not", "in", "results", ",", "'Use Fuse after lazy operations'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames.__init__": [[34, 40], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "clip_len", ",", "num_clips", "=", "1", ",", "test_mode", "=", "False", ",", "seed", "=", "255", ")", ":", "\n", "\n", "        ", "self", ".", "clip_len", "=", "clip_len", "\n", "self", ".", "num_clips", "=", "num_clips", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "seed", "=", "seed", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_train_clips": [[41, 69], ["numpy.random.randint", "numpy.arange", "numpy.arange", "numpy.random.choice", "numpy.zeros", "numpy.cumsum", "numpy.array", "numpy.diff", "numpy.random.randint", "range"], "methods", ["None"], ["", "def", "_get_train_clips", "(", "self", ",", "num_frames", ",", "clip_len", ")", ":", "\n", "        ", "\"\"\"Uniformly sample indices for training clips.\n\n        Args:\n            num_frames (int): The number of frames.\n            clip_len (int): The length of the clip.\n        \"\"\"", "\n", "\n", "assert", "self", ".", "num_clips", "==", "1", "\n", "if", "num_frames", "<", "clip_len", ":", "\n", "            ", "start", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "num_frames", ")", "\n", "inds", "=", "np", ".", "arange", "(", "start", ",", "start", "+", "clip_len", ")", "\n", "", "elif", "clip_len", "<=", "num_frames", "<", "2", "*", "clip_len", ":", "\n", "            ", "basic", "=", "np", ".", "arange", "(", "clip_len", ")", "\n", "inds", "=", "np", ".", "random", ".", "choice", "(", "\n", "clip_len", "+", "1", ",", "num_frames", "-", "clip_len", ",", "replace", "=", "False", ")", "\n", "offset", "=", "np", ".", "zeros", "(", "clip_len", "+", "1", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "offset", "[", "inds", "]", "=", "1", "\n", "offset", "=", "np", ".", "cumsum", "(", "offset", ")", "\n", "inds", "=", "basic", "+", "offset", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "bids", "=", "np", ".", "array", "(", "\n", "[", "i", "*", "num_frames", "//", "clip_len", "for", "i", "in", "range", "(", "clip_len", "+", "1", ")", "]", ")", "\n", "bsize", "=", "np", ".", "diff", "(", "bids", ")", "\n", "bst", "=", "bids", "[", ":", "clip_len", "]", "\n", "offset", "=", "np", ".", "random", ".", "randint", "(", "bsize", ")", "\n", "inds", "=", "bst", "+", "offset", "\n", "", "return", "inds", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_test_clips": [[70, 113], ["numpy.random.seed", "numpy.concatenate", "list", "range", "numpy.concatenate", "numpy.array", "numpy.diff", "range", "numpy.concatenate", "range", "numpy.arange", "numpy.arange", "numpy.random.choice", "numpy.zeros", "numpy.cumsum", "all_inds.append", "numpy.random.randint", "all_inds.append", "range", "range"], "methods", ["None"], ["", "def", "_get_test_clips", "(", "self", ",", "num_frames", ",", "clip_len", ")", ":", "\n", "        ", "\"\"\"Uniformly sample indices for testing clips.\n\n        Args:\n            num_frames (int): The number of frames.\n            clip_len (int): The length of the clip.\n        \"\"\"", "\n", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "seed", ")", "\n", "if", "num_frames", "<", "clip_len", ":", "\n", "# Then we use a simple strategy", "\n", "            ", "if", "num_frames", "<", "self", ".", "num_clips", ":", "\n", "                ", "start_inds", "=", "list", "(", "range", "(", "self", ".", "num_clips", ")", ")", "\n", "", "else", ":", "\n", "                ", "start_inds", "=", "[", "\n", "i", "*", "num_frames", "//", "self", ".", "num_clips", "\n", "for", "i", "in", "range", "(", "self", ".", "num_clips", ")", "\n", "]", "\n", "", "inds", "=", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "arange", "(", "i", ",", "i", "+", "clip_len", ")", "for", "i", "in", "start_inds", "]", ")", "\n", "", "elif", "clip_len", "<=", "num_frames", "<", "clip_len", "*", "2", ":", "\n", "            ", "all_inds", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_clips", ")", ":", "\n", "                ", "basic", "=", "np", ".", "arange", "(", "clip_len", ")", "\n", "inds", "=", "np", ".", "random", ".", "choice", "(", "\n", "clip_len", "+", "1", ",", "num_frames", "-", "clip_len", ",", "replace", "=", "False", ")", "\n", "offset", "=", "np", ".", "zeros", "(", "clip_len", "+", "1", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "offset", "[", "inds", "]", "=", "1", "\n", "offset", "=", "np", ".", "cumsum", "(", "offset", ")", "\n", "inds", "=", "basic", "+", "offset", "[", ":", "-", "1", "]", "\n", "all_inds", ".", "append", "(", "inds", ")", "\n", "", "inds", "=", "np", ".", "concatenate", "(", "all_inds", ")", "\n", "", "else", ":", "\n", "            ", "bids", "=", "np", ".", "array", "(", "\n", "[", "i", "*", "num_frames", "//", "clip_len", "for", "i", "in", "range", "(", "clip_len", "+", "1", ")", "]", ")", "\n", "bsize", "=", "np", ".", "diff", "(", "bids", ")", "\n", "bst", "=", "bids", "[", ":", "clip_len", "]", "\n", "all_inds", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_clips", ")", ":", "\n", "                ", "offset", "=", "np", ".", "random", ".", "randint", "(", "bsize", ")", "\n", "all_inds", ".", "append", "(", "bst", "+", "offset", ")", "\n", "", "inds", "=", "np", ".", "concatenate", "(", "all_inds", ")", "\n", "", "return", "inds", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames.__call__": [[114, 131], ["numpy.mod", "pose_loading.UniformSampleFrames.astype", "pose_loading.UniformSampleFrames._get_test_clips", "pose_loading.UniformSampleFrames._get_train_clips"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_test_clips", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames._get_train_clips"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "num_frames", "=", "results", "[", "'total_frames'", "]", "\n", "\n", "if", "self", ".", "test_mode", ":", "\n", "            ", "inds", "=", "self", ".", "_get_test_clips", "(", "num_frames", ",", "self", ".", "clip_len", ")", "\n", "", "else", ":", "\n", "            ", "inds", "=", "self", ".", "_get_train_clips", "(", "num_frames", ",", "self", ".", "clip_len", ")", "\n", "\n", "", "inds", "=", "np", ".", "mod", "(", "inds", ",", "num_frames", ")", "\n", "start_index", "=", "results", "[", "'start_index'", "]", "\n", "inds", "=", "inds", "+", "start_index", "\n", "\n", "results", "[", "'frame_inds'", "]", "=", "inds", ".", "astype", "(", "np", ".", "int", ")", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "None", "\n", "results", "[", "'num_clips'", "]", "=", "self", ".", "num_clips", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.UniformSampleFrames.__repr__": [[132, 139], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'clip_len={self.clip_len}, '", "\n", "f'num_clips={self.num_clips}, '", "\n", "f'test_mode={self.test_mode}, '", "\n", "f'seed={self.seed})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseDecode._load_kp": [[150, 160], ["x[].astype"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "_load_kp", "(", "kp", ",", "frame_inds", ")", ":", "\n", "        ", "\"\"\"Load keypoints given frame indices.\n\n        Args:\n            kp (np.ndarray): The keypoint coordinates.\n            frame_inds (np.ndarray): The frame indices.\n        \"\"\"", "\n", "\n", "return", "[", "x", "[", "frame_inds", "]", ".", "astype", "(", "np", ".", "float32", ")", "for", "x", "in", "kp", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseDecode._load_kpscore": [[161, 171], ["x[].astype"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_load_kpscore", "(", "kpscore", ",", "frame_inds", ")", ":", "\n", "        ", "\"\"\"Load keypoint scores given frame indices.\n\n        Args:\n            kpscore (np.ndarray): The confidence scores of keypoints.\n            frame_inds (np.ndarray): The frame indices.\n        \"\"\"", "\n", "\n", "return", "[", "x", "[", "frame_inds", "]", ".", "astype", "(", "np", ".", "float32", ")", "for", "x", "in", "kpscore", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseDecode.__call__": [[172, 193], ["results.get", "numpy.arange", "numpy.squeeze", "kpscore[].astype", "[].astype"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "\n", "        ", "if", "'frame_inds'", "not", "in", "results", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "arange", "(", "results", "[", "'total_frames'", "]", ")", "\n", "\n", "", "if", "results", "[", "'frame_inds'", "]", ".", "ndim", "!=", "1", ":", "\n", "            ", "results", "[", "'frame_inds'", "]", "=", "np", ".", "squeeze", "(", "results", "[", "'frame_inds'", "]", ")", "\n", "\n", "", "offset", "=", "results", ".", "get", "(", "'offset'", ",", "0", ")", "\n", "frame_inds", "=", "results", "[", "'frame_inds'", "]", "+", "offset", "\n", "\n", "if", "'keypoint_score'", "in", "results", ":", "\n", "            ", "kpscore", "=", "results", "[", "'keypoint_score'", "]", "\n", "results", "[", "'keypoint_score'", "]", "=", "kpscore", "[", ":", ",", "\n", "frame_inds", "]", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "if", "'keypoint'", "in", "results", ":", "\n", "            ", "results", "[", "'keypoint'", "]", "=", "results", "[", "'keypoint'", "]", "[", ":", ",", "frame_inds", "]", ".", "astype", "(", "\n", "np", ".", "float32", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseDecode.__repr__": [[194, 197], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "f'{self.__class__.__name__}()'", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.LoadKineticsPose.__init__": [[221, 250], ["dict", "copy.deepcopy", "dict", "dict", "NotImplementedError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "io_backend", "=", "'disk'", ",", "\n", "squeeze", "=", "True", ",", "\n", "max_person", "=", "100", ",", "\n", "keypoint_weight", "=", "dict", "(", "face", "=", "1", ",", "torso", "=", "2", ",", "limb", "=", "3", ")", ",", "\n", "source", "=", "'mmpose'", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "self", ".", "io_backend", "=", "io_backend", "\n", "self", ".", "squeeze", "=", "squeeze", "\n", "self", ".", "max_person", "=", "max_person", "\n", "self", ".", "keypoint_weight", "=", "cp", ".", "deepcopy", "(", "keypoint_weight", ")", "\n", "self", ".", "source", "=", "source", "\n", "\n", "if", "source", "==", "'openpose'", ":", "\n", "            ", "self", ".", "kpsubset", "=", "dict", "(", "\n", "face", "=", "[", "0", ",", "14", ",", "15", ",", "16", ",", "17", "]", ",", "\n", "torso", "=", "[", "1", ",", "2", ",", "8", ",", "5", ",", "11", "]", ",", "\n", "limb", "=", "[", "3", ",", "4", ",", "6", ",", "7", ",", "9", ",", "10", ",", "12", ",", "13", "]", ")", "\n", "", "elif", "source", "==", "'mmpose'", ":", "\n", "            ", "self", ".", "kpsubset", "=", "dict", "(", "\n", "face", "=", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", ",", "\n", "torso", "=", "[", "5", ",", "6", ",", "11", ",", "12", "]", ",", "\n", "limb", "=", "[", "7", ",", "8", ",", "9", ",", "10", ",", "13", ",", "14", ",", "15", ",", "16", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unknown source of Kinetics Pose'", ")", "\n", "\n", "", "self", ".", "kwargs", "=", "kwargs", "\n", "self", ".", "file_client", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.LoadKineticsPose.__call__": [[251, 336], ["results.pop", "results.pop", "pose_loading.LoadKineticsPose.file_client.get", "pickle.loads", "results.pop", "list", "numpy.zeros", "numpy.zeros", "numpy.zeros", "zip", "results.pop", "mmcv.fileio.FileClient", "numpy.unique", "numpy.array", "pose_loading.LoadKineticsPose.__call__.mapinds"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "\n", "        ", "assert", "'filename'", "in", "results", "\n", "filename", "=", "results", ".", "pop", "(", "'filename'", ")", "\n", "\n", "# only applicable to source == 'mmpose'", "\n", "anno_inds", "=", "None", "\n", "if", "'anno_inds'", "in", "results", ":", "\n", "            ", "assert", "self", ".", "source", "==", "'mmpose'", "\n", "anno_inds", "=", "results", ".", "pop", "(", "'anno_inds'", ")", "\n", "", "results", ".", "pop", "(", "'box_score'", ",", "None", ")", "\n", "\n", "if", "self", ".", "file_client", "is", "None", ":", "\n", "            ", "self", ".", "file_client", "=", "FileClient", "(", "self", ".", "io_backend", ",", "**", "self", ".", "kwargs", ")", "\n", "\n", "", "bytes", "=", "self", ".", "file_client", ".", "get", "(", "filename", ")", "\n", "\n", "# only the kp array is in the pickle file, each kp include x, y, score.", "\n", "kps", "=", "pickle", ".", "loads", "(", "bytes", ")", "\n", "\n", "total_frames", "=", "results", "[", "'total_frames'", "]", "\n", "\n", "frame_inds", "=", "results", ".", "pop", "(", "'frame_inds'", ")", "\n", "\n", "if", "anno_inds", "is", "not", "None", ":", "\n", "            ", "kps", "=", "kps", "[", "anno_inds", "]", "\n", "frame_inds", "=", "frame_inds", "[", "anno_inds", "]", "\n", "\n", "", "frame_inds", "=", "list", "(", "frame_inds", ")", "\n", "\n", "def", "mapinds", "(", "inds", ")", ":", "\n", "            ", "uni", "=", "np", ".", "unique", "(", "inds", ")", "\n", "map_", "=", "{", "x", ":", "i", "for", "i", ",", "x", "in", "enumerate", "(", "uni", ")", "}", "\n", "inds", "=", "[", "map_", "[", "x", "]", "for", "x", "in", "inds", "]", "\n", "return", "np", ".", "array", "(", "inds", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "\n", "", "if", "self", ".", "squeeze", ":", "\n", "            ", "frame_inds", "=", "mapinds", "(", "frame_inds", ")", "\n", "total_frames", "=", "np", ".", "max", "(", "frame_inds", ")", "+", "1", "\n", "\n", "# write it back", "\n", "", "results", "[", "'total_frames'", "]", "=", "total_frames", "\n", "\n", "h", ",", "w", "=", "results", "[", "'img_shape'", "]", "\n", "if", "self", ".", "source", "==", "'openpose'", ":", "\n", "            ", "kps", "[", ":", ",", ":", ",", "0", "]", "*=", "w", "\n", "kps", "[", ":", ",", ":", ",", "1", "]", "*=", "h", "\n", "\n", "", "num_kp", "=", "kps", ".", "shape", "[", "1", "]", "\n", "num_person", "=", "mode", "(", "frame_inds", ")", "[", "-", "1", "]", "[", "0", "]", "\n", "\n", "new_kp", "=", "np", ".", "zeros", "(", "[", "num_person", ",", "total_frames", ",", "num_kp", ",", "2", "]", ",", "\n", "dtype", "=", "np", ".", "float16", ")", "\n", "new_kpscore", "=", "np", ".", "zeros", "(", "[", "num_person", ",", "total_frames", ",", "num_kp", "]", ",", "\n", "dtype", "=", "np", ".", "float16", ")", "\n", "# 32768 is enough", "\n", "num_person_frame", "=", "np", ".", "zeros", "(", "[", "total_frames", "]", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "\n", "for", "frame_ind", ",", "kp", "in", "zip", "(", "frame_inds", ",", "kps", ")", ":", "\n", "            ", "person_ind", "=", "num_person_frame", "[", "frame_ind", "]", "\n", "new_kp", "[", "person_ind", ",", "frame_ind", "]", "=", "kp", "[", ":", ",", ":", "2", "]", "\n", "new_kpscore", "[", "person_ind", ",", "frame_ind", "]", "=", "kp", "[", ":", ",", "2", "]", "\n", "num_person_frame", "[", "frame_ind", "]", "+=", "1", "\n", "\n", "", "kpgrp", "=", "self", ".", "kpsubset", "\n", "weight", "=", "self", ".", "keypoint_weight", "\n", "results", "[", "'num_person'", "]", "=", "num_person", "\n", "\n", "if", "num_person", ">", "self", ".", "max_person", ":", "\n", "            ", "for", "i", "in", "range", "(", "total_frames", ")", ":", "\n", "                ", "np_frame", "=", "num_person_frame", "[", "i", "]", "\n", "val", "=", "new_kpscore", "[", ":", "np_frame", ",", "i", "]", "\n", "\n", "val", "=", "(", "\n", "np", ".", "sum", "(", "val", "[", ":", ",", "kpgrp", "[", "'face'", "]", "]", ",", "1", ")", "*", "weight", "[", "'face'", "]", "+", "\n", "np", ".", "sum", "(", "val", "[", ":", ",", "kpgrp", "[", "'torso'", "]", "]", ",", "1", ")", "*", "weight", "[", "'torso'", "]", "+", "\n", "np", ".", "sum", "(", "val", "[", ":", ",", "kpgrp", "[", "'limb'", "]", "]", ",", "1", ")", "*", "weight", "[", "'limb'", "]", ")", "\n", "inds", "=", "sorted", "(", "range", "(", "np_frame", ")", ",", "key", "=", "lambda", "x", ":", "-", "val", "[", "x", "]", ")", "\n", "new_kpscore", "[", ":", "np_frame", ",", "i", "]", "=", "new_kpscore", "[", "inds", ",", "i", "]", "\n", "new_kp", "[", ":", "np_frame", ",", "i", "]", "=", "new_kp", "[", "inds", ",", "i", "]", "\n", "", "results", "[", "'num_person'", "]", "=", "self", ".", "max_person", "\n", "\n", "", "results", "[", "'keypoint'", "]", "=", "new_kp", "[", ":", "self", ".", "max_person", "]", "\n", "results", "[", "'keypoint_score'", "]", "=", "new_kpscore", "[", ":", "self", ".", "max_person", "]", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.LoadKineticsPose.__repr__": [[337, 346], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'io_backend={self.io_backend}, '", "\n", "f'squeeze={self.squeeze}, '", "\n", "f'max_person={self.max_person}, '", "\n", "f'keypoint_weight={self.keypoint_weight}, '", "\n", "f'source={self.source}, '", "\n", "f'kwargs={self.kwargs})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.__init__": [[377, 404], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "sigma", "=", "0.6", ",", "\n", "use_score", "=", "True", ",", "\n", "with_kp", "=", "True", ",", "\n", "with_limb", "=", "False", ",", "\n", "skeletons", "=", "(", "(", "0", ",", "1", ")", ",", "(", "0", ",", "2", ")", ",", "(", "1", ",", "3", ")", ",", "(", "2", ",", "4", ")", ",", "(", "0", ",", "5", ")", ",", "(", "5", ",", "7", ")", ",", "\n", "(", "7", ",", "9", ")", ",", "(", "0", ",", "6", ")", ",", "(", "6", ",", "8", ")", ",", "(", "8", ",", "10", ")", ",", "(", "5", ",", "11", ")", ",", "(", "11", ",", "13", ")", ",", "\n", "(", "13", ",", "15", ")", ",", "(", "6", ",", "12", ")", ",", "(", "12", ",", "14", ")", ",", "(", "14", ",", "16", ")", ",", "(", "11", ",", "12", ")", ")", ",", "\n", "double", "=", "False", ",", "\n", "left_kp", "=", "(", "1", ",", "3", ",", "5", ",", "7", ",", "9", ",", "11", ",", "13", ",", "15", ")", ",", "\n", "right_kp", "=", "(", "2", ",", "4", ",", "6", ",", "8", ",", "10", ",", "12", ",", "14", ",", "16", ")", ")", ":", "\n", "\n", "        ", "self", ".", "sigma", "=", "sigma", "\n", "self", ".", "use_score", "=", "use_score", "\n", "self", ".", "with_kp", "=", "with_kp", "\n", "self", ".", "with_limb", "=", "with_limb", "\n", "self", ".", "double", "=", "double", "\n", "\n", "# an auxiliary const", "\n", "self", ".", "eps", "=", "1e-4", "\n", "\n", "assert", "self", ".", "with_kp", "or", "self", ".", "with_limb", ",", "(", "\n", "'At least one of \"with_limb\" '", "\n", "'and \"with_kp\" should be set as True.'", ")", "\n", "self", ".", "left_kp", "=", "left_kp", "\n", "self", ".", "right_kp", "=", "right_kp", "\n", "self", ".", "skeletons", "=", "skeletons", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_a_heatmap": [[405, 446], ["numpy.zeros", "zip", "max", "min", "max", "min", "numpy.arange", "numpy.arange", "numpy.exp", "numpy.maximum", "int", "int", "int", "int", "len", "len"], "methods", ["None"], ["", "def", "generate_a_heatmap", "(", "self", ",", "img_h", ",", "img_w", ",", "centers", ",", "sigma", ",", "max_values", ")", ":", "\n", "        ", "\"\"\"Generate pseudo heatmap for one keypoint in one frame.\n\n        Args:\n            img_h (int): The height of the heatmap.\n            img_w (int): The width of the heatmap.\n            centers (np.ndarray): The coordinates of corresponding keypoints\n                (of multiple persons).\n            sigma (float): The sigma of generated gaussian.\n            max_values (np.ndarray): The max values of each keypoint.\n\n        Returns:\n            np.ndarray: The generated pseudo heatmap.\n        \"\"\"", "\n", "\n", "heatmap", "=", "np", ".", "zeros", "(", "[", "img_h", ",", "img_w", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "center", ",", "max_value", "in", "zip", "(", "centers", ",", "max_values", ")", ":", "\n", "            ", "mu_x", ",", "mu_y", "=", "center", "[", "0", "]", ",", "center", "[", "1", "]", "\n", "if", "max_value", "<", "self", ".", "eps", ":", "\n", "                ", "continue", "\n", "\n", "", "st_x", "=", "max", "(", "int", "(", "mu_x", "-", "3", "*", "sigma", ")", ",", "0", ")", "\n", "ed_x", "=", "min", "(", "int", "(", "mu_x", "+", "3", "*", "sigma", ")", "+", "1", ",", "img_w", ")", "\n", "st_y", "=", "max", "(", "int", "(", "mu_y", "-", "3", "*", "sigma", ")", ",", "0", ")", "\n", "ed_y", "=", "min", "(", "int", "(", "mu_y", "+", "3", "*", "sigma", ")", "+", "1", ",", "img_h", ")", "\n", "x", "=", "np", ".", "arange", "(", "st_x", ",", "ed_x", ",", "1", ",", "np", ".", "float32", ")", "\n", "y", "=", "np", ".", "arange", "(", "st_y", ",", "ed_y", ",", "1", ",", "np", ".", "float32", ")", "\n", "\n", "# if the keypoint not in the heatmap coordinate system", "\n", "if", "not", "(", "len", "(", "x", ")", "and", "len", "(", "y", ")", ")", ":", "\n", "                ", "continue", "\n", "", "y", "=", "y", "[", ":", ",", "None", "]", "\n", "\n", "patch", "=", "np", ".", "exp", "(", "-", "(", "(", "x", "-", "mu_x", ")", "**", "2", "+", "(", "y", "-", "mu_y", ")", "**", "2", ")", "/", "2", "/", "sigma", "**", "2", ")", "\n", "patch", "=", "patch", "*", "max_value", "\n", "heatmap", "[", "st_y", ":", "ed_y", ",", "\n", "st_x", ":", "ed_x", "]", "=", "np", ".", "maximum", "(", "heatmap", "[", "st_y", ":", "ed_y", ",", "st_x", ":", "ed_x", "]", ",", "\n", "patch", ")", "\n", "\n", "", "return", "heatmap", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_a_limb_heatmap": [[447, 532], ["numpy.zeros", "zip", "min", "max", "min", "max", "min", "numpy.arange", "numpy.arange", "numpy.zeros_like", "numpy.zeros_like", "numpy.stack", "numpy.exp", "numpy.maximum", "min", "max", "min", "max", "int", "int", "pose_loading.GeneratePoseTarget.generate_a_heatmap", "numpy.maximum", "int", "int", "len", "len", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_a_heatmap"], ["", "def", "generate_a_limb_heatmap", "(", "self", ",", "img_h", ",", "img_w", ",", "starts", ",", "ends", ",", "sigma", ",", "\n", "start_values", ",", "end_values", ")", ":", "\n", "        ", "\"\"\"Generate pseudo heatmap for one limb in one frame.\n\n        Args:\n            img_h (int): The height of the heatmap.\n            img_w (int): The width of the heatmap.\n            starts (np.ndarray): The coordinates of one keypoint in the\n                corresponding limbs (of multiple persons).\n            ends (np.ndarray): The coordinates of the other keypoint in the\n                corresponding limbs (of multiple persons).\n            sigma (float): The sigma of generated gaussian.\n            start_values (np.ndarray): The max values of one keypoint in the\n                corresponding limbs.\n            end_values (np.ndarray): The max values of the other keypoint in\n                the corresponding limbs.\n\n        Returns:\n            np.ndarray: The generated pseudo heatmap.\n        \"\"\"", "\n", "\n", "heatmap", "=", "np", ".", "zeros", "(", "[", "img_h", ",", "img_w", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "start", ",", "end", ",", "start_value", ",", "end_value", "in", "zip", "(", "starts", ",", "ends", ",", "\n", "start_values", ",", "\n", "end_values", ")", ":", "\n", "            ", "value_coeff", "=", "min", "(", "start_value", ",", "end_value", ")", "\n", "if", "value_coeff", "<", "self", ".", "eps", ":", "\n", "                ", "continue", "\n", "\n", "", "min_x", ",", "max_x", "=", "min", "(", "start", "[", "0", "]", ",", "end", "[", "0", "]", ")", ",", "max", "(", "start", "[", "0", "]", ",", "end", "[", "0", "]", ")", "\n", "min_y", ",", "max_y", "=", "min", "(", "start", "[", "1", "]", ",", "end", "[", "1", "]", ")", ",", "max", "(", "start", "[", "1", "]", ",", "end", "[", "1", "]", ")", "\n", "\n", "min_x", "=", "max", "(", "int", "(", "min_x", "-", "3", "*", "sigma", ")", ",", "0", ")", "\n", "max_x", "=", "min", "(", "int", "(", "max_x", "+", "3", "*", "sigma", ")", "+", "1", ",", "img_w", ")", "\n", "min_y", "=", "max", "(", "int", "(", "min_y", "-", "3", "*", "sigma", ")", ",", "0", ")", "\n", "max_y", "=", "min", "(", "int", "(", "max_y", "+", "3", "*", "sigma", ")", "+", "1", ",", "img_h", ")", "\n", "\n", "x", "=", "np", ".", "arange", "(", "min_x", ",", "max_x", ",", "1", ",", "np", ".", "float32", ")", "\n", "y", "=", "np", ".", "arange", "(", "min_y", ",", "max_y", ",", "1", ",", "np", ".", "float32", ")", "\n", "\n", "if", "not", "(", "len", "(", "x", ")", "and", "len", "(", "y", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "y", "=", "y", "[", ":", ",", "None", "]", "\n", "x_0", "=", "np", ".", "zeros_like", "(", "x", ")", "\n", "y_0", "=", "np", ".", "zeros_like", "(", "y", ")", "\n", "\n", "# distance to start keypoints", "\n", "d2_start", "=", "(", "(", "x", "-", "start", "[", "0", "]", ")", "**", "2", "+", "(", "y", "-", "start", "[", "1", "]", ")", "**", "2", ")", "\n", "\n", "# distance to end keypoints", "\n", "d2_end", "=", "(", "(", "x", "-", "end", "[", "0", "]", ")", "**", "2", "+", "(", "y", "-", "end", "[", "1", "]", ")", "**", "2", ")", "\n", "\n", "# the distance between start and end keypoints.", "\n", "d2_ab", "=", "(", "(", "start", "[", "0", "]", "-", "end", "[", "0", "]", ")", "**", "2", "+", "(", "start", "[", "1", "]", "-", "end", "[", "1", "]", ")", "**", "2", ")", "\n", "\n", "if", "d2_ab", "<", "1", ":", "\n", "                ", "full_map", "=", "self", ".", "generate_a_heatmap", "(", "img_h", ",", "img_w", ",", "[", "start", "]", ",", "\n", "sigma", ",", "[", "start_value", "]", ")", "\n", "heatmap", "=", "np", ".", "maximum", "(", "heatmap", ",", "full_map", ")", "\n", "continue", "\n", "\n", "", "coeff", "=", "(", "d2_start", "-", "d2_end", "+", "d2_ab", ")", "/", "2.", "/", "d2_ab", "\n", "\n", "a_dominate", "=", "coeff", "<=", "0", "\n", "b_dominate", "=", "coeff", ">=", "1", "\n", "seg_dominate", "=", "1", "-", "a_dominate", "-", "b_dominate", "\n", "\n", "position", "=", "np", ".", "stack", "(", "[", "x", "+", "y_0", ",", "y", "+", "x_0", "]", ",", "axis", "=", "-", "1", ")", "\n", "projection", "=", "start", "+", "np", ".", "stack", "(", "[", "coeff", ",", "coeff", "]", ",", "axis", "=", "-", "1", ")", "*", "(", "\n", "end", "-", "start", ")", "\n", "d2_line", "=", "position", "-", "projection", "\n", "d2_line", "=", "d2_line", "[", ":", ",", ":", ",", "0", "]", "**", "2", "+", "d2_line", "[", ":", ",", ":", ",", "1", "]", "**", "2", "\n", "d2_seg", "=", "(", "\n", "a_dominate", "*", "d2_start", "+", "b_dominate", "*", "d2_end", "+", "\n", "seg_dominate", "*", "d2_line", ")", "\n", "\n", "patch", "=", "np", ".", "exp", "(", "-", "d2_seg", "/", "2.", "/", "sigma", "**", "2", ")", "\n", "patch", "=", "patch", "*", "value_coeff", "\n", "\n", "heatmap", "[", "min_y", ":", "max_y", ",", "min_x", ":", "max_x", "]", "=", "np", ".", "maximum", "(", "\n", "heatmap", "[", "min_y", ":", "max_y", ",", "min_x", ":", "max_x", "]", ",", "patch", ")", "\n", "\n", "", "return", "heatmap", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_heatmap": [[533, 571], ["numpy.stack", "range", "pose_loading.GeneratePoseTarget.generate_a_heatmap", "heatmaps.append", "pose_loading.GeneratePoseTarget.generate_a_limb_heatmap", "heatmaps.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_a_heatmap", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_a_limb_heatmap"], ["", "def", "generate_heatmap", "(", "self", ",", "img_h", ",", "img_w", ",", "kps", ",", "sigma", ",", "max_values", ")", ":", "\n", "        ", "\"\"\"Generate pseudo heatmap for all keypoints and limbs in one frame (if\n        needed).\n\n        Args:\n            img_h (int): The height of the heatmap.\n            img_w (int): The width of the heatmap.\n            kps (np.ndarray): The coordinates of keypoints in this frame.\n            sigma (float): The sigma of generated gaussian.\n            max_values (np.ndarray): The confidence score of each keypoint.\n\n        Returns:\n            np.ndarray: The generated pseudo heatmap.\n        \"\"\"", "\n", "\n", "heatmaps", "=", "[", "]", "\n", "if", "self", ".", "with_kp", ":", "\n", "            ", "num_kp", "=", "kps", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "num_kp", ")", ":", "\n", "                ", "heatmap", "=", "self", ".", "generate_a_heatmap", "(", "img_h", ",", "img_w", ",", "kps", "[", ":", ",", "i", "]", ",", "\n", "sigma", ",", "max_values", "[", ":", ",", "i", "]", ")", "\n", "heatmaps", ".", "append", "(", "heatmap", ")", "\n", "\n", "", "", "if", "self", ".", "with_limb", ":", "\n", "            ", "for", "limb", "in", "self", ".", "skeletons", ":", "\n", "                ", "start_idx", ",", "end_idx", "=", "limb", "\n", "starts", "=", "kps", "[", ":", ",", "start_idx", "]", "\n", "ends", "=", "kps", "[", ":", ",", "end_idx", "]", "\n", "\n", "start_values", "=", "max_values", "[", ":", ",", "start_idx", "]", "\n", "end_values", "=", "max_values", "[", ":", ",", "end_idx", "]", "\n", "heatmap", "=", "self", ".", "generate_a_limb_heatmap", "(", "img_h", ",", "img_w", ",", "starts", ",", "\n", "ends", ",", "sigma", ",", "\n", "start_values", ",", "\n", "end_values", ")", "\n", "heatmaps", ".", "append", "(", "heatmap", ")", "\n", "\n", "", "", "return", "np", ".", "stack", "(", "heatmaps", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.gen_an_aug": [[572, 607], ["range", "numpy.ones", "numpy.ones", "pose_loading.GeneratePoseTarget.generate_heatmap", "imgs.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.generate_heatmap"], ["", "def", "gen_an_aug", "(", "self", ",", "results", ")", ":", "\n", "        ", "\"\"\"Generate pseudo heatmaps for all frames.\n\n        Args:\n            results (dict): The dictionary that contains all info of a sample.\n\n        Returns:\n            list[np.ndarray]: The generated pseudo heatmaps.\n        \"\"\"", "\n", "\n", "all_kps", "=", "results", "[", "'keypoint'", "]", "\n", "kp_shape", "=", "all_kps", ".", "shape", "\n", "\n", "if", "'keypoint_score'", "in", "results", ":", "\n", "            ", "all_kpscores", "=", "results", "[", "'keypoint_score'", "]", "\n", "", "else", ":", "\n", "            ", "all_kpscores", "=", "np", ".", "ones", "(", "kp_shape", "[", ":", "-", "1", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "img_h", ",", "img_w", "=", "results", "[", "'img_shape'", "]", "\n", "num_frame", "=", "kp_shape", "[", "1", "]", "\n", "\n", "imgs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_frame", ")", ":", "\n", "            ", "sigma", "=", "self", ".", "sigma", "\n", "kps", "=", "all_kps", "[", ":", ",", "i", "]", "\n", "kpscores", "=", "all_kpscores", "[", ":", ",", "i", "]", "\n", "\n", "max_values", "=", "np", ".", "ones", "(", "kpscores", ".", "shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "self", ".", "use_score", ":", "\n", "                ", "max_values", "=", "kpscores", "\n", "\n", "", "hmap", "=", "self", ".", "generate_heatmap", "(", "img_h", ",", "img_w", ",", "kps", ",", "sigma", ",", "max_values", ")", "\n", "imgs", ".", "append", "(", "hmap", ")", "\n", "\n", "", "return", "imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.__call__": [[608, 620], ["numpy.stack", "copy.deepcopy", "augmentations.Flip", "augmentations.Flip.", "numpy.concatenate", "pose_loading.GeneratePoseTarget.gen_an_aug", "pose_loading.GeneratePoseTarget.gen_an_aug", "pose_loading.GeneratePoseTarget.gen_an_aug"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.gen_an_aug", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.gen_an_aug", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.gen_an_aug"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "if", "not", "self", ".", "double", ":", "\n", "            ", "results", "[", "'imgs'", "]", "=", "np", ".", "stack", "(", "self", ".", "gen_an_aug", "(", "results", ")", ")", "\n", "", "else", ":", "\n", "            ", "results_", "=", "cp", ".", "deepcopy", "(", "results", ")", "\n", "flip", "=", "Flip", "(", "\n", "flip_ratio", "=", "1", ",", "left_kp", "=", "self", ".", "left_kp", ",", "right_kp", "=", "self", ".", "right_kp", ")", "\n", "results_", "=", "flip", "(", "results_", ")", "\n", "results", "[", "'imgs'", "]", "=", "np", ".", "concatenate", "(", "\n", "[", "self", ".", "gen_an_aug", "(", "results", ")", ",", "\n", "self", ".", "gen_an_aug", "(", "results_", ")", "]", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.GeneratePoseTarget.__repr__": [[621, 632], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "repr_str", "=", "(", "f'{self.__class__.__name__}('", "\n", "f'sigma={self.sigma}, '", "\n", "f'use_score={self.use_score}, '", "\n", "f'with_kp={self.with_kp}, '", "\n", "f'with_limb={self.with_limb}, '", "\n", "f'skeletons={self.skeletons}, '", "\n", "f'double={self.double}, '", "\n", "f'left_kp={self.left_kp}, '", "\n", "f'right_kp={self.right_kp})'", ")", "\n", "return", "repr_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PaddingWithLoop.__init__": [[650, 654], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "clip_len", ",", "num_clips", "=", "1", ")", ":", "\n", "\n", "        ", "self", ".", "clip_len", "=", "clip_len", "\n", "self", ".", "num_clips", "=", "num_clips", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PaddingWithLoop.__call__": [[655, 667], ["numpy.arange", "numpy.mod", "numpy.mod.astype"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "num_frames", "=", "results", "[", "'total_frames'", "]", "\n", "\n", "start", "=", "0", "\n", "inds", "=", "np", ".", "arange", "(", "start", ",", "start", "+", "self", ".", "clip_len", ")", "\n", "inds", "=", "np", ".", "mod", "(", "inds", ",", "num_frames", ")", "\n", "\n", "results", "[", "'frame_inds'", "]", "=", "inds", ".", "astype", "(", "np", ".", "int", ")", "\n", "results", "[", "'clip_len'", "]", "=", "self", ".", "clip_len", "\n", "results", "[", "'frame_interval'", "]", "=", "None", "\n", "results", "[", "'num_clips'", "]", "=", "self", ".", "num_clips", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseNormalize.__init__": [[679, 688], ["numpy.array().reshape", "numpy.array().reshape", "numpy.array().reshape", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "mean", "=", "(", "960.", ",", "540.", ",", "0.5", ")", ",", "\n", "min_value", "=", "(", "0.", ",", "0.", ",", "0.", ")", ",", "\n", "max_value", "=", "(", "1920", ",", "1080", ",", "1.", ")", ")", ":", "\n", "        ", "self", ".", "mean", "=", "np", ".", "array", "(", "mean", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "min_value", "=", "np", ".", "array", "(", "\n", "min_value", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "max_value", "=", "np", ".", "array", "(", "\n", "max_value", ",", "dtype", "=", "np", ".", "float32", ")", ".", "reshape", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.pose_loading.PoseNormalize.__call__": [[689, 696], ["dict"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "results", ")", ":", "\n", "        ", "keypoint", "=", "results", "[", "'keypoint'", "]", "\n", "keypoint", "=", "(", "keypoint", "-", "self", ".", "mean", ")", "/", "(", "self", ".", "max_value", "-", "self", ".", "min_value", ")", "\n", "results", "[", "'keypoint'", "]", "=", "keypoint", "\n", "results", "[", "'keypoint_norm_cfg'", "]", "=", "dict", "(", "\n", "mean", "=", "self", ".", "mean", ",", "min_value", "=", "self", ".", "min_value", ",", "max_value", "=", "self", ".", "max_value", ")", "\n", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.compose.Compose.__init__": [[19, 37], ["isinstance", "isinstance", "transform[].startswith", "compose.Compose.transforms.append", "callable", "augmentations.TorchvisionTrans", "transform[].startswith", "compose.Compose.transforms.append", "TypeError", "mmcv.utils.build_from_cfg.pop", "augmentations.PytorchVideoTrans", "mmcv.utils.build_from_cfg", "mmcv.utils.build_from_cfg.pop", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "assert", "isinstance", "(", "transforms", ",", "Sequence", ")", "\n", "self", ".", "transforms", "=", "[", "]", "\n", "for", "transform", "in", "transforms", ":", "\n", "            ", "if", "isinstance", "(", "transform", ",", "dict", ")", ":", "\n", "                ", "if", "transform", "[", "'type'", "]", ".", "startswith", "(", "'torchvision.'", ")", ":", "\n", "                    ", "trans_type", "=", "transform", ".", "pop", "(", "'type'", ")", "[", "12", ":", "]", "\n", "transform", "=", "TorchvisionTrans", "(", "trans_type", ",", "**", "transform", ")", "\n", "", "elif", "transform", "[", "'type'", "]", ".", "startswith", "(", "'pytorchvideo.'", ")", ":", "\n", "                    ", "trans_type", "=", "transform", ".", "pop", "(", "'type'", ")", "[", "13", ":", "]", "\n", "transform", "=", "PytorchVideoTrans", "(", "trans_type", ",", "**", "transform", ")", "\n", "", "else", ":", "\n", "                    ", "transform", "=", "build_from_cfg", "(", "transform", ",", "PIPELINES", ")", "\n", "", "self", ".", "transforms", ".", "append", "(", "transform", ")", "\n", "", "elif", "callable", "(", "transform", ")", ":", "\n", "                ", "self", ".", "transforms", ".", "append", "(", "transform", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "f'transform must be callable or a dict, '", "\n", "f'but got {type(transform)}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.compose.Compose.__call__": [[39, 54], ["t"], "methods", ["None"], ["", "", "", "def", "__call__", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"Call function to apply transforms sequentially.\n\n        Args:\n            data (dict): A result dict contains the data to transform.\n\n        Returns:\n            dict: Transformed data.\n        \"\"\"", "\n", "\n", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "data", "=", "t", "(", "data", ")", "\n", "if", "data", "is", "None", ":", "\n", "                ", "return", "None", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.pipelines.compose.Compose.__repr__": [[55, 62], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "format_string", "=", "self", ".", "__class__", ".", "__name__", "+", "'('", "\n", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "format_string", "+=", "'\\n'", "\n", "format_string", "+=", "'    {0}'", ".", "format", "(", "t", ")", "\n", "", "format_string", "+=", "'\\n)'", "\n", "return", "format_string", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.inference.init_recognizer": [[19, 54], ["isinstance", "mmaction.models.build_recognizer", "mmaction.models.build_recognizer.to", "mmaction.models.build_recognizer.eval", "warnings.warn", "mmcv.Config.fromfile", "mmcv.runner.load_checkpoint", "isinstance", "TypeError", "mmcv.Config.fromfile.get", "type"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_recognizer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "_logger", "=", "logging", ".", "getLogger", "(", "'inference'", ")", "\n", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch ImageNet Inference'", ")", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "'./'", ",", "\n", "help", "=", "'path to output files'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "'-m'", ",", "metavar", "=", "'MODEL'", ",", "default", "=", "'dpn92'", ",", "\n", "help", "=", "'model architecture (default: dpn92)'", ")", "\n", "parser", ".", "add_argument", "(", "'-j'", ",", "'--workers'", ",", "default", "=", "2", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of data loading workers (default: 2)'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch-size'", ",", "default", "=", "256", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'mini-batch size (default: 256)'", ")", "\n", "parser", ".", "add_argument", "(", "'--img-size'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Input image dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "None", ",", "nargs", "=", "3", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N N N'", ",", "help", "=", "'Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'MEAN'", ",", "\n", "help", "=", "'Override mean pixel value of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--std'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'STD'", ",", "\n", "help", "=", "'Override std deviation of of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--interpolation'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'Image resize interpolation type (overrides model)'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "'Number classes in dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-freq'", ",", "default", "=", "10", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'batch logging frequency (default: 10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'path to latest checkpoint (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "dest", "=", "'pretrained'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use pre-trained model'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-gpu'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of GPUS to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-test-pool'", ",", "dest", "=", "'no_test_pool'", ",", "action", "=", "'store_true'", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.inference.inference_recognizer": [[56, 183], ["isinstance", "isinstance", "mmaction.datasets.pipelines.Compose", "mmaction.datasets.pipelines.Compose.", "mmcv.parallel.collate", "tuple", "sorted", "warnings.warn", "warnings.warn", "isinstance", "isinstance", "next", "modality_map.get", "dict", "range", "dict", "range", "cfg.data.test.get", "cfg.data.test.get", "cfg.data.test.get", "pattern.replace.replace", "len", "dict", "range", "next", "mmaction.core.OutputHook", "zip", "model.parameters", "len", "dict", "len", "pattern.replace.replace", "list", "len", "model.parameters", "mmcv.parallel.scatter", "torch.no_grad", "range", "operator.itemgetter", "len", "isinstance", "video.startswith", "dict", "dict", "filter", "dict", "model", "isinstance", "os.exists", "os.isfile", "os.isdir", "RuntimeError", "dict", "pattern.replace.find", "os.listdir", "os.listdir", "pattern.replace.find", "re.match", "type"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["parser", ".", "add_argument", "(", "'--topk'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Top-k to output to CSV'", ")", "\n", "\n", "\n", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "# might as well try to do something useful...", "\n", "args", ".", "pretrained", "=", "args", ".", "pretrained", "or", "not", "args", ".", "checkpoint", "\n", "\n", "# create model", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "in_chans", "=", "3", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "checkpoint_path", "=", "args", ".", "checkpoint", ")", "\n", "\n", "_logger", ".", "info", "(", "'Model %s created, param count: %d'", "%", "\n", "(", "args", ".", "model", ",", "sum", "(", "[", "m", ".", "numel", "(", ")", "for", "m", "in", "model", ".", "parameters", "(", ")", "]", ")", ")", ")", "\n", "\n", "config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ")", "\n", "model", ",", "test_time_pool", "=", "(", "model", ",", "False", ")", "if", "args", ".", "no_test_pool", "else", "apply_test_time_pool", "(", "model", ",", "config", ")", "\n", "\n", "if", "args", ".", "num_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "list", "(", "range", "(", "args", ".", "num_gpu", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "", "loader", "=", "create_loader", "(", "\n", "ImageDataset", "(", "args", ".", "data", ")", ",", "\n", "input_size", "=", "config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "use_prefetcher", "=", "True", ",", "\n", "interpolation", "=", "config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "config", "[", "'mean'", "]", ",", "\n", "std", "=", "config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "crop_pct", "=", "1.0", "if", "test_time_pool", "else", "config", "[", "'crop_pct'", "]", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "k", "=", "min", "(", "args", ".", "topk", ",", "args", ".", "num_classes", ")", "\n", "batch_time", "=", "AverageMeter", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "topk_ids", "=", "[", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "_", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "labels", "=", "model", "(", "input", ")", "\n", "topk", "=", "labels", ".", "topk", "(", "k", ")", "[", "1", "]", "\n", "topk_ids", ".", "append", "(", "topk", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# measure elapsed time", "\n", "batch_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "batch_idx", "%", "args", ".", "log_freq", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "'Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})'", ".", "format", "(", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "batch_time", "=", "batch_time", ")", ")", "\n", "\n", "", "", "", "topk_ids", "=", "np", ".", "concatenate", "(", "topk_ids", ",", "axis", "=", "0", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'./topk_ids.csv'", ")", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "filenames", "=", "loader", ".", "dataset", ".", "filenames", "(", "basename", "=", "True", ")", "\n", "for", "filename", ",", "label", "in", "zip", "(", "filenames", ",", "topk_ids", ")", ":", "\n", "            ", "out_file", ".", "write", "(", "'{0},{1}\\n'", ".", "format", "(", "\n", "filename", ",", "','", ".", "join", "(", "[", "str", "(", "v", ")", "for", "v", "in", "label", "]", ")", ")", ")", "\n", "\n", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.init_random_seed": [[22, 53], ["mmcv.runner.get_dist_info", "numpy.random.randint", "torch.broadcast", "torch.tensor.item", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["from", "collections", "import", "OrderedDict", "\n", "from", "contextlib", "import", "suppress", "\n", "from", "datetime", "import", "datetime", "\n", "\n", "import", "torch", "\n", "import", "torch", ".", "nn", "as", "nn", "\n", "import", "torchvision", ".", "utils", "\n", "from", "mmcv", ".", "cnn", "import", "get_model_complexity_info", "\n", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "NativeDDP", "\n", "\n", "# from custom_tools import read_and_chunk_list", "\n", "from", "timm", ".", "data", "import", "create_dataset", ",", "create_loader", ",", "resolve_data_config", ",", "Mixup", ",", "FastCollateMixup", ",", "AugMixDataset", "\n", "from", "timm", ".", "models", "import", "create_model", ",", "safe_model_name", ",", "resume_checkpoint", ",", "load_checkpoint", ",", "convert_splitbn_model", ",", "model_parameters", "\n", "from", "timm", ".", "utils", "import", "*", "\n", "from", "timm", ".", "loss", "import", "*", "\n", "from", "timm", ".", "optim", "import", "create_optimizer_v2", ",", "optimizer_kwargs", "\n", "from", "timm", ".", "scheduler", "import", "create_scheduler", "\n", "from", "timm", ".", "utils", "import", "ApexScaler", ",", "NativeScaler", "\n", "\n", "try", ":", "\n", "    ", "from", "apex", "import", "amp", "\n", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "ApexDDP", "\n", "from", "apex", ".", "parallel", "import", "convert_syncbn_model", "\n", "has_apex", "=", "True", "\n", "", "except", "ImportError", ":", "\n", "    ", "has_apex", "=", "False", "\n", "\n", "", "has_native_amp", "=", "False", "\n", "try", ":", "\n", "    ", "if", "getattr", "(", "torch", ".", "cuda", ".", "amp", ",", "'autocast'", ")", "is", "not", "None", ":", "\n", "        ", "has_native_amp", "=", "True", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.train_model": [[55, 290], ["dict", "utils.get_root_logger", "dict", "dict", "mmcv.runner.build_optimizer", "Runner", "cfg.get", "print", "Runner.register_training_hooks", "cfg.get", "dict", "Runner.run", "isinstance", "cfg.data.get", "cfg.data.get", "cfg.get", "mmcv.parallel.MMDistributedDataParallel", "mmcv.parallel.MMDataParallel", "print", "mmcv.runner.hooks.Fp16OptimizerHook", "cfg.get", "datasets.build_dataset", "dict", "datasets.build_dataloader", "utils.PreciseBNHook", "Runner.register_hook", "cfg.get", "datasets.build_dataset", "dict", "dict", "datasets.build_dataloader", "Runner.register_hook", "Runner.resume", "dict", "datasets.build_dataset", "cfg.get().get", "cfg.get().get", "dict", "dict", "datasets.build_dataloader", "zip", "cfg.data.get", "cfg.data.get", "cfg.data.get", "len", "cfg.data.get", "datasets.build_dataloader", "datasets.build_dataloader", "mmcv.parallel.MMDataParallel.cuda", "mmcv.parallel.MMDataParallel.cuda", "mmcv.runner.OptimizerHook", "Runner.register_hook", "Runner.register_hook", "dict", "core.DistEvalHook", "core.EvalHook", "hasattr", "dict", "os.join", "names.append", "ckpts.append", "names.append", "ckpts.append", "test.multi_gpu_test", "mmcv.runner.get_dist_info", "len", "len", "copy.deepcopy", "dataloader_settings.append", "zip", "core.OmniSourceDistSamplerSeedHook", "mmcv.runner.DistSamplerSeedHook", "cfg.data.get", "cfg.data.get", "len", "cfg.get", "cfg.data.get", "cfg.data.get", "cfg.data.get", "len", "cfg.data.get", "train.inflate_3d_state_dict", "mmcv.runner.load_state_dict", "print", "Runner.load_checkpoint", "cfg.get", "cfg.get", "cfg.data.get", "cfg.data.get", "cfg.data.get", "len", "cfg.data.get", "Runner.load_checkpoint", "os.join", "datasets.build_dataset.dump_results", "cfg.get", "datasets.build_dataset.evaluate", "Runner.logger.info", "test_dataset.evaluate.items", "torch.cuda.current_device", "torch.cuda.current_device", "os.exists", "Runner.logger.info", "Runner.logger.info", "cfg.get.pop", "Runner.logger.info"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataset", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.builder.build_dataloader", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model.ActivationStatsHook.register_hook", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.inflate_3d_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.datasets.ava_dataset.AVADataset.dump_results", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate"], ["    ", "pass", "\n", "\n", "", "try", ":", "\n", "    ", "import", "wandb", "\n", "has_wandb", "=", "True", "\n", "# os.environ[\"WANDB_API_KEY\"] = YOUR_KEY_HERE", "\n", "", "except", "ImportError", ":", "\n", "    ", "has_wandb", "=", "False", "\n", "\n", "", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "_logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "\n", "# The first arg parser parses out only the --config argument, this argument is used to", "\n", "# load a yaml file containing key-values that override the defaults for the main parser below", "\n", "config_parser", "=", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Training Config'", ",", "add_help", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-c'", ",", "'--config'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'YAML config file specifying default arguments'", ")", "\n", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch ImageNet Training'", ")", "\n", "\n", "# Dataset parameters", "\n", "parser", ".", "add_argument", "(", "'data_dir'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "'-d'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "''", ",", "\n", "help", "=", "'dataset type (default: ImageFolder/ImageTar if empty)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-split'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "'train'", ",", "\n", "help", "=", "'dataset train split (default: train)'", ")", "\n", "parser", ".", "add_argument", "(", "'--val-split'", ",", "metavar", "=", "'NAME'", ",", "default", "=", "'validation'", ",", "\n", "help", "=", "'dataset validation split (default: validation)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset-download'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Allow download of dataset for torch/ and tfds/ datasets that support it.'", ")", "\n", "parser", ".", "add_argument", "(", "'--class-map'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILENAME'", ",", "\n", "help", "=", "'path to class to idx mapping file (default: \"\")'", ")", "\n", "\n", "# Model parameters", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "default", "=", "'resnet50'", ",", "type", "=", "str", ",", "metavar", "=", "'MODEL'", ",", "\n", "help", "=", "'Name of model to train (default: \"resnet50\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Start with pretrained version of specified network (if avail)'", ")", "\n", "parser", ".", "add_argument", "(", "'--initial-checkpoint'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'Initialize model from this checkpoint (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'Resume full model and optimizer state from checkpoint (default: none)'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-resume-opt'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'prevent resume of optimizer state when resuming model'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of label classes (Model default if None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--gp'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "metavar", "=", "'POOL'", ",", "\n", "help", "=", "'Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.'", ")", "\n", "parser", ".", "add_argument", "(", "'--img-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Image patch size (default: None => model default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "None", ",", "nargs", "=", "3", ",", "type", "=", "int", ",", "\n", "metavar", "=", "'N N N'", ",", "help", "=", "'Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty'", ")", "\n", "parser", ".", "add_argument", "(", "'--crop-pct'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "\n", "metavar", "=", "'N'", ",", "help", "=", "'Input image center crop percent (for validation only)'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'MEAN'", ",", "\n", "help", "=", "'Override mean pixel value of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--std'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'STD'", ",", "\n", "help", "=", "'Override std deviation of of dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--interpolation'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'Image resize interpolation type (overrides model)'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'input batch size for training (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "'-vb'", ",", "'--validation-batch-size'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'validation batch size override (default: None)'", ")", "\n", "\n", "# Optimizer parameters", "\n", "parser", ".", "add_argument", "(", "'--opt'", ",", "default", "=", "'sgd'", ",", "type", "=", "str", ",", "metavar", "=", "'OPTIMIZER'", ",", "\n", "help", "=", "'Optimizer (default: \"sgd\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-eps'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "metavar", "=", "'EPSILON'", ",", "\n", "help", "=", "'Optimizer Epsilon (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-betas'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "metavar", "=", "'BETA'", ",", "\n", "help", "=", "'Optimizer Betas (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'Optimizer momentum (default: 0.9)'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "2e-5", ",", "\n", "help", "=", "'weight decay (default: 2e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-grad'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'Clip gradient norm (default: None, no clipping)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-mode'", ",", "type", "=", "str", ",", "default", "=", "'norm'", ",", "\n", "help", "=", "'Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")'", ")", "\n", "\n", "\n", "# Learning rate schedule parameters", "\n", "parser", ".", "add_argument", "(", "'--sched'", ",", "default", "=", "'cosine'", ",", "type", "=", "str", ",", "metavar", "=", "'SCHEDULER'", ",", "\n", "help", "=", "'LR scheduler (default: \"step\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'learning rate (default: 0.05)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'pct, pct'", ",", "\n", "help", "=", "'learning rate noise on/off epoch percentages'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-pct'", ",", "type", "=", "float", ",", "default", "=", "0.67", ",", "metavar", "=", "'PERCENT'", ",", "\n", "help", "=", "'learning rate noise limit percent (default: 0.67)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'STDDEV'", ",", "\n", "help", "=", "'learning rate noise std-dev (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-mul'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'MULT'", ",", "\n", "help", "=", "'learning rate cycle len multiplier (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-decay'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "metavar", "=", "'MULT'", ",", "\n", "help", "=", "'amount to decay each learning rate cycle (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-cycle-limit'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'learning rate cycle limit, cycles enabled if > 1'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-k-decay'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'learning rate k-decay for cosine/poly (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-lr'", ",", "type", "=", "float", ",", "default", "=", "0.0001", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'warmup learning rate (default: 0.0001)'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'lower lr bound for cyclic schedulers that hit 0 (1e-5)'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of epochs to train (default: 300)'", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch-repeats'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).'", ")", "\n", "parser", ".", "add_argument", "(", "'--start-epoch'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'manual epoch number (useful on restarts)'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-epochs'", ",", "type", "=", "float", ",", "default", "=", "100", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch interval to decay LR'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epochs'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to warmup LR, if scheduler supports'", ")", "\n", "parser", ".", "add_argument", "(", "'--cooldown-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to cooldown LR at min_lr, after cyclic schedule ends'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'patience epochs for Plateau LR scheduler (default: 10'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "'--dr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'RATE'", ",", "\n", "help", "=", "'LR decay rate (default: 0.1)'", ")", "\n", "\n", "# Augmentation & regularization parameters", "\n", "parser", ".", "add_argument", "(", "'--no-aug'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable all training augmentation, override other train aug args'", ")", "\n", "parser", ".", "add_argument", "(", "'--scale'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "0.08", ",", "1.0", "]", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Random resize scale (default: 0.08 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "[", "3.", "/", "4.", ",", "4.", "/", "3.", "]", ",", "metavar", "=", "'RATIO'", ",", "\n", "help", "=", "'Random resize aspect ratio (default: 0.75 1.33)'", ")", "\n", "parser", ".", "add_argument", "(", "'--hflip'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "'Horizontal flip training aug probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--vflip'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'Vertical flip training aug probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--color-jitter'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Color jitter factor (default: 0.4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aa'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'Use AutoAugment policy. \"v0\" or \"original\". (default: None)'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--aug-repeats'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Number of augmentation repetitions (distributed training only) (default: 0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aug-splits'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Number of augmentation splits (default: 0, valid: 0 or >=2)'", ")", "\n", "parser", ".", "add_argument", "(", "'--jsd-loss'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.'", ")", "\n", "parser", ".", "add_argument", "(", "'--bce-loss'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable BCE loss w/ Mixup/CutMix use.'", ")", "\n", "parser", ".", "add_argument", "(", "'--bce-target-thresh'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "\n", "help", "=", "'Threshold for binarizing softened BCE targets (default: None, disabled)'", ")", "\n", "parser", ".", "add_argument", "(", "'--reprob'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Random erase prob (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--remode'", ",", "type", "=", "str", ",", "default", "=", "'pixel'", ",", "\n", "help", "=", "'Random erase mode (default: \"pixel\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--recount'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random erase count (default: 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--resplit'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Do not random erase first (clean) augmentation split'", ")", "\n", "parser", ".", "add_argument", "(", "'--mixup'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'mixup alpha, mixup enabled if > 0. (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--cutmix'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'cutmix alpha, cutmix enabled if > 0. (default: 0.)'", ")", "\n", "\n", "###############  New added property", "\n", "parser", ".", "add_argument", "(", "'--project'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'name of project'", ")", "\n", "parser", ".", "add_argument", "(", "'--wandb-id'", ",", "default", "=", "''", ",", "help", "=", "'id for wandb'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-image'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'log training and validation images to wandb'", ")", "\n", "parser", ".", "add_argument", "(", "'--step-by-iteration'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'update lr by iteration'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained2d'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'using imagenet pretrained weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--wandb-offline'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'upload the log to wandb later'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrain-dataset'", ",", "default", "=", "'k400'", ",", "type", "=", "str", ",", "\n", "help", "=", "'name of pretrained dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-list'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILENAME'", ",", "\n", "help", "=", "'path to train list for video dataset file (default: \"\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--val-list'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'FILENAME'", ",", "\n", "help", "=", "'path to train list for video dataset file (default: \"\")'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--split-list'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'split a huge datset into several parts'", ")", "\n", "###############", "\n", "\n", "parser", ".", "add_argument", "(", "'--cutmix-minmax'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "\n", "help", "=", "'cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--mixup-prob'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Probability of performing mixup or cutmix when either/both is enabled'", ")", "\n", "parser", ".", "add_argument", "(", "'--mixup-switch-prob'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "'Probability of switching to cutmix when both mixup and cutmix enabled'", ")", "\n", "parser", ".", "add_argument", "(", "'--mixup-mode'", ",", "type", "=", "str", ",", "default", "=", "'batch'", ",", "\n", "help", "=", "'How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--mixup-off-epoch'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Turn off mixup after this epoch, disabled if 0 (default: 0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--smoothing'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Label smoothing (default: 0.1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-interpolation'", ",", "type", "=", "str", ",", "default", "=", "'random'", ",", "\n", "help", "=", "'Training interpolation (random, bilinear, bicubic default: \"random\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Dropout rate (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop-connect'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Drop connect rate, DEPRECATED, use drop-path (default: None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop-path'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Drop path rate (default: None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop-block'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Drop block rate (default: None)'", ")", "\n", "\n", "# Batch norm parameters (only works with gen_efficientnet based models currently)", "\n", "parser", ".", "add_argument", "(", "'--bn-tf'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use Tensorflow BatchNorm defaults for models that support it (default: False)'", ")", "\n", "parser", ".", "add_argument", "(", "'--bn-momentum'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "\n", "help", "=", "'BatchNorm momentum override (if not None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--bn-eps'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "\n", "help", "=", "'BatchNorm epsilon override (if not None)'", ")", "\n", "parser", ".", "add_argument", "(", "'--sync-bn'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable NVIDIA Apex or Torch synchronized BatchNorm.'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-bn'", ",", "type", "=", "str", ",", "default", "=", "'reduce'", ",", "\n", "help", "=", "'Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--split-bn'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable separate BN layers per augmentation split.'", ")", "\n", "\n", "# Model Exponential Moving Average", "\n", "parser", ".", "add_argument", "(", "'--model-ema'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Enable tracking moving average of model weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--model-ema-force-cpu'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--model-ema-decay'", ",", "type", "=", "float", ",", "default", "=", "0.9998", ",", "\n", "help", "=", "'decay factor for model weights moving average (default: 0.9998)'", ")", "\n", "\n", "# Misc", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "42", ",", "metavar", "=", "'S'", ",", "\n", "help", "=", "'random seed (default: 42)'", ")", "\n", "parser", ".", "add_argument", "(", "'--worker-seeding'", ",", "type", "=", "str", ",", "default", "=", "'all'", ",", "\n", "help", "=", "'worker seed mode (default: all)'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "metavar", "=", "'N'", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.inflate_3d_state_dict": [[292, 312], ["print", "getattr", "collections.OrderedDict", "collections.OrderedDict.items", "torch.load", "torch.load", "collections.OrderedDict", "collections.OrderedDict", "range", "re.sub", "len", "v.size", "collections.OrderedDict.items"], "function", ["None"], ["parser", ".", "add_argument", "(", "'--recovery-interval'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'how many batches to wait before writing recovery checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint-hist'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of checkpoints to keep (default: 10)'", ")", "\n", "parser", ".", "add_argument", "(", "'-j'", ",", "'--workers'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'how many training processes to use (default: 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-images'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'save images of input bathes every log interval for debugging'", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'use NVIDIA Apex AMP or Native AMP for mixed precision training'", ")", "\n", "parser", ".", "add_argument", "(", "'--apex-amp'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use NVIDIA Apex AMP mixed precision'", ")", "\n", "parser", ".", "add_argument", "(", "'--native-amp'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use Native Torch AMP mixed precision'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-ddp-bb'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Force broadcast buffers for native DDP to off.'", ")", "\n", "parser", ".", "add_argument", "(", "'--channels-last'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Use channels_last memory layout'", ")", "\n", "parser", ".", "add_argument", "(", "'--pin-mem'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-prefetcher'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.apis.train.inflate_motion_state_dict": [[313, 335], ["getattr", "collections.OrderedDict.items", "torch.load", "torch.load", "collections.OrderedDict", "collections.OrderedDict", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "print", "re.sub", "len", "torch.cat.size", "range", "list", "collections.OrderedDict.items"], "function", ["None"], ["help", "=", "'disable fast prefetcher'", ")", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'PATH'", ",", "\n", "help", "=", "'path to output folder (default: none, current dir)'", ")", "\n", "parser", ".", "add_argument", "(", "'--experiment'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "metavar", "=", "'NAME'", ",", "\n", "help", "=", "'name of train experiment, name of sub-folder for output'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval-metric'", ",", "default", "=", "'top1'", ",", "type", "=", "str", ",", "metavar", "=", "'EVAL_METRIC'", ",", "\n", "help", "=", "'Best metric (default: \"top1\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--tta'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Test/inference time augmentation (oversampling) factor. 0=None (default: 0)'", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--use-multi-epochs-loader'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'use the multi-epochs-loader to save time at the beginning of every epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--torchscript'", ",", "dest", "=", "'torchscript'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'convert model torchscript for inference'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-wandb'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'log training and validation metrics to wandb'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Perform evaluation only'", ")", "\n", "\n", "\n", "\n", "def", "_parse_args", "(", ")", ":", "\n", "# Do we have a config file to parse?", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.Identity.forward": [[13, 15], ["None"], "methods", ["None"], ["def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.DownSample.__init__": [[45, 74], ["dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "torch.MaxPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "3", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "1", ",", "0", ",", "0", ")", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "downsample_position", "=", "'after'", ",", "\n", "downsample_scale", "=", "(", "1", ",", "2", ",", "2", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", ",", "\n", "padding", ",", "\n", "groups", "=", "groups", ",", "\n", "bias", "=", "bias", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "assert", "downsample_position", "in", "[", "'before'", ",", "'after'", "]", "\n", "self", ".", "downsample_position", "=", "downsample_position", "\n", "self", ".", "pool", "=", "nn", ".", "MaxPool3d", "(", "\n", "downsample_scale", ",", "downsample_scale", ",", "(", "0", ",", "0", ",", "0", ")", ",", "ceil_mode", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.DownSample.forward": [[75, 83], ["tpn.DownSample.pool", "tpn.DownSample.conv", "tpn.DownSample.conv", "tpn.DownSample.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "downsample_position", "==", "'before'", ":", "\n", "            ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.LevelFusion.__init__": [[103, 137], ["torch.Module.__init__", "len", "torch.ModuleList", "torch.ModuleList", "range", "mmcv.cnn.ConvModule", "tpn.DownSample", "tpn.LevelFusion.downsamples.append", "sum", "dict", "dict", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "downsample_scales", "=", "(", "(", "1", ",", "1", ",", "1", ")", ",", "(", "1", ",", "1", ",", "1", ")", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "num_stages", "=", "len", "(", "in_channels", ")", "\n", "\n", "self", ".", "downsamples", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "downsample", "=", "DownSample", "(", "\n", "in_channels", "[", "i", "]", ",", "\n", "mid_channels", "[", "i", "]", ",", "\n", "kernel_size", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "padding", "=", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "groups", "=", "32", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "downsample_position", "=", "'before'", ",", "\n", "downsample_scale", "=", "downsample_scales", "[", "i", "]", ")", "\n", "self", ".", "downsamples", ".", "append", "(", "downsample", ")", "\n", "\n", "", "self", ".", "fusion_conv", "=", "ConvModule", "(", "\n", "sum", "(", "mid_channels", ")", ",", "\n", "out_channels", ",", "\n", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.LevelFusion.forward": [[138, 144], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "tpn.LevelFusion.fusion_conv", "enumerate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "[", "self", ".", "downsamples", "[", "i", "]", "(", "feature", ")", "for", "i", ",", "feature", "in", "enumerate", "(", "x", ")", "]", "\n", "out", "=", "torch", ".", "cat", "(", "out", ",", "1", ")", "\n", "out", "=", "self", ".", "fusion_conv", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.SpatialModulation.__init__": [[159, 184], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "int", "torch.ModuleList", "torch.ModuleList", "tpn.SpatialModulation.spatial_modulation.append", "numpy.log2", "tpn.Identity", "range", "Identity.append", "mmcv.cnn.ConvModule", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "spatial_modulation", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "channel", "in", "in_channels", ":", "\n", "            ", "downsample_scale", "=", "out_channels", "//", "channel", "\n", "downsample_factor", "=", "int", "(", "np", ".", "log2", "(", "downsample_scale", ")", ")", "\n", "op", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "downsample_factor", "<", "1", ":", "\n", "                ", "op", "=", "Identity", "(", ")", "\n", "", "else", ":", "\n", "                ", "for", "factor", "in", "range", "(", "downsample_factor", ")", ":", "\n", "                    ", "in_factor", "=", "2", "**", "factor", "\n", "out_factor", "=", "2", "**", "(", "factor", "+", "1", ")", "\n", "op", ".", "append", "(", "\n", "ConvModule", "(", "\n", "channel", "*", "in_factor", ",", "\n", "channel", "*", "out_factor", ",", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ")", ")", "\n", "", "", "self", ".", "spatial_modulation", ".", "append", "(", "op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.SpatialModulation.forward": [[185, 196], ["enumerate", "isinstance", "out.append", "out.append", "op"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "out", "=", "[", "]", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "x", ")", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "spatial_modulation", "[", "i", "]", ",", "nn", ".", "ModuleList", ")", ":", "\n", "                ", "out_", "=", "x", "[", "i", "]", "\n", "for", "op", "in", "self", ".", "spatial_modulation", "[", "i", "]", ":", "\n", "                    ", "out_", "=", "op", "(", "out_", ")", "\n", "", "out", ".", "append", "(", "out_", ")", "\n", "", "else", ":", "\n", "                ", "out", ".", "append", "(", "self", ".", "spatial_modulation", "[", "i", "]", "(", "x", "[", "i", "]", ")", ")", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.AuxHead.__init__": [[213, 233], ["dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "builder.build_loss", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "loss_weight", "=", "0.5", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "in_channels", "*", "2", ",", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "0.5", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_channels", "*", "2", ",", "out_channels", ")", "\n", "self", ".", "loss_cls", "=", "build_loss", "(", "loss_cls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.AuxHead.init_weights": [[234, 242], ["tpn.AuxHead.modules", "isinstance", "isinstance", "isinstance", "mmcv.cnn.normal_init", "mmcv.cnn.xavier_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "normal_init", "(", "m", ",", "std", "=", "0.01", ")", "\n", "", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                ", "xavier_init", "(", "m", ",", "distribution", "=", "'uniform'", ")", "\n", "", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.AuxHead.forward": [[243, 257], ["dict", "tpn.AuxHead.conv", "tpn.AuxHead.avg_pool().squeeze().squeeze().squeeze", "tpn.AuxHead.dropout", "tpn.AuxHead.fc", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "target.unsqueeze.unsqueeze.unsqueeze", "tpn.AuxHead.loss_cls", "tpn.AuxHead.avg_pool().squeeze().squeeze", "tpn.AuxHead.avg_pool().squeeze", "tpn.AuxHead.avg_pool"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ",", "target", "=", "None", ")", ":", "\n", "        ", "losses", "=", "dict", "(", ")", "\n", "if", "target", "is", "None", ":", "\n", "            ", "return", "losses", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "avg_pool", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", ".", "squeeze", "(", "-", "1", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "\n", "if", "target", ".", "shape", "==", "torch", ".", "Size", "(", "[", "]", ")", ":", "\n", "            ", "target", "=", "target", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "", "losses", "[", "'loss_aux'", "]", "=", "self", ".", "loss_weight", "*", "self", ".", "loss_cls", "(", "x", ",", "target", ")", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.TemporalModulation.__init__": [[271, 286], ["torch.Module.__init__", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "torch.MaxPool3d", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "downsample_scale", "=", "8", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "conv", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "(", "3", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "1", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "False", ",", "\n", "groups", "=", "32", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "pool", "=", "nn", ".", "MaxPool3d", "(", "(", "downsample_scale", ",", "1", ",", "1", ")", ",", "\n", "(", "downsample_scale", ",", "1", ",", "1", ")", ",", "(", "0", ",", "0", ",", "0", ")", ",", "\n", "ceil_mode", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.TemporalModulation.forward": [[287, 291], ["tpn.TemporalModulation.conv", "tpn.TemporalModulation.pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.TPN.__init__": [[321, 399], ["torch.Module.__init__", "isinstance", "isinstance", "len", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "tpn.LevelFusion", "tpn.SpatialModulation", "range", "tpn.LevelFusion", "mmcv.cnn.ConvModule", "tpn.TPN.init_weights", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "ValueError", "tpn.AuxHead", "tpn.TemporalModulation", "tpn.TPN.temporal_modulation_ops.append", "dict", "dict", "torch.Upsample", "torch.Upsample", "tpn.TPN.upsample_ops.append", "tpn.DownSample", "tpn.TPN.downsample_ops.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "spatial_modulation_cfg", "=", "None", ",", "\n", "temporal_modulation_cfg", "=", "None", ",", "\n", "upsample_cfg", "=", "None", ",", "\n", "downsample_cfg", "=", "None", ",", "\n", "level_fusion_cfg", "=", "None", ",", "\n", "aux_head_cfg", "=", "None", ",", "\n", "flow_type", "=", "'cascade'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "in_channels", ",", "tuple", ")", "\n", "assert", "isinstance", "(", "out_channels", ",", "int", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "num_tpn_stages", "=", "len", "(", "in_channels", ")", "\n", "\n", "assert", "spatial_modulation_cfg", "is", "None", "or", "isinstance", "(", "\n", "spatial_modulation_cfg", ",", "dict", ")", "\n", "assert", "temporal_modulation_cfg", "is", "None", "or", "isinstance", "(", "\n", "temporal_modulation_cfg", ",", "dict", ")", "\n", "assert", "upsample_cfg", "is", "None", "or", "isinstance", "(", "upsample_cfg", ",", "dict", ")", "\n", "assert", "downsample_cfg", "is", "None", "or", "isinstance", "(", "downsample_cfg", ",", "dict", ")", "\n", "assert", "aux_head_cfg", "is", "None", "or", "isinstance", "(", "aux_head_cfg", ",", "dict", ")", "\n", "assert", "level_fusion_cfg", "is", "None", "or", "isinstance", "(", "level_fusion_cfg", ",", "dict", ")", "\n", "\n", "if", "flow_type", "not", "in", "[", "'cascade'", ",", "'parallel'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"flow type in TPN should be 'cascade' or 'parallel', \"", "\n", "f'but got {flow_type} instead.'", ")", "\n", "", "self", ".", "flow_type", "=", "flow_type", "\n", "\n", "self", ".", "temporal_modulation_ops", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "upsample_ops", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "downsample_ops", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "self", ".", "level_fusion_1", "=", "LevelFusion", "(", "**", "level_fusion_cfg", ")", "\n", "self", ".", "spatial_modulation", "=", "SpatialModulation", "(", "**", "spatial_modulation_cfg", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_tpn_stages", ")", ":", "\n", "\n", "            ", "if", "temporal_modulation_cfg", "is", "not", "None", ":", "\n", "                ", "downsample_scale", "=", "temporal_modulation_cfg", "[", "\n", "'downsample_scales'", "]", "[", "i", "]", "\n", "temporal_modulation", "=", "TemporalModulation", "(", "\n", "in_channels", "[", "-", "1", "]", ",", "out_channels", ",", "downsample_scale", ")", "\n", "self", ".", "temporal_modulation_ops", ".", "append", "(", "temporal_modulation", ")", "\n", "\n", "", "if", "i", "<", "self", ".", "num_tpn_stages", "-", "1", ":", "\n", "                ", "if", "upsample_cfg", "is", "not", "None", ":", "\n", "                    ", "upsample", "=", "nn", ".", "Upsample", "(", "**", "upsample_cfg", ")", "\n", "self", ".", "upsample_ops", ".", "append", "(", "upsample", ")", "\n", "\n", "", "if", "downsample_cfg", "is", "not", "None", ":", "\n", "                    ", "downsample", "=", "DownSample", "(", "out_channels", ",", "out_channels", ",", "\n", "**", "downsample_cfg", ")", "\n", "self", ".", "downsample_ops", ".", "append", "(", "downsample", ")", "\n", "\n", "", "", "", "out_dims", "=", "level_fusion_cfg", "[", "'out_channels'", "]", "\n", "\n", "# two pyramids", "\n", "self", ".", "level_fusion_2", "=", "LevelFusion", "(", "**", "level_fusion_cfg", ")", "\n", "\n", "self", ".", "pyramid_fusion", "=", "ConvModule", "(", "\n", "out_dims", "*", "2", ",", "\n", "2048", ",", "\n", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ")", "\n", "\n", "if", "aux_head_cfg", "is", "not", "None", ":", "\n", "            ", "self", ".", "aux_head", "=", "AuxHead", "(", "self", ".", "in_channels", "[", "-", "2", "]", ",", "**", "aux_head_cfg", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "aux_head", "=", "None", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.TPN.init_weights": [[401, 410], ["tpn.TPN.modules", "isinstance", "isinstance", "tpn.TPN.aux_head.init_weights", "mmcv.cnn.xavier_init", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                ", "xavier_init", "(", "m", ",", "distribution", "=", "'uniform'", ")", "\n", "", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm3d", ")", ":", "\n", "                ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n", "", "", "if", "self", ".", "aux_head", "is", "not", "None", ":", "\n", "            ", "self", ".", "aux_head", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.necks.tpn.TPN.forward": [[411, 450], ["dict", "tpn.TPN.spatial_modulation", "enumerate", "tpn.TPN.level_fusion_1", "tpn.TPN.level_fusion_2", "tpn.TPN.pyramid_fusion", "tpn.TPN.aux_head", "temporal_modulation_outs.append", "out.clone", "len", "range", "len", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "temporal_modulation", "out.clone"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "target", "=", "None", ")", ":", "\n", "        ", "loss_aux", "=", "dict", "(", ")", "\n", "\n", "# Auxiliary loss", "\n", "if", "self", ".", "aux_head", "is", "not", "None", ":", "\n", "            ", "loss_aux", "=", "self", ".", "aux_head", "(", "x", "[", "-", "2", "]", ",", "target", ")", "\n", "\n", "# Spatial Modulation", "\n", "", "spatial_modulation_outs", "=", "self", ".", "spatial_modulation", "(", "x", ")", "\n", "\n", "# Temporal Modulation", "\n", "temporal_modulation_outs", "=", "[", "]", "\n", "for", "i", ",", "temporal_modulation", "in", "enumerate", "(", "self", ".", "temporal_modulation_ops", ")", ":", "\n", "            ", "temporal_modulation_outs", ".", "append", "(", "\n", "temporal_modulation", "(", "spatial_modulation_outs", "[", "i", "]", ")", ")", "\n", "\n", "", "outs", "=", "[", "out", ".", "clone", "(", ")", "for", "out", "in", "temporal_modulation_outs", "]", "\n", "if", "len", "(", "self", ".", "upsample_ops", ")", "!=", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "num_tpn_stages", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "                ", "outs", "[", "i", "-", "1", "]", "=", "outs", "[", "i", "-", "1", "]", "+", "self", ".", "upsample_ops", "[", "i", "-", "1", "]", "(", "outs", "[", "i", "]", ")", "\n", "\n", "# Get top-down outs", "\n", "", "", "top_down_outs", "=", "self", ".", "level_fusion_1", "(", "outs", ")", "\n", "\n", "# Build bottom-up flow using downsample operation", "\n", "if", "self", ".", "flow_type", "==", "'parallel'", ":", "\n", "            ", "outs", "=", "[", "out", ".", "clone", "(", ")", "for", "out", "in", "temporal_modulation_outs", "]", "\n", "", "if", "len", "(", "self", ".", "downsample_ops", ")", "!=", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "num_tpn_stages", "-", "1", ")", ":", "\n", "                ", "outs", "[", "i", "+", "1", "]", "=", "outs", "[", "i", "+", "1", "]", "+", "self", ".", "downsample_ops", "[", "i", "]", "(", "outs", "[", "i", "]", ")", "\n", "\n", "# Get bottom-up outs", "\n", "", "", "botton_up_outs", "=", "self", ".", "level_fusion_2", "(", "outs", ")", "\n", "\n", "# fuse two pyramid outs", "\n", "outs", "=", "self", ".", "pyramid_fusion", "(", "\n", "torch", ".", "cat", "(", "[", "top_down_outs", ",", "botton_up_outs", "]", ",", "1", ")", ")", "\n", "\n", "return", "outs", ",", "loss_aux", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D.forward_train": [[13, 49], ["imgs.reshape.reshape.reshape", "dict", "recognizer2d.Recognizer2D.extract_feat", "recognizer2d.Recognizer2D.cls_head", "labels.squeeze", "recognizer2d.Recognizer2D.cls_head.loss", "dict.update", "x.squeeze.squeeze.reshape", "x.squeeze.squeeze.reshape", "recognizer2d.Recognizer2D.neck", "x.squeeze.squeeze.squeeze", "dict.update", "each.reshape().transpose().contiguous", "labels.squeeze", "len", "torch.nn.AdaptiveAvgPool2d", "each.reshape().transpose", "each.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["def", "forward_train", "(", "self", ",", "imgs", ",", "labels", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when training.\"\"\"", "\n", "\n", "assert", "self", ".", "with_cls_head", "\n", "batches", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "num_segs", "=", "imgs", ".", "shape", "[", "0", "]", "//", "batches", "\n", "\n", "losses", "=", "dict", "(", ")", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "\n", "if", "self", ".", "backbone_from", "in", "[", "'torchvision'", ",", "'timm'", "]", ":", "\n", "            ", "if", "len", "(", "x", ".", "shape", ")", "==", "4", "and", "(", "x", ".", "shape", "[", "2", "]", ">", "1", "or", "x", ".", "shape", "[", "3", "]", ">", "1", ")", ":", "\n", "# apply adaptive avg pooling", "\n", "                ", "x", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "(", "x", ")", "\n", "", "x", "=", "x", ".", "reshape", "(", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "x", ".", "shape", "+", "(", "1", ",", "1", ")", ")", "\n", "\n", "", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", "=", "[", "\n", "each", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "\n", "each", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "for", "each", "in", "x", "\n", "]", "\n", "x", ",", "loss_aux", "=", "self", ".", "neck", "(", "x", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "x", "=", "x", ".", "squeeze", "(", "2", ")", "\n", "num_segs", "=", "1", "\n", "losses", ".", "update", "(", "loss_aux", ")", "\n", "\n", "", "cls_score", "=", "self", ".", "cls_head", "(", "x", ",", "num_segs", ")", "\n", "gt_labels", "=", "labels", ".", "squeeze", "(", ")", "\n", "loss_cls", "=", "self", ".", "cls_head", ".", "loss", "(", "cls_score", ",", "gt_labels", ",", "**", "kwargs", ")", "\n", "losses", ".", "update", "(", "loss_cls", ")", "\n", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D._do_test": [[50, 102], ["imgs.reshape.reshape.reshape", "recognizer2d.Recognizer2D.extract_feat", "recognizer2d.Recognizer2D.cls_head", "recognizer2d.Recognizer2D.average_clip", "x.mean.mean.reshape", "x.mean.mean.reshape", "recognizer2d.Recognizer2D.neck", "x.mean.mean.squeeze", "torch.nn.AdaptiveAvgPool2d", "torch.nn.AdaptiveAvgPool2d.", "x.mean.mean.reshape", "x.mean.mean.mean", "each.reshape().transpose().contiguous", "len", "torch.nn.AdaptiveAvgPool2d", "recognizer2d.Recognizer2D.size", "recognizer2d.Recognizer2D.size", "each.reshape().transpose", "each.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.average_clip"], ["", "def", "_do_test", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation,\n        testing and gradcam.\"\"\"", "\n", "batches", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "num_segs", "=", "imgs", ".", "shape", "[", "0", "]", "//", "batches", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "\n", "if", "self", ".", "backbone_from", "in", "[", "'torchvision'", ",", "'timm'", "]", ":", "\n", "            ", "if", "len", "(", "x", ".", "shape", ")", "==", "4", "and", "(", "x", ".", "shape", "[", "2", "]", ">", "1", "or", "x", ".", "shape", "[", "3", "]", ">", "1", ")", ":", "\n", "# apply adaptive avg pooling", "\n", "                ", "x", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "(", "x", ")", "\n", "", "x", "=", "x", ".", "reshape", "(", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ")", "\n", "x", "=", "x", ".", "reshape", "(", "x", ".", "shape", "+", "(", "1", ",", "1", ")", ")", "\n", "\n", "", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", "=", "[", "\n", "each", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "\n", "each", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "for", "each", "in", "x", "\n", "]", "\n", "x", ",", "_", "=", "self", ".", "neck", "(", "x", ")", "\n", "x", "=", "x", ".", "squeeze", "(", "2", ")", "\n", "num_segs", "=", "1", "\n", "\n", "", "if", "self", ".", "feature_extraction", ":", "\n", "# perform spatial pooling", "\n", "            ", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "x", "=", "avg_pool", "(", "x", ")", "\n", "# squeeze dimensions", "\n", "x", "=", "x", ".", "reshape", "(", "(", "batches", ",", "num_segs", ",", "-", "1", ")", ")", "\n", "# temporal average pooling", "\n", "x", "=", "x", ".", "mean", "(", "axis", "=", "1", ")", "\n", "return", "x", "\n", "\n", "# When using `TSNHead` or `TPNHead`, shape is [batch_size, num_classes]", "\n", "# When using `TSMHead`, shape is [batch_size * num_crops, num_classes]", "\n", "# `num_crops` is calculated by:", "\n", "#   1) `twice_sample` in `SampleFrames`", "\n", "#   2) `num_sample_positions` in `DenseSampleFrames`", "\n", "#   3) `ThreeCrop/TenCrop` in `test_pipeline`", "\n", "#   4) `num_clips` in `SampleFrames` or its subclass if `clip_len != 1`", "\n", "\n", "# should have cls_head if not extracting features", "\n", "", "cls_score", "=", "self", ".", "cls_head", "(", "x", ",", "num_segs", ")", "\n", "\n", "assert", "cls_score", ".", "size", "(", ")", "[", "0", "]", "%", "batches", "==", "0", "\n", "# calculate num_crops automatically", "\n", "cls_score", "=", "self", ".", "average_clip", "(", "cls_score", ",", "\n", "cls_score", ".", "size", "(", ")", "[", "0", "]", "//", "batches", ")", "\n", "return", "cls_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D._do_fcn_test": [[103, 139], ["torch.flip.reshape", "recognizer2d.Recognizer2D.test_cfg.get", "recognizer2d.Recognizer2D.test_cfg.get", "recognizer2d.Recognizer2D.extract_feat", "recognizer2d.Recognizer2D.cls_head", "recognizer2d.Recognizer2D.average_clip", "torch.flip", "recognizer2d.Recognizer2D.neck", "x.reshape().transpose().contiguous.reshape().transpose().contiguous.reshape().transpose().contiguous", "each.reshape().transpose().contiguous", "x.reshape().transpose().contiguous.reshape().transpose().contiguous.reshape().transpose", "recognizer2d.Recognizer2D.size", "recognizer2d.Recognizer2D.size", "each.reshape().transpose", "x.reshape().transpose().contiguous.reshape().transpose().contiguous.reshape", "each.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.average_clip"], ["", "def", "_do_fcn_test", "(", "self", ",", "imgs", ")", ":", "\n", "# [N, num_crops * num_segs, C, H, W] ->", "\n", "# [N * num_crops * num_segs, C, H, W]", "\n", "        ", "batches", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "num_segs", "=", "self", ".", "test_cfg", ".", "get", "(", "'num_segs'", ",", "self", ".", "backbone", ".", "num_segments", ")", "\n", "\n", "if", "self", ".", "test_cfg", ".", "get", "(", "'flip'", ",", "False", ")", ":", "\n", "            ", "imgs", "=", "torch", ".", "flip", "(", "imgs", ",", "[", "-", "1", "]", ")", "\n", "", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "\n", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", "=", "[", "\n", "each", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "\n", "each", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "for", "each", "in", "x", "\n", "]", "\n", "x", ",", "_", "=", "self", ".", "neck", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "\n", "x", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "# When using `TSNHead` or `TPNHead`, shape is [batch_size, num_classes]", "\n", "# When using `TSMHead`, shape is [batch_size * num_crops, num_classes]", "\n", "# `num_crops` is calculated by:", "\n", "#   1) `twice_sample` in `SampleFrames`", "\n", "#   2) `num_sample_positions` in `DenseSampleFrames`", "\n", "#   3) `ThreeCrop/TenCrop` in `test_pipeline`", "\n", "#   4) `num_clips` in `SampleFrames` or its subclass if `clip_len != 1`", "\n", "", "cls_score", "=", "self", ".", "cls_head", "(", "x", ",", "fcn_test", "=", "True", ")", "\n", "\n", "assert", "cls_score", ".", "size", "(", ")", "[", "0", "]", "%", "batches", "==", "0", "\n", "# calculate num_crops automatically", "\n", "cls_score", "=", "self", ".", "average_clip", "(", "cls_score", ",", "\n", "cls_score", ".", "size", "(", ")", "[", "0", "]", "//", "batches", ")", "\n", "return", "cls_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D.forward_test": [[140, 149], ["recognizer2d.Recognizer2D.test_cfg.get", "recognizer2d.Recognizer2D._do_test().cpu().numpy", "recognizer2d.Recognizer2D._do_fcn_test().cpu().numpy", "recognizer2d.Recognizer2D._do_test().cpu", "recognizer2d.Recognizer2D._do_fcn_test().cpu", "recognizer2d.Recognizer2D._do_test", "recognizer2d.Recognizer2D._do_fcn_test"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D._do_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D._do_fcn_test"], ["", "def", "forward_test", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation and\n        testing.\"\"\"", "\n", "if", "self", ".", "test_cfg", ".", "get", "(", "'fcn_test'", ",", "False", ")", ":", "\n", "# If specified, spatially fully-convolutional testing is performed", "\n", "            ", "assert", "not", "self", ".", "feature_extraction", "\n", "assert", "self", ".", "with_cls_head", "\n", "return", "self", ".", "_do_fcn_test", "(", "imgs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "return", "self", ".", "_do_test", "(", "imgs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D.forward_dummy": [[150, 181], ["imgs.reshape.reshape.reshape", "recognizer2d.Recognizer2D.extract_feat", "recognizer2d.Recognizer2D.cls_head", "recognizer2d.Recognizer2D.neck", "x.squeeze.squeeze.squeeze", "torch.nn.functional.softmax", "each.reshape().transpose().contiguous", "each.reshape().transpose", "each.reshape"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward_dummy", "(", "self", ",", "imgs", ",", "softmax", "=", "False", ")", ":", "\n", "        ", "\"\"\"Used for computing network FLOPs.\n\n        See ``tools/analysis/get_flops.py``.\n\n        Args:\n            imgs (torch.Tensor): Input images.\n\n        Returns:\n            Tensor: Class score.\n        \"\"\"", "\n", "assert", "self", ".", "with_cls_head", "\n", "batches", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "num_segs", "=", "imgs", ".", "shape", "[", "0", "]", "//", "batches", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", "=", "[", "\n", "each", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "\n", "each", ".", "shape", "[", "1", ":", "]", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "for", "each", "in", "x", "\n", "]", "\n", "x", ",", "_", "=", "self", ".", "neck", "(", "x", ")", "\n", "x", "=", "x", ".", "squeeze", "(", "2", ")", "\n", "num_segs", "=", "1", "\n", "\n", "", "outs", "=", "self", ".", "cls_head", "(", "x", ",", "num_segs", ")", "\n", "if", "softmax", ":", "\n", "            ", "outs", "=", "nn", ".", "functional", ".", "softmax", "(", "outs", ")", "\n", "", "return", "(", "outs", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer2d.Recognizer2D.forward_gradcam": [[182, 187], ["recognizer2d.Recognizer2D._do_test"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D._do_test"], ["", "def", "forward_gradcam", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when using gradcam\n        utils.\"\"\"", "\n", "assert", "self", ".", "with_cls_head", "\n", "return", "self", ".", "_do_test", "(", "imgs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.__init__": [[33, 113], ["torch.Module.__init__", "backbone[].startswith", "base.BaseRecognizer.init_weights", "mmcls_builder.build_backbone", "backbone[].startswith", "builder.build_neck", "builder.build_head", "isinstance", "build_from_cfg", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Identity", "backbone[].startswith", "ImportError", "backbone.pop", "timm.create_model", "builder.build_backbone", "ImportError", "backbone.pop", "ImportError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_backbone", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_neck", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_head", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.factory.create_model", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_backbone"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "multi_class", "=", "False", ",", "\n", "num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n", "power", "=", "0", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ann_file", "=", "ann_file", "\n", "self", ".", "data_prefix", "=", "osp", ".", "realpath", "(", "\n", "data_prefix", ")", "if", "data_prefix", "is", "not", "None", "and", "osp", ".", "isdir", "(", "\n", "data_prefix", ")", "else", "data_prefix", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "multi_class", "=", "multi_class", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "start_index", "=", "start_index", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "sample_by_class", "=", "sample_by_class", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "dynamic_length", "=", "dynamic_length", "\n", "\n", "assert", "not", "(", "self", ".", "multi_class", "and", "self", ".", "sample_by_class", ")", "\n", "\n", "self", ".", "pipeline", "=", "Compose", "(", "pipeline", ")", "\n", "self", ".", "video_infos", "=", "self", ".", "load_annotations", "(", ")", "\n", "if", "self", ".", "sample_by_class", ":", "\n", "            ", "self", ".", "video_infos_by_class", "=", "self", ".", "parse_by_class", "(", ")", "\n", "\n", "class_prob", "=", "[", "]", "\n", "for", "_", ",", "samples", "in", "self", ".", "video_infos_by_class", ".", "items", "(", ")", ":", "\n", "                ", "class_prob", ".", "append", "(", "len", "(", "samples", ")", "/", "len", "(", "self", ".", "video_infos", ")", ")", "\n", "", "class_prob", "=", "[", "x", "**", "self", ".", "power", "for", "x", "in", "class_prob", "]", "\n", "\n", "summ", "=", "sum", "(", "class_prob", ")", "\n", "class_prob", "=", "[", "x", "/", "summ", "for", "x", "in", "class_prob", "]", "\n", "\n", "self", ".", "class_prob", "=", "dict", "(", "zip", "(", "self", ".", "video_infos_by_class", ",", "class_prob", ")", ")", "\n", "\n", "", "", "@", "abstractmethod", "\n", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n", "\n", "# json annotations already looks like video_infos, so for each dataset,", "\n", "# this func should be the same", "\n", "", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "path_key", "=", "'frame_dir'", "if", "'frame_dir'", "in", "video_infos", "[", "0", "]", "else", "'filename'", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.with_neck": [[114, 118], ["hasattr"], "methods", ["None"], ["for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.with_cls_head": [[119, 123], ["hasattr"], "methods", ["None"], ["if", "self", ".", "multi_class", ":", "\n", "                ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n", "", "else", ":", "\n", "                ", "assert", "len", "(", "video_infos", "[", "i", "]", "[", "'label'", "]", ")", "==", "1", "\n", "video_infos", "[", "i", "]", "[", "'label'", "]", "=", "video_infos", "[", "i", "]", "[", "'label'", "]", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.init_weights": [[124, 141], ["base.BaseRecognizer.backbone.init_weights", "base.BaseRecognizer.cls_head.init_weights", "base.BaseRecognizer.neck.init_weights", "warnings.warn", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "", "return", "video_infos", "\n", "\n", "", "def", "parse_by_class", "(", "self", ")", ":", "\n", "        ", "video_infos_by_class", "=", "defaultdict", "(", "list", ")", "\n", "for", "item", "in", "self", ".", "video_infos", ":", "\n", "            ", "label", "=", "item", "[", "'label'", "]", "\n", "video_infos_by_class", "[", "label", "]", ".", "append", "(", "item", ")", "\n", "", "return", "video_infos_by_class", "\n", "\n", "", "@", "staticmethod", "\n", "def", "label2array", "(", "num", ",", "label", ")", ":", "\n", "        ", "arr", "=", "np", ".", "zeros", "(", "num", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "arr", "[", "label", "]", "=", "1.", "\n", "return", "arr", "\n", "\n", "", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'top_k_accuracy'", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.extract_feat": [[142, 165], ["mmcv.runner.auto_fp16", "hasattr", "base.BaseRecognizer.backbone.features", "base.BaseRecognizer.backbone.forward_features", "base.BaseRecognizer.backbone", "isinstance", "base.BaseRecognizer.backbone", "len"], "methods", ["None"], ["metric_options", "=", "dict", "(", "top_k_accuracy", "=", "dict", "(", "topk", "=", "(", "1", ",", "5", ")", ")", ")", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        ", "\"\"\"Perform evaluation for common datasets.\n\n        Args:\n            results (list): Output results.\n            metrics (str | sequence[str]): Metrics to be performed.\n                Defaults: 'top_k_accuracy'.\n            metric_options (dict): Dict for metric options. Options are\n                ``topk`` for ``top_k_accuracy``.\n                Default: ``dict(top_k_accuracy=dict(topk=(1, 5)))``.\n            logger (logging.Logger | None): Logger for recording.\n                Default: None.\n            deprecated_kwargs (dict): Used for containing deprecated arguments.\n                See 'https://github.com/open-mmlab/mmaction2/pull/286'.\n\n        Returns:\n            dict: Evaluation results dict.\n        \"\"\"", "\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.average_clip": [[166, 201], ["cls_score.mean.mean.view", "base.BaseRecognizer.test_cfg.keys", "KeyError", "ValueError", "torch.softmax().mean", "torch.softmax().mean", "torch.softmax().mean", "torch.softmax().mean", "cls_score.mean.mean.mean", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n", "\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'top_k_accuracy'", "]", "=", "dict", "(", "\n", "metric_options", "[", "'top_k_accuracy'", "]", ",", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "allowed_metrics", "=", "[", "\n", "'top_k_accuracy'", ",", "'mean_class_accuracy'", ",", "'mean_average_precision'", ",", "\n", "'mmit_mean_average_precision'", "\n", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "allowed_metrics", ":", "\n", "                ", "raise", "KeyError", "(", "f'metric {metric} is not supported'", ")", "\n", "\n", "", "", "eval_results", "=", "OrderedDict", "(", ")", "\n", "gt_labels", "=", "[", "ann", "[", "'label'", "]", "for", "ann", "in", "self", ".", "video_infos", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "msg", "=", "f'Evaluating {metric} ...'", "\n", "if", "logger", "is", "None", ":", "\n", "                ", "msg", "=", "'\\n'", "+", "msg", "\n", "", "print_log", "(", "msg", ",", "logger", "=", "logger", ")", "\n", "\n", "if", "metric", "==", "'top_k_accuracy'", ":", "\n", "                ", "topk", "=", "metric_options", ".", "setdefault", "(", "'top_k_accuracy'", ",", "\n", "{", "}", ")", ".", "setdefault", "(", "\n", "'topk'", ",", "(", "1", ",", "5", ")", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.forward_train": [[202, 205], ["None"], "methods", ["None"], ["if", "not", "isinstance", "(", "topk", ",", "(", "int", ",", "tuple", ")", ")", ":", "\n", "                    ", "raise", "TypeError", "(", "'topk must be int or tuple of int, '", "\n", "f'but got {type(topk)}'", ")", "\n", "", "if", "isinstance", "(", "topk", ",", "int", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.forward_test": [[206, 210], ["None"], "methods", ["None"], ["                    ", "topk", "=", "(", "topk", ",", ")", "\n", "\n", "", "top_k_acc", "=", "top_k_accuracy", "(", "results", ",", "gt_labels", ",", "topk", ")", "\n", "log_msg", "=", "[", "]", "\n", "for", "k", ",", "acc", "in", "zip", "(", "topk", ",", "top_k_acc", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.forward_gradcam": [[211, 215], ["None"], "methods", ["None"], ["                    ", "eval_results", "[", "f'top{k}_acc'", "]", "=", "acc", "\n", "log_msg", ".", "append", "(", "f'\\ntop{k}_acc\\t{acc:.4f}'", ")", "\n", "", "log_msg", "=", "''", ".", "join", "(", "log_msg", ")", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer._parse_losses": [[216, 251], ["collections.OrderedDict", "losses.items", "sum", "collections.OrderedDict.items", "isinstance", "loss_value.data.clone.data.clone.item", "loss_value.data.clone.data.clone.mean", "isinstance", "torch.is_available", "torch.is_available", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.is_initialized", "torch.is_initialized", "loss_value.data.clone.data.clone.data.clone", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "sum", "TypeError", "collections.OrderedDict.items", "loss_value.data.clone.data.clone.div_", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "_loss.mean"], "methods", ["None"], ["\n", "", "if", "metric", "==", "'mean_class_accuracy'", ":", "\n", "                ", "mean_acc", "=", "mean_class_accuracy", "(", "results", ",", "gt_labels", ")", "\n", "eval_results", "[", "'mean_class_accuracy'", "]", "=", "mean_acc", "\n", "log_msg", "=", "f'\\nmean_acc\\t{mean_acc:.4f}'", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "if", "metric", "in", "[", "\n", "'mean_average_precision'", ",", "'mmit_mean_average_precision'", "\n", "]", ":", "\n", "                ", "gt_labels_arrays", "=", "[", "\n", "self", ".", "label2array", "(", "self", ".", "num_classes", ",", "label", ")", "\n", "for", "label", "in", "gt_labels", "\n", "]", "\n", "if", "metric", "==", "'mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mean_average_precision", "(", "results", ",", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmean_average_precision\\t{mAP:.4f}'", "\n", "", "elif", "metric", "==", "'mmit_mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mmit_mean_average_precision", "(", "results", ",", "\n", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mmit_mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmmit_mean_average_precision\\t{mAP:.4f}'", "\n", "", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "", "return", "eval_results", "\n", "\n", "", "@", "staticmethod", "\n", "def", "dump_results", "(", "results", ",", "out", ")", ":", "\n", "        ", "\"\"\"Dump data to json/yaml/pickle strings or files.\"\"\"", "\n", "return", "mmcv", ".", "dump", "(", "results", ",", "out", ")", "\n", "\n", "", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.forward": [[252, 266], ["kwargs.get", "base.BaseRecognizer.forward_gradcam", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "base.BaseRecognizer.forward_test", "ValueError", "base.BaseRecognizer.blending", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "base.BaseRecognizer.forward_train"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.forward_gradcam", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "# If HVU, type(results['label']) is dict", "\n", "if", "self", ".", "multi_class", "and", "isinstance", "(", "results", "[", "'label'", "]", ",", "list", ")", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n", "", "def", "prepare_test_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for testing given the index.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.train_step": [[267, 312], ["base.BaseRecognizer.", "base.BaseRecognizer._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n", "results", "[", "'start_index'", "]", "=", "self", ".", "start_index", "\n", "\n", "# prepare tensor in getitem", "\n", "# If HVU, type(results['label']) is dict", "\n", "if", "self", ".", "multi_class", "and", "isinstance", "(", "results", "[", "'label'", "]", ",", "list", ")", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the size of the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "video_infos", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Get the sample for either training or testing given index.\"\"\"", "\n", "if", "self", ".", "test_mode", ":", "\n", "            ", "return", "self", ".", "prepare_test_frames", "(", "idx", ")", "\n", "\n", "", "return", "self", ".", "prepare_train_frames", "(", "idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.val_step": [[313, 338], ["base.BaseRecognizer.", "base.BaseRecognizer._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D.forward_train": [[13, 35], ["dict", "recognizer3d.Recognizer3D.extract_feat", "recognizer3d.Recognizer3D.cls_head", "labels.squeeze", "recognizer3d.Recognizer3D.cls_head.loss", "dict.update", "imgs.squeeze.squeeze.reshape", "imgs.squeeze.squeeze.permute", "imgs.squeeze.squeeze.squeeze", "recognizer3d.Recognizer3D.neck", "dict.update", "labels.squeeze"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["def", "forward_train", "(", "self", ",", "imgs", ",", "labels", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when training.\"\"\"", "\n", "\n", "assert", "self", ".", "with_cls_head", "\n", "if", "imgs", ".", "shape", "[", "3", "]", "!=", "1", ":", "\n", "            ", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "imgs", "=", "imgs", ".", "permute", "(", "0", ",", "3", ",", "2", ",", "1", ",", "4", ",", "5", ")", "\n", "imgs", "=", "imgs", ".", "squeeze", "(", ")", "\n", "", "losses", "=", "dict", "(", ")", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", ",", "loss_aux", "=", "self", ".", "neck", "(", "x", ",", "labels", ".", "squeeze", "(", ")", ")", "\n", "losses", ".", "update", "(", "loss_aux", ")", "\n", "\n", "", "cls_score", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "gt_labels", "=", "labels", ".", "squeeze", "(", ")", "\n", "loss_cls", "=", "self", ".", "cls_head", ".", "loss", "(", "cls_score", ",", "gt_labels", ",", "**", "kwargs", ")", "\n", "losses", ".", "update", "(", "loss_cls", ")", "\n", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D._do_test": [[36, 111], ["recognizer3d.Recognizer3D.cls_head", "recognizer3d.Recognizer3D.average_clip", "imgs.view.view.reshape", "imgs.view.view.permute", "imgs.view.view.squeeze", "isinstance", "recognizer3d.Recognizer3D.extract_feat", "imgs.view.view.view", "imgs.view.view.view", "recognizer3d.Recognizer3D.extract_feat", "feats.append", "len", "tuple", "torch.cat", "recognizer3d.Recognizer3D.neck", "isinstance", "len", "len", "torch.nn.AdaptiveAvgPool3d", "isinstance", "nn.AdaptiveAvgPool3d.reshape", "nn.AdaptiveAvgPool3d.mean", "recognizer3d.Recognizer3D.neck", "torch.cat", "feat[].size", "nn.AdaptiveAvgPool3d.size", "torch.cat", "torch.nn.AdaptiveAvgPool3d.", "range", "torch.nn.AdaptiveAvgPool3d."], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.average_clip", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat"], ["", "def", "_do_test", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation,\n        testing and gradcam.\"\"\"", "\n", "batches", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "num_segs", "=", "imgs", ".", "shape", "[", "1", "]", "\n", "\n", "if", "imgs", ".", "shape", "[", "3", "]", "!=", "1", ":", "\n", "            ", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "num_segs", "%", "3", "==", "0", ":", "\n", "                ", "imgs", "=", "imgs", ".", "view", "(", "(", "batches", ",", "3", ",", "num_segs", "//", "3", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "imgs", "=", "imgs", ".", "view", "(", "(", "batches", "*", "3", ",", "num_segs", "//", "3", ")", "+", "imgs", ".", "shape", "[", "3", ":", "]", ")", "\n", "num_segs", "=", "3", "\n", "", "else", ":", "\n", "                ", "num_segs", "=", "1", "\n", "\n", "", "imgs", "=", "imgs", ".", "permute", "(", "0", ",", "3", ",", "2", ",", "1", ",", "4", ",", "5", ")", "\n", "imgs", "=", "imgs", ".", "squeeze", "(", ")", "\n", "\n", "", "if", "self", ".", "max_testing_views", "is", "not", "None", ":", "\n", "            ", "total_views", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "assert", "num_segs", "==", "total_views", ",", "(", "\n", "'max_testing_views is only compatible '", "\n", "'with batch_size == 1'", ")", "\n", "view_ptr", "=", "0", "\n", "feats", "=", "[", "]", "\n", "while", "view_ptr", "<", "total_views", ":", "\n", "                ", "batch_imgs", "=", "imgs", "[", "view_ptr", ":", "view_ptr", "+", "self", ".", "max_testing_views", "]", "\n", "x", "=", "self", ".", "extract_feat", "(", "batch_imgs", ")", "\n", "if", "self", ".", "with_neck", ":", "\n", "                    ", "x", ",", "_", "=", "self", ".", "neck", "(", "x", ")", "\n", "", "feats", ".", "append", "(", "x", ")", "\n", "view_ptr", "+=", "self", ".", "max_testing_views", "\n", "# should consider the case that feat is a tuple", "\n", "", "if", "isinstance", "(", "feats", "[", "0", "]", ",", "tuple", ")", ":", "\n", "                ", "len_tuple", "=", "len", "(", "feats", "[", "0", "]", ")", "\n", "feat", "=", "[", "\n", "torch", ".", "cat", "(", "[", "x", "[", "i", "]", "for", "x", "in", "feats", "]", ")", "for", "i", "in", "range", "(", "len_tuple", ")", "\n", "]", "\n", "feat", "=", "tuple", "(", "feat", ")", "\n", "", "else", ":", "\n", "                ", "feat", "=", "torch", ".", "cat", "(", "feats", ")", "\n", "", "", "else", ":", "\n", "            ", "feat", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "if", "self", ".", "with_neck", ":", "\n", "                ", "feat", ",", "_", "=", "self", ".", "neck", "(", "feat", ")", "\n", "\n", "", "", "if", "self", ".", "feature_extraction", ":", "\n", "            ", "feat_dim", "=", "len", "(", "feat", "[", "0", "]", ".", "size", "(", ")", ")", "if", "isinstance", "(", "feat", ",", "tuple", ")", "else", "len", "(", "\n", "feat", ".", "size", "(", ")", ")", "\n", "assert", "feat_dim", "in", "[", "\n", "5", ",", "2", "\n", "]", ",", "(", "'Got feature of unknown architecture, '", "\n", "'only 3D-CNN-like ([N, in_channels, T, H, W]), and '", "\n", "'transformer-like ([N, in_channels]) features are supported.'", ")", "\n", "if", "feat_dim", "==", "5", ":", "# 3D-CNN architecture", "\n", "# perform spatio-temporal pooling", "\n", "                ", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "if", "isinstance", "(", "feat", ",", "tuple", ")", ":", "\n", "                    ", "feat", "=", "[", "avg_pool", "(", "x", ")", "for", "x", "in", "feat", "]", "\n", "# concat them", "\n", "feat", "=", "torch", ".", "cat", "(", "feat", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "                    ", "feat", "=", "avg_pool", "(", "feat", ")", "\n", "# squeeze dimensions", "\n", "", "feat", "=", "feat", ".", "reshape", "(", "(", "batches", ",", "num_segs", ",", "-", "1", ")", ")", "\n", "# temporal average pooling", "\n", "feat", "=", "feat", ".", "mean", "(", "axis", "=", "1", ")", "\n", "", "return", "feat", "\n", "\n", "# should have cls_head if not extracting features", "\n", "", "assert", "self", ".", "with_cls_head", "\n", "cls_score", "=", "self", ".", "cls_head", "(", "feat", ")", "\n", "cls_score", "=", "self", ".", "average_clip", "(", "cls_score", ",", "num_segs", ")", "\n", "return", "cls_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D.forward_test": [[112, 116], ["recognizer3d.Recognizer3D._do_test().cpu().numpy", "recognizer3d.Recognizer3D._do_test().cpu", "recognizer3d.Recognizer3D._do_test"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D._do_test"], ["", "def", "forward_test", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation and\n        testing.\"\"\"", "\n", "return", "self", ".", "_do_test", "(", "imgs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D.forward_dummy": [[117, 139], ["imgs.reshape.reshape.reshape", "recognizer3d.Recognizer3D.extract_feat", "recognizer3d.Recognizer3D.cls_head", "recognizer3d.Recognizer3D.neck", "torch.nn.functional.softmax"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "def", "forward_dummy", "(", "self", ",", "imgs", ",", "softmax", "=", "False", ")", ":", "\n", "        ", "\"\"\"Used for computing network FLOPs.\n\n        See ``tools/analysis/get_flops.py``.\n\n        Args:\n            imgs (torch.Tensor): Input images.\n\n        Returns:\n            Tensor: Class score.\n        \"\"\"", "\n", "assert", "self", ".", "with_cls_head", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "\n", "if", "self", ".", "with_neck", ":", "\n", "            ", "x", ",", "_", "=", "self", ".", "neck", "(", "x", ")", "\n", "\n", "", "outs", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "if", "softmax", ":", "\n", "            ", "outs", "=", "nn", ".", "functional", ".", "softmax", "(", "outs", ")", "\n", "", "return", "(", "outs", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D.forward_gradcam": [[140, 145], ["recognizer3d.Recognizer3D._do_test"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.recognizer3d.Recognizer3D._do_test"], ["", "def", "forward_gradcam", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when using gradcam\n        utils.\"\"\"", "\n", "assert", "self", ".", "with_cls_head", "\n", "return", "self", ".", "_do_test", "(", "imgs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.forward": [[10, 18], ["audio_recognizer.AudioRecognizer.forward_test", "audio_recognizer.AudioRecognizer.forward_train", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["def", "forward", "(", "self", ",", "audios", ",", "label", "=", "None", ",", "return_loss", "=", "True", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\"\"\"", "\n", "if", "return_loss", ":", "\n", "            ", "if", "label", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Label should not be None.'", ")", "\n", "", "return", "self", ".", "forward_train", "(", "audios", ",", "label", ")", "\n", "\n", "", "return", "self", ".", "forward_test", "(", "audios", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.forward_train": [[19, 28], ["audios.reshape.reshape.reshape", "audio_recognizer.AudioRecognizer.extract_feat", "audio_recognizer.AudioRecognizer.cls_head", "labels.squeeze", "audio_recognizer.AudioRecognizer.cls_head.loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.loss"], ["", "def", "forward_train", "(", "self", ",", "audios", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when training.\"\"\"", "\n", "audios", "=", "audios", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "audios", ".", "shape", "[", "2", ":", "]", ")", "\n", "x", "=", "self", ".", "extract_feat", "(", "audios", ")", "\n", "cls_score", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "gt_labels", "=", "labels", ".", "squeeze", "(", ")", "\n", "loss", "=", "self", ".", "cls_head", ".", "loss", "(", "cls_score", ",", "gt_labels", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.forward_test": [[29, 39], ["audios.reshape.reshape.reshape", "audio_recognizer.AudioRecognizer.extract_feat", "audio_recognizer.AudioRecognizer.cls_head", "audio_recognizer.AudioRecognizer.average_clip", "audio_recognizer.AudioRecognizer.cpu().numpy", "audio_recognizer.AudioRecognizer.cpu"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.base.BaseRecognizer.average_clip"], ["", "def", "forward_test", "(", "self", ",", "audios", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation and\n        testing.\"\"\"", "\n", "num_segs", "=", "audios", ".", "shape", "[", "1", "]", "\n", "audios", "=", "audios", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "audios", ".", "shape", "[", "2", ":", "]", ")", "\n", "x", "=", "self", ".", "extract_feat", "(", "audios", ")", "\n", "cls_score", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "cls_score", "=", "self", ".", "average_clip", "(", "cls_score", ",", "num_segs", ")", "\n", "\n", "return", "cls_score", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.forward_gradcam": [[40, 42], ["None"], "methods", ["None"], ["", "def", "forward_gradcam", "(", "self", ",", "audios", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.train_step": [[43, 82], ["audio_recognizer.AudioRecognizer.", "audio_recognizer.AudioRecognizer._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["", "def", "train_step", "(", "self", ",", "data_batch", ",", "optimizer", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"The iteration step during training.\n\n        This method defines an iteration step during training, except for the\n        back propagation and optimizer updating, which are done in an optimizer\n        hook. Note that in some complicated cases or models, the whole process\n        including back propagation and optimizer updating is also defined in\n        this method, such as GAN.\n\n        Args:\n            data_batch (dict): The output of dataloader.\n            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n                runner is passed to ``train_step()``. This argument is unused\n                and reserved.\n\n        Returns:\n            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,\n                ``num_samples``.\n                ``loss`` is a tensor for back propagation, which can be a\n                weighted sum of multiple losses.\n                ``log_vars`` contains all the variables to be sent to the\n                logger.\n                ``num_samples`` indicates the batch size (when the model is\n                DDP, it means the batch size on each GPU), which is used for\n                averaging the logs.\n        \"\"\"", "\n", "audios", "=", "data_batch", "[", "'audios'", "]", "\n", "label", "=", "data_batch", "[", "'label'", "]", "\n", "\n", "losses", "=", "self", "(", "audios", ",", "label", ")", "\n", "\n", "loss", ",", "log_vars", "=", "self", ".", "_parse_losses", "(", "losses", ")", "\n", "\n", "outputs", "=", "dict", "(", "\n", "loss", "=", "loss", ",", "\n", "log_vars", "=", "log_vars", ",", "\n", "num_samples", "=", "len", "(", "next", "(", "iter", "(", "data_batch", ".", "values", "(", ")", ")", ")", ")", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.recognizers.audio_recognizer.AudioRecognizer.val_step": [[83, 103], ["audio_recognizer.AudioRecognizer.", "audio_recognizer.AudioRecognizer._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["", "def", "val_step", "(", "self", ",", "data_batch", ",", "optimizer", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"The iteration step during validation.\n\n        This method shares the same signature as :func:`train_step`, but used\n        during val epochs. Note that the evaluation after training epochs is\n        not implemented with this method, but an evaluation hook.\n        \"\"\"", "\n", "audios", "=", "data_batch", "[", "'audios'", "]", "\n", "label", "=", "data_batch", "[", "'label'", "]", "\n", "\n", "losses", "=", "self", "(", "audios", ",", "label", ")", "\n", "\n", "loss", ",", "log_vars", "=", "self", ".", "_parse_losses", "(", "losses", ")", "\n", "\n", "outputs", "=", "dict", "(", "\n", "loss", "=", "loss", ",", "\n", "log_vars", "=", "log_vars", ",", "\n", "num_samples", "=", "len", "(", "next", "(", "iter", "(", "data_batch", ".", "values", "(", ")", ")", ")", ")", ")", "\n", "\n", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.__init__": [[28, 39], ["torch.Module.__init__", "builder.build_backbone", "base.BaseGCN.init_weights", "builder.build_head"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_backbone", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_head"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.with_cls_head": [[40, 44], ["hasattr"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.init_weights": [[45, 55], ["base.BaseGCN.backbone.init_weights", "NotImplementedError", "base.BaseGCN.cls_head.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.forward_train": [[56, 59], ["None"], "methods", ["None"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.forward_test": [[60, 63], ["None"], "methods", ["None"], ["ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN._parse_losses": [[64, 99], ["collections.OrderedDict", "losses.items", "sum", "collections.OrderedDict.items", "isinstance", "loss_value.data.clone.data.clone.item", "loss_value.data.clone.data.clone.mean", "isinstance", "torch.is_available", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.is_initialized", "loss_value.data.clone.data.clone.data.clone", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "sum", "TypeError", "collections.OrderedDict.items", "loss_value.data.clone.data.clone.div_", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "_loss.mean"], "methods", ["None"], ["multi_class", "=", "False", ",", "\n", "num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n", "power", "=", "0", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ann_file", "=", "ann_file", "\n", "self", ".", "data_prefix", "=", "osp", ".", "realpath", "(", "\n", "data_prefix", ")", "if", "data_prefix", "is", "not", "None", "and", "osp", ".", "isdir", "(", "\n", "data_prefix", ")", "else", "data_prefix", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "multi_class", "=", "multi_class", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "start_index", "=", "start_index", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "sample_by_class", "=", "sample_by_class", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "dynamic_length", "=", "dynamic_length", "\n", "\n", "assert", "not", "(", "self", ".", "multi_class", "and", "self", ".", "sample_by_class", ")", "\n", "\n", "self", ".", "pipeline", "=", "Compose", "(", "pipeline", ")", "\n", "self", ".", "video_infos", "=", "self", ".", "load_annotations", "(", ")", "\n", "if", "self", ".", "sample_by_class", ":", "\n", "            ", "self", ".", "video_infos_by_class", "=", "self", ".", "parse_by_class", "(", ")", "\n", "\n", "class_prob", "=", "[", "]", "\n", "for", "_", ",", "samples", "in", "self", ".", "video_infos_by_class", ".", "items", "(", ")", ":", "\n", "                ", "class_prob", ".", "append", "(", "len", "(", "samples", ")", "/", "len", "(", "self", ".", "video_infos", ")", ")", "\n", "", "class_prob", "=", "[", "x", "**", "self", ".", "power", "for", "x", "in", "class_prob", "]", "\n", "\n", "summ", "=", "sum", "(", "class_prob", ")", "\n", "class_prob", "=", "[", "x", "/", "summ", "for", "x", "in", "class_prob", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.forward": [[100, 108], ["base.BaseGCN.forward_test", "base.BaseGCN.forward_train", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["\n", "self", ".", "class_prob", "=", "dict", "(", "zip", "(", "self", ".", "video_infos_by_class", ",", "class_prob", ")", ")", "\n", "\n", "", "", "@", "abstractmethod", "\n", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n", "\n", "# json annotations already looks like video_infos, so for each dataset,", "\n", "# this func should be the same", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.extract_feat": [[109, 120], ["base.BaseGCN.backbone"], "methods", ["None"], ["", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "path_key", "=", "'frame_dir'", "if", "'frame_dir'", "in", "video_infos", "[", "0", "]", "else", "'filename'", "\n", "for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n", "if", "self", ".", "multi_class", ":", "\n", "                ", "assert", "self", ".", "num_classes", "is", "not", "None", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.train_step": [[121, 158], ["label.squeeze.squeeze.squeeze", "base.BaseGCN.", "base.BaseGCN._parse_losses", "dict", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["", "else", ":", "\n", "                ", "assert", "len", "(", "video_infos", "[", "i", "]", "[", "'label'", "]", ")", "==", "1", "\n", "video_infos", "[", "i", "]", "[", "'label'", "]", "=", "video_infos", "[", "i", "]", "[", "'label'", "]", "[", "0", "]", "\n", "", "", "return", "video_infos", "\n", "\n", "", "def", "parse_by_class", "(", "self", ")", ":", "\n", "        ", "video_infos_by_class", "=", "defaultdict", "(", "list", ")", "\n", "for", "item", "in", "self", ".", "video_infos", ":", "\n", "            ", "label", "=", "item", "[", "'label'", "]", "\n", "video_infos_by_class", "[", "label", "]", ".", "append", "(", "item", ")", "\n", "", "return", "video_infos_by_class", "\n", "\n", "", "@", "staticmethod", "\n", "def", "label2array", "(", "num", ",", "label", ")", ":", "\n", "        ", "arr", "=", "np", ".", "zeros", "(", "num", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "arr", "[", "label", "]", "=", "1.", "\n", "return", "arr", "\n", "\n", "", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'top_k_accuracy'", ",", "\n", "metric_options", "=", "dict", "(", "top_k_accuracy", "=", "dict", "(", "topk", "=", "(", "1", ",", "5", ")", ")", ")", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.base.BaseGCN.val_step": [[159, 176], ["base.BaseGCN.", "base.BaseGCN._parse_losses", "dict", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n", "\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'top_k_accuracy'", "]", "=", "dict", "(", "\n", "metric_options", "[", "'top_k_accuracy'", "]", ",", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.skeletongcn.SkeletonGCN.forward_train": [[9, 21], ["dict", "skeletongcn.SkeletonGCN.extract_feat", "skeletongcn.SkeletonGCN.cls_head", "labels.squeeze", "skeletongcn.SkeletonGCN.cls_head.loss", "dict.update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["def", "forward_train", "(", "self", ",", "skeletons", ",", "labels", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when training.\"\"\"", "\n", "assert", "self", ".", "with_cls_head", "\n", "losses", "=", "dict", "(", ")", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "skeletons", ")", "\n", "output", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "gt_labels", "=", "labels", ".", "squeeze", "(", "-", "1", ")", "\n", "loss", "=", "self", ".", "cls_head", ".", "loss", "(", "output", ",", "gt_labels", ")", "\n", "losses", ".", "update", "(", "loss", ")", "\n", "\n", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.skeleton_gcn.skeletongcn.SkeletonGCN.forward_test": [[22, 30], ["skeletongcn.SkeletonGCN.extract_feat", "skeletongcn.SkeletonGCN.cls_head", "skeletongcn.SkeletonGCN.data.cpu().numpy", "skeletongcn.SkeletonGCN.data.cpu"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat"], ["", "def", "forward_test", "(", "self", ",", "skeletons", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call when evaluation and\n        testing.\"\"\"", "\n", "x", "=", "self", ".", "extract_feat", "(", "skeletons", ")", "\n", "assert", "self", ".", "with_cls_head", "\n", "output", "=", "self", ".", "cls_head", "(", "x", ")", "\n", "\n", "return", "output", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator.forward_train": [[21, 24], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator.forward_test": [[25, 28], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator.forward": [[29, 32], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator._parse_losses": [[33, 68], ["collections.OrderedDict", "losses.items", "sum", "collections.OrderedDict.items", "isinstance", "loss_value.data.clone.data.clone.item", "loss_value.data.clone.data.clone.mean", "isinstance", "torch.is_available", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.is_initialized", "loss_value.data.clone.data.clone.data.clone", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "sum", "TypeError", "collections.OrderedDict.items", "loss_value.data.clone.data.clone.div_", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "_loss.mean"], "methods", ["None"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n", "ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "multi_class", "=", "False", ",", "\n", "num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator.train_step": [[69, 105], ["base.BaseTAPGenerator.forward", "base.BaseTAPGenerator._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["power", "=", "0", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ann_file", "=", "ann_file", "\n", "self", ".", "data_prefix", "=", "osp", ".", "realpath", "(", "\n", "data_prefix", ")", "if", "data_prefix", "is", "not", "None", "and", "osp", ".", "isdir", "(", "\n", "data_prefix", ")", "else", "data_prefix", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "multi_class", "=", "multi_class", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "start_index", "=", "start_index", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "sample_by_class", "=", "sample_by_class", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "dynamic_length", "=", "dynamic_length", "\n", "\n", "assert", "not", "(", "self", ".", "multi_class", "and", "self", ".", "sample_by_class", ")", "\n", "\n", "self", ".", "pipeline", "=", "Compose", "(", "pipeline", ")", "\n", "self", ".", "video_infos", "=", "self", ".", "load_annotations", "(", ")", "\n", "if", "self", ".", "sample_by_class", ":", "\n", "            ", "self", ".", "video_infos_by_class", "=", "self", ".", "parse_by_class", "(", ")", "\n", "\n", "class_prob", "=", "[", "]", "\n", "for", "_", ",", "samples", "in", "self", ".", "video_infos_by_class", ".", "items", "(", ")", ":", "\n", "                ", "class_prob", ".", "append", "(", "len", "(", "samples", ")", "/", "len", "(", "self", ".", "video_infos", ")", ")", "\n", "", "class_prob", "=", "[", "x", "**", "self", ".", "power", "for", "x", "in", "class_prob", "]", "\n", "\n", "summ", "=", "sum", "(", "class_prob", ")", "\n", "class_prob", "=", "[", "x", "/", "summ", "for", "x", "in", "class_prob", "]", "\n", "\n", "self", ".", "class_prob", "=", "dict", "(", "zip", "(", "self", ".", "video_infos_by_class", ",", "class_prob", ")", ")", "\n", "\n", "", "", "@", "abstractmethod", "\n", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAPGenerator.val_step": [[106, 118], ["base.BaseTAPGenerator.forward", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward"], ["\n", "# json annotations already looks like video_infos, so for each dataset,", "\n", "# this func should be the same", "\n", "", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n", "video_infos", "=", "mmcv", ".", "load", "(", "self", ".", "ann_file", ")", "\n", "num_videos", "=", "len", "(", "video_infos", ")", "\n", "path_key", "=", "'frame_dir'", "if", "'frame_dir'", "in", "video_infos", "[", "0", "]", "else", "'filename'", "\n", "for", "i", "in", "range", "(", "num_videos", ")", ":", "\n", "            ", "path_value", "=", "video_infos", "[", "i", "]", "[", "path_key", "]", "\n", "if", "self", ".", "data_prefix", "is", "not", "None", ":", "\n", "                ", "path_value", "=", "osp", ".", "join", "(", "self", ".", "data_prefix", ",", "path_value", ")", "\n", "", "video_infos", "[", "i", "]", "[", "path_key", "]", "=", "path_value", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.__init__": [[128, 136], ["torch.Module.__init__", "builder.build_backbone", "builder.build_head", "base.BaseTAGClassifier.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_backbone", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_head", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["for", "item", "in", "self", ".", "video_infos", ":", "\n", "            ", "label", "=", "item", "[", "'label'", "]", "\n", "video_infos_by_class", "[", "label", "]", ".", "append", "(", "item", ")", "\n", "", "return", "video_infos_by_class", "\n", "\n", "", "@", "staticmethod", "\n", "def", "label2array", "(", "num", ",", "label", ")", ":", "\n", "        ", "arr", "=", "np", ".", "zeros", "(", "num", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "arr", "[", "label", "]", "=", "1.", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.init_weights": [[137, 141], ["base.BaseTAGClassifier.backbone.init_weights", "base.BaseTAGClassifier.cls_head.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["return", "arr", "\n", "\n", "", "def", "evaluate", "(", "self", ",", "\n", "results", ",", "\n", "metrics", "=", "'top_k_accuracy'", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat": [[142, 152], ["base.BaseTAGClassifier.backbone"], "methods", ["None"], ["metric_options", "=", "dict", "(", "top_k_accuracy", "=", "dict", "(", "topk", "=", "(", "1", ",", "5", ")", ")", ")", ",", "\n", "logger", "=", "None", ",", "\n", "**", "deprecated_kwargs", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.forward_train": [[153, 156], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.forward_test": [[157, 160], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.forward": [[161, 167], ["base.BaseTAGClassifier.forward_test", "base.BaseTAGClassifier.forward_train"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["\n", "# Protect ``metric_options`` since it uses mutable value as default", "\n", "metric_options", "=", "copy", ".", "deepcopy", "(", "metric_options", ")", "\n", "\n", "if", "deprecated_kwargs", "!=", "{", "}", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'Option arguments for metrics has been changed to '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses": [[168, 203], ["collections.OrderedDict", "losses.items", "sum", "collections.OrderedDict.items", "isinstance", "loss_value.data.clone.data.clone.item", "loss_value.data.clone.data.clone.mean", "isinstance", "torch.is_available", "torch.is_available", "torch.is_available", "torch.is_initialized", "torch.is_initialized", "torch.is_initialized", "loss_value.data.clone.data.clone.data.clone", "torch.all_reduce", "torch.all_reduce", "torch.all_reduce", "sum", "TypeError", "collections.OrderedDict.items", "loss_value.data.clone.data.clone.div_", "torch.get_world_size", "torch.get_world_size", "torch.get_world_size", "_loss.mean"], "methods", ["None"], ["\"`metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' \"", "# noqa: E501", "\n", "'for more details'", ")", "\n", "metric_options", "[", "'top_k_accuracy'", "]", "=", "dict", "(", "\n", "metric_options", "[", "'top_k_accuracy'", "]", ",", "**", "deprecated_kwargs", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "results", ",", "list", ")", ":", "\n", "            ", "raise", "TypeError", "(", "f'results must be a list, but got {type(results)}'", ")", "\n", "", "assert", "len", "(", "results", ")", "==", "len", "(", "self", ")", ",", "(", "\n", "f'The length of results is not equal to the dataset len: '", "\n", "f'{len(results)} != {len(self)}'", ")", "\n", "\n", "metrics", "=", "metrics", "if", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "metrics", "]", "\n", "allowed_metrics", "=", "[", "\n", "'top_k_accuracy'", ",", "'mean_class_accuracy'", ",", "'mean_average_precision'", ",", "\n", "'mmit_mean_average_precision'", "\n", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "if", "metric", "not", "in", "allowed_metrics", ":", "\n", "                ", "raise", "KeyError", "(", "f'metric {metric} is not supported'", ")", "\n", "\n", "", "", "eval_results", "=", "OrderedDict", "(", ")", "\n", "gt_labels", "=", "[", "ann", "[", "'label'", "]", "for", "ann", "in", "self", ".", "video_infos", "]", "\n", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "msg", "=", "f'Evaluating {metric} ...'", "\n", "if", "logger", "is", "None", ":", "\n", "                ", "msg", "=", "'\\n'", "+", "msg", "\n", "", "print_log", "(", "msg", ",", "logger", "=", "logger", ")", "\n", "\n", "if", "metric", "==", "'top_k_accuracy'", ":", "\n", "                ", "topk", "=", "metric_options", ".", "setdefault", "(", "'top_k_accuracy'", ",", "\n", "{", "}", ")", ".", "setdefault", "(", "\n", "'topk'", ",", "(", "1", ",", "5", ")", ")", "\n", "if", "not", "isinstance", "(", "topk", ",", "(", "int", ",", "tuple", ")", ")", ":", "\n", "                    ", "raise", "TypeError", "(", "'topk must be int or tuple of int, '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.train_step": [[204, 240], ["base.BaseTAGClassifier.forward", "base.BaseTAGClassifier._parse_losses", "dict", "len", "next", "iter", "data_batch.values"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier._parse_losses"], ["f'but got {type(topk)}'", ")", "\n", "", "if", "isinstance", "(", "topk", ",", "int", ")", ":", "\n", "                    ", "topk", "=", "(", "topk", ",", ")", "\n", "\n", "", "top_k_acc", "=", "top_k_accuracy", "(", "results", ",", "gt_labels", ",", "topk", ")", "\n", "log_msg", "=", "[", "]", "\n", "for", "k", ",", "acc", "in", "zip", "(", "topk", ",", "top_k_acc", ")", ":", "\n", "                    ", "eval_results", "[", "f'top{k}_acc'", "]", "=", "acc", "\n", "log_msg", ".", "append", "(", "f'\\ntop{k}_acc\\t{acc:.4f}'", ")", "\n", "", "log_msg", "=", "''", ".", "join", "(", "log_msg", ")", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "if", "metric", "==", "'mean_class_accuracy'", ":", "\n", "                ", "mean_acc", "=", "mean_class_accuracy", "(", "results", ",", "gt_labels", ")", "\n", "eval_results", "[", "'mean_class_accuracy'", "]", "=", "mean_acc", "\n", "log_msg", "=", "f'\\nmean_acc\\t{mean_acc:.4f}'", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n", "continue", "\n", "\n", "", "if", "metric", "in", "[", "\n", "'mean_average_precision'", ",", "'mmit_mean_average_precision'", "\n", "]", ":", "\n", "                ", "gt_labels_arrays", "=", "[", "\n", "self", ".", "label2array", "(", "self", ".", "num_classes", ",", "label", ")", "\n", "for", "label", "in", "gt_labels", "\n", "]", "\n", "if", "metric", "==", "'mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mean_average_precision", "(", "results", ",", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmean_average_precision\\t{mAP:.4f}'", "\n", "", "elif", "metric", "==", "'mmit_mean_average_precision'", ":", "\n", "                    ", "mAP", "=", "mmit_mean_average_precision", "(", "results", ",", "\n", "gt_labels_arrays", ")", "\n", "eval_results", "[", "'mmit_mean_average_precision'", "]", "=", "mAP", "\n", "log_msg", "=", "f'\\nmmit_mean_average_precision\\t{mAP:.4f}'", "\n", "", "print_log", "(", "log_msg", ",", "logger", "=", "logger", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.val_step": [[241, 253], ["base.BaseTAGClassifier.forward", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward"], ["continue", "\n", "\n", "", "", "return", "eval_results", "\n", "\n", "", "@", "staticmethod", "\n", "def", "dump_results", "(", "results", ",", "out", ")", ":", "\n", "        ", "\"\"\"Dump data to json/yaml/pickle strings or files.\"\"\"", "\n", "return", "mmcv", ".", "dump", "(", "results", ",", "out", ")", "\n", "\n", "", "def", "prepare_train_frames", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"Prepare the frames for training given the index.\"\"\"", "\n", "results", "=", "copy", ".", "deepcopy", "(", "self", ".", "video_infos", "[", "idx", "]", ")", "\n", "results", "[", "'modality'", "]", "=", "self", ".", "modality", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseLocalizer.__init__": [[258, 263], ["warnings.warn", "base.BaseTAGClassifier.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["if", "self", ".", "multi_class", "and", "isinstance", "(", "results", "[", "'label'", "]", ",", "list", ")", ":", "\n", "            ", "onehot", "=", "torch", ".", "zeros", "(", "self", ".", "num_classes", ")", "\n", "onehot", "[", "results", "[", "'label'", "]", "]", "=", "1.", "\n", "results", "[", "'label'", "]", "=", "onehot", "\n", "\n", "", "return", "self", ".", "pipeline", "(", "results", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.ssn.SSN.__init__": [[29, 58], ["dict", "base.BaseTAGClassifier.__init__", "builder.build_loss", "torch.AvgPool2d", "torch.AvgPool2d", "torch.Dropout", "torch.Dropout", "torch.MaxPool2d", "torch.MaxPool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss"], ["def", "__init__", "(", "self", ",", "\n", "backbone", ",", "\n", "cls_head", ",", "\n", "in_channels", "=", "3", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "dropout_ratio", "=", "0.5", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'SSNLoss'", ")", ",", "\n", "train_cfg", "=", "None", ",", "\n", "test_cfg", "=", "None", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "backbone", ",", "cls_head", ",", "train_cfg", ",", "test_cfg", ")", "\n", "\n", "self", ".", "is_test_prepared", "=", "False", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AvgPool2d", "(", "(", "7", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "", "elif", "self", ".", "spatial_type", "==", "'max'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "MaxPool2d", "(", "(", "7", ",", "7", ")", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pool", "=", "None", "\n", "\n", "", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "loss_cls", "=", "builder", ".", "build_loss", "(", "loss_cls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.ssn.SSN.forward_train": [[59, 80], ["imgs.reshape.reshape.reshape", "ssn.SSN.extract_feat", "ssn.SSN.cls_head", "ssn.SSN.loss_cls", "dict", "ssn.SSN.pool", "ssn.SSN.dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat"], ["", "def", "forward_train", "(", "self", ",", "imgs", ",", "proposal_scale_factor", ",", "proposal_type", ",", "\n", "proposal_labels", ",", "reg_targets", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when training.\"\"\"", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "in_channels", ")", "+", "imgs", ".", "shape", "[", "4", ":", "]", ")", "\n", "\n", "x", "=", "self", ".", "extract_feat", "(", "imgs", ")", "\n", "\n", "if", "self", ".", "pool", ":", "\n", "            ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "", "activity_scores", ",", "completeness_scores", ",", "bbox_preds", "=", "self", ".", "cls_head", "(", "\n", "(", "x", ",", "proposal_scale_factor", ")", ")", "\n", "\n", "loss", "=", "self", ".", "loss_cls", "(", "activity_scores", ",", "completeness_scores", ",", "bbox_preds", ",", "\n", "proposal_type", ",", "proposal_labels", ",", "reg_targets", ",", "\n", "self", ".", "train_cfg", ")", "\n", "loss_dict", "=", "dict", "(", "**", "loss", ")", "\n", "\n", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.ssn.SSN.forward_test": [[81, 136], ["imgs.reshape.reshape.reshape", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "relative_proposal_list.cpu().numpy.cpu().numpy.squeeze", "proposal_tick_list.squeeze.squeeze.squeeze", "scale_factor_list.squeeze.squeeze.squeeze", "reg_norm_consts.squeeze.squeeze.squeeze", "ssn.SSN.cls_head", "relative_proposal_list.cpu().numpy.cpu().numpy.cpu().numpy", "activity_scores.cpu().numpy.cpu().numpy.cpu().numpy", "completeness_scores.cpu().numpy.cpu().numpy.cpu().numpy", "imgs[].view", "ssn.SSN.extract_feat", "ssn.SSN.reshape().mean", "torch.cat.append", "torch.cat.append", "ssn.SSN.cls_head.prepare_test_fc", "bbox_preds.cpu().numpy.cpu().numpy.view", "bbox_preds.cpu().numpy.cpu().numpy.cpu().numpy", "dict", "ssn.SSN.pool", "relative_proposal_list.cpu().numpy.cpu().numpy.cpu", "activity_scores.cpu().numpy.cpu().numpy.cpu", "completeness_scores.cpu().numpy.cpu().numpy.cpu", "ssn.SSN.reshape", "bbox_preds.cpu().numpy.cpu().numpy.cpu", "ssn.SSN.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.extract_feat", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.SSNHead.prepare_test_fc"], ["", "def", "forward_test", "(", "self", ",", "imgs", ",", "relative_proposal_list", ",", "scale_factor_list", ",", "\n", "proposal_tick_list", ",", "reg_norm_consts", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when testing.\"\"\"", "\n", "num_crops", "=", "imgs", ".", "shape", "[", "0", "]", "\n", "imgs", "=", "imgs", ".", "reshape", "(", "(", "num_crops", ",", "-", "1", ",", "self", ".", "in_channels", ")", "+", "imgs", ".", "shape", "[", "3", ":", "]", ")", "\n", "num_ticks", "=", "imgs", ".", "shape", "[", "1", "]", "\n", "\n", "output", "=", "[", "]", "\n", "minibatch_size", "=", "self", ".", "test_cfg", ".", "ssn", ".", "sampler", ".", "batch_size", "\n", "for", "idx", "in", "range", "(", "0", ",", "num_ticks", ",", "minibatch_size", ")", ":", "\n", "            ", "chunk", "=", "imgs", "[", ":", ",", "idx", ":", "idx", "+", "\n", "minibatch_size", ",", ":", ",", ":", ",", ":", "]", ".", "view", "(", "(", "-", "1", ",", ")", "+", "imgs", ".", "shape", "[", "2", ":", "]", ")", "\n", "x", "=", "self", ".", "extract_feat", "(", "chunk", ")", "\n", "if", "self", ".", "pool", ":", "\n", "                ", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "# Merge crop to save memory.", "\n", "", "x", "=", "x", ".", "reshape", "(", "(", "num_crops", ",", "x", ".", "size", "(", "0", ")", "//", "num_crops", ",", "-", "1", ")", ")", ".", "mean", "(", "dim", "=", "0", ")", "\n", "output", ".", "append", "(", "x", ")", "\n", "", "output", "=", "torch", ".", "cat", "(", "output", ",", "dim", "=", "0", ")", "\n", "\n", "relative_proposal_list", "=", "relative_proposal_list", ".", "squeeze", "(", "0", ")", "\n", "proposal_tick_list", "=", "proposal_tick_list", ".", "squeeze", "(", "0", ")", "\n", "scale_factor_list", "=", "scale_factor_list", ".", "squeeze", "(", "0", ")", "\n", "reg_norm_consts", "=", "reg_norm_consts", ".", "squeeze", "(", "0", ")", "\n", "\n", "if", "not", "self", ".", "is_test_prepared", ":", "\n", "            ", "self", ".", "is_test_prepared", "=", "self", ".", "cls_head", ".", "prepare_test_fc", "(", "\n", "self", ".", "cls_head", ".", "consensus", ".", "num_multipliers", ")", "\n", "\n", "", "(", "output", ",", "activity_scores", ",", "completeness_scores", ",", "\n", "bbox_preds", ")", "=", "self", ".", "cls_head", "(", "\n", "(", "output", ",", "proposal_tick_list", ",", "scale_factor_list", ")", ",", "test_mode", "=", "True", ")", "\n", "\n", "relative_proposal_list", "=", "relative_proposal_list", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "activity_scores", "=", "activity_scores", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "completeness_scores", "=", "completeness_scores", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "bbox_preds", "is", "not", "None", ":", "\n", "            ", "bbox_preds", "=", "bbox_preds", ".", "view", "(", "-", "1", ",", "self", ".", "cls_head", ".", "num_classes", ",", "2", ")", "\n", "bbox_preds", "[", ":", ",", ":", ",", "0", "]", "=", "(", "\n", "bbox_preds", "[", ":", ",", ":", ",", "0", "]", "*", "reg_norm_consts", "[", "1", ",", "0", "]", "+", "\n", "reg_norm_consts", "[", "0", ",", "0", "]", ")", "\n", "bbox_preds", "[", ":", ",", ":", ",", "1", "]", "=", "(", "\n", "bbox_preds", "[", ":", ",", ":", ",", "1", "]", "*", "reg_norm_consts", "[", "1", ",", "1", "]", "+", "\n", "reg_norm_consts", "[", "0", ",", "1", "]", ")", "\n", "bbox_preds", "=", "bbox_preds", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "result", "=", "[", "\n", "dict", "(", "\n", "relative_proposal_list", "=", "relative_proposal_list", ",", "\n", "activity_scores", "=", "activity_scores", ",", "\n", "completeness_scores", "=", "completeness_scores", ",", "\n", "bbox_preds", "=", "bbox_preds", ")", "\n", "]", "\n", "\n", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN.__init__": [[41, 138], ["dict", "base.BaseTAPGenerator.__init__", "builder.build_loss", "bmn.BMN._get_interp1d_mask", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "bmn.BMN._temporal_anchors", "bmn.BMN._match_map", "bmn.BMN._get_bm_mask", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Sigmoid", "torch.Sigmoid", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Sigmoid", "torch.Sigmoid", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_interp1d_mask", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM._temporal_anchors", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._match_map", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_bm_mask"], ["def", "__init__", "(", "self", ",", "\n", "temporal_dim", ",", "\n", "boundary_ratio", ",", "\n", "num_samples", ",", "\n", "num_samples_per_bin", ",", "\n", "feat_dim", ",", "\n", "soft_nms_alpha", ",", "\n", "soft_nms_low_threshold", ",", "\n", "soft_nms_high_threshold", ",", "\n", "post_process_top_k", ",", "\n", "feature_extraction_interval", "=", "16", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'BMNLoss'", ")", ",", "\n", "hidden_dim_1d", "=", "256", ",", "\n", "hidden_dim_2d", "=", "128", ",", "\n", "hidden_dim_3d", "=", "512", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "tscale", "=", "temporal_dim", "\n", "self", ".", "boundary_ratio", "=", "boundary_ratio", "\n", "self", ".", "num_samples", "=", "num_samples", "\n", "self", ".", "num_samples_per_bin", "=", "num_samples_per_bin", "\n", "self", ".", "feat_dim", "=", "feat_dim", "\n", "self", ".", "soft_nms_alpha", "=", "soft_nms_alpha", "\n", "self", ".", "soft_nms_low_threshold", "=", "soft_nms_low_threshold", "\n", "self", ".", "soft_nms_high_threshold", "=", "soft_nms_high_threshold", "\n", "self", ".", "post_process_top_k", "=", "post_process_top_k", "\n", "self", ".", "feature_extraction_interval", "=", "feature_extraction_interval", "\n", "self", ".", "loss_cls", "=", "build_loss", "(", "loss_cls", ")", "\n", "self", ".", "hidden_dim_1d", "=", "hidden_dim_1d", "\n", "self", ".", "hidden_dim_2d", "=", "hidden_dim_2d", "\n", "self", ".", "hidden_dim_3d", "=", "hidden_dim_3d", "\n", "\n", "self", ".", "_get_interp1d_mask", "(", ")", "\n", "\n", "# Base Module", "\n", "self", ".", "x_1d_b", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "self", ".", "feat_dim", ",", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "4", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv1d", "(", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "4", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "\n", "# Temporal Evaluation Module", "\n", "self", ".", "x_1d_s", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "4", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv1d", "(", "self", ".", "hidden_dim_1d", ",", "1", ",", "kernel_size", "=", "1", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "x_1d_e", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "4", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv1d", "(", "self", ".", "hidden_dim_1d", ",", "1", ",", "kernel_size", "=", "1", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "\n", "# Proposal Evaluation Module", "\n", "self", ".", "x_1d_p", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "x_3d_p", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv3d", "(", "\n", "self", ".", "hidden_dim_1d", ",", "\n", "self", ".", "hidden_dim_3d", ",", "\n", "kernel_size", "=", "(", "self", ".", "num_samples", ",", "1", ",", "1", ")", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "self", ".", "x_2d_p", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "hidden_dim_3d", ",", "self", ".", "hidden_dim_2d", ",", "kernel_size", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "\n", "self", ".", "hidden_dim_2d", ",", "\n", "self", ".", "hidden_dim_2d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "\n", "self", ".", "hidden_dim_2d", ",", "\n", "self", ".", "hidden_dim_2d", ",", "\n", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "self", ".", "hidden_dim_2d", ",", "2", ",", "kernel_size", "=", "1", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "anchors_tmins", ",", "self", ".", "anchors_tmaxs", "=", "self", ".", "_temporal_anchors", "(", "\n", "-", "0.5", ",", "1.5", ")", "\n", "self", ".", "match_map", "=", "self", ".", "_match_map", "(", ")", "\n", "self", ".", "bm_mask", "=", "self", ".", "_get_bm_mask", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._match_map": [[139, 154], ["range", "numpy.array", "numpy.transpose", "numpy.reshape", "range", "numpy.reshape.append", "match_window.append"], "methods", ["None"], ["", "def", "_match_map", "(", "self", ")", ":", "\n", "        ", "\"\"\"Generate match map.\"\"\"", "\n", "temporal_gap", "=", "1.", "/", "self", ".", "tscale", "\n", "match_map", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "            ", "match_window", "=", "[", "]", "\n", "tmin", "=", "temporal_gap", "*", "idx", "\n", "for", "jdx", "in", "range", "(", "1", ",", "self", ".", "tscale", "+", "1", ")", ":", "\n", "                ", "tmax", "=", "tmin", "+", "temporal_gap", "*", "jdx", "\n", "match_window", ".", "append", "(", "[", "tmin", ",", "tmax", "]", ")", "\n", "", "match_map", ".", "append", "(", "match_window", ")", "\n", "", "match_map", "=", "np", ".", "array", "(", "match_map", ")", "\n", "match_map", "=", "np", ".", "transpose", "(", "match_map", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "match_map", "=", "np", ".", "reshape", "(", "match_map", ",", "[", "-", "1", ",", "2", "]", ")", "\n", "return", "match_map", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._temporal_anchors": [[155, 176], ["range", "anchors_tmins.append", "anchors_tmaxs.append"], "methods", ["None"], ["", "def", "_temporal_anchors", "(", "self", ",", "tmin_offset", "=", "0.", ",", "tmax_offset", "=", "1.", ")", ":", "\n", "        ", "\"\"\"Generate temporal anchors.\n\n        Args:\n            tmin_offset (int): Offset for the minimum value of temporal anchor.\n                Default: 0.\n            tmax_offset (int): Offset for the maximum value of temporal anchor.\n                Default: 1.\n\n        Returns:\n            tuple[Sequence[float]]: The minimum and maximum values of temporal\n                anchors.\n        \"\"\"", "\n", "temporal_gap", "=", "1.", "/", "self", ".", "tscale", "\n", "anchors_tmins", "=", "[", "]", "\n", "anchors_tmaxs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "            ", "anchors_tmins", ".", "append", "(", "temporal_gap", "*", "(", "i", "+", "tmin_offset", ")", ")", "\n", "anchors_tmaxs", ".", "append", "(", "temporal_gap", "*", "(", "i", "+", "tmax_offset", ")", ")", "\n", "\n", "", "return", "anchors_tmins", ",", "anchors_tmaxs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._forward": [[177, 203], ["bmn.BMN.x_1d_b", "bmn.BMN.x_1d_s().squeeze", "bmn.BMN.x_1d_e().squeeze", "bmn.BMN.x_1d_p", "bmn.BMN._boundary_matching_layer", "bmn.BMN.x_3d_p().squeeze", "bmn.BMN.x_2d_p", "bmn.BMN.x_1d_s", "bmn.BMN.x_1d_e", "bmn.BMN.x_3d_p"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._boundary_matching_layer"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# x.shape [batch_size, self.feat_dim, self.tscale]", "\n", "base_feature", "=", "self", ".", "x_1d_b", "(", "x", ")", "\n", "# base_feature.shape [batch_size, self.hidden_dim_1d, self.tscale]", "\n", "start", "=", "self", ".", "x_1d_s", "(", "base_feature", ")", ".", "squeeze", "(", "1", ")", "\n", "# start.shape [batch_size, self.tscale]", "\n", "end", "=", "self", ".", "x_1d_e", "(", "base_feature", ")", ".", "squeeze", "(", "1", ")", "\n", "# end.shape [batch_size, self.tscale]", "\n", "confidence_map", "=", "self", ".", "x_1d_p", "(", "base_feature", ")", "\n", "# [batch_size, self.hidden_dim_1d, self.tscale]", "\n", "confidence_map", "=", "self", ".", "_boundary_matching_layer", "(", "confidence_map", ")", "\n", "# [batch_size, self.hidden_dim_1d,, self.num_sampls, self.tscale, self.tscale] # noqa", "\n", "confidence_map", "=", "self", ".", "x_3d_p", "(", "confidence_map", ")", ".", "squeeze", "(", "2", ")", "\n", "# [batch_size, self.hidden_dim_3d, self.tscale, self.tscale]", "\n", "confidence_map", "=", "self", ".", "x_2d_p", "(", "confidence_map", ")", "\n", "# [batch_size, 2, self.tscale, self.tscale]", "\n", "\n", "return", "confidence_map", ",", "start", ",", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._boundary_matching_layer": [[204, 213], ["x.size", "torch.matmul().reshape", "torch.matmul().reshape", "torch.matmul().reshape", "torch.matmul().reshape", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["", "def", "_boundary_matching_layer", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Generate matching layer.\"\"\"", "\n", "input_size", "=", "x", ".", "size", "(", ")", "\n", "out", "=", "torch", ".", "matmul", "(", "x", ",", "\n", "self", ".", "sample_mask", ")", ".", "reshape", "(", "input_size", "[", "0", "]", ",", "\n", "input_size", "[", "1", "]", ",", "\n", "self", ".", "num_samples", ",", "\n", "self", ".", "tscale", ",", "self", ".", "tscale", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN.forward_test": [[214, 275], ["bmn.BMN._forward", "start[].cpu().numpy", "end[].cpu().numpy", "[].cpu().numpy", "[].cpu().numpy", "max", "max", "numpy.zeros", "numpy.zeros", "range", "range", "numpy.stack", "dict", "utils.post_processing", "len", "len", "range", "dict", "start[].cpu", "end[].cpu", "[].cpu", "[].cpu", "numpy.stack.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.post_processing.post_processing"], ["", "def", "forward_test", "(", "self", ",", "raw_feature", ",", "video_meta", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when testing.\"\"\"", "\n", "confidence_map", ",", "start", ",", "end", "=", "self", ".", "_forward", "(", "raw_feature", ")", "\n", "start_scores", "=", "start", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "end_scores", "=", "end", "[", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "cls_confidence", "=", "(", "confidence_map", "[", "0", "]", "[", "1", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "reg_confidence", "=", "(", "confidence_map", "[", "0", "]", "[", "0", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "max_start", "=", "max", "(", "start_scores", ")", "\n", "max_end", "=", "max", "(", "end_scores", ")", "\n", "\n", "# generate the set of start points and end points", "\n", "start_bins", "=", "np", ".", "zeros", "(", "len", "(", "start_scores", ")", ")", "\n", "start_bins", "[", "0", "]", "=", "1", "# [1,0,0...,0,0]", "\n", "end_bins", "=", "np", ".", "zeros", "(", "len", "(", "end_scores", ")", ")", "\n", "end_bins", "[", "-", "1", "]", "=", "1", "# [0,0,0...,0,1]", "\n", "for", "idx", "in", "range", "(", "1", ",", "self", ".", "tscale", "-", "1", ")", ":", "\n", "            ", "if", "start_scores", "[", "idx", "]", ">", "start_scores", "[", "\n", "idx", "+", "1", "]", "and", "start_scores", "[", "idx", "]", ">", "start_scores", "[", "idx", "-", "1", "]", ":", "\n", "                ", "start_bins", "[", "idx", "]", "=", "1", "\n", "", "elif", "start_scores", "[", "idx", "]", ">", "(", "0.5", "*", "max_start", ")", ":", "\n", "                ", "start_bins", "[", "idx", "]", "=", "1", "\n", "", "if", "end_scores", "[", "idx", "]", ">", "end_scores", "[", "\n", "idx", "+", "1", "]", "and", "end_scores", "[", "idx", "]", ">", "end_scores", "[", "idx", "-", "1", "]", ":", "\n", "                ", "end_bins", "[", "idx", "]", "=", "1", "\n", "", "elif", "end_scores", "[", "idx", "]", ">", "(", "0.5", "*", "max_end", ")", ":", "\n", "                ", "end_bins", "[", "idx", "]", "=", "1", "\n", "\n", "# iterate through all combinations of start_index and end_index", "\n", "", "", "new_proposals", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "            ", "for", "jdx", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "                ", "start_index", "=", "jdx", "\n", "end_index", "=", "start_index", "+", "idx", "+", "1", "\n", "if", "end_index", "<", "self", ".", "tscale", "and", "start_bins", "[", "\n", "start_index", "]", "==", "1", "and", "end_bins", "[", "end_index", "]", "==", "1", ":", "\n", "                    ", "tmin", "=", "start_index", "/", "self", ".", "tscale", "\n", "tmax", "=", "end_index", "/", "self", ".", "tscale", "\n", "tmin_score", "=", "start_scores", "[", "start_index", "]", "\n", "tmax_score", "=", "end_scores", "[", "end_index", "]", "\n", "cls_score", "=", "cls_confidence", "[", "idx", ",", "jdx", "]", "\n", "reg_score", "=", "reg_confidence", "[", "idx", ",", "jdx", "]", "\n", "score", "=", "tmin_score", "*", "tmax_score", "*", "cls_score", "*", "reg_score", "\n", "new_proposals", ".", "append", "(", "[", "\n", "tmin", ",", "tmax", ",", "tmin_score", ",", "tmax_score", ",", "cls_score", ",", "\n", "reg_score", ",", "score", "\n", "]", ")", "\n", "", "", "", "new_proposals", "=", "np", ".", "stack", "(", "new_proposals", ")", "\n", "video_info", "=", "dict", "(", "video_meta", "[", "0", "]", ")", "\n", "proposal_list", "=", "post_processing", "(", "new_proposals", ",", "video_info", ",", "\n", "self", ".", "soft_nms_alpha", ",", "\n", "self", ".", "soft_nms_low_threshold", ",", "\n", "self", ".", "soft_nms_high_threshold", ",", "\n", "self", ".", "post_process_top_k", ",", "\n", "self", ".", "feature_extraction_interval", ")", "\n", "output", "=", "[", "\n", "dict", "(", "\n", "video_name", "=", "video_info", "[", "'video_name'", "]", ",", "\n", "proposal_list", "=", "proposal_list", ")", "\n", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN.forward_train": [[276, 285], ["bmn.BMN._forward", "bmn.BMN.loss_cls", "dict", "bmn.BMN.bm_mask.to"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], ["", "def", "forward_train", "(", "self", ",", "raw_feature", ",", "label_confidence", ",", "label_start", ",", "\n", "label_end", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when training.\"\"\"", "\n", "confidence_map", ",", "start", ",", "end", "=", "self", ".", "_forward", "(", "raw_feature", ")", "\n", "loss", "=", "self", ".", "loss_cls", "(", "confidence_map", ",", "start", ",", "end", ",", "label_confidence", ",", "\n", "label_start", ",", "label_end", ",", "\n", "self", ".", "bm_mask", ".", "to", "(", "raw_feature", ".", "device", ")", ")", "\n", "loss_dict", "=", "dict", "(", "loss", "=", "loss", "[", "0", "]", ")", "\n", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN.generate_labels": [[286, 339], ["torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.array().astype", "numpy.max", "numpy.stack", "numpy.stack", "zip", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "isinstance", "isinstance", "temporal_iou", "numpy.reshape", "numpy.max.append", "match_score_start.append", "match_score_end.append", "start.numpy.numpy.numpy", "end.numpy.numpy.numpy", "numpy.array", "numpy.max", "numpy.max", "temporal_iop", "temporal_iop"], "methods", ["None"], ["", "def", "generate_labels", "(", "self", ",", "gt_bbox", ")", ":", "\n", "        ", "\"\"\"Generate training labels.\"\"\"", "\n", "match_score_confidence_list", "=", "[", "]", "\n", "match_score_start_list", "=", "[", "]", "\n", "match_score_end_list", "=", "[", "]", "\n", "for", "every_gt_bbox", "in", "gt_bbox", ":", "\n", "            ", "gt_iou_map", "=", "[", "]", "\n", "for", "start", ",", "end", "in", "every_gt_bbox", ":", "\n", "                ", "if", "isinstance", "(", "start", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "start", "=", "start", ".", "numpy", "(", ")", "\n", "", "if", "isinstance", "(", "end", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "end", "=", "end", ".", "numpy", "(", ")", "\n", "", "current_gt_iou_map", "=", "temporal_iou", "(", "self", ".", "match_map", "[", ":", ",", "0", "]", ",", "\n", "self", ".", "match_map", "[", ":", ",", "1", "]", ",", "start", ",", "\n", "end", ")", "\n", "current_gt_iou_map", "=", "np", ".", "reshape", "(", "current_gt_iou_map", ",", "\n", "[", "self", ".", "tscale", ",", "self", ".", "tscale", "]", ")", "\n", "gt_iou_map", ".", "append", "(", "current_gt_iou_map", ")", "\n", "", "gt_iou_map", "=", "np", ".", "array", "(", "gt_iou_map", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "gt_iou_map", "=", "np", ".", "max", "(", "gt_iou_map", ",", "axis", "=", "0", ")", "\n", "\n", "gt_tmins", "=", "every_gt_bbox", "[", ":", ",", "0", "]", "\n", "gt_tmaxs", "=", "every_gt_bbox", "[", ":", ",", "1", "]", "\n", "\n", "gt_len_pad", "=", "3", "*", "(", "1.", "/", "self", ".", "tscale", ")", "\n", "\n", "gt_start_bboxs", "=", "np", ".", "stack", "(", "\n", "(", "gt_tmins", "-", "gt_len_pad", "/", "2", ",", "gt_tmins", "+", "gt_len_pad", "/", "2", ")", ",", "axis", "=", "1", ")", "\n", "gt_end_bboxs", "=", "np", ".", "stack", "(", "\n", "(", "gt_tmaxs", "-", "gt_len_pad", "/", "2", ",", "gt_tmaxs", "+", "gt_len_pad", "/", "2", ")", ",", "axis", "=", "1", ")", "\n", "\n", "match_score_start", "=", "[", "]", "\n", "match_score_end", "=", "[", "]", "\n", "\n", "for", "anchor_tmin", ",", "anchor_tmax", "in", "zip", "(", "self", ".", "anchors_tmins", ",", "\n", "self", ".", "anchors_tmaxs", ")", ":", "\n", "                ", "match_score_start", ".", "append", "(", "\n", "np", ".", "max", "(", "\n", "temporal_iop", "(", "anchor_tmin", ",", "anchor_tmax", ",", "\n", "gt_start_bboxs", "[", ":", ",", "0", "]", ",", "gt_start_bboxs", "[", ":", ",", "\n", "1", "]", ")", ")", ")", "\n", "match_score_end", ".", "append", "(", "\n", "np", ".", "max", "(", "\n", "temporal_iop", "(", "anchor_tmin", ",", "anchor_tmax", ",", "\n", "gt_end_bboxs", "[", ":", ",", "0", "]", ",", "gt_end_bboxs", "[", ":", ",", "1", "]", ")", ")", ")", "\n", "", "match_score_confidence_list", ".", "append", "(", "gt_iou_map", ")", "\n", "match_score_start_list", ".", "append", "(", "match_score_start", ")", "\n", "match_score_end_list", ".", "append", "(", "match_score_end", ")", "\n", "", "match_score_confidence_list", "=", "torch", ".", "Tensor", "(", "match_score_confidence_list", ")", "\n", "match_score_start_list", "=", "torch", ".", "Tensor", "(", "match_score_start_list", ")", "\n", "match_score_end_list", "=", "torch", ".", "Tensor", "(", "match_score_end_list", ")", "\n", "return", "(", "match_score_confidence_list", ",", "match_score_start_list", ",", "\n", "match_score_end_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN.forward": [[340, 357], ["bmn.BMN.forward_test", "bmn.BMN.generate_labels", "label_confidence.to.to.to", "label_start.to.to.to", "label_end.to.to.to", "bmn.BMN.forward_train"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.generate_labels", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["", "def", "forward", "(", "self", ",", "\n", "raw_feature", ",", "\n", "gt_bbox", "=", "None", ",", "\n", "video_meta", "=", "None", ",", "\n", "return_loss", "=", "True", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\"\"\"", "\n", "if", "return_loss", ":", "\n", "            ", "label_confidence", ",", "label_start", ",", "label_end", "=", "(", "\n", "self", ".", "generate_labels", "(", "gt_bbox", ")", ")", "\n", "device", "=", "raw_feature", ".", "device", "\n", "label_confidence", "=", "label_confidence", ".", "to", "(", "device", ")", "\n", "label_start", "=", "label_start", ".", "to", "(", "device", ")", "\n", "label_end", "=", "label_end", ".", "to", "(", "device", ")", "\n", "return", "self", ".", "forward_train", "(", "raw_feature", ",", "label_confidence", ",", "\n", "label_start", ",", "label_end", ")", "\n", "\n", "", "return", "self", ".", "forward_test", "(", "raw_feature", ",", "video_meta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_interp1d_bin_mask": [[358, 384], ["float", "range", "numpy.stack", "numpy.zeros", "numpy.stack.append", "range", "math.ceil", "math.modf", "int", "int", "int", "int"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_interp1d_bin_mask", "(", "seg_tmin", ",", "seg_tmax", ",", "tscale", ",", "num_samples", ",", "\n", "num_samples_per_bin", ")", ":", "\n", "        ", "\"\"\"Generate sample mask for a boundary-matching pair.\"\"\"", "\n", "plen", "=", "float", "(", "seg_tmax", "-", "seg_tmin", ")", "\n", "plen_sample", "=", "plen", "/", "(", "num_samples", "*", "num_samples_per_bin", "-", "1.0", ")", "\n", "total_samples", "=", "[", "\n", "seg_tmin", "+", "plen_sample", "*", "i", "\n", "for", "i", "in", "range", "(", "num_samples", "*", "num_samples_per_bin", ")", "\n", "]", "\n", "p_mask", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "bin_samples", "=", "total_samples", "[", "idx", "*", "num_samples_per_bin", ":", "(", "idx", "+", "1", ")", "*", "\n", "num_samples_per_bin", "]", "\n", "bin_vector", "=", "np", ".", "zeros", "(", "tscale", ")", "\n", "for", "sample", "in", "bin_samples", ":", "\n", "                ", "sample_upper", "=", "math", ".", "ceil", "(", "sample", ")", "\n", "sample_decimal", ",", "sample_down", "=", "math", ".", "modf", "(", "sample", ")", "\n", "if", "0", "<=", "int", "(", "sample_down", ")", "<=", "(", "tscale", "-", "1", ")", ":", "\n", "                    ", "bin_vector", "[", "int", "(", "sample_down", ")", "]", "+=", "1", "-", "sample_decimal", "\n", "", "if", "0", "<=", "int", "(", "sample_upper", ")", "<=", "(", "tscale", "-", "1", ")", ":", "\n", "                    ", "bin_vector", "[", "int", "(", "sample_upper", ")", "]", "+=", "sample_decimal", "\n", "", "", "bin_vector", "=", "1.0", "/", "num_samples_per_bin", "*", "bin_vector", "\n", "p_mask", ".", "append", "(", "bin_vector", ")", "\n", "", "p_mask", "=", "np", ".", "stack", "(", "p_mask", ",", "axis", "=", "1", ")", "\n", "return", "p_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_interp1d_mask": [[385, 409], ["range", "numpy.stack", "mask_mat.astype.astype.astype", "torch.Parameter", "torch.Parameter", "range", "numpy.stack", "mask_mat.astype.astype.append", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "numpy.stack.append", "bmn.BMN._get_interp1d_bin_mask", "numpy.zeros", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "float"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_interp1d_bin_mask"], ["", "def", "_get_interp1d_mask", "(", "self", ")", ":", "\n", "        ", "\"\"\"Generate sample mask for each point in Boundary-Matching Map.\"\"\"", "\n", "mask_mat", "=", "[", "]", "\n", "for", "start_index", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "            ", "mask_mat_vector", "=", "[", "]", "\n", "for", "duration_index", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "                ", "if", "start_index", "+", "duration_index", "<", "self", ".", "tscale", ":", "\n", "                    ", "p_tmin", "=", "start_index", "\n", "p_tmax", "=", "start_index", "+", "duration_index", "\n", "center_len", "=", "float", "(", "p_tmax", "-", "p_tmin", ")", "+", "1", "\n", "sample_tmin", "=", "p_tmin", "-", "(", "center_len", "*", "self", ".", "boundary_ratio", ")", "\n", "sample_tmax", "=", "p_tmax", "+", "(", "center_len", "*", "self", ".", "boundary_ratio", ")", "\n", "p_mask", "=", "self", ".", "_get_interp1d_bin_mask", "(", "\n", "sample_tmin", ",", "sample_tmax", ",", "self", ".", "tscale", ",", "\n", "self", ".", "num_samples", ",", "self", ".", "num_samples_per_bin", ")", "\n", "", "else", ":", "\n", "                    ", "p_mask", "=", "np", ".", "zeros", "(", "[", "self", ".", "tscale", ",", "self", ".", "num_samples", "]", ")", "\n", "", "mask_mat_vector", ".", "append", "(", "p_mask", ")", "\n", "", "mask_mat_vector", "=", "np", ".", "stack", "(", "mask_mat_vector", ",", "axis", "=", "2", ")", "\n", "mask_mat", ".", "append", "(", "mask_mat_vector", ")", "\n", "", "mask_mat", "=", "np", ".", "stack", "(", "mask_mat", ",", "axis", "=", "3", ")", "\n", "mask_mat", "=", "mask_mat", ".", "astype", "(", "np", ".", "float32", ")", "\n", "self", ".", "sample_mask", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "tensor", "(", "mask_mat", ")", ".", "view", "(", "self", ".", "tscale", ",", "-", "1", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bmn.BMN._get_bm_mask": [[410, 418], ["range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.append", "torch.tensor.append"], "methods", ["None"], ["", "def", "_get_bm_mask", "(", "self", ")", ":", "\n", "        ", "\"\"\"Generate Boundary-Matching Mask.\"\"\"", "\n", "bm_mask", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "tscale", ")", ":", "\n", "            ", "mask_vector", "=", "[", "1", "]", "*", "(", "self", ".", "tscale", "-", "idx", ")", "+", "[", "0", "]", "*", "idx", "\n", "bm_mask", ".", "append", "(", "mask_vector", ")", "\n", "", "bm_mask", "=", "torch", ".", "tensor", "(", "bm_mask", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "return", "bm_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.__init__": [[36, 83], ["dict", "base.BaseTAPGenerator.__init__", "builder.build_loss", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "bsn.TEM._temporal_anchors"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM._temporal_anchors"], ["def", "__init__", "(", "self", ",", "\n", "temporal_dim", ",", "\n", "boundary_ratio", ",", "\n", "tem_feat_dim", ",", "\n", "tem_hidden_dim", ",", "\n", "tem_match_threshold", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'BinaryLogisticRegressionLoss'", ")", ",", "\n", "loss_weight", "=", "2", ",", "\n", "output_dim", "=", "3", ",", "\n", "conv1_ratio", "=", "1", ",", "\n", "conv2_ratio", "=", "1", ",", "\n", "conv3_ratio", "=", "0.01", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "temporal_dim", "=", "temporal_dim", "\n", "self", ".", "boundary_ratio", "=", "boundary_ratio", "\n", "self", ".", "feat_dim", "=", "tem_feat_dim", "\n", "self", ".", "c_hidden", "=", "tem_hidden_dim", "\n", "self", ".", "match_threshold", "=", "tem_match_threshold", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "self", ".", "loss_cls", "=", "build_loss", "(", "loss_cls", ")", "\n", "self", ".", "loss_weight", "=", "loss_weight", "\n", "self", ".", "conv1_ratio", "=", "conv1_ratio", "\n", "self", ".", "conv2_ratio", "=", "conv2_ratio", "\n", "self", ".", "conv3_ratio", "=", "conv3_ratio", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "self", ".", "feat_dim", ",", "\n", "out_channels", "=", "self", ".", "c_hidden", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "1", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "self", ".", "c_hidden", ",", "\n", "out_channels", "=", "self", ".", "c_hidden", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "1", ")", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "self", ".", "c_hidden", ",", "\n", "out_channels", "=", "self", ".", "output_dim", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ")", "\n", "self", ".", "anchors_tmins", ",", "self", ".", "anchors_tmaxs", "=", "self", ".", "_temporal_anchors", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM._temporal_anchors": [[84, 105], ["range", "anchors_tmins.append", "anchors_tmaxs.append"], "methods", ["None"], ["", "def", "_temporal_anchors", "(", "self", ",", "tmin_offset", "=", "0.", ",", "tmax_offset", "=", "1.", ")", ":", "\n", "        ", "\"\"\"Generate temporal anchors.\n\n        Args:\n            tmin_offset (int): Offset for the minimum value of temporal anchor.\n                Default: 0.\n            tmax_offset (int): Offset for the maximum value of temporal anchor.\n                Default: 1.\n\n        Returns:\n            tuple[Sequence[float]]: The minimum and maximum values of temporal\n                anchors.\n        \"\"\"", "\n", "temporal_gap", "=", "1.", "/", "self", ".", "temporal_dim", "\n", "anchors_tmins", "=", "[", "]", "\n", "anchors_tmaxs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "temporal_dim", ")", ":", "\n", "            ", "anchors_tmins", ".", "append", "(", "temporal_gap", "*", "(", "i", "+", "tmin_offset", ")", ")", "\n", "anchors_tmaxs", ".", "append", "(", "temporal_gap", "*", "(", "i", "+", "tmax_offset", ")", ")", "\n", "\n", "", "return", "anchors_tmins", ",", "anchors_tmaxs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM._forward": [[106, 119], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "bsn.TEM.conv1", "bsn.TEM.conv2", "bsn.TEM.conv3"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv1_ratio", "*", "self", ".", "conv1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "conv2_ratio", "*", "self", ".", "conv2", "(", "x", ")", ")", "\n", "x", "=", "torch", ".", "sigmoid", "(", "self", ".", "conv3_ratio", "*", "self", ".", "conv3", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.forward_train": [[120, 140], ["bsn.TEM._forward", "bsn.TEM.loss_cls", "bsn.TEM.loss_cls", "bsn.TEM.loss_cls"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], ["", "def", "forward_train", "(", "self", ",", "raw_feature", ",", "label_action", ",", "label_start", ",", "label_end", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when training.\"\"\"", "\n", "tem_output", "=", "self", ".", "_forward", "(", "raw_feature", ")", "\n", "score_action", "=", "tem_output", "[", ":", ",", "0", ",", ":", "]", "\n", "score_start", "=", "tem_output", "[", ":", ",", "1", ",", ":", "]", "\n", "score_end", "=", "tem_output", "[", ":", ",", "2", ",", ":", "]", "\n", "\n", "loss_action", "=", "self", ".", "loss_cls", "(", "score_action", ",", "label_action", ",", "\n", "self", ".", "match_threshold", ")", "\n", "loss_start_small", "=", "self", ".", "loss_cls", "(", "score_start", ",", "label_start", ",", "\n", "self", ".", "match_threshold", ")", "\n", "loss_end_small", "=", "self", ".", "loss_cls", "(", "score_end", ",", "label_end", ",", "\n", "self", ".", "match_threshold", ")", "\n", "loss_dict", "=", "{", "\n", "'loss_action'", ":", "loss_action", "*", "self", ".", "loss_weight", ",", "\n", "'loss_start'", ":", "loss_start_small", ",", "\n", "'loss_end'", ":", "loss_end_small", "\n", "}", "\n", "\n", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.forward_test": [[141, 162], ["bsn.TEM._forward().cpu().numpy", "enumerate", "dict", "numpy.stack", "video_results.append", "bsn.TEM._forward().cpu", "bsn.TEM._forward"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], ["", "def", "forward_test", "(", "self", ",", "raw_feature", ",", "video_meta", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when testing.\"\"\"", "\n", "tem_output", "=", "self", ".", "_forward", "(", "raw_feature", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "batch_action", "=", "tem_output", "[", ":", ",", "0", ",", ":", "]", "\n", "batch_start", "=", "tem_output", "[", ":", ",", "1", ",", ":", "]", "\n", "batch_end", "=", "tem_output", "[", ":", ",", "2", ",", ":", "]", "\n", "\n", "video_meta_list", "=", "[", "dict", "(", "x", ")", "for", "x", "in", "video_meta", "]", "\n", "\n", "video_results", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "_", "in", "enumerate", "(", "batch_action", ")", ":", "\n", "            ", "video_name", "=", "video_meta_list", "[", "batch_idx", "]", "[", "'video_name'", "]", "\n", "video_action", "=", "batch_action", "[", "batch_idx", "]", "\n", "video_start", "=", "batch_start", "[", "batch_idx", "]", "\n", "video_end", "=", "batch_end", "[", "batch_idx", "]", "\n", "video_result", "=", "np", ".", "stack", "(", "(", "video_action", ",", "video_start", ",", "video_end", ",", "\n", "self", ".", "anchors_tmins", ",", "self", ".", "anchors_tmaxs", ")", ",", "\n", "axis", "=", "1", ")", "\n", "video_results", ".", "append", "(", "(", "video_name", ",", "video_result", ")", ")", "\n", "", "return", "video_results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.generate_labels": [[163, 208], ["torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "every_gt_bbox[].cpu().numpy", "every_gt_bbox[].cpu().numpy", "numpy.maximum", "numpy.stack", "numpy.stack", "zip", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "torch.Tensor.append", "match_score_action.append", "match_score_start.append", "match_score_end.append", "every_gt_bbox[].cpu", "every_gt_bbox[].cpu", "numpy.max", "numpy.max", "numpy.max", "temporal_iop", "temporal_iop", "temporal_iop"], "methods", ["None"], ["", "def", "generate_labels", "(", "self", ",", "gt_bbox", ")", ":", "\n", "        ", "\"\"\"Generate training labels.\"\"\"", "\n", "match_score_action_list", "=", "[", "]", "\n", "match_score_start_list", "=", "[", "]", "\n", "match_score_end_list", "=", "[", "]", "\n", "for", "every_gt_bbox", "in", "gt_bbox", ":", "\n", "            ", "gt_tmins", "=", "every_gt_bbox", "[", ":", ",", "0", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "gt_tmaxs", "=", "every_gt_bbox", "[", ":", ",", "1", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "gt_lens", "=", "gt_tmaxs", "-", "gt_tmins", "\n", "gt_len_pad", "=", "np", ".", "maximum", "(", "1.", "/", "self", ".", "temporal_dim", ",", "\n", "self", ".", "boundary_ratio", "*", "gt_lens", ")", "\n", "\n", "gt_start_bboxs", "=", "np", ".", "stack", "(", "\n", "(", "gt_tmins", "-", "gt_len_pad", "/", "2", ",", "gt_tmins", "+", "gt_len_pad", "/", "2", ")", ",", "axis", "=", "1", ")", "\n", "gt_end_bboxs", "=", "np", ".", "stack", "(", "\n", "(", "gt_tmaxs", "-", "gt_len_pad", "/", "2", ",", "gt_tmaxs", "+", "gt_len_pad", "/", "2", ")", ",", "axis", "=", "1", ")", "\n", "\n", "match_score_action", "=", "[", "]", "\n", "match_score_start", "=", "[", "]", "\n", "match_score_end", "=", "[", "]", "\n", "\n", "for", "anchor_tmin", ",", "anchor_tmax", "in", "zip", "(", "self", ".", "anchors_tmins", ",", "\n", "self", ".", "anchors_tmaxs", ")", ":", "\n", "                ", "match_score_action", ".", "append", "(", "\n", "np", ".", "max", "(", "\n", "temporal_iop", "(", "anchor_tmin", ",", "anchor_tmax", ",", "gt_tmins", ",", "\n", "gt_tmaxs", ")", ")", ")", "\n", "match_score_start", ".", "append", "(", "\n", "np", ".", "max", "(", "\n", "temporal_iop", "(", "anchor_tmin", ",", "anchor_tmax", ",", "\n", "gt_start_bboxs", "[", ":", ",", "0", "]", ",", "gt_start_bboxs", "[", ":", ",", "\n", "1", "]", ")", ")", ")", "\n", "match_score_end", ".", "append", "(", "\n", "np", ".", "max", "(", "\n", "temporal_iop", "(", "anchor_tmin", ",", "anchor_tmax", ",", "\n", "gt_end_bboxs", "[", ":", ",", "0", "]", ",", "gt_end_bboxs", "[", ":", ",", "1", "]", ")", ")", ")", "\n", "", "match_score_action_list", ".", "append", "(", "match_score_action", ")", "\n", "match_score_start_list", ".", "append", "(", "match_score_start", ")", "\n", "match_score_end_list", ".", "append", "(", "match_score_end", ")", "\n", "", "match_score_action_list", "=", "torch", ".", "Tensor", "(", "match_score_action_list", ")", "\n", "match_score_start_list", "=", "torch", ".", "Tensor", "(", "match_score_start_list", ")", "\n", "match_score_end_list", "=", "torch", ".", "Tensor", "(", "match_score_end_list", ")", "\n", "return", "(", "match_score_action_list", ",", "match_score_start_list", ",", "\n", "match_score_end_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.forward": [[209, 226], ["bsn.TEM.forward_test", "bsn.TEM.generate_labels", "label_action.to.to.to", "label_start.to.to.to", "label_end.to.to.to", "bsn.TEM.forward_train"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.TEM.generate_labels", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["", "def", "forward", "(", "self", ",", "\n", "raw_feature", ",", "\n", "gt_bbox", "=", "None", ",", "\n", "video_meta", "=", "None", ",", "\n", "return_loss", "=", "True", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\"\"\"", "\n", "if", "return_loss", ":", "\n", "            ", "label_action", ",", "label_start", ",", "label_end", "=", "(", "\n", "self", ".", "generate_labels", "(", "gt_bbox", ")", ")", "\n", "device", "=", "raw_feature", ".", "device", "\n", "label_action", "=", "label_action", ".", "to", "(", "device", ")", "\n", "label_start", "=", "label_start", ".", "to", "(", "device", ")", "\n", "label_end", "=", "label_end", ".", "to", "(", "device", ")", "\n", "return", "self", ".", "forward_train", "(", "raw_feature", ",", "label_action", ",", "label_start", ",", "\n", "label_end", ")", "\n", "\n", "", "return", "self", ".", "forward_test", "(", "raw_feature", ",", "video_meta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.__init__": [[257, 295], ["base.BaseTAPGenerator.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "pem_feat_dim", ",", "\n", "pem_hidden_dim", ",", "\n", "pem_u_ratio_m", ",", "\n", "pem_u_ratio_l", ",", "\n", "pem_high_temporal_iou_threshold", ",", "\n", "pem_low_temporal_iou_threshold", ",", "\n", "soft_nms_alpha", ",", "\n", "soft_nms_low_threshold", ",", "\n", "soft_nms_high_threshold", ",", "\n", "post_process_top_k", ",", "\n", "feature_extraction_interval", "=", "16", ",", "\n", "fc1_ratio", "=", "0.1", ",", "\n", "fc2_ratio", "=", "0.1", ",", "\n", "output_dim", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "feat_dim", "=", "pem_feat_dim", "\n", "self", ".", "hidden_dim", "=", "pem_hidden_dim", "\n", "self", ".", "u_ratio_m", "=", "pem_u_ratio_m", "\n", "self", ".", "u_ratio_l", "=", "pem_u_ratio_l", "\n", "self", ".", "pem_high_temporal_iou_threshold", "=", "pem_high_temporal_iou_threshold", "\n", "self", ".", "pem_low_temporal_iou_threshold", "=", "pem_low_temporal_iou_threshold", "\n", "self", ".", "soft_nms_alpha", "=", "soft_nms_alpha", "\n", "self", ".", "soft_nms_low_threshold", "=", "soft_nms_low_threshold", "\n", "self", ".", "soft_nms_high_threshold", "=", "soft_nms_high_threshold", "\n", "self", ".", "post_process_top_k", "=", "post_process_top_k", "\n", "self", ".", "feature_extraction_interval", "=", "feature_extraction_interval", "\n", "self", ".", "fc1_ratio", "=", "fc1_ratio", "\n", "self", ".", "fc2_ratio", "=", "fc2_ratio", "\n", "self", ".", "output_dim", "=", "output_dim", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "\n", "in_features", "=", "self", ".", "feat_dim", ",", "out_features", "=", "self", ".", "hidden_dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "\n", "in_features", "=", "self", ".", "hidden_dim", ",", "\n", "out_features", "=", "self", ".", "output_dim", ",", "\n", "bias", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM._forward": [[296, 309], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.relu", "torch.relu", "torch.relu", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "list", "bsn.PEM.fc1", "bsn.PEM.fc2"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "_forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "torch", ".", "cat", "(", "list", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1_ratio", "*", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "torch", ".", "sigmoid", "(", "self", ".", "fc2_ratio", "*", "self", ".", "fc2", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train": [[310, 352], ["bsn.PEM._forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "reference_temporal_iou.to.to.to", "bsn.PEM.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "dict", "list", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "u_hmask.size", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "u_hmask.size", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], ["", "def", "forward_train", "(", "self", ",", "bsp_feature", ",", "reference_temporal_iou", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when training.\"\"\"", "\n", "pem_output", "=", "self", ".", "_forward", "(", "bsp_feature", ")", "\n", "reference_temporal_iou", "=", "torch", ".", "cat", "(", "list", "(", "reference_temporal_iou", ")", ")", "\n", "device", "=", "pem_output", ".", "device", "\n", "reference_temporal_iou", "=", "reference_temporal_iou", ".", "to", "(", "device", ")", "\n", "\n", "anchors_temporal_iou", "=", "pem_output", ".", "view", "(", "-", "1", ")", "\n", "u_hmask", "=", "(", "reference_temporal_iou", ">", "\n", "self", ".", "pem_high_temporal_iou_threshold", ")", ".", "float", "(", ")", "\n", "u_mmask", "=", "(", "\n", "(", "reference_temporal_iou", "<=", "self", ".", "pem_high_temporal_iou_threshold", ")", "\n", "&", "(", "reference_temporal_iou", ">", "self", ".", "pem_low_temporal_iou_threshold", ")", "\n", ")", ".", "float", "(", ")", "\n", "u_lmask", "=", "(", "reference_temporal_iou", "<=", "\n", "self", ".", "pem_low_temporal_iou_threshold", ")", ".", "float", "(", ")", "\n", "\n", "num_h", "=", "torch", ".", "sum", "(", "u_hmask", ")", "\n", "num_m", "=", "torch", ".", "sum", "(", "u_mmask", ")", "\n", "num_l", "=", "torch", ".", "sum", "(", "u_lmask", ")", "\n", "\n", "r_m", "=", "self", ".", "u_ratio_m", "*", "num_h", "/", "(", "num_m", ")", "\n", "r_m", "=", "torch", ".", "min", "(", "r_m", ",", "torch", ".", "Tensor", "(", "[", "1.0", "]", ")", ".", "to", "(", "device", ")", ")", "[", "0", "]", "\n", "u_smmask", "=", "torch", ".", "rand", "(", "u_hmask", ".", "size", "(", ")", "[", "0", "]", ",", "device", "=", "device", ")", "\n", "u_smmask", "=", "u_smmask", "*", "u_mmask", "\n", "u_smmask", "=", "(", "u_smmask", ">", "(", "1.", "-", "r_m", ")", ")", ".", "float", "(", ")", "\n", "\n", "r_l", "=", "self", ".", "u_ratio_l", "*", "num_h", "/", "(", "num_l", ")", "\n", "r_l", "=", "torch", ".", "min", "(", "r_l", ",", "torch", ".", "Tensor", "(", "[", "1.0", "]", ")", ".", "to", "(", "device", ")", ")", "[", "0", "]", "\n", "u_slmask", "=", "torch", ".", "rand", "(", "u_hmask", ".", "size", "(", ")", "[", "0", "]", ",", "device", "=", "device", ")", "\n", "u_slmask", "=", "u_slmask", "*", "u_lmask", "\n", "u_slmask", "=", "(", "u_slmask", ">", "(", "1.", "-", "r_l", ")", ")", ".", "float", "(", ")", "\n", "\n", "temporal_iou_weights", "=", "u_hmask", "+", "u_smmask", "+", "u_slmask", "\n", "temporal_iou_loss", "=", "F", ".", "smooth_l1_loss", "(", "anchors_temporal_iou", ",", "\n", "reference_temporal_iou", ")", "\n", "temporal_iou_loss", "=", "torch", ".", "sum", "(", "\n", "temporal_iou_loss", "*", "\n", "temporal_iou_weights", ")", "/", "torch", ".", "sum", "(", "temporal_iou_weights", ")", "\n", "loss_dict", "=", "dict", "(", "temporal_iou_loss", "=", "temporal_iou_loss", ")", "\n", "\n", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test": [[353, 380], ["bsn.PEM._forward().view().cpu().numpy().reshape", "tmin.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy().reshape", "tmax.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy().reshape", "tmin_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy().reshape", "tmax_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy().reshape", "numpy.array().reshape", "numpy.concatenate", "result.reshape.reshape.reshape", "dict", "utils.post_processing", "dict", "bsn.PEM._forward().view().cpu().numpy", "tmin.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy", "tmax.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy", "tmin_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy", "tmax_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu().numpy", "numpy.array", "bsn.PEM._forward().view().cpu", "tmin.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu", "tmax.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu", "tmin_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu", "tmax_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view().cpu", "bsn.PEM._forward().view", "tmin.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view", "tmax.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view", "tmin_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view", "tmax_score.view().cpu().numpy().reshape.view().cpu().numpy().reshape.view", "bsn.PEM._forward"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.post_processing.post_processing", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], ["", "def", "forward_test", "(", "self", ",", "bsp_feature", ",", "tmin", ",", "tmax", ",", "tmin_score", ",", "tmax_score", ",", "\n", "video_meta", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call when testing.\"\"\"", "\n", "pem_output", "=", "self", ".", "_forward", "(", "bsp_feature", ")", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "\n", "-", "1", ",", "1", ")", "\n", "\n", "tmin", "=", "tmin", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "tmax", "=", "tmax", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "tmin_score", "=", "tmin_score", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "tmax_score", "=", "tmax_score", ".", "view", "(", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "score", "=", "np", ".", "array", "(", "pem_output", "*", "tmin_score", "*", "tmax_score", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "result", "=", "np", ".", "concatenate", "(", "\n", "(", "tmin", ",", "tmax", ",", "tmin_score", ",", "tmax_score", ",", "pem_output", ",", "score", ")", ",", "axis", "=", "1", ")", "\n", "result", "=", "result", ".", "reshape", "(", "-", "1", ",", "6", ")", "\n", "video_info", "=", "dict", "(", "video_meta", "[", "0", "]", ")", "\n", "proposal_list", "=", "post_processing", "(", "result", ",", "video_info", ",", "\n", "self", ".", "soft_nms_alpha", ",", "\n", "self", ".", "soft_nms_low_threshold", ",", "\n", "self", ".", "soft_nms_high_threshold", ",", "\n", "self", ".", "post_process_top_k", ",", "\n", "self", ".", "feature_extraction_interval", ")", "\n", "output", "=", "[", "\n", "dict", "(", "\n", "video_name", "=", "video_info", "[", "'video_name'", "]", ",", "\n", "proposal_list", "=", "proposal_list", ")", "\n", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward": [[381, 396], ["bsn.PEM.forward_test", "bsn.PEM.forward_train"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_test", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.bsn.PEM.forward_train"], ["", "def", "forward", "(", "self", ",", "\n", "bsp_feature", ",", "\n", "reference_temporal_iou", "=", "None", ",", "\n", "tmin", "=", "None", ",", "\n", "tmax", "=", "None", ",", "\n", "tmin_score", "=", "None", ",", "\n", "tmax_score", "=", "None", ",", "\n", "video_meta", "=", "None", ",", "\n", "return_loss", "=", "True", ")", ":", "\n", "        ", "\"\"\"Define the computation performed at every call.\"\"\"", "\n", "if", "return_loss", ":", "\n", "            ", "return", "self", ".", "forward_train", "(", "bsp_feature", ",", "reference_temporal_iou", ")", "\n", "\n", "", "return", "self", ".", "forward_test", "(", "bsp_feature", ",", "tmin", ",", "tmax", ",", "tmin_score", ",", "\n", "tmax_score", ",", "video_meta", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsn_head.TSNHead.__init__": [[26, 60], ["dict", "dict", "base.BaseHead.__init__", "consensus.copy", "consensus.copy.pop", "torch.Linear", "base.AvgConsensus", "torch.AdaptiveAvgPool2d", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "consensus", "=", "dict", "(", "type", "=", "'AvgConsensus'", ",", "dim", "=", "1", ")", ",", "\n", "dropout_ratio", "=", "0.4", ",", "\n", "init_std", "=", "0.01", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", "=", "loss_cls", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "consensus_", "=", "consensus", ".", "copy", "(", ")", "\n", "\n", "consensus_type", "=", "consensus_", ".", "pop", "(", "'type'", ")", "\n", "if", "consensus_type", "==", "'AvgConsensus'", ":", "\n", "            ", "self", ".", "consensus", "=", "AvgConsensus", "(", "**", "consensus_", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "consensus", "=", "None", "\n", "\n", "", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool2d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n", "", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsn_head.TSNHead.init_weights": [[61, 64], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsn_head.TSNHead.forward": [[65, 96], ["tsn_head.TSNHead.reshape", "tsn_head.TSNHead.consensus", "tsn_head.TSNHead.squeeze", "tsn_head.TSNHead.view", "tsn_head.TSNHead.fc_cls", "isinstance", "tsn_head.TSNHead.avg_pool", "tsn_head.TSNHead.dropout", "tsn_head.TSNHead.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "num_segs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            num_segs (int): Number of segments into which a video\n                is divided.\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N * num_segs, in_channels, 7, 7]", "\n", "if", "self", ".", "avg_pool", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "x", ",", "tuple", ")", ":", "\n", "                ", "shapes", "=", "[", "y", ".", "shape", "for", "y", "in", "x", "]", "\n", "assert", "1", "==", "0", ",", "f'x is tuple {shapes}'", "\n", "", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "# [N * num_segs, in_channels, 1, 1]", "\n", "", "x", "=", "x", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "x", ".", "shape", "[", "1", ":", "]", ")", "\n", "# [N, num_segs, in_channels, 1, 1]", "\n", "x", "=", "self", ".", "consensus", "(", "x", ")", "\n", "# [N, 1, in_channels, 1, 1]", "\n", "x", "=", "x", ".", "squeeze", "(", "1", ")", "\n", "# [N, in_channels, 1, 1]", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "# [N, in_channels, 1, 1]", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "# [N, in_channels]", "\n", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.audio_tsn_head.AudioTSNHead.__init__": [[25, 50], ["dict", "base.BaseHead.__init__", "torch.Linear", "torch.AdaptiveAvgPool2d", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "dropout_ratio", "=", "0.4", ",", "\n", "init_std", "=", "0.01", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", "=", "loss_cls", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool2d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n", "", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.audio_tsn_head.AudioTSNHead.init_weights": [[51, 54], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.audio_tsn_head.AudioTSNHead.forward": [[55, 75], ["audio_tsn_head.AudioTSNHead.avg_pool", "audio_tsn_head.AudioTSNHead.view", "audio_tsn_head.AudioTSNHead.fc_cls", "audio_tsn_head.AudioTSNHead.size", "audio_tsn_head.AudioTSNHead.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N * num_segs, in_channels, h, w]", "\n", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "# [N, in_channels, 1, 1]", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "# [N, in_channels]", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "# [N, in_channels]", "\n", "", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tpn_head.TPNHead.__init__": [[27, 38], ["tsn_head.TSNHead.__init__", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool3d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool3d", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool3d", "=", "None", "\n", "\n", "", "self", ".", "avg_pool2d", "=", "None", "\n", "self", ".", "new_cls", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tpn_head.TPNHead._init_new_cls": [[39, 45], ["torch.Conv3d", "tpn_head.TPNHead.new_cls.weight.copy_", "tpn_head.TPNHead.new_cls.bias.copy_", "next", "tpn_head.TPNHead.new_cls.cuda", "tpn_head.TPNHead.fc_cls.parameters"], "methods", ["None"], ["", "def", "_init_new_cls", "(", "self", ")", ":", "\n", "        ", "self", ".", "new_cls", "=", "nn", ".", "Conv3d", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ",", "1", ",", "1", ",", "0", ")", "\n", "if", "next", "(", "self", ".", "fc_cls", ".", "parameters", "(", ")", ")", ".", "is_cuda", ":", "\n", "            ", "self", ".", "new_cls", "=", "self", ".", "new_cls", ".", "cuda", "(", ")", "\n", "", "self", ".", "new_cls", ".", "weight", ".", "copy_", "(", "self", ".", "fc_cls", ".", "weight", "[", "...", ",", "None", ",", "None", ",", "None", "]", ")", "\n", "self", ".", "new_cls", ".", "bias", ".", "copy_", "(", "self", ".", "fc_cls", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tpn_head.TPNHead.forward": [[46, 92], ["tpn_head.TPNHead.view", "tpn_head.TPNHead.fc_cls", "tpn_head.TPNHead.new_cls", "torch.AvgPool3d", "tpn_head.TPNHead.avg_pool3d", "tpn_head.TPNHead.avg_pool2d", "tpn_head.TPNHead.reshape", "tpn_head.TPNHead.consensus", "tpn_head.TPNHead.squeeze", "tpn_head.TPNHead.dropout", "tpn_head.TPNHead.size", "tpn_head.TPNHead.avg_pool3d", "tpn_head.TPNHead._init_new_cls"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tpn_head.TPNHead._init_new_cls"], ["", "def", "forward", "(", "self", ",", "x", ",", "num_segs", "=", "None", ",", "fcn_test", "=", "False", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            num_segs (int | None): Number of segments into which a video\n                is divided. Default: None.\n            fcn_test (bool): Whether to apply full convolution (fcn) testing.\n                Default: False.\n\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "if", "fcn_test", ":", "\n", "            ", "if", "self", ".", "avg_pool3d", ":", "\n", "                ", "x", "=", "self", ".", "avg_pool3d", "(", "x", ")", "\n", "", "if", "self", ".", "new_cls", "is", "None", ":", "\n", "                ", "self", ".", "_init_new_cls", "(", ")", "\n", "", "cls_score_feat_map", "=", "self", ".", "new_cls", "(", "x", ")", "\n", "return", "cls_score_feat_map", "\n", "\n", "", "if", "self", ".", "avg_pool2d", "is", "None", ":", "\n", "            ", "kernel_size", "=", "(", "1", ",", "x", ".", "shape", "[", "-", "2", "]", ",", "x", ".", "shape", "[", "-", "1", "]", ")", "\n", "self", ".", "avg_pool2d", "=", "nn", ".", "AvgPool3d", "(", "kernel_size", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", "\n", "\n", "", "if", "num_segs", "is", "None", ":", "\n", "# [N, in_channels, 3, 7, 7]", "\n", "            ", "x", "=", "self", ".", "avg_pool3d", "(", "x", ")", "\n", "", "else", ":", "\n", "# [N * num_segs, in_channels, 7, 7]", "\n", "            ", "x", "=", "self", ".", "avg_pool2d", "(", "x", ")", "\n", "# [N * num_segs, in_channels, 1, 1]", "\n", "x", "=", "x", ".", "reshape", "(", "(", "-", "1", ",", "num_segs", ")", "+", "x", ".", "shape", "[", "1", ":", "]", ")", "\n", "# [N, num_segs, in_channels, 1, 1]", "\n", "x", "=", "self", ".", "consensus", "(", "x", ")", "\n", "# [N, 1, in_channels, 1, 1]", "\n", "x", "=", "x", ".", "squeeze", "(", "1", ")", "\n", "# [N, in_channels, 1, 1]", "\n", "", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "# [N, in_channels, 1, 1]", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "# [N, in_channels]", "\n", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.AvgConsensus.__init__": [[19, 22], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["class", "BaseDataset", "(", "Dataset", ",", "metaclass", "=", "ABCMeta", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.AvgConsensus.forward": [[23, 26], ["x.mean"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.BaseHead.__init__": [[47, 59], ["dict", "torch.Module.__init__", "builder.build_loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.builder.build_loss"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.BaseHead.init_weights": [[60, 64], ["None"], "methods", ["None"], ["ann_file", ",", "\n", "pipeline", ",", "\n", "data_prefix", "=", "None", ",", "\n", "test_mode", "=", "False", ",", "\n", "multi_class", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.BaseHead.forward": [[65, 68], ["None"], "methods", ["None"], ["num_classes", "=", "None", ",", "\n", "start_index", "=", "1", ",", "\n", "modality", "=", "'RGB'", ",", "\n", "sample_by_class", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.base.BaseHead.loss": [[69, 110], ["dict", "base.BaseHead.loss_cls", "isinstance", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "labels.unsqueeze.unsqueeze.unsqueeze", "core.top_k_accuracy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "dict.update", "labels.unsqueeze.unsqueeze.unsqueeze", "cls_score.size", "labels.unsqueeze.unsqueeze.size", "cls_score.detach().cpu().numpy", "labels.unsqueeze.unsqueeze.detach().cpu().numpy", "labels.unsqueeze.unsqueeze.dim", "labels.unsqueeze.unsqueeze.size", "cls_score.size", "cls_score.detach().cpu", "labels.unsqueeze.unsqueeze.detach().cpu", "cls_score.detach", "labels.unsqueeze.unsqueeze.detach"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.top_k_accuracy", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["power", "=", "0", ",", "\n", "dynamic_length", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ann_file", "=", "ann_file", "\n", "self", ".", "data_prefix", "=", "osp", ".", "realpath", "(", "\n", "data_prefix", ")", "if", "data_prefix", "is", "not", "None", "and", "osp", ".", "isdir", "(", "\n", "data_prefix", ")", "else", "data_prefix", "\n", "self", ".", "test_mode", "=", "test_mode", "\n", "self", ".", "multi_class", "=", "multi_class", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "start_index", "=", "start_index", "\n", "self", ".", "modality", "=", "modality", "\n", "self", ".", "sample_by_class", "=", "sample_by_class", "\n", "self", ".", "power", "=", "power", "\n", "self", ".", "dynamic_length", "=", "dynamic_length", "\n", "\n", "assert", "not", "(", "self", ".", "multi_class", "and", "self", ".", "sample_by_class", ")", "\n", "\n", "self", ".", "pipeline", "=", "Compose", "(", "pipeline", ")", "\n", "self", ".", "video_infos", "=", "self", ".", "load_annotations", "(", ")", "\n", "if", "self", ".", "sample_by_class", ":", "\n", "            ", "self", ".", "video_infos_by_class", "=", "self", ".", "parse_by_class", "(", ")", "\n", "\n", "class_prob", "=", "[", "]", "\n", "for", "_", ",", "samples", "in", "self", ".", "video_infos_by_class", ".", "items", "(", ")", ":", "\n", "                ", "class_prob", ".", "append", "(", "len", "(", "samples", ")", "/", "len", "(", "self", ".", "video_infos", ")", ")", "\n", "", "class_prob", "=", "[", "x", "**", "self", ".", "power", "for", "x", "in", "class_prob", "]", "\n", "\n", "summ", "=", "sum", "(", "class_prob", ")", "\n", "class_prob", "=", "[", "x", "/", "summ", "for", "x", "in", "class_prob", "]", "\n", "\n", "self", ".", "class_prob", "=", "dict", "(", "zip", "(", "self", ".", "video_infos_by_class", ",", "class_prob", ")", ")", "\n", "\n", "", "", "@", "abstractmethod", "\n", "def", "load_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the annotation according to ann_file into video_infos.\"\"\"", "\n", "\n", "# json annotations already looks like video_infos, so for each dataset,", "\n", "# this func should be the same", "\n", "", "def", "load_json_annotations", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load json annotation file to get video information.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.slowfast_head.SlowFastHead.__init__": [[26, 50], ["dict", "base.BaseHead.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "dropout_ratio", "=", "0.8", ",", "\n", "init_std", "=", "0.01", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ",", "**", "kwargs", ")", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "in_channels", ",", "num_classes", ")", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.slowfast_head.SlowFastHead.init_weights": [[51, 54], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.slowfast_head.SlowFastHead.forward": [[55, 81], ["slowfast_head.SlowFastHead.avg_pool", "slowfast_head.SlowFastHead.avg_pool", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "slowfast_head.SlowFastHead.view", "slowfast_head.SlowFastHead.fc_cls", "slowfast_head.SlowFastHead.dropout", "slowfast_head.SlowFastHead.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# ([N, channel_fast, T, H, W], [(N, channel_slow, T, H, W)])", "\n", "x_fast", ",", "x_slow", "=", "x", "\n", "# ([N, channel_fast, 1, 1, 1], [N, channel_slow, 1, 1, 1])", "\n", "x_fast", "=", "self", ".", "avg_pool", "(", "x_fast", ")", "\n", "x_slow", "=", "self", ".", "avg_pool", "(", "x_slow", ")", "\n", "# [N, channel_fast + channel_slow, 1, 1, 1]", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x_slow", ",", "x_fast", ")", ",", "dim", "=", "1", ")", "\n", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "# [N x C]", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "# [N x num_classes]", "\n", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsm_head.TSMHead.__init__": [[32, 72], ["dict", "dict", "base.BaseHead.__init__", "consensus.copy", "consensus.copy.pop", "torch.Linear", "torch.Linear", "base.AvgConsensus", "torch.Dropout", "torch.Dropout", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "num_segments", "=", "8", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "consensus", "=", "dict", "(", "type", "=", "'AvgConsensus'", ",", "dim", "=", "1", ")", ",", "\n", "dropout_ratio", "=", "0.8", ",", "\n", "init_std", "=", "0.001", ",", "\n", "is_shift", "=", "True", ",", "\n", "temporal_pool", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "init_std", "=", "init_std", "\n", "self", ".", "is_shift", "=", "is_shift", "\n", "self", ".", "temporal_pool", "=", "temporal_pool", "\n", "\n", "consensus_", "=", "consensus", ".", "copy", "(", ")", "\n", "\n", "consensus_type", "=", "consensus_", ".", "pop", "(", "'type'", ")", "\n", "if", "consensus_type", "==", "'AvgConsensus'", ":", "\n", "            ", "self", ".", "consensus", "=", "AvgConsensus", "(", "**", "consensus_", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "consensus", "=", "None", "\n", "\n", "", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ")", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool2d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsm_head.TSMHead.init_weights": [[73, 76], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.tsm_head.TSMHead.forward": [[77, 113], ["torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "tsm_head.TSMHead.fc_cls", "tsm_head.TSMHead.consensus", "cls_score.view.view.squeeze", "tsm_head.TSMHead.avg_pool", "tsm_head.TSMHead.dropout", "cls_score.view.view.view", "cls_score.view.view.view", "cls_score.view.view.size", "cls_score.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "num_segs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            num_segs (int): Useless in TSMHead. By default, `num_segs`\n                is equal to `clip_len * num_clips * num_crops`, which is\n                automatically generated in Recognizer forward phase and\n                useless in TSM models. The `self.num_segments` we need is a\n                hyper parameter to build TSM models.\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N * num_segs, in_channels, 7, 7]", "\n", "if", "self", ".", "avg_pool", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "# [N * num_segs, in_channels, 1, 1]", "\n", "", "x", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "# [N * num_segs, in_channels]", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "# [N * num_segs, num_classes]", "\n", "", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "\n", "if", "self", ".", "is_shift", "and", "self", ".", "temporal_pool", ":", "\n", "# [2 * N, num_segs // 2, num_classes]", "\n", "            ", "cls_score", "=", "cls_score", ".", "view", "(", "(", "-", "1", ",", "self", ".", "num_segments", "//", "2", ")", "+", "\n", "cls_score", ".", "size", "(", ")", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "# [N, num_segs, num_classes]", "\n", "            ", "cls_score", "=", "cls_score", ".", "view", "(", "(", "-", "1", ",", "self", ".", "num_segments", ")", "+", "\n", "cls_score", ".", "size", "(", ")", "[", "1", ":", "]", ")", "\n", "# [N, 1, num_classes]", "\n", "", "cls_score", "=", "self", ".", "consensus", "(", "cls_score", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", ".", "squeeze", "(", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.lfb_infer_head.LFBInferHead.__init__": [[34, 67], ["torch.Module.__init__", "mmcv.runner.get_dist_info", "print", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "os.exists", "print", "mmcv.mkdir_or_exist"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "lfb_prefix_path", ",", "\n", "dataset_mode", "=", "'train'", ",", "\n", "use_half_precision", "=", "True", ",", "\n", "temporal_pool_type", "=", "'avg'", ",", "\n", "spatial_pool_type", "=", "'max'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "rank", ",", "_", "=", "get_dist_info", "(", ")", "\n", "if", "rank", "==", "0", ":", "\n", "            ", "if", "not", "osp", ".", "exists", "(", "lfb_prefix_path", ")", ":", "\n", "                ", "print", "(", "f'lfb prefix path {lfb_prefix_path} does not exist. '", "\n", "f'Creating the folder...'", ")", "\n", "mmcv", ".", "mkdir_or_exist", "(", "lfb_prefix_path", ")", "\n", "", "print", "(", "'\\nInferring LFB...'", ")", "\n", "\n", "", "assert", "temporal_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "assert", "spatial_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "self", ".", "lfb_prefix_path", "=", "lfb_prefix_path", "\n", "self", ".", "dataset_mode", "=", "dataset_mode", "\n", "self", ".", "use_half_precision", "=", "use_half_precision", "\n", "\n", "# Pool by default", "\n", "if", "temporal_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "if", "spatial_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "\n", "", "self", ".", "all_features", "=", "[", "]", "\n", "self", ".", "all_metadata", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.lfb_infer_head.LFBInferHead.init_weights": [[68, 71], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "# LFBInferHead has no parameters to be initialized.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.lfb_infer_head.LFBInferHead.forward": [[72, 86], ["lfb_infer_head.LFBInferHead.temporal_pool", "lfb_infer_head.LFBInferHead.spatial_pool", "rois[].type", "list", "features.half.half.half", "lfb_infer_head.LFBInferHead.all_metadata.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "rois", ",", "img_metas", ",", "**", "kwargs", ")", ":", "\n", "# [N, C, 1, 1, 1]", "\n", "        ", "features", "=", "self", ".", "temporal_pool", "(", "x", ")", "\n", "features", "=", "self", ".", "spatial_pool", "(", "features", ")", "\n", "if", "self", ".", "use_half_precision", ":", "\n", "            ", "features", "=", "features", ".", "half", "(", ")", "\n", "\n", "", "inds", "=", "rois", "[", ":", ",", "0", "]", ".", "type", "(", "torch", ".", "int64", ")", "\n", "for", "ind", "in", "inds", ":", "\n", "            ", "self", ".", "all_metadata", ".", "append", "(", "img_metas", "[", "ind", "]", "[", "'img_key'", "]", ")", "\n", "", "self", ".", "all_features", "+=", "list", "(", "features", ")", "\n", "\n", "# Return the input directly and doesn't affect the input.", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.lfb_infer_head.LFBInferHead.__del__": [[87, 143], ["mmcv.runner.get_dist_info", "zip", "os.normpath", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "print", "range", "os.normpath", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "len", "len", "torch.barrier", "torch.barrier", "torch.barrier", "metadata.split", "int", "[].append", "os.join", "torch.barrier", "torch.barrier", "torch.barrier", "os.normpath", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "os.join", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "os.join", "len", "len", "lfb[].update"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "all_features", ")", "==", "len", "(", "self", ".", "all_metadata", ")", ",", "(", "\n", "'features and metadata are not equal in length!'", ")", "\n", "\n", "rank", ",", "world_size", "=", "get_dist_info", "(", ")", "\n", "if", "world_size", ">", "1", ":", "\n", "            ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "_lfb", "=", "{", "}", "\n", "for", "feature", ",", "metadata", "in", "zip", "(", "self", ".", "all_features", ",", "self", ".", "all_metadata", ")", ":", "\n", "            ", "video_id", ",", "timestamp", "=", "metadata", ".", "split", "(", "','", ")", "\n", "timestamp", "=", "int", "(", "timestamp", ")", "\n", "\n", "if", "video_id", "not", "in", "_lfb", ":", "\n", "                ", "_lfb", "[", "video_id", "]", "=", "{", "}", "\n", "", "if", "timestamp", "not", "in", "_lfb", "[", "video_id", "]", ":", "\n", "                ", "_lfb", "[", "video_id", "]", "[", "timestamp", "]", "=", "[", "]", "\n", "\n", "", "_lfb", "[", "video_id", "]", "[", "timestamp", "]", ".", "append", "(", "torch", ".", "squeeze", "(", "feature", ")", ")", "\n", "\n", "", "_lfb_file_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "\n", "f'_lfb_{self.dataset_mode}_{rank}.pkl'", ")", ")", "\n", "torch", ".", "save", "(", "_lfb", ",", "_lfb_file_path", ")", "\n", "print", "(", "f'{len(self.all_features)} features from {len(_lfb)} videos '", "\n", "f'on GPU {rank} have been stored in {_lfb_file_path}.'", ")", "\n", "\n", "# Synchronizes all processes to make sure all gpus have stored their", "\n", "# roi features", "\n", "if", "world_size", ">", "1", ":", "\n", "            ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "rank", ">", "0", ":", "\n", "            ", "return", "\n", "\n", "", "print", "(", "'Gathering all the roi features...'", ")", "\n", "\n", "lfb", "=", "{", "}", "\n", "for", "rank_id", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "_lfb_file_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "\n", "f'_lfb_{self.dataset_mode}_{rank_id}.pkl'", ")", ")", "\n", "\n", "# Since each frame will only be distributed to one GPU,", "\n", "# the roi features on the same timestamp of the same video are all", "\n", "# on the same GPU", "\n", "_lfb", "=", "torch", ".", "load", "(", "_lfb_file_path", ")", "\n", "for", "video_id", "in", "_lfb", ":", "\n", "                ", "if", "video_id", "not", "in", "lfb", ":", "\n", "                    ", "lfb", "[", "video_id", "]", "=", "_lfb", "[", "video_id", "]", "\n", "", "else", ":", "\n", "                    ", "lfb", "[", "video_id", "]", ".", "update", "(", "_lfb", "[", "video_id", "]", ")", "\n", "\n", "", "", "", "lfb_file_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "f'lfb_{self.dataset_mode}.pkl'", ")", ")", "\n", "torch", ".", "save", "(", "lfb", ",", "lfb_file_path", ")", "\n", "print", "(", "f'LFB has been constructed in {lfb_file_path}!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.__init__": [[42, 106], ["torch.Module.__init__", "all", "torch.Linear", "torch.Linear", "torch.Linear", "isinstance", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "isinstance", "all", "TypeError", "isinstance", "type"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "temporal_pool_type", "=", "'avg'", ",", "\n", "spatial_pool_type", "=", "'max'", ",", "\n", "in_channels", "=", "2048", ",", "\n", "# The first class is reserved, to classify bbox as pos / neg", "\n", "focal_gamma", "=", "0.", ",", "\n", "focal_alpha", "=", "1.", ",", "\n", "num_classes", "=", "81", ",", "\n", "dropout_ratio", "=", "0", ",", "\n", "dropout_before_pool", "=", "True", ",", "\n", "topk", "=", "(", "3", ",", "5", ")", ",", "\n", "multilabel", "=", "True", ")", ":", "\n", "\n", "        ", "super", "(", "BBoxHeadAVA", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "temporal_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "assert", "spatial_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "self", ".", "temporal_pool_type", "=", "temporal_pool_type", "\n", "self", ".", "spatial_pool_type", "=", "spatial_pool_type", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "dropout_before_pool", "=", "dropout_before_pool", "\n", "\n", "self", ".", "multilabel", "=", "multilabel", "\n", "\n", "self", ".", "focal_gamma", "=", "focal_gamma", "\n", "self", ".", "focal_alpha", "=", "focal_alpha", "\n", "\n", "if", "topk", "is", "None", ":", "\n", "            ", "self", ".", "topk", "=", "(", ")", "\n", "", "elif", "isinstance", "(", "topk", ",", "int", ")", ":", "\n", "            ", "self", ".", "topk", "=", "(", "topk", ",", ")", "\n", "", "elif", "isinstance", "(", "topk", ",", "tuple", ")", ":", "\n", "            ", "assert", "all", "(", "[", "isinstance", "(", "k", ",", "int", ")", "for", "k", "in", "topk", "]", ")", "\n", "self", ".", "topk", "=", "topk", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'topk should be int or tuple[int], '", "\n", "f'but get {type(topk)}'", ")", "\n", "# Class 0 is ignored when calculaing multilabel accuracy,", "\n", "# so topk cannot be equal to num_classes", "\n", "", "assert", "all", "(", "[", "k", "<", "num_classes", "for", "k", "in", "self", ".", "topk", "]", ")", "\n", "\n", "# Handle AVA first", "\n", "assert", "self", ".", "multilabel", "\n", "\n", "in_channels", "=", "self", ".", "in_channels", "\n", "# Pool by default", "\n", "if", "self", ".", "temporal_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "if", "self", ".", "spatial_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "\n", "", "if", "dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_ratio", ")", "\n", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "in_channels", ",", "num_classes", ")", "\n", "self", ".", "debug_imgs", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.init_weights": [[107, 110], ["torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "normal_", "(", "self", ".", "fc_cls", ".", "weight", ",", "0", ",", "0.01", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "fc_cls", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.forward": [[111, 125], ["bbox_head.BBoxHeadAVA.temporal_pool", "bbox_head.BBoxHeadAVA.spatial_pool", "bbox_head.BBoxHeadAVA.view", "bbox_head.BBoxHeadAVA.fc_cls", "bbox_head.BBoxHeadAVA.dropout", "bbox_head.BBoxHeadAVA.dropout", "bbox_head.BBoxHeadAVA.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "dropout_before_pool", "and", "self", ".", "dropout_ratio", ">", "0", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "", "x", "=", "self", ".", "temporal_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "spatial_pool", "(", "x", ")", "\n", "\n", "if", "not", "self", ".", "dropout_before_pool", "and", "self", ".", "dropout_ratio", ">", "0", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# We do not predict bbox, so return None", "\n", "return", "cls_score", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.get_targets": [[126, 134], ["mmaction.core.bbox.bbox_target"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.bbox.bbox_target.bbox_target"], ["", "@", "staticmethod", "\n", "def", "get_targets", "(", "sampling_results", ",", "gt_bboxes", ",", "gt_labels", ",", "rcnn_train_cfg", ")", ":", "\n", "        ", "pos_proposals", "=", "[", "res", ".", "pos_bboxes", "for", "res", "in", "sampling_results", "]", "\n", "neg_proposals", "=", "[", "res", ".", "neg_bboxes", "for", "res", "in", "sampling_results", "]", "\n", "pos_gt_labels", "=", "[", "res", ".", "pos_gt_labels", "for", "res", "in", "sampling_results", "]", "\n", "cls_reg_targets", "=", "bbox_target", "(", "pos_proposals", ",", "neg_proposals", ",", "\n", "pos_gt_labels", ",", "rcnn_train_cfg", ")", "\n", "return", "cls_reg_targets", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.recall_prec": [[135, 148], ["correct.sum", "target_vec.sum().float", "correct.sum", "recall.mean", "prec.mean", "pred_vec.sum", "target_vec.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "recall_prec", "(", "pred_vec", ",", "target_vec", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            pred_vec (tensor[N x C]): each element is either 0 or 1\n            target_vec (tensor[N x C]): each element is either 0 or 1\n\n        \"\"\"", "\n", "correct", "=", "pred_vec", "&", "target_vec", "\n", "# Seems torch 1.5 has no auto type conversion", "\n", "recall", "=", "correct", ".", "sum", "(", "1", ")", "/", "target_vec", ".", "sum", "(", "1", ")", ".", "float", "(", ")", "\n", "prec", "=", "correct", ".", "sum", "(", "1", ")", "/", "(", "pred_vec", ".", "sum", "(", "1", ")", "+", "1e-6", ")", "\n", "return", "recall", ".", "mean", "(", ")", ",", "prec", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.multi_label_accuracy": [[149, 168], ["pred.sigmoid.sigmoid.sigmoid", "bbox_head.BBoxHeadAVA.recall_prec", "pred.sigmoid.sigmoid.topk", "pred.sigmoid.sigmoid.new_full", "range", "bbox_head.BBoxHeadAVA.recall_prec", "recalls.append", "precs.append", "pred.sigmoid.sigmoid.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.recall_prec", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.recall_prec"], ["", "def", "multi_label_accuracy", "(", "self", ",", "pred", ",", "target", ",", "thr", "=", "0.5", ")", ":", "\n", "        ", "pred", "=", "pred", ".", "sigmoid", "(", ")", "\n", "pred_vec", "=", "pred", ">", "thr", "\n", "# Target is 0 or 1, so using 0.5 as the borderline is OK", "\n", "target_vec", "=", "target", ">", "0.5", "\n", "recall_thr", ",", "prec_thr", "=", "self", ".", "recall_prec", "(", "pred_vec", ",", "target_vec", ")", "\n", "\n", "recalls", ",", "precs", "=", "[", "]", ",", "[", "]", "\n", "for", "k", "in", "self", ".", "topk", ":", "\n", "            ", "_", ",", "pred_label", "=", "pred", ".", "topk", "(", "k", ",", "1", ",", "True", ",", "True", ")", "\n", "pred_vec", "=", "pred", ".", "new_full", "(", "pred", ".", "size", "(", ")", ",", "0", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "\n", "num_sample", "=", "pred", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "num_sample", ")", ":", "\n", "                ", "pred_vec", "[", "i", ",", "pred_label", "[", "i", "]", "]", "=", "1", "\n", "", "recall_k", ",", "prec_k", "=", "self", ".", "recall_prec", "(", "pred_vec", ",", "target_vec", ")", "\n", "recalls", ".", "append", "(", "recall_k", ")", "\n", "precs", ".", "append", "(", "prec_k", ")", "\n", "", "return", "recall_thr", ",", "prec_thr", ",", "recalls", ",", "precs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.loss": [[169, 202], ["dict", "bce_loss", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "bbox_head.BBoxHeadAVA.multi_label_accuracy", "enumerate", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.multi_label_accuracy"], ["", "def", "loss", "(", "self", ",", "\n", "cls_score", ",", "\n", "bbox_pred", ",", "\n", "rois", ",", "\n", "labels", ",", "\n", "label_weights", ",", "\n", "bbox_targets", "=", "None", ",", "\n", "bbox_weights", "=", "None", ",", "\n", "reduce", "=", "True", ")", ":", "\n", "\n", "        ", "losses", "=", "dict", "(", ")", "\n", "if", "cls_score", "is", "not", "None", ":", "\n", "# Only use the cls_score", "\n", "            ", "labels", "=", "labels", "[", ":", ",", "1", ":", "]", "\n", "pos_inds", "=", "torch", ".", "sum", "(", "labels", ",", "dim", "=", "-", "1", ")", ">", "0", "\n", "cls_score", "=", "cls_score", "[", "pos_inds", ",", "1", ":", "]", "\n", "labels", "=", "labels", "[", "pos_inds", "]", "\n", "\n", "bce_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "\n", "\n", "loss", "=", "bce_loss", "(", "cls_score", ",", "labels", ",", "reduction", "=", "'none'", ")", "\n", "pt", "=", "torch", ".", "exp", "(", "-", "loss", ")", "\n", "F_loss", "=", "self", ".", "focal_alpha", "*", "(", "1", "-", "pt", ")", "**", "self", ".", "focal_gamma", "*", "loss", "\n", "losses", "[", "'loss_action_cls'", "]", "=", "torch", ".", "mean", "(", "F_loss", ")", "\n", "\n", "recall_thr", ",", "prec_thr", ",", "recall_k", ",", "prec_k", "=", "self", ".", "multi_label_accuracy", "(", "\n", "cls_score", ",", "labels", ",", "thr", "=", "0.5", ")", "\n", "losses", "[", "'recall@thr=0.5'", "]", "=", "recall_thr", "\n", "losses", "[", "'prec@thr=0.5'", "]", "=", "prec_thr", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "self", ".", "topk", ")", ":", "\n", "                ", "losses", "[", "f'recall@top{k}'", "]", "=", "recall_k", "[", "i", "]", "\n", "losses", "[", "f'prec@top{k}'", "]", "=", "prec_k", "[", "i", "]", "\n", "", "", "return", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.bbox_head.BBoxHeadAVA.get_det_bboxes": [[203, 245], ["isinstance", "bbox_head.BBoxHeadAVA.get_det_bboxes._bbox_crop_undo"], "methods", ["None"], ["", "def", "get_det_bboxes", "(", "self", ",", "\n", "rois", ",", "\n", "cls_score", ",", "\n", "img_shape", ",", "\n", "flip", "=", "False", ",", "\n", "crop_quadruple", "=", "None", ",", "\n", "cfg", "=", "None", ")", ":", "\n", "\n", "# might be used by testing w. augmentation", "\n", "        ", "if", "isinstance", "(", "cls_score", ",", "list", ")", ":", "\n", "            ", "cls_score", "=", "sum", "(", "cls_score", ")", "/", "float", "(", "len", "(", "cls_score", ")", ")", "\n", "\n", "", "assert", "self", ".", "multilabel", "\n", "\n", "scores", "=", "cls_score", ".", "sigmoid", "(", ")", "if", "cls_score", "is", "not", "None", "else", "None", "\n", "bboxes", "=", "rois", "[", ":", ",", "1", ":", "]", "\n", "assert", "bboxes", ".", "shape", "[", "-", "1", "]", "==", "4", "\n", "\n", "# First reverse the flip", "\n", "img_h", ",", "img_w", "=", "img_shape", "\n", "if", "flip", ":", "\n", "            ", "bboxes_", "=", "bboxes", ".", "clone", "(", ")", "\n", "bboxes_", "[", ":", ",", "0", "]", "=", "img_w", "-", "1", "-", "bboxes", "[", ":", ",", "2", "]", "\n", "bboxes_", "[", ":", ",", "2", "]", "=", "img_w", "-", "1", "-", "bboxes", "[", ":", ",", "0", "]", "\n", "bboxes", "=", "bboxes_", "\n", "\n", "# Then normalize the bbox to [0, 1]", "\n", "", "bboxes", "[", ":", ",", "0", ":", ":", "2", "]", "/=", "img_w", "\n", "bboxes", "[", ":", ",", "1", ":", ":", "2", "]", "/=", "img_h", "\n", "\n", "def", "_bbox_crop_undo", "(", "bboxes", ",", "crop_quadruple", ")", ":", "\n", "            ", "decropped", "=", "bboxes", ".", "clone", "(", ")", "\n", "\n", "if", "crop_quadruple", "is", "not", "None", ":", "\n", "                ", "x1", ",", "y1", ",", "tw", ",", "th", "=", "crop_quadruple", "\n", "decropped", "[", ":", ",", "0", ":", ":", "2", "]", "=", "bboxes", "[", "...", ",", "0", ":", ":", "2", "]", "*", "tw", "+", "x1", "\n", "decropped", "[", ":", ",", "1", ":", ":", "2", "]", "=", "bboxes", "[", "...", ",", "1", ":", ":", "2", "]", "*", "th", "+", "y1", "\n", "\n", "", "return", "decropped", "\n", "\n", "", "bboxes", "=", "_bbox_crop_undo", "(", "bboxes", ",", "crop_quadruple", ")", "\n", "return", "bboxes", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.timesformer_head.TimeSformerHead.__init__": [[23, 32], ["dict", "base.BaseHead.__init__", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "init_std", "=", "0.02", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ",", "**", "kwargs", ")", "\n", "self", ".", "init_std", "=", "init_std", "\n", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.timesformer_head.TimeSformerHead.init_weights": [[33, 36], ["mmcv.cnn.trunc_normal_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "trunc_normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.timesformer_head.TimeSformerHead.forward": [[37, 42], ["timesformer_head.TimeSformerHead.fc_cls"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# [N, in_channels]", "\n", "        ", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.misc_head.ACRNHead.__init__": [[37, 93], ["dict", "dict", "dict", "torch.Module.__init__", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "range", "torch.ModuleList", "torch.ModuleList", "mmcv.cnn.ConvModule", "convs.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "stride", "=", "1", ",", "\n", "num_convs", "=", "1", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "num_convs", "=", "num_convs", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "max_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "1", ")", "\n", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "\n", "assert", "num_convs", ">=", "1", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", ",", "stride", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "\n", "convs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", ":", "\n", "            ", "conv", "=", "ConvModule", "(", "\n", "out_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "convs", ".", "append", "(", "conv", ")", "\n", "", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "convs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.misc_head.ACRNHead.init_weights": [[94, 101], ["misc_head.ACRNHead.modules", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Weight Initialization for ACRNHead.\"\"\"", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.misc_head.ACRNHead.forward": [[102, 131], ["misc_head.ACRNHead.max_pool", "misc_head.ACRNHead.repeat", "rois[].type", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "misc_head.ACRNHead.conv1", "misc_head.ACRNHead.conv2", "conv"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ",", "feat", ",", "rois", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The extracted RoI feature.\n            feat (torch.Tensor): The context feature.\n            rois (torch.Tensor): The regions of interest.\n\n        Returns:\n            torch.Tensor: The RoI features that have interacted with context\n                feature.\n        \"\"\"", "\n", "# We use max pooling by default", "\n", "x", "=", "self", ".", "max_pool", "(", "x", ")", "\n", "\n", "h", ",", "w", "=", "feat", ".", "shape", "[", "-", "2", ":", "]", "\n", "x_tile", "=", "x", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "h", ",", "w", ")", "\n", "\n", "roi_inds", "=", "rois", "[", ":", ",", "0", "]", ".", "type", "(", "torch", ".", "long", ")", "\n", "roi_gfeat", "=", "feat", "[", "roi_inds", "]", "\n", "\n", "new_feat", "=", "torch", ".", "cat", "(", "[", "x_tile", ",", "roi_gfeat", "]", ",", "dim", "=", "1", ")", "\n", "new_feat", "=", "self", ".", "conv1", "(", "new_feat", ")", "\n", "new_feat", "=", "self", ".", "conv2", "(", "new_feat", ")", "\n", "\n", "for", "conv", "in", "self", ".", "convs", ":", "\n", "            ", "new_feat", "=", "conv", "(", "new_feat", ")", "\n", "\n", "", "return", "new_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.NonLocalLayer.__init__": [[43, 111], ["torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.ReLU", "torch.ReLU", "mmcv.cnn.ConvModule", "dict", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "st_feat_channels", ",", "\n", "lt_feat_channels", ",", "\n", "latent_channels", ",", "\n", "num_st_feat", ",", "\n", "num_lt_feat", ",", "\n", "use_scale", "=", "True", ",", "\n", "pre_activate", "=", "True", ",", "\n", "pre_activate_with_ln", "=", "True", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "dropout_ratio", "=", "0.2", ",", "\n", "zero_init_out_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "conv_cfg", "is", "None", ":", "\n", "            ", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", "\n", "", "self", ".", "st_feat_channels", "=", "st_feat_channels", "\n", "self", ".", "lt_feat_channels", "=", "lt_feat_channels", "\n", "self", ".", "latent_channels", "=", "latent_channels", "\n", "self", ".", "num_st_feat", "=", "num_st_feat", "\n", "self", ".", "num_lt_feat", "=", "num_lt_feat", "\n", "self", ".", "use_scale", "=", "use_scale", "\n", "self", ".", "pre_activate", "=", "pre_activate", "\n", "self", ".", "pre_activate_with_ln", "=", "pre_activate_with_ln", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "zero_init_out_conv", "=", "zero_init_out_conv", "\n", "\n", "self", ".", "st_feat_conv", "=", "ConvModule", "(", "\n", "self", ".", "st_feat_channels", ",", "\n", "self", ".", "latent_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "lt_feat_conv", "=", "ConvModule", "(", "\n", "self", ".", "lt_feat_channels", ",", "\n", "self", ".", "latent_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "global_conv", "=", "ConvModule", "(", "\n", "self", ".", "lt_feat_channels", ",", "\n", "self", ".", "latent_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "if", "pre_activate", ":", "\n", "            ", "self", ".", "ln", "=", "nn", ".", "LayerNorm", "(", "[", "latent_channels", ",", "num_st_feat", ",", "1", ",", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "ln", "=", "nn", ".", "LayerNorm", "(", "[", "st_feat_channels", ",", "num_st_feat", ",", "1", ",", "1", "]", ")", "\n", "\n", "", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "out_conv", "=", "ConvModule", "(", "\n", "self", ".", "latent_channels", ",", "\n", "self", ".", "st_feat_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "if", "self", ".", "dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "dropout_ratio", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.NonLocalLayer.init_weights": [[112, 129], ["isinstance", "mmaction.utils.get_root_logger", "mmaction.utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "fbo_head.NonLocalLayer.modules", "TypeError", "isinstance", "mmcv.cnn.constant_init", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {pretrained}'", ")", "\n", "load_checkpoint", "(", "self", ",", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "elif", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "if", "self", ".", "zero_init_out_conv", ":", "\n", "                ", "constant_init", "(", "self", ".", "out_conv", ",", "0", ",", "bias", "=", "0", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.NonLocalLayer.forward": [[130, 170], ["fbo_head.NonLocalLayer.st_feat_conv", "theta.view.view.view", "fbo_head.NonLocalLayer.lt_feat_conv", "phi.view.view.view", "fbo_head.NonLocalLayer.global_conv", "g.view.view.view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul.softmax", "torch.matmul.softmax", "torch.matmul().view", "torch.matmul().view", "torch.matmul().view", "torch.matmul().view", "fbo_head.NonLocalLayer.out_conv", "st_feat.size", "theta.view.view.permute", "fbo_head.NonLocalLayer.relu", "fbo_head.NonLocalLayer.ln", "fbo_head.NonLocalLayer.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "fbo_head.NonLocalLayer.ln", "torch.matmul.softmax.permute"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax"], ["", "", "def", "forward", "(", "self", ",", "st_feat", ",", "lt_feat", ")", ":", "\n", "        ", "n", ",", "c", "=", "st_feat", ".", "size", "(", "0", ")", ",", "self", ".", "latent_channels", "\n", "num_st_feat", ",", "num_lt_feat", "=", "self", ".", "num_st_feat", ",", "self", ".", "num_lt_feat", "\n", "\n", "theta", "=", "self", ".", "st_feat_conv", "(", "st_feat", ")", "\n", "theta", "=", "theta", ".", "view", "(", "n", ",", "c", ",", "num_st_feat", ")", "\n", "\n", "phi", "=", "self", ".", "lt_feat_conv", "(", "lt_feat", ")", "\n", "phi", "=", "phi", ".", "view", "(", "n", ",", "c", ",", "num_lt_feat", ")", "\n", "\n", "g", "=", "self", ".", "global_conv", "(", "lt_feat", ")", "\n", "g", "=", "g", ".", "view", "(", "n", ",", "c", ",", "num_lt_feat", ")", "\n", "\n", "# (n, num_st_feat, c), (n, c, num_lt_feat)", "\n", "# -> (n, num_st_feat, num_lt_feat)", "\n", "theta_phi", "=", "torch", ".", "matmul", "(", "theta", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "phi", ")", "\n", "if", "self", ".", "use_scale", ":", "\n", "            ", "theta_phi", "/=", "c", "**", "0.5", "\n", "\n", "", "p", "=", "theta_phi", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# (n, c, num_lt_feat), (n, num_lt_feat, num_st_feat)", "\n", "# -> (n, c, num_st_feat, 1, 1)", "\n", "out", "=", "torch", ".", "matmul", "(", "g", ",", "p", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", ".", "view", "(", "n", ",", "c", ",", "num_st_feat", ",", "1", ",", "1", ")", "\n", "\n", "# If need to activate it before out_conv, use relu here, otherwise", "\n", "# use relu outside the non local layer.", "\n", "if", "self", ".", "pre_activate", ":", "\n", "            ", "if", "self", ".", "pre_activate_with_ln", ":", "\n", "                ", "out", "=", "self", ".", "ln", "(", "out", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "out_conv", "(", "out", ")", "\n", "\n", "if", "not", "self", ".", "pre_activate", ":", "\n", "            ", "out", "=", "self", ".", "ln", "(", "out", ")", "\n", "", "if", "self", ".", "dropout_ratio", ">", "0", ":", "\n", "            ", "out", "=", "self", ".", "dropout", "(", "out", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBONonLocal.__init__": [[193, 246], ["torch.Module.__init__", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "range", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ReLU", "torch.ReLU", "fbo_head.FBONonLocal.add_module", "fbo_head.FBONonLocal.non_local_layers.append", "fbo_head.NonLocalLayer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "st_feat_channels", ",", "\n", "lt_feat_channels", ",", "\n", "latent_channels", ",", "\n", "num_st_feat", ",", "\n", "num_lt_feat", ",", "\n", "num_non_local_layers", "=", "2", ",", "\n", "st_feat_dropout_ratio", "=", "0.2", ",", "\n", "lt_feat_dropout_ratio", "=", "0.2", ",", "\n", "pre_activate", "=", "True", ",", "\n", "zero_init_out_conv", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "num_non_local_layers", ">=", "1", ",", "(", "\n", "'At least one non_local_layer is needed.'", ")", "\n", "self", ".", "st_feat_channels", "=", "st_feat_channels", "\n", "self", ".", "lt_feat_channels", "=", "lt_feat_channels", "\n", "self", ".", "latent_channels", "=", "latent_channels", "\n", "self", ".", "num_st_feat", "=", "num_st_feat", "\n", "self", ".", "num_lt_feat", "=", "num_lt_feat", "\n", "self", ".", "num_non_local_layers", "=", "num_non_local_layers", "\n", "self", ".", "st_feat_dropout_ratio", "=", "st_feat_dropout_ratio", "\n", "self", ".", "lt_feat_dropout_ratio", "=", "lt_feat_dropout_ratio", "\n", "self", ".", "pre_activate", "=", "pre_activate", "\n", "self", ".", "zero_init_out_conv", "=", "zero_init_out_conv", "\n", "\n", "self", ".", "st_feat_conv", "=", "nn", ".", "Conv3d", "(", "\n", "st_feat_channels", ",", "latent_channels", ",", "kernel_size", "=", "1", ")", "\n", "self", ".", "lt_feat_conv", "=", "nn", ".", "Conv3d", "(", "\n", "lt_feat_channels", ",", "latent_channels", ",", "kernel_size", "=", "1", ")", "\n", "\n", "if", "self", ".", "st_feat_dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "st_feat_dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "st_feat_dropout_ratio", ")", "\n", "\n", "", "if", "self", ".", "lt_feat_dropout_ratio", ">", "0", ":", "\n", "            ", "self", ".", "lt_feat_dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "lt_feat_dropout_ratio", ")", "\n", "\n", "", "if", "not", "self", ".", "pre_activate", ":", "\n", "            ", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "", "self", ".", "non_local_layers", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "self", ".", "num_non_local_layers", ")", ":", "\n", "            ", "layer_name", "=", "f'non_local_layer_{idx + 1}'", "\n", "self", ".", "add_module", "(", "\n", "layer_name", ",", "\n", "NonLocalLayer", "(", "\n", "latent_channels", ",", "\n", "latent_channels", ",", "\n", "latent_channels", ",", "\n", "num_st_feat", ",", "\n", "num_lt_feat", ",", "\n", "pre_activate", "=", "self", ".", "pre_activate", ",", "\n", "zero_init_out_conv", "=", "self", ".", "zero_init_out_conv", ")", ")", "\n", "self", ".", "non_local_layers", ".", "append", "(", "layer_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBONonLocal.init_weights": [[247, 259], ["isinstance", "mmaction.utils.get_root_logger", "mmcv.runner.load_checkpoint", "mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "TypeError", "getattr", "getattr.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "if", "isinstance", "(", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "load_checkpoint", "(", "self", ",", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "elif", "pretrained", "is", "None", ":", "\n", "            ", "kaiming_init", "(", "self", ".", "st_feat_conv", ")", "\n", "kaiming_init", "(", "self", ".", "lt_feat_conv", ")", "\n", "for", "layer_name", "in", "self", ".", "non_local_layers", ":", "\n", "                ", "non_local_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "non_local_layer", ".", "init_weights", "(", "pretrained", "=", "pretrained", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBONonLocal.forward": [[260, 282], ["fbo_head.FBONonLocal.st_feat_conv", "fbo_head.FBONonLocal.lt_feat_conv", "fbo_head.FBONonLocal.st_feat_dropout", "fbo_head.FBONonLocal.lt_feat_dropout", "getattr", "getattr.", "fbo_head.FBONonLocal.relu"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "st_feat", ",", "lt_feat", ")", ":", "\n", "# prepare st_feat", "\n", "        ", "st_feat", "=", "self", ".", "st_feat_conv", "(", "st_feat", ")", "\n", "if", "self", ".", "st_feat_dropout_ratio", ">", "0", ":", "\n", "            ", "st_feat", "=", "self", ".", "st_feat_dropout", "(", "st_feat", ")", "\n", "\n", "# prepare lt_feat", "\n", "", "lt_feat", "=", "self", ".", "lt_feat_conv", "(", "lt_feat", ")", "\n", "if", "self", ".", "lt_feat_dropout_ratio", ">", "0", ":", "\n", "            ", "lt_feat", "=", "self", ".", "lt_feat_dropout", "(", "lt_feat", ")", "\n", "\n", "# fuse short-term and long-term features in NonLocal Layer", "\n", "", "for", "layer_name", "in", "self", ".", "non_local_layers", ":", "\n", "            ", "identity", "=", "st_feat", "\n", "non_local_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "nl_out", "=", "non_local_layer", "(", "st_feat", ",", "lt_feat", ")", "\n", "nl_out", "=", "identity", "+", "nl_out", "\n", "if", "not", "self", ".", "pre_activate", ":", "\n", "                ", "nl_out", "=", "self", ".", "relu", "(", "nl_out", ")", "\n", "", "st_feat", "=", "nl_out", "\n", "\n", "", "return", "nl_out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOAvg.__init__": [[287, 290], ["torch.Module.__init__", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOAvg.init_weights": [[291, 294], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "# FBOAvg has no parameters to be initialized.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOAvg.forward": [[295, 298], ["fbo_head.FBOAvg.avg_pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "st_feat", ",", "lt_feat", ")", ":", "\n", "        ", "out", "=", "self", ".", "avg_pool", "(", "lt_feat", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOMax.__init__": [[303, 306], ["torch.Module.__init__", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOMax.init_weights": [[307, 310], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "# FBOMax has no parameters to be initialized.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOMax.forward": [[311, 314], ["fbo_head.FBOMax.max_pool"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "st_feat", ",", "lt_feat", ")", ":", "\n", "        ", "out", "=", "self", ".", "max_pool", "(", "lt_feat", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOHead.__init__": [[336, 362], ["torch.Module.__init__", "fbo_cfg.pop", "copy.deepcopy", "copy.deepcopy", "mmaction.models.common.LFB", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d", "torch.AdaptiveMaxPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "lfb_cfg", ",", "\n", "fbo_cfg", ",", "\n", "temporal_pool_type", "=", "'avg'", ",", "\n", "spatial_pool_type", "=", "'max'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "fbo_type", "=", "fbo_cfg", ".", "pop", "(", "'type'", ",", "'non_local'", ")", "\n", "assert", "fbo_type", "in", "FBOHead", ".", "fbo_dict", "\n", "assert", "temporal_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "assert", "spatial_pool_type", "in", "[", "'max'", ",", "'avg'", "]", "\n", "\n", "self", ".", "lfb_cfg", "=", "copy", ".", "deepcopy", "(", "lfb_cfg", ")", "\n", "self", ".", "fbo_cfg", "=", "copy", ".", "deepcopy", "(", "fbo_cfg", ")", "\n", "\n", "self", ".", "lfb", "=", "LFB", "(", "**", "self", ".", "lfb_cfg", ")", "\n", "self", ".", "fbo", "=", "self", ".", "fbo_dict", "[", "fbo_type", "]", "(", "**", "self", ".", "fbo_cfg", ")", "\n", "\n", "# Pool by default", "\n", "if", "temporal_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "temporal_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "1", ",", "None", ",", "None", ")", ")", "\n", "", "if", "spatial_pool_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "spatial_pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "None", ",", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOHead.init_weights": [[363, 371], ["fbo_head.FBOHead.fbo.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize the weights in the module.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Default: None.\n        \"\"\"", "\n", "self", ".", "fbo", ".", "init_weights", "(", "pretrained", "=", "pretrained", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOHead.sample_lfb": [[372, 382], ["rois[].type", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "lt_feat.permute().contiguous.permute().contiguous.permute().contiguous", "lt_feat.permute().contiguous.permute().contiguous.unsqueeze().unsqueeze", "lt_feat_list.append", "fbo_head.FBOHead.lfb[].to", "lt_feat.permute().contiguous.permute().contiguous.permute", "lt_feat.permute().contiguous.permute().contiguous.unsqueeze"], "methods", ["None"], ["", "def", "sample_lfb", "(", "self", ",", "rois", ",", "img_metas", ")", ":", "\n", "        ", "\"\"\"Sample long-term features for each ROI feature.\"\"\"", "\n", "inds", "=", "rois", "[", ":", ",", "0", "]", ".", "type", "(", "torch", ".", "int64", ")", "\n", "lt_feat_list", "=", "[", "]", "\n", "for", "ind", "in", "inds", ":", "\n", "            ", "lt_feat_list", ".", "append", "(", "self", ".", "lfb", "[", "img_metas", "[", "ind", "]", "[", "'img_key'", "]", "]", ".", "to", "(", ")", ")", "\n", "", "lt_feat", "=", "torch", ".", "stack", "(", "lt_feat_list", ",", "dim", "=", "0", ")", "\n", "# [N, lfb_channels, window_size * max_num_feat_per_step]", "\n", "lt_feat", "=", "lt_feat", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "return", "lt_feat", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOHead.forward": [[383, 396], ["fbo_head.FBOHead.temporal_pool", "fbo_head.FBOHead.spatial_pool", "fbo_head.FBOHead.sample_lfb().to", "fbo_head.FBOHead.fbo", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fbo_head.FBOHead.sample_lfb"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.fbo_head.FBOHead.sample_lfb"], ["", "def", "forward", "(", "self", ",", "x", ",", "rois", ",", "img_metas", ",", "**", "kwargs", ")", ":", "\n", "# [N, C, 1, 1, 1]", "\n", "        ", "st_feat", "=", "self", ".", "temporal_pool", "(", "x", ")", "\n", "st_feat", "=", "self", ".", "spatial_pool", "(", "st_feat", ")", "\n", "identity", "=", "st_feat", "\n", "\n", "# [N, C, window_size * num_feat_per_step, 1, 1]", "\n", "lt_feat", "=", "self", ".", "sample_lfb", "(", "rois", ",", "img_metas", ")", ".", "to", "(", "st_feat", ".", "device", ")", "\n", "\n", "fbo_feat", "=", "self", ".", "fbo", "(", "st_feat", ",", "lt_feat", ")", "\n", "\n", "out", "=", "torch", ".", "cat", "(", "[", "identity", ",", "fbo_feat", "]", ",", "dim", "=", "1", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain.__init__": [[38, 52], ["torch.Module.__init__", "ssn_head.parse_stage_config", "ssn_head.parse_stage_config", "ssn_head.parse_stage_config"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config"], ["def", "__init__", "(", "self", ",", "stpp_stage", "=", "(", "1", ",", "(", "1", ",", "2", ")", ",", "1", ")", ",", "num_segments_list", "=", "(", "2", ",", "5", ",", "2", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "starting_part", ",", "starting_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "0", "]", ")", "\n", "course_part", ",", "course_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "1", "]", ")", "\n", "ending_part", ",", "ending_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "2", "]", ")", "\n", "\n", "self", ".", "num_multipliers", "=", "(", "\n", "starting_multiplier", "+", "course_multiplier", "+", "ending_multiplier", ")", "\n", "self", ".", "stpp_stages", "=", "(", "starting_part", ",", "course_part", ",", "ending_part", ")", "\n", "self", ".", "multiplier_list", "=", "(", "starting_multiplier", ",", "course_multiplier", ",", "\n", "ending_multiplier", ")", "\n", "\n", "self", ".", "num_segments_list", "=", "num_segments_list", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain._extract_stage_feature": [[53, 82], ["stage_feat.size", "torch.arange().int", "torch.arange().int", "torch.arange().int", "torch.arange().int", "range", "stage_stpp_feat.append", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "stage_feat[].mean", "scale_factors.view"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_extract_stage_feature", "(", "stage_feat", ",", "stage_parts", ",", "num_multipliers", ",", "\n", "scale_factors", ",", "num_samples", ")", ":", "\n", "        ", "\"\"\"Extract stage feature based on structured temporal pyramid pooling.\n\n        Args:\n            stage_feat (torch.Tensor): Stage features to be STPP.\n            stage_parts (tuple): Config of STPP.\n            num_multipliers (int): Total number of parts in the stage.\n            scale_factors (list): Ratios of the effective sampling lengths\n                to augmented lengths.\n            num_samples (int): Number of samples.\n\n        Returns:\n            torch.Tensor: Features of the stage.\n        \"\"\"", "\n", "stage_stpp_feat", "=", "[", "]", "\n", "stage_len", "=", "stage_feat", ".", "size", "(", "1", ")", "\n", "for", "stage_part", "in", "stage_parts", ":", "\n", "            ", "ticks", "=", "torch", ".", "arange", "(", "0", ",", "stage_len", "+", "1e-5", ",", "\n", "stage_len", "/", "stage_part", ")", ".", "int", "(", ")", "\n", "for", "i", "in", "range", "(", "stage_part", ")", ":", "\n", "                ", "part_feat", "=", "stage_feat", "[", ":", ",", "ticks", "[", "i", "]", ":", "ticks", "[", "i", "+", "1", "]", ",", ":", "]", ".", "mean", "(", "\n", "dim", "=", "1", ")", "/", "num_multipliers", "\n", "if", "scale_factors", "is", "not", "None", ":", "\n", "                    ", "part_feat", "=", "(", "\n", "part_feat", "*", "scale_factors", ".", "view", "(", "num_samples", ",", "1", ")", ")", "\n", "", "stage_stpp_feat", ".", "append", "(", "part_feat", ")", "\n", "", "", "return", "stage_stpp_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain.forward": [[83, 123], ["x.view.view.size", "x.view.view.view", "x.view.view.size", "scale_factors.view.view.view", "stage_stpp_feats.extend", "stage_stpp_feats.extend", "stage_stpp_feats.extend", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x[].mean", "ssn_head.STPPTrain._extract_stage_feature", "ssn_head.STPPTrain._extract_stage_feature", "ssn_head.STPPTrain._extract_stage_feature"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain._extract_stage_feature", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain._extract_stage_feature", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTrain._extract_stage_feature"], ["", "def", "forward", "(", "self", ",", "x", ",", "scale_factors", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            scale_factors (list): Ratios of the effective sampling lengths\n                to augmented lengths.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]:\n                Features for predicting activity scores and\n                completeness scores.\n        \"\"\"", "\n", "x0", "=", "self", ".", "num_segments_list", "[", "0", "]", "\n", "x1", "=", "x0", "+", "self", ".", "num_segments_list", "[", "1", "]", "\n", "num_segments", "=", "x1", "+", "self", ".", "num_segments_list", "[", "2", "]", "\n", "\n", "feat_dim", "=", "x", ".", "size", "(", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "num_segments", ",", "feat_dim", ")", "\n", "num_samples", "=", "x", ".", "size", "(", "0", ")", "\n", "\n", "scale_factors", "=", "scale_factors", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "\n", "stage_stpp_feats", "=", "[", "]", "\n", "stage_stpp_feats", ".", "extend", "(", "\n", "self", ".", "_extract_stage_feature", "(", "x", "[", ":", ",", ":", "x0", ",", ":", "]", ",", "self", ".", "stpp_stages", "[", "0", "]", ",", "\n", "self", ".", "multiplier_list", "[", "0", "]", ",", "\n", "scale_factors", "[", ":", ",", "0", "]", ",", "num_samples", ")", ")", "\n", "stage_stpp_feats", ".", "extend", "(", "\n", "self", ".", "_extract_stage_feature", "(", "x", "[", ":", ",", "x0", ":", "x1", ",", ":", "]", ",", "self", ".", "stpp_stages", "[", "1", "]", ",", "\n", "self", ".", "multiplier_list", "[", "1", "]", ",", "None", ",", "\n", "num_samples", ")", ")", "\n", "stage_stpp_feats", ".", "extend", "(", "\n", "self", ".", "_extract_stage_feature", "(", "x", "[", ":", ",", "x1", ":", ",", ":", "]", ",", "self", ".", "stpp_stages", "[", "2", "]", ",", "\n", "self", ".", "multiplier_list", "[", "2", "]", ",", "\n", "scale_factors", "[", ":", ",", "1", "]", ",", "num_samples", ")", ")", "\n", "stpp_feat", "=", "torch", ".", "cat", "(", "stage_stpp_feats", ",", "dim", "=", "1", ")", "\n", "\n", "course_feat", "=", "x", "[", ":", ",", "x0", ":", "x1", ",", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "\n", "return", "course_feat", ",", "stpp_feat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTest.__init__": [[136, 170], ["torch.Module.__init__", "ssn_head.parse_stage_config", "ssn_head.parse_stage_config", "ssn_head.parse_stage_config", "slice", "slice", "slice"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "use_regression", "=", "True", ",", "\n", "stpp_stage", "=", "(", "1", ",", "(", "1", ",", "2", ")", ",", "1", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "activity_score_len", "=", "num_classes", "+", "1", "\n", "self", ".", "complete_score_len", "=", "num_classes", "\n", "self", ".", "reg_score_len", "=", "num_classes", "*", "2", "\n", "self", ".", "use_regression", "=", "use_regression", "\n", "\n", "starting_parts", ",", "starting_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "0", "]", ")", "\n", "course_parts", ",", "course_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "1", "]", ")", "\n", "ending_parts", ",", "ending_multiplier", "=", "parse_stage_config", "(", "stpp_stage", "[", "2", "]", ")", "\n", "\n", "self", ".", "num_multipliers", "=", "(", "\n", "starting_multiplier", "+", "course_multiplier", "+", "ending_multiplier", ")", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "self", ".", "feat_dim", "=", "(", "\n", "self", ".", "activity_score_len", "+", "self", ".", "num_multipliers", "*", "\n", "(", "self", ".", "complete_score_len", "+", "self", ".", "reg_score_len", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "feat_dim", "=", "(", "\n", "self", ".", "activity_score_len", "+", "\n", "self", ".", "num_multipliers", "*", "self", ".", "complete_score_len", ")", "\n", "", "self", ".", "stpp_stage", "=", "(", "starting_parts", ",", "course_parts", ",", "ending_parts", ")", "\n", "\n", "self", ".", "activity_slice", "=", "slice", "(", "0", ",", "self", ".", "activity_score_len", ")", "\n", "self", ".", "complete_slice", "=", "slice", "(", "\n", "self", ".", "activity_slice", ".", "stop", ",", "self", ".", "activity_slice", ".", "stop", "+", "\n", "self", ".", "complete_score_len", "*", "self", ".", "num_multipliers", ")", "\n", "self", ".", "reg_slice", "=", "slice", "(", "\n", "self", ".", "complete_slice", ".", "stop", ",", "self", ".", "complete_slice", ".", "stop", "+", "\n", "self", ".", "reg_score_len", "*", "self", ".", "num_multipliers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTest._pyramids_pooling": [[171, 220], ["enumerate", "sum", "float", "max", "torch.arange().int", "torch.arange().int", "torch.arange().int", "torch.arange().int", "range", "raw_scores.size", "len", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "raw_scale_score.detach().cpu", "raw_score.mean", "raw_scale_score.detach"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_pyramids_pooling", "(", "out_scores", ",", "index", ",", "raw_scores", ",", "ticks", ",", "scale_factors", ",", "\n", "score_len", ",", "stpp_stage", ")", ":", "\n", "        ", "\"\"\"Perform pyramids pooling.\n\n        Args:\n            out_scores (torch.Tensor): Scores to be returned.\n            index (int): Index of output scores.\n            raw_scores (torch.Tensor): Raw scores before STPP.\n            ticks (list): Ticks of raw scores.\n            scale_factors (list): Ratios of the effective sampling lengths\n                to augmented lengths.\n            score_len (int): Length of the score.\n            stpp_stage (tuple): Config of STPP.\n        \"\"\"", "\n", "offset", "=", "0", "\n", "for", "stage_idx", ",", "stage_cfg", "in", "enumerate", "(", "stpp_stage", ")", ":", "\n", "            ", "if", "stage_idx", "==", "0", ":", "\n", "                ", "scale_factor", "=", "scale_factors", "[", "0", "]", "\n", "", "elif", "stage_idx", "==", "len", "(", "stpp_stage", ")", "-", "1", ":", "\n", "                ", "scale_factor", "=", "scale_factors", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "scale_factor", "=", "1.0", "\n", "\n", "", "sum_parts", "=", "sum", "(", "stage_cfg", ")", "\n", "tick_left", "=", "ticks", "[", "stage_idx", "]", "\n", "tick_right", "=", "float", "(", "max", "(", "ticks", "[", "stage_idx", "]", "+", "1", ",", "ticks", "[", "stage_idx", "+", "1", "]", ")", ")", "\n", "\n", "if", "tick_right", "<=", "0", "or", "tick_left", ">=", "raw_scores", ".", "size", "(", "0", ")", ":", "\n", "                ", "offset", "+=", "sum_parts", "\n", "continue", "\n", "", "for", "num_parts", "in", "stage_cfg", ":", "\n", "                ", "part_ticks", "=", "torch", ".", "arange", "(", "tick_left", ",", "tick_right", "+", "1e-5", ",", "\n", "(", "tick_right", "-", "tick_left", ")", "/", "\n", "num_parts", ")", ".", "int", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_parts", ")", ":", "\n", "                    ", "part_tick_left", "=", "part_ticks", "[", "i", "]", "\n", "part_tick_right", "=", "part_ticks", "[", "i", "+", "1", "]", "\n", "if", "part_tick_right", "-", "part_tick_left", ">=", "1", ":", "\n", "                        ", "raw_score", "=", "raw_scores", "[", "part_tick_left", ":", "part_tick_right", ",", "\n", "offset", "*", "\n", "score_len", ":", "(", "offset", "+", "1", ")", "*", "\n", "score_len", "]", "\n", "raw_scale_score", "=", "raw_score", ".", "mean", "(", "dim", "=", "0", ")", "*", "scale_factor", "\n", "out_scores", "[", "index", ",", ":", "]", "+=", "raw_scale_score", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "", "offset", "+=", "1", "\n", "\n", "", "", "", "return", "out_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTest.forward": [[221, 271], ["proposal_ticks.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "x.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "raw_activity_scores[].mean", "ssn_head.STPPTest._pyramids_pooling", "ssn_head.STPPTest._pyramids_pooling", "max"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTest._pyramids_pooling", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.STPPTest._pyramids_pooling"], ["", "def", "forward", "(", "self", ",", "x", ",", "proposal_ticks", ",", "scale_factors", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            proposal_ticks (list): Ticks of proposals to be STPP.\n            scale_factors (list): Ratios of the effective sampling lengths\n                to augmented lengths.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n                out_activity_scores (torch.Tensor): Activity scores\n                out_complete_scores (torch.Tensor): Completeness scores.\n                out_reg_scores (torch.Tensor): Regression scores.\n        \"\"\"", "\n", "assert", "x", ".", "size", "(", "1", ")", "==", "self", ".", "feat_dim", "\n", "num_ticks", "=", "proposal_ticks", ".", "size", "(", "0", ")", "\n", "\n", "out_activity_scores", "=", "torch", ".", "zeros", "(", "(", "num_ticks", ",", "self", ".", "activity_score_len", ")", ",", "\n", "dtype", "=", "x", ".", "dtype", ")", "\n", "raw_activity_scores", "=", "x", "[", ":", ",", "self", ".", "activity_slice", "]", "\n", "\n", "out_complete_scores", "=", "torch", ".", "zeros", "(", "(", "num_ticks", ",", "self", ".", "complete_score_len", ")", ",", "\n", "dtype", "=", "x", ".", "dtype", ")", "\n", "raw_complete_scores", "=", "x", "[", ":", ",", "self", ".", "complete_slice", "]", "\n", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "out_reg_scores", "=", "torch", ".", "zeros", "(", "(", "num_ticks", ",", "self", ".", "reg_score_len", ")", ",", "\n", "dtype", "=", "x", ".", "dtype", ")", "\n", "raw_reg_scores", "=", "x", "[", ":", ",", "self", ".", "reg_slice", "]", "\n", "", "else", ":", "\n", "            ", "out_reg_scores", "=", "None", "\n", "raw_reg_scores", "=", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "num_ticks", ")", ":", "\n", "            ", "ticks", "=", "proposal_ticks", "[", "i", "]", "\n", "\n", "out_activity_scores", "[", "i", ",", ":", "]", "=", "raw_activity_scores", "[", "\n", "ticks", "[", "1", "]", ":", "max", "(", "ticks", "[", "1", "]", "+", "1", ",", "ticks", "[", "2", "]", ")", ",", ":", "]", ".", "mean", "(", "dim", "=", "0", ")", "\n", "\n", "out_complete_scores", "=", "self", ".", "_pyramids_pooling", "(", "\n", "out_complete_scores", ",", "i", ",", "raw_complete_scores", ",", "ticks", ",", "\n", "scale_factors", "[", "i", "]", ",", "self", ".", "complete_score_len", ",", "self", ".", "stpp_stage", ")", "\n", "\n", "if", "self", ".", "use_regression", ":", "\n", "                ", "out_reg_scores", "=", "self", ".", "_pyramids_pooling", "(", "\n", "out_reg_scores", ",", "i", ",", "raw_reg_scores", ",", "ticks", ",", "scale_factors", "[", "i", "]", ",", "\n", "self", ".", "reg_score_len", ",", "self", ".", "stpp_stage", ")", "\n", "\n", "", "", "return", "out_activity_scores", ",", "out_complete_scores", ",", "out_reg_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.SSNHead.__init__": [[287, 331], ["dict", "torch.Module.__init__", "consensus.copy", "consensus.copy.pop", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "ssn_head.STPPTrain", "torch.Linear", "torch.Linear", "ssn_head.STPPTest"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "dropout_ratio", "=", "0.8", ",", "\n", "in_channels", "=", "1024", ",", "\n", "num_classes", "=", "20", ",", "\n", "consensus", "=", "dict", "(", "\n", "type", "=", "'STPPTrain'", ",", "\n", "standalong_classifier", "=", "True", ",", "\n", "stpp_cfg", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "num_seg", "=", "(", "2", ",", "5", ",", "2", ")", ")", ",", "\n", "use_regression", "=", "True", ",", "\n", "init_std", "=", "0.001", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "use_regression", "=", "use_regression", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "\n", "# Based on this copy, the model will utilize different", "\n", "# structured temporal pyramid pooling at training and testing.", "\n", "# Warning: this copy cannot be removed.", "\n", "", "consensus_", "=", "consensus", ".", "copy", "(", ")", "\n", "consensus_type", "=", "consensus_", ".", "pop", "(", "'type'", ")", "\n", "if", "consensus_type", "==", "'STPPTrain'", ":", "\n", "            ", "self", ".", "consensus", "=", "STPPTrain", "(", "**", "consensus_", ")", "\n", "", "elif", "consensus_type", "==", "'STPPTest'", ":", "\n", "            ", "consensus_", "[", "'num_classes'", "]", "=", "self", ".", "num_classes", "\n", "self", ".", "consensus", "=", "STPPTest", "(", "**", "consensus_", ")", "\n", "\n", "", "self", ".", "in_channels_activity", "=", "in_channels", "\n", "self", ".", "in_channels_complete", "=", "(", "\n", "self", ".", "consensus", ".", "num_multipliers", "*", "in_channels", ")", "\n", "self", ".", "activity_fc", "=", "nn", ".", "Linear", "(", "in_channels", ",", "num_classes", "+", "1", ")", "\n", "self", ".", "completeness_fc", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels_complete", ",", "\n", "num_classes", ")", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "self", ".", "regressor_fc", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels_complete", ",", "\n", "num_classes", "*", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.SSNHead.init_weights": [[332, 338], ["mmcv.cnn.normal_init", "mmcv.cnn.normal_init", "mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "activity_fc", ",", "std", "=", "self", ".", "init_std", ")", "\n", "normal_init", "(", "self", ".", "completeness_fc", ",", "std", "=", "self", ".", "init_std", ")", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "normal_init", "(", "self", ".", "regressor_fc", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.SSNHead.prepare_test_fc": [[339, 384], ["torch.Linear", "torch.Linear", "ssn_head.SSNHead.completeness_fc.weight.data.view().transpose().contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ssn_head.SSNHead.completeness_fc.bias.data.view().expand().contiguous().view", "ssn_head.SSNHead.regressor_fc.weight.data.view().transpose().contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ssn_head.SSNHead.completeness_fc.weight.data.view().transpose().contiguous", "ssn_head.SSNHead.regressor_fc.bias.data.view().expand().contiguous().view", "ssn_head.SSNHead.completeness_fc.bias.data.view().expand().contiguous", "ssn_head.SSNHead.regressor_fc.weight.data.view().transpose().contiguous", "ssn_head.SSNHead.completeness_fc.weight.data.view().transpose", "ssn_head.SSNHead.regressor_fc.bias.data.view().expand().contiguous", "ssn_head.SSNHead.completeness_fc.bias.data.view().expand", "ssn_head.SSNHead.regressor_fc.weight.data.view().transpose", "ssn_head.SSNHead.completeness_fc.weight.data.view", "ssn_head.SSNHead.regressor_fc.bias.data.view().expand", "ssn_head.SSNHead.completeness_fc.bias.data.view", "ssn_head.SSNHead.regressor_fc.weight.data.view", "ssn_head.SSNHead.regressor_fc.bias.data.view"], "methods", ["None"], ["", "", "def", "prepare_test_fc", "(", "self", ",", "stpp_feat_multiplier", ")", ":", "\n", "        ", "\"\"\"Reorganize the shape of fully connected layer at testing, in order\n        to improve testing efficiency.\n\n        Args:\n            stpp_feat_multiplier (int): Total number of parts.\n\n        Returns:\n            bool: Whether the shape transformation is ready for testing.\n        \"\"\"", "\n", "\n", "in_features", "=", "self", ".", "activity_fc", ".", "in_features", "\n", "out_features", "=", "(", "\n", "self", ".", "activity_fc", ".", "out_features", "+", "\n", "self", ".", "completeness_fc", ".", "out_features", "*", "stpp_feat_multiplier", ")", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "out_features", "+=", "(", "\n", "self", ".", "regressor_fc", ".", "out_features", "*", "stpp_feat_multiplier", ")", "\n", "", "self", ".", "test_fc", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "\n", "# Fetch weight and bias of the reorganized fc.", "\n", "complete_weight", "=", "self", ".", "completeness_fc", ".", "weight", ".", "data", ".", "view", "(", "\n", "self", ".", "completeness_fc", ".", "out_features", ",", "stpp_feat_multiplier", ",", "\n", "in_features", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "in_features", ")", "\n", "complete_bias", "=", "self", ".", "completeness_fc", ".", "bias", ".", "data", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand", "(", "\n", "stpp_feat_multiplier", ",", "self", ".", "completeness_fc", ".", "out_features", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "/", "stpp_feat_multiplier", "\n", "\n", "weight", "=", "torch", ".", "cat", "(", "(", "self", ".", "activity_fc", ".", "weight", ".", "data", ",", "complete_weight", ")", ")", "\n", "bias", "=", "torch", ".", "cat", "(", "(", "self", ".", "activity_fc", ".", "bias", ".", "data", ",", "complete_bias", ")", ")", "\n", "\n", "if", "self", ".", "use_regression", ":", "\n", "            ", "reg_weight", "=", "self", ".", "regressor_fc", ".", "weight", ".", "data", ".", "view", "(", "\n", "self", ".", "regressor_fc", ".", "out_features", ",", "stpp_feat_multiplier", ",", "\n", "in_features", ")", ".", "transpose", "(", "0", ",", "\n", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "in_features", ")", "\n", "reg_bias", "=", "self", ".", "regressor_fc", ".", "bias", ".", "data", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand", "(", "\n", "stpp_feat_multiplier", ",", "self", ".", "regressor_fc", ".", "out_features", "\n", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "/", "stpp_feat_multiplier", "\n", "weight", "=", "torch", ".", "cat", "(", "(", "weight", ",", "reg_weight", ")", ")", "\n", "bias", "=", "torch", ".", "cat", "(", "(", "bias", ",", "reg_bias", ")", ")", "\n", "\n", "", "self", ".", "test_fc", ".", "weight", ".", "data", "=", "weight", "\n", "self", ".", "test_fc", ".", "bias", ".", "data", "=", "bias", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.SSNHead.forward": [[385, 414], ["ssn_head.SSNHead.test_fc", "ssn_head.SSNHead.consensus", "ssn_head.SSNHead.consensus", "ssn_head.SSNHead.activity_fc", "ssn_head.SSNHead.completeness_fc", "ssn_head.SSNHead.dropout", "ssn_head.SSNHead.dropout", "ssn_head.SSNHead.regressor_fc", "bbox_preds.view.view.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "if", "not", "test_mode", ":", "\n", "            ", "x", ",", "proposal_scale_factor", "=", "x", "\n", "activity_feat", ",", "completeness_feat", "=", "self", ".", "consensus", "(", "\n", "x", ",", "proposal_scale_factor", ")", "\n", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "                ", "activity_feat", "=", "self", ".", "dropout", "(", "activity_feat", ")", "\n", "completeness_feat", "=", "self", ".", "dropout", "(", "completeness_feat", ")", "\n", "\n", "", "activity_scores", "=", "self", ".", "activity_fc", "(", "activity_feat", ")", "\n", "complete_scores", "=", "self", ".", "completeness_fc", "(", "completeness_feat", ")", "\n", "if", "self", ".", "use_regression", ":", "\n", "                ", "bbox_preds", "=", "self", ".", "regressor_fc", "(", "completeness_feat", ")", "\n", "bbox_preds", "=", "bbox_preds", ".", "view", "(", "-", "1", ",", "\n", "self", ".", "completeness_fc", ".", "out_features", ",", "\n", "2", ")", "\n", "", "else", ":", "\n", "                ", "bbox_preds", "=", "None", "\n", "", "return", "activity_scores", ",", "complete_scores", ",", "bbox_preds", "\n", "\n", "", "x", ",", "proposal_tick_list", ",", "scale_factor_list", "=", "x", "\n", "test_scores", "=", "self", ".", "test_fc", "(", "x", ")", "\n", "(", "activity_scores", ",", "completeness_scores", ",", "\n", "bbox_preds", ")", "=", "self", ".", "consensus", "(", "test_scores", ",", "proposal_tick_list", ",", "\n", "scale_factor_list", ")", "\n", "\n", "return", "(", "test_scores", ",", "activity_scores", ",", "completeness_scores", ",", "bbox_preds", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.ssn_head.parse_stage_config": [[9, 26], ["isinstance", "isinstance", "ValueError", "sum"], "function", ["None"], ["def", "parse_stage_config", "(", "stage_cfg", ")", ":", "\n", "    ", "\"\"\"Parse config of STPP for three stages.\n\n    Args:\n        stage_cfg (int | tuple[int]):\n            Config of structured temporal pyramid pooling.\n\n    Returns:\n        tuple[tuple[int], int]:\n            Config of structured temporal pyramid pooling and\n            total number of parts(number of multipliers).\n    \"\"\"", "\n", "if", "isinstance", "(", "stage_cfg", ",", "int", ")", ":", "\n", "        ", "return", "(", "stage_cfg", ",", ")", ",", "stage_cfg", "\n", "", "if", "isinstance", "(", "stage_cfg", ",", "tuple", ")", ":", "\n", "        ", "return", "stage_cfg", ",", "sum", "(", "stage_cfg", ")", "\n", "", "raise", "ValueError", "(", "f'Incorrect STPP config {stage_cfg}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.i3d_head.I3DHead.__init__": [[25, 49], ["dict", "base.BaseHead.__init__", "torch.Linear", "torch.Dropout", "torch.AdaptiveAvgPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "dropout_ratio", "=", "0.5", ",", "\n", "init_std", "=", "0.01", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ")", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool3d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.i3d_head.I3DHead.init_weights": [[50, 53], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.i3d_head.I3DHead.forward": [[54, 75], ["i3d_head.I3DHead.view", "i3d_head.I3DHead.fc_cls", "i3d_head.I3DHead.avg_pool", "i3d_head.I3DHead.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N, in_channels, 4, 7, 7]", "\n", "if", "self", ".", "avg_pool", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "# [N, in_channels, 1, 1, 1]", "\n", "", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "# [N, in_channels, 1, 1, 1]", "\n", "", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# [N, in_channels]", "\n", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.x3d_head.X3DHead.__init__": [[24, 59], ["dict", "base.BaseHead.__init__", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.Dropout", "torch.AdaptiveAvgPool3d", "torch.AdaptiveMaxPool3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "dropout_ratio", "=", "0.5", ",", "\n", "init_std", "=", "0.01", ",", "\n", "fc1_bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "mid_channels", "=", "2048", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "fc1_bias", "=", "fc1_bias", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "in_channels", ",", "self", ".", "mid_channels", ",", "bias", "=", "self", ".", "fc1_bias", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "self", ".", "mid_channels", ",", "self", ".", "num_classes", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "pool", "=", "None", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "elif", "self", ".", "spatial_type", "==", "'max'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveMaxPool3d", "(", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.x3d_head.X3DHead.init_weights": [[60, 64], ["mmcv.cnn.normal_init", "mmcv.cnn.normal_init"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc1", ",", "std", "=", "self", ".", "init_std", ")", "\n", "normal_init", "(", "self", ".", "fc2", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.x3d_head.X3DHead.forward": [[65, 91], ["x3d_head.X3DHead.pool", "x3d_head.X3DHead.view", "x3d_head.X3DHead.fc1", "x3d_head.X3DHead.relu", "x3d_head.X3DHead.fc2", "x3d_head.X3DHead.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N, in_channels, T, H, W]", "\n", "assert", "self", ".", "pool", "is", "not", "None", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "# [N, in_channels, 1, 1, 1]", "\n", "# [N, in_channels, 1, 1, 1]", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "# [N, in_channels]", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "# [N, 2048]", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "", "cls_score", "=", "self", ".", "fc2", "(", "x", ")", "\n", "# [N, num_classes]", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModule.__init__": [[23, 33], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "hidden_dim", ",", "num_segments", ",", "num_classes", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "bottleneck_dim", "=", "512", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "num_segments", "*", "self", ".", "hidden_dim", ",", "bottleneck_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "bottleneck_dim", ",", "self", ".", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModule.init_weights": [[34, 37], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# Use the default kaiming_uniform for all nn.linear layers.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModule.forward": [[38, 43], ["trn_head.RelationModule.view", "trn_head.RelationModule.classifier", "trn_head.RelationModule.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# [N, num_segs * hidden_dim]", "\n", "        ", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "classifier", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModuleMultiScale.__init__": [[55, 84], ["torch.Module.__init__", "range", "torch.ModuleList", "torch.ModuleList", "list", "trn_head.RelationModuleMultiScale.relations_scales.append", "trn_head.RelationModuleMultiScale.subsample_scales.append", "len", "torch.Sequential", "torch.Sequential", "trn_head.RelationModuleMultiScale.fc_fusion_scales.append", "itertools.combinations", "min", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "range", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "hidden_dim", ",", "num_segments", ",", "num_classes", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "# generate the multiple frame relations", "\n", "self", ".", "scales", "=", "range", "(", "num_segments", ",", "1", ",", "-", "1", ")", "\n", "\n", "self", ".", "relations_scales", "=", "[", "]", "\n", "self", ".", "subsample_scales", "=", "[", "]", "\n", "max_subsample", "=", "3", "\n", "for", "scale", "in", "self", ".", "scales", ":", "\n", "# select the different frame features for different scales", "\n", "            ", "relations_scale", "=", "list", "(", "\n", "itertools", ".", "combinations", "(", "range", "(", "self", ".", "num_segments", ")", ",", "scale", ")", ")", "\n", "self", ".", "relations_scales", ".", "append", "(", "relations_scale", ")", "\n", "# sample `max_subsample` relation_scale at most", "\n", "self", ".", "subsample_scales", ".", "append", "(", "\n", "min", "(", "max_subsample", ",", "len", "(", "relations_scale", ")", ")", ")", "\n", "", "assert", "len", "(", "self", ".", "relations_scales", "[", "0", "]", ")", "==", "1", "\n", "\n", "bottleneck_dim", "=", "256", "\n", "self", ".", "fc_fusion_scales", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "scale", "in", "self", ".", "scales", ":", "\n", "            ", "fc_fusion", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "scale", "*", "self", ".", "hidden_dim", ",", "bottleneck_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "bottleneck_dim", ",", "self", ".", "num_classes", ")", ")", "\n", "self", ".", "fc_fusion_scales", ".", "append", "(", "fc_fusion", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModuleMultiScale.init_weights": [[85, 88], ["None"], "methods", ["None"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "# Use the default kaiming_uniform for all nn.linear layers.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.RelationModuleMultiScale.forward": [[89, 110], ["act_all.view.view.view", "range", "act_all.view.view.size", "len", "numpy.random.choice", "len", "act_relation.view.view.view", "act_relation.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# the first one is the largest scale", "\n", "        ", "act_all", "=", "x", "[", ":", ",", "self", ".", "relations_scales", "[", "0", "]", "[", "0", "]", ",", ":", "]", "\n", "act_all", "=", "act_all", ".", "view", "(", "\n", "act_all", ".", "size", "(", "0", ")", ",", "self", ".", "scales", "[", "0", "]", "*", "self", ".", "hidden_dim", ")", "\n", "act_all", "=", "self", ".", "fc_fusion_scales", "[", "0", "]", "(", "act_all", ")", "\n", "\n", "for", "scaleID", "in", "range", "(", "1", ",", "len", "(", "self", ".", "scales", ")", ")", ":", "\n", "# iterate over the scales", "\n", "            ", "idx_relations_randomsample", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "relations_scales", "[", "scaleID", "]", ")", ",", "\n", "self", ".", "subsample_scales", "[", "scaleID", "]", ",", "\n", "replace", "=", "False", ")", "\n", "for", "idx", "in", "idx_relations_randomsample", ":", "\n", "                ", "act_relation", "=", "x", "[", ":", ",", "self", ".", "relations_scales", "[", "scaleID", "]", "[", "idx", "]", ",", ":", "]", "\n", "act_relation", "=", "act_relation", ".", "view", "(", "\n", "act_relation", ".", "size", "(", "0", ")", ",", "\n", "self", ".", "scales", "[", "scaleID", "]", "*", "self", ".", "hidden_dim", ")", "\n", "act_relation", "=", "self", ".", "fc_fusion_scales", "[", "scaleID", "]", "(", "act_relation", ")", "\n", "act_all", "+=", "act_relation", "\n", "", "", "return", "act_all", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.TRNHead.__init__": [[133, 176], ["dict", "base.BaseHead.__init__", "torch.Linear", "torch.Linear", "trn_head.RelationModule", "torch.Dropout", "torch.Dropout", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "trn_head.RelationModuleMultiScale", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "num_segments", "=", "8", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "relation_type", "=", "'TRNMultiScale'", ",", "\n", "hidden_dim", "=", "256", ",", "\n", "dropout_ratio", "=", "0.8", ",", "\n", "init_std", "=", "0.001", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "relation_type", "=", "relation_type", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "if", "self", ".", "relation_type", "==", "'TRN'", ":", "\n", "            ", "self", ".", "consensus", "=", "RelationModule", "(", "self", ".", "hidden_dim", ",", "self", ".", "num_segments", ",", "\n", "self", ".", "num_classes", ")", "\n", "", "elif", "self", ".", "relation_type", "==", "'TRNMultiScale'", ":", "\n", "            ", "self", ".", "consensus", "=", "RelationModuleMultiScale", "(", "self", ".", "hidden_dim", ",", "\n", "self", ".", "num_segments", ",", "\n", "self", ".", "num_classes", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f'Unknown Relation Type {self.relation_type}!'", ")", "\n", "\n", "", "if", "self", ".", "dropout_ratio", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "fc_cls", "=", "nn", ".", "Linear", "(", "self", ".", "in_channels", ",", "self", ".", "hidden_dim", ")", "\n", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "# use `nn.AdaptiveAvgPool2d` to adaptively match the in_channels.", "\n", "            ", "self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "avg_pool", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.TRNHead.init_weights": [[177, 181], ["mmcv.cnn.normal_init", "trn_head.TRNHead.consensus.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "normal_init", "(", "self", ".", "fc_cls", ",", "std", "=", "self", ".", "init_std", ")", "\n", "self", ".", "consensus", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.trn_head.TRNHead.forward": [[182, 212], ["torch.flatten", "torch.flatten", "torch.flatten", "torch.flatten", "trn_head.TRNHead.fc_cls", "trn_head.TRNHead.view", "trn_head.TRNHead.consensus", "trn_head.TRNHead.avg_pool", "trn_head.TRNHead.dropout", "trn_head.TRNHead.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "num_segs", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n            num_segs (int): Useless in TRNHead. By default, `num_segs`\n                is equal to `clip_len * num_clips * num_crops`, which is\n                automatically generated in Recognizer forward phase and\n                useless in TRN models. The `self.num_segments` we need is a\n                hyper parameter to build TRN models.\n        Returns:\n            torch.Tensor: The classification scores for input samples.\n        \"\"\"", "\n", "# [N * num_segs, in_channels, 7, 7]", "\n", "if", "self", ".", "avg_pool", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "# [N * num_segs, in_channels, 1, 1]", "\n", "", "x", "=", "torch", ".", "flatten", "(", "x", ",", "1", ")", "\n", "# [N * num_segs, in_channels]", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "\n", "# [N, num_segs, hidden_dim]", "\n", "", "cls_score", "=", "self", ".", "fc_cls", "(", "x", ")", "\n", "cls_score", "=", "cls_score", ".", "view", "(", "(", "-", "1", ",", "self", ".", "num_segments", ")", "+", "\n", "cls_score", ".", "size", "(", ")", "[", "1", ":", "]", ")", "\n", "\n", "# [N, num_classes]", "\n", "cls_score", "=", "self", ".", "consensus", "(", "cls_score", ")", "\n", "return", "cls_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.stgcn_head.STGCNHead.__init__": [[22, 46], ["dict", "base.BaseHead.__init__", "torch.Conv2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveMaxPool2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_classes", ",", "\n", "in_channels", ",", "\n", "loss_cls", "=", "dict", "(", "type", "=", "'CrossEntropyLoss'", ")", ",", "\n", "spatial_type", "=", "'avg'", ",", "\n", "num_person", "=", "2", ",", "\n", "init_std", "=", "0.01", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_classes", ",", "in_channels", ",", "loss_cls", ")", "\n", "\n", "self", ".", "spatial_type", "=", "spatial_type", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_person", "=", "num_person", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "self", ".", "pool", "=", "None", "\n", "if", "self", ".", "spatial_type", "==", "'avg'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "", "elif", "self", ".", "spatial_type", "==", "'max'", ":", "\n", "            ", "self", ".", "pool", "=", "nn", ".", "AdaptiveMaxPool2d", "(", "(", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Conv2d", "(", "self", ".", "in_channels", ",", "self", ".", "num_classes", ",", "kernel_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.stgcn_head.STGCNHead.init_weights": [[47, 49], ["mmcv.cnn.normal_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "normal_init", "(", "self", ".", "fc", ",", "std", "=", "self", ".", "init_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.heads.stgcn_head.STGCNHead.forward": [[50, 62], ["stgcn_head.STGCNHead.pool", "x.view.view.view().mean", "stgcn_head.STGCNHead.fc", "x.view.view.view", "x.view.view.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# global pooling", "\n", "        ", "assert", "self", ".", "pool", "is", "not", "None", "\n", "x", "=", "self", ".", "pool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", "//", "self", ".", "num_person", ",", "self", ".", "num_person", ",", "-", "1", ",", "1", ",", "\n", "1", ")", ".", "mean", "(", "dim", "=", "1", ")", "\n", "\n", "# prediction", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TABlock.__init__": [[29, 41], ["dict", "torch.Module.__init__", "copy.deepcopy", "common.TAM", "isinstance", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "block", ",", "num_segments", ",", "tam_cfg", "=", "dict", "(", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tam_cfg", "=", "deepcopy", "(", "tam_cfg", ")", "\n", "self", ".", "block", "=", "block", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "tam", "=", "TAM", "(", "\n", "in_channels", "=", "block", ".", "conv1", ".", "out_channels", ",", "\n", "num_segments", "=", "num_segments", ",", "\n", "**", "self", ".", "tam_cfg", ")", "\n", "\n", "if", "not", "isinstance", "(", "self", ".", "block", ",", "Bottleneck", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'TA-Blocks have not been fully '", "\n", "'implemented except the pattern based '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TABlock.forward": [[44, 71], ["isinstance", "tanet.TABlock.block.relu", "tanet.TABlock.block.conv1", "tanet.TABlock.tam", "tanet.TABlock.block.conv2", "tanet.TABlock.block.conv3", "torch.utils.checkpoint.checkpoint", "tanet.TABlock.forward._inner_forward"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "assert", "isinstance", "(", "self", ".", "block", ",", "Bottleneck", ")", "\n", "\n", "def", "_inner_forward", "(", "x", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "block", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "tam", "(", "out", ")", "\n", "out", "=", "self", ".", "block", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "block", ".", "conv3", "(", "out", ")", "\n", "\n", "if", "self", ".", "block", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "block", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "block", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "\n", "", "out", "=", "self", ".", "block", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TANet.__init__": [[92, 97], ["dict", "resnet.ResNet.__init__", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "depth", ",", "num_segments", ",", "tam_cfg", "=", "dict", "(", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "depth", ",", "**", "kwargs", ")", "\n", "assert", "num_segments", ">=", "3", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "tam_cfg", "=", "deepcopy", "(", "tam_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TANet.init_weights": [[98, 101], ["super().init_weights", "tanet.TANet.make_tam_modeling"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TANet.make_tam_modeling"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "init_weights", "(", ")", "\n", "self", ".", "make_tam_modeling", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.tanet.TANet.make_tam_modeling": [[102, 116], ["range", "dict", "list", "enumerate", "torch.Sequential", "getattr", "setattr", "stage.children", "tanet.TABlock", "tanet.TANet.make_tam_modeling.make_tam_block"], "methods", ["None"], ["", "def", "make_tam_modeling", "(", "self", ")", ":", "\n", "        ", "\"\"\"Replace ResNet-Block with TA-Block.\"\"\"", "\n", "\n", "def", "make_tam_block", "(", "stage", ",", "num_segments", ",", "tam_cfg", "=", "dict", "(", ")", ")", ":", "\n", "            ", "blocks", "=", "list", "(", "stage", ".", "children", "(", ")", ")", "\n", "for", "i", ",", "block", "in", "enumerate", "(", "blocks", ")", ":", "\n", "                ", "blocks", "[", "i", "]", "=", "TABlock", "(", "block", ",", "num_segments", ",", "deepcopy", "(", "tam_cfg", ")", ")", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "self", ".", "num_stages", ")", ":", "\n", "            ", "layer_name", "=", "f'layer{i + 1}'", "\n", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "setattr", "(", "self", ",", "layer_name", ",", "\n", "make_tam_block", "(", "res_layer", ",", "self", ".", "num_segments", ",", "self", ".", "tam_cfg", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.c3d.C3D.__init__": [[32, 84], ["torch.Module.__init__", "dict", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.MaxPool3d", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.Dropout", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "pretrained", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "dropout_ratio", "=", "0.5", ",", "\n", "init_std", "=", "0.005", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "conv_cfg", "is", "None", ":", "\n", "            ", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", "\n", "", "if", "act_cfg", "is", "None", ":", "\n", "            ", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", "\n", "", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "dropout_ratio", "=", "dropout_ratio", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "c3d_conv_param", "=", "dict", "(", "\n", "kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", ",", "\n", "padding", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv1a", "=", "ConvModule", "(", "3", ",", "64", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "pool1", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "1", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv2a", "=", "ConvModule", "(", "64", ",", "128", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv3a", "=", "ConvModule", "(", "128", ",", "256", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "conv3b", "=", "ConvModule", "(", "256", ",", "256", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "pool3", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv4a", "=", "ConvModule", "(", "256", ",", "512", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "conv4b", "=", "ConvModule", "(", "512", ",", "512", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "pool4", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "\n", "self", ".", "conv5a", "=", "ConvModule", "(", "512", ",", "512", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "conv5b", "=", "ConvModule", "(", "512", ",", "512", ",", "**", "c3d_conv_param", ")", "\n", "self", ".", "pool5", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "(", "2", ",", "2", ",", "2", ")", ",", "stride", "=", "(", "2", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "fc6", "=", "nn", ".", "Linear", "(", "8192", ",", "4096", ")", "\n", "self", ".", "fc7", "=", "nn", ".", "Linear", "(", "4096", ",", "4096", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_ratio", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.c3d.C3D.init_weights": [[85, 105], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "c3d.C3D.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.normal_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "load_checkpoint", "(", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "\n", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                    ", "normal_init", "(", "m", ",", "std", "=", "self", ".", "init_std", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.c3d.C3D.forward": [[106, 141], ["c3d.C3D.conv1a", "c3d.C3D.pool1", "c3d.C3D.conv2a", "c3d.C3D.pool2", "c3d.C3D.conv3a", "c3d.C3D.conv3b", "c3d.C3D.pool3", "c3d.C3D.conv4a", "c3d.C3D.conv4b", "c3d.C3D.pool4", "c3d.C3D.conv5a", "c3d.C3D.conv5b", "c3d.C3D.pool5", "c3d.C3D.flatten", "c3d.C3D.relu", "c3d.C3D.dropout", "c3d.C3D.relu", "c3d.C3D.fc6", "c3d.C3D.fc7"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n                the size of x is (num_batches, 3, 16, 112, 112).\n\n        Returns:\n            torch.Tensor: The feature of the input\n            samples extracted by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1a", "(", "x", ")", "\n", "x", "=", "self", ".", "pool1", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv2a", "(", "x", ")", "\n", "x", "=", "self", ".", "pool2", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv3a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3b", "(", "x", ")", "\n", "x", "=", "self", ".", "pool3", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv4a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv4b", "(", "x", ")", "\n", "x", "=", "self", ".", "pool4", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "conv5a", "(", "x", ")", "\n", "x", "=", "self", ".", "conv5b", "(", "x", ")", "\n", "x", "=", "self", ".", "pool5", "(", "x", ")", "\n", "\n", "x", "=", "x", ".", "flatten", "(", "start_dim", "=", "1", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "fc6", "(", "x", ")", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "fc7", "(", "x", ")", ")", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.__init__": [[29, 89], ["dict", "torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "int", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "mmcv.cnn.build_norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "resnet2plus1d.Conv2plus1d_reshape.init_weights", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "len", "(", "stride", ")", "==", "len", "(", "padding", ")", "==", "3", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "output_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "self", ".", "transposed", "=", "False", "\n", "\n", "# The middle-plane is calculated according to:", "\n", "# M_i = \\floor{\\frac{t * d^2 N_i-1 * N_i}", "\n", "#   {d^2 * N_i-1 + t * N_i}}", "\n", "# where d, t are spatial and temporal kernel, and", "\n", "# N_i, N_i-1 are planes", "\n", "# and inplanes. https://arxiv.org/pdf/1711.11248.pdf", "\n", "mid_channels", "=", "3", "*", "(", "\n", "in_channels", "*", "out_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", ")", "\n", "mid_channels", "/=", "(", "\n", "in_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "+", "3", "*", "out_channels", ")", "\n", "mid_channels", "=", "int", "(", "mid_channels", ")", "\n", "\n", "\n", "self", ".", "conv_s", "=", "nn", ".", "Conv3d", "(", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "_", ",", "self", ".", "bn_s", "=", "build_norm_layer", "(", "self", ".", "norm_cfg", ",", "mid_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv_t", "=", "nn", ".", "Conv3d", "(", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "self", ".", "mid_channel", "=", "mid_channels", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.forward": [[90, 102], ["resnet2plus1d.Conv2plus1d_reshape.conv_s", "resnet2plus1d.Conv2plus1d_reshape.bn_s", "resnet2plus1d.Conv2plus1d_reshape.relu", "resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv"], ["\n", "x", "=", "self", ".", "conv_s", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_s", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_t", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.init_weights": [[103, 108], ["mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["kaiming_init", "(", "self", ".", "conv_s", ")", "\n", "kaiming_init", "(", "self", ".", "conv_t", ")", "\n", "constant_init", "(", "self", ".", "bn_s", ",", "1", ",", "bias", "=", "0", ")", "\n", "\n", "\n", "", "", "@", "CONV_LAYERS", ".", "register_module", "(", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.reshape_temporal_conv": [[111, 128], ["conv3d_para[].view", "resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal", "torch.conv3d", "torch.conv3d", "torch.conv3d", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "conv3d_para[].view.size", "motion_para.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.Conv2plus1d_reshape.apperance_pretrain_temporal": [[129, 149], ["x.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "einops.rearrange", "einops.rearrange", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "einops.rearrange", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "len", "(", "stride", ")", "==", "len", "(", "padding", ")", "==", "3", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.ResNet2Plus1d.__init__": [[162, 166], ["resnet3d.ResNet3d.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["mid_channels", "/=", "(", "\n", "in_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "+", "3", "*", "out_channels", ")", "\n", "mid_channels", "=", "int", "(", "mid_channels", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.ResNet2Plus1d._freeze_stages": [[167, 180], ["range", "resnet2plus1d.ResNet2Plus1d.conv1.eval", "resnet2plus1d.ResNet2Plus1d.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["self", ".", "conv_s", "=", "nn", ".", "Conv3d", "(", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "_", ",", "self", ".", "bn_s", "=", "build_norm_layer", "(", "self", ".", "norm_cfg", ",", "mid_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv_t", "=", "nn", ".", "Conv3d", "(", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet2plus1d.ResNet2Plus1d.forward": [[181, 199], ["resnet2plus1d.ResNet2Plus1d.conv1", "resnet2plus1d.ResNet2Plus1d.maxpool", "getattr", "getattr."], "methods", ["None"], ["padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "self", ".", "mid_channel", "=", "mid_channels", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv_s", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_s", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "reshape_temporal_conv", "(", "self", ".", "conv_t", ",", "x", ")", "\n", "return", "x", "\n", "\n", "", "def", "init_weights", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.SEModule.__init__": [[21, 31], ["torch.Module.__init__", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "torch.AdaptiveAvgPool3d", "x3d.SEModule._round_width", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Conv3d", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_width"], ["self", ".", "avg_pool", "=", "nn", ".", "AdaptiveAvgPool3d", "(", "1", ")", "\n", "self", ".", "bottleneck", "=", "self", ".", "_round_width", "(", "channels", ",", "reduction", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Conv3d", "(", "\n", "channels", ",", "self", ".", "bottleneck", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Conv3d", "(", "\n", "self", ".", "bottleneck", ",", "channels", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_round_width", "(", "width", ",", "multiplier", ",", "min_width", "=", "8", ",", "divisor", "=", "8", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.SEModule._round_width": [[32, 41], ["max", "int", "int"], "methods", ["None"], ["        ", "width", "*=", "multiplier", "\n", "min_width", "=", "min_width", "or", "divisor", "\n", "width_out", "=", "max", "(", "min_width", ",", "\n", "int", "(", "width", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "if", "width_out", "<", "0.9", "*", "width", ":", "\n", "            ", "width_out", "+=", "divisor", "\n", "", "return", "int", "(", "width_out", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "module_input", "=", "x", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.SEModule.forward": [[42, 50], ["x3d.SEModule.avg_pool", "x3d.SEModule.fc1", "x3d.SEModule.relu", "x3d.SEModule.fc2", "x3d.SEModule.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["x", "=", "self", ".", "avg_pool", "(", "x", ")", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "sigmoid", "(", "x", ")", "\n", "return", "module_input", "*", "x", "\n", "\n", "\n", "", "", "class", "BlockX3D", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.BlockX3D.__init__": [[75, 144], ["dict", "dict", "dict", "torch.Module.__init__", "dict", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.Swish", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "x3d.SEModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["planes", ",", "\n", "outplanes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "se_ratio", "=", "None", ",", "\n", "use_swish", "=", "True", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "outplanes", "=", "outplanes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "se_ratio", "=", "se_ratio", "\n", "self", ".", "use_swish", "=", "use_swish", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "act_cfg_swish", "=", "dict", "(", "type", "=", "'Swish'", ")", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "in_channels", "=", "inplanes", ",", "\n", "out_channels", "=", "planes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "# Here we use the channel-wise conv", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "in_channels", "=", "planes", ",", "\n", "out_channels", "=", "planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "(", "1", ",", "self", ".", "spatial_stride", ",", "self", ".", "spatial_stride", ")", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "planes", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "swish", "=", "Swish", "(", ")", "\n", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "in_channels", "=", "planes", ",", "\n", "out_channels", "=", "outplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "if", "self", ".", "se_ratio", "is", "not", "None", ":", "\n", "            ", "self", ".", "se_module", "=", "SEModule", "(", "planes", ",", "self", ".", "se_ratio", ")", "\n", "\n", "", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.BlockX3D.forward": [[145, 176], ["x3d.BlockX3D.relu", "x3d.BlockX3D.conv1", "x3d.BlockX3D.swish", "x3d.BlockX3D.conv3", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "x3d.BlockX3D.forward._inner_forward"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.swish"], ["            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "if", "self", ".", "se_ratio", "is", "not", "None", ":", "\n", "                ", "out", "=", "self", ".", "se_module", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "swish", "(", "out", ")", "\n", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "return", "out", "\n", "\n", "\n", "\n", "# We do not support initialize with 2D pretrain weight for X3D", "\n", "\n", "", "", "class", "X3D", "(", "nn", ".", "Module", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D.__init__": [[219, 319], ["dict", "dict", "dict", "torch.Module.__init__", "x3d.X3D._round_width", "x3d.X3D._make_stem_layer", "enumerate", "mmcv.cnn.ConvModule", "int", "x3d.X3D._round_repeats", "len", "int", "x3d.X3D.make_res_layer", "x3d.X3D.add_module", "x3d.X3D.res_layers.append", "int", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_width", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_repeats", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["in_channels", "=", "3", ",", "\n", "num_stages", "=", "4", ",", "\n", "spatial_strides", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "se_style", "=", "'half'", ",", "\n", "se_ratio", "=", "1", "/", "16", ",", "\n", "use_swish", "=", "True", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma_w", "=", "gamma_w", "\n", "self", ".", "gamma_b", "=", "gamma_b", "\n", "self", ".", "gamma_d", "=", "gamma_d", "\n", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "# Hard coded, can be changed by gamma_w", "\n", "self", ".", "base_channels", "=", "24", "\n", "self", ".", "stage_blocks", "=", "[", "1", ",", "2", ",", "5", ",", "3", "]", "\n", "\n", "# apply parameters gamma_w and gamma_d", "\n", "self", ".", "base_channels", "=", "self", ".", "_round_width", "(", "self", ".", "base_channels", ",", "\n", "self", ".", "gamma_w", ")", "\n", "\n", "self", ".", "stage_blocks", "=", "[", "\n", "self", ".", "_round_repeats", "(", "x", ",", "self", ".", "gamma_d", ")", "for", "x", "in", "self", ".", "stage_blocks", "\n", "]", "\n", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "spatial_strides", "=", "spatial_strides", "\n", "assert", "len", "(", "spatial_strides", ")", "==", "num_stages", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "\n", "self", ".", "se_style", "=", "se_style", "\n", "assert", "self", ".", "se_style", "in", "[", "'all'", ",", "'half'", "]", "\n", "self", ".", "se_ratio", "=", "se_ratio", "\n", "assert", "(", "self", ".", "se_ratio", "is", "None", ")", "or", "(", "self", ".", "se_ratio", ">", "0", ")", "\n", "self", ".", "use_swish", "=", "use_swish", "\n", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "self", ".", "block", "=", "BlockX3D", "\n", "self", ".", "stage_blocks", "=", "self", ".", "stage_blocks", "[", ":", "num_stages", "]", "\n", "self", ".", "layer_inplanes", "=", "self", ".", "base_channels", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "spatial_stride", "=", "spatial_strides", "[", "i", "]", "\n", "inplanes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "planes", "=", "int", "(", "inplanes", "*", "self", ".", "gamma_b", ")", "\n", "\n", "res_layer", "=", "self", ".", "make_res_layer", "(", "\n", "self", ".", "block", ",", "\n", "self", ".", "layer_inplanes", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "se_style", "=", "self", ".", "se_style", ",", "\n", "se_ratio", "=", "self", ".", "se_ratio", ",", "\n", "use_swish", "=", "self", ".", "use_swish", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "layer_inplanes", "=", "inplanes", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "base_channels", "*", "2", "**", "(", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "self", ".", "conv5", "=", "ConvModule", "(", "\n", "self", ".", "feat_dim", ",", "\n", "int", "(", "self", ".", "feat_dim", "*", "self", ".", "gamma_b", ")", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "self", ".", "feat_dim", "=", "int", "(", "self", ".", "feat_dim", "*", "self", ".", "gamma_b", ")", "\n", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_round_width", "(", "width", ",", "multiplier", ",", "min_depth", "=", "8", ",", "divisor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Round width of filters based on width multiplier.\"\"\"", "\n", "if", "not", "multiplier", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_width": [[320, 333], ["max", "int", "int"], "methods", ["None"], ["            ", "return", "width", "\n", "\n", "", "width", "*=", "multiplier", "\n", "min_depth", "=", "min_depth", "or", "divisor", "\n", "new_filters", "=", "max", "(", "min_depth", ",", "\n", "int", "(", "width", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "if", "new_filters", "<", "0.9", "*", "width", ":", "\n", "            ", "new_filters", "+=", "divisor", "\n", "", "return", "int", "(", "new_filters", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_round_repeats", "(", "repeats", ",", "multiplier", ")", ":", "\n", "        ", "\"\"\"Round number of layers based on depth multiplier.\"\"\"", "\n", "if", "not", "multiplier", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._round_repeats": [[334, 340], ["int", "math.ceil"], "methods", ["None"], ["            ", "return", "repeats", "\n", "", "return", "int", "(", "math", ".", "ceil", "(", "multiplier", "*", "repeats", ")", ")", "\n", "\n", "# the module is parameterized with gamma_b", "\n", "# no temporal_stride", "\n", "", "def", "make_res_layer", "(", "self", ",", "\n", "block", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D.make_res_layer": [[343, 442], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule", "block", "layers.append", "block", "range"], "methods", ["None"], ["planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "se_style", "=", "'half'", ",", "\n", "se_ratio", "=", "None", ",", "\n", "use_swish", "=", "True", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Build residual layer for ResNet3D.\n\n        Args:\n            block (nn.Module): Residual module to be built.\n            layer_inplanes (int): Number of channels for the input feature\n                of the res layer.\n            inplanes (int): Number of channels for the input feature in each\n                block, which equals to base_channels * gamma_w.\n            planes (int): Number of channels for the output feature in each\n                block, which equals to base_channel * gamma_w * gamma_b.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int): Spatial strides in residual and conv layers.\n                Default: 1.\n            se_style (str): The style of inserting SE modules into BlockX3D,\n                'half' denotes insert into half of the blocks, while 'all'\n                denotes insert into all blocks. Default: 'half'.\n            se_ratio (float | None): The reduction ratio of squeeze and\n                excitation unit. If set as None, it means not using SE unit.\n                Default: None.\n            use_swish (bool): Whether to use swish as the activation function\n                before and after the 3x3x3 conv. Default: True.\n            conv_cfg (dict | None): Config for norm layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool | None): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n\n        Returns:\n            nn.Module: A residual layer for the given config.\n        \"\"\"", "\n", "downsample", "=", "None", "\n", "if", "spatial_stride", "!=", "1", "or", "layer_inplanes", "!=", "inplanes", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "layer_inplanes", ",", "\n", "inplanes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "1", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "padding", "=", "0", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "use_se", "=", "[", "False", "]", "*", "blocks", "\n", "if", "self", ".", "se_style", "==", "'all'", ":", "\n", "            ", "use_se", "=", "[", "True", "]", "*", "blocks", "\n", "", "elif", "self", ".", "se_style", "==", "'half'", ":", "\n", "            ", "use_se", "=", "[", "i", "%", "2", "==", "0", "for", "i", "in", "range", "(", "blocks", ")", "]", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "layer_inplanes", ",", "\n", "planes", ",", "\n", "inplanes", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "downsample", "=", "downsample", ",", "\n", "se_ratio", "=", "se_ratio", "if", "use_se", "[", "0", "]", "else", "None", ",", "\n", "use_swish", "=", "use_swish", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "inplanes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "se_ratio", "=", "se_ratio", "if", "use_se", "[", "i", "]", "else", "None", ",", "\n", "use_swish", "=", "use_swish", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n", "", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1_s", "=", "ConvModule", "(", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._make_stem_layer": [[443, 467], ["mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule"], "methods", ["None"], ["self", ".", "in_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "conv1_t", "=", "ConvModule", "(", "\n", "self", ".", "base_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "(", "5", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "2", ",", "0", ",", "0", ")", ",", "\n", "groups", "=", "self", ".", "base_channels", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D._freeze_stages": [[468, 484], ["range", "x3d.X3D.conv1_s.eval", "x3d.X3D.conv1_t.eval", "x3d.X3D.conv1_s.parameters", "x3d.X3D.conv1_t.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["            ", "self", ".", "conv1_s", ".", "eval", "(", ")", "\n", "self", ".", "conv1_t", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1_s", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "param", "in", "self", ".", "conv1_t", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D.init_weights": [[485, 507], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "x3d.X3D.modules", "TypeError", "isinstance", "x3d.X3D.modules", "mmcv.cnn.kaiming_init", "isinstance", "isinstance", "mmcv.cnn.constant_init", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n", "# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ",", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D.forward": [[508, 525], ["x3d.X3D.conv1_s", "x3d.X3D.conv1_t", "x3d.X3D.conv5", "getattr", "getattr."], "methods", ["None"], ["\n", "x", "=", "self", ".", "conv1_s", "(", "x", ")", "\n", "x", "=", "self", ".", "conv1_t", "(", "x", ")", "\n", "for", "layer_name", "in", "self", ".", "res_layers", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "x", "=", "res_layer", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "return", "x", "\n", "\n", "\n", "", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.x3d.X3D.train": [[526, 534], ["super().train", "x3d.X3D._freeze_stages", "x3d.X3D.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.InvertedResidual.__init__": [[58, 105], ["dict", "dict", "torch.Module.__init__", "int", "layers.extend", "torch.Sequential", "torch.Sequential", "round", "layers.append", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "stride", ",", "\n", "expand_ratio", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU6'", ")", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", "InvertedResidual", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stride", "=", "stride", "\n", "assert", "stride", "in", "[", "1", ",", "2", "]", ",", "f'stride must in [1, 2]. '", "f'But received {stride}.'", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "use_res_connect", "=", "self", ".", "stride", "==", "1", "and", "in_channels", "==", "out_channels", "\n", "hidden_dim", "=", "int", "(", "round", "(", "in_channels", "*", "expand_ratio", ")", ")", "\n", "\n", "layers", "=", "[", "]", "\n", "if", "expand_ratio", "!=", "1", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "ConvModule", "(", "\n", "in_channels", "=", "in_channels", ",", "\n", "out_channels", "=", "hidden_dim", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", ")", "\n", "", "layers", ".", "extend", "(", "[", "\n", "ConvModule", "(", "\n", "in_channels", "=", "hidden_dim", ",", "\n", "out_channels", "=", "hidden_dim", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "1", ",", "\n", "groups", "=", "hidden_dim", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", ",", "\n", "ConvModule", "(", "\n", "in_channels", "=", "hidden_dim", ",", "\n", "out_channels", "=", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "]", ")", "\n", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.InvertedResidual.forward": [[106, 120], ["mobilenet_v2.InvertedResidual.conv", "torch.checkpoint", "torch.checkpoint", "mobilenet_v2.InvertedResidual.forward._inner_forward"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "def", "_inner_forward", "(", "x", ")", ":", "\n", "            ", "if", "self", ".", "use_res_connect", ":", "\n", "                ", "return", "x", "+", "self", ".", "conv", "(", "x", ")", "\n", "\n", "", "return", "self", ".", "conv", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.__init__": [[154, 226], ["dict", "dict", "dict", "torch.Module.__init__", "mobilenet_v2.make_divisible", "mmcv.cnn.ConvModule", "enumerate", "mmcv.cnn.ConvModule", "mobilenet_v2.MobileNetV2.add_module", "mobilenet_v2.MobileNetV2.layers.append", "range", "ValueError", "mobilenet_v2.make_divisible", "mobilenet_v2.MobileNetV2.make_layer", "mobilenet_v2.MobileNetV2.add_module", "mobilenet_v2.MobileNetV2.layers.append", "int", "range", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.make_layer"], ["def", "__init__", "(", "self", ",", "\n", "pretrained", "=", "None", ",", "\n", "widen_factor", "=", "1.", ",", "\n", "out_indices", "=", "(", "7", ",", ")", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN2d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU6'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "widen_factor", "=", "widen_factor", "\n", "self", ".", "out_indices", "=", "out_indices", "\n", "for", "index", "in", "out_indices", ":", "\n", "            ", "if", "index", "not", "in", "range", "(", "0", ",", "8", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'the item in out_indices must in '", "\n", "f'range(0, 8). But received {index}'", ")", "\n", "\n", "", "", "if", "frozen_stages", "not", "in", "range", "(", "-", "1", ",", "9", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'frozen_stages must be in range(-1, 9). '", "\n", "f'But received {frozen_stages}'", ")", "\n", "", "self", ".", "out_indices", "=", "out_indices", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n", "self", ".", "in_channels", "=", "make_divisible", "(", "32", "*", "widen_factor", ",", "8", ")", "\n", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "in_channels", "=", "3", ",", "\n", "out_channels", "=", "self", ".", "in_channels", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "layers", "=", "[", "]", "\n", "\n", "for", "i", ",", "layer_cfg", "in", "enumerate", "(", "self", ".", "arch_settings", ")", ":", "\n", "            ", "expand_ratio", ",", "channel", ",", "num_blocks", ",", "stride", "=", "layer_cfg", "\n", "out_channels", "=", "make_divisible", "(", "channel", "*", "widen_factor", ",", "8", ")", "\n", "inverted_res_layer", "=", "self", ".", "make_layer", "(", "\n", "out_channels", "=", "out_channels", ",", "\n", "num_blocks", "=", "num_blocks", ",", "\n", "stride", "=", "stride", ",", "\n", "expand_ratio", "=", "expand_ratio", ")", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "inverted_res_layer", ")", "\n", "self", ".", "layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "if", "widen_factor", ">", "1.0", ":", "\n", "            ", "self", ".", "out_channel", "=", "int", "(", "1280", "*", "widen_factor", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out_channel", "=", "1280", "\n", "\n", "", "layer", "=", "ConvModule", "(", "\n", "in_channels", "=", "self", ".", "in_channels", ",", "\n", "out_channels", "=", "self", ".", "out_channel", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "self", ".", "add_module", "(", "'conv2'", ",", "layer", ")", "\n", "self", ".", "layers", ".", "append", "(", "'conv2'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.make_layer": [[227, 254], ["range", "torch.Sequential", "torch.Sequential", "layers.append", "mobilenet_v2.InvertedResidual"], "methods", ["None"], ["", "def", "make_layer", "(", "self", ",", "out_channels", ",", "num_blocks", ",", "stride", ",", "expand_ratio", ")", ":", "\n", "        ", "\"\"\"Stack InvertedResidual blocks to build a layer for MobileNetV2.\n\n        Args:\n            out_channels (int): out_channels of block.\n            num_blocks (int): number of blocks.\n            stride (int): stride of the first block. Default: 1\n            expand_ratio (int): Expand the number of channels of the\n                hidden layer in InvertedResidual by this ratio. Default: 6.\n        \"\"\"", "\n", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_blocks", ")", ":", "\n", "            ", "if", "i", ">=", "1", ":", "\n", "                ", "stride", "=", "1", "\n", "", "layers", ".", "append", "(", "\n", "InvertedResidual", "(", "\n", "self", ".", "in_channels", ",", "\n", "out_channels", ",", "\n", "stride", ",", "\n", "expand_ratio", "=", "expand_ratio", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "with_cp", "=", "self", ".", "with_cp", ")", ")", "\n", "self", ".", "in_channels", "=", "out_channels", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.init_weights": [[255, 267], ["isinstance", "utils.get_root_logger", "mmcv.runner.load_checkpoint", "mobilenet_v2.MobileNetV2.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "load_checkpoint", "(", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "(", "_BatchNorm", ",", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.forward": [[268, 282], ["mobilenet_v2.MobileNetV2.conv1", "enumerate", "tuple", "getattr", "getattr.", "len", "outs.append"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "\n", "outs", "=", "[", "]", "\n", "for", "i", ",", "layer_name", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "x", "=", "layer", "(", "x", ")", "\n", "if", "i", "in", "self", ".", "out_indices", ":", "\n", "                ", "outs", ".", "append", "(", "x", ")", "\n", "\n", "", "", "if", "len", "(", "outs", ")", "==", "1", ":", "\n", "            ", "return", "outs", "[", "0", "]", "\n", "\n", "", "return", "tuple", "(", "outs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2._freeze_stages": [[283, 294], ["range", "mobilenet_v2.MobileNetV2.conv1.eval", "mobilenet_v2.MobileNetV2.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "layer_name", "=", "self", ".", "layers", "[", "i", "-", "1", "]", "\n", "layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "layer", ".", "eval", "(", ")", "\n", "for", "param", "in", "layer", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.MobileNetV2.train": [[295, 302], ["super().train", "mobilenet_v2.MobileNetV2._freeze_stages", "mobilenet_v2.MobileNetV2.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "", "", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", "MobileNetV2", ",", "self", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2.make_divisible": [[12, 35], ["max", "int"], "function", ["None"], ["def", "make_divisible", "(", "value", ",", "divisor", ",", "min_value", "=", "None", ",", "min_ratio", "=", "0.9", ")", ":", "\n", "    ", "\"\"\"Make divisible function.\n\n    This function rounds the channel number down to the nearest value that can\n    be divisible by the divisor.\n    Args:\n        value (int): The original channel number.\n        divisor (int): The divisor to fully divide the channel number.\n        min_value (int, optional): The minimum value of the output channel.\n            Default: None, means that the minimum value equal to the divisor.\n        min_ratio (float, optional): The minimum ratio of the rounded channel\n            number to the original channel number. Default: 0.9.\n    Returns:\n        int: The modified output channel number\n    \"\"\"", "\n", "\n", "if", "min_value", "is", "None", ":", "\n", "        ", "min_value", "=", "divisor", "\n", "", "new_value", "=", "max", "(", "min_value", ",", "int", "(", "value", "+", "divisor", "/", "2", ")", "//", "divisor", "*", "divisor", ")", "\n", "# Make sure that round down does not go down by more than (1-min_ratio).", "\n", "if", "new_value", "<", "min_ratio", "*", "value", ":", "\n", "        ", "new_value", "+=", "divisor", "\n", "", "return", "new_value", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.CombineNet.__init__": [[77, 81], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "net1", ",", "net2", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net1", "=", "net1", "\n", "self", ".", "net2", "=", "net2", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.CombineNet.forward": [[82, 97], ["resnet_tin.CombineNet.net1", "resnet_tin.CombineNet.net2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# input shape: [num_batches * num_segments, C, H, W]", "\n", "# output x shape: [num_batches * num_segments, C, H, W]", "\n", "x", "=", "self", ".", "net1", "(", "x", ")", "\n", "# [num_batches * num_segments, C, H, W]", "\n", "x", "=", "self", ".", "net2", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.WeightNet.__init__": [[113, 121], ["torch.Module.__init__", "torch.Sigmoid", "torch.Sigmoid", "torch.Conv1d", "torch.Conv1d", "resnet_tin.WeightNet.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "groups", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "self", ".", "groups", "=", "groups", "\n", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "in_channels", ",", "groups", ",", "3", ",", "padding", "=", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.WeightNet.init_weights": [[122, 128], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "# we set the initial bias of the convolution", "\n", "# layer to 0, and the final initial output will be 1.0", "\n", "self", ".", "conv", ".", "bias", ".", "data", "[", "...", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.WeightNet.forward": [[129, 151], ["resnet_tin.WeightNet.conv", "x.permute.permute.view", "x.permute.permute.permute", "resnet_tin.WeightNet.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# calculate weight", "\n", "# [N, C, T]", "\n", "n", ",", "_", ",", "t", "=", "x", ".", "shape", "\n", "# [N, groups, T]", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "self", ".", "groups", ",", "t", ")", "\n", "# [N, T, groups]", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "\n", "# scale the output to range (0, 2)", "\n", "x", "=", "2", "*", "self", ".", "sigmoid", "(", "x", ")", "\n", "# [N, T, groups]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.OffsetNet.__init__": [[168, 181], ["torch.Module.__init__", "torch.Sigmoid", "torch.Sigmoid", "torch.Conv1d", "torch.Conv1d", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "resnet_tin.OffsetNet.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "groups", ",", "num_segments", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "sigmoid", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "# hard code ``kernel_size`` and ``padding`` according to original repo.", "\n", "kernel_size", "=", "3", "\n", "padding", "=", "1", "\n", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "in_channels", ",", "1", ",", "kernel_size", ",", "padding", "=", "padding", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "num_segments", ",", "num_segments", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "num_segments", ",", "groups", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.OffsetNet.init_weights": [[182, 188], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "# The bias of the last fc layer is initialized to", "\n", "# make the post-sigmoid output start from 1", "\n", "self", ".", "fc2", ".", "bias", ".", "data", "[", "...", "]", "=", "0.5108", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.OffsetNet.forward": [[189, 217], ["resnet_tin.OffsetNet.conv", "x.view.view.view", "resnet_tin.OffsetNet.relu", "resnet_tin.OffsetNet.fc2", "x.view.view.view", "resnet_tin.OffsetNet.fc1", "resnet_tin.OffsetNet.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.activations.sigmoid"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# calculate offset", "\n", "# [N, C, T]", "\n", "n", ",", "_", ",", "t", "=", "x", ".", "shape", "\n", "# [N, 1, T]", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "# [N, T]", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "t", ")", "\n", "# [N, T]", "\n", "x", "=", "self", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "# [N, groups]", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "# [N, 1, groups]", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "1", ",", "-", "1", ")", "\n", "\n", "# to make sure the output is in (-t/2, t/2)", "\n", "# where t = num_segments = 8", "\n", "x", "=", "4", "*", "(", "self", ".", "sigmoid", "(", "x", ")", "-", "0.5", ")", "\n", "# [N, 1, groups]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.TemporalInterlace.__init__": [[231, 243], ["torch.Module.__init__", "resnet_tin.OffsetNet", "resnet_tin.WeightNet"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "num_segments", "=", "3", ",", "shift_div", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "shift_div", "=", "shift_div", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "# hard code ``deform_groups`` according to original repo.", "\n", "self", ".", "deform_groups", "=", "2", "\n", "\n", "self", ".", "offset_net", "=", "OffsetNet", "(", "in_channels", "//", "shift_div", ",", "\n", "self", ".", "deform_groups", ",", "num_segments", ")", "\n", "self", ".", "weight_net", "=", "WeightNet", "(", "in_channels", "//", "shift_div", ",", "\n", "self", ".", "deform_groups", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.TemporalInterlace.forward": [[244, 306], ["x.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x[].view", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "x_pooled.permute().contiguous.permute().contiguous.permute().contiguous", "resnet_tin.TemporalInterlace.offset_net().view", "resnet_tin.TemporalInterlace.weight_net", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "resnet_tin.linear_sampler", "x_weight.view.view.repeat", "x_weight.view.view.view", "x_shift.contiguous().view.contiguous().view.contiguous().view", "x_weight.view.view.size", "x_weight.view.view.size", "x_pooled.permute().contiguous.permute().contiguous.permute", "resnet_tin.TemporalInterlace.offset_net", "x_shift.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.linear_sampler"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# x: [N, C, H, W],", "\n", "# where N = num_batches x num_segments, C = shift_div * num_folds", "\n", "n", ",", "c", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "num_batches", "=", "n", "//", "self", ".", "num_segments", "\n", "num_folds", "=", "c", "//", "self", ".", "shift_div", "\n", "\n", "# x_out: [num_batches x num_segments, C, H, W]", "\n", "x_out", "=", "torch", ".", "zeros", "(", "(", "n", ",", "c", ",", "h", ",", "w", ")", ",", "device", "=", "x", ".", "device", ")", "\n", "# x_descriptor: [num_batches, num_segments, num_folds, H, W]", "\n", "x_descriptor", "=", "x", "[", ":", ",", ":", "num_folds", ",", ":", ",", ":", "]", ".", "view", "(", "num_batches", ",", "\n", "self", ".", "num_segments", ",", "\n", "num_folds", ",", "h", ",", "w", ")", "\n", "\n", "# x should only obtain information on temporal and channel dimensions", "\n", "# x_pooled: [num_batches, num_segments, num_folds, W]", "\n", "x_pooled", "=", "torch", ".", "mean", "(", "x_descriptor", ",", "3", ")", "\n", "# x_pooled: [num_batches, num_segments, num_folds]", "\n", "x_pooled", "=", "torch", ".", "mean", "(", "x_pooled", ",", "3", ")", "\n", "# x_pooled: [num_batches, num_folds, num_segments]", "\n", "x_pooled", "=", "x_pooled", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Calculate weight and bias, here groups = 2", "\n", "# x_offset: [num_batches, groups]", "\n", "x_offset", "=", "self", ".", "offset_net", "(", "x_pooled", ")", ".", "view", "(", "num_batches", ",", "-", "1", ")", "\n", "# x_weight: [num_batches, num_segments, groups]", "\n", "x_weight", "=", "self", ".", "weight_net", "(", "x_pooled", ")", "\n", "\n", "# x_offset: [num_batches, 2 * groups]", "\n", "x_offset", "=", "torch", ".", "cat", "(", "[", "x_offset", ",", "-", "x_offset", "]", ",", "1", ")", "\n", "# x_shift: [num_batches, num_segments, num_folds, H, W]", "\n", "x_shift", "=", "linear_sampler", "(", "x_descriptor", ",", "x_offset", ")", "\n", "\n", "# x_weight: [num_batches, num_segments, groups, 1]", "\n", "x_weight", "=", "x_weight", "[", ":", ",", ":", ",", ":", ",", "None", "]", "\n", "# x_weight:", "\n", "# [num_batches, num_segments, groups * 2, c // self.shift_div // 4]", "\n", "x_weight", "=", "x_weight", ".", "repeat", "(", "1", ",", "1", ",", "2", ",", "num_folds", "//", "2", "//", "2", ")", "\n", "# x_weight:", "\n", "# [num_batches, num_segments, c // self.shift_div = num_folds]", "\n", "x_weight", "=", "x_weight", ".", "view", "(", "x_weight", ".", "size", "(", "0", ")", ",", "x_weight", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "\n", "# x_weight: [num_batches, num_segments, num_folds, 1, 1]", "\n", "x_weight", "=", "x_weight", "[", ":", ",", ":", ",", ":", ",", "None", ",", "None", "]", "\n", "# x_shift: [num_batches, num_segments, num_folds, H, W]", "\n", "x_shift", "=", "x_shift", "*", "x_weight", "\n", "# x_shift: [num_batches, num_segments, num_folds, H, W]", "\n", "x_shift", "=", "x_shift", ".", "contiguous", "(", ")", ".", "view", "(", "n", ",", "num_folds", ",", "h", ",", "w", ")", "\n", "\n", "# x_out: [num_batches x num_segments, C, H, W]", "\n", "x_out", "[", ":", ",", ":", "num_folds", ",", ":", "]", "=", "x_shift", "\n", "x_out", "[", ":", ",", "num_folds", ":", ",", ":", "]", "=", "x", "[", ":", ",", "num_folds", ":", ",", ":", "]", "\n", "\n", "return", "x_out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.ResNetTIN.__init__": [[320, 330], ["resnet_tsm.ResNetTSM.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "num_segments", "=", "8", ",", "\n", "is_tin", "=", "True", ",", "\n", "shift_div", "=", "4", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "depth", ",", "**", "kwargs", ")", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "is_tin", "=", "is_tin", "\n", "self", ".", "shift_div", "=", "shift_div", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.ResNetTIN.make_temporal_interlace": [[331, 371], ["resnet_tin.ResNetTIN.make_temporal_interlace.make_block_interlace"], "methods", ["None"], ["", "def", "make_temporal_interlace", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make temporal interlace for some layers.\"\"\"", "\n", "num_segment_list", "=", "[", "self", ".", "num_segments", "]", "*", "4", "\n", "assert", "num_segment_list", "[", "-", "1", "]", ">", "0", "\n", "\n", "n_round", "=", "1", "\n", "if", "len", "(", "list", "(", "self", ".", "layer3", ".", "children", "(", ")", ")", ")", ">=", "23", ":", "\n", "            ", "print", "(", "f'=> Using n_round {n_round} to insert temporal shift.'", ")", "\n", "\n", "", "def", "make_block_interlace", "(", "stage", ",", "num_segments", ",", "shift_div", ")", ":", "\n", "            ", "\"\"\"Apply Deformable shift for a ResNet layer module.\n\n            Args:\n                stage (nn.module): A ResNet layer to be deformed.\n                num_segments (int): Number of frame segments.\n                shift_div (int): Number of division parts for shift.\n\n            Returns:\n                nn.Sequential: A Sequential container consisted of\n                    deformed Interlace blocks.\n            \"\"\"", "\n", "blocks", "=", "list", "(", "stage", ".", "children", "(", ")", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "blocks", ")", ":", "\n", "                ", "if", "i", "%", "n_round", "==", "0", ":", "\n", "                    ", "tds", "=", "TemporalInterlace", "(", "\n", "b", ".", "conv1", ".", "in_channels", ",", "\n", "num_segments", "=", "num_segments", ",", "\n", "shift_div", "=", "shift_div", ")", "\n", "blocks", "[", "i", "]", ".", "conv1", ".", "conv", "=", "CombineNet", "(", "tds", ",", "\n", "blocks", "[", "i", "]", ".", "conv1", ".", "conv", ")", "\n", "", "", "return", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n", "", "self", ".", "layer1", "=", "make_block_interlace", "(", "self", ".", "layer1", ",", "num_segment_list", "[", "0", "]", ",", "\n", "self", ".", "shift_div", ")", "\n", "self", ".", "layer2", "=", "make_block_interlace", "(", "self", ".", "layer2", ",", "num_segment_list", "[", "1", "]", ",", "\n", "self", ".", "shift_div", ")", "\n", "self", ".", "layer3", "=", "make_block_interlace", "(", "self", ".", "layer3", ",", "num_segment_list", "[", "2", "]", ",", "\n", "self", ".", "shift_div", ")", "\n", "self", ".", "layer4", "=", "make_block_interlace", "(", "self", ".", "layer4", ",", "num_segment_list", "[", "3", "]", ",", "\n", "self", ".", "shift_div", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.ResNetTIN.init_weights": [[372, 380], ["super().init_weights", "resnet_tin.ResNetTIN.make_temporal_interlace", "len", "resnet_tin.ResNetTIN.make_non_local"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.ResNetTIN.make_temporal_interlace", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_non_local"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "super", "(", "ResNetTSM", ",", "self", ")", ".", "init_weights", "(", ")", "\n", "if", "self", ".", "is_tin", ":", "\n", "            ", "self", ".", "make_temporal_interlace", "(", ")", "\n", "", "if", "len", "(", "self", ".", "non_local_cfg", ")", "!=", "0", ":", "\n", "            ", "self", ".", "make_non_local", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tin.linear_sampler": [[18, 65], ["torch.floor().int", "torch.floor().int", "data.view().contiguous.view().contiguous", "tin_shift", "tin_shift", "weight0[].repeat", "weight0.view.view", "weight1[].repeat", "weight1.view.view", "output.view.view", "weight0.view.size", "weight1.view.size", "torch.floor", "torch.floor", "data.view().contiguous.view", "torch.floor().int.float"], "function", ["None"], ["", "", "def", "linear_sampler", "(", "data", ",", "offset", ")", ":", "\n", "    ", "\"\"\"Differentiable Temporal-wise Frame Sampling, which is essentially a\n    linear interpolation process.\n\n    It gets the feature map which has been split into several groups\n    and shift them by different offsets according to their groups.\n    Then compute the weighted sum along with the temporal dimension.\n\n    Args:\n        data (torch.Tensor): Split data for certain group in shape\n            [N, num_segments, C, H, W].\n        offset (torch.Tensor): Data offsets for this group data in shape\n            [N, num_segments].\n    \"\"\"", "\n", "# [N, num_segments, C, H, W]", "\n", "n", ",", "t", ",", "c", ",", "h", ",", "w", "=", "data", ".", "shape", "\n", "\n", "# offset0, offset1: [N, num_segments]", "\n", "offset0", "=", "torch", ".", "floor", "(", "offset", ")", ".", "int", "(", ")", "\n", "offset1", "=", "offset0", "+", "1", "\n", "\n", "# data, data0, data1: [N, num_segments, C, H * W]", "\n", "data", "=", "data", ".", "view", "(", "n", ",", "t", ",", "c", ",", "h", "*", "w", ")", ".", "contiguous", "(", ")", "\n", "data0", "=", "tin_shift", "(", "data", ",", "offset0", ")", "\n", "data1", "=", "tin_shift", "(", "data", ",", "offset1", ")", "\n", "\n", "# weight0, weight1: [N, num_segments]", "\n", "weight0", "=", "1", "-", "(", "offset", "-", "offset0", ".", "float", "(", ")", ")", "\n", "weight1", "=", "1", "-", "weight0", "\n", "\n", "# weight0, weight1:", "\n", "# [N, num_segments] -> [N, num_segments, C // num_segments] -> [N, C]", "\n", "group_size", "=", "offset", ".", "shape", "[", "1", "]", "\n", "weight0", "=", "weight0", "[", ":", ",", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "1", ",", "c", "//", "group_size", ")", "\n", "weight0", "=", "weight0", ".", "view", "(", "weight0", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "weight1", "=", "weight1", "[", ":", ",", ":", ",", "None", "]", ".", "repeat", "(", "1", ",", "1", ",", "c", "//", "group_size", ")", "\n", "weight1", "=", "weight1", ".", "view", "(", "weight1", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "# weight0, weight1: [N, C] -> [N, 1, C, 1]", "\n", "weight0", "=", "weight0", "[", ":", ",", "None", ",", ":", ",", "None", "]", "\n", "weight1", "=", "weight1", "[", ":", ",", "None", ",", ":", ",", "None", "]", "\n", "\n", "# output: [N, num_segments, C, H * W] -> [N, num_segments, C, H, W]", "\n", "output", "=", "weight0", "*", "data0", "+", "weight1", "*", "data1", "\n", "output", "=", "output", ".", "view", "(", "n", ",", "t", ",", "c", ",", "h", ",", "w", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.NL3DWrapper.__init__": [[22, 29], ["dict", "torch.Module.__init__", "mmcv.cnn.NonLocal3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "block", ",", "num_segments", ",", "non_local_cfg", "=", "dict", "(", ")", ")", ":", "\n", "        ", "super", "(", "NL3DWrapper", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "block", "=", "block", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "self", ".", "non_local_block", "=", "NonLocal3d", "(", "self", ".", "block", ".", "conv3", ".", "norm", ".", "num_features", ",", "\n", "**", "self", ".", "non_local_cfg", ")", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.NL3DWrapper.forward": [[30, 39], ["resnet_tsm.NL3DWrapper.block", "x.transpose().contiguous().view.transpose().contiguous().view.size", "x.transpose().contiguous().view.transpose().contiguous().view.view().transpose().contiguous", "resnet_tsm.NL3DWrapper.non_local_block", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "x.transpose().contiguous().view.transpose().contiguous().view.view().transpose", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "x.transpose().contiguous().view.transpose().contiguous().view.view", "x.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "block", "(", "x", ")", "\n", "\n", "n", ",", "c", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "n", "//", "self", ".", "num_segments", ",", "self", ".", "num_segments", ",", "c", ",", "h", ",", "\n", "w", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "x", "=", "self", ".", "non_local_block", "(", "x", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "n", ",", "c", ",", "h", ",", "w", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.TemporalShift.__init__": [[54, 59], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "net", ",", "num_segments", "=", "3", ",", "shift_div", "=", "8", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "net", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "shift_div", "=", "shift_div", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.TemporalShift.forward": [[60, 71], ["resnet_tsm.TemporalShift.shift", "resnet_tsm.TemporalShift.net"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.TemporalShift.shift"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "self", ".", "shift", "(", "x", ",", "self", ".", "num_segments", ",", "shift_div", "=", "self", ".", "shift_div", ")", "\n", "return", "self", ".", "net", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.TemporalShift.shift": [[72, 123], ["x.view.view.size", "x.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.view", "torch.cat.view"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "shift", "(", "x", ",", "num_segments", ",", "shift_div", "=", "3", ")", ":", "\n", "        ", "\"\"\"Perform temporal shift operation on the feature.\n\n        Args:\n            x (torch.Tensor): The input feature to be shifted.\n            num_segments (int): Number of frame segments.\n            shift_div (int): Number of divisions for shift. Default: 3.\n\n        Returns:\n            torch.Tensor: The shifted feature.\n        \"\"\"", "\n", "# [N, C, H, W]", "\n", "n", ",", "c", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "\n", "# [N // num_segments, num_segments, C, H*W]", "\n", "# can't use 5 dimensional array on PPL2D backend for caffe", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "num_segments", ",", "c", ",", "h", "*", "w", ")", "\n", "\n", "# get shift fold", "\n", "fold", "=", "c", "//", "shift_div", "\n", "\n", "# split c channel into three parts:", "\n", "# left_split, mid_split, right_split", "\n", "left_split", "=", "x", "[", ":", ",", ":", ",", ":", "fold", ",", ":", "]", "\n", "mid_split", "=", "x", "[", ":", ",", ":", ",", "fold", ":", "2", "*", "fold", ",", ":", "]", "\n", "right_split", "=", "x", "[", ":", ",", ":", ",", "2", "*", "fold", ":", ",", ":", "]", "\n", "\n", "# can't use torch.zeros(*A.shape) or torch.zeros_like(A)", "\n", "# because array on caffe inference must be got by computing", "\n", "\n", "# shift left on num_segments channel in `left_split`", "\n", "zeros", "=", "left_split", "-", "left_split", "\n", "blank", "=", "zeros", "[", ":", ",", ":", "1", ",", ":", ",", ":", "]", "\n", "left_split", "=", "left_split", "[", ":", ",", "1", ":", ",", ":", ",", ":", "]", "\n", "left_split", "=", "torch", ".", "cat", "(", "(", "left_split", ",", "blank", ")", ",", "1", ")", "\n", "\n", "# shift right on num_segments channel in `mid_split`", "\n", "zeros", "=", "mid_split", "-", "mid_split", "\n", "blank", "=", "zeros", "[", ":", ",", ":", "1", ",", ":", ",", ":", "]", "\n", "mid_split", "=", "mid_split", "[", ":", ",", ":", "-", "1", ",", ":", ",", ":", "]", "\n", "mid_split", "=", "torch", ".", "cat", "(", "(", "blank", ",", "mid_split", ")", ",", "1", ")", "\n", "\n", "# right_split: no shift", "\n", "\n", "# concatenate", "\n", "out", "=", "torch", ".", "cat", "(", "(", "left_split", ",", "mid_split", ",", "right_split", ")", ",", "2", ")", "\n", "\n", "# [N, C, H, W]", "\n", "# restore the original dimension", "\n", "return", "out", ".", "view", "(", "n", ",", "c", ",", "h", ",", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.__init__": [[148, 167], ["dict", "resnet.ResNet.__init__", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "num_segments", "=", "8", ",", "\n", "is_shift", "=", "True", ",", "\n", "non_local", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "shift_div", "=", "8", ",", "\n", "shift_place", "=", "'blockres'", ",", "\n", "temporal_pool", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "depth", ",", "**", "kwargs", ")", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "is_shift", "=", "is_shift", "\n", "self", ".", "shift_div", "=", "shift_div", "\n", "self", ".", "shift_place", "=", "shift_place", "\n", "self", ".", "temporal_pool", "=", "temporal_pool", "\n", "self", ".", "non_local", "=", "non_local", "\n", "self", ".", "non_local_stages", "=", "_ntuple", "(", "self", ".", "num_stages", ")", "(", "non_local", ")", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_temporal_shift": [[168, 234], ["ValueError", "resnet_tsm.ResNetTSM.make_temporal_shift.make_block_temporal"], "methods", ["None"], ["", "def", "make_temporal_shift", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make temporal shift for some layers.\"\"\"", "\n", "if", "self", ".", "temporal_pool", ":", "\n", "            ", "num_segment_list", "=", "[", "\n", "self", ".", "num_segments", ",", "self", ".", "num_segments", "//", "2", ",", "\n", "self", ".", "num_segments", "//", "2", ",", "self", ".", "num_segments", "//", "2", "\n", "]", "\n", "", "else", ":", "\n", "            ", "num_segment_list", "=", "[", "self", ".", "num_segments", "]", "*", "4", "\n", "", "if", "num_segment_list", "[", "-", "1", "]", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "'num_segment_list[-1] must be positive'", ")", "\n", "\n", "", "if", "self", ".", "shift_place", "==", "'block'", ":", "\n", "\n", "            ", "def", "make_block_temporal", "(", "stage", ",", "num_segments", ")", ":", "\n", "                ", "\"\"\"Make temporal shift on some blocks.\n\n                Args:\n                    stage (nn.Module): Model layers to be shifted.\n                    num_segments (int): Number of frame segments.\n\n                Returns:\n                    nn.Module: The shifted blocks.\n                \"\"\"", "\n", "blocks", "=", "list", "(", "stage", ".", "children", "(", ")", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "blocks", ")", ":", "\n", "                    ", "blocks", "[", "i", "]", "=", "TemporalShift", "(", "\n", "b", ",", "num_segments", "=", "num_segments", ",", "shift_div", "=", "self", ".", "shift_div", ")", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n", "", "self", ".", "layer1", "=", "make_block_temporal", "(", "self", ".", "layer1", ",", "num_segment_list", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "make_block_temporal", "(", "self", ".", "layer2", ",", "num_segment_list", "[", "1", "]", ")", "\n", "self", ".", "layer3", "=", "make_block_temporal", "(", "self", ".", "layer3", ",", "num_segment_list", "[", "2", "]", ")", "\n", "self", ".", "layer4", "=", "make_block_temporal", "(", "self", ".", "layer4", ",", "num_segment_list", "[", "3", "]", ")", "\n", "\n", "", "elif", "'blockres'", "in", "self", ".", "shift_place", ":", "\n", "            ", "n_round", "=", "1", "\n", "if", "len", "(", "list", "(", "self", ".", "layer3", ".", "children", "(", ")", ")", ")", ">=", "23", ":", "\n", "                ", "n_round", "=", "2", "\n", "\n", "", "def", "make_block_temporal", "(", "stage", ",", "num_segments", ")", ":", "\n", "                ", "\"\"\"Make temporal shift on some blocks.\n\n                Args:\n                    stage (nn.Module): Model layers to be shifted.\n                    num_segments (int): Number of frame segments.\n\n                Returns:\n                    nn.Module: The shifted blocks.\n                \"\"\"", "\n", "blocks", "=", "list", "(", "stage", ".", "children", "(", ")", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "blocks", ")", ":", "\n", "                    ", "if", "i", "%", "n_round", "==", "0", ":", "\n", "                        ", "blocks", "[", "i", "]", ".", "conv1", ".", "conv", "=", "TemporalShift", "(", "\n", "b", ".", "conv1", ".", "conv", ",", "\n", "num_segments", "=", "num_segments", ",", "\n", "shift_div", "=", "self", ".", "shift_div", ")", "\n", "", "", "return", "nn", ".", "Sequential", "(", "*", "blocks", ")", "\n", "\n", "", "self", ".", "layer1", "=", "make_block_temporal", "(", "self", ".", "layer1", ",", "num_segment_list", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "make_block_temporal", "(", "self", ".", "layer2", ",", "num_segment_list", "[", "1", "]", ")", "\n", "self", ".", "layer3", "=", "make_block_temporal", "(", "self", ".", "layer3", ",", "num_segment_list", "[", "2", "]", ")", "\n", "self", ".", "layer4", "=", "make_block_temporal", "(", "self", ".", "layer4", ",", "num_segment_list", "[", "3", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_temporal_pool": [[235, 269], ["TemporalPool", "resnet.ResNet.__init__", "torch.MaxPool3d", "torch.MaxPool3d", "x.transpose().contiguous().view.transpose().contiguous().view.size", "x.transpose().contiguous().view.transpose().contiguous().view.view().transpose", "resnet_tsm.ResNetTSM.max_pool3d", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "resnet_tsm.ResNetTSM.net", "x.transpose().contiguous().view.transpose().contiguous().view.view", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "x.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["", "", "def", "make_temporal_pool", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make temporal pooling between layer1 and layer2, using a 3D max\n        pooling layer.\"\"\"", "\n", "\n", "class", "TemporalPool", "(", "nn", ".", "Module", ")", ":", "\n", "            ", "\"\"\"Temporal pool module.\n\n            Wrap layer2 in ResNet50 with a 3D max pooling layer.\n\n            Args:\n                net (nn.Module): Module to make temporal pool.\n                num_segments (int): Number of frame segments.\n            \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "net", ",", "num_segments", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "net", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "max_pool3d", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "(", "3", ",", "1", ",", "1", ")", ",", "stride", "=", "(", "2", ",", "1", ",", "1", ")", ",", "padding", "=", "(", "1", ",", "0", ",", "0", ")", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# [N, C, H, W]", "\n", "                ", "n", ",", "c", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "# [N // num_segments, C, num_segments, H, W]", "\n", "x", "=", "x", ".", "view", "(", "n", "//", "self", ".", "num_segments", ",", "self", ".", "num_segments", ",", "c", ",", "h", ",", "\n", "w", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# [N // num_segmnets, C, num_segments // 2, H, W]", "\n", "x", "=", "self", ".", "max_pool3d", "(", "x", ")", "\n", "# [N // 2, C, H, W]", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "n", "//", "2", ",", "c", ",", "h", ",", "w", ")", "\n", "return", "self", ".", "net", "(", "x", ")", "\n", "\n", "", "", "self", ".", "layer2", "=", "TemporalPool", "(", "self", ".", "layer2", ",", "self", ".", "num_segments", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_non_local": [[270, 285], ["range", "getattr", "enumerate", "sum", "resnet_tsm.NL3DWrapper"], "methods", ["None"], ["", "def", "make_non_local", "(", "self", ")", ":", "\n", "# This part is for ResNet50", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "num_stages", ")", ":", "\n", "            ", "non_local_stage", "=", "self", ".", "non_local_stages", "[", "i", "]", "\n", "if", "sum", "(", "non_local_stage", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "layer_name", "=", "f'layer{i + 1}'", "\n", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "\n", "for", "idx", ",", "non_local", "in", "enumerate", "(", "non_local_stage", ")", ":", "\n", "                ", "if", "non_local", ":", "\n", "                    ", "res_layer", "[", "idx", "]", "=", "NL3DWrapper", "(", "res_layer", "[", "idx", "]", ",", "\n", "self", ".", "num_segments", ",", "\n", "self", ".", "non_local_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.init_weights": [[286, 296], ["super().init_weights", "resnet_tsm.ResNetTSM.make_temporal_shift", "len", "resnet_tsm.ResNetTSM.make_non_local", "resnet_tsm.ResNetTSM.make_temporal_pool"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2_tsm.MobileNetV2TSM.make_temporal_shift", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_non_local", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_tsm.ResNetTSM.make_temporal_pool"], ["", "", "", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "super", "(", ")", ".", "init_weights", "(", ")", "\n", "if", "self", ".", "is_shift", ":", "\n", "            ", "self", ".", "make_temporal_shift", "(", ")", "\n", "", "if", "len", "(", "self", ".", "non_local_cfg", ")", "!=", "0", ":", "\n", "            ", "self", ".", "make_non_local", "(", ")", "\n", "", "if", "self", ".", "temporal_pool", ":", "\n", "            ", "self", ".", "make_temporal_pool", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.BasicBlock3d.__init__": [[55, 139], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "set().issubset", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "mmcv.cnn.NonLocal3d", "set"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "True", ",", "\n", "non_local", "=", "False", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "with_cp", "=", "False", ",", "\n", "drop_path", "=", "None", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "# make sure that only ``inflate_style`` is passed into kwargs", "\n", "print", "(", "kwargs", ")", "\n", "assert", "set", "(", "kwargs", ")", ".", "issubset", "(", "[", "'inflate_style'", "]", ")", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "inflate", "=", "inflate", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "non_local", "=", "non_local", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "self", ".", "conv1_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv2_stride_s", "=", "1", "\n", "self", ".", "conv1_stride_t", "=", "temporal_stride", "\n", "self", ".", "conv2_stride_t", "=", "1", "\n", "\n", "if", "self", ".", "inflate", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv1_padding", "=", "(", "1", ",", "dilation", ",", "dilation", ")", "\n", "conv2_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv1_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "0", ",", "1", ",", "1", ")", "\n", "\n", "", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "conv1_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "conv1_padding", ",", "\n", "dilation", "=", "(", "1", ",", "dilation", ",", "dilation", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv2_stride_t", ",", "self", ".", "conv2_stride_s", ",", "\n", "self", ".", "conv2_stride_s", ")", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "self", ".", "non_local_block", "=", "NonLocal3d", "(", "self", ".", "conv2", ".", "norm", ".", "num_features", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.BasicBlock3d.forward": [[140, 166], ["resnet3d.BasicBlock3d.relu", "resnet3d.BasicBlock3d.conv1", "resnet3d.BasicBlock3d.conv2", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "resnet3d.BasicBlock3d.forward._inner_forward"], "methods", ["None"], ["**", "self", ".", "non_local_cfg", ")", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ",", "block_id", "=", "None", ",", "drop_path", "=", "None", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "\n", "if", "drop_path", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ",", "drop_path", "=", "self", ".", "drop_path", ")", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.Bottleneck3d.__init__": [[199, 308], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.build_activation_layer", "mmcv.cnn.NonLocal3d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["\n", "expansion", "=", "4", "\n", "\n", "def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "True", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ",", "\n", "drop_path", "=", "None", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "assert", "inflate_style", "in", "[", "'3x1x1'", ",", "'3x3x3'", "]", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "inflate", "=", "inflate", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "non_local", "=", "non_local", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "\n", "if", "self", ".", "style", "==", "'pytorch'", ":", "\n", "            ", "self", ".", "conv1_stride_s", "=", "1", "\n", "self", ".", "conv2_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv1_stride_t", "=", "1", "\n", "self", ".", "conv2_stride_t", "=", "temporal_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1_stride_s", "=", "spatial_stride", "\n", "self", ".", "conv2_stride_s", "=", "1", "\n", "self", ".", "conv1_stride_t", "=", "temporal_stride", "\n", "self", ".", "conv2_stride_t", "=", "1", "\n", "\n", "", "if", "self", ".", "inflate", ":", "\n", "            ", "if", "inflate_style", "==", "'3x1x1'", ":", "\n", "                ", "conv1_kernel_size", "=", "(", "3", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "1", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "self", ".", "conv1_stride_t", "=", "1", "\n", "conv2_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "", "else", ":", "\n", "                ", "conv1_kernel_size", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "3", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "1", ",", "dilation", ",", "dilation", ")", "\n", "", "", "else", ":", "\n", "            ", "conv1_kernel_size", "=", "(", "1", ",", "1", ",", "1", ")", "\n", "conv1_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "conv2_kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "0", ",", "dilation", ",", "dilation", ")", "\n", "\n", "", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "conv1_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "conv1_padding", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "(", "self", ".", "conv2_stride_t", ",", "self", ".", "conv2_stride_s", ",", "\n", "self", ".", "conv2_stride_s", ")", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "dilation", "=", "(", "1", ",", "dilation", ",", "dilation", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "# No activation in the third ConvModule for bottleneck", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "drop_path", "=", "drop_path", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "reshape_t", "=", "reshape_t", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.Bottleneck3d.forward": [[309, 345], ["resnet3d.Bottleneck3d.relu", "resnet3d.Bottleneck3d.conv3", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "torch.checkpoint", "resnet3d.Bottleneck3d.forward._inner_forward"], "methods", ["None"], ["self", ".", "reshape_st", "=", "reshape_st", "\n", "self", ".", "relu", "=", "build_activation_layer", "(", "self", ".", "act_cfg", ")", "\n", "\n", "if", "self", ".", "non_local", ":", "\n", "            ", "self", ".", "non_local_block", "=", "NonLocal3d", "(", "self", ".", "conv3", ".", "norm", ".", "num_features", ",", "\n", "**", "self", ".", "non_local_cfg", ")", "\n", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ",", "drop_path", "=", "None", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "if", "self", ".", "inflate", ":", "\n", "                ", "if", "self", ".", "reshape_t", ":", "\n", "\n", "                    ", "out", "=", "STS_3dconv", "(", "self", ".", "conv2", ",", "x", ")", "\n", "", "else", ":", "\n", "                    ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "\n", "", "", "else", ":", "\n", "                    ", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "", "if", "self", ".", "reshape_st", ":", "\n", "                    ", "out", "=", "STS_3dconv", "(", "self", ".", "conv2", ",", "out", ")", "\n", "\n", "", "else", ":", "\n", "                     ", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "if", "drop_path", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "drop_path", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.__init__": [[420, 537], ["dict", "dict", "dict", "dict", "torch.Module.__init__", "resnet3d.ResNet3d._make_stem_layer", "enumerate", "KeyError", "max", "len", "len", "len", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "resnet3d.ResNet3d.make_res_layer", "resnet3d.ResNet3d.add_module", "resnet3d.ResNet3d.res_layers.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["\n", "\n", "arch_settings", "=", "{", "\n", "18", ":", "(", "[", "BasicBlock3d", ",", "BasicBlock3d", ",", "BasicBlock3d", ",", "BasicBlock3d", "]", ",", "(", "2", ",", "2", ",", "2", ",", "2", ")", ")", ",", "\n", "34", ":", "(", "[", "BasicBlock3d", ",", "BasicBlock3d", ",", "BasicBlock3d", ",", "BasicBlock3d", "]", ",", "(", "3", ",", "4", ",", "6", ",", "3", ")", ")", ",", "\n", "50", ":", "(", "[", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", "]", ",", "(", "3", ",", "4", ",", "6", ",", "3", ")", ")", ",", "\n", "101", ":", "(", "[", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", "]", ",", "(", "3", ",", "4", ",", "23", ",", "3", ")", ")", ",", "\n", "152", ":", "(", "[", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", ",", "Bottleneck3d", "]", ",", "(", "3", ",", "8", ",", "36", ",", "3", ")", ")", "\n", "}", "\n", "\n", "def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "stage_blocks", "=", "None", ",", "\n", "pretrained2d", "=", "False", ",", "\n", "in_channels", "=", "3", ",", "\n", "num_stages", "=", "4", ",", "\n", "base_channels", "=", "64", ",", "\n", "out_indices", "=", "(", "3", ",", ")", ",", "\n", "spatial_strides", "=", "(", "1", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "temporal_strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_kernel", "=", "(", "3", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_s", "=", "2", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_s", "=", "2", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "with_pool1", "=", "True", ",", "\n", "with_pool2", "=", "True", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "inflate", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "non_local", "=", "(", "0", ",", "0", ",", "0", ",", "0", ")", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "drop_path_rate", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "depth", "not", "in", "self", ".", "arch_settings", ":", "\n", "            ", "raise", "KeyError", "(", "f'invalid depth {depth} for resnet'", ")", "\n", "", "self", ".", "depth", "=", "depth", "\n", "self", ".", "pretrained2d", "=", "pretrained2d", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "base_channels", "=", "base_channels", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "stage_blocks", "=", "stage_blocks", "\n", "self", ".", "out_indices", "=", "out_indices", "\n", "assert", "max", "(", "out_indices", ")", "<", "num_stages", "\n", "self", ".", "spatial_strides", "=", "spatial_strides", "\n", "self", ".", "temporal_strides", "=", "temporal_strides", "\n", "self", ".", "dilations", "=", "dilations", "\n", "assert", "len", "(", "spatial_strides", ")", "==", "len", "(", "temporal_strides", ")", "==", "len", "(", "\n", "dilations", ")", "==", "num_stages", "\n", "if", "self", ".", "stage_blocks", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "self", ".", "stage_blocks", ")", "==", "num_stages", "\n", "\n", "", "self", ".", "conv1_kernel", "=", "conv1_kernel", "\n", "self", ".", "conv1_stride_s", "=", "conv1_stride_s", "\n", "self", ".", "conv1_stride_t", "=", "conv1_stride_t", "\n", "self", ".", "pool1_stride_s", "=", "pool1_stride_s", "\n", "self", ".", "pool1_stride_t", "=", "pool1_stride_t", "\n", "self", ".", "with_pool1", "=", "with_pool1", "\n", "self", ".", "with_pool2", "=", "with_pool2", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "self", ".", "stage_inflations", "=", "_ntuple", "(", "num_stages", ")", "(", "inflate", ")", "\n", "self", ".", "non_local_stages", "=", "_ntuple", "(", "num_stages", ")", "(", "non_local", ")", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "self", ".", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "\n", "if", "self", ".", "stage_blocks", "is", "None", ":", "\n", "            ", "self", ".", "stage_blocks", "=", "stage_blocks", "[", ":", "num_stages", "]", "\n", "\n", "", "self", ".", "inplanes", "=", "self", ".", "base_channels", "\n", "\n", "self", ".", "non_local_cfg", "=", "non_local_cfg", "\n", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "net_block_idx", "=", "0", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "spatial_stride", "=", "spatial_strides", "[", "i", "]", "\n", "temporal_stride", "=", "temporal_strides", "[", "i", "]", "\n", "dilation", "=", "dilations", "[", "i", "]", "\n", "planes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "res_layer", ",", "net_block_idx", "=", "self", ".", "make_res_layer", "(", "\n", "self", ".", "block", "[", "i", "]", ",", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "self", ".", "style", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "non_local", "=", "self", ".", "non_local_stages", "[", "i", "]", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.make_res_layer": [[538, 657], ["dict", "layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule", "block", "layers.append", "isinstance", "isinstance", "len", "len", "block"], "methods", ["None"], ["non_local_cfg", "=", "self", ".", "non_local_cfg", ",", "\n", "inflate", "=", "self", ".", "stage_inflations", "[", "i", "]", ",", "\n", "inflate_style", "=", "self", ".", "inflate_style", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path_rate", "=", "drop_path_rate", ",", "\n", "net_num_blocks", "=", "sum", "(", "self", ".", "stage_blocks", ")", ",", "\n", "net_block_idx", "=", "net_block_idx", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", "[", "i", "]", ".", "expansion", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "block", "[", "i", "]", ".", "expansion", "*", "self", ".", "base_channels", "*", "2", "**", "(", "\n", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "", "@", "staticmethod", "\n", "def", "make_res_layer", "(", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "0", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "net_num_blocks", "=", "16", ",", "\n", "net_block_idx", "=", "0", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "pretrained", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Build residual layer for ResNet3D.\n        Args:\n            block (nn.Module): Residual module to be built.\n            inplanes (int): Number of channels for the input feature\n                in each block.\n            planes (int): Number of channels for the output feature\n                in each block.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int | Sequence[int]): Spatial strides in\n                residual and conv layers. Default: 1.\n            temporal_stride (int | Sequence[int]): Temporal strides in\n                residual and conv layers. Default: 1.\n            dilation (int): Spacing between kernel elements. Default: 1.\n            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,\n                the stride-two layer is the 3x3 conv layer, otherwise\n                the stride-two layer is the first 1x1 conv layer.\n                Default: ``pytorch``.\n            inflate (int | Sequence[int]): Determine whether to inflate\n                for each block. Default: 1.\n            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines\n                the kernel sizes and padding strides for conv1 and conv2\n                in each block. Default: '3x1x1'.\n            non_local (int | Sequence[int]): Determine whether to apply\n                non-local module in the corresponding block of each stages.\n                Default: 0.\n            non_local_cfg (dict): Config for non-local module.\n                Default: ``dict()``.\n            conv_cfg (dict | None): Config for norm layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool | None): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n        Returns:\n            nn.Module: A residual layer for the given config.\n\n        \"\"\"", "\n", "inflate", "=", "inflate", "if", "not", "isinstance", "(", "inflate", ",", "\n", "int", ")", "else", "(", "inflate", ",", ")", "*", "blocks", "\n", "non_local", "=", "non_local", "if", "not", "isinstance", "(", "\n", "non_local", ",", "int", ")", "else", "(", "non_local", ",", ")", "*", "blocks", "\n", "assert", "len", "(", "inflate", ")", "==", "blocks", "and", "len", "(", "non_local", ")", "==", "blocks", "\n", "downsample", "=", "None", "\n", "if", "spatial_stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "temporal_stride", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n", "temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "downsample", "=", "downsample", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "0", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "0", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", "**", "kwargs", ")", ")", "\n", "net_block_idx", "+=", "1", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_conv_params": [[658, 685], ["conv3d.weight.data.copy_", "inflated_param_names.append", "conv2d_weight.data.unsqueeze().expand_as", "getattr", "conv3d.bias.data.copy_", "inflated_param_names.append", "conv2d_weight.data.unsqueeze"], "methods", ["None"], ["for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "i", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "i", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", "**", "kwargs", ")", ")", "\n", "net_block_idx", "+=", "1", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", ",", "net_block_idx", "\n", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_bn_params": [[686, 718], ["bn3d.named_parameters", "bn3d.named_buffers", "param.data.copy_", "inflated_param_names.append", "warnings.warn", "param.data.copy_", "inflated_param_names.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_inflate_conv_params", "(", "conv3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        ", "\"\"\"Inflate a conv module from 2d to 3d.\n        Args:\n            conv3d (nn.Module): The destination conv3d module.\n            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n            module_name_2d (str): The name of corresponding conv module in the\n                2d model.\n            inflated_param_names (list[str]): List of parameters that have been\n                inflated.\n        \"\"\"", "\n", "weight_2d_name", "=", "module_name_2d", "+", "'.weight'", "\n", "\n", "conv2d_weight", "=", "state_dict_2d", "[", "weight_2d_name", "]", "\n", "kernel_t", "=", "conv3d", ".", "weight", ".", "data", ".", "shape", "[", "2", "]", "\n", "\n", "new_weight", "=", "conv2d_weight", ".", "data", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "conv3d", ".", "weight", ")", "/", "kernel_t", "\n", "conv3d", ".", "weight", ".", "data", ".", "copy_", "(", "new_weight", ")", "\n", "inflated_param_names", ".", "append", "(", "weight_2d_name", ")", "\n", "\n", "if", "getattr", "(", "conv3d", ",", "'bias'", ")", "is", "not", "None", ":", "\n", "            ", "bias_2d_name", "=", "module_name_2d", "+", "'.bias'", "\n", "conv3d", ".", "bias", ".", "data", ".", "copy_", "(", "state_dict_2d", "[", "bias_2d_name", "]", ")", "\n", "inflated_param_names", ".", "append", "(", "bias_2d_name", ")", "\n", "\n", "", "", "@", "staticmethod", "\n", "def", "_inflate_bn_params", "(", "bn3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_weights": [[719, 782], ["mmcv.runner._load_checkpoint", "resnet3d.ResNet3d.named_modules", "isinstance", "set", "set", "logger.info", "mmcv.runner._load_checkpoint.keys", "name.replace", "logger.warning", "logger.warning", "resnet3d.ResNet3d._inflate_bn_params", "logger.warning", "resnet3d.ResNet3d._inflate_conv_params"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_bn_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params"], ["\n", "for", "param_name", ",", "param", "in", "bn3d", ".", "named_parameters", "(", ")", ":", "\n", "            ", "param_2d_name", "=", "f'{module_name_2d}.{param_name}'", "\n", "param_2d", "=", "state_dict_2d", "[", "param_2d_name", "]", "\n", "if", "param", ".", "data", ".", "shape", "!=", "param_2d", ".", "shape", ":", "\n", "                ", "warnings", ".", "warn", "(", "f'The parameter of {module_name_2d} is not'", "\n", "'loaded due to incompatible shapes. '", ")", "\n", "return", "\n", "\n", "", "param", ".", "data", ".", "copy_", "(", "param_2d", ")", "\n", "inflated_param_names", ".", "append", "(", "param_2d_name", ")", "\n", "\n", "", "for", "param_name", ",", "param", "in", "bn3d", ".", "named_buffers", "(", ")", ":", "\n", "            ", "param_2d_name", "=", "f'{module_name_2d}.{param_name}'", "\n", "# some buffers like num_batches_tracked may not exist in old", "\n", "# checkpoints", "\n", "if", "param_2d_name", "in", "state_dict_2d", ":", "\n", "                ", "param_2d", "=", "state_dict_2d", "[", "param_2d_name", "]", "\n", "param", ".", "data", ".", "copy_", "(", "param_2d", ")", "\n", "inflated_param_names", ".", "append", "(", "param_2d_name", ")", "\n", "\n", "", "", "", "@", "staticmethod", "\n", "def", "_inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "\"\"\"Inflate the resnet2d parameters to resnet3d.\n        The differences between resnet3d and resnet2d mainly lie in an extra\n        axis of conv kernel. To utilize the pretrained parameters in 2d model,\n        the weight of conv2d models should be inflated to fit in the shapes of\n        the 3d counterpart.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging information.\n        \"\"\"", "\n", "\n", "state_dict_r2d", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict_r2d", ":", "\n", "            ", "state_dict_r2d", "=", "state_dict_r2d", "[", "'state_dict'", "]", "\n", "\n", "", "inflated_param_names", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "ConvModule", ")", ":", "\n", "# we use a ConvModule to wrap conv+bn+relu layers, thus the", "\n", "# name mapping is needed", "\n", "                ", "if", "'downsample'", "in", "name", ":", "\n", "# layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0", "\n", "                    ", "original_conv_name", "=", "name", "+", "'.0'", "\n", "# layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1", "\n", "original_bn_name", "=", "name", "+", "'.1'", "\n", "", "else", ":", "\n", "# layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}", "\n", "                    ", "original_conv_name", "=", "name", "\n", "# layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}", "\n", "original_bn_name", "=", "name", ".", "replace", "(", "'conv'", ",", "'bn'", ")", "\n", "", "if", "original_conv_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_conv_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "shape_2d", "=", "state_dict_r2d", "[", "original_conv_name", "+", "\n", "'.weight'", "]", ".", "shape", "\n", "shape_3d", "=", "module", ".", "conv", ".", "weight", ".", "data", ".", "shape", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.inflate_weights": [[784, 786], ["resnet3d.ResNet3d._inflate_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_weights"], ["                        ", "logger", ".", "warning", "(", "f'Weight shape mismatch for '", "\n", "f': {original_conv_name} : '", "\n", "f'3d weight shape: {shape_3d}; '", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._make_stem_layer": [[787, 809], ["mmcv.cnn.ConvModule", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "torch.MaxPool3d", "tuple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple"], "methods", ["None"], ["f'2d weight shape: {shape_2d}. '", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_inflate_conv_params", "(", "module", ".", "conv", ",", "state_dict_r2d", ",", "\n", "original_conv_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "", "", "if", "original_bn_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_bn_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_bn_params", "(", "module", ".", "bn", ",", "state_dict_r2d", ",", "\n", "original_bn_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "# check if any parameters in the 2d checkpoint are not loaded", "\n", "", "", "", "remaining_names", "=", "set", "(", "\n", "state_dict_r2d", ".", "keys", "(", ")", ")", "-", "set", "(", "inflated_param_names", ")", "\n", "if", "remaining_names", ":", "\n", "            ", "logger", ".", "info", "(", "f'These parameters in the 2d checkpoint are not loaded'", "\n", "f': {remaining_names}'", ")", "\n", "\n", "", "", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "self", ".", "_inflate_weights", "(", "self", ",", "logger", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._freeze_stages": [[810, 823], ["range", "resnet3d.ResNet3d.conv1.eval", "resnet3d.ResNet3d.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["\n", "", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "self", ".", "conv1_kernel", ",", "\n", "stride", "=", "(", "self", ".", "conv1_stride_t", ",", "self", ".", "conv1_stride_s", ",", "\n", "self", ".", "conv1_stride_s", ")", ",", "\n", "padding", "=", "tuple", "(", "[", "(", "k", "-", "1", ")", "//", "2", "for", "k", "in", "_triple", "(", "self", ".", "conv1_kernel", ")", "]", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights": [[824, 864], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "resnet3d.ResNet3d.inflate_weights", "mmcv.runner.load_checkpoint", "resnet3d.ResNet3d.modules", "TypeError", "isinstance", "resnet3d.ResNet3d.modules", "mmcv.cnn.kaiming_init", "isinstance", "isinstance", "mmcv.cnn.constant_init", "mmcv.cnn.constant_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.inflate_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool3d", "(", "\n", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "\n", "stride", "=", "(", "self", ".", "pool1_stride_t", ",", "self", ".", "pool1_stride_s", ",", "\n", "self", ".", "pool1_stride_s", ")", ",", "\n", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", "\n", "\n", "self", ".", "pool2", "=", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "2", ",", "1", ",", "1", ")", ",", "stride", "=", "(", "2", ",", "1", ",", "1", ")", ")", "\n", "\n", "", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\n        Args:\n            pretrained (str | None): The path of the pretrained weight. Will\n                override the original `pretrained` if set. The arg is added to\n                be compatible with mmdet. Default: None.\n        \"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "if", "self", ".", "pretrained2d", ":", "\n", "                ", "logger", ".", "info", "(", "f'load 2D model using inflate weights'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.init_weights": [[865, 867], ["resnet3d.ResNet3d._init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights"], ["# Inflate 2D model into 3D model.", "\n", "self", ".", "inflate_weights", "(", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.forward": [[868, 893], ["resnet3d.ResNet3d.conv1", "enumerate", "tuple", "resnet3d.ResNet3d.maxpool", "getattr", "getattr.", "len", "resnet3d.ResNet3d.pool2", "outs.append"], "methods", ["None"], ["", "else", ":", "\n", "                ", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n", "# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "\n", "strict", "=", "False", ",", "\n", "logger", "=", "logger", ",", "\n", "map_location", "=", "'cpu'", ",", "\n", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", ",", "[", "'get_feature.'", ",", "''", "]", "]", ")", "\n", "\n", "", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv3d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n", "", "", "if", "self", ".", "zero_init_residual", ":", "\n", "                ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "m", ",", "Bottleneck3d", ")", ":", "\n", "                        ", "constant_init", "(", "m", ".", "conv3", ".", "bn", ",", "0", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "BasicBlock3d", ")", ":", "\n", "                        ", "constant_init", "(", "m", ".", "conv2", ".", "bn", ",", "0", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d.train": [[894, 902], ["super().train", "resnet3d.ResNet3d._freeze_stages", "resnet3d.ResNet3d.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "self", ".", "_init_weights", "(", "self", ",", "pretrained", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer.__init__": [[943, 1022], ["dict", "dict", "dict", "torch.Module.__init__", "resnet3d.ResNet3dLayer.make_res_layer", "resnet3d.ResNet3dLayer.add_module"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["\n", "\n", "def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "pretrained2d", "=", "True", ",", "\n", "stage", "=", "3", ",", "\n", "base_channels", "=", "64", ",", "\n", "spatial_stride", "=", "2", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "all_frozen", "=", "False", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv3d'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "zero_init_residual", "=", "True", ",", "\n", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "arch_settings", "=", "ResNet3d", ".", "arch_settings", "\n", "assert", "depth", "in", "self", ".", "arch_settings", "\n", "\n", "self", ".", "make_res_layer", "=", "ResNet3d", ".", "make_res_layer", "\n", "self", ".", "_inflate_conv_params", "=", "ResNet3d", ".", "_inflate_conv_params", "\n", "self", ".", "_inflate_bn_params", "=", "ResNet3d", ".", "_inflate_bn_params", "\n", "self", ".", "_inflate_weights", "=", "ResNet3d", ".", "_inflate_weights", "\n", "self", ".", "_init_weights", "=", "ResNet3d", ".", "_init_weights", "\n", "\n", "self", ".", "depth", "=", "depth", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "pretrained2d", "=", "pretrained2d", "\n", "self", ".", "stage", "=", "stage", "\n", "# stage index is 0 based", "\n", "assert", "0", "<=", "stage", "<=", "3", "\n", "self", ".", "base_channels", "=", "base_channels", "\n", "\n", "self", ".", "spatial_stride", "=", "spatial_stride", "\n", "self", ".", "temporal_stride", "=", "temporal_stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "all_frozen", "=", "all_frozen", "\n", "\n", "self", ".", "stage_inflation", "=", "inflate", "\n", "self", ".", "inflate_style", "=", "inflate_style", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer.inflate_weights": [[1023, 1025], ["resnet3d.ResNet3dLayer._inflate_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_weights"], ["self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer._freeze_stages": [[1026, 1034], ["getattr", "getattr.eval", "getattr.parameters"], "methods", ["None"], ["\n", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "stage_block", "=", "stage_blocks", "[", "stage", "]", "\n", "planes", "=", "64", "*", "2", "**", "stage", "\n", "inplanes", "=", "64", "*", "2", "**", "(", "stage", "-", "1", ")", "*", "block", ".", "expansion", "\n", "\n", "res_layer", "=", "self", ".", "make_res_layer", "(", "\n", "block", ",", "\n", "inplanes", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer.init_weights": [[1035, 1037], ["resnet3d.ResNet3dLayer._init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._init_weights"], ["planes", ",", "\n", "stage_block", ",", "\n", "spatial_stride", "=", "spatial_stride", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer.forward": [[1038, 1051], ["getattr", "getattr."], "methods", ["None"], ["temporal_stride", "=", "temporal_stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "self", ".", "style", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "inflate", "=", "self", ".", "stage_inflation", ",", "\n", "inflate_style", "=", "self", ".", "inflate_style", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "**", "kwargs", ")", "\n", "\n", "self", ".", "layer_name", "=", "f'layer{stage + 1}'", "\n", "self", ".", "add_module", "(", "self", ".", "layer_name", ",", "res_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3dLayer.train": [[1052, 1060], ["super().train", "resnet3d.ResNet3dLayer._freeze_stages", "resnet3d.ResNet3dLayer.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "self", ".", "_inflate_weights", "(", "self", ",", "logger", ")", "\n", "\n", "", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "all_frozen", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "self", ".", "layer_name", ")", "\n", "layer", ".", "eval", "(", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.__init__": [[40, 89], ["resnet3d.ResNet3d.__init__", "range", "mmcv.cnn.ConvModule", "len", "setattr", "resnet3d_slowfast.ResNet3dPathway.lateral_connections.append", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["self", ".", "speed_ratio", "=", "speed_ratio", "\n", "self", ".", "channel_ratio", "=", "channel_ratio", "\n", "self", ".", "fusion_kernel", "=", "fusion_kernel", "\n", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "inplanes", "=", "self", ".", "base_channels", "\n", "if", "self", ".", "lateral", ":", "\n", "            ", "self", ".", "conv1_lateral", "=", "ConvModule", "(", "\n", "self", ".", "inplanes", "//", "self", ".", "channel_ratio", ",", "\n", "# https://arxiv.org/abs/1812.03982, the", "\n", "# third type of lateral connection has out_channel:", "\n", "# 2 * \\beta * C", "\n", "self", ".", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", ",", "\n", "kernel_size", "=", "(", "fusion_kernel", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "self", ".", "speed_ratio", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "(", "fusion_kernel", "-", "1", ")", "//", "2", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "self", ".", "lateral_connections", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "stage_blocks", ")", ")", ":", "\n", "            ", "planes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", "[", "i", "]", ".", "expansion", "\n", "\n", "if", "lateral", "and", "i", "!=", "self", ".", "num_stages", "-", "1", ":", "\n", "# no lateral connection needed in final stage", "\n", "                ", "lateral_name", "=", "f'layer{(i + 1)}_lateral'", "\n", "setattr", "(", "\n", "self", ",", "lateral_name", ",", "\n", "ConvModule", "(", "\n", "self", ".", "inplanes", "//", "self", ".", "channel_ratio", ",", "\n", "self", ".", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", ",", "\n", "kernel_size", "=", "(", "fusion_kernel", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "(", "fusion_kernel", "-", "1", ")", "//", "2", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", ")", "\n", "self", ".", "lateral_connections", ".", "append", "(", "lateral_name", ")", "\n", "\n", "", "", "", "def", "make_res_layer", "(", "self", ",", "\n", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "spatial_stride", "=", "1", ",", "\n", "temporal_stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.make_res_layer": [[90, 211], ["dict", "layers.append", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "mmcv.cnn.ConvModule", "block", "layers.append", "isinstance", "isinstance", "len", "len", "block"], "methods", ["None"], ["style", "=", "'pytorch'", ",", "\n", "inflate", "=", "1", ",", "\n", "inflate_style", "=", "'3x1x1'", ",", "\n", "non_local", "=", "0", ",", "\n", "non_local_cfg", "=", "dict", "(", ")", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "drop_path_rate", "=", "0.", ",", "\n", "net_num_blocks", "=", "16", ",", "\n", "net_block_idx", "=", "0", ",", "\n", "pretrained", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"Build residual layer for Slowfast.\n        Args:\n            block (nn.Module): Residual module to be built.\n            inplanes (int): Number of channels for the input\n                feature in each block.\n            planes (int): Number of channels for the output\n                feature in each block.\n            blocks (int): Number of residual blocks.\n            spatial_stride (int | Sequence[int]): Spatial strides\n                in residual and conv layers. Default: 1.\n            temporal_stride (int | Sequence[int]): Temporal strides in\n                residual and conv layers. Default: 1.\n            dilation (int): Spacing between kernel elements. Default: 1.\n            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,\n                the stride-two layer is the 3x3 conv layer,\n                otherwise the stride-two layer is the first 1x1 conv layer.\n                Default: ``pytorch``.\n            inflate (int | Sequence[int]): Determine whether to inflate\n                for each block. Default: 1.\n            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines\n                the kernel sizes and padding strides for conv1 and\n                conv2 in each block. Default: ``3x1x1``.\n            non_local (int | Sequence[int]): Determine whether to apply\n                non-local module in the corresponding block of each stages.\n                Default: 0.\n            non_local_cfg (dict): Config for non-local module.\n                Default: ``dict()``.\n            conv_cfg (dict | None): Config for conv layers. Default: None.\n            norm_cfg (dict | None): Config for norm layers. Default: None.\n            act_cfg (dict | None): Config for activate layers. Default: None.\n            with_cp (bool): Use checkpoint or not. Using checkpoint will save\n                some memory while slowing down the training speed.\n                Default: False.\n        Returns:\n            nn.Module: A residual layer for the given config.\n        \"\"\"", "\n", "inflate", "=", "inflate", "if", "not", "isinstance", "(", "inflate", ",", "\n", "int", ")", "else", "(", "inflate", ",", ")", "*", "blocks", "\n", "non_local", "=", "non_local", "if", "not", "isinstance", "(", "\n", "non_local", ",", "int", ")", "else", "(", "non_local", ",", ")", "*", "blocks", "\n", "assert", "len", "(", "inflate", ")", "==", "blocks", "and", "len", "(", "non_local", ")", "==", "blocks", "\n", "if", "self", ".", "lateral", ":", "\n", "            ", "lateral_inplanes", "=", "inplanes", "*", "2", "//", "self", ".", "channel_ratio", "\n", "", "else", ":", "\n", "            ", "lateral_inplanes", "=", "0", "\n", "", "if", "(", "spatial_stride", "!=", "1", "\n", "or", "(", "inplanes", "+", "lateral_inplanes", ")", "!=", "planes", "*", "block", ".", "expansion", ")", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", "+", "lateral_inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "temporal_stride", ",", "spatial_stride", ",", "spatial_stride", ")", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "", "else", ":", "\n", "            ", "downsample", "=", "None", "\n", "\n", "", "layers", "=", "[", "]", "\n", "block_dpr", "=", "drop_path_rate", "*", "net_block_idx", "/", "(", "net_num_blocks", "-", "1", ")", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", "+", "lateral_inplanes", ",", "\n", "planes", ",", "\n", "spatial_stride", ",", "\n", "temporal_stride", ",", "\n", "dilation", ",", "\n", "downsample", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "0", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "0", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", ")", ")", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "net_block_idx", "+=", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "1", ",", "\n", "dilation", ",", "\n", "style", "=", "style", ",", "\n", "inflate", "=", "(", "inflate", "[", "i", "]", "==", "1", ")", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "non_local", "=", "(", "non_local", "[", "i", "]", "==", "1", ")", ",", "\n", "non_local_cfg", "=", "non_local_cfg", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "drop_path", "=", "DropPath", "(", "block_dpr", ")", "if", "block_dpr", ">", "0.", "else", "None", ",", "\n", ")", ")", "\n", "net_block_idx", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.inflate_weights": [[212, 267], ["mmcv.runner._load_checkpoint", "resnet3d_slowfast.ResNet3dPathway.named_modules", "isinstance", "set", "set", "logger.info", "mmcv.runner._load_checkpoint.keys", "name.replace", "logger.warning", "resnet3d_slowfast.ResNet3dPathway._inflate_conv_params", "logger.warning", "resnet3d_slowfast.ResNet3dPathway._inflate_bn_params"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d.ResNet3d._inflate_bn_params"], ["\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", ",", "net_block_idx", "\n", "\n", "", "def", "inflate_weights", "(", "self", ",", "logger", ")", ":", "\n", "        ", "\"\"\"Inflate the resnet2d parameters to resnet3d pathway.\n        The differences between resnet3d and resnet2d mainly lie in an extra\n        axis of conv kernel. To utilize the pretrained parameters in 2d model,\n        the weight of conv2d models should be inflated to fit in the shapes of\n        the 3d counterpart. For pathway the ``lateral_connection`` part should\n        not be inflated from 2d weights.\n        Args:\n            logger (logging.Logger): The logger used to print\n                debugging information.\n        \"\"\"", "\n", "\n", "state_dict_r2d", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict_r2d", ":", "\n", "            ", "state_dict_r2d", "=", "state_dict_r2d", "[", "'state_dict'", "]", "\n", "\n", "", "inflated_param_names", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "'lateral'", "in", "name", ":", "\n", "                ", "continue", "\n", "", "if", "isinstance", "(", "module", ",", "ConvModule", ")", ":", "\n", "# we use a ConvModule to wrap conv+bn+relu layers, thus the", "\n", "# name mapping is needed", "\n", "                ", "if", "'downsample'", "in", "name", ":", "\n", "# layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0", "\n", "                    ", "original_conv_name", "=", "name", "+", "'.0'", "\n", "# layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1", "\n", "original_bn_name", "=", "name", "+", "'.1'", "\n", "", "else", ":", "\n", "# layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}", "\n", "                    ", "original_conv_name", "=", "name", "\n", "# layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}", "\n", "original_bn_name", "=", "name", ".", "replace", "(", "'conv'", ",", "'bn'", ")", "\n", "", "if", "original_conv_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_conv_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_conv_params", "(", "module", ".", "conv", ",", "state_dict_r2d", ",", "\n", "original_conv_name", ",", "\n", "inflated_param_names", ")", "\n", "", "if", "original_bn_name", "+", "'.weight'", "not", "in", "state_dict_r2d", ":", "\n", "                    ", "logger", ".", "warning", "(", "f'Module not exist in the state_dict_r2d'", "\n", "f': {original_bn_name}'", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_inflate_bn_params", "(", "module", ".", "bn", ",", "state_dict_r2d", ",", "\n", "original_bn_name", ",", "\n", "inflated_param_names", ")", "\n", "\n", "# check if any parameters in the 2d checkpoint are not loaded", "\n", "", "", "", "remaining_names", "=", "set", "(", "\n", "state_dict_r2d", ".", "keys", "(", ")", ")", "-", "set", "(", "inflated_param_names", ")", "\n", "if", "remaining_names", ":", "\n", "            ", "logger", ".", "info", "(", "f'These parameters in the 2d checkpoint are not loaded'", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._inflate_conv_params": [[269, 317], ["conv3d.weight.data.copy_", "inflated_param_names.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.data.unsqueeze().expand_as", "torch.cat.data.unsqueeze().expand_as", "torch.cat.data.unsqueeze().expand_as", "getattr", "conv3d.bias.data.copy_", "inflated_param_names.append", "warnings.warn", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.zeros().type_as().to", "torch.cat.data.unsqueeze", "torch.cat.data.unsqueeze", "torch.cat.data.unsqueeze", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["\n", "", "", "def", "_inflate_conv_params", "(", "self", ",", "conv3d", ",", "state_dict_2d", ",", "module_name_2d", ",", "\n", "inflated_param_names", ")", ":", "\n", "        ", "\"\"\"Inflate a conv module from 2d to 3d.\n        The differences of conv modules betweene 2d and 3d in Pathway\n        mainly lie in the inplanes due to lateral connections. To fit the\n        shapes of the lateral connection counterpart, it will expand\n        parameters by concatting conv2d parameters and extra zero paddings.\n        Args:\n            conv3d (nn.Module): The destination conv3d module.\n            state_dict_2d (OrderedDict): The state dict of pretrained 2d model.\n            module_name_2d (str): The name of corresponding conv module in the\n                2d model.\n            inflated_param_names (list[str]): List of parameters that have been\n                inflated.\n        \"\"\"", "\n", "weight_2d_name", "=", "module_name_2d", "+", "'.weight'", "\n", "conv2d_weight", "=", "state_dict_2d", "[", "weight_2d_name", "]", "\n", "old_shape", "=", "conv2d_weight", ".", "shape", "\n", "new_shape", "=", "conv3d", ".", "weight", ".", "data", ".", "shape", "\n", "kernel_t", "=", "new_shape", "[", "2", "]", "\n", "\n", "if", "new_shape", "[", "1", "]", "!=", "old_shape", "[", "1", "]", ":", "\n", "            ", "if", "new_shape", "[", "1", "]", "<", "old_shape", "[", "1", "]", ":", "\n", "                ", "warnings", ".", "warn", "(", "f'The parameter of {module_name_2d} is not'", "\n", "'loaded due to incompatible shapes. '", ")", "\n", "return", "\n", "# Inplanes may be different due to lateral connections", "\n", "", "new_channels", "=", "new_shape", "[", "1", "]", "-", "old_shape", "[", "1", "]", "\n", "pad_shape", "=", "old_shape", "\n", "pad_shape", "=", "pad_shape", "[", ":", "1", "]", "+", "(", "new_channels", ",", ")", "+", "pad_shape", "[", "2", ":", "]", "\n", "# Expand parameters by concat extra channels", "\n", "conv2d_weight", "=", "torch", ".", "cat", "(", "\n", "(", "conv2d_weight", ",", "\n", "torch", ".", "zeros", "(", "pad_shape", ")", ".", "type_as", "(", "conv2d_weight", ")", ".", "to", "(", "\n", "conv2d_weight", ".", "device", ")", ")", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "", "new_weight", "=", "conv2d_weight", ".", "data", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "conv3d", ".", "weight", ")", "/", "kernel_t", "\n", "conv3d", ".", "weight", ".", "data", ".", "copy_", "(", "new_weight", ")", "\n", "inflated_param_names", ".", "append", "(", "weight_2d_name", ")", "\n", "\n", "if", "getattr", "(", "conv3d", ",", "'bias'", ")", "is", "not", "None", ":", "\n", "            ", "bias_2d_name", "=", "module_name_2d", "+", "'.bias'", "\n", "conv3d", ".", "bias", ".", "data", ".", "copy_", "(", "state_dict_2d", "[", "bias_2d_name", "]", ")", "\n", "inflated_param_names", ".", "append", "(", "bias_2d_name", ")", "\n", "\n", "", "", "def", "_freeze_stages", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway._freeze_stages": [[318, 339], ["range", "resnet3d_slowfast.ResNet3dPathway.conv1.eval", "resnet3d_slowfast.ResNet3dPathway.conv1.parameters", "getattr", "getattr.eval", "getattr.parameters", "getattr", "getattr.eval", "getattr.parameters", "len"], "methods", ["None"], ["        ", "\"\"\"Prevent all the parameters from being optimized before\n        `self.frozen_stages`.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "eval", "(", ")", "\n", "for", "param", "in", "self", ".", "conv1", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "if", "i", "!=", "len", "(", "self", ".", "res_layers", ")", "and", "self", ".", "lateral", ":", "\n", "# No fusion needed in the final stage", "\n", "                ", "lateral_name", "=", "self", ".", "lateral_connections", "[", "i", "-", "1", "]", "\n", "conv_lateral", "=", "getattr", "(", "self", ",", "lateral_name", ")", "\n", "conv_lateral", ".", "eval", "(", ")", "\n", "for", "param", "in", "conv_lateral", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dPathway.init_weights": [[340, 353], ["super().init_weights", "getattr", "getattr.modules", "isinstance", "mmcv.cnn.kaiming_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "\n", "# Override the init_weights of i3d", "\n", "", "super", "(", ")", ".", "init_weights", "(", ")", "\n", "for", "module_name", "in", "self", ".", "lateral_connections", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "module_name", ")", "\n", "for", "m", "in", "layer", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "(", "nn", ".", "Conv3d", ",", "nn", ".", "Conv2d", ")", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dSlowFast.__init__": [[430, 470], ["dict", "dict", "torch.Module.__init__", "resnet3d_slowfast.build_pathway", "resnet3d_slowfast.build_pathway"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.build_pathway", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.build_pathway"], ["reshape_st", "=", "False", ",", "\n", "conv1_kernel", "=", "(", "1", ",", "7", ",", "7", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "inflate", "=", "(", "0", ",", "0", ",", "1", ",", "1", ")", ",", "\n", "drop_path_rate", "=", "0.", ")", ",", "\n", "fast_pathway", "=", "dict", "(", "\n", "type", "=", "'resnet3d'", ",", "\n", "depth", "=", "50", ",", "\n", "pretrained", "=", "None", ",", "\n", "lateral", "=", "False", ",", "\n", "reshape_t", "=", "True", ",", "\n", "reshape_st", "=", "False", ",", "\n", "base_channels", "=", "8", ",", "\n", "conv1_kernel", "=", "(", "5", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "drop_path_rate", "=", "0.", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "resample_rate", "=", "resample_rate", "\n", "self", ".", "speed_ratio", "=", "speed_ratio", "\n", "self", ".", "channel_ratio", "=", "channel_ratio", "\n", "\n", "if", "slow_pathway", "[", "'lateral'", "]", ":", "\n", "            ", "slow_pathway", "[", "'speed_ratio'", "]", "=", "speed_ratio", "\n", "slow_pathway", "[", "'channel_ratio'", "]", "=", "channel_ratio", "\n", "\n", "", "self", ".", "slow_path", "=", "build_pathway", "(", "slow_pathway", ")", "\n", "self", ".", "fast_path", "=", "build_pathway", "(", "fast_pathway", ")", "\n", "\n", "", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "\n", "", "if", "self", ".", "pretrained", "is", "None", ":", "\n", "# Init two branch separately.", "\n", "            ", "self", ".", "fast_path", ".", "init_weights", "(", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dSlowFast.init_weights": [[471, 489], ["isinstance", "utils.get_root_logger", "mmcv.utils.print_log", "mmcv.runner.load_checkpoint", "resnet3d_slowfast.ResNet3dSlowFast.fast_path.init_weights", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.init_weights", "TypeError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["self", ".", "slow_path", ".", "init_weights", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n", "", "", "@", "staticmethod", "\n", "def", "_init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\n        Args:\n            pretrained (str | None): The path of the pretrained weight. Will\n                override the original `pretrained` if set. The arg is added to\n                be compatible with mmdet. Default: None.\n        \"\"\"", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "logging", ".", "getLogger", "(", "'train'", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "logger", ".", "info", "(", "f'load 3D model'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.ResNet3dSlowFast.forward": [[490, 536], ["torch.functional.interpolate", "torch.functional.interpolate", "torch.functional.interpolate", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.conv1", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.maxpool", "torch.functional.interpolate", "torch.functional.interpolate", "torch.functional.interpolate", "resnet3d_slowfast.ResNet3dSlowFast.fast_path.conv1", "resnet3d_slowfast.ResNet3dSlowFast.fast_path.maxpool", "enumerate", "resnet3d_slowfast.ResNet3dSlowFast.slow_path.conv1_lateral", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "getattr", "getattr.", "getattr", "getattr.", "getattr", "getattr.", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len"], "methods", ["None"], ["# Directly load 3D model.", "\n", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ",", "revise_keys", "=", "[", "(", "'backbone.'", ",", "''", ")", ",", "(", "'get_feature.'", ",", "''", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n        Returns:\n            tuple[torch.Tensor]: The feature of the input samples extracted\n                by the backbone.\n        \"\"\"", "\n", "# x_slow = nn.functional.interpolate(", "\n", "#     x,", "\n", "#     mode='nearest',", "\n", "#     scale_factor=(1.0 / self.resample_rate, 1.0, 1.0))", "\n", "x_slow", "=", "self", ".", "slow_path", ".", "conv1", "(", "x", ")", "\n", "x_slow", "=", "self", ".", "slow_path", ".", "maxpool", "(", "x_slow", ")", "\n", "\n", "x_fast", "=", "nn", ".", "functional", ".", "interpolate", "(", "\n", "x", ",", "\n", "mode", "=", "'nearest'", ",", "\n", "scale_factor", "=", "(", "1.0", "/", "(", "self", ".", "resample_rate", "//", "self", ".", "speed_ratio", ")", ",", "1.0", ",", "\n", "1.0", ")", ")", "\n", "x_fast", "=", "self", ".", "fast_path", ".", "conv1", "(", "x_fast", ")", "\n", "x_fast", "=", "self", ".", "fast_path", ".", "maxpool", "(", "x_fast", ")", "\n", "\n", "if", "self", ".", "slow_path", ".", "lateral", ":", "\n", "            ", "x_fast_lateral", "=", "self", ".", "slow_path", ".", "conv1_lateral", "(", "x_fast", ")", "\n", "x_slow", "=", "torch", ".", "cat", "(", "(", "x_slow", ",", "x_fast_lateral", ")", ",", "dim", "=", "1", ")", "\n", "", "ids", "=", "1", "\n", "idf", "=", "1", "\n", "for", "i", ",", "layer_name", "in", "enumerate", "(", "self", ".", "slow_path", ".", "res_layers", ")", ":", "\n", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ".", "slow_path", ",", "layer_name", ")", "\n", "for", "block", "in", "res_layer", ":", "\n", "                ", "x_slow", "=", "block", "(", "x_slow", ")", "\n", "ids", "+=", "1", "\n", "\n", "", "res_layer_fast", "=", "getattr", "(", "self", ".", "fast_path", ",", "layer_name", ")", "\n", "\n", "for", "block", "in", "res_layer_fast", ":", "\n", "                ", "x_fast", "=", "block", "(", "x_fast", ")", "\n", "idf", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowfast.build_pathway": [[361, 383], ["cfg.copy", "cfg.copy.pop", "pathway_cls", "TypeError", "KeyError", "isinstance"], "function", ["None"], ["    ", "\"\"\"Build pathway.\n    Args:\n        cfg (None or dict): cfg should contain:\n            - type (str): identify conv layer type.\n    Returns:\n        nn.Module: Created pathway.\n    \"\"\"", "\n", "if", "not", "(", "isinstance", "(", "cfg", ",", "dict", ")", "and", "'type'", "in", "cfg", ")", ":", "\n", "        ", "raise", "TypeError", "(", "'cfg must be a dict containing the key \"type\"'", ")", "\n", "", "cfg_", "=", "cfg", ".", "copy", "(", ")", "\n", "\n", "pathway_type", "=", "cfg_", ".", "pop", "(", "'type'", ")", "\n", "if", "pathway_type", "not", "in", "pathway_cfg", ":", "\n", "        ", "raise", "KeyError", "(", "f'Unrecognized pathway type {pathway_type}'", ")", "\n", "\n", "", "pathway_cls", "=", "pathway_cfg", "[", "pathway_type", "]", "\n", "pathway", "=", "pathway_cls", "(", "*", "args", ",", "**", "kwargs", ",", "**", "cfg_", ")", "\n", "\n", "return", "pathway", "\n", "\n", "\n", "", "class", "ResNet3dSlowFast", "(", "nn", ".", "Module", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_csn.CSNBottleneck3d.__init__": [[33, 73], ["resnet3d.Bottleneck3d.__init__", "bool", "mmcv.cnn.ConvModule", "conv2.append", "torch.Sequential", "conv2.append", "mmcv.cnn.ConvModule"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "*", "args", ",", "\n", "bottleneck_mode", "=", "'ir'", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "CSNBottleneck3d", ",", "self", ")", ".", "__init__", "(", "inplanes", ",", "planes", ",", "*", "args", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "bottleneck_mode", "=", "bottleneck_mode", "\n", "conv2", "=", "[", "]", "\n", "if", "self", ".", "bottleneck_mode", "==", "'ip'", ":", "\n", "            ", "conv2", ".", "append", "(", "\n", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "stride", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", ")", "\n", "", "conv2_kernel_size", "=", "self", ".", "conv2", ".", "conv", ".", "kernel_size", "\n", "conv2_stride", "=", "self", ".", "conv2", ".", "conv", ".", "stride", "\n", "conv2_padding", "=", "self", ".", "conv2", ".", "conv", ".", "padding", "\n", "conv2_dilation", "=", "self", ".", "conv2", ".", "conv", ".", "dilation", "\n", "conv2_bias", "=", "bool", "(", "self", ".", "conv2", ".", "conv", ".", "bias", ")", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "conv2_kernel_size", ",", "\n", "stride", "=", "conv2_stride", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "dilation", "=", "conv2_dilation", ",", "\n", "bias", "=", "conv2_bias", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ",", "\n", "groups", "=", "planes", ")", "\n", "conv2", ".", "append", "(", "self", ".", "conv2", ")", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "*", "conv2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_csn.ResNet3dCSN.__init__": [[110, 148], ["dict", "resnet3d.ResNet3d.__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "temporal_strides", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_kernel", "=", "(", "3", ",", "7", ",", "7", ")", ",", "\n", "conv1_stride_t", "=", "1", ",", "\n", "pool1_stride_t", "=", "1", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ",", "requires_grad", "=", "True", ",", "eps", "=", "1e-3", ")", ",", "\n", "inflate_style", "=", "'3x3x3'", ",", "\n", "bottleneck_mode", "=", "'ir'", ",", "\n", "reshape_t", "=", "False", ",", "\n", "reshape_st", "=", "False", ",", "\n", "bn_frozen", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "arch_settings", "=", "{", "\n", "# 18: (BasicBlock3d, (2, 2, 2, 2)),", "\n", "# 34: (BasicBlock3d, (3, 4, 6, 3)),", "\n", "50", ":", "(", "CSNBottleneck3d", ",", "(", "3", ",", "4", ",", "6", ",", "3", ")", ")", ",", "\n", "101", ":", "(", "CSNBottleneck3d", ",", "(", "3", ",", "4", ",", "23", ",", "3", ")", ")", ",", "\n", "152", ":", "(", "CSNBottleneck3d", ",", "(", "3", ",", "8", ",", "36", ",", "3", ")", ")", "\n", "}", "\n", "self", ".", "bn_frozen", "=", "bn_frozen", "\n", "if", "bottleneck_mode", "not", "in", "[", "'ip'", ",", "'ir'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "f'Bottleneck mode must be \"ip\" or \"ir\",'", "\n", "f'but got {bottleneck_mode}.'", ")", "\n", "", "super", "(", "ResNet3dCSN", ",", "self", ")", ".", "__init__", "(", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "temporal_strides", "=", "temporal_strides", ",", "\n", "conv1_kernel", "=", "conv1_kernel", ",", "\n", "conv1_stride_t", "=", "conv1_stride_t", ",", "\n", "pool1_stride_t", "=", "pool1_stride_t", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "inflate_style", "=", "inflate_style", ",", "\n", "bottleneck_mode", "=", "bottleneck_mode", ",", "\n", "reshape_t", "=", "reshape_t", ",", "\n", "reshape_st", "=", "reshape_st", ",", "\n", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_csn.ResNet3dCSN.train": [[149, 159], ["super().train", "resnet3d_csn.ResNet3dCSN._freeze_stages", "resnet3d_csn.ResNet3dCSN.modules", "isinstance", "m.eval", "m.parameters"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "super", "(", "ResNet3d", ",", "self", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "if", "self", ".", "bn_frozen", ":", "\n", "                        ", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                            ", "param", ".", "requires_grad", "=", "False", "", "", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.STS_3dconv": [[16, 66], ["isinstance", "isinstance", "torch.chunk", "torch.chunk", "torch.chunk", "torch.cat", "torch.cat", "torch.cat", "torch.chunk", "torch.chunk", "torch.chunk", "STS_Conv.apperance_st", "torch.conv3d", "STS_Conv.apperance_t", "torch.conv3d", "conv3d.norm", "conv3d.activate"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_st", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_t"], ["", "def", "STS_3dconv", "(", "conv3d", ",", "x", ")", ":", "\n", "    ", "\"\"\".\n\n    Args:\n        conv3d (ConvModule): Number of channels in the input feature map.\n            Same as that in ``nn._ConvNd``.\n        x (tensor): Number of channels in the input feature map.\n            Same as that in ``nn._ConvNd``.\n    \"\"\"", "\n", "\n", "assert", "isinstance", "(", "conv3d", ",", "ConvModule", ")", ",", "'conv3d should be a ConvModule'", "\n", "\n", "# extract the parameters of conv3d", "\n", "conv3d_para", "=", "conv3d", ".", "conv", ".", "weight", "\n", "outplanes", ",", "inplanes", ",", "k_t", ",", "k_s", ",", "_", "=", "conv3d_para", ".", "shape", "\n", "stride_t", ",", "stride_s", ",", "_", "=", "conv3d", ".", "stride", "\n", "assert", "stride_t", "==", "1", ",", "'We dont support temporal downsampling currently'", "\n", "padding_t", ",", "padding_s", "=", "(", "k_t", "-", "1", ")", "//", "2", ",", "(", "k_s", "-", "1", ")", "//", "2", "\n", "groups", "=", "conv3d", ".", "groups", "\n", "\n", "# separate 3d conv into dynamic and static channels", "\n", "app_para", ",", "motion_para", "=", "torch", ".", "chunk", "(", "conv3d_para", ",", "2", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "if", "k_s", ">", "1", ":", "# this is not a temporal 3d conv", "\n", "        ", "assert", "groups", ">", "1", ",", "'We only support depthwise 3D conv currently (3x3x3)'", "\n", "x_app", ",", "x_motion", "=", "torch", ".", "chunk", "(", "x", ",", "2", ",", "dim", "=", "1", ")", "\n", "\n", "# static appearance modeling by splitting 3D kernel", "\n", "out_app", "=", "apperance_st", "(", "x_app", ",", "app_para", ",", "stride", "=", "(", "stride_t", ",", "stride_s", ")", ",", "groups", "=", "groups", "//", "2", ")", "\n", "\n", "# dynamic motion modeling by normal 3D conv", "\n", "out_motion", "=", "F", ".", "conv3d", "(", "x_motion", ",", "weight", "=", "motion_para", ",", "stride", "=", "(", "stride_t", ",", "stride_s", ",", "stride_s", ")", ",", "groups", "=", "groups", "//", "2", ",", "padding", "=", "(", "padding_t", ",", "padding_s", ",", "padding_s", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "assert", "groups", "==", "1", ",", "'We only support fully-connected temporal convolution (3x1x1)'", "\n", "\n", "# static appearance modeling by splitting 3D kernel", "\n", "out_app", "=", "apperance_t", "(", "x", ",", "app_para", ",", "padding_t", "=", "padding_t", ",", "groups", "=", "groups", ")", "\n", "\n", "# dynamic motion modeling by normal 3D conv", "\n", "out_motion", "=", "F", ".", "conv3d", "(", "x", ",", "weight", "=", "motion_para", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "(", "padding_t", ",", "padding_s", ",", "padding_s", ")", ")", "\n", "\n", "# concat the feature map", "\n", "", "out", "=", "torch", ".", "cat", "(", "[", "out_app", ",", "out_motion", "]", ",", "dim", "=", "1", ")", "\n", "if", "conv3d", ".", "norm_cfg", "is", "not", "None", ":", "\n", "        ", "out", "=", "conv3d", ".", "norm", "(", "out", ")", "\n", "", "if", "conv3d", ".", "act_cfg", "is", "not", "None", ":", "\n", "        ", "out", "=", "conv3d", ".", "activate", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_st": [[69, 100], ["x.size", "torch.conv3d", "app_para[].view", "einops.rearrange", "torch.conv1d", "einops.rearrange", "app_para[].view", "einops.rearrange", "torch.conv1d", "einops.rearrange", "app_para[].view.size", "app_para[].view.size", "spatial_para.size", "spatial_para.size"], "function", ["None"], ["", "def", "apperance_st", "(", "x", ",", "app_para", ",", "stride", "=", "(", "1", ",", "1", ")", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\".\n\n        Args:\n            x (tensor): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n            app_para (weight): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n        \"\"\"", "\n", "\n", "# prepare parameters", "\n", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "outplanes", ",", "inplanes", ",", "_", ",", "_", ",", "_", "=", "app_para", ".", "shape", "\n", "\n", "# middle 2D kernel 1x3x3", "\n", "spatial_para", "=", "app_para", "[", ":", ",", ":", ",", "1", ":", "2", ",", ":", ",", ":", "]", "\n", "out_s", "=", "F", ".", "conv3d", "(", "x", ",", "weight", "=", "spatial_para", ",", "stride", "=", "(", "stride", "[", "0", "]", ",", "stride", "[", "1", "]", ",", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "0", ",", "(", "spatial_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ",", "(", "spatial_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "\n", "# t1 kernel reshapes into 1D (1x9), then convolve along the row direction", "\n", "# note that some backbones use temporal dowsampling, we don't support temporal downsampling by using 1D conv", "\n", "row_para", "=", "app_para", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", ".", "view", "(", "outplanes", ",", "inplanes", ",", "-", "1", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "x", ",", "'b c t h w  -> (b t) c (h w)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "F", ".", "conv1d", "(", "out_row", ",", "weight", "=", "row_para", ",", "stride", "=", "(", "stride", "[", "1", "]", "*", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "(", "row_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "out_row", ",", "'(b t) c (h w)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "\n", "# t2 kernel reshapes into 1D (9x1), then convolve along the column direction", "\n", "column_para", "=", "app_para", "[", ":", ",", ":", ",", "2", ",", ":", ",", ":", "]", ".", "view", "(", "outplanes", ",", "inplanes", ",", "-", "1", ")", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "x", ",", "'b c t h w  -> (b t) c (w h)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_column", "=", "F", ".", "conv1d", "(", "out_column", ",", "weight", "=", "column_para", ",", "stride", "=", "(", "stride", "[", "1", "]", "*", "stride", "[", "1", "]", ")", ",", "groups", "=", "groups", ",", "padding", "=", "(", "(", "column_para", ".", "size", "(", "-", "1", ")", "-", "1", ")", "//", "2", ")", ")", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "out_column", ",", "'(b t) c (w h)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", "//", "stride", "[", "1", "]", ",", "w", "=", "w", "//", "stride", "[", "1", "]", ")", "\n", "\n", "out", "=", "out_s", "+", "out_row", "+", "out_column", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.STS_Conv.apperance_t": [[105, 135], ["x.size", "app_para.view.view", "torch.chunk", "torch.chunk", "torch.chunk", "einops.rearrange", "einops.rearrange", "torch.conv1d", "torch.conv1d", "einops.rearrange", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "app_para.view.size", "app_para.view.size"], "function", ["None"], ["\n", "\n", "", "def", "apperance_t", "(", "x", ",", "app_para", ",", "padding_t", ",", "groups", "=", "1", ")", ":", "\n", "    ", "\"\"\".\n\n        Args:\n            conv3d (int): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n            conv3d (int): Number of channels in the input feature map.\n                Same as that in ``nn._ConvNd``.\n        \"\"\"", "\n", "\n", "# prepare parameters", "\n", "b", ",", "c", ",", "t", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "app_para", "=", "app_para", ".", "view", "(", "app_para", ".", "size", "(", "0", ")", ",", "app_para", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "app_kernel_chunks", "=", "torch", ".", "chunk", "(", "app_para", ",", "2", ",", "dim", "=", "0", ")", "\n", "\n", "\n", "# reshape to column or row", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> (b t) c ( h w)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "x", ",", "'b  c  t  h  w  -> (b t) c ( w h)'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "\n", "# convolve", "\n", "out_column", "=", "F", ".", "conv1d", "(", "out_column", ",", "weight", "=", "app_kernel_chunks", "[", "0", "]", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "padding_t", ")", "\n", "out_row", "=", "F", ".", "conv1d", "(", "out_row", ",", "weight", "=", "app_kernel_chunks", "[", "1", "]", ",", "stride", "=", "1", ",", "groups", "=", "groups", ",", "padding", "=", "padding_t", ")", "\n", "\n", "# reshape back to original shape", "\n", "out_column", "=", "einops", ".", "rearrange", "(", "out_column", ",", "'(b t) c ( h w)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "out_row", "=", "einops", ".", "rearrange", "(", "out_row", ",", "'(b t) c ( w h)  -> b c t h w'", ",", "t", "=", "t", ",", "h", "=", "h", ",", "w", "=", "w", ")", "\n", "\n", "# concat along the channel dimension", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.BasicBlock.__init__": [[35, 79], ["dict", "dict", "dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "dilation", ",", "\n", "dilation", "=", "dilation", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "assert", "not", "with_cp", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.BasicBlock.forward": [[80, 101], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.conv2", "resnet.BasicBlock.relu", "resnet.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.Bottleneck.__init__": [[129, 186], ["dict", "dict", "dict", "torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "style", "in", "[", "'pytorch'", ",", "'caffe'", "]", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "if", "style", "==", "'pytorch'", ":", "\n", "            ", "self", ".", "conv1_stride", "=", "1", "\n", "self", ".", "conv2_stride", "=", "stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv1_stride", "=", "stride", "\n", "self", ".", "conv2_stride", "=", "1", "\n", "", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "self", ".", "conv1_stride", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "3", ",", "\n", "stride", "=", "self", ".", "conv2_stride", ",", "\n", "padding", "=", "dilation", ",", "\n", "dilation", "=", "dilation", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ")", "\n", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.Bottleneck.forward": [[187, 220], ["resnet.Bottleneck.relu", "resnet.Bottleneck.conv1", "resnet.Bottleneck.conv2", "resnet.Bottleneck.conv3", "torch.utils.checkpoint.checkpoint", "resnet.Bottleneck.forward._inner_forward"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "\n", "def", "_inner_forward", "(", "x", ")", ":", "\n", "            ", "\"\"\"Forward wrapper for utilizing checkpoint.\"\"\"", "\n", "identity", "=", "x", "\n", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "=", "out", "+", "identity", "\n", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet.__init__": [[334, 404], ["dict", "dict", "dict", "torch.Module.__init__", "resnet.ResNet._make_stem_layer", "enumerate", "KeyError", "max", "len", "len", "resnet.make_res_layer", "resnet.ResNet.add_module", "resnet.ResNet.res_layers.append", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", "=", "None", ",", "\n", "torchvision_pretrain", "=", "True", ",", "\n", "in_channels", "=", "3", ",", "\n", "num_stages", "=", "4", ",", "\n", "out_indices", "=", "(", "3", ",", ")", ",", "\n", "strides", "=", "(", "1", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN2d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "partial_bn", "=", "False", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "depth", "not", "in", "self", ".", "arch_settings", ":", "\n", "            ", "raise", "KeyError", "(", "f'invalid depth {depth} for resnet'", ")", "\n", "", "self", ".", "depth", "=", "depth", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "torchvision_pretrain", "=", "torchvision_pretrain", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "out_indices", "=", "out_indices", "\n", "assert", "max", "(", "out_indices", ")", "<", "num_stages", "\n", "self", ".", "strides", "=", "strides", "\n", "self", ".", "dilations", "=", "dilations", "\n", "assert", "len", "(", "strides", ")", "==", "len", "(", "dilations", ")", "==", "num_stages", "\n", "self", ".", "style", "=", "style", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "partial_bn", "=", "partial_bn", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n", "self", ".", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "self", ".", "stage_blocks", "=", "stage_blocks", "[", ":", "num_stages", "]", "\n", "self", ".", "inplanes", "=", "64", "\n", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "stride", "=", "strides", "[", "i", "]", "\n", "dilation", "=", "dilations", "[", "i", "]", "\n", "planes", "=", "64", "*", "2", "**", "i", "\n", "res_layer", "=", "make_res_layer", "(", "\n", "self", ".", "block", ",", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "stride", "=", "stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "style", "=", "self", ".", "style", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", ".", "expansion", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "block", ".", "expansion", "*", "64", "*", "2", "**", "(", "\n", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._make_stem_layer": [[405, 419], ["mmcv.cnn.ConvModule", "torch.MaxPool2d"], "methods", ["None"], ["", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "64", ",", "\n", "kernel_size", "=", "7", ",", "\n", "stride", "=", "2", ",", "\n", "padding", "=", "3", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "self", ".", "conv_cfg", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "self", ".", "maxpool", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_conv_params": [[420, 445], ["conv.weight.data.copy_", "loaded_param_names.append", "getattr", "conv.bias.data.copy_", "loaded_param_names.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_load_conv_params", "(", "conv", ",", "state_dict_tv", ",", "module_name_tv", ",", "\n", "loaded_param_names", ")", ":", "\n", "        ", "\"\"\"Load the conv parameters of resnet from torchvision.\n\n        Args:\n            conv (nn.Module): The destination conv module.\n            state_dict_tv (OrderedDict): The state dict of pretrained\n                torchvision model.\n            module_name_tv (str): The name of corresponding conv module in the\n                torchvision model.\n            loaded_param_names (list[str]): List of parameters that have been\n                loaded.\n        \"\"\"", "\n", "\n", "weight_tv_name", "=", "module_name_tv", "+", "'.weight'", "\n", "if", "conv", ".", "weight", ".", "data", ".", "shape", "==", "state_dict_tv", "[", "weight_tv_name", "]", ".", "shape", ":", "\n", "            ", "conv", ".", "weight", ".", "data", ".", "copy_", "(", "state_dict_tv", "[", "weight_tv_name", "]", ")", "\n", "loaded_param_names", ".", "append", "(", "weight_tv_name", ")", "\n", "\n", "", "if", "getattr", "(", "conv", ",", "'bias'", ")", "is", "not", "None", ":", "\n", "            ", "bias_tv_name", "=", "module_name_tv", "+", "'.bias'", "\n", "if", "conv", ".", "bias", ".", "data", ".", "shape", "==", "state_dict_tv", "[", "bias_tv_name", "]", ".", "shape", ":", "\n", "                ", "conv", ".", "bias", ".", "data", ".", "copy_", "(", "state_dict_tv", "[", "bias_tv_name", "]", ")", "\n", "loaded_param_names", ".", "append", "(", "bias_tv_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_bn_params": [[446, 475], ["bn.named_parameters", "bn.named_buffers", "param.data.copy_", "loaded_param_names.append", "param.data.copy_", "loaded_param_names.append"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "_load_bn_params", "(", "bn", ",", "state_dict_tv", ",", "module_name_tv", ",", "loaded_param_names", ")", ":", "\n", "        ", "\"\"\"Load the bn parameters of resnet from torchvision.\n\n        Args:\n            bn (nn.Module): The destination bn module.\n            state_dict_tv (OrderedDict): The state dict of pretrained\n                torchvision model.\n            module_name_tv (str): The name of corresponding bn module in the\n                torchvision model.\n            loaded_param_names (list[str]): List of parameters that have been\n                loaded.\n        \"\"\"", "\n", "\n", "for", "param_name", ",", "param", "in", "bn", ".", "named_parameters", "(", ")", ":", "\n", "            ", "param_tv_name", "=", "f'{module_name_tv}.{param_name}'", "\n", "param_tv", "=", "state_dict_tv", "[", "param_tv_name", "]", "\n", "if", "param", ".", "data", ".", "shape", "==", "param_tv", ".", "shape", ":", "\n", "                ", "param", ".", "data", ".", "copy_", "(", "param_tv", ")", "\n", "loaded_param_names", ".", "append", "(", "param_tv_name", ")", "\n", "\n", "", "", "for", "param_name", ",", "param", "in", "bn", ".", "named_buffers", "(", ")", ":", "\n", "            ", "param_tv_name", "=", "f'{module_name_tv}.{param_name}'", "\n", "# some buffers like num_batches_tracked may not exist", "\n", "if", "param_tv_name", "in", "state_dict_tv", ":", "\n", "                ", "param_tv", "=", "state_dict_tv", "[", "param_tv_name", "]", "\n", "if", "param", ".", "data", ".", "shape", "==", "param_tv", ".", "shape", ":", "\n", "                    ", "param", ".", "data", ".", "copy_", "(", "param_tv", ")", "\n", "loaded_param_names", ".", "append", "(", "param_tv_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_torchvision_checkpoint": [[476, 508], ["mmcv.runner._load_checkpoint", "resnet.ResNet.named_modules", "isinstance", "set", "set", "logger.info", "resnet.ResNet._load_conv_params", "resnet.ResNet._load_bn_params", "mmcv.runner._load_checkpoint.keys", "name.replace"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.named_modules", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_conv_params", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_bn_params"], ["", "", "", "", "def", "_load_torchvision_checkpoint", "(", "self", ",", "logger", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from torchvision pretrained checkpoint.\"\"\"", "\n", "state_dict_torchvision", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict_torchvision", ":", "\n", "            ", "state_dict_torchvision", "=", "state_dict_torchvision", "[", "'state_dict'", "]", "\n", "\n", "", "loaded_param_names", "=", "[", "]", "\n", "for", "name", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "module", ",", "ConvModule", ")", ":", "\n", "# we use a ConvModule to wrap conv+bn+relu layers, thus the", "\n", "# name mapping is needed", "\n", "                ", "if", "'downsample'", "in", "name", ":", "\n", "# layer{X}.{Y}.downsample.conv->layer{X}.{Y}.downsample.0", "\n", "                    ", "original_conv_name", "=", "name", "+", "'.0'", "\n", "# layer{X}.{Y}.downsample.bn->layer{X}.{Y}.downsample.1", "\n", "original_bn_name", "=", "name", "+", "'.1'", "\n", "", "else", ":", "\n", "# layer{X}.{Y}.conv{n}.conv->layer{X}.{Y}.conv{n}", "\n", "                    ", "original_conv_name", "=", "name", "\n", "# layer{X}.{Y}.conv{n}.bn->layer{X}.{Y}.bn{n}", "\n", "original_bn_name", "=", "name", ".", "replace", "(", "'conv'", ",", "'bn'", ")", "\n", "", "self", ".", "_load_conv_params", "(", "module", ".", "conv", ",", "state_dict_torchvision", ",", "\n", "original_conv_name", ",", "loaded_param_names", ")", "\n", "self", ".", "_load_bn_params", "(", "module", ".", "bn", ",", "state_dict_torchvision", ",", "\n", "original_bn_name", ",", "loaded_param_names", ")", "\n", "\n", "# check if any parameters in the 2d checkpoint are not loaded", "\n", "", "", "remaining_names", "=", "set", "(", "\n", "state_dict_torchvision", ".", "keys", "(", ")", ")", "-", "set", "(", "loaded_param_names", ")", "\n", "if", "remaining_names", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f'These parameters in pretrained checkpoint are not loaded'", "\n", "f': {remaining_names}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet.init_weights": [[510, 530], ["isinstance", "utils.get_root_logger", "resnet.ResNet._load_torchvision_checkpoint", "mmcv.runner.load_checkpoint", "resnet.ResNet.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._load_torchvision_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "if", "self", ".", "torchvision_pretrain", ":", "\n", "# torchvision's", "\n", "                ", "self", ".", "_load_torchvision_checkpoint", "(", "logger", ")", "\n", "", "else", ":", "\n", "# ours", "\n", "                ", "load_checkpoint", "(", "\n", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet.forward": [[531, 553], ["resnet.ResNet.conv1", "resnet.ResNet.maxpool", "enumerate", "tuple", "getattr", "getattr.", "len", "outs.append"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The feature of the input samples extracted\n            by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "maxpool", "(", "x", ")", "\n", "outs", "=", "[", "]", "\n", "for", "i", ",", "layer_name", "in", "enumerate", "(", "self", ".", "res_layers", ")", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "x", "=", "res_layer", "(", "x", ")", "\n", "if", "i", "in", "self", ".", "out_indices", ":", "\n", "                ", "outs", ".", "append", "(", "x", ")", "\n", "", "", "if", "len", "(", "outs", ")", "==", "1", ":", "\n", "            ", "return", "outs", "[", "0", "]", "\n", "\n", "", "return", "tuple", "(", "outs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._freeze_stages": [[554, 568], ["range", "resnet.ResNet.conv1.bn.eval", "resnet.ResNet.conv1.modules", "getattr", "getattr.eval", "getattr.parameters", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "bn", ".", "eval", "(", ")", "\n", "for", "m", "in", "self", ".", "conv1", ".", "modules", "(", ")", ":", "\n", "                ", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._partial_bn": [[569, 581], ["utils.get_root_logger", "utils.get_root_logger.info", "resnet.ResNet.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger"], ["", "", "", "def", "_partial_bn", "(", "self", ")", ":", "\n", "        ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "'Freezing BatchNorm2D except the first one.'", ")", "\n", "count_bn", "=", "0", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "count_bn", "+=", "1", "\n", "if", "count_bn", ">=", "2", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "# shutdown update in frozen mode", "\n", "m", ".", "weight", ".", "requires_grad", "=", "False", "\n", "m", ".", "bias", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet.train": [[582, 592], ["super().train", "resnet.ResNet._freeze_stages", "resnet.ResNet.modules", "resnet.ResNet._partial_bn", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.ResNet._partial_bn"], ["", "", "", "", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "if", "mode", "and", "self", ".", "partial_bn", ":", "\n", "            ", "self", ".", "_partial_bn", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet.make_res_layer": [[222, 294], ["layers.append", "range", "torch.Sequential", "mmcv.cnn.ConvModule", "block", "layers.append", "block"], "function", ["None"], ["", "", "def", "make_res_layer", "(", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "style", "=", "'pytorch'", ",", "\n", "conv_cfg", "=", "None", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "    ", "\"\"\"Build residual layer for ResNet.\n\n    Args:\n        block: (nn.Module): Residual module to be built.\n        inplanes (int): Number of channels for the input feature in each block.\n        planes (int): Number of channels for the output feature in each block.\n        blocks (int): Number of residual blocks.\n        stride (int): Stride in the conv layer. Default: 1.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer. Default: 'pytorch'.\n        conv_cfg (dict | None): Config for norm layers. Default: None.\n        norm_cfg (dict | None): Config for norm layers. Default: None.\n        act_cfg (dict | None): Config for activate layers. Default: None.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed. Default: False.\n\n    Returns:\n        nn.Module: A residual layer for the given config.\n    \"\"\"", "\n", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "        ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", ",", "\n", "dilation", ",", "\n", "downsample", ",", "\n", "style", "=", "style", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", ")", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "_", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "        ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "dilation", ",", "\n", "style", "=", "style", ",", "\n", "conv_cfg", "=", "conv_cfg", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "act_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.PatchEmbed.__init__": [[29, 55], ["dict", "torch.Module.__init__", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "mmcv.cnn.build_conv_layer", "timesformer.PatchEmbed.init_weights"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "img_size", ",", "\n", "patch_size", ",", "\n", "in_channels", "=", "3", ",", "\n", "embed_dims", "=", "768", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv2d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "img_size", "=", "_pair", "(", "img_size", ")", "\n", "self", ".", "patch_size", "=", "_pair", "(", "patch_size", ")", "\n", "\n", "num_patches", "=", "(", "self", ".", "img_size", "[", "1", "]", "//", "self", ".", "patch_size", "[", "1", "]", ")", "*", "(", "\n", "self", ".", "img_size", "[", "0", "]", "//", "self", ".", "patch_size", "[", "0", "]", ")", "\n", "assert", "num_patches", "*", "self", ".", "patch_size", "[", "0", "]", "*", "self", ".", "patch_size", "[", "1", "]", "==", "self", ".", "img_size", "[", "0", "]", "*", "self", ".", "img_size", "[", "1", "]", ",", "'The image size H*W must be divisible by patch size'", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "# Use conv layer to embed", "\n", "self", ".", "projection", "=", "build_conv_layer", "(", "\n", "conv_cfg", ",", "\n", "in_channels", ",", "\n", "embed_dims", ",", "\n", "kernel_size", "=", "patch_size", ",", "\n", "stride", "=", "patch_size", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.PatchEmbed.init_weights": [[56, 59], ["mmcv.cnn.kaiming_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# Lecun norm from ClassyVision", "\n", "        ", "kaiming_init", "(", "self", ".", "projection", ",", "mode", "=", "'fan_in'", ",", "nonlinearity", "=", "'linear'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.PatchEmbed.forward": [[60, 64], ["einops.rearrange", "timesformer.PatchEmbed.projection().flatten().transpose", "timesformer.PatchEmbed.projection().flatten", "timesformer.PatchEmbed.projection"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optim.adamp.projection"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "rearrange", "(", "x", ",", "'b c t h w -> (b t) c h w'", ")", "\n", "x", "=", "self", ".", "projection", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.TimeSformer.__init__": [[98, 216], ["dict", "torch.Module.__init__", "timesformer.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "mmcv.cnn.bricks.transformer.build_transformer_layer_sequence", "isinstance", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "mmcv.cnn.build_norm_layer", "numpy.linspace", "mmcv.ConfigDict", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "dict", "dict", "dict", "range", "range", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "num_frames", ",", "\n", "img_size", ",", "\n", "patch_size", ",", "\n", "pretrained", "=", "None", ",", "\n", "embed_dims", "=", "768", ",", "\n", "num_heads", "=", "12", ",", "\n", "num_transformer_layers", "=", "12", ",", "\n", "in_channels", "=", "3", ",", "\n", "dropout_ratio", "=", "0.", ",", "\n", "transformer_layers", "=", "None", ",", "\n", "attention_type", "=", "'divided_space_time'", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ",", "eps", "=", "1e-6", ")", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "assert", "attention_type", "in", "self", ".", "supported_attention_types", ",", "(", "\n", "f'Unsupported Attention Type {attention_type}!'", ")", "\n", "assert", "transformer_layers", "is", "None", "or", "isinstance", "(", "\n", "transformer_layers", ",", "(", "dict", ",", "list", ")", ")", "\n", "\n", "self", ".", "num_frames", "=", "num_frames", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "num_transformer_layers", "=", "num_transformer_layers", "\n", "self", ".", "attention_type", "=", "attention_type", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", ",", "\n", "patch_size", "=", "patch_size", ",", "\n", "in_channels", "=", "in_channels", ",", "\n", "embed_dims", "=", "embed_dims", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dims", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dims", ")", ")", "\n", "self", ".", "drop_after_pos", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_ratio", ")", "\n", "if", "self", ".", "attention_type", "!=", "'space_only'", ":", "\n", "            ", "self", ".", "time_embed", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "zeros", "(", "1", ",", "num_frames", ",", "embed_dims", ")", ")", "\n", "self", ".", "drop_after_time", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_ratio", ")", "\n", "\n", "", "self", ".", "norm", "=", "build_norm_layer", "(", "norm_cfg", ",", "embed_dims", ")", "[", "1", "]", "\n", "\n", "if", "transformer_layers", "is", "None", ":", "\n", "# stochastic depth decay rule", "\n", "            ", "dpr", "=", "np", ".", "linspace", "(", "0", ",", "0.1", ",", "num_transformer_layers", ")", "\n", "\n", "if", "self", ".", "attention_type", "==", "'divided_space_time'", ":", "\n", "                ", "_transformerlayers_cfg", "=", "[", "\n", "dict", "(", "\n", "type", "=", "'BaseTransformerLayer'", ",", "\n", "attn_cfgs", "=", "[", "\n", "dict", "(", "\n", "type", "=", "'DividedTemporalAttentionWithNorm'", ",", "\n", "embed_dims", "=", "embed_dims", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "num_frames", "=", "num_frames", ",", "\n", "dropout_layer", "=", "dict", "(", "\n", "type", "=", "'DropPath'", ",", "drop_prob", "=", "dpr", "[", "i", "]", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ",", "eps", "=", "1e-6", ")", ")", ",", "\n", "dict", "(", "\n", "type", "=", "'DividedSpatialAttentionWithNorm'", ",", "\n", "embed_dims", "=", "embed_dims", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "num_frames", "=", "num_frames", ",", "\n", "dropout_layer", "=", "dict", "(", "\n", "type", "=", "'DropPath'", ",", "drop_prob", "=", "dpr", "[", "i", "]", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ",", "eps", "=", "1e-6", ")", ")", "\n", "]", ",", "\n", "ffn_cfgs", "=", "dict", "(", "\n", "type", "=", "'FFNWithNorm'", ",", "\n", "embed_dims", "=", "embed_dims", ",", "\n", "feedforward_channels", "=", "embed_dims", "*", "4", ",", "\n", "num_fcs", "=", "2", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'GELU'", ")", ",", "\n", "dropout_layer", "=", "dict", "(", "\n", "type", "=", "'DropPath'", ",", "drop_prob", "=", "dpr", "[", "i", "]", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ",", "eps", "=", "1e-6", ")", ")", ",", "\n", "operation_order", "=", "(", "'self_attn'", ",", "'self_attn'", ",", "'ffn'", ")", ")", "\n", "for", "i", "in", "range", "(", "num_transformer_layers", ")", "\n", "]", "\n", "", "else", ":", "\n", "# Sapce Only & Joint Space Time", "\n", "                ", "_transformerlayers_cfg", "=", "[", "\n", "dict", "(", "\n", "type", "=", "'BaseTransformerLayer'", ",", "\n", "attn_cfgs", "=", "[", "\n", "dict", "(", "\n", "type", "=", "'MultiheadAttention'", ",", "\n", "embed_dims", "=", "embed_dims", ",", "\n", "num_heads", "=", "num_heads", ",", "\n", "batch_first", "=", "True", ",", "\n", "dropout_layer", "=", "dict", "(", "\n", "type", "=", "'DropPath'", ",", "drop_prob", "=", "dpr", "[", "i", "]", ")", ")", "\n", "]", ",", "\n", "ffn_cfgs", "=", "dict", "(", "\n", "type", "=", "'FFN'", ",", "\n", "embed_dims", "=", "embed_dims", ",", "\n", "feedforward_channels", "=", "embed_dims", "*", "4", ",", "\n", "num_fcs", "=", "2", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'GELU'", ")", ",", "\n", "dropout_layer", "=", "dict", "(", "\n", "type", "=", "'DropPath'", ",", "drop_prob", "=", "dpr", "[", "i", "]", ")", ")", ",", "\n", "operation_order", "=", "(", "'norm'", ",", "'self_attn'", ",", "'norm'", ",", "'ffn'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ",", "eps", "=", "1e-6", ")", ",", "\n", "batch_first", "=", "True", ")", "\n", "for", "i", "in", "range", "(", "num_transformer_layers", ")", "\n", "]", "\n", "\n", "", "transformer_layers", "=", "ConfigDict", "(", "\n", "dict", "(", "\n", "type", "=", "'TransformerLayerSequence'", ",", "\n", "transformerlayers", "=", "_transformerlayers_cfg", ",", "\n", "num_layers", "=", "num_transformer_layers", ")", ")", "\n", "\n", "", "self", ".", "transformer_layers", "=", "build_transformer_layer_sequence", "(", "\n", "transformer_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.TimeSformer.init_weights": [[217, 252], ["mmcv.cnn.utils.weight_init.trunc_normal_", "mmcv.cnn.utils.weight_init.trunc_normal_", "isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner._load_checkpoint", "mmcv.runner.load_state_dict", "list", "list", "mmcv.runner._load_checkpoint.keys", "mmcv.runner._load_checkpoint.keys", "old_key.replace", "old_key.replace.replace", "mmcv.runner._load_checkpoint.pop", "old_key.replace", "state_dict[].clone"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.weight_init.trunc_normal_", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEma._load_checkpoint", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_state_dict"], ["", "def", "init_weights", "(", "self", ",", "pretrained", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "\n", "if", "pretrained", ":", "\n", "            ", "self", ".", "pretrained", "=", "pretrained", "\n", "", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "state_dict", "=", "_load_checkpoint", "(", "self", ".", "pretrained", ")", "\n", "if", "'state_dict'", "in", "state_dict", ":", "\n", "                ", "state_dict", "=", "state_dict", "[", "'state_dict'", "]", "\n", "\n", "", "if", "self", ".", "attention_type", "==", "'divided_space_time'", ":", "\n", "# modify the key names of norm layers", "\n", "                ", "old_state_dict_keys", "=", "list", "(", "state_dict", ".", "keys", "(", ")", ")", "\n", "for", "old_key", "in", "old_state_dict_keys", ":", "\n", "                    ", "if", "'norms'", "in", "old_key", ":", "\n", "                        ", "new_key", "=", "old_key", ".", "replace", "(", "'norms.0'", ",", "\n", "'attentions.0.norm'", ")", "\n", "new_key", "=", "new_key", ".", "replace", "(", "'norms.1'", ",", "'ffns.0.norm'", ")", "\n", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# copy the parameters of space attention to time attention", "\n", "", "", "old_state_dict_keys", "=", "list", "(", "state_dict", ".", "keys", "(", ")", ")", "\n", "for", "old_key", "in", "old_state_dict_keys", ":", "\n", "                    ", "if", "'attentions.0'", "in", "old_key", ":", "\n", "                        ", "new_key", "=", "old_key", ".", "replace", "(", "'attentions.0'", ",", "\n", "'attentions.1'", ")", "\n", "state_dict", "[", "new_key", "]", "=", "state_dict", "[", "old_key", "]", ".", "clone", "(", ")", "\n", "\n", "", "", "", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.timesformer.TimeSformer.forward": [[253, 286], ["timesformer.TimeSformer.patch_embed", "timesformer.TimeSformer.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "timesformer.TimeSformer.drop_after_pos", "timesformer.TimeSformer.transformer_layers", "timesformer.TimeSformer.norm", "torch.mean.size", "torch.mean.size", "x[].unsqueeze", "einops.rearrange", "timesformer.TimeSformer.drop_after_time", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean.view", "torch.mean.view", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean.size", "torch.mean.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "# x [batch_size * num_frames, num_patches, embed_dims]", "\n", "batches", "=", "x", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "\n", "# x [batch_size * num_frames, num_patches + 1, embed_dims]", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ",", "-", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", "+", "self", ".", "pos_embed", "\n", "x", "=", "self", ".", "drop_after_pos", "(", "x", ")", "\n", "\n", "# Add Time Embedding", "\n", "if", "self", ".", "attention_type", "!=", "'space_only'", ":", "\n", "# x [batch_size, num_patches * num_frames + 1, embed_dims]", "\n", "            ", "cls_tokens", "=", "x", "[", ":", "batches", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "\n", "x", "=", "rearrange", "(", "x", "[", ":", ",", "1", ":", ",", ":", "]", ",", "'(b t) p m -> (b p) t m'", ",", "b", "=", "batches", ")", "\n", "x", "=", "x", "+", "self", ".", "time_embed", "\n", "x", "=", "self", ".", "drop_after_time", "(", "x", ")", "\n", "x", "=", "rearrange", "(", "x", ",", "'(b p) t m -> b (p t) m'", ",", "b", "=", "batches", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "x", "=", "self", ".", "transformer_layers", "(", "x", ",", "None", ",", "None", ")", "\n", "\n", "if", "self", ".", "attention_type", "==", "'space_only'", ":", "\n", "# x [batch_size, num_patches + 1, embed_dims]", "\n", "            ", "x", "=", "x", ".", "view", "(", "-", "1", ",", "self", ".", "num_frames", ",", "*", "x", ".", "size", "(", ")", "[", "-", "2", ":", "]", ")", "\n", "x", "=", "torch", ".", "mean", "(", "x", ",", "1", ")", "\n", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "# Return Class Token", "\n", "return", "x", "[", ":", ",", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.Bottleneck2dAudio.__init__": [[31, 87], ["torch.Module.__init__", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "torch.ReLU", "torch.ReLU", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", "=", "2", ",", "\n", "dilation", "=", "1", ",", "\n", "downsample", "=", "None", ",", "\n", "factorize", "=", "True", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "inplanes", "=", "inplanes", "\n", "self", ".", "planes", "=", "planes", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "factorize", "=", "factorize", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "\n", "self", ".", "conv1_stride", "=", "1", "\n", "self", ".", "conv2_stride", "=", "stride", "\n", "\n", "conv1_kernel_size", "=", "(", "1", ",", "1", ")", "\n", "conv1_padding", "=", "0", "\n", "conv2_kernel_size", "=", "(", "3", ",", "3", ")", "\n", "conv2_padding", "=", "(", "dilation", ",", "dilation", ")", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "conv1_kernel_size", ",", "\n", "padding", "=", "conv1_padding", ",", "\n", "dilation", "=", "dilation", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "conv2", "=", "ConvModule", "(", "\n", "planes", ",", "\n", "planes", ",", "\n", "kernel_size", "=", "conv2_kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "conv2_padding", ",", "\n", "dilation", "=", "dilation", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'ConvAudio'", ")", "if", "factorize", "else", "dict", "(", "\n", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "act_cfg", "=", "None", ")", "\n", "self", ".", "conv3", "=", "ConvModule", "(", "\n", "2", "*", "planes", "if", "factorize", "else", "planes", ",", "\n", "planes", "*", "self", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "bias", "=", "False", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "downsample", "=", "downsample", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.Bottleneck2dAudio.forward": [[88, 110], ["resnet_audio.Bottleneck2dAudio.relu", "resnet_audio.Bottleneck2dAudio.conv1", "resnet_audio.Bottleneck2dAudio.conv2", "resnet_audio.Bottleneck2dAudio.conv3", "torch.checkpoint", "torch.checkpoint", "resnet_audio.Bottleneck2dAudio.forward._inner_forward"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "def", "_inner_forward", "(", "x", ")", ":", "\n", "            ", "identity", "=", "x", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "conv3", "(", "out", ")", "\n", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "                ", "identity", "=", "self", ".", "downsample", "(", "x", ")", "\n", "", "out", "+=", "identity", "\n", "\n", "return", "out", "\n", "\n", "", "if", "self", ".", "with_cp", "and", "x", ".", "requires_grad", ":", "\n", "            ", "out", "=", "cp", ".", "checkpoint", "(", "_inner_forward", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "_inner_forward", "(", "x", ")", "\n", "\n", "", "out", "=", "self", ".", "relu", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.__init__": [[158, 225], ["dict", "dict", "dict", "torch.Module.__init__", "resnet_audio.ResNetAudio._make_stem_layer", "enumerate", "KeyError", "torch.nn.modules.utils._ntuple", "torch.nn.modules.utils._ntuple", "resnet_audio.ResNetAudio.make_res_layer", "resnet_audio.ResNetAudio.add_module", "resnet_audio.ResNetAudio.res_layers.append", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.layers.helpers._ntuple", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer"], ["def", "__init__", "(", "self", ",", "\n", "depth", ",", "\n", "pretrained", ",", "\n", "in_channels", "=", "1", ",", "\n", "num_stages", "=", "4", ",", "\n", "base_channels", "=", "32", ",", "\n", "strides", "=", "(", "1", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "dilations", "=", "(", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "conv1_kernel", "=", "9", ",", "\n", "conv1_stride", "=", "1", ",", "\n", "frozen_stages", "=", "-", "1", ",", "\n", "factorize", "=", "(", "1", ",", "1", ",", "0", ",", "0", ")", ",", "\n", "norm_eval", "=", "False", ",", "\n", "with_cp", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN2d'", ",", "requires_grad", "=", "True", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ",", "inplace", "=", "True", ")", ",", "\n", "zero_init_residual", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "depth", "not", "in", "self", ".", "arch_settings", ":", "\n", "            ", "raise", "KeyError", "(", "f'invalid depth {depth} for resnet'", ")", "\n", "", "self", ".", "depth", "=", "depth", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "base_channels", "=", "base_channels", "\n", "self", ".", "num_stages", "=", "num_stages", "\n", "assert", "1", "<=", "num_stages", "<=", "4", "\n", "self", ".", "dilations", "=", "dilations", "\n", "self", ".", "conv1_kernel", "=", "conv1_kernel", "\n", "self", ".", "conv1_stride", "=", "conv1_stride", "\n", "self", ".", "frozen_stages", "=", "frozen_stages", "\n", "self", ".", "stage_factorization", "=", "_ntuple", "(", "num_stages", ")", "(", "factorize", ")", "\n", "self", ".", "norm_eval", "=", "norm_eval", "\n", "self", ".", "with_cp", "=", "with_cp", "\n", "self", ".", "conv_cfg", "=", "conv_cfg", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "act_cfg", "=", "act_cfg", "\n", "self", ".", "zero_init_residual", "=", "zero_init_residual", "\n", "\n", "self", ".", "block", ",", "stage_blocks", "=", "self", ".", "arch_settings", "[", "depth", "]", "\n", "self", ".", "stage_blocks", "=", "stage_blocks", "[", ":", "num_stages", "]", "\n", "self", ".", "inplanes", "=", "self", ".", "base_channels", "\n", "\n", "self", ".", "_make_stem_layer", "(", ")", "\n", "\n", "self", ".", "res_layers", "=", "[", "]", "\n", "for", "i", ",", "num_blocks", "in", "enumerate", "(", "self", ".", "stage_blocks", ")", ":", "\n", "            ", "stride", "=", "strides", "[", "i", "]", "\n", "dilation", "=", "dilations", "[", "i", "]", "\n", "planes", "=", "self", ".", "base_channels", "*", "2", "**", "i", "\n", "res_layer", "=", "self", ".", "make_res_layer", "(", "\n", "self", ".", "block", ",", "\n", "self", ".", "inplanes", ",", "\n", "planes", ",", "\n", "num_blocks", ",", "\n", "stride", "=", "stride", ",", "\n", "dilation", "=", "dilation", ",", "\n", "factorize", "=", "self", ".", "stage_factorization", "[", "i", "]", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "self", ".", "block", ".", "expansion", "\n", "layer_name", "=", "f'layer{i + 1}'", "\n", "self", ".", "add_module", "(", "layer_name", ",", "res_layer", ")", "\n", "self", ".", "res_layers", ".", "append", "(", "layer_name", ")", "\n", "\n", "", "self", ".", "feat_dim", "=", "self", ".", "block", ".", "expansion", "*", "self", ".", "base_channels", "*", "2", "**", "(", "\n", "len", "(", "self", ".", "stage_blocks", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.make_res_layer": [[226, 298], ["layers.append", "range", "torch.Sequential", "torch.Sequential", "len", "mmcv.cnn.ConvModule", "block", "layers.append", "isinstance", "block"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "make_res_layer", "(", "block", ",", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "blocks", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "1", ",", "\n", "factorize", "=", "1", ",", "\n", "norm_cfg", "=", "None", ",", "\n", "with_cp", "=", "False", ")", ":", "\n", "        ", "\"\"\"Build residual layer for ResNetAudio.\n\n        Args:\n            block (nn.Module): Residual module to be built.\n            inplanes (int): Number of channels for the input feature\n                in each block.\n            planes (int): Number of channels for the output feature\n                in each block.\n            blocks (int): Number of residual blocks.\n            stride (Sequence[int]): Strides of residual blocks of each stage.\n                Default: (1, 2, 2, 2).\n            dilation (int): Spacing between kernel elements. Default: 1.\n            factorize (int | Sequence[int]): Determine whether to factorize\n                for each block. Default: 1.\n            norm_cfg (dict):\n                Config for norm layers. required keys are `type` and\n                `requires_grad`. Default: None.\n            with_cp (bool): Use checkpoint or not. Using checkpoint will save\n                some memory while slowing down the training speed.\n                Default: False.\n\n        Returns:\n            A residual layer for the given config.\n        \"\"\"", "\n", "factorize", "=", "factorize", "if", "not", "isinstance", "(", "\n", "factorize", ",", "int", ")", "else", "(", "factorize", ",", ")", "*", "blocks", "\n", "assert", "len", "(", "factorize", ")", "==", "blocks", "\n", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "ConvModule", "(", "\n", "inplanes", ",", "\n", "planes", "*", "block", ".", "expansion", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "stride", ",", "\n", "bias", "=", "False", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "act_cfg", "=", "None", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "stride", ",", "\n", "dilation", ",", "\n", "downsample", ",", "\n", "factorize", "=", "(", "factorize", "[", "0", "]", "==", "1", ")", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", ")", "\n", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "\n", "block", "(", "\n", "inplanes", ",", "\n", "planes", ",", "\n", "1", ",", "\n", "dilation", ",", "\n", "factorize", "=", "(", "factorize", "[", "i", "]", "==", "1", ")", ",", "\n", "norm_cfg", "=", "norm_cfg", ",", "\n", "with_cp", "=", "with_cp", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._make_stem_layer": [[299, 311], ["mmcv.cnn.ConvModule", "dict"], "methods", ["None"], ["", "def", "_make_stem_layer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct the stem layers consists of a conv+norm+act module and a\n        pooling layer.\"\"\"", "\n", "self", ".", "conv1", "=", "ConvModule", "(", "\n", "self", ".", "in_channels", ",", "\n", "self", ".", "base_channels", ",", "\n", "kernel_size", "=", "self", ".", "conv1_kernel", ",", "\n", "stride", "=", "self", ".", "conv1_stride", ",", "\n", "bias", "=", "False", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'ConvAudio'", ",", "op", "=", "'sum'", ")", ",", "\n", "norm_cfg", "=", "self", ".", "norm_cfg", ",", "\n", "act_cfg", "=", "self", ".", "act_cfg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages": [[312, 326], ["range", "resnet_audio.ResNetAudio.conv1.bn.eval", "getattr", "getattr.eval", "getattr.parameters", "getattr.parameters"], "methods", ["None"], ["", "def", "_freeze_stages", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prevent all the parameters from being optimized before\n        ``self.frozen_stages``.\"\"\"", "\n", "if", "self", ".", "frozen_stages", ">=", "0", ":", "\n", "            ", "self", ".", "conv1", ".", "bn", ".", "eval", "(", ")", "\n", "for", "m", "in", "[", "self", ".", "conv1", ".", "conv", ",", "self", ".", "conv1", ".", "bn", "]", ":", "\n", "                ", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "for", "i", "in", "range", "(", "1", ",", "self", ".", "frozen_stages", "+", "1", ")", ":", "\n", "            ", "m", "=", "getattr", "(", "self", ",", "f'layer{i}'", ")", "\n", "m", ".", "eval", "(", ")", "\n", "for", "param", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.init_weights": [[327, 350], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "resnet_audio.ResNetAudio.modules", "TypeError", "isinstance", "resnet_audio.ResNetAudio.modules", "mmcv.cnn.kaiming_init", "isinstance", "isinstance", "mmcv.cnn.constant_init", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "load_checkpoint", "(", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "\n", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "\n", "", "", "if", "self", ".", "zero_init_residual", ":", "\n", "                ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "m", ",", "Bottleneck2dAudio", ")", ":", "\n", "                        ", "constant_init", "(", "m", ".", "conv3", ".", "bn", ",", "0", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.forward": [[351, 366], ["resnet_audio.ResNetAudio.conv1", "getattr", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The feature of the input samples extracted\n            by the backbone.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "for", "layer_name", "in", "self", ".", "res_layers", ":", "\n", "            ", "res_layer", "=", "getattr", "(", "self", ",", "layer_name", ")", "\n", "x", "=", "res_layer", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio.train": [[367, 375], ["super().train", "resnet_audio.ResNetAudio._freeze_stages", "resnet_audio.ResNetAudio.modules", "isinstance", "m.eval"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet_audio.ResNetAudio._freeze_stages"], ["", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "        ", "\"\"\"Set the optimization status when training.\"\"\"", "\n", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_freeze_stages", "(", ")", "\n", "if", "mode", "and", "self", ".", "norm_eval", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowonly.ResNet3dSlowOnly.__init__": [[30, 54], ["resnet3d_slowfast.ResNet3dPathway.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "*", "args", ",", "\n", "lateral", "=", "lateral", ",", "\n", "conv1_kernel", "=", "conv1_kernel", ",", "\n", "conv1_stride_t", "=", "conv1_stride_t", ",", "\n", "pool1_stride_t", "=", "pool1_stride_t", ",", "\n", "inflate", "=", "inflate", ",", "\n", "with_pool2", "=", "with_pool2", ",", "\n", "**", "kwargs", ")", "\n", "\n", "assert", "not", "self", ".", "lateral", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.resnet3d_slowonly.ResNet3dFastOnly.__init__": [[74, 98], ["resnet3d_slowfast.ResNet3dPathway.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.STGCNBlock.__init__": [[53, 89], ["torch.Module.__init__", "stgcn.ConvTemporalGraphical", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "len", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "dropout", "=", "0", ",", "\n", "residual", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "kernel_size", ")", "==", "2", "\n", "assert", "kernel_size", "[", "0", "]", "%", "2", "==", "1", "\n", "padding", "=", "(", "(", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", ",", "0", ")", "\n", "\n", "self", ".", "gcn", "=", "ConvTemporalGraphical", "(", "in_channels", ",", "out_channels", ",", "\n", "kernel_size", "[", "1", "]", ")", "\n", "self", ".", "tcn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "out_channels", ",", "out_channels", ",", "(", "kernel_size", "[", "0", "]", ",", "1", ")", ",", "\n", "(", "stride", ",", "1", ")", ",", "padding", ")", ",", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ",", "inplace", "=", "True", ")", ")", "\n", "\n", "if", "not", "residual", ":", "\n", "            ", "self", ".", "residual", "=", "zero", "\n", "\n", "", "elif", "(", "in_channels", "==", "out_channels", ")", "and", "(", "stride", "==", "1", ")", ":", "\n", "            ", "self", ".", "residual", "=", "identity", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "residual", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "stride", ",", "1", ")", ")", ",", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ")", "\n", "\n", "", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.STGCNBlock.forward": [[90, 97], ["stgcn.STGCNBlock.residual", "stgcn.STGCNBlock.gcn", "stgcn.STGCNBlock.tcn", "stgcn.STGCNBlock.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "adj_mat", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "res", "=", "self", ".", "residual", "(", "x", ")", "\n", "x", ",", "adj_mat", "=", "self", ".", "gcn", "(", "x", ",", "adj_mat", ")", "\n", "x", "=", "self", ".", "tcn", "(", "x", ")", "+", "res", "\n", "\n", "return", "self", ".", "relu", "(", "x", ")", ",", "adj_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.ConvTemporalGraphical.__init__": [[133, 153], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "t_kernel_size", "=", "1", ",", "\n", "t_stride", "=", "1", ",", "\n", "t_padding", "=", "0", ",", "\n", "t_dilation", "=", "1", ",", "\n", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "conv", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", "*", "kernel_size", ",", "\n", "kernel_size", "=", "(", "t_kernel_size", ",", "1", ")", ",", "\n", "padding", "=", "(", "t_padding", ",", "0", ")", ",", "\n", "stride", "=", "(", "t_stride", ",", "1", ")", ",", "\n", "dilation", "=", "(", "t_dilation", ",", "1", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.ConvTemporalGraphical.forward": [[154, 165], ["stgcn.ConvTemporalGraphical.conv", "torch.einsum.size", "torch.einsum.size", "torch.einsum.view", "torch.einsum.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "adj_mat.size", "torch.einsum.contiguous", "torch.einsum.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "adj_mat", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "assert", "adj_mat", ".", "size", "(", "0", ")", "==", "self", ".", "kernel_size", "\n", "\n", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "\n", "n", ",", "kc", ",", "t", ",", "v", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "self", ".", "kernel_size", ",", "kc", "//", "self", ".", "kernel_size", ",", "t", ",", "v", ")", "\n", "x", "=", "torch", ".", "einsum", "(", "'nkctv,kvw->nctw'", ",", "(", "x", ",", "adj_mat", ")", ")", "\n", "\n", "return", "x", ".", "contiguous", "(", ")", ",", "adj_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.STGCN.__init__": [[190, 237], ["torch.Module.__init__", "skeleton_gcn.utils.Graph", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "stgcn.STGCN.register_buffer", "torch.tensor.size", "torch.tensor.size", "torch.ModuleList", "torch.ModuleList", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ParameterList", "torch.ParameterList", "kwargs.items", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "stgcn.STGCNBlock", "torch.tensor.size", "torch.tensor.size", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "stgcn.STGCN.A.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "graph_cfg", ",", "\n", "edge_importance_weighting", "=", "True", ",", "\n", "data_bn", "=", "True", ",", "\n", "pretrained", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# load graph", "\n", "self", ".", "graph", "=", "Graph", "(", "**", "graph_cfg", ")", "\n", "A", "=", "torch", ".", "tensor", "(", "\n", "self", ".", "graph", ".", "A", ",", "dtype", "=", "torch", ".", "float32", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "A", ")", "\n", "\n", "# build networks", "\n", "spatial_kernel_size", "=", "A", ".", "size", "(", "0", ")", "\n", "temporal_kernel_size", "=", "9", "\n", "kernel_size", "=", "(", "temporal_kernel_size", ",", "spatial_kernel_size", ")", "\n", "self", ".", "data_bn", "=", "nn", ".", "BatchNorm1d", "(", "in_channels", "*", "\n", "A", ".", "size", "(", "1", ")", ")", "if", "data_bn", "else", "identity", "\n", "\n", "kwargs0", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", "if", "k", "!=", "'dropout'", "}", "\n", "self", ".", "st_gcn_networks", "=", "nn", ".", "ModuleList", "(", "(", "\n", "STGCNBlock", "(", "\n", "in_channels", ",", "64", ",", "kernel_size", ",", "1", ",", "residual", "=", "False", ",", "**", "kwargs0", ")", ",", "\n", "STGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "64", ",", "128", ",", "kernel_size", ",", "2", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "128", ",", "128", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "128", ",", "128", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "128", ",", "256", ",", "kernel_size", ",", "2", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "256", ",", "256", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", "STGCNBlock", "(", "256", ",", "256", ",", "kernel_size", ",", "1", ",", "**", "kwargs", ")", ",", "\n", ")", ")", "\n", "\n", "# initialize parameters for edge importance weighting", "\n", "if", "edge_importance_weighting", ":", "\n", "            ", "self", ".", "edge_importance", "=", "nn", ".", "ParameterList", "(", "[", "\n", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "self", ".", "A", ".", "size", "(", ")", ")", ")", "\n", "for", "i", "in", "self", ".", "st_gcn_networks", "\n", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "edge_importance", "=", "[", "1", "for", "_", "in", "self", ".", "st_gcn_networks", "]", "\n", "\n", "", "self", ".", "pretrained", "=", "pretrained", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.STGCN.init_weights": [[238, 257], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "stgcn.STGCN.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.normal_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "load_checkpoint", "(", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "\n", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                    ", "normal_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.STGCN.forward": [[258, 281], ["x.view.view.float", "x.view.view.size", "x.view.view.permute().contiguous", "x.view.view.view", "stgcn.STGCN.data_bn", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "zip", "gcn", "x.view.view.permute", "x.view.view.permute"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# data normalization", "\n", "x", "=", "x", ".", "float", "(", ")", "\n", "n", ",", "c", ",", "t", ",", "v", ",", "m", "=", "x", ".", "size", "(", ")", "# bs 3 300 25(17) 2", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "4", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "# N M V C T", "\n", "x", "=", "x", ".", "view", "(", "n", "*", "m", ",", "v", "*", "c", ",", "t", ")", "\n", "x", "=", "self", ".", "data_bn", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "m", ",", "v", ",", "c", ",", "t", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "4", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "n", "*", "m", ",", "c", ",", "t", ",", "v", ")", "# bsx2 3 300 25(17)", "\n", "\n", "# forward", "\n", "for", "gcn", ",", "importance", "in", "zip", "(", "self", ".", "st_gcn_networks", ",", "self", ".", "edge_importance", ")", ":", "\n", "            ", "x", ",", "_", "=", "gcn", "(", "x", ",", "self", ".", "A", "*", "importance", ")", "\n", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.zero": [[12, 15], ["None"], "function", ["None"], ["def", "zero", "(", "x", ")", ":", "\n", "    ", "\"\"\"return zero.\"\"\"", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.stgcn.identity": [[17, 20], ["None"], "function", ["None"], ["", "def", "identity", "(", "x", ")", ":", "\n", "    ", "\"\"\"return input itself.\"\"\"", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2_tsm.MobileNetV2TSM.__init__": [[19, 24], ["mobilenet_v2.MobileNetV2.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "num_segments", "=", "8", ",", "is_shift", "=", "True", ",", "shift_div", "=", "8", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "is_shift", "=", "is_shift", "\n", "self", ".", "shift_div", "=", "shift_div", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2_tsm.MobileNetV2TSM.make_temporal_shift": [[25, 34], ["mobilenet_v2_tsm.MobileNetV2TSM.modules", "isinstance", "resnet_tsm.TemporalShift", "len"], "methods", ["None"], ["", "def", "make_temporal_shift", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make temporal shift for some layers.\"\"\"", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "InvertedResidual", ")", "and", "len", "(", "m", ".", "conv", ")", "==", "3", "and", "m", ".", "use_res_connect", ":", "\n", "                ", "m", ".", "conv", "[", "0", "]", "=", "TemporalShift", "(", "\n", "m", ".", "conv", "[", "0", "]", ",", "\n", "num_segments", "=", "self", ".", "num_segments", ",", "\n", "shift_div", "=", "self", ".", "shift_div", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2_tsm.MobileNetV2TSM.init_weights": [[36, 42], ["super().init_weights", "mobilenet_v2_tsm.MobileNetV2TSM.make_temporal_shift"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.mobilenet_v2_tsm.MobileNetV2TSM.make_temporal_shift"], ["", "", "", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "super", "(", ")", ".", "init_weights", "(", ")", "\n", "if", "self", ".", "is_shift", ":", "\n", "            ", "self", ".", "make_temporal_shift", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.AGCNBlock.__init__": [[55, 92], ["torch.Module.__init__", "agcn.ConvTemporalGraphical", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "len", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "adj_len", "=", "17", ",", "\n", "dropout", "=", "0", ",", "\n", "residual", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "len", "(", "kernel_size", ")", "==", "2", "\n", "assert", "kernel_size", "[", "0", "]", "%", "2", "==", "1", "\n", "padding", "=", "(", "(", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", ",", "0", ")", "\n", "\n", "self", ".", "gcn", "=", "ConvTemporalGraphical", "(", "\n", "in_channels", ",", "out_channels", ",", "kernel_size", "[", "1", "]", ",", "adj_len", "=", "adj_len", ")", "\n", "self", ".", "tcn", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "out_channels", ",", "out_channels", ",", "(", "kernel_size", "[", "0", "]", ",", "1", ")", ",", "\n", "(", "stride", ",", "1", ")", ",", "padding", ")", ",", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ",", "\n", "nn", ".", "Dropout", "(", "dropout", ",", "inplace", "=", "True", ")", ")", "\n", "\n", "if", "not", "residual", ":", "\n", "            ", "self", ".", "residual", "=", "zero", "\n", "\n", "", "elif", "(", "in_channels", "==", "out_channels", ")", "and", "(", "stride", "==", "1", ")", ":", "\n", "            ", "self", ".", "residual", "=", "identity", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "residual", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "1", ",", "\n", "stride", "=", "(", "stride", ",", "1", ")", ")", ",", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ")", "\n", "\n", "", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.AGCNBlock.forward": [[93, 101], ["agcn.AGCNBlock.residual", "agcn.AGCNBlock.gcn", "agcn.AGCNBlock.tcn", "agcn.AGCNBlock.relu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "adj_mat", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "res", "=", "self", ".", "residual", "(", "x", ")", "\n", "x", ",", "adj_mat", "=", "self", ".", "gcn", "(", "x", ",", "adj_mat", ")", "\n", "\n", "x", "=", "self", ".", "tcn", "(", "x", ")", "+", "res", "\n", "\n", "return", "self", ".", "relu", "(", "x", ")", ",", "adj_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.ConvTemporalGraphical.__init__": [[139, 177], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.BatchNorm2d", "torch.BatchNorm2d", "torch.Softmax", "torch.Softmax", "torch.ReLU", "torch.ReLU", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "agcn.ConvTemporalGraphical.conv_a.append", "agcn.ConvTemporalGraphical.conv_b.append", "agcn.ConvTemporalGraphical.conv_d.append", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.BatchNorm2d", "torch.BatchNorm2d"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "t_kernel_size", "=", "1", ",", "\n", "t_stride", "=", "1", ",", "\n", "t_padding", "=", "0", ",", "\n", "t_dilation", "=", "1", ",", "\n", "adj_len", "=", "17", ",", "\n", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "\n", "self", ".", "PA", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "3", ",", "adj_len", ",", "adj_len", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "self", ".", "PA", ",", "1e-6", ")", "\n", "\n", "self", ".", "num_subset", "=", "3", "\n", "inter_channels", "=", "out_channels", "//", "4", "\n", "self", ".", "inter_c", "=", "inter_channels", "\n", "self", ".", "conv_a", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "conv_b", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "conv_d", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_subset", ")", ":", "\n", "            ", "self", ".", "conv_a", ".", "append", "(", "nn", ".", "Conv2d", "(", "in_channels", ",", "inter_channels", ",", "1", ")", ")", "\n", "self", ".", "conv_b", ".", "append", "(", "nn", ".", "Conv2d", "(", "in_channels", ",", "inter_channels", ",", "1", ")", ")", "\n", "self", ".", "conv_d", ".", "append", "(", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ")", ")", "\n", "\n", "", "if", "in_channels", "!=", "out_channels", ":", "\n", "            ", "self", ".", "down", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "1", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "down", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "bn", "=", "nn", ".", "BatchNorm2d", "(", "out_channels", ")", "\n", "self", ".", "soft", "=", "nn", ".", "Softmax", "(", "-", "2", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.ConvTemporalGraphical.forward": [[178, 199], ["x.size", "range", "agcn.ConvTemporalGraphical.bn", "agcn.ConvTemporalGraphical.down", "adj_mat.size", "agcn.ConvTemporalGraphical.soft", "x.view", "agcn.ConvTemporalGraphical.relu", "torch.matmul().view", "torch.matmul().view", "torch.matmul().view", "torch.matmul().view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "agcn.ConvTemporalGraphical.size", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "adj_mat", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\"\"\"", "\n", "assert", "adj_mat", ".", "size", "(", "0", ")", "==", "self", ".", "kernel_size", "\n", "\n", "N", ",", "C", ",", "T", ",", "V", "=", "x", ".", "size", "(", ")", "\n", "A", "=", "adj_mat", "+", "self", ".", "PA", "\n", "\n", "y", "=", "None", "\n", "for", "i", "in", "range", "(", "self", ".", "num_subset", ")", ":", "\n", "            ", "A1", "=", "self", ".", "conv_a", "[", "i", "]", "(", "x", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "N", ",", "V", ",", "self", ".", "inter_c", "*", "T", ")", "\n", "A2", "=", "self", ".", "conv_b", "[", "i", "]", "(", "x", ")", ".", "view", "(", "N", ",", "self", ".", "inter_c", "*", "T", ",", "V", ")", "\n", "A1", "=", "self", ".", "soft", "(", "torch", ".", "matmul", "(", "A1", ",", "A2", ")", "/", "A1", ".", "size", "(", "-", "1", ")", ")", "# N V V", "\n", "A1", "=", "A1", "+", "A", "[", "i", "]", "\n", "A2", "=", "x", ".", "view", "(", "N", ",", "C", "*", "T", ",", "V", ")", "\n", "z", "=", "self", ".", "conv_d", "[", "i", "]", "(", "torch", ".", "matmul", "(", "A2", ",", "A1", ")", ".", "view", "(", "N", ",", "C", ",", "T", ",", "V", ")", ")", "\n", "y", "=", "z", "+", "y", "if", "y", "is", "not", "None", "else", "z", "\n", "", "y", "=", "self", ".", "bn", "(", "y", ")", "\n", "y", "+=", "self", ".", "down", "(", "x", ")", "\n", "\n", "return", "self", ".", "relu", "(", "y", ")", ",", "adj_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.AGCN.__init__": [[223, 266], ["torch.Module.__init__", "skeleton_gcn.utils.Graph", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "agcn.AGCN.register_buffer", "torch.tensor.size", "torch.tensor.size", "torch.ModuleList", "torch.ModuleList", "torch.BatchNorm1d", "torch.BatchNorm1d", "kwargs.items", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "agcn.AGCNBlock", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size", "torch.tensor.size"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "graph_cfg", ",", "\n", "data_bn", "=", "True", ",", "\n", "pretrained", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# load graph", "\n", "self", ".", "graph", "=", "Graph", "(", "**", "graph_cfg", ")", "\n", "A", "=", "torch", ".", "tensor", "(", "\n", "self", ".", "graph", ".", "A", ",", "dtype", "=", "torch", ".", "float32", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "register_buffer", "(", "'A'", ",", "A", ")", "\n", "\n", "# build networks", "\n", "spatial_kernel_size", "=", "A", ".", "size", "(", "0", ")", "\n", "temporal_kernel_size", "=", "9", "\n", "kernel_size", "=", "(", "temporal_kernel_size", ",", "spatial_kernel_size", ")", "\n", "self", ".", "data_bn", "=", "nn", ".", "BatchNorm1d", "(", "in_channels", "*", "\n", "A", ".", "size", "(", "1", ")", ")", "if", "data_bn", "else", "identity", "\n", "\n", "kwargs0", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", "if", "k", "!=", "'dropout'", "}", "\n", "self", ".", "agcn_networks", "=", "nn", ".", "ModuleList", "(", "(", "\n", "AGCNBlock", "(", "\n", "in_channels", ",", "\n", "64", ",", "\n", "kernel_size", ",", "\n", "1", ",", "\n", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "\n", "residual", "=", "False", ",", "\n", "**", "kwargs0", ")", ",", "\n", "AGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "64", ",", "64", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "64", ",", "128", ",", "kernel_size", ",", "2", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "128", ",", "128", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "128", ",", "128", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "128", ",", "256", ",", "kernel_size", ",", "2", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "256", ",", "256", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", "AGCNBlock", "(", "256", ",", "256", ",", "kernel_size", ",", "1", ",", "adj_len", "=", "A", ".", "size", "(", "1", ")", ",", "**", "kwargs", ")", ",", "\n", ")", ")", "\n", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.AGCN.init_weights": [[267, 286], ["isinstance", "utils.get_root_logger", "utils.get_root_logger.info", "mmcv.runner.load_checkpoint", "agcn.AGCN.modules", "TypeError", "isinstance", "mmcv.cnn.kaiming_init", "isinstance", "mmcv.cnn.normal_init", "isinstance", "mmcv.cnn.constant_init"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.models.helpers.load_checkpoint"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters either from existing checkpoint or from\n        scratch.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "pretrained", ",", "str", ")", ":", "\n", "            ", "logger", "=", "get_root_logger", "(", ")", "\n", "logger", ".", "info", "(", "f'load model from: {self.pretrained}'", ")", "\n", "\n", "load_checkpoint", "(", "self", ",", "self", ".", "pretrained", ",", "strict", "=", "False", ",", "logger", "=", "logger", ")", "\n", "\n", "", "elif", "self", ".", "pretrained", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                    ", "kaiming_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                    ", "normal_init", "(", "m", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "_BatchNorm", ")", ":", "\n", "                    ", "constant_init", "(", "m", ",", "1", ")", "\n", "", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'pretrained must be a str or None'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.AGCN.forward": [[287, 309], ["x.view.view.float", "x.view.view.size", "x.view.view.permute().contiguous", "x.view.view.view", "agcn.AGCN.data_bn", "x.view.view.view", "x.view.view.permute().contiguous", "x.view.view.view", "gcn", "x.view.view.permute", "x.view.view.permute"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# data normalization", "\n", "x", "=", "x", ".", "float", "(", ")", "\n", "n", ",", "c", ",", "t", ",", "v", ",", "m", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "4", ",", "3", ",", "1", ",", "2", ")", ".", "contiguous", "(", ")", "# N M V C T", "\n", "x", "=", "x", ".", "view", "(", "n", "*", "m", ",", "v", "*", "c", ",", "t", ")", "\n", "x", "=", "self", ".", "data_bn", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "n", ",", "m", ",", "v", ",", "c", ",", "t", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "4", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "x", "=", "x", ".", "view", "(", "n", "*", "m", ",", "c", ",", "t", ",", "v", ")", "\n", "\n", "for", "gcn", "in", "self", ".", "agcn_networks", ":", "\n", "            ", "x", ",", "_", "=", "gcn", "(", "x", ",", "self", ".", "A", ")", "\n", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.zero": [[12, 15], ["None"], "function", ["None"], ["def", "zero", "(", "x", ")", ":", "\n", "    ", "\"\"\"return zero.\"\"\"", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.backbones.agcn.identity": [[17, 20], ["None"], "function", ["None"], ["", "def", "identity", "(", "x", ")", ":", "\n", "    ", "\"\"\"return input itself.\"\"\"", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.roi_extractors.single_straight3d.SingleRoIExtractor3D.__init__": [[52, 88], ["torch.Module.__init__", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "torch.AdaptiveAvgPool2d", "RoIPool", "RoIAlign"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "roi_layer_type", "=", "'RoIAlign'", ",", "\n", "featmap_stride", "=", "16", ",", "\n", "output_size", "=", "16", ",", "\n", "sampling_ratio", "=", "0", ",", "\n", "pool_mode", "=", "'avg'", ",", "\n", "aligned", "=", "True", ",", "\n", "with_temporal_pool", "=", "True", ",", "\n", "temporal_pool_mode", "=", "'avg'", ",", "\n", "with_global", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "roi_layer_type", "=", "roi_layer_type", "\n", "assert", "self", ".", "roi_layer_type", "in", "[", "'RoIPool'", ",", "'RoIAlign'", "]", "\n", "self", ".", "featmap_stride", "=", "featmap_stride", "\n", "self", ".", "spatial_scale", "=", "1.", "/", "self", ".", "featmap_stride", "\n", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "sampling_ratio", "=", "sampling_ratio", "\n", "self", ".", "pool_mode", "=", "pool_mode", "\n", "self", ".", "aligned", "=", "aligned", "\n", "\n", "self", ".", "with_temporal_pool", "=", "with_temporal_pool", "\n", "self", ".", "temporal_pool_mode", "=", "temporal_pool_mode", "\n", "\n", "self", ".", "with_global", "=", "with_global", "\n", "\n", "if", "self", ".", "roi_layer_type", "==", "'RoIPool'", ":", "\n", "            ", "self", ".", "roi_layer", "=", "RoIPool", "(", "self", ".", "output_size", ",", "self", ".", "spatial_scale", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "roi_layer", "=", "RoIAlign", "(", "\n", "self", ".", "output_size", ",", "\n", "self", ".", "spatial_scale", ",", "\n", "sampling_ratio", "=", "self", ".", "sampling_ratio", ",", "\n", "pool_mode", "=", "self", ".", "pool_mode", ",", "\n", "aligned", "=", "self", ".", "aligned", ")", "\n", "", "self", ".", "global_pool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.roi_extractors.single_straight3d.SingleRoIExtractor3D.init_weights": [[89, 91], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.roi_extractors.single_straight3d.SingleRoIExtractor3D.forward": [[93, 126], ["torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "torch.cat().contiguous", "range", "isinstance", "len", "max", "torch.cat().contiguous.size", "torch.cat().contiguous.size", "torch.cat().contiguous.size", "feat[].contiguous", "single_straight3d.SingleRoIExtractor3D.roi_layer", "roi_feats.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.interpolate().contiguous", "torch.interpolate().contiguous", "torch.interpolate().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "single_straight3d.SingleRoIExtractor3D.global_pool", "rois[].type", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "roi_feat.contiguous.contiguous.contiguous", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "feat[].contiguous.contiguous", "torch.interpolate", "torch.interpolate", "torch.interpolate", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "feat", ",", "rois", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "feat", ",", "tuple", ")", ":", "\n", "            ", "feat", "=", "(", "feat", ",", ")", "\n", "\n", "", "if", "len", "(", "feat", ")", ">=", "2", ":", "\n", "            ", "maxT", "=", "max", "(", "[", "x", ".", "shape", "[", "2", "]", "for", "x", "in", "feat", "]", ")", "\n", "max_shape", "=", "(", "maxT", ",", ")", "+", "feat", "[", "0", "]", ".", "shape", "[", "3", ":", "]", "\n", "# resize each feat to the largest shape (w. nearest)", "\n", "feat", "=", "[", "F", ".", "interpolate", "(", "x", ",", "max_shape", ")", ".", "contiguous", "(", ")", "for", "x", "in", "feat", "]", "\n", "\n", "", "if", "self", ".", "with_temporal_pool", ":", "\n", "            ", "if", "self", ".", "temporal_pool_mode", "==", "'avg'", ":", "\n", "                ", "feat", "=", "[", "torch", ".", "mean", "(", "x", ",", "2", ",", "keepdim", "=", "True", ")", "for", "x", "in", "feat", "]", "\n", "", "elif", "self", ".", "temporal_pool_mode", "==", "'max'", ":", "\n", "                ", "feat", "=", "[", "torch", ".", "max", "(", "x", ",", "2", ",", "keepdim", "=", "True", ")", "[", "0", "]", "for", "x", "in", "feat", "]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "", "feat", "=", "torch", ".", "cat", "(", "feat", ",", "axis", "=", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "roi_feats", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "feat", ".", "size", "(", "2", ")", ")", ":", "\n", "            ", "frame_feat", "=", "feat", "[", ":", ",", ":", ",", "t", "]", ".", "contiguous", "(", ")", "\n", "roi_feat", "=", "self", ".", "roi_layer", "(", "frame_feat", ",", "rois", ")", "\n", "if", "self", ".", "with_global", ":", "\n", "                ", "global_feat", "=", "self", ".", "global_pool", "(", "frame_feat", ".", "contiguous", "(", ")", ")", "\n", "inds", "=", "rois", "[", ":", ",", "0", "]", ".", "type", "(", "torch", ".", "int64", ")", "\n", "global_feat", "=", "global_feat", "[", "inds", "]", "\n", "roi_feat", "=", "torch", ".", "cat", "(", "[", "roi_feat", ",", "global_feat", "]", ",", "dim", "=", "1", ")", "\n", "roi_feat", "=", "roi_feat", ".", "contiguous", "(", ")", "\n", "", "roi_feats", ".", "append", "(", "roi_feat", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "roi_feats", ",", "dim", "=", "2", ")", ",", "feat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.base.BaseWeightedLoss.__init__": [[18, 21], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["\n", "class", "BaseDataset", "(", "Dataset", ",", "metaclass", "=", "ABCMeta", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.base.BaseWeightedLoss._forward": [[22, 25], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.base.BaseWeightedLoss.forward": [[26, 46], ["base.BaseWeightedLoss._forward", "isinstance"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.forward": [[13, 53], ["pred.size", "torch.zeros", "torch.zeros", "range", "losses.view().contiguous.view().contiguous.view().contiguous", "torch.sort", "int", "torch.zeros", "range", "pred.size", "losses.view().contiguous.view().contiguous.size", "len", "ValueError", "max", "losses.view().contiguous.view().contiguous.size", "sorted_losses[].sum", "losses.view().contiguous.view().contiguous.view", "len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "pred", ",", "labels", ",", "is_positive", ",", "ohem_ratio", ",", "group_size", ")", ":", "\n", "        ", "\"\"\"Calculate OHEM hinge loss.\n\n        Args:\n            pred (torch.Tensor): Predicted completeness score.\n            labels (torch.Tensor): Groundtruth class label.\n            is_positive (int): Set to 1 when proposals are positive and\n                set to -1 when proposals are incomplete.\n            ohem_ratio (float): Ratio of hard examples.\n            group_size (int): Number of proposals sampled per video.\n\n        Returns:\n            torch.Tensor: Returned class-wise hinge loss.\n        \"\"\"", "\n", "num_samples", "=", "pred", ".", "size", "(", "0", ")", "\n", "if", "num_samples", "!=", "len", "(", "labels", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f'Number of samples should be equal to that '", "\n", "f'of labels, but got {num_samples} samples and '", "\n", "f'{len(labels)} labels.'", ")", "\n", "\n", "", "losses", "=", "torch", ".", "zeros", "(", "num_samples", ",", "device", "=", "pred", ".", "device", ")", "\n", "slopes", "=", "torch", ".", "zeros", "(", "num_samples", ",", "device", "=", "pred", ".", "device", ")", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "losses", "[", "i", "]", "=", "max", "(", "0", ",", "1", "-", "is_positive", "*", "pred", "[", "i", ",", "labels", "[", "i", "]", "-", "1", "]", ")", "\n", "slopes", "[", "i", "]", "=", "-", "is_positive", "if", "losses", "[", "i", "]", "!=", "0", "else", "0", "\n", "\n", "", "losses", "=", "losses", ".", "view", "(", "-", "1", ",", "group_size", ")", ".", "contiguous", "(", ")", "\n", "sorted_losses", ",", "indices", "=", "torch", ".", "sort", "(", "losses", ",", "dim", "=", "1", ",", "descending", "=", "True", ")", "\n", "keep_length", "=", "int", "(", "group_size", "*", "ohem_ratio", ")", "\n", "loss", "=", "torch", ".", "zeros", "(", "1", ",", "device", "=", "pred", ".", "device", ")", "\n", "for", "i", "in", "range", "(", "losses", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "loss", "+=", "sorted_losses", "[", "i", ",", ":", "keep_length", "]", ".", "sum", "(", ")", "\n", "", "ctx", ".", "loss_index", "=", "indices", "[", ":", ",", ":", "keep_length", "]", "\n", "ctx", ".", "labels", "=", "labels", "\n", "ctx", ".", "slopes", "=", "slopes", "\n", "ctx", ".", "shape", "=", "pred", ".", "size", "(", ")", "\n", "ctx", ".", "group_size", "=", "group_size", "\n", "ctx", ".", "num_groups", "=", "losses", ".", "size", "(", "0", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ohem_hinge_loss.OHEMHingeLoss.backward": [[54, 66], ["torch.zeros", "range", "torch.autograd.Variable"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "labels", "=", "ctx", ".", "labels", "\n", "slopes", "=", "ctx", ".", "slopes", "\n", "\n", "grad_in", "=", "torch", ".", "zeros", "(", "ctx", ".", "shape", ",", "device", "=", "ctx", ".", "slopes", ".", "device", ")", "\n", "for", "group", "in", "range", "(", "ctx", ".", "num_groups", ")", ":", "\n", "            ", "for", "idx", "in", "ctx", ".", "loss_index", "[", "group", "]", ":", "\n", "                ", "loc", "=", "idx", "+", "group", "*", "ctx", ".", "group_size", "\n", "grad_in", "[", "loc", ",", "labels", "[", "loc", "]", "-", "1", "]", "=", "(", "\n", "slopes", "[", "loc", "]", "*", "grad_output", ".", "data", "[", "0", "]", ")", "\n", "", "", "return", "torch", ".", "autograd", ".", "Variable", "(", "grad_in", ")", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.activity_loss": [[13, 30], ["torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "activity_loss", "(", "activity_score", ",", "labels", ",", "activity_indexer", ")", ":", "\n", "        ", "\"\"\"Activity Loss.\n\n        It will calculate activity loss given activity_score and label.\n\n        Args\uff1a\n            activity_score (torch.Tensor): Predicted activity score.\n            labels (torch.Tensor): Groundtruth class label.\n            activity_indexer (torch.Tensor): Index slices of proposals.\n\n        Returns:\n            torch.Tensor: Returned cross entropy loss.\n        \"\"\"", "\n", "pred", "=", "activity_score", "[", "activity_indexer", ",", ":", "]", "\n", "gt", "=", "labels", "[", "activity_indexer", "]", "\n", "return", "F", ".", "cross_entropy", "(", "pred", ",", "gt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.completeness_loss": [[31, 81], ["pred.view.view.size", "pred.view.view.view", "gt.view.view.view", "pred[].contiguous().view", "pred[].contiguous().view", "ohem_hinge_loss.OHEMHingeLoss.apply", "ohem_hinge_loss.OHEMHingeLoss.apply", "pred[].contiguous().view.size", "int", "gt[].contiguous().view", "gt[].contiguous().view", "float", "pred[].contiguous", "pred[].contiguous", "pred[].contiguous().view.size", "gt[].contiguous", "gt[].contiguous"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "completeness_loss", "(", "completeness_score", ",", "\n", "labels", ",", "\n", "completeness_indexer", ",", "\n", "positive_per_video", ",", "\n", "incomplete_per_video", ",", "\n", "ohem_ratio", "=", "0.17", ")", ":", "\n", "        ", "\"\"\"Completeness Loss.\n\n        It will calculate completeness loss given completeness_score and label.\n\n        Args\uff1a\n            completeness_score (torch.Tensor): Predicted completeness score.\n            labels (torch.Tensor): Groundtruth class label.\n            completeness_indexer (torch.Tensor): Index slices of positive and\n                incomplete proposals.\n            positive_per_video (int): Number of positive proposals sampled\n                per video.\n            incomplete_per_video (int): Number of incomplete proposals sampled\n                pre video.\n            ohem_ratio (float): Ratio of online hard example mining.\n                Default: 0.17.\n\n        Returns:\n            torch.Tensor: Returned class-wise completeness loss.\n        \"\"\"", "\n", "pred", "=", "completeness_score", "[", "completeness_indexer", ",", ":", "]", "\n", "gt", "=", "labels", "[", "completeness_indexer", "]", "\n", "\n", "pred_dim", "=", "pred", ".", "size", "(", "1", ")", "\n", "pred", "=", "pred", ".", "view", "(", "-", "1", ",", "positive_per_video", "+", "incomplete_per_video", ",", "\n", "pred_dim", ")", "\n", "gt", "=", "gt", ".", "view", "(", "-", "1", ",", "positive_per_video", "+", "incomplete_per_video", ")", "\n", "\n", "# yapf:disable", "\n", "positive_pred", "=", "pred", "[", ":", ",", ":", "positive_per_video", ",", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "pred_dim", ")", "# noqa:E501", "\n", "incomplete_pred", "=", "pred", "[", ":", ",", "positive_per_video", ":", ",", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "pred_dim", ")", "# noqa:E501", "\n", "# yapf:enable", "\n", "\n", "positive_loss", "=", "OHEMHingeLoss", ".", "apply", "(", "\n", "positive_pred", ",", "gt", "[", ":", ",", ":", "positive_per_video", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ",", "1", ",", "\n", "1.0", ",", "positive_per_video", ")", "\n", "incomplete_loss", "=", "OHEMHingeLoss", ".", "apply", "(", "\n", "incomplete_pred", ",", "gt", "[", ":", ",", "positive_per_video", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", ",", "\n", "-", "1", ",", "ohem_ratio", ",", "incomplete_per_video", ")", "\n", "num_positives", "=", "positive_pred", ".", "size", "(", "0", ")", "\n", "num_incompletes", "=", "int", "(", "incomplete_pred", ".", "size", "(", "0", ")", "*", "ohem_ratio", ")", "\n", "\n", "return", "(", "(", "positive_loss", "+", "incomplete_loss", ")", "/", "\n", "float", "(", "num_positives", "+", "num_incompletes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.classwise_regression_loss": [[82, 115], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.diag().view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "reg_target.view", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.diag"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "classwise_regression_loss", "(", "bbox_pred", ",", "labels", ",", "bbox_targets", ",", "\n", "regression_indexer", ")", ":", "\n", "        ", "\"\"\"Classwise Regression Loss.\n\n        It will calculate classwise_regression loss given\n        class_reg_pred and targets.\n\n        Args\uff1a\n            bbox_pred (torch.Tensor): Predicted interval center and span\n                of positive proposals.\n            labels (torch.Tensor): Groundtruth class label.\n            bbox_targets (torch.Tensor): Groundtruth center and span\n                of positive proposals.\n            regression_indexer (torch.Tensor): Index slices of\n                positive proposals.\n\n        Returns:\n            torch.Tensor: Returned class-wise regression loss.\n        \"\"\"", "\n", "pred", "=", "bbox_pred", "[", "regression_indexer", ",", ":", ",", ":", "]", "\n", "gt", "=", "labels", "[", "regression_indexer", "]", "\n", "reg_target", "=", "bbox_targets", "[", "regression_indexer", ",", ":", "]", "\n", "\n", "class_idx", "=", "gt", ".", "data", "-", "1", "\n", "classwise_pred", "=", "pred", "[", ":", ",", "class_idx", ",", ":", "]", "\n", "classwise_reg_pred", "=", "torch", ".", "cat", "(", "\n", "(", "torch", ".", "diag", "(", "classwise_pred", "[", ":", ",", ":", ",", "0", "]", ")", ".", "view", "(", "\n", "-", "1", ",", "1", ")", ",", "torch", ".", "diag", "(", "classwise_pred", "[", ":", ",", ":", ",", "1", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", ",", "\n", "dim", "=", "1", ")", "\n", "loss", "=", "F", ".", "smooth_l1_loss", "(", "\n", "classwise_reg_pred", ".", "view", "(", "-", "1", ")", ",", "reg_target", ".", "view", "(", "-", "1", ")", ")", "*", "2", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.forward": [[116, 181], ["dict", "proposal_type.view.view.view", "labels.view.view.view", "int", "int", "ssn_loss.SSNLoss.activity_loss", "ssn_loss.SSNLoss.completeness_loss", "bbox_targets.view.view.view", "ssn_loss.SSNLoss.classwise_regression_loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.activity_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.completeness_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.ssn_loss.SSNLoss.classwise_regression_loss"], ["", "def", "forward", "(", "self", ",", "activity_score", ",", "completeness_score", ",", "bbox_pred", ",", "\n", "proposal_type", ",", "labels", ",", "bbox_targets", ",", "train_cfg", ")", ":", "\n", "        ", "\"\"\"Calculate Boundary Matching Network Loss.\n\n        Args:\n            activity_score (torch.Tensor): Predicted activity score.\n            completeness_score (torch.Tensor): Predicted completeness score.\n            bbox_pred (torch.Tensor): Predicted interval center and span\n                of positive proposals.\n            proposal_type (torch.Tensor): Type index slices of proposals.\n            labels (torch.Tensor): Groundtruth class label.\n            bbox_targets (torch.Tensor): Groundtruth center and span\n                of positive proposals.\n            train_cfg (dict): Config for training.\n\n        Returns:\n            dict([torch.Tensor, torch.Tensor, torch.Tensor]):\n                (loss_activity, loss_completeness, loss_reg).\n                Loss_activity is the activity loss, loss_completeness is\n                the class-wise completeness loss,\n                loss_reg is the class-wise regression loss.\n        \"\"\"", "\n", "self", ".", "sampler", "=", "train_cfg", ".", "ssn", ".", "sampler", "\n", "self", ".", "loss_weight", "=", "train_cfg", ".", "ssn", ".", "loss_weight", "\n", "losses", "=", "dict", "(", ")", "\n", "\n", "proposal_type", "=", "proposal_type", ".", "view", "(", "-", "1", ")", "\n", "labels", "=", "labels", ".", "view", "(", "-", "1", ")", "\n", "activity_indexer", "=", "(", "(", "proposal_type", "==", "0", ")", "+", "\n", "(", "proposal_type", "==", "2", ")", ")", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", "\n", "completeness_indexer", "=", "(", "(", "proposal_type", "==", "0", ")", "+", "\n", "(", "proposal_type", "==", "1", ")", ")", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "total_ratio", "=", "(", "\n", "self", ".", "sampler", ".", "positive_ratio", "+", "self", ".", "sampler", ".", "background_ratio", "+", "\n", "self", ".", "sampler", ".", "incomplete_ratio", ")", "\n", "positive_per_video", "=", "int", "(", "self", ".", "sampler", ".", "num_per_video", "*", "\n", "(", "self", ".", "sampler", ".", "positive_ratio", "/", "total_ratio", ")", ")", "\n", "background_per_video", "=", "int", "(", "\n", "self", ".", "sampler", ".", "num_per_video", "*", "\n", "(", "self", ".", "sampler", ".", "background_ratio", "/", "total_ratio", ")", ")", "\n", "incomplete_per_video", "=", "(", "\n", "self", ".", "sampler", ".", "num_per_video", "-", "positive_per_video", "-", "\n", "background_per_video", ")", "\n", "\n", "losses", "[", "'loss_activity'", "]", "=", "self", ".", "activity_loss", "(", "activity_score", ",", "labels", ",", "\n", "activity_indexer", ")", "\n", "\n", "losses", "[", "'loss_completeness'", "]", "=", "self", ".", "completeness_loss", "(", "\n", "completeness_score", ",", "\n", "labels", ",", "\n", "completeness_indexer", ",", "\n", "positive_per_video", ",", "\n", "incomplete_per_video", ",", "\n", "ohem_ratio", "=", "positive_per_video", "/", "incomplete_per_video", ")", "\n", "losses", "[", "'loss_completeness'", "]", "*=", "self", ".", "loss_weight", ".", "comp_loss_weight", "\n", "\n", "if", "bbox_pred", "is", "not", "None", ":", "\n", "            ", "regression_indexer", "=", "(", "proposal_type", "==", "0", ")", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", "\n", "bbox_targets", "=", "bbox_targets", ".", "view", "(", "-", "1", ",", "2", ")", "\n", "losses", "[", "'loss_reg'", "]", "=", "self", ".", "classwise_regression_loss", "(", "\n", "bbox_pred", ",", "labels", ",", "bbox_targets", ",", "regression_indexer", ")", "\n", "losses", "[", "'loss_reg'", "]", "*=", "self", ".", "loss_weight", ".", "reg_loss_weight", "\n", "\n", "", "return", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.binary_logistic_regression_loss.BinaryLogisticRegressionLoss.forward": [[40, 63], ["binary_logistic_regression_loss.binary_logistic_regression_loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.binary_logistic_regression_loss.binary_logistic_regression_loss"], ["def", "forward", "(", "self", ",", "\n", "reg_score", ",", "\n", "label", ",", "\n", "threshold", "=", "0.5", ",", "\n", "ratio_range", "=", "(", "1.05", ",", "21", ")", ",", "\n", "eps", "=", "1e-5", ")", ":", "\n", "        ", "\"\"\"Calculate Binary Logistic Regression Loss.\n\n        Args:\n                reg_score (torch.Tensor): Predicted score by model.\n                label (torch.Tensor): Groundtruth labels.\n                threshold (float): Threshold for positive instances.\n                    Default: 0.5.\n                ratio_range (tuple): Lower bound and upper bound for ratio.\n                    Default: (1.05, 21)\n                eps (float): Epsilon for small value. Default: 1e-5.\n\n        Returns:\n                torch.Tensor: Returned binary logistic loss.\n        \"\"\"", "\n", "\n", "return", "binary_logistic_regression_loss", "(", "reg_score", ",", "label", ",", "threshold", ",", "\n", "ratio_range", ",", "eps", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.binary_logistic_regression_loss.binary_logistic_regression_loss": [[8, 30], ["label.view().to.view().to", "reg_score.contiguous().view.contiguous().view", "max", "len", "min", "torch.sum", "torch.sum", "max", "torch.mean", "torch.mean", "label.view().to.view", "reg_score.contiguous().view.contiguous", "torch.log", "torch.log", "torch.log", "torch.log"], "function", ["None"], ["def", "binary_logistic_regression_loss", "(", "reg_score", ",", "\n", "label", ",", "\n", "threshold", "=", "0.5", ",", "\n", "ratio_range", "=", "(", "1.05", ",", "21", ")", ",", "\n", "eps", "=", "1e-5", ")", ":", "\n", "    ", "\"\"\"Binary Logistic Regression Loss.\"\"\"", "\n", "label", "=", "label", ".", "view", "(", "-", "1", ")", ".", "to", "(", "reg_score", ".", "device", ")", "\n", "reg_score", "=", "reg_score", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "pmask", "=", "(", "label", ">", "threshold", ")", ".", "float", "(", ")", ".", "to", "(", "reg_score", ".", "device", ")", "\n", "num_positive", "=", "max", "(", "torch", ".", "sum", "(", "pmask", ")", ",", "1", ")", "\n", "num_entries", "=", "len", "(", "label", ")", "\n", "ratio", "=", "num_entries", "/", "num_positive", "\n", "# clip ratio value between ratio_range", "\n", "ratio", "=", "min", "(", "max", "(", "ratio", ",", "ratio_range", "[", "0", "]", ")", ",", "ratio_range", "[", "1", "]", ")", "\n", "\n", "coef_0", "=", "0.5", "*", "ratio", "/", "(", "ratio", "-", "1", ")", "\n", "coef_1", "=", "0.5", "*", "ratio", "\n", "loss", "=", "coef_1", "*", "pmask", "*", "torch", ".", "log", "(", "reg_score", "+", "eps", ")", "+", "coef_0", "*", "(", "\n", "1.0", "-", "pmask", ")", "*", "torch", ".", "log", "(", "1.0", "-", "reg_score", "+", "eps", ")", "\n", "loss", "=", "-", "torch", ".", "mean", "(", "loss", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.hvu_loss.HVULoss.__init__": [[35, 61], ["base.BaseWeightedLoss.__init__", "range", "len", "len", "hvu_loss.HVULoss.category_startidx.append", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "categories", "=", "(", "'action'", ",", "'attribute'", ",", "'concept'", ",", "'event'", ",", "\n", "'object'", ",", "'scene'", ")", ",", "\n", "category_nums", "=", "(", "739", ",", "117", ",", "291", ",", "69", ",", "1678", ",", "248", ")", ",", "\n", "category_loss_weights", "=", "(", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ")", ",", "\n", "loss_type", "=", "'all'", ",", "\n", "with_mask", "=", "False", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "loss_weight", "=", "1.0", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "loss_weight", ")", "\n", "self", ".", "categories", "=", "categories", "\n", "self", ".", "category_nums", "=", "category_nums", "\n", "self", ".", "category_loss_weights", "=", "category_loss_weights", "\n", "assert", "len", "(", "self", ".", "category_nums", ")", "==", "len", "(", "self", ".", "category_loss_weights", ")", "\n", "for", "category_loss_weight", "in", "self", ".", "category_loss_weights", ":", "\n", "            ", "assert", "category_loss_weight", ">=", "0", "\n", "", "self", ".", "loss_type", "=", "loss_type", "\n", "self", ".", "with_mask", "=", "with_mask", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "category_startidx", "=", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "category_nums", ")", "-", "1", ")", ":", "\n", "            ", "self", ".", "category_startidx", ".", "append", "(", "self", ".", "category_startidx", "[", "-", "1", "]", "+", "\n", "self", ".", "category_nums", "[", "i", "]", ")", "\n", "", "assert", "self", ".", "loss_type", "in", "[", "'individual'", ",", "'all'", "]", "\n", "assert", "self", ".", "reduction", "in", "[", "'mean'", ",", "'sum'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.hvu_loss.HVULoss._forward": [[62, 142], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "dict", "zip", "sum", "sum", "losses.update", "ValueError", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "dict", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "hvu_loss.HVULoss.categories.index", "loss_weights.values", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "category_mask[].reshape", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "loss_weights.items", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "loss_weights.items", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "_forward", "(", "self", ",", "cls_score", ",", "label", ",", "mask", ",", "category_mask", ")", ":", "\n", "        ", "\"\"\"Forward function.\n\n        Args:\n            cls_score (torch.Tensor): The class score.\n            label (torch.Tensor): The ground truth label.\n            mask (torch.Tensor): The mask of tags. 0 indicates that the\n                category of this tag is missing in the label of the video.\n            category_mask (torch.Tensor): The category mask. For each sample,\n                it's a tensor with length `len(self.categories)`, denotes that\n                if the category is labeled for this video.\n\n        Returns:\n            torch.Tensor: The returned CrossEntropy loss.\n        \"\"\"", "\n", "\n", "if", "self", ".", "loss_type", "==", "'all'", ":", "\n", "            ", "loss_cls", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "cls_score", ",", "label", ",", "reduction", "=", "'none'", ")", "\n", "if", "self", ".", "with_mask", ":", "\n", "                ", "w_loss_cls", "=", "mask", "*", "loss_cls", "\n", "w_loss_cls", "=", "torch", ".", "sum", "(", "w_loss_cls", ",", "dim", "=", "1", ")", "\n", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "                    ", "w_loss_cls", "=", "w_loss_cls", "/", "torch", ".", "sum", "(", "mask", ",", "dim", "=", "1", ")", "\n", "", "w_loss_cls", "=", "torch", ".", "mean", "(", "w_loss_cls", ")", "\n", "return", "dict", "(", "loss_cls", "=", "w_loss_cls", ")", "\n", "\n", "", "if", "self", ".", "reduction", "==", "'sum'", ":", "\n", "                ", "loss_cls", "=", "torch", ".", "sum", "(", "loss_cls", ",", "dim", "=", "-", "1", ")", "\n", "", "return", "dict", "(", "loss_cls", "=", "torch", ".", "mean", "(", "loss_cls", ")", ")", "\n", "\n", "", "if", "self", ".", "loss_type", "==", "'individual'", ":", "\n", "            ", "losses", "=", "{", "}", "\n", "loss_weights", "=", "{", "}", "\n", "for", "name", ",", "num", ",", "start_idx", "in", "zip", "(", "self", ".", "categories", ",", "\n", "self", ".", "category_nums", ",", "\n", "self", ".", "category_startidx", ")", ":", "\n", "                ", "category_score", "=", "cls_score", "[", ":", ",", "start_idx", ":", "start_idx", "+", "num", "]", "\n", "category_label", "=", "label", "[", ":", ",", "start_idx", ":", "start_idx", "+", "num", "]", "\n", "category_loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "category_score", ",", "category_label", ",", "reduction", "=", "'none'", ")", "\n", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "                    ", "category_loss", "=", "torch", ".", "mean", "(", "category_loss", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "reduction", "==", "'sum'", ":", "\n", "                    ", "category_loss", "=", "torch", ".", "sum", "(", "category_loss", ",", "dim", "=", "1", ")", "\n", "\n", "", "idx", "=", "self", ".", "categories", ".", "index", "(", "name", ")", "\n", "if", "self", ".", "with_mask", ":", "\n", "                    ", "category_mask_i", "=", "category_mask", "[", ":", ",", "idx", "]", ".", "reshape", "(", "-", "1", ")", "\n", "# there should be at least one sample which contains tags", "\n", "# in this category", "\n", "if", "torch", ".", "sum", "(", "category_mask_i", ")", "<", "0.5", ":", "\n", "                        ", "losses", "[", "f'{name}_LOSS'", "]", "=", "torch", ".", "tensor", "(", ".0", ")", ".", "cuda", "(", ")", "\n", "loss_weights", "[", "f'{name}_LOSS'", "]", "=", ".0", "\n", "continue", "\n", "", "category_loss", "=", "torch", ".", "sum", "(", "category_loss", "*", "category_mask_i", ")", "\n", "category_loss", "=", "category_loss", "/", "torch", ".", "sum", "(", "category_mask_i", ")", "\n", "", "else", ":", "\n", "                    ", "category_loss", "=", "torch", ".", "mean", "(", "category_loss", ")", "\n", "# We name the loss of each category as 'LOSS', since we only", "\n", "# want to monitor them, not backward them. We will also provide", "\n", "# the loss used for backward in the losses dictionary", "\n", "", "losses", "[", "f'{name}_LOSS'", "]", "=", "category_loss", "\n", "loss_weights", "[", "f'{name}_LOSS'", "]", "=", "self", ".", "category_loss_weights", "[", "idx", "]", "\n", "", "loss_weight_sum", "=", "sum", "(", "loss_weights", ".", "values", "(", ")", ")", "\n", "loss_weights", "=", "{", "\n", "k", ":", "v", "/", "loss_weight_sum", "\n", "for", "k", ",", "v", "in", "loss_weights", ".", "items", "(", ")", "\n", "}", "\n", "loss_cls", "=", "sum", "(", "[", "losses", "[", "k", "]", "*", "loss_weights", "[", "k", "]", "for", "k", "in", "losses", "]", ")", "\n", "losses", "[", "'loss_cls'", "]", "=", "loss_cls", "\n", "# We also trace the loss weights", "\n", "losses", ".", "update", "(", "{", "\n", "k", "+", "'_weight'", ":", "torch", ".", "tensor", "(", "v", ")", ".", "to", "(", "losses", "[", "k", "]", ".", "device", ")", "\n", "for", "k", ",", "v", "in", "loss_weights", ".", "items", "(", ")", "\n", "}", ")", "\n", "# Note that the loss weights are just for reference.", "\n", "return", "losses", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"loss_type should be 'all' or 'individual', \"", "\n", "f'but got {self.loss_type}'", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.nll_loss.NLLLoss._forward": [[15, 28], ["torch.nll_loss"], "methods", ["None"], ["def", "_forward", "(", "self", ",", "cls_score", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Forward function.\n\n        Args:\n            cls_score (torch.Tensor): The class score.\n            label (torch.Tensor): The ground truth label.\n            kwargs: Any keyword argument to be used to calculate nll loss.\n\n        Returns:\n            torch.Tensor: The returned nll loss.\n        \"\"\"", "\n", "loss_cls", "=", "F", ".", "nll_loss", "(", "cls_score", ",", "label", ",", "**", "kwargs", ")", "\n", "return", "loss_cls", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.tem_loss": [[26, 46], ["binary_logistic_regression_loss.binary_logistic_regression_loss.binary_logistic_regression_loss", "binary_logistic_regression_loss.binary_logistic_regression_loss.binary_logistic_regression_loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.binary_logistic_regression_loss.binary_logistic_regression_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.binary_logistic_regression_loss.binary_logistic_regression_loss"], ["@", "staticmethod", "\n", "def", "tem_loss", "(", "pred_start", ",", "pred_end", ",", "gt_start", ",", "gt_end", ")", ":", "\n", "        ", "\"\"\"Calculate Temporal Evaluation Module Loss.\n\n        This function calculate the binary_logistic_regression_loss for start\n        and end respectively and returns the sum of their losses.\n\n        Args:\n            pred_start (torch.Tensor): Predicted start score by BMN model.\n            pred_end (torch.Tensor): Predicted end score by BMN model.\n            gt_start (torch.Tensor): Groundtruth confidence score for start.\n            gt_end (torch.Tensor): Groundtruth confidence score for end.\n\n        Returns:\n            torch.Tensor: Returned binary logistic loss.\n        \"\"\"", "\n", "loss_start", "=", "binary_logistic_regression_loss", "(", "pred_start", ",", "gt_start", ")", "\n", "loss_end", "=", "binary_logistic_regression_loss", "(", "pred_end", ",", "gt_end", ")", "\n", "loss", "=", "loss_start", "+", "loss_end", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.pem_reg_loss": [[47, 95], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.rand_like", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pem_reg_loss", "(", "pred_score", ",", "\n", "gt_iou_map", ",", "\n", "mask", ",", "\n", "high_temporal_iou_threshold", "=", "0.7", ",", "\n", "low_temporal_iou_threshold", "=", "0.3", ")", ":", "\n", "        ", "\"\"\"Calculate Proposal Evaluation Module Regression Loss.\n\n        Args:\n            pred_score (torch.Tensor): Predicted temporal_iou score by BMN.\n            gt_iou_map (torch.Tensor): Groundtruth temporal_iou score.\n            mask (torch.Tensor): Boundary-Matching mask.\n            high_temporal_iou_threshold (float): Higher threshold of\n                temporal_iou. Default: 0.7.\n            low_temporal_iou_threshold (float): Higher threshold of\n                temporal_iou. Default: 0.3.\n\n        Returns:\n            torch.Tensor: Proposal evaluation regression loss.\n        \"\"\"", "\n", "u_hmask", "=", "(", "gt_iou_map", ">", "high_temporal_iou_threshold", ")", ".", "float", "(", ")", "\n", "u_mmask", "=", "(", "(", "gt_iou_map", "<=", "high_temporal_iou_threshold", ")", "&", "\n", "(", "gt_iou_map", ">", "low_temporal_iou_threshold", ")", ")", ".", "float", "(", ")", "\n", "u_lmask", "=", "(", "(", "gt_iou_map", "<=", "low_temporal_iou_threshold", ")", "&", "\n", "(", "gt_iou_map", ">", "0.", ")", ")", ".", "float", "(", ")", "\n", "u_lmask", "=", "u_lmask", "*", "mask", "\n", "\n", "num_h", "=", "torch", ".", "sum", "(", "u_hmask", ")", "\n", "num_m", "=", "torch", ".", "sum", "(", "u_mmask", ")", "\n", "num_l", "=", "torch", ".", "sum", "(", "u_lmask", ")", "\n", "\n", "r_m", "=", "num_h", "/", "num_m", "\n", "u_smmask", "=", "torch", ".", "rand_like", "(", "gt_iou_map", ")", "\n", "u_smmask", "=", "u_mmask", "*", "u_smmask", "\n", "u_smmask", "=", "(", "u_smmask", ">", "(", "1.", "-", "r_m", ")", ")", ".", "float", "(", ")", "\n", "\n", "r_l", "=", "num_h", "/", "num_l", "\n", "u_slmask", "=", "torch", ".", "rand_like", "(", "gt_iou_map", ")", "\n", "u_slmask", "=", "u_lmask", "*", "u_slmask", "\n", "u_slmask", "=", "(", "u_slmask", ">", "(", "1.", "-", "r_l", ")", ")", ".", "float", "(", ")", "\n", "\n", "weights", "=", "u_hmask", "+", "u_smmask", "+", "u_slmask", "\n", "\n", "loss", "=", "F", ".", "mse_loss", "(", "pred_score", "*", "weights", ",", "gt_iou_map", "*", "weights", ")", "\n", "loss", "=", "0.5", "*", "torch", ".", "sum", "(", "\n", "loss", "*", "torch", ".", "ones_like", "(", "weights", ")", ")", "/", "torch", ".", "sum", "(", "weights", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.pem_cls_loss": [[96, 134], ["max", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "pem_cls_loss", "(", "pred_score", ",", "\n", "gt_iou_map", ",", "\n", "mask", ",", "\n", "threshold", "=", "0.9", ",", "\n", "ratio_range", "=", "(", "1.05", ",", "21", ")", ",", "\n", "eps", "=", "1e-5", ")", ":", "\n", "        ", "\"\"\"Calculate Proposal Evaluation Module Classification Loss.\n\n        Args:\n            pred_score (torch.Tensor): Predicted temporal_iou score by BMN.\n            gt_iou_map (torch.Tensor): Groundtruth temporal_iou score.\n            mask (torch.Tensor): Boundary-Matching mask.\n            threshold (float): Threshold of temporal_iou for positive\n                instances. Default: 0.9.\n            ratio_range (tuple): Lower bound and upper bound for ratio.\n                Default: (1.05, 21)\n            eps (float): Epsilon for small value. Default: 1e-5\n\n        Returns:\n            torch.Tensor: Proposal evaluation classification loss.\n        \"\"\"", "\n", "pmask", "=", "(", "gt_iou_map", ">", "threshold", ")", ".", "float", "(", ")", "\n", "nmask", "=", "(", "gt_iou_map", "<=", "threshold", ")", ".", "float", "(", ")", "\n", "nmask", "=", "nmask", "*", "mask", "\n", "\n", "num_positive", "=", "max", "(", "torch", ".", "sum", "(", "pmask", ")", ",", "1", ")", "\n", "num_entries", "=", "num_positive", "+", "torch", ".", "sum", "(", "nmask", ")", "\n", "ratio", "=", "num_entries", "/", "num_positive", "\n", "ratio", "=", "torch", ".", "clamp", "(", "ratio", ",", "ratio_range", "[", "0", "]", ",", "ratio_range", "[", "1", "]", ")", "\n", "\n", "coef_0", "=", "0.5", "*", "ratio", "/", "(", "ratio", "-", "1", ")", "\n", "coef_1", "=", "0.5", "*", "ratio", "\n", "\n", "loss_pos", "=", "coef_1", "*", "torch", ".", "log", "(", "pred_score", "+", "eps", ")", "*", "pmask", "\n", "loss_neg", "=", "coef_0", "*", "torch", ".", "log", "(", "1.0", "-", "pred_score", "+", "eps", ")", "*", "nmask", "\n", "loss", "=", "-", "1", "*", "torch", ".", "sum", "(", "loss_pos", "+", "loss_neg", ")", "/", "num_entries", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.forward": [[135, 182], ["pred_bm[].contiguous", "pred_bm[].contiguous", "bmn_loss.BMNLoss.pem_reg_loss", "bmn_loss.BMNLoss.pem_cls_loss", "bmn_loss.BMNLoss.tem_loss"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.pem_reg_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.pem_cls_loss", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.bmn_loss.BMNLoss.tem_loss"], ["", "def", "forward", "(", "self", ",", "\n", "pred_bm", ",", "\n", "pred_start", ",", "\n", "pred_end", ",", "\n", "gt_iou_map", ",", "\n", "gt_start", ",", "\n", "gt_end", ",", "\n", "bm_mask", ",", "\n", "weight_tem", "=", "1.0", ",", "\n", "weight_pem_reg", "=", "10.0", ",", "\n", "weight_pem_cls", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"Calculate Boundary Matching Network Loss.\n\n        Args:\n            pred_bm (torch.Tensor): Predicted confidence score for boundary\n                matching map.\n            pred_start (torch.Tensor): Predicted confidence score for start.\n            pred_end (torch.Tensor): Predicted confidence score for end.\n            gt_iou_map (torch.Tensor): Groundtruth score for boundary matching\n                map.\n            gt_start (torch.Tensor): Groundtruth temporal_iou score for start.\n            gt_end (torch.Tensor): Groundtruth temporal_iou score for end.\n            bm_mask (torch.Tensor): Boundary-Matching mask.\n            weight_tem (float): Weight for tem loss. Default: 1.0.\n            weight_pem_reg (float): Weight for pem regression loss.\n                Default: 10.0.\n            weight_pem_cls (float): Weight for pem classification loss.\n                Default: 1.0.\n\n        Returns:\n            tuple([torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]):\n                (loss, tem_loss, pem_reg_loss, pem_cls_loss). Loss is the bmn\n                loss, tem_loss is the temporal evaluation loss, pem_reg_loss is\n                the proposal evaluation regression loss, pem_cls_loss is the\n                proposal evaluation classification loss.\n        \"\"\"", "\n", "pred_bm_reg", "=", "pred_bm", "[", ":", ",", "0", "]", ".", "contiguous", "(", ")", "\n", "pred_bm_cls", "=", "pred_bm", "[", ":", ",", "1", "]", ".", "contiguous", "(", ")", "\n", "gt_iou_map", "=", "gt_iou_map", "*", "bm_mask", "\n", "\n", "pem_reg_loss", "=", "self", ".", "pem_reg_loss", "(", "pred_bm_reg", ",", "gt_iou_map", ",", "bm_mask", ")", "\n", "pem_cls_loss", "=", "self", ".", "pem_cls_loss", "(", "pred_bm_cls", ",", "gt_iou_map", ",", "bm_mask", ")", "\n", "tem_loss", "=", "self", ".", "tem_loss", "(", "pred_start", ",", "pred_end", ",", "gt_start", ",", "gt_end", ")", "\n", "loss", "=", "(", "\n", "weight_tem", "*", "tem_loss", "+", "weight_pem_reg", "*", "pem_reg_loss", "+", "\n", "weight_pem_cls", "*", "pem_cls_loss", ")", "\n", "return", "loss", ",", "tem_loss", ",", "pem_reg_loss", ",", "pem_cls_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.CrossEntropyLoss.__init__": [[33, 38], ["base.BaseWeightedLoss.__init__", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "loss_weight", "=", "1.0", ",", "class_weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "loss_weight", "=", "loss_weight", ")", "\n", "self", ".", "class_weight", "=", "None", "\n", "if", "class_weight", "is", "not", "None", ":", "\n", "            ", "self", ".", "class_weight", "=", "torch", ".", "Tensor", "(", "class_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.CrossEntropyLoss._forward": [[39, 83], ["cls_score.size", "label.size", "torch.log_softmax", "torch.log_softmax", "torch.cross_entropy", "torch.cross_entropy", "cls_score.dim", "len", "cross_entropy_loss.CrossEntropyLoss.class_weight.to", "loss_cls.mean.mean.mean", "cross_entropy_loss.CrossEntropyLoss.class_weight.to", "cross_entropy_loss.CrossEntropyLoss.class_weight.unsqueeze", "loss_cls.mean.mean.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "cross_entropy_loss.CrossEntropyLoss.class_weight.unsqueeze"], "methods", ["None"], ["", "", "def", "_forward", "(", "self", ",", "cls_score", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Forward function.\n\n        Args:\n            cls_score (torch.Tensor): The class score.\n            label (torch.Tensor): The ground truth label.\n            kwargs: Any keyword argument to be used to calculate\n                CrossEntropy loss.\n\n        Returns:\n            torch.Tensor: The returned CrossEntropy loss.\n        \"\"\"", "\n", "if", "cls_score", ".", "size", "(", ")", "==", "label", ".", "size", "(", ")", ":", "\n", "# calculate loss for soft label", "\n", "\n", "            ", "assert", "cls_score", ".", "dim", "(", ")", "==", "2", ",", "'Only support 2-dim soft label'", "\n", "assert", "len", "(", "kwargs", ")", "==", "0", ",", "(", "'For now, no extra args are supported for soft label, '", "\n", "f'but get {kwargs}'", ")", "\n", "\n", "lsm", "=", "F", ".", "log_softmax", "(", "cls_score", ",", "1", ")", "\n", "if", "self", ".", "class_weight", "is", "not", "None", ":", "\n", "                ", "self", ".", "class_weight", "=", "self", ".", "class_weight", ".", "to", "(", "cls_score", ".", "device", ")", "\n", "lsm", "=", "lsm", "*", "self", ".", "class_weight", ".", "unsqueeze", "(", "0", ")", "\n", "", "loss_cls", "=", "-", "(", "label", "*", "lsm", ")", ".", "sum", "(", "1", ")", "\n", "\n", "# default reduction 'mean'", "\n", "if", "self", ".", "class_weight", "is", "not", "None", ":", "\n", "# Use weighted average as pytorch CrossEntropyLoss does.", "\n", "# For more information, please visit https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html # noqa", "\n", "                ", "loss_cls", "=", "loss_cls", ".", "sum", "(", ")", "/", "torch", ".", "sum", "(", "\n", "self", ".", "class_weight", ".", "unsqueeze", "(", "0", ")", "*", "label", ")", "\n", "", "else", ":", "\n", "                ", "loss_cls", "=", "loss_cls", ".", "mean", "(", ")", "\n", "", "", "else", ":", "\n", "# calculate loss for hard label", "\n", "\n", "            ", "if", "self", ".", "class_weight", "is", "not", "None", ":", "\n", "                ", "assert", "'weight'", "not", "in", "kwargs", ",", "\"The key 'weight' already exists.\"", "\n", "kwargs", "[", "'weight'", "]", "=", "self", ".", "class_weight", ".", "to", "(", "cls_score", ".", "device", ")", "\n", "", "loss_cls", "=", "F", ".", "cross_entropy", "(", "cls_score", ",", "label", ",", "**", "kwargs", ")", "\n", "\n", "", "return", "loss_cls", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits.__init__": [[98, 103], ["base.BaseWeightedLoss.__init__", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "loss_weight", "=", "1.0", ",", "class_weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "loss_weight", "=", "loss_weight", ")", "\n", "self", ".", "class_weight", "=", "None", "\n", "if", "class_weight", "is", "not", "None", ":", "\n", "            ", "self", ".", "class_weight", "=", "torch", ".", "Tensor", "(", "class_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.losses.cross_entropy_loss.BCELossWithLogits._forward": [[104, 122], ["torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "cross_entropy_loss.BCELossWithLogits.class_weight.to"], "methods", ["None"], ["", "", "def", "_forward", "(", "self", ",", "cls_score", ",", "label", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Forward function.\n\n        Args:\n            cls_score (torch.Tensor): The class score.\n            label (torch.Tensor): The ground truth label.\n            kwargs: Any keyword argument to be used to calculate\n                bce loss with logits.\n\n        Returns:\n            torch.Tensor: The returned bce loss with logits.\n        \"\"\"", "\n", "if", "self", ".", "class_weight", "is", "not", "None", ":", "\n", "            ", "assert", "'weight'", "not", "in", "kwargs", ",", "\"The key 'weight' already exists.\"", "\n", "kwargs", "[", "'weight'", "]", "=", "self", ".", "class_weight", ".", "to", "(", "cls_score", ".", "device", ")", "\n", "", "loss_cls", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "cls_score", ",", "label", ",", "\n", "**", "kwargs", ")", "\n", "return", "loss_cls", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.__init__": [[71, 124], ["mmcv.runner.get_dist_info", "os.exists", "ValueError", "isinstance", "isinstance", "lfb.LFB.load_lfb", "lfb.LFB.load_lfb", "warnings.warn", "os.normpath", "lmdb.open", "ValueError", "os.join", "print", "lfb.LFB.load_lfb_on_lmdb", "torch.barrier", "torch.barrier"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.load_lfb", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.load_lfb", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.load_lfb_on_lmdb"], ["def", "__init__", "(", "self", ",", "\n", "lfb_prefix_path", ",", "\n", "max_num_sampled_feat", "=", "5", ",", "\n", "window_size", "=", "60", ",", "\n", "lfb_channels", "=", "2048", ",", "\n", "dataset_modes", "=", "(", "'train'", ",", "'val'", ")", ",", "\n", "device", "=", "'gpu'", ",", "\n", "lmdb_map_size", "=", "4e9", ",", "\n", "construct_lmdb", "=", "True", ")", ":", "\n", "        ", "if", "not", "osp", ".", "exists", "(", "lfb_prefix_path", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f'lfb prefix path {lfb_prefix_path} does not exist!'", ")", "\n", "", "self", ".", "lfb_prefix_path", "=", "lfb_prefix_path", "\n", "self", ".", "max_num_sampled_feat", "=", "max_num_sampled_feat", "\n", "self", ".", "window_size", "=", "window_size", "\n", "self", ".", "lfb_channels", "=", "lfb_channels", "\n", "if", "not", "isinstance", "(", "dataset_modes", ",", "tuple", ")", ":", "\n", "            ", "assert", "isinstance", "(", "dataset_modes", ",", "str", ")", "\n", "dataset_modes", "=", "(", "dataset_modes", ",", ")", "\n", "", "self", ".", "dataset_modes", "=", "dataset_modes", "\n", "self", ".", "device", "=", "device", "\n", "\n", "rank", ",", "world_size", "=", "get_dist_info", "(", ")", "\n", "\n", "# Loading LFB", "\n", "if", "self", ".", "device", "==", "'gpu'", ":", "\n", "            ", "self", ".", "load_lfb", "(", "f'cuda:{rank}'", ")", "\n", "", "elif", "self", ".", "device", "==", "'cpu'", ":", "\n", "            ", "if", "world_size", ">", "1", ":", "\n", "                ", "warnings", ".", "warn", "(", "\n", "'If distributed training is used with multi-GPUs, lfb '", "\n", "'will be loaded multiple times on RAM. In this case, '", "\n", "\"'lmdb' is recommended.\"", ",", "UserWarning", ")", "\n", "", "self", ".", "load_lfb", "(", "'cpu'", ")", "\n", "", "elif", "self", ".", "device", "==", "'lmdb'", ":", "\n", "            ", "assert", "lmdb_imported", ",", "(", "\n", "'Please install `lmdb` to load lfb on lmdb!'", ")", "\n", "self", ".", "lmdb_map_size", "=", "lmdb_map_size", "\n", "self", ".", "construct_lmdb", "=", "construct_lmdb", "\n", "self", ".", "lfb_lmdb_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "'lmdb'", ")", ")", "\n", "\n", "if", "rank", "==", "0", "and", "self", ".", "construct_lmdb", ":", "\n", "                ", "print", "(", "'Constructing LFB lmdb...'", ")", "\n", "self", ".", "load_lfb_on_lmdb", "(", ")", "\n", "\n", "# Synchronizes all processes to make sure lfb lmdb exist.", "\n", "", "if", "world_size", ">", "1", ":", "\n", "                ", "dist", ".", "barrier", "(", ")", "\n", "", "self", ".", "lmdb_env", "=", "lmdb", ".", "open", "(", "self", ".", "lfb_lmdb_path", ",", "readonly", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Device must be 'gpu', 'cpu' or 'lmdb', \"", ",", "\n", "f'but get {self.device}.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.load_lfb": [[125, 133], ["print", "os.normpath", "print", "lfb.LFB.lfb.update", "os.join", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "", "def", "load_lfb", "(", "self", ",", "map_location", ")", ":", "\n", "        ", "self", ".", "lfb", "=", "{", "}", "\n", "for", "dataset_mode", "in", "self", ".", "dataset_modes", ":", "\n", "            ", "lfb_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "f'lfb_{dataset_mode}.pkl'", ")", ")", "\n", "print", "(", "f'Loading LFB from {lfb_path}...'", ")", "\n", "self", ".", "lfb", ".", "update", "(", "torch", ".", "load", "(", "lfb_path", ",", "map_location", "=", "map_location", ")", ")", "\n", "", "print", "(", "f'LFB has been loaded on {map_location}.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.load_lfb_on_lmdb": [[134, 152], ["lmdb.open", "lfb.items", "print", "os.normpath", "lfb.update", "lmdb.open.begin", "io.BytesIO", "torch.save", "torch.save", "torch.save", "torch.save", "io.BytesIO.seek", "lmdb.open.begin.put", "lmdb.open.begin.commit", "io.BytesIO.close", "os.join", "torch.load", "torch.load", "torch.load", "torch.load", "key.encode", "io.BytesIO.read"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "load_lfb_on_lmdb", "(", "self", ")", ":", "\n", "        ", "lfb", "=", "{", "}", "\n", "for", "dataset_mode", "in", "self", ".", "dataset_modes", ":", "\n", "            ", "lfb_path", "=", "osp", ".", "normpath", "(", "\n", "osp", ".", "join", "(", "self", ".", "lfb_prefix_path", ",", "f'lfb_{dataset_mode}.pkl'", ")", ")", "\n", "lfb", ".", "update", "(", "torch", ".", "load", "(", "lfb_path", ",", "map_location", "=", "'cpu'", ")", ")", "\n", "\n", "", "lmdb_env", "=", "lmdb", ".", "open", "(", "self", ".", "lfb_lmdb_path", ",", "map_size", "=", "self", ".", "lmdb_map_size", ")", "\n", "for", "key", ",", "value", "in", "lfb", ".", "items", "(", ")", ":", "\n", "            ", "txn", "=", "lmdb_env", ".", "begin", "(", "write", "=", "True", ")", "\n", "buff", "=", "io", ".", "BytesIO", "(", ")", "\n", "torch", ".", "save", "(", "value", ",", "buff", ")", "\n", "buff", ".", "seek", "(", "0", ")", "\n", "txn", ".", "put", "(", "key", ".", "encode", "(", ")", ",", "buff", ".", "read", "(", ")", ")", "\n", "txn", ".", "commit", "(", ")", "\n", "buff", ".", "close", "(", ")", "\n", "\n", "", "print", "(", "f'LFB lmdb has been constructed on {self.lfb_lmdb_path}!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.sample_long_term_features": [[153, 180], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "range", "lfb.LFB.lmdb_env.begin", "txn.get", "torch.load", "torch.load", "torch.load", "torch.load", "len", "min", "numpy.random.choice", "enumerate", "video_id.encode", "io.BytesIO", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "sample_long_term_features", "(", "self", ",", "video_id", ",", "timestamp", ")", ":", "\n", "        ", "if", "self", ".", "device", "==", "'lmdb'", ":", "\n", "            ", "with", "self", ".", "lmdb_env", ".", "begin", "(", "write", "=", "False", ")", "as", "txn", ":", "\n", "                ", "buf", "=", "txn", ".", "get", "(", "video_id", ".", "encode", "(", ")", ")", "\n", "video_features", "=", "torch", ".", "load", "(", "io", ".", "BytesIO", "(", "buf", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "video_features", "=", "self", ".", "lfb", "[", "video_id", "]", "\n", "\n", "# Sample long term features.", "\n", "", "window_size", ",", "K", "=", "self", ".", "window_size", ",", "self", ".", "max_num_sampled_feat", "\n", "start", "=", "timestamp", "-", "(", "window_size", "//", "2", ")", "\n", "lt_feats", "=", "torch", ".", "zeros", "(", "window_size", "*", "K", ",", "self", ".", "lfb_channels", ")", "\n", "\n", "for", "idx", ",", "sec", "in", "enumerate", "(", "range", "(", "start", ",", "start", "+", "window_size", ")", ")", ":", "\n", "            ", "if", "sec", "in", "video_features", ":", "\n", "# `num_feat` is the number of roi features in this second.", "\n", "                ", "num_feat", "=", "len", "(", "video_features", "[", "sec", "]", ")", "\n", "num_feat_sampled", "=", "min", "(", "num_feat", ",", "K", ")", "\n", "# Sample some roi features randomly.", "\n", "random_lfb_indices", "=", "np", ".", "random", ".", "choice", "(", "\n", "range", "(", "num_feat", ")", ",", "num_feat_sampled", ",", "replace", "=", "False", ")", "\n", "\n", "for", "k", ",", "rand_idx", "in", "enumerate", "(", "random_lfb_indices", ")", ":", "\n", "                    ", "lt_feats", "[", "idx", "*", "K", "+", "k", "]", "=", "video_features", "[", "sec", "]", "[", "rand_idx", "]", "\n", "\n", "# [window_size * max_num_sampled_feat, lfb_channels]", "\n", "", "", "", "return", "lt_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.__getitem__": [[181, 186], ["img_key.split", "lfb.LFB.sample_long_term_features", "int"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.sample_long_term_features"], ["", "def", "__getitem__", "(", "self", ",", "img_key", ")", ":", "\n", "        ", "\"\"\"Sample long term features like `lfb['0f39OWEqJ24,0902']` where `lfb`\n        is a instance of class LFB.\"\"\"", "\n", "video_id", ",", "timestamp", "=", "img_key", ".", "split", "(", "','", ")", "\n", "return", "self", ".", "sample_long_term_features", "(", "video_id", ",", "int", "(", "timestamp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.lfb.LFB.__len__": [[187, 190], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of videos whose ROI features are stored in LFB.\"\"\"", "\n", "return", "len", "(", "self", ".", "lfb", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv_audio.ConvAudio.__init__": [[30, 82], ["torch.Module.__init__", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "torch.nn.modules.utils._pair", "mmcv.cnn.ConvModule", "mmcv.cnn.ConvModule", "conv_audio.ConvAudio.init_weights", "dict", "dict", "dict", "dict", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "op", "=", "'concat'", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_pair", "(", "kernel_size", ")", "\n", "stride", "=", "_pair", "(", "stride", ")", "\n", "padding", "=", "_pair", "(", "padding", ")", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "assert", "op", "in", "[", "'concat'", ",", "'sum'", "]", "\n", "self", ".", "op", "=", "op", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "output_padding", "=", "(", "0", ",", "0", ")", "\n", "self", ".", "transposed", "=", "False", "\n", "\n", "self", ".", "conv_1", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ")", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "(", "kernel_size", "[", "0", "]", "//", "2", ",", "0", ")", ",", "\n", "bias", "=", "bias", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ")", "\n", "\n", "self", ".", "conv_2", "=", "ConvModule", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ")", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "(", "0", ",", "kernel_size", "[", "1", "]", "//", "2", ")", ",", "\n", "bias", "=", "bias", ",", "\n", "conv_cfg", "=", "dict", "(", "type", "=", "'Conv'", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN'", ")", ",", "\n", "act_cfg", "=", "dict", "(", "type", "=", "'ReLU'", ")", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv_audio.ConvAudio.forward": [[83, 99], ["conv_audio.ConvAudio.conv_1", "conv_audio.ConvAudio.conv_2", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x_1", "=", "self", ".", "conv_1", "(", "x", ")", "\n", "x_2", "=", "self", ".", "conv_2", "(", "x", ")", "\n", "if", "self", ".", "op", "==", "'concat'", ":", "\n", "            ", "out", "=", "torch", ".", "cat", "(", "[", "x_1", ",", "x_2", "]", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "x_1", "+", "x_2", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv_audio.ConvAudio.init_weights": [[100, 106], ["mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "mmcv.cnn.constant_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "kaiming_init", "(", "self", ".", "conv_1", ".", "conv", ")", "\n", "kaiming_init", "(", "self", ".", "conv_2", ".", "conv", ")", "\n", "constant_init", "(", "self", ".", "conv_1", ".", "bn", ",", "1", ",", "bias", "=", "0", ")", "\n", "constant_init", "(", "self", ".", "conv_2", ".", "bn", ",", "1", ",", "bias", "=", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv2plus1d.Conv2plus1d.__init__": [[26, 85], ["dict", "torch.Module.__init__", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "torch.nn.modules.utils._triple", "int", "torch.Conv3d", "mmcv.cnn.build_norm_layer", "torch.ReLU", "torch.Conv3d", "conv2plus1d.Conv2plus1d.init_weights", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "0", ",", "\n", "dilation", "=", "1", ",", "\n", "groups", "=", "1", ",", "\n", "bias", "=", "True", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'BN3d'", ")", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "kernel_size", "=", "_triple", "(", "kernel_size", ")", "\n", "stride", "=", "_triple", "(", "stride", ")", "\n", "padding", "=", "_triple", "(", "padding", ")", "\n", "assert", "len", "(", "kernel_size", ")", "==", "len", "(", "stride", ")", "==", "len", "(", "padding", ")", "==", "3", "\n", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dilation", "=", "dilation", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "bias", "=", "bias", "\n", "self", ".", "norm_cfg", "=", "norm_cfg", "\n", "self", ".", "output_padding", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "self", ".", "transposed", "=", "False", "\n", "\n", "# The middle-plane is calculated according to:", "\n", "# M_i = \\floor{\\frac{t * d^2 N_i-1 * N_i}", "\n", "#   {d^2 * N_i-1 + t * N_i}}", "\n", "# where d, t are spatial and temporal kernel, and", "\n", "# N_i, N_i-1 are planes", "\n", "# and inplanes. https://arxiv.org/pdf/1711.11248.pdf", "\n", "mid_channels", "=", "3", "*", "(", "\n", "in_channels", "*", "out_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", ")", "\n", "mid_channels", "/=", "(", "\n", "in_channels", "*", "kernel_size", "[", "1", "]", "*", "kernel_size", "[", "2", "]", "+", "3", "*", "out_channels", ")", "\n", "mid_channels", "=", "int", "(", "mid_channels", ")", "\n", "\n", "self", ".", "conv_s", "=", "nn", ".", "Conv3d", "(", "\n", "in_channels", ",", "\n", "mid_channels", ",", "\n", "kernel_size", "=", "(", "1", ",", "kernel_size", "[", "1", "]", ",", "kernel_size", "[", "2", "]", ")", ",", "\n", "stride", "=", "(", "1", ",", "stride", "[", "1", "]", ",", "stride", "[", "2", "]", ")", ",", "\n", "padding", "=", "(", "0", ",", "padding", "[", "1", "]", ",", "padding", "[", "2", "]", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "_", ",", "self", ".", "bn_s", "=", "build_norm_layer", "(", "self", ".", "norm_cfg", ",", "mid_channels", ")", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "conv_t", "=", "nn", ".", "Conv3d", "(", "\n", "mid_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "(", "kernel_size", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "stride", "=", "(", "stride", "[", "0", "]", ",", "1", ",", "1", ")", ",", "\n", "padding", "=", "(", "padding", "[", "0", "]", ",", "0", ",", "0", ")", ",", "\n", "bias", "=", "bias", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv2plus1d.Conv2plus1d.forward": [[86, 100], ["conv2plus1d.Conv2plus1d.conv_s", "conv2plus1d.Conv2plus1d.bn_s", "conv2plus1d.Conv2plus1d.relu", "conv2plus1d.Conv2plus1d.conv_t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "x", "=", "self", ".", "conv_s", "(", "x", ")", "\n", "x", "=", "self", ".", "bn_s", "(", "x", ")", "\n", "x", "=", "self", ".", "relu", "(", "x", ")", "\n", "x", "=", "self", ".", "conv_t", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.conv2plus1d.Conv2plus1d.init_weights": [[101, 106], ["mmcv.cnn.kaiming_init", "mmcv.cnn.kaiming_init", "mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initiate the parameters from scratch.\"\"\"", "\n", "kaiming_init", "(", "self", ".", "conv_s", ")", "\n", "kaiming_init", "(", "self", ".", "conv_t", ")", "\n", "constant_init", "(", "self", ".", "bn_s", ",", "1", ",", "bias", "=", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.tam.TAM.__init__": [[34, 74], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "\n", "in_channels", ",", "\n", "num_segments", ",", "\n", "alpha", "=", "2", ",", "\n", "adaptive_kernel_size", "=", "3", ",", "\n", "beta", "=", "4", ",", "\n", "conv1d_kernel_size", "=", "3", ",", "\n", "adaptive_convolution_stride", "=", "1", ",", "\n", "adaptive_convolution_padding", "=", "1", ",", "\n", "init_std", "=", "0.001", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "beta", ">", "0", "and", "alpha", ">", "0", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "adaptive_kernel_size", "=", "adaptive_kernel_size", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "conv1d_kernel_size", "=", "conv1d_kernel_size", "\n", "self", ".", "adaptive_convolution_stride", "=", "adaptive_convolution_stride", "\n", "self", ".", "adaptive_convolution_padding", "=", "adaptive_convolution_padding", "\n", "self", ".", "init_std", "=", "init_std", "\n", "\n", "self", ".", "G", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "num_segments", ",", "num_segments", "*", "alpha", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "num_segments", "*", "alpha", ")", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "num_segments", "*", "alpha", ",", "adaptive_kernel_size", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Softmax", "(", "-", "1", ")", ")", "\n", "\n", "self", ".", "L", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "in_channels", ",", "\n", "in_channels", "//", "beta", ",", "\n", "conv1d_kernel_size", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "conv1d_kernel_size", "//", "2", ",", "\n", "bias", "=", "False", ")", ",", "nn", ".", "BatchNorm1d", "(", "in_channels", "//", "beta", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv1d", "(", "in_channels", "//", "beta", ",", "in_channels", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.tam.TAM.forward": [[75, 123], ["x.permute().contiguous.permute().contiguous.size", "x.permute().contiguous.permute().contiguous.view", "x.permute().contiguous.permute().contiguous.permute().contiguous", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d", "tam.TAM.G().view", "tam.TAM.L().view", "torch.conv2d", "torch.conv2d", "y.permute().contiguous().view.permute().contiguous().view.view", "y.permute().contiguous().view.permute().contiguous().view.permute().contiguous().view", "x.permute().contiguous.permute().contiguous.view", "new_x.view", "x.permute().contiguous.permute().contiguous.permute", "tam.TAM.G", "tam.TAM.L", "y.permute().contiguous().view.permute().contiguous().view.permute().contiguous", "torch.adaptive_avg_pool2d.view", "torch.adaptive_avg_pool2d.view", "y.permute().contiguous().view.permute().contiguous().view.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Defines the computation performed at every call.\n\n        Args:\n            x (torch.Tensor): The input data.\n\n        Returns:\n            torch.Tensor: The output of the module.\n        \"\"\"", "\n", "# [n, c, h, w]", "\n", "n", ",", "c", ",", "h", ",", "w", "=", "x", ".", "size", "(", ")", "\n", "num_segments", "=", "self", ".", "num_segments", "\n", "num_batches", "=", "n", "//", "num_segments", "\n", "assert", "c", "==", "self", ".", "in_channels", "\n", "\n", "# [num_batches, c, num_segments, h, w]", "\n", "x", "=", "x", ".", "view", "(", "num_batches", ",", "num_segments", ",", "c", ",", "h", ",", "w", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", ".", "contiguous", "(", ")", "\n", "\n", "# [num_batches * c, num_segments, 1, 1]", "\n", "theta_out", "=", "F", ".", "adaptive_avg_pool2d", "(", "\n", "x", ".", "view", "(", "-", "1", ",", "num_segments", ",", "h", ",", "w", ")", ",", "(", "1", ",", "1", ")", ")", "\n", "\n", "# [num_batches * c, 1, adaptive_kernel_size, 1]", "\n", "conv_kernel", "=", "self", ".", "G", "(", "theta_out", ".", "view", "(", "-", "1", ",", "num_segments", ")", ")", ".", "view", "(", "\n", "num_batches", "*", "c", ",", "1", ",", "-", "1", ",", "1", ")", "\n", "\n", "# [num_batches, c, num_segments, 1, 1]", "\n", "local_activation", "=", "self", ".", "L", "(", "theta_out", ".", "view", "(", "-", "1", ",", "c", ",", "num_segments", ")", ")", ".", "view", "(", "\n", "num_batches", ",", "c", ",", "num_segments", ",", "1", ",", "1", ")", "\n", "\n", "# [num_batches, c, num_segments, h, w]", "\n", "new_x", "=", "x", "*", "local_activation", "\n", "\n", "# [1, num_batches * c, num_segments, h * w]", "\n", "y", "=", "F", ".", "conv2d", "(", "\n", "new_x", ".", "view", "(", "1", ",", "num_batches", "*", "c", ",", "num_segments", ",", "h", "*", "w", ")", ",", "\n", "conv_kernel", ",", "\n", "bias", "=", "None", ",", "\n", "stride", "=", "(", "self", ".", "adaptive_convolution_stride", ",", "1", ")", ",", "\n", "padding", "=", "(", "self", ".", "adaptive_convolution_padding", ",", "0", ")", ",", "\n", "groups", "=", "num_batches", "*", "c", ")", "\n", "\n", "# [n, c, h, w]", "\n", "y", "=", "y", ".", "view", "(", "num_batches", ",", "c", ",", "num_segments", ",", "h", ",", "w", ")", "\n", "y", "=", "y", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", ".", "contiguous", "(", ")", ".", "view", "(", "n", ",", "c", ",", "h", ",", "w", ")", "\n", "\n", "return", "y", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedTemporalAttentionWithNorm.__init__": [[33, 59], ["dict", "dict", "mmcv.runner.base_module.BaseModule.__init__", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "transformer.DividedTemporalAttentionWithNorm.init_weights", "mmcv.cnn.build_norm_layer", "mmcv.utils.digit_version", "mmcv.utils.digit_version", "kwargs.pop", "mmcv.cnn.bricks.transformer.build_dropout", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "embed_dims", ",", "\n", "num_heads", ",", "\n", "num_frames", ",", "\n", "attn_drop", "=", "0.", ",", "\n", "proj_drop", "=", "0.", ",", "\n", "dropout_layer", "=", "dict", "(", "type", "=", "'DropPath'", ",", "drop_prob", "=", "0.1", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ")", ",", "\n", "init_cfg", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "init_cfg", ")", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "num_frames", "=", "num_frames", "\n", "self", ".", "norm", "=", "build_norm_layer", "(", "norm_cfg", ",", "self", ".", "embed_dims", ")", "[", "1", "]", "\n", "\n", "if", "digit_version", "(", "torch", ".", "__version__", ")", "<", "digit_version", "(", "'1.9.0'", ")", ":", "\n", "            ", "kwargs", ".", "pop", "(", "'batch_first'", ",", "None", ")", "\n", "", "self", ".", "attn", "=", "nn", ".", "MultiheadAttention", "(", "embed_dims", ",", "num_heads", ",", "attn_drop", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "self", ".", "dropout_layer", "=", "build_dropout", "(", "\n", "dropout_layer", ")", "if", "dropout_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "temporal_fc", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dims", ",", "self", ".", "embed_dims", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedTemporalAttentionWithNorm.init_weights": [[60, 62], ["mmcv.cnn.constant_init"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "constant_init", "(", "self", ".", "temporal_fc", ",", "val", "=", "0", ",", "bias", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedTemporalAttentionWithNorm.forward": [[63, 88], ["query[].unsqueeze", "transformer.DividedTemporalAttentionWithNorm.size", "transformer.DividedTemporalAttentionWithNorm.norm().permute", "[].permute", "transformer.DividedTemporalAttentionWithNorm.dropout_layer", "transformer.DividedTemporalAttentionWithNorm.temporal_fc", "res_temporal.reshape.reshape.reshape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "transformer.DividedTemporalAttentionWithNorm.proj_drop", "transformer.DividedTemporalAttentionWithNorm.norm", "res_temporal.reshape.reshape.contiguous", "transformer.DividedTemporalAttentionWithNorm.reshape", "transformer.DividedTemporalAttentionWithNorm.attn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", "=", "None", ",", "value", "=", "None", ",", "residual", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "residual", "is", "None", ",", "(", "\n", "'Always adding the shortcut in the forward function'", ")", "\n", "\n", "init_cls_token", "=", "query", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "\n", "identity", "=", "query_t", "=", "query", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "\n", "# query_t [batch_size, num_patches * num_frames, embed_dims]", "\n", "b", ",", "pt", ",", "m", "=", "query_t", ".", "size", "(", ")", "\n", "p", ",", "t", "=", "pt", "//", "self", ".", "num_frames", ",", "self", ".", "num_frames", "\n", "\n", "# res_temporal [batch_size * num_patches, num_frames, embed_dims]", "\n", "query_t", "=", "self", ".", "norm", "(", "query_t", ".", "reshape", "(", "b", "*", "p", ",", "t", ",", "m", ")", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "res_temporal", "=", "self", ".", "attn", "(", "query_t", ",", "query_t", ",", "query_t", ")", "[", "0", "]", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "res_temporal", "=", "self", ".", "dropout_layer", "(", "\n", "self", ".", "proj_drop", "(", "res_temporal", ".", "contiguous", "(", ")", ")", ")", "\n", "res_temporal", "=", "self", ".", "temporal_fc", "(", "res_temporal", ")", "\n", "\n", "# res_temporal [batch_size, num_patches * num_frames, embed_dims]", "\n", "res_temporal", "=", "res_temporal", ".", "reshape", "(", "b", ",", "p", "*", "t", ",", "m", ")", "\n", "\n", "# ret_value [batch_size, num_patches * num_frames + 1, embed_dims]", "\n", "new_query_t", "=", "identity", "+", "res_temporal", "\n", "new_query", "=", "torch", ".", "cat", "(", "(", "init_cls_token", ",", "new_query_t", ")", ",", "1", ")", "\n", "return", "new_query", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.__init__": [[111, 135], ["dict", "dict", "mmcv.runner.base_module.BaseModule.__init__", "torch.MultiheadAttention", "torch.MultiheadAttention", "torch.Dropout", "torch.Dropout", "transformer.DividedSpatialAttentionWithNorm.init_weights", "mmcv.cnn.build_norm_layer", "mmcv.utils.digit_version", "mmcv.utils.digit_version", "kwargs.pop", "mmcv.cnn.bricks.transformer.build_dropout", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights"], ["def", "__init__", "(", "self", ",", "\n", "embed_dims", ",", "\n", "num_heads", ",", "\n", "num_frames", ",", "\n", "attn_drop", "=", "0.", ",", "\n", "proj_drop", "=", "0.", ",", "\n", "dropout_layer", "=", "dict", "(", "type", "=", "'DropPath'", ",", "drop_prob", "=", "0.1", ")", ",", "\n", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ")", ",", "\n", "init_cfg", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "init_cfg", ")", "\n", "self", ".", "embed_dims", "=", "embed_dims", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "num_frames", "=", "num_frames", "\n", "self", ".", "norm", "=", "build_norm_layer", "(", "norm_cfg", ",", "self", ".", "embed_dims", ")", "[", "1", "]", "\n", "if", "digit_version", "(", "torch", ".", "__version__", ")", "<", "digit_version", "(", "'1.9.0'", ")", ":", "\n", "            ", "kwargs", ".", "pop", "(", "'batch_first'", ",", "None", ")", "\n", "", "self", ".", "attn", "=", "nn", ".", "MultiheadAttention", "(", "embed_dims", ",", "num_heads", ",", "attn_drop", ",", "\n", "**", "kwargs", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "self", ".", "dropout_layer", "=", "build_dropout", "(", "\n", "dropout_layer", ")", "if", "dropout_layer", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.init_weights": [[136, 139], ["None"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "# init DividedSpatialAttentionWithNorm by default", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.DividedSpatialAttentionWithNorm.forward": [[140, 177], ["query[].unsqueeze", "transformer.DividedSpatialAttentionWithNorm.size", "query[].unsqueeze.repeat().reshape().unsqueeze", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "transformer.DividedSpatialAttentionWithNorm.norm().permute", "[].permute", "transformer.DividedSpatialAttentionWithNorm.dropout_layer", "res_spatial[].reshape", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "einops.rearrange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "transformer.DividedSpatialAttentionWithNorm.proj_drop", "query[].unsqueeze.repeat().reshape", "transformer.DividedSpatialAttentionWithNorm.norm", "torch.cat.contiguous", "torch.cat.contiguous", "transformer.DividedSpatialAttentionWithNorm.attn", "query[].unsqueeze.repeat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "key", "=", "None", ",", "value", "=", "None", ",", "residual", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "residual", "is", "None", ",", "(", "\n", "'Always adding the shortcut in the forward function'", ")", "\n", "\n", "identity", "=", "query", "\n", "init_cls_token", "=", "query", "[", ":", ",", "0", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", "\n", "query_s", "=", "query", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "\n", "# query_s [batch_size, num_patches * num_frames, embed_dims]", "\n", "b", ",", "pt", ",", "m", "=", "query_s", ".", "size", "(", ")", "\n", "p", ",", "t", "=", "pt", "//", "self", ".", "num_frames", ",", "self", ".", "num_frames", "\n", "\n", "# cls_token [batch_size * num_frames, 1, embed_dims]", "\n", "cls_token", "=", "init_cls_token", ".", "repeat", "(", "1", ",", "t", ",", "1", ")", ".", "reshape", "(", "b", "*", "t", ",", "\n", "m", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# query_s [batch_size * num_frames, num_patches + 1, embed_dims]", "\n", "query_s", "=", "rearrange", "(", "query_s", ",", "'b (p t) m -> (b t) p m'", ",", "p", "=", "p", ",", "t", "=", "t", ")", "\n", "query_s", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "query_s", ")", ",", "1", ")", "\n", "\n", "# res_spatial [batch_size * num_frames, num_patches + 1, embed_dims]", "\n", "query_s", "=", "self", ".", "norm", "(", "query_s", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "res_spatial", "=", "self", ".", "attn", "(", "query_s", ",", "query_s", ",", "query_s", ")", "[", "0", "]", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "res_spatial", "=", "self", ".", "dropout_layer", "(", "\n", "self", ".", "proj_drop", "(", "res_spatial", ".", "contiguous", "(", ")", ")", ")", "\n", "\n", "# cls_token [batch_size, 1, embed_dims]", "\n", "cls_token", "=", "res_spatial", "[", ":", ",", "0", ",", ":", "]", ".", "reshape", "(", "b", ",", "t", ",", "m", ")", "\n", "cls_token", "=", "torch", ".", "mean", "(", "cls_token", ",", "1", ",", "True", ")", "\n", "\n", "# res_spatial [batch_size * num_frames, num_patches + 1, embed_dims]", "\n", "res_spatial", "=", "rearrange", "(", "\n", "res_spatial", "[", ":", ",", "1", ":", ",", ":", "]", ",", "'(b t) p m -> b (p t) m'", ",", "p", "=", "p", ",", "t", "=", "t", ")", "\n", "res_spatial", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "res_spatial", ")", ",", "1", ")", "\n", "\n", "new_query", "=", "identity", "+", "res_spatial", "\n", "return", "new_query", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.__init__": [[210, 213], ["dict", "mmcv.cnn.bricks.transformer.FFN.__init__", "mmcv.cnn.build_norm_layer"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "norm_cfg", "=", "dict", "(", "type", "=", "'LN'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "norm", "=", "build_norm_layer", "(", "norm_cfg", ",", "self", ".", "embed_dims", ")", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward": [[214, 217], ["super().forward", "transformer.FFNWithNorm.norm"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.common.transformer.FFNWithNorm.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "residual", "=", "None", ")", ":", "\n", "        ", "assert", "residual", "is", "None", ",", "(", "'Cannot apply pre-norm with FFNWithNorm'", ")", "\n", "return", "super", "(", ")", ".", "forward", "(", "self", ".", "norm", "(", "x", ")", ",", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization.__init__": [[24, 53], ["numpy.linspace", "utils.get_root_logger", "eval_detection.ActivityNetLocalization._import_ground_truth", "eval_detection.ActivityNetLocalization._import_prediction", "IOError", "IOError", "mmcv.utils.print_log", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.logger.get_root_logger", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization._import_ground_truth", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization._import_prediction"], ["def", "__init__", "(", "self", ",", "\n", "ground_truth_filename", "=", "None", ",", "\n", "prediction_filename", "=", "None", ",", "\n", "tiou_thresholds", "=", "np", ".", "linspace", "(", "0.5", ",", "0.95", ",", "10", ")", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "not", "ground_truth_filename", ":", "\n", "            ", "raise", "IOError", "(", "'Please input a valid ground truth file.'", ")", "\n", "", "if", "not", "prediction_filename", ":", "\n", "            ", "raise", "IOError", "(", "'Please input a valid prediction file.'", ")", "\n", "", "self", ".", "ground_truth_filename", "=", "ground_truth_filename", "\n", "self", ".", "prediction_filename", "=", "prediction_filename", "\n", "self", ".", "tiou_thresholds", "=", "tiou_thresholds", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "ap", "=", "None", "\n", "self", ".", "logger", "=", "get_root_logger", "(", ")", "\n", "# Import ground truth and predictions.", "\n", "self", ".", "ground_truth", ",", "self", ".", "activity_index", "=", "self", ".", "_import_ground_truth", "(", "\n", "ground_truth_filename", ")", "\n", "self", ".", "prediction", "=", "self", ".", "_import_prediction", "(", "prediction_filename", ")", "\n", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "log_msg", "=", "(", "\n", "'[INIT] Loaded ground_truth from '", "\n", "f'{self.ground_truth_filename}, prediction from '", "\n", "f'{self.prediction_filename}.\\n'", "\n", "f'Number of ground truth instances: {len(self.ground_truth)}\\n'", "\n", "f'Number of predictions: {len(self.prediction)}\\n'", "\n", "f'Fixed threshold for tiou score: {self.tiou_thresholds}'", ")", "\n", "print_log", "(", "log_msg", ",", "logger", "=", "self", ".", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization._import_ground_truth": [[54, 88], ["json.load.items", "open", "json.load", "float", "float", "ground_truth.append"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_import_ground_truth", "(", "ground_truth_filename", ")", ":", "\n", "        ", "\"\"\"Read ground truth file and return the ground truth instances and the\n        activity classes.\n\n        Args:\n            ground_truth_filename (str): Full path to the ground truth json\n                file.\n\n        Returns:\n            tuple[list, dict]: (ground_truth, activity_index).\n                ground_truth contains the ground truth instances, which is in a\n                    dict format.\n                activity_index contains classes index.\n        \"\"\"", "\n", "with", "open", "(", "ground_truth_filename", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "# Checking format", "\n", "", "activity_index", ",", "class_idx", "=", "{", "}", ",", "0", "\n", "ground_truth", "=", "[", "]", "\n", "for", "video_id", ",", "video_info", "in", "data", ".", "items", "(", ")", ":", "\n", "            ", "for", "anno", "in", "video_info", "[", "'annotations'", "]", ":", "\n", "                ", "if", "anno", "[", "'label'", "]", "not", "in", "activity_index", ":", "\n", "                    ", "activity_index", "[", "anno", "[", "'label'", "]", "]", "=", "class_idx", "\n", "class_idx", "+=", "1", "\n", "# old video_anno", "\n", "", "ground_truth_item", "=", "{", "}", "\n", "ground_truth_item", "[", "'video-id'", "]", "=", "video_id", "[", "2", ":", "]", "\n", "ground_truth_item", "[", "'t-start'", "]", "=", "float", "(", "anno", "[", "'segment'", "]", "[", "0", "]", ")", "\n", "ground_truth_item", "[", "'t-end'", "]", "=", "float", "(", "anno", "[", "'segment'", "]", "[", "1", "]", ")", "\n", "ground_truth_item", "[", "'label'", "]", "=", "activity_index", "[", "anno", "[", "'label'", "]", "]", "\n", "ground_truth", ".", "append", "(", "ground_truth_item", ")", "\n", "\n", "", "", "return", "ground_truth", ",", "activity_index", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization._import_prediction": [[89, 113], ["data[].items", "open", "json.load", "dict", "float", "float", "prediction.append"], "methods", ["None"], ["", "def", "_import_prediction", "(", "self", ",", "prediction_filename", ")", ":", "\n", "        ", "\"\"\"Read prediction file and return the prediction instances.\n\n        Args:\n            prediction_filename (str): Full path to the prediction json file.\n\n        Returns:\n            List: List containing the prediction instances (dictionaries).\n        \"\"\"", "\n", "with", "open", "(", "prediction_filename", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "# Read predictions.", "\n", "", "prediction", "=", "[", "]", "\n", "for", "video_id", ",", "video_info", "in", "data", "[", "'results'", "]", ".", "items", "(", ")", ":", "\n", "            ", "for", "result", "in", "video_info", ":", "\n", "                ", "prediction_item", "=", "dict", "(", ")", "\n", "prediction_item", "[", "'video-id'", "]", "=", "video_id", "\n", "prediction_item", "[", "'label'", "]", "=", "self", ".", "activity_index", "[", "result", "[", "'label'", "]", "]", "\n", "prediction_item", "[", "'t-start'", "]", "=", "float", "(", "result", "[", "'segment'", "]", "[", "0", "]", ")", "\n", "prediction_item", "[", "'t-end'", "]", "=", "float", "(", "result", "[", "'segment'", "]", "[", "1", "]", ")", "\n", "prediction_item", "[", "'score'", "]", "=", "result", "[", "'score'", "]", "\n", "prediction", ".", "append", "(", "prediction_item", ")", "\n", "\n", "", "", "return", "prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization.wrapper_compute_average_precision": [[114, 136], ["numpy.zeros", "range", "range", "len", "ground_truth_by_label.append", "prediction_by_label.append", "ground_truth_by_label[].append", "prediction_by_label[].append", "len", "eval_detection.compute_average_precision_detection", "len", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.compute_average_precision_detection"], ["", "def", "wrapper_compute_average_precision", "(", "self", ")", ":", "\n", "        ", "\"\"\"Computes average precision for each class.\"\"\"", "\n", "ap", "=", "np", ".", "zeros", "(", "(", "len", "(", "self", ".", "tiou_thresholds", ")", ",", "len", "(", "self", ".", "activity_index", ")", ")", ")", "\n", "\n", "# Adaptation to query faster", "\n", "ground_truth_by_label", "=", "[", "]", "\n", "prediction_by_label", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "activity_index", ")", ")", ":", "\n", "            ", "ground_truth_by_label", ".", "append", "(", "[", "]", ")", "\n", "prediction_by_label", ".", "append", "(", "[", "]", ")", "\n", "", "for", "gt", "in", "self", ".", "ground_truth", ":", "\n", "            ", "ground_truth_by_label", "[", "gt", "[", "'label'", "]", "]", ".", "append", "(", "gt", ")", "\n", "", "for", "pred", "in", "self", ".", "prediction", ":", "\n", "            ", "prediction_by_label", "[", "pred", "[", "'label'", "]", "]", ".", "append", "(", "pred", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "activity_index", ")", ")", ":", "\n", "            ", "ap_result", "=", "compute_average_precision_detection", "(", "\n", "ground_truth_by_label", "[", "i", "]", ",", "prediction_by_label", "[", "i", "]", ",", "\n", "self", ".", "tiou_thresholds", ")", "\n", "ap", "[", ":", ",", "i", "]", "=", "ap_result", "\n", "\n", "", "return", "ap", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization.evaluate": [[137, 149], ["eval_detection.ActivityNetLocalization.wrapper_compute_average_precision", "eval_detection.ActivityNetLocalization.ap.mean", "eval_detection.ActivityNetLocalization.mAP.mean"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.ActivityNetLocalization.wrapper_compute_average_precision"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Evaluates a prediction file.\n\n        For the detection task we measure the interpolated mean average\n        precision to measure the performance of a method.\n        \"\"\"", "\n", "self", ".", "ap", "=", "self", ".", "wrapper_compute_average_precision", "(", ")", "\n", "\n", "self", ".", "mAP", "=", "self", ".", "ap", ".", "mean", "(", "axis", "=", "1", ")", "\n", "self", ".", "average_mAP", "=", "self", ".", "mAP", ".", "mean", "(", ")", "\n", "\n", "return", "self", ".", "mAP", ",", "self", ".", "average_mAP", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.eval_detection.compute_average_precision_detection": [[151, 235], ["numpy.linspace", "len", "len", "len", "numpy.zeros", "float", "prediction.sort", "numpy.zeros", "numpy.zeros", "enumerate", "enumerate", "numpy.cumsum().astype", "numpy.cumsum().astype", "range", "len", "numpy.ones", "ground_truth_by_videoid.setdefault().append", "accuracy.pairwise_temporal_iou", "tiou_arr.reshape.reshape", "enumerate", "len", "accuracy.interpolated_precision_recall", "numpy.array", "numpy.array", "tiou_arr.reshape.argsort", "numpy.cumsum", "numpy.cumsum", "ground_truth_by_videoid.setdefault", "numpy.array"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.pairwise_temporal_iou", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.interpolated_precision_recall"], ["", "", "def", "compute_average_precision_detection", "(", "ground_truth", ",", "\n", "prediction", ",", "\n", "tiou_thresholds", "=", "np", ".", "linspace", "(", "\n", "0.5", ",", "0.95", ",", "10", ")", ")", ":", "\n", "    ", "\"\"\"Compute average precision (detection task) between ground truth and\n    predictions data frames. If multiple predictions occurs for the same\n    predicted segment, only the one with highest score is matches as true\n    positive. This code is greatly inspired by Pascal VOC devkit.\n\n    Args:\n        ground_truth (list[dict]): List containing the ground truth instances\n            (dictionaries). Required keys are 'video-id', 't-start' and\n            't-end'.\n        prediction (list[dict]): List containing the prediction instances\n            (dictionaries). Required keys are: 'video-id', 't-start', 't-end'\n            and 'score'.\n        tiou_thresholds (np.ndarray): A 1darray indicates the temporal\n            intersection over union threshold, which is optional.\n            Default: ``np.linspace(0.5, 0.95, 10)``.\n\n    Returns:\n        Float: ap, Average precision score.\n    \"\"\"", "\n", "num_thresholds", "=", "len", "(", "tiou_thresholds", ")", "\n", "num_gts", "=", "len", "(", "ground_truth", ")", "\n", "num_preds", "=", "len", "(", "prediction", ")", "\n", "ap", "=", "np", ".", "zeros", "(", "num_thresholds", ")", "\n", "if", "len", "(", "prediction", ")", "==", "0", ":", "\n", "        ", "return", "ap", "\n", "\n", "", "num_positive", "=", "float", "(", "num_gts", ")", "\n", "lock_gt", "=", "np", ".", "ones", "(", "(", "num_thresholds", ",", "num_gts", ")", ")", "*", "-", "1", "\n", "# Sort predictions by decreasing score order.", "\n", "prediction", ".", "sort", "(", "key", "=", "lambda", "x", ":", "-", "x", "[", "'score'", "]", ")", "\n", "# Initialize true positive and false positive vectors.", "\n", "tp", "=", "np", ".", "zeros", "(", "(", "num_thresholds", ",", "num_preds", ")", ")", "\n", "fp", "=", "np", ".", "zeros", "(", "(", "num_thresholds", ",", "num_preds", ")", ")", "\n", "\n", "# Adaptation to query faster", "\n", "ground_truth_by_videoid", "=", "{", "}", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "ground_truth", ")", ":", "\n", "        ", "item", "[", "'index'", "]", "=", "i", "\n", "ground_truth_by_videoid", ".", "setdefault", "(", "item", "[", "'video-id'", "]", ",", "[", "]", ")", ".", "append", "(", "item", ")", "\n", "\n", "# Assigning true positive to truly grount truth instances.", "\n", "", "for", "idx", ",", "pred", "in", "enumerate", "(", "prediction", ")", ":", "\n", "        ", "if", "pred", "[", "'video-id'", "]", "in", "ground_truth_by_videoid", ":", "\n", "            ", "gts", "=", "ground_truth_by_videoid", "[", "pred", "[", "'video-id'", "]", "]", "\n", "", "else", ":", "\n", "            ", "fp", "[", ":", ",", "idx", "]", "=", "1", "\n", "continue", "\n", "\n", "", "tiou_arr", "=", "pairwise_temporal_iou", "(", "\n", "np", ".", "array", "(", "[", "pred", "[", "'t-start'", "]", ",", "pred", "[", "'t-end'", "]", "]", ")", ",", "\n", "np", ".", "array", "(", "[", "np", ".", "array", "(", "[", "gt", "[", "'t-start'", "]", ",", "gt", "[", "'t-end'", "]", "]", ")", "for", "gt", "in", "gts", "]", ")", ")", "\n", "tiou_arr", "=", "tiou_arr", ".", "reshape", "(", "-", "1", ")", "\n", "# We would like to retrieve the predictions with highest tiou score.", "\n", "tiou_sorted_idx", "=", "tiou_arr", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "t_idx", ",", "tiou_threshold", "in", "enumerate", "(", "tiou_thresholds", ")", ":", "\n", "            ", "for", "j_idx", "in", "tiou_sorted_idx", ":", "\n", "                ", "if", "tiou_arr", "[", "j_idx", "]", "<", "tiou_threshold", ":", "\n", "                    ", "fp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "break", "\n", "", "if", "lock_gt", "[", "t_idx", ",", "gts", "[", "j_idx", "]", "[", "'index'", "]", "]", ">=", "0", ":", "\n", "                    ", "continue", "\n", "# Assign as true positive after the filters above.", "\n", "", "tp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "lock_gt", "[", "t_idx", ",", "gts", "[", "j_idx", "]", "[", "'index'", "]", "]", "=", "idx", "\n", "break", "\n", "\n", "", "if", "fp", "[", "t_idx", ",", "idx", "]", "==", "0", "and", "tp", "[", "t_idx", ",", "idx", "]", "==", "0", ":", "\n", "                ", "fp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "\n", "", "", "", "tp_cumsum", "=", "np", ".", "cumsum", "(", "tp", ",", "axis", "=", "1", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "fp_cumsum", "=", "np", ".", "cumsum", "(", "fp", ",", "axis", "=", "1", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "recall_cumsum", "=", "tp_cumsum", "/", "num_positive", "\n", "\n", "precision_cumsum", "=", "tp_cumsum", "/", "(", "tp_cumsum", "+", "fp_cumsum", ")", "\n", "\n", "for", "t_idx", "in", "range", "(", "len", "(", "tiou_thresholds", ")", ")", ":", "\n", "        ", "ap", "[", "t_idx", "]", "=", "interpolated_precision_recall", "(", "precision_cumsum", "[", "t_idx", ",", ":", "]", ",", "\n", "recall_cumsum", "[", "t_idx", ",", ":", "]", ")", "\n", "\n", "", "return", "ap", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.confusion_matrix": [[5, 67], ["isinstance", "isinstance", "numpy.unique", "len", "numpy.zeros", "enumerate", "numpy.bincount().reshape", "ValueError", "numpy.array", "isinstance", "TypeError", "TypeError", "numpy.array", "isinstance", "TypeError", "TypeError", "numpy.concatenate", "numpy.errstate", "numpy.nan_to_num", "numpy.bincount", "np.nan_to_num.sum", "type", "type", "np.nan_to_num.sum", "np.nan_to_num.sum"], "function", ["None"], ["def", "confusion_matrix", "(", "y_pred", ",", "y_real", ",", "normalize", "=", "None", ")", ":", "\n", "    ", "\"\"\"Compute confusion matrix.\n\n    Args:\n        y_pred (list[int] | np.ndarray[int]): Prediction labels.\n        y_real (list[int] | np.ndarray[int]): Ground truth labels.\n        normalize (str | None): Normalizes confusion matrix over the true\n            (rows), predicted (columns) conditions or all the population.\n            If None, confusion matrix will not be normalized. Options are\n            \"true\", \"pred\", \"all\", None. Default: None.\n\n    Returns:\n        np.ndarray: Confusion matrix.\n    \"\"\"", "\n", "if", "normalize", "not", "in", "[", "'true'", ",", "'pred'", ",", "'all'", ",", "None", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"normalize must be one of {'true', 'pred', \"", "\n", "\"'all', None}\"", ")", "\n", "\n", "", "if", "isinstance", "(", "y_pred", ",", "list", ")", ":", "\n", "        ", "y_pred", "=", "np", ".", "array", "(", "y_pred", ")", "\n", "", "if", "not", "isinstance", "(", "y_pred", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\n", "f'y_pred must be list or np.ndarray, but got {type(y_pred)}'", ")", "\n", "", "if", "not", "y_pred", ".", "dtype", "==", "np", ".", "int64", ":", "\n", "        ", "raise", "TypeError", "(", "\n", "f'y_pred dtype must be np.int64, but got {y_pred.dtype}'", ")", "\n", "\n", "", "if", "isinstance", "(", "y_real", ",", "list", ")", ":", "\n", "        ", "y_real", "=", "np", ".", "array", "(", "y_real", ")", "\n", "", "if", "not", "isinstance", "(", "y_real", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "raise", "TypeError", "(", "\n", "f'y_real must be list or np.ndarray, but got {type(y_real)}'", ")", "\n", "", "if", "not", "y_real", ".", "dtype", "==", "np", ".", "int64", ":", "\n", "        ", "raise", "TypeError", "(", "\n", "f'y_real dtype must be np.int64, but got {y_real.dtype}'", ")", "\n", "\n", "", "label_set", "=", "np", ".", "unique", "(", "np", ".", "concatenate", "(", "(", "y_pred", ",", "y_real", ")", ")", ")", "\n", "num_labels", "=", "len", "(", "label_set", ")", "\n", "max_label", "=", "label_set", "[", "-", "1", "]", "\n", "label_map", "=", "np", ".", "zeros", "(", "max_label", "+", "1", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "label_set", ")", ":", "\n", "        ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "y_pred_mapped", "=", "label_map", "[", "y_pred", "]", "\n", "y_real_mapped", "=", "label_map", "[", "y_real", "]", "\n", "\n", "confusion_mat", "=", "np", ".", "bincount", "(", "\n", "num_labels", "*", "y_real_mapped", "+", "y_pred_mapped", ",", "\n", "minlength", "=", "num_labels", "**", "2", ")", ".", "reshape", "(", "num_labels", ",", "num_labels", ")", "\n", "\n", "with", "np", ".", "errstate", "(", "all", "=", "'ignore'", ")", ":", "\n", "        ", "if", "normalize", "==", "'true'", ":", "\n", "            ", "confusion_mat", "=", "(", "\n", "confusion_mat", "/", "confusion_mat", ".", "sum", "(", "axis", "=", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "", "elif", "normalize", "==", "'pred'", ":", "\n", "            ", "confusion_mat", "=", "(", "\n", "confusion_mat", "/", "confusion_mat", ".", "sum", "(", "axis", "=", "0", ",", "keepdims", "=", "True", ")", ")", "\n", "", "elif", "normalize", "==", "'all'", ":", "\n", "            ", "confusion_mat", "=", "(", "confusion_mat", "/", "confusion_mat", ".", "sum", "(", ")", ")", "\n", "", "confusion_mat", "=", "np", ".", "nan_to_num", "(", "confusion_mat", ")", "\n", "\n", "", "return", "confusion_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_class_accuracy": [[69, 89], ["numpy.argmax", "confusion_matrix().astype", "confusion_matrix().astype.sum", "numpy.diag", "numpy.mean", "accuracy.confusion_matrix", "zip"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.confusion_matrix"], ["", "def", "mean_class_accuracy", "(", "scores", ",", "labels", ")", ":", "\n", "    ", "\"\"\"Calculate mean class accuracy.\n\n    Args:\n        scores (list[np.ndarray]): Prediction scores for each class.\n        labels (list[int]): Ground truth labels.\n\n    Returns:\n        np.ndarray: Mean class accuracy.\n    \"\"\"", "\n", "pred", "=", "np", ".", "argmax", "(", "scores", ",", "axis", "=", "1", ")", "\n", "cf_mat", "=", "confusion_matrix", "(", "pred", ",", "labels", ")", ".", "astype", "(", "float", ")", "\n", "\n", "cls_cnt", "=", "cf_mat", ".", "sum", "(", "axis", "=", "1", ")", "\n", "cls_hit", "=", "np", ".", "diag", "(", "cf_mat", ")", "\n", "\n", "mean_class_acc", "=", "np", ".", "mean", "(", "\n", "[", "hit", "/", "cnt", "if", "cnt", "else", "0.0", "for", "cnt", ",", "hit", "in", "zip", "(", "cls_cnt", ",", "cls_hit", ")", "]", ")", "\n", "\n", "return", "mean_class_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.top_k_classes": [[91, 128], ["numpy.argmax", "confusion_matrix().astype", "confusion_matrix().astype.sum", "numpy.diag", "numpy.array", "list", "list", "accuracy.confusion_matrix", "zip", "numpy.argsort", "zip", "zip", "numpy.argsort"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.confusion_matrix"], ["", "def", "top_k_classes", "(", "scores", ",", "labels", ",", "k", "=", "10", ",", "mode", "=", "'accurate'", ")", ":", "\n", "    ", "\"\"\"Calculate the most K accurate (inaccurate) classes.\n\n    Given the prediction scores, ground truth label and top-k value,\n    compute the top K accurate (inaccurate) classes.\n\n    Args:\n        scores (list[np.ndarray]): Prediction scores for each class.\n        labels (list[int] | np.ndarray): Ground truth labels.\n        k (int): Top-k values. Default: 10.\n        mode (str): Comparison mode for Top-k. Options are 'accurate'\n            and 'inaccurate'. Default: 'accurate'.\n\n    Return:\n        list: List of sorted (from high accuracy to low accuracy for\n            'accurate' mode, and from low accuracy to high accuracy for\n            inaccurate mode) top K classes in format of (label_id,\n            acc_ratio).\n    \"\"\"", "\n", "assert", "mode", "in", "[", "'accurate'", ",", "'inaccurate'", "]", "\n", "pred", "=", "np", ".", "argmax", "(", "scores", ",", "axis", "=", "1", ")", "\n", "cf_mat", "=", "confusion_matrix", "(", "pred", ",", "labels", ")", ".", "astype", "(", "float", ")", "\n", "\n", "cls_cnt", "=", "cf_mat", ".", "sum", "(", "axis", "=", "1", ")", "\n", "cls_hit", "=", "np", ".", "diag", "(", "cf_mat", ")", "\n", "hit_ratio", "=", "np", ".", "array", "(", "\n", "[", "hit", "/", "cnt", "if", "cnt", "else", "0.0", "for", "cnt", ",", "hit", "in", "zip", "(", "cls_cnt", ",", "cls_hit", ")", "]", ")", "\n", "\n", "if", "mode", "==", "'accurate'", ":", "\n", "        ", "max_index", "=", "np", ".", "argsort", "(", "hit_ratio", ")", "[", "-", "k", ":", "]", "[", ":", ":", "-", "1", "]", "\n", "max_value", "=", "hit_ratio", "[", "max_index", "]", "\n", "results", "=", "list", "(", "zip", "(", "max_index", ",", "max_value", ")", ")", "\n", "", "else", ":", "\n", "        ", "min_index", "=", "np", ".", "argsort", "(", "hit_ratio", ")", "[", ":", "k", "]", "\n", "min_value", "=", "hit_ratio", "[", "min_index", "]", "\n", "results", "=", "list", "(", "zip", "(", "min_index", ",", "min_value", ")", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.top_k_accuracy": [[130, 150], ["numpy.array", "numpy.logical_or.reduce", "res.append", "np.logical_or.reduce.sum", "numpy.argsort"], "function", ["None"], ["", "def", "top_k_accuracy", "(", "scores", ",", "labels", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Calculate top k accuracy score.\n\n    Args:\n        scores (list[np.ndarray]): Prediction scores for each class.\n        labels (list[int]): Ground truth labels.\n        topk (tuple[int]): K value for top_k_accuracy. Default: (1, ).\n\n    Returns:\n        list[float]: Top k accuracy score for each k.\n    \"\"\"", "\n", "res", "=", "[", "]", "\n", "labels", "=", "np", ".", "array", "(", "labels", ")", "[", ":", ",", "np", ".", "newaxis", "]", "\n", "for", "k", "in", "topk", ":", "\n", "        ", "max_k_preds", "=", "np", ".", "argsort", "(", "scores", ",", "axis", "=", "1", ")", "[", ":", ",", "-", "k", ":", "]", "[", ":", ",", ":", ":", "-", "1", "]", "\n", "match_array", "=", "np", ".", "logical_or", ".", "reduce", "(", "max_k_preds", "==", "labels", ",", "axis", "=", "1", ")", "\n", "topk_acc_score", "=", "match_array", ".", "sum", "(", ")", "/", "match_array", ".", "shape", "[", "0", "]", "\n", "res", ".", "append", "(", "topk_acc_score", ")", "\n", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mmit_mean_average_precision": [[152, 173], ["zip", "numpy.mean", "accuracy.binary_precision_recall_curve", "results.append", "numpy.sum", "numpy.diff", "numpy.array"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.binary_precision_recall_curve"], ["", "def", "mmit_mean_average_precision", "(", "scores", ",", "labels", ")", ":", "\n", "    ", "\"\"\"Mean average precision for multi-label recognition. Used for reporting\n    MMIT style mAP on Multi-Moments in Times. The difference is that this\n    method calculates average-precision for each sample and averages them among\n    samples.\n\n    Args:\n        scores (list[np.ndarray]): Prediction scores of different classes for\n            each sample.\n        labels (list[np.ndarray]): Ground truth many-hot vector for each\n            sample.\n\n    Returns:\n        np.float: The MMIT style mean average precision.\n    \"\"\"", "\n", "results", "=", "[", "]", "\n", "for", "score", ",", "label", "in", "zip", "(", "scores", ",", "labels", ")", ":", "\n", "        ", "precision", ",", "recall", ",", "_", "=", "binary_precision_recall_curve", "(", "score", ",", "label", ")", "\n", "ap", "=", "-", "np", ".", "sum", "(", "np", ".", "diff", "(", "recall", ")", "*", "np", ".", "array", "(", "precision", ")", "[", ":", "-", "1", "]", ")", "\n", "results", ".", "append", "(", "ap", ")", "\n", "", "return", "np", ".", "mean", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.mean_average_precision": [[175, 199], ["zip", "numpy.mean", "numpy.stack", "numpy.stack", "accuracy.binary_precision_recall_curve", "results.append", "numpy.sum", "numpy.isnan", "numpy.diff", "numpy.array"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.binary_precision_recall_curve"], ["", "def", "mean_average_precision", "(", "scores", ",", "labels", ")", ":", "\n", "    ", "\"\"\"Mean average precision for multi-label recognition.\n\n    Args:\n        scores (list[np.ndarray]): Prediction scores of different classes for\n            each sample.\n        labels (list[np.ndarray]): Ground truth many-hot vector for each\n            sample.\n\n    Returns:\n        np.float: The mean average precision.\n    \"\"\"", "\n", "results", "=", "[", "]", "\n", "scores", "=", "np", ".", "stack", "(", "scores", ")", ".", "T", "\n", "labels", "=", "np", ".", "stack", "(", "labels", ")", ".", "T", "\n", "\n", "for", "score", ",", "label", "in", "zip", "(", "scores", ",", "labels", ")", ":", "\n", "        ", "precision", ",", "recall", ",", "_", "=", "binary_precision_recall_curve", "(", "score", ",", "label", ")", "\n", "ap", "=", "-", "np", ".", "sum", "(", "np", ".", "diff", "(", "recall", ")", "*", "np", ".", "array", "(", "precision", ")", "[", ":", "-", "1", "]", ")", "\n", "results", ".", "append", "(", "ap", ")", "\n", "", "results", "=", "[", "x", "for", "x", "in", "results", "if", "not", "np", ".", "isnan", "(", "x", ")", "]", "\n", "if", "results", "==", "[", "]", ":", "\n", "        ", "return", "np", ".", "nan", "\n", "", "return", "np", ".", "mean", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.binary_precision_recall_curve": [[201, 243], ["isinstance", "isinstance", "tps.searchsorted", "slice", "numpy.argsort", "numpy.where", "numpy.cumsum", "numpy.diff", "numpy.isnan"], "function", ["None"], ["", "def", "binary_precision_recall_curve", "(", "y_score", ",", "y_true", ")", ":", "\n", "    ", "\"\"\"Calculate the binary precision recall curve at step thresholds.\n\n    Args:\n        y_score (np.ndarray): Prediction scores for each class.\n            Shape should be (num_classes, ).\n        y_true (np.ndarray): Ground truth many-hot vector.\n            Shape should be (num_classes, ).\n\n    Returns:\n        precision (np.ndarray): The precision of different thresholds.\n        recall (np.ndarray): The recall of different thresholds.\n        thresholds (np.ndarray): Different thresholds at which precision and\n            recall are tested.\n    \"\"\"", "\n", "assert", "isinstance", "(", "y_score", ",", "np", ".", "ndarray", ")", "\n", "assert", "isinstance", "(", "y_true", ",", "np", ".", "ndarray", ")", "\n", "assert", "y_score", ".", "shape", "==", "y_true", ".", "shape", "\n", "\n", "# make y_true a boolean vector", "\n", "y_true", "=", "(", "y_true", "==", "1", ")", "\n", "# sort scores and corresponding truth values", "\n", "desc_score_indices", "=", "np", ".", "argsort", "(", "y_score", ",", "kind", "=", "'mergesort'", ")", "[", ":", ":", "-", "1", "]", "\n", "y_score", "=", "y_score", "[", "desc_score_indices", "]", "\n", "y_true", "=", "y_true", "[", "desc_score_indices", "]", "\n", "# There may be ties in values, therefore find the `distinct_value_inds`", "\n", "distinct_value_inds", "=", "np", ".", "where", "(", "np", ".", "diff", "(", "y_score", ")", ")", "[", "0", "]", "\n", "threshold_inds", "=", "np", ".", "r_", "[", "distinct_value_inds", ",", "y_true", ".", "size", "-", "1", "]", "\n", "# accumulate the true positives with decreasing threshold", "\n", "tps", "=", "np", ".", "cumsum", "(", "y_true", ")", "[", "threshold_inds", "]", "\n", "fps", "=", "1", "+", "threshold_inds", "-", "tps", "\n", "thresholds", "=", "y_score", "[", "threshold_inds", "]", "\n", "\n", "precision", "=", "tps", "/", "(", "tps", "+", "fps", ")", "\n", "precision", "[", "np", ".", "isnan", "(", "precision", ")", "]", "=", "0", "\n", "recall", "=", "tps", "/", "tps", "[", "-", "1", "]", "\n", "# stop when full recall attained", "\n", "# and reverse the outputs so recall is decreasing", "\n", "last_ind", "=", "tps", ".", "searchsorted", "(", "tps", "[", "-", "1", "]", ")", "\n", "sl", "=", "slice", "(", "last_ind", ",", "None", ",", "-", "1", ")", "\n", "\n", "return", "np", ".", "r_", "[", "precision", "[", "sl", "]", ",", "1", "]", ",", "np", ".", "r_", "[", "recall", "[", "sl", "]", ",", "0", "]", ",", "thresholds", "[", "sl", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.pairwise_temporal_iou": [[245, 303], ["numpy.empty", "range", "ValueError", "numpy.empty", "numpy.maximum", "numpy.minimum", "numpy.squeeze", "segments_intersection.astype", "numpy.squeeze", "segments_intersection.astype"], "function", ["None"], ["", "def", "pairwise_temporal_iou", "(", "candidate_segments", ",", "\n", "target_segments", ",", "\n", "calculate_overlap_self", "=", "False", ")", ":", "\n", "    ", "\"\"\"Compute intersection over union between segments.\n\n    Args:\n        candidate_segments (np.ndarray): 1-dim/2-dim array in format\n            ``[init, end]/[m x 2:=[init, end]]``.\n        target_segments (np.ndarray): 2-dim array in format\n            ``[n x 2:=[init, end]]``.\n        calculate_overlap_self (bool): Whether to calculate overlap_self\n            (union / candidate_length) or not. Default: False.\n\n    Returns:\n        t_iou (np.ndarray): 1-dim array [n] /\n            2-dim array [n x m] with IoU ratio.\n        t_overlap_self (np.ndarray, optional): 1-dim array [n] /\n            2-dim array [n x m] with overlap_self, returns when\n            calculate_overlap_self is True.\n    \"\"\"", "\n", "candidate_segments_ndim", "=", "candidate_segments", ".", "ndim", "\n", "if", "target_segments", ".", "ndim", "!=", "2", "or", "candidate_segments_ndim", "not", "in", "[", "1", ",", "2", "]", ":", "\n", "        ", "raise", "ValueError", "(", "'Dimension of arguments is incorrect'", ")", "\n", "\n", "", "if", "candidate_segments_ndim", "==", "1", ":", "\n", "        ", "candidate_segments", "=", "candidate_segments", "[", "np", ".", "newaxis", ",", ":", "]", "\n", "\n", "", "n", ",", "m", "=", "target_segments", ".", "shape", "[", "0", "]", ",", "candidate_segments", ".", "shape", "[", "0", "]", "\n", "t_iou", "=", "np", ".", "empty", "(", "(", "n", ",", "m", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "calculate_overlap_self", ":", "\n", "        ", "t_overlap_self", "=", "np", ".", "empty", "(", "(", "n", ",", "m", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "        ", "candidate_segment", "=", "candidate_segments", "[", "i", ",", ":", "]", "\n", "tt1", "=", "np", ".", "maximum", "(", "candidate_segment", "[", "0", "]", ",", "target_segments", "[", ":", ",", "0", "]", ")", "\n", "tt2", "=", "np", ".", "minimum", "(", "candidate_segment", "[", "1", "]", ",", "target_segments", "[", ":", ",", "1", "]", ")", "\n", "# Intersection including Non-negative overlap score.", "\n", "segments_intersection", "=", "(", "tt2", "-", "tt1", ")", ".", "clip", "(", "0", ")", "\n", "# Segment union.", "\n", "segments_union", "=", "(", "(", "target_segments", "[", ":", ",", "1", "]", "-", "target_segments", "[", ":", ",", "0", "]", ")", "+", "\n", "(", "candidate_segment", "[", "1", "]", "-", "candidate_segment", "[", "0", "]", ")", "-", "\n", "segments_intersection", ")", "\n", "# Compute overlap as the ratio of the intersection", "\n", "# over union of two segments.", "\n", "t_iou", "[", ":", ",", "i", "]", "=", "(", "segments_intersection", ".", "astype", "(", "float", ")", "/", "segments_union", ")", "\n", "if", "calculate_overlap_self", ":", "\n", "            ", "candidate_length", "=", "candidate_segment", "[", "1", "]", "-", "candidate_segment", "[", "0", "]", "\n", "t_overlap_self", "[", ":", ",", "i", "]", "=", "(", "\n", "segments_intersection", ".", "astype", "(", "float", ")", "/", "candidate_length", ")", "\n", "\n", "", "", "if", "candidate_segments_ndim", "==", "1", ":", "\n", "        ", "t_iou", "=", "np", ".", "squeeze", "(", "t_iou", ",", "axis", "=", "1", ")", "\n", "", "if", "calculate_overlap_self", ":", "\n", "        ", "if", "candidate_segments_ndim", "==", "1", ":", "\n", "            ", "t_overlap_self", "=", "np", ".", "squeeze", "(", "t_overlap_self", ",", "axis", "=", "1", ")", "\n", "", "return", "t_iou", ",", "t_overlap_self", "\n", "\n", "", "return", "t_iou", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.average_recall_at_avg_proposals": [[305, 427], ["numpy.linspace", "len", "numpy.empty", "numpy.empty", "numpy.empty", "enumerate", "np.empty.mean", "numpy.trapz", "this_video_proposals[].astype", "ground_truth_video_id[].astype", "numpy.minimum", "accuracy.pairwise_temporal_iou", "score_list.append", "enumerate", "float", "float", "proposals_video_id[].argsort", "score_list.append", "numpy.expand_dims", "numpy.expand_dims", "int", "numpy.arange", "numpy.minimum", "enumerate", "np.empty.sum", "np.empty.sum", "float", "float", "numpy.zeros", "float", "numpy.count_nonzero", "true_positives_temporal_iou[].sum"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.pairwise_temporal_iou"], ["", "def", "average_recall_at_avg_proposals", "(", "ground_truth", ",", "\n", "proposals", ",", "\n", "total_num_proposals", ",", "\n", "max_avg_proposals", "=", "None", ",", "\n", "temporal_iou_thresholds", "=", "np", ".", "linspace", "(", "\n", "0.5", ",", "0.95", ",", "10", ")", ")", ":", "\n", "    ", "\"\"\"Computes the average recall given an average number (percentile) of\n    proposals per video.\n\n    Args:\n        ground_truth (dict): Dict containing the ground truth instances.\n        proposals (dict): Dict containing the proposal instances.\n        total_num_proposals (int): Total number of proposals in the\n            proposal dict.\n        max_avg_proposals (int | None): Max number of proposals for one video.\n            Default: None.\n        temporal_iou_thresholds (np.ndarray): 1D array with temporal_iou\n            thresholds. Default: ``np.linspace(0.5, 0.95, 10)``.\n\n    Returns:\n        tuple([np.ndarray, np.ndarray, np.ndarray, float]):\n            (recall, average_recall, proposals_per_video, auc)\n            In recall, ``recall[i,j]`` is recall at i-th temporal_iou threshold\n            at the j-th average number (percentile) of average number of\n            proposals per video. The average_recall is recall averaged\n            over a list of temporal_iou threshold (1D array). This is\n            equivalent to ``recall.mean(axis=0)``. The ``proposals_per_video``\n            is the average number of proposals per video. The auc is the area\n            under ``AR@AN`` curve.\n    \"\"\"", "\n", "\n", "total_num_videos", "=", "len", "(", "ground_truth", ")", "\n", "\n", "if", "not", "max_avg_proposals", ":", "\n", "        ", "max_avg_proposals", "=", "float", "(", "total_num_proposals", ")", "/", "total_num_videos", "\n", "\n", "", "ratio", "=", "(", "max_avg_proposals", "*", "float", "(", "total_num_videos", ")", "/", "total_num_proposals", ")", "\n", "\n", "# For each video, compute temporal_iou scores among the retrieved proposals", "\n", "score_list", "=", "[", "]", "\n", "total_num_retrieved_proposals", "=", "0", "\n", "for", "video_id", "in", "ground_truth", ":", "\n", "# Get proposals for this video.", "\n", "        ", "proposals_video_id", "=", "proposals", "[", "video_id", "]", "\n", "this_video_proposals", "=", "proposals_video_id", "[", ":", ",", ":", "2", "]", "\n", "# Sort proposals by score.", "\n", "sort_idx", "=", "proposals_video_id", "[", ":", ",", "2", "]", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "\n", "this_video_proposals", "=", "this_video_proposals", "[", "sort_idx", ",", ":", "]", ".", "astype", "(", "\n", "np", ".", "float32", ")", "\n", "\n", "# Get ground-truth instances associated to this video.", "\n", "ground_truth_video_id", "=", "ground_truth", "[", "video_id", "]", "\n", "this_video_ground_truth", "=", "ground_truth_video_id", "[", ":", ",", ":", "2", "]", ".", "astype", "(", "\n", "np", ".", "float32", ")", "\n", "if", "this_video_proposals", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "            ", "n", "=", "this_video_ground_truth", ".", "shape", "[", "0", "]", "\n", "score_list", ".", "append", "(", "np", ".", "zeros", "(", "(", "n", ",", "1", ")", ")", ")", "\n", "continue", "\n", "\n", "", "if", "this_video_proposals", ".", "ndim", "!=", "2", ":", "\n", "            ", "this_video_proposals", "=", "np", ".", "expand_dims", "(", "this_video_proposals", ",", "axis", "=", "0", ")", "\n", "", "if", "this_video_ground_truth", ".", "ndim", "!=", "2", ":", "\n", "            ", "this_video_ground_truth", "=", "np", ".", "expand_dims", "(", "\n", "this_video_ground_truth", ",", "axis", "=", "0", ")", "\n", "\n", "", "num_retrieved_proposals", "=", "np", ".", "minimum", "(", "\n", "int", "(", "this_video_proposals", ".", "shape", "[", "0", "]", "*", "ratio", ")", ",", "\n", "this_video_proposals", ".", "shape", "[", "0", "]", ")", "\n", "total_num_retrieved_proposals", "+=", "num_retrieved_proposals", "\n", "this_video_proposals", "=", "this_video_proposals", "[", ":", "\n", "num_retrieved_proposals", ",", ":", "]", "\n", "\n", "# Compute temporal_iou scores.", "\n", "t_iou", "=", "pairwise_temporal_iou", "(", "this_video_proposals", ",", "\n", "this_video_ground_truth", ")", "\n", "score_list", ".", "append", "(", "t_iou", ")", "\n", "\n", "# Given that the length of the videos is really varied, we", "\n", "# compute the number of proposals in terms of a ratio of the total", "\n", "# proposals retrieved, i.e. average recall at a percentage of proposals", "\n", "# retrieved per video.", "\n", "\n", "# Computes average recall.", "\n", "", "pcn_list", "=", "np", ".", "arange", "(", "1", ",", "101", ")", "/", "100.0", "*", "(", "\n", "max_avg_proposals", "*", "float", "(", "total_num_videos", ")", "/", "\n", "total_num_retrieved_proposals", ")", "\n", "matches", "=", "np", ".", "empty", "(", "(", "total_num_videos", ",", "pcn_list", ".", "shape", "[", "0", "]", ")", ")", "\n", "positives", "=", "np", ".", "empty", "(", "total_num_videos", ")", "\n", "recall", "=", "np", ".", "empty", "(", "(", "temporal_iou_thresholds", ".", "shape", "[", "0", "]", ",", "pcn_list", ".", "shape", "[", "0", "]", ")", ")", "\n", "# Iterates over each temporal_iou threshold.", "\n", "for", "ridx", ",", "temporal_iou", "in", "enumerate", "(", "temporal_iou_thresholds", ")", ":", "\n", "# Inspect positives retrieved per video at different", "\n", "# number of proposals (percentage of the total retrieved).", "\n", "        ", "for", "i", ",", "score", "in", "enumerate", "(", "score_list", ")", ":", "\n", "# Total positives per video.", "\n", "            ", "positives", "[", "i", "]", "=", "score", ".", "shape", "[", "0", "]", "\n", "# Find proposals that satisfies minimum temporal_iou threshold.", "\n", "true_positives_temporal_iou", "=", "score", ">=", "temporal_iou", "\n", "# Get number of proposals as a percentage of total retrieved.", "\n", "pcn_proposals", "=", "np", ".", "minimum", "(", "\n", "(", "score", ".", "shape", "[", "1", "]", "*", "pcn_list", ")", ".", "astype", "(", "np", ".", "int", ")", ",", "score", ".", "shape", "[", "1", "]", ")", "\n", "\n", "for", "j", ",", "num_retrieved_proposals", "in", "enumerate", "(", "pcn_proposals", ")", ":", "\n", "# Compute the number of matches", "\n", "# for each percentage of the proposals", "\n", "                ", "matches", "[", "i", ",", "j", "]", "=", "np", ".", "count_nonzero", "(", "\n", "(", "true_positives_temporal_iou", "[", ":", ",", ":", "num_retrieved_proposals", "]", "\n", ")", ".", "sum", "(", "axis", "=", "1", ")", ")", "\n", "\n", "# Computes recall given the set of matches per video.", "\n", "", "", "recall", "[", "ridx", ",", ":", "]", "=", "matches", ".", "sum", "(", "axis", "=", "0", ")", "/", "positives", ".", "sum", "(", ")", "\n", "\n", "# Recall is averaged.", "\n", "", "avg_recall", "=", "recall", ".", "mean", "(", "axis", "=", "0", ")", "\n", "\n", "# Get the average number of proposals per video.", "\n", "proposals_per_video", "=", "pcn_list", "*", "(", "\n", "float", "(", "total_num_retrieved_proposals", ")", "/", "total_num_videos", ")", "\n", "# Get AUC", "\n", "area_under_curve", "=", "np", ".", "trapz", "(", "avg_recall", ",", "proposals_per_video", ")", "\n", "auc", "=", "100.", "*", "float", "(", "area_under_curve", ")", "/", "proposals_per_video", "[", "-", "1", "]", "\n", "return", "recall", ",", "avg_recall", ",", "proposals_per_video", ",", "auc", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.get_weighted_score": [[429, 454], ["len", "range", "numpy.array", "numpy.array", "list", "len", "len", "len", "len", "numpy.dot"], "function", ["None"], ["", "def", "get_weighted_score", "(", "score_list", ",", "coeff_list", ")", ":", "\n", "    ", "\"\"\"Get weighted score with given scores and coefficients.\n\n    Given n predictions by different classifier: [score_1, score_2, ...,\n    score_n] (score_list) and their coefficients: [coeff_1, coeff_2, ...,\n    coeff_n] (coeff_list), return weighted score: weighted_score =\n    score_1 * coeff_1 + score_2 * coeff_2 + ... + score_n * coeff_n\n\n    Args:\n        score_list (list[list[np.ndarray]]): List of list of scores, with shape\n            n(number of predictions) X num_samples X num_classes\n        coeff_list (list[float]): List of coefficients, with shape n.\n\n    Returns:\n        list[np.ndarray]: List of weighted scores.\n    \"\"\"", "\n", "assert", "len", "(", "score_list", ")", "==", "len", "(", "coeff_list", ")", "\n", "num_samples", "=", "len", "(", "score_list", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "score_list", ")", ")", ":", "\n", "        ", "assert", "len", "(", "score_list", "[", "i", "]", ")", "==", "num_samples", "\n", "\n", "", "scores", "=", "np", ".", "array", "(", "score_list", ")", "# (num_coeff, num_samples, num_classes)", "\n", "coeff", "=", "np", ".", "array", "(", "coeff_list", ")", "# (num_coeff, )", "\n", "weighted_scores", "=", "list", "(", "np", ".", "dot", "(", "scores", ".", "T", ",", "coeff", ")", ".", "T", ")", "\n", "return", "weighted_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.softmax": [[456, 460], ["numpy.exp", "np.exp.sum", "numpy.max"], "function", ["None"], ["", "def", "softmax", "(", "x", ",", "dim", "=", "1", ")", ":", "\n", "    ", "\"\"\"Compute softmax values for each sets of scores in x.\"\"\"", "\n", "e_x", "=", "np", ".", "exp", "(", "x", "-", "np", ".", "max", "(", "x", ",", "axis", "=", "dim", ",", "keepdims", "=", "True", ")", ")", "\n", "return", "e_x", "/", "e_x", ".", "sum", "(", "axis", "=", "dim", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.interpolated_precision_recall": [[462, 479], ["numpy.hstack", "numpy.hstack", "numpy.sum", "range", "max", "numpy.where", "len"], "function", ["None"], ["", "def", "interpolated_precision_recall", "(", "precision", ",", "recall", ")", ":", "\n", "    ", "\"\"\"Interpolated AP - VOCdevkit from VOC 2011.\n\n    Args:\n        precision (np.ndarray): The precision of different thresholds.\n        recall (np.ndarray): The recall of different thresholds.\n\n    Returns\uff1a\n        float: Average precision score.\n    \"\"\"", "\n", "mprecision", "=", "np", ".", "hstack", "(", "[", "[", "0", "]", ",", "precision", ",", "[", "0", "]", "]", ")", "\n", "mrecall", "=", "np", ".", "hstack", "(", "[", "[", "0", "]", ",", "recall", ",", "[", "1", "]", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "mprecision", ")", "-", "1", ")", "[", ":", ":", "-", "1", "]", ":", "\n", "        ", "mprecision", "[", "i", "]", "=", "max", "(", "mprecision", "[", "i", "]", ",", "mprecision", "[", "i", "+", "1", "]", ")", "\n", "", "idx", "=", "np", ".", "where", "(", "mrecall", "[", "1", ":", ":", "]", "!=", "mrecall", "[", "0", ":", "-", "1", "]", ")", "[", "0", "]", "+", "1", "\n", "ap", "=", "np", ".", "sum", "(", "(", "mrecall", "[", "idx", "]", "-", "mrecall", "[", "idx", "-", "1", "]", ")", "*", "mprecision", "[", "idx", "]", ")", "\n", "return", "ap", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.average_precision_at_temporal_iou": [[481, 565], ["numpy.linspace", "numpy.zeros", "dict", "numpy.array", "prediction[].astype", "numpy.zeros", "numpy.zeros", "enumerate", "numpy.cumsum().astype", "numpy.cumsum().astype", "range", "len", "len", "len", "numpy.argsort", "accuracy.pairwise_temporal_iou", "enumerate", "len", "accuracy.interpolated_precision_recall", "numpy.ones", "len", "len", "len", "len", "numpy.array", "this_pred[].astype", "pairwise_temporal_iou.argsort", "numpy.cumsum", "numpy.cumsum", "len", "len"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.pairwise_temporal_iou", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.accuracy.interpolated_precision_recall"], ["", "def", "average_precision_at_temporal_iou", "(", "ground_truth", ",", "\n", "prediction", ",", "\n", "temporal_iou_thresholds", "=", "(", "np", ".", "linspace", "(", "\n", "0.5", ",", "0.95", ",", "10", ")", ")", ")", ":", "\n", "    ", "\"\"\"Compute average precision (in detection task) between ground truth and\n    predicted data frames. If multiple predictions match the same predicted\n    segment, only the one with highest score is matched as true positive. This\n    code is greatly inspired by Pascal VOC devkit.\n\n    Args:\n        ground_truth (dict): Dict containing the ground truth instances.\n            Key: 'video_id'\n            Value (np.ndarray): 1D array of 't-start' and 't-end'.\n        prediction (np.ndarray): 2D array containing the information of\n            proposal instances, including 'video_id', 'class_id', 't-start',\n            't-end' and 'score'.\n        temporal_iou_thresholds (np.ndarray): 1D array with temporal_iou\n            thresholds. Default: ``np.linspace(0.5, 0.95, 10)``.\n\n    Returns:\n        np.ndarray: 1D array of average precision score.\n    \"\"\"", "\n", "ap", "=", "np", ".", "zeros", "(", "len", "(", "temporal_iou_thresholds", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "prediction", ")", "<", "1", ":", "\n", "        ", "return", "ap", "\n", "\n", "", "num_gts", "=", "0.", "\n", "lock_gt", "=", "dict", "(", ")", "\n", "for", "key", "in", "ground_truth", ":", "\n", "        ", "lock_gt", "[", "key", "]", "=", "np", ".", "ones", "(", "\n", "(", "len", "(", "temporal_iou_thresholds", ")", ",", "len", "(", "ground_truth", "[", "key", "]", ")", ")", ")", "*", "-", "1", "\n", "num_gts", "+=", "len", "(", "ground_truth", "[", "key", "]", ")", "\n", "\n", "# Sort predictions by decreasing score order.", "\n", "", "prediction", "=", "np", ".", "array", "(", "prediction", ")", "\n", "scores", "=", "prediction", "[", ":", ",", "4", "]", ".", "astype", "(", "float", ")", "\n", "sort_idx", "=", "np", ".", "argsort", "(", "scores", ")", "[", ":", ":", "-", "1", "]", "\n", "prediction", "=", "prediction", "[", "sort_idx", "]", "\n", "\n", "# Initialize true positive and false positive vectors.", "\n", "tp", "=", "np", ".", "zeros", "(", "(", "len", "(", "temporal_iou_thresholds", ")", ",", "len", "(", "prediction", ")", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "fp", "=", "np", ".", "zeros", "(", "(", "len", "(", "temporal_iou_thresholds", ")", ",", "len", "(", "prediction", ")", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Assigning true positive to truly grount truth instances.", "\n", "for", "idx", ",", "this_pred", "in", "enumerate", "(", "prediction", ")", ":", "\n", "\n", "# Check if there is at least one ground truth in the video.", "\n", "        ", "if", "this_pred", "[", "0", "]", "in", "ground_truth", ":", "\n", "            ", "this_gt", "=", "np", ".", "array", "(", "ground_truth", "[", "this_pred", "[", "0", "]", "]", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "            ", "fp", "[", ":", ",", "idx", "]", "=", "1", "\n", "continue", "\n", "\n", "", "t_iou", "=", "pairwise_temporal_iou", "(", "this_pred", "[", "2", ":", "4", "]", ".", "astype", "(", "float", ")", ",", "this_gt", ")", "\n", "# We would like to retrieve the predictions with highest t_iou score.", "\n", "t_iou_sorted_idx", "=", "t_iou", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "t_idx", ",", "t_iou_threshold", "in", "enumerate", "(", "temporal_iou_thresholds", ")", ":", "\n", "            ", "for", "jdx", "in", "t_iou_sorted_idx", ":", "\n", "                ", "if", "t_iou", "[", "jdx", "]", "<", "t_iou_threshold", ":", "\n", "                    ", "fp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "break", "\n", "", "if", "lock_gt", "[", "this_pred", "[", "0", "]", "]", "[", "t_idx", ",", "jdx", "]", ">=", "0", ":", "\n", "                    ", "continue", "\n", "# Assign as true positive after the filters above.", "\n", "", "tp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "lock_gt", "[", "this_pred", "[", "0", "]", "]", "[", "t_idx", ",", "jdx", "]", "=", "idx", "\n", "break", "\n", "\n", "", "if", "fp", "[", "t_idx", ",", "idx", "]", "==", "0", "and", "tp", "[", "t_idx", ",", "idx", "]", "==", "0", ":", "\n", "                ", "fp", "[", "t_idx", ",", "idx", "]", "=", "1", "\n", "\n", "", "", "", "tp_cumsum", "=", "np", ".", "cumsum", "(", "tp", ",", "axis", "=", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "fp_cumsum", "=", "np", ".", "cumsum", "(", "fp", ",", "axis", "=", "1", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "recall_cumsum", "=", "tp_cumsum", "/", "num_gts", "\n", "\n", "precision_cumsum", "=", "tp_cumsum", "/", "(", "tp_cumsum", "+", "fp_cumsum", ")", "\n", "\n", "for", "t_idx", "in", "range", "(", "len", "(", "temporal_iou_thresholds", ")", ")", ":", "\n", "        ", "ap", "[", "t_idx", "]", "=", "interpolated_precision_recall", "(", "precision_cumsum", "[", "t_idx", ",", ":", "]", ",", "\n", "recall_cumsum", "[", "t_idx", ",", ":", "]", ")", "\n", "\n", "", "return", "ap", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.det2csv": [[13, 31], ["range", "len", "enumerate", "tuple", "csv_results.append", "bbox.tolist"], "function", ["None"], ["def", "det2csv", "(", "dataset", ",", "results", ",", "custom_classes", ")", ":", "\n", "    ", "csv_results", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "dataset", ")", ")", ":", "\n", "        ", "video_id", "=", "dataset", ".", "video_infos", "[", "idx", "]", "[", "'video_id'", "]", "\n", "timestamp", "=", "dataset", ".", "video_infos", "[", "idx", "]", "[", "'timestamp'", "]", "\n", "result", "=", "results", "[", "idx", "]", "\n", "for", "label", ",", "_", "in", "enumerate", "(", "result", ")", ":", "\n", "            ", "for", "bbox", "in", "result", "[", "label", "]", ":", "\n", "                ", "bbox_", "=", "tuple", "(", "bbox", ".", "tolist", "(", ")", ")", "\n", "if", "custom_classes", "is", "not", "None", ":", "\n", "                    ", "actual_label", "=", "custom_classes", "[", "label", "+", "1", "]", "\n", "", "else", ":", "\n", "                    ", "actual_label", "=", "label", "+", "1", "\n", "", "csv_results", ".", "append", "(", "(", "\n", "video_id", ",", "\n", "timestamp", ",", "\n", ")", "+", "bbox_", "[", ":", "4", "]", "+", "(", "actual_label", ",", ")", "+", "bbox_", "[", "4", ":", "]", ")", "\n", "", "", "", "return", "csv_results", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.results2csv": [[34, 48], ["isinstance", "ava_utils.det2csv", "isinstance", "str", "open", "f.write", "f.write", "map"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.det2csv"], ["", "def", "results2csv", "(", "dataset", ",", "results", ",", "out_file", ",", "custom_classes", "=", "None", ")", ":", "\n", "    ", "if", "isinstance", "(", "results", "[", "0", "]", ",", "list", ")", ":", "\n", "        ", "csv_results", "=", "det2csv", "(", "dataset", ",", "results", ",", "custom_classes", ")", "\n", "\n", "# save space for float", "\n", "", "def", "to_str", "(", "item", ")", ":", "\n", "        ", "if", "isinstance", "(", "item", ",", "float", ")", ":", "\n", "            ", "return", "f'{item:.3f}'", "\n", "", "return", "str", "(", "item", ")", "\n", "\n", "", "with", "open", "(", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "csv_result", "in", "csv_results", ":", "\n", "            ", "f", ".", "write", "(", "','", ".", "join", "(", "map", "(", "to_str", ",", "csv_result", ")", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time": [[50, 52], ["print", "time.time"], "function", ["None"], ["", "", "", "def", "print_time", "(", "message", ",", "start", ")", ":", "\n", "    ", "print", "(", "'==> %g seconds to %s'", "%", "(", "time", ".", "time", "(", ")", "-", "start", ",", "message", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.make_image_key": [[54, 57], ["int"], "function", ["None"], ["", "def", "make_image_key", "(", "video_id", ",", "timestamp", ")", ":", "\n", "    ", "\"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"", "\n", "return", "f'{video_id},{int(timestamp):04d}'", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_csv": [[59, 107], ["time.time", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "csv.reader", "ava_utils.print_time", "ava_utils.make_image_key", "int", "entries[].append", "sorted", "len", "float", "len", "float"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.make_image_key"], ["", "def", "read_csv", "(", "csv_file", ",", "class_whitelist", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n\n    CSV file format described at https://research.google.com/ava/download.html.\n\n    Args:\n        csv_file: A file object.\n        class_whitelist: If provided, boxes corresponding to (integer) class\n        labels not in this set are skipped.\n\n    Returns:\n        boxes: A dictionary mapping each unique image key (string) to a list of\n        boxes, given as coordinates [y1, x1, y2, x2].\n        labels: A dictionary mapping each unique image key (string) to a list\n        of integer class labels, matching the corresponding box in `boxes`.\n        scores: A dictionary mapping each unique image key (string) to a list\n        of score values labels, matching the corresponding label in `labels`.\n        If scores are not provided in the csv, then they will default to 1.0.\n    \"\"\"", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "entries", "=", "defaultdict", "(", "list", ")", "\n", "boxes", "=", "defaultdict", "(", "list", ")", "\n", "labels", "=", "defaultdict", "(", "list", ")", "\n", "scores", "=", "defaultdict", "(", "list", ")", "\n", "reader", "=", "csv", ".", "reader", "(", "csv_file", ")", "\n", "for", "row", "in", "reader", ":", "\n", "        ", "assert", "len", "(", "row", ")", "in", "[", "7", ",", "8", "]", ",", "'Wrong number of columns: '", "+", "row", "\n", "image_key", "=", "make_image_key", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ")", "\n", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "[", "float", "(", "n", ")", "for", "n", "in", "row", "[", "2", ":", "6", "]", "]", "\n", "action_id", "=", "int", "(", "row", "[", "6", "]", ")", "\n", "if", "class_whitelist", "and", "action_id", "not", "in", "class_whitelist", ":", "\n", "            ", "continue", "\n", "\n", "", "score", "=", "1.0", "\n", "if", "len", "(", "row", ")", "==", "8", ":", "\n", "            ", "score", "=", "float", "(", "row", "[", "7", "]", ")", "\n", "\n", "", "entries", "[", "image_key", "]", ".", "append", "(", "(", "score", ",", "action_id", ",", "y1", ",", "x1", ",", "y2", ",", "x2", ")", ")", "\n", "\n", "", "for", "image_key", "in", "entries", ":", "\n", "# Evaluation API assumes boxes with descending scores", "\n", "        ", "entry", "=", "sorted", "(", "entries", "[", "image_key", "]", ",", "key", "=", "lambda", "tup", ":", "-", "tup", "[", "0", "]", ")", "\n", "boxes", "[", "image_key", "]", "=", "[", "x", "[", "2", ":", "]", "for", "x", "in", "entry", "]", "\n", "labels", "[", "image_key", "]", "=", "[", "x", "[", "1", "]", "for", "x", "in", "entry", "]", "\n", "scores", "[", "image_key", "]", "=", "[", "x", "[", "0", "]", "for", "x", "in", "entry", "]", "\n", "\n", "", "print_time", "(", "'read file '", "+", "csv_file", ".", "name", ",", "start", ")", "\n", "return", "boxes", ",", "labels", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_exclusions": [[109, 127], ["set", "csv.reader", "set.add", "len", "ava_utils.make_image_key"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.make_image_key"], ["", "def", "read_exclusions", "(", "exclusions_file", ")", ":", "\n", "    ", "\"\"\"Reads a CSV file of excluded timestamps.\n\n    Args:\n        exclusions_file: A file object containing a csv of video-id,timestamp.\n\n    Returns:\n        A set of strings containing excluded image keys, e.g.\n        \"aaaaaaaaaaa,0904\",\n        or an empty set if exclusions file is None.\n    \"\"\"", "\n", "excluded", "=", "set", "(", ")", "\n", "if", "exclusions_file", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "exclusions_file", ")", "\n", "", "for", "row", "in", "reader", ":", "\n", "        ", "assert", "len", "(", "row", ")", "==", "2", ",", "'Expected only 2 columns, got: '", "+", "row", "\n", "excluded", ".", "add", "(", "make_image_key", "(", "row", "[", "0", "]", ",", "row", "[", "1", "]", ")", ")", "\n", "", "return", "excluded", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_labelmap": [[129, 153], ["set", "line.startswith", "line.split", "line.startswith", "line.startswith", "int", "labelmap.append", "set.add", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["", "def", "read_labelmap", "(", "labelmap_file", ")", ":", "\n", "    ", "\"\"\"Reads a labelmap without the dependency on protocol buffers.\n\n    Args:\n        labelmap_file: A file object containing a label map protocol buffer.\n\n    Returns:\n        labelmap: The label map in the form used by the\n        object_detection_evaluation\n        module - a list of {\"id\": integer, \"name\": classname } dicts.\n        class_ids: A set containing all of the valid class id integers.\n    \"\"\"", "\n", "labelmap", "=", "[", "]", "\n", "class_ids", "=", "set", "(", ")", "\n", "name", "=", "''", "\n", "class_id", "=", "''", "\n", "for", "line", "in", "labelmap_file", ":", "\n", "        ", "if", "line", ".", "startswith", "(", "'  name:'", ")", ":", "\n", "            ", "name", "=", "line", ".", "split", "(", "'\"'", ")", "[", "1", "]", "\n", "", "elif", "line", ".", "startswith", "(", "'  id:'", ")", "or", "line", ".", "startswith", "(", "'  label_id:'", ")", ":", "\n", "            ", "class_id", "=", "int", "(", "line", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "[", "-", "1", "]", ")", "\n", "labelmap", ".", "append", "(", "{", "'id'", ":", "class_id", ",", "'name'", ":", "name", "}", ")", "\n", "class_ids", ".", "add", "(", "class_id", ")", "\n", "", "", "return", "labelmap", ",", "class_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.ava_eval": [[156, 237], ["time.time", "ava_utils.read_labelmap", "ava_utils.read_csv", "time.time", "ava_utils.read_csv", "ava_evaluation.object_detection_evaluation.PascalDetectionEvaluator", "time.time", "time.time", "time.time", "det_eval.PascalDetectionEvaluator.evaluate", "open", "set().issubset", "open", "ava_utils.print_time", "ava_utils.read_exclusions", "list", "open", "ava_utils.print_time", "det_eval.PascalDetectionEvaluator.add_single_ground_truth_image_info", "ava_utils.print_time", "det_eval.PascalDetectionEvaluator.add_single_detected_image_info", "ava_utils.print_time", "ava_utils.print_time", "print", "set", "open", "logging.info", "logging.info", "set", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_labelmap", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_csv", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_csv", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.read_exclusions", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.evaluation.ava_utils.print_time", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["", "def", "ava_eval", "(", "result_file", ",", "\n", "result_type", ",", "\n", "label_file", ",", "\n", "ann_file", ",", "\n", "exclude_file", ",", "\n", "verbose", "=", "True", ",", "\n", "custom_classes", "=", "None", ")", ":", "\n", "\n", "    ", "assert", "result_type", "in", "[", "'mAP'", "]", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "categories", ",", "class_whitelist", "=", "read_labelmap", "(", "open", "(", "label_file", ")", ")", "\n", "if", "custom_classes", "is", "not", "None", ":", "\n", "        ", "custom_classes", "=", "custom_classes", "[", "1", ":", "]", "\n", "assert", "set", "(", "custom_classes", ")", ".", "issubset", "(", "set", "(", "class_whitelist", ")", ")", "\n", "class_whitelist", "=", "custom_classes", "\n", "categories", "=", "[", "cat", "for", "cat", "in", "categories", "if", "cat", "[", "'id'", "]", "in", "custom_classes", "]", "\n", "\n", "# loading gt, do not need gt score", "\n", "", "gt_boxes", ",", "gt_labels", ",", "_", "=", "read_csv", "(", "open", "(", "ann_file", ")", ",", "class_whitelist", ")", "\n", "if", "verbose", ":", "\n", "        ", "print_time", "(", "'Reading detection results'", ",", "start", ")", "\n", "\n", "", "if", "exclude_file", "is", "not", "None", ":", "\n", "        ", "excluded_keys", "=", "read_exclusions", "(", "open", "(", "exclude_file", ")", ")", "\n", "", "else", ":", "\n", "        ", "excluded_keys", "=", "list", "(", ")", "\n", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "boxes", ",", "labels", ",", "scores", "=", "read_csv", "(", "open", "(", "result_file", ")", ",", "class_whitelist", ")", "\n", "if", "verbose", ":", "\n", "        ", "print_time", "(", "'Reading detection results'", ",", "start", ")", "\n", "\n", "# Evaluation for mAP", "\n", "", "pascal_evaluator", "=", "det_eval", ".", "PascalDetectionEvaluator", "(", "categories", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "for", "image_key", "in", "gt_boxes", ":", "\n", "        ", "if", "verbose", "and", "image_key", "in", "excluded_keys", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "'Found excluded timestamp in detections: %s.'", "\n", "'It will be ignored.'", ",", "image_key", ")", "\n", "continue", "\n", "", "pascal_evaluator", ".", "add_single_ground_truth_image_info", "(", "\n", "image_key", ",", "{", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_boxes", ":", "\n", "np", ".", "array", "(", "gt_boxes", "[", "image_key", "]", ",", "dtype", "=", "float", ")", ",", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_classes", ":", "\n", "np", ".", "array", "(", "gt_labels", "[", "image_key", "]", ",", "dtype", "=", "int", ")", "\n", "}", ")", "\n", "", "if", "verbose", ":", "\n", "        ", "print_time", "(", "'Convert groundtruth'", ",", "start", ")", "\n", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "for", "image_key", "in", "boxes", ":", "\n", "        ", "if", "verbose", "and", "image_key", "in", "excluded_keys", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "'Found excluded timestamp in detections: %s.'", "\n", "'It will be ignored.'", ",", "image_key", ")", "\n", "continue", "\n", "", "pascal_evaluator", ".", "add_single_detected_image_info", "(", "\n", "image_key", ",", "{", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_boxes", ":", "\n", "np", ".", "array", "(", "boxes", "[", "image_key", "]", ",", "dtype", "=", "float", ")", ",", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_classes", ":", "\n", "np", ".", "array", "(", "labels", "[", "image_key", "]", ",", "dtype", "=", "int", ")", ",", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_scores", ":", "\n", "np", ".", "array", "(", "scores", "[", "image_key", "]", ",", "dtype", "=", "float", ")", "\n", "}", ")", "\n", "", "if", "verbose", ":", "\n", "        ", "print_time", "(", "'convert detections'", ",", "start", ")", "\n", "\n", "", "start", "=", "time", ".", "time", "(", ")", "\n", "metrics", "=", "pascal_evaluator", ".", "evaluate", "(", ")", "\n", "if", "verbose", ":", "\n", "        ", "print_time", "(", "'run_evaluator'", ",", "start", ")", "\n", "", "for", "display_name", "in", "metrics", ":", "\n", "        ", "print", "(", "f'{display_name}=\\t{metrics[display_name]}'", ")", "\n", "", "return", "{", "\n", "display_name", ":", "metrics", "[", "display_name", "]", "\n", "for", "display_name", "in", "metrics", "if", "'ByCategory'", "not", "in", "display_name", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation.__init__": [[31, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_groundtruth_classes", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"Initialized PerImageEvaluation by evaluation parameters.\n\n        Args:\n            num_groundtruth_classes: Number of ground truth object classes\n            matching_iou_threshold: A ratio of area intersection to union,\n                which is the threshold to consider whether a detection is true\n                positive or not\n        \"\"\"", "\n", "self", ".", "matching_iou_threshold", "=", "matching_iou_threshold", "\n", "self", ".", "num_groundtruth_classes", "=", "num_groundtruth_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation.compute_object_detection_metrics": [[43, 105], ["per_image_evaluation.PerImageEvaluation._remove_invalid_boxes", "per_image_evaluation.PerImageEvaluation._compute_tp_fp"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._remove_invalid_boxes", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp"], ["", "def", "compute_object_detection_metrics", "(", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Evaluates detections as being tp, fp or ignored from a single image.\n\n        The evaluation is done in two stages:\n        1. All detections are matched to non group-of boxes.\n\n        Args:\n            detected_boxes: A float numpy array of shape [N, 4], representing N\n                regions of detected object regions.\n                Each row is of the format [y_min, x_min, y_max, x_max]\n            detected_scores: A float numpy array of shape [N, 1], representing\n                the confidence scores of the detected N object instances.\n            detected_class_labels: A integer numpy array of shape [N, 1],\n                repreneting the class labels of the detected N object\n                instances.\n            groundtruth_boxes: A float numpy array of shape [M, 4],\n                representing M regions of object instances in ground truth\n            groundtruth_class_labels: An integer numpy array of shape [M, 1],\n                representing M class labels of object instances in ground truth\n            detected_masks: (optional) A uint8 numpy array of shape\n                [N, height, width]. If not None, the metrics will be computed\n                based on masks.\n            groundtruth_masks: (optional) A uint8 numpy array of shape\n                [M, height, width].\n\n        Returns:\n            scores: A list of C float numpy arrays. Each numpy array is of\n                shape [K, 1], representing K scores detected with object class\n                label c\n            tp_fp_labels: A list of C boolean numpy arrays. Each numpy array\n                is of shape [K, 1], representing K True/False positive label of\n                object instances detected with class label c\n        \"\"\"", "\n", "(", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", ",", "\n", ")", "=", "self", ".", "_remove_invalid_boxes", "(", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", ",", "\n", ")", "\n", "scores", ",", "tp_fp_labels", "=", "self", ".", "_compute_tp_fp", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "detected_class_labels", "=", "detected_class_labels", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_class_labels", ",", "\n", "detected_masks", "=", "detected_masks", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "\n", "return", "scores", ",", "tp_fp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp": [[106, 174], ["range", "ValueError", "ValueError", "per_image_evaluation.PerImageEvaluation._get_ith_class_arrays", "per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class", "result_scores.append", "result_tp_fp_labels.append"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_ith_class_arrays", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class"], ["", "def", "_compute_tp_fp", "(", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Labels true/false positives of detections of an image across all\n        classes.\n\n        Args:\n            detected_boxes: A float numpy array of shape [N, 4], representing N\n                regions of detected object regions.\n                Each row is of the format [y_min, x_min, y_max, x_max]\n            detected_scores: A float numpy array of shape [N, 1], representing\n                the confidence scores of the detected N object instances.\n            detected_class_labels: A integer numpy array of shape [N, 1],\n                repreneting the class labels of the detected N object\n                instances.\n            groundtruth_boxes: A float numpy array of shape [M, 4],\n                representing M regions of object instances in ground truth\n            groundtruth_class_labels: An integer numpy array of shape [M, 1],\n                representing M class labels of object instances in ground truth\n            detected_masks: (optional) A np.uint8 numpy array of shape\n                [N, height, width]. If not None, the scores will be computed\n                based on masks.\n            groundtruth_masks: (optional) A np.uint8 numpy array of shape\n                [M, height, width].\n\n        Returns:\n            result_scores: A list of float numpy arrays. Each numpy array is of\n                shape [K, 1], representing K scores detected with object class\n                label c\n            result_tp_fp_labels: A list of boolean numpy array. Each numpy\n                array is of shape [K, 1], representing K True/False positive\n                label of object instances detected with class label c\n\n        Raises:\n            ValueError: If detected masks is not None but groundtruth masks are\n                None, or the other way around.\n        \"\"\"", "\n", "if", "detected_masks", "is", "not", "None", "and", "groundtruth_masks", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Detected masks is available but groundtruth masks is not.'", ")", "\n", "", "if", "detected_masks", "is", "None", "and", "groundtruth_masks", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Groundtruth masks is available but detected masks is not.'", ")", "\n", "\n", "", "result_scores", "=", "[", "]", "\n", "result_tp_fp_labels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_groundtruth_classes", ")", ":", "\n", "            ", "(", "gt_boxes_at_ith_class", ",", "gt_masks_at_ith_class", ",", "\n", "detected_boxes_at_ith_class", ",", "detected_scores_at_ith_class", ",", "\n", "detected_masks_at_ith_class", ")", "=", "self", ".", "_get_ith_class_arrays", "(", "\n", "detected_boxes", ",", "detected_scores", ",", "detected_masks", ",", "\n", "detected_class_labels", ",", "groundtruth_boxes", ",", "groundtruth_masks", ",", "\n", "groundtruth_class_labels", ",", "i", ")", "\n", "scores", ",", "tp_fp_labels", "=", "self", ".", "_compute_tp_fp_for_single_class", "(", "\n", "detected_boxes", "=", "detected_boxes_at_ith_class", ",", "\n", "detected_scores", "=", "detected_scores_at_ith_class", ",", "\n", "groundtruth_boxes", "=", "gt_boxes_at_ith_class", ",", "\n", "detected_masks", "=", "detected_masks_at_ith_class", ",", "\n", "groundtruth_masks", "=", "gt_masks_at_ith_class", ",", "\n", ")", "\n", "result_scores", ".", "append", "(", "scores", ")", "\n", "result_tp_fp_labels", ".", "append", "(", "tp_fp_labels", ")", "\n", "", "return", "result_scores", ",", "result_tp_fp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode": [[175, 207], ["np_box_list.BoxList", "np_box_list.BoxList.add_field", "np_box_list.BoxList", "np_box_ops.iou", "np_box_list.BoxList.get_field", "np_box_list.BoxList.num_boxes", "np_box_list.BoxList.get", "np_box_list.BoxList.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.add_field", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.iou", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get_field", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.num_boxes", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "@", "staticmethod", "\n", "def", "_get_overlaps_and_scores_box_mode", "(", "detected_boxes", ",", "detected_scores", ",", "\n", "groundtruth_boxes", ")", ":", "\n", "        ", "\"\"\"Computes overlaps and scores between detected and groudntruth boxes.\n\n        Args:\n            detected_boxes: A numpy array of shape [N, 4] representing detected\n                box coordinates\n            detected_scores: A 1-d numpy array of length N representing\n                classification score\n            groundtruth_boxes: A numpy array of shape [M, 4] representing\n                ground truth box coordinates\n\n        Returns:\n            iou: A float numpy array of size [num_detected_boxes,\n                num_gt_boxes]. If gt_non_group_of_boxlist.num_boxes() == 0 it\n                will be None.\n            ioa: A float numpy array of size [num_detected_boxes,\n                num_gt_boxes]. If gt_group_of_boxlist.num_boxes() == 0 it will\n                be None.\n            scores: The score of the detected boxlist.\n            num_boxes: Number of non-maximum suppressed detected boxes.\n        \"\"\"", "\n", "detected_boxlist", "=", "np_box_list", ".", "BoxList", "(", "detected_boxes", ")", "\n", "detected_boxlist", ".", "add_field", "(", "'scores'", ",", "detected_scores", ")", "\n", "gt_non_group_of_boxlist", "=", "np_box_list", ".", "BoxList", "(", "groundtruth_boxes", ")", "\n", "\n", "iou", "=", "np_box_ops", ".", "iou", "(", "detected_boxlist", ".", "get", "(", ")", ",", "\n", "gt_non_group_of_boxlist", ".", "get", "(", ")", ")", "\n", "scores", "=", "detected_boxlist", ".", "get_field", "(", "'scores'", ")", "\n", "num_boxes", "=", "detected_boxlist", ".", "num_boxes", "(", ")", "\n", "return", "iou", ",", "None", ",", "scores", ",", "num_boxes", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._compute_tp_fp_for_single_class": [[208, 269], ["per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode", "numpy.zeros", "numpy.argmax", "numpy.zeros", "range", "numpy.array", "numpy.array", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_overlaps_and_scores_box_mode"], ["", "def", "_compute_tp_fp_for_single_class", "(", "self", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "groundtruth_boxes", ",", "\n", "detected_masks", "=", "None", ",", "\n", "groundtruth_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Labels boxes detected with the same class from the same image as\n        tp/fp.\n\n        Args:\n            detected_boxes: A numpy array of shape [N, 4] representing detected\n                box coordinates\n            detected_scores: A 1-d numpy array of length N representing\n                classification score\n            groundtruth_boxes: A numpy array of shape [M, 4] representing\n                groundtruth box coordinates\n            detected_masks: (optional) A uint8 numpy array of shape\n                [N, height, width]. If not None, the scores will be computed\n                based on masks.\n            groundtruth_masks: (optional) A uint8 numpy array of shape\n                [M, height, width].\n\n        Returns:\n            Two arrays of the same size, containing all boxes that were\n            evaluated as being true positives or false positives.\n\n            scores: A numpy array representing the detection scores.\n            tp_fp_labels: a boolean numpy array indicating whether a detection\n                is a true positive.\n        \"\"\"", "\n", "if", "detected_boxes", ".", "size", "==", "0", ":", "\n", "            ", "return", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", ",", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "(", "iou", ",", "_", ",", "scores", ",", "\n", "num_detected_boxes", ")", "=", "self", ".", "_get_overlaps_and_scores_box_mode", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ")", "\n", "\n", "if", "groundtruth_boxes", ".", "size", "==", "0", ":", "\n", "            ", "return", "scores", ",", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "tp_fp_labels", "=", "np", ".", "zeros", "(", "num_detected_boxes", ",", "dtype", "=", "bool", ")", "\n", "\n", "# The evaluation is done in two stages:", "\n", "# 1. All detections are matched to non group-of boxes.", "\n", "# 2. Detections that are determined as false positives are matched", "\n", "#    against group-of boxes and ignored if matched.", "\n", "\n", "# Tp-fp evaluation for non-group of boxes (if any).", "\n", "if", "iou", ".", "shape", "[", "1", "]", ">", "0", ":", "\n", "            ", "max_overlap_gt_ids", "=", "np", ".", "argmax", "(", "iou", ",", "axis", "=", "1", ")", "\n", "is_gt_box_detected", "=", "np", ".", "zeros", "(", "iou", ".", "shape", "[", "1", "]", ",", "dtype", "=", "bool", ")", "\n", "for", "i", "in", "range", "(", "num_detected_boxes", ")", ":", "\n", "                ", "gt_id", "=", "max_overlap_gt_ids", "[", "i", "]", "\n", "if", "iou", "[", "i", ",", "gt_id", "]", ">=", "self", ".", "matching_iou_threshold", ":", "\n", "                    ", "if", "not", "is_gt_box_detected", "[", "gt_id", "]", ":", "\n", "                        ", "tp_fp_labels", "[", "i", "]", "=", "True", "\n", "is_gt_box_detected", "[", "gt_id", "]", "=", "True", "\n", "\n", "", "", "", "", "return", "scores", ",", "tp_fp_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._get_ith_class_arrays": [[270, 317], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_ith_class_arrays", "(", "detected_boxes", ",", "detected_scores", ",", "detected_masks", ",", "\n", "detected_class_labels", ",", "groundtruth_boxes", ",", "\n", "groundtruth_masks", ",", "groundtruth_class_labels", ",", "\n", "class_index", ")", ":", "\n", "        ", "\"\"\"Returns numpy arrays belonging to class with index `class_index`.\n\n        Args:\n            detected_boxes: A numpy array containing detected boxes.\n            detected_scores: A numpy array containing detected scores.\n            detected_masks: A numpy array containing detected masks.\n            detected_class_labels: A numpy array containing detected class\n                labels.\n            groundtruth_boxes: A numpy array containing groundtruth boxes.\n            groundtruth_masks: A numpy array containing groundtruth masks.\n            groundtruth_class_labels: A numpy array containing groundtruth\n                class labels.\n            class_index: An integer index.\n\n        Returns:\n            gt_boxes_at_ith_class: A numpy array containing groundtruth boxes\n                labeled as ith class.\n            gt_masks_at_ith_class: A numpy array containing groundtruth masks\n                labeled as ith class.\n            detected_boxes_at_ith_class: A numpy array containing detected\n                boxes corresponding to the ith class.\n            detected_scores_at_ith_class: A numpy array containing detected\n                scores corresponding to the ith class.\n            detected_masks_at_ith_class: A numpy array containing detected\n                masks corresponding to the ith class.\n        \"\"\"", "\n", "selected_groundtruth", "=", "groundtruth_class_labels", "==", "class_index", "\n", "gt_boxes_at_ith_class", "=", "groundtruth_boxes", "[", "selected_groundtruth", "]", "\n", "if", "groundtruth_masks", "is", "not", "None", ":", "\n", "            ", "gt_masks_at_ith_class", "=", "groundtruth_masks", "[", "selected_groundtruth", "]", "\n", "", "else", ":", "\n", "            ", "gt_masks_at_ith_class", "=", "None", "\n", "", "selected_detections", "=", "detected_class_labels", "==", "class_index", "\n", "detected_boxes_at_ith_class", "=", "detected_boxes", "[", "selected_detections", "]", "\n", "detected_scores_at_ith_class", "=", "detected_scores", "[", "selected_detections", "]", "\n", "if", "detected_masks", "is", "not", "None", ":", "\n", "            ", "detected_masks_at_ith_class", "=", "detected_masks", "[", "selected_detections", "]", "\n", "", "else", ":", "\n", "            ", "detected_masks_at_ith_class", "=", "None", "\n", "", "return", "(", "gt_boxes_at_ith_class", ",", "gt_masks_at_ith_class", ",", "\n", "detected_boxes_at_ith_class", ",", "detected_scores_at_ith_class", ",", "\n", "detected_masks_at_ith_class", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation._remove_invalid_boxes": [[318, 358], ["numpy.logical_and"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_remove_invalid_boxes", "(", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Removes entries with invalid boxes.\n\n        A box is invalid if either its xmax is smaller than its xmin, or its\n        ymax is smaller than its ymin.\n\n        Args:\n            detected_boxes: A float numpy array of size [num_boxes, 4]\n                containing box coordinates in [ymin, xmin, ymax, xmax] format.\n            detected_scores: A float numpy array of size [num_boxes].\n            detected_class_labels: A int32 numpy array of size [num_boxes].\n            detected_masks: A uint8 numpy array of size\n                [num_boxes, height, width].\n\n        Returns:\n            valid_detected_boxes: A float numpy array of size\n                [num_valid_boxes, 4] containing box coordinates in\n                [ymin, xmin, ymax, xmax] format.\n            valid_detected_scores: A float numpy array of size\n                [num_valid_boxes].\n            valid_detected_class_labels: A int32 numpy array of size\n                [num_valid_boxes].\n            valid_detected_masks: A uint8 numpy array of size\n                [num_valid_boxes, height, width].\n        \"\"\"", "\n", "valid_indices", "=", "np", ".", "logical_and", "(", "\n", "detected_boxes", "[", ":", ",", "0", "]", "<", "detected_boxes", "[", ":", ",", "2", "]", ",", "\n", "detected_boxes", "[", ":", ",", "1", "]", "<", "detected_boxes", "[", ":", ",", "3", "]", ")", "\n", "detected_boxes", "=", "detected_boxes", "[", "valid_indices", "]", "\n", "detected_scores", "=", "detected_scores", "[", "valid_indices", "]", "\n", "detected_class_labels", "=", "detected_class_labels", "[", "valid_indices", "]", "\n", "if", "detected_masks", "is", "not", "None", ":", "\n", "            ", "detected_masks", "=", "detected_masks", "[", "valid_indices", "]", "\n", "", "return", "[", "\n", "detected_boxes", ",", "detected_scores", ",", "detected_class_labels", ",", "\n", "detected_masks", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.__init__": [[32, 53], ["isinstance", "ValueError", "ValueError", "ValueError", "np_box_list.BoxList._is_valid_boxes", "ValueError", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList._is_valid_boxes"], ["def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"Constructs box collection.\n\n        Args:\n            data: a numpy array of shape [N, 4] representing box coordinates\n\n        Raises:\n            ValueError: if bbox data is not a numpy array\n            ValueError: if invalid dimensions for bbox data\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "data", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'data must be a numpy array.'", ")", "\n", "", "if", "len", "(", "data", ".", "shape", ")", "!=", "2", "or", "data", ".", "shape", "[", "1", "]", "!=", "4", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid dimensions for box data.'", ")", "\n", "", "if", "data", ".", "dtype", "!=", "np", ".", "float32", "and", "data", ".", "dtype", "!=", "np", ".", "float64", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Invalid data type for box data: float is required.'", ")", "\n", "", "if", "not", "self", ".", "_is_valid_boxes", "(", "data", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid box data. data must be a numpy array of '", "\n", "'N*[y_min, x_min, y_max, x_max]'", ")", "\n", "", "self", ".", "data", "=", "{", "'boxes'", ":", "data", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.num_boxes": [[54, 57], ["None"], "methods", ["None"], ["", "def", "num_boxes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return number of boxes held in collections.\"\"\"", "\n", "return", "self", ".", "data", "[", "'boxes'", "]", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get_extra_fields": [[58, 61], ["None"], "methods", ["None"], ["", "def", "get_extra_fields", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return all non-box fields.\"\"\"", "\n", "return", "[", "k", "for", "k", "in", "self", ".", "data", "if", "k", "!=", "'boxes'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.has_field": [[62, 64], ["None"], "methods", ["None"], ["", "def", "has_field", "(", "self", ",", "field", ")", ":", "\n", "        ", "return", "field", "in", "self", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.add_field": [[65, 83], ["np_box_list.BoxList.has_field", "ValueError", "ValueError", "len", "np_box_list.BoxList.num_boxes"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.has_field", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.num_boxes"], ["", "def", "add_field", "(", "self", ",", "field", ",", "field_data", ")", ":", "\n", "        ", "\"\"\"Add data to a specified field.\n\n        Args:\n            field: a string parameter used to specify a related field to be\n                accessed.\n            field_data: a numpy array of [N, ...] representing the data\n                associated with the field.\n        Raises:\n            ValueError: if the field is already exist or the dimension of the\n                field data does not matches the number of boxes.\n        \"\"\"", "\n", "if", "self", ".", "has_field", "(", "field", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Field '", "+", "field", "+", "'already exists'", ")", "\n", "", "if", "len", "(", "field_data", ".", "shape", ")", "<", "1", "or", "field_data", ".", "shape", "[", "0", "]", "!=", "self", ".", "num_boxes", "(", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid dimensions for field data'", ")", "\n", "", "self", ".", "data", "[", "field", "]", "=", "field_data", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get": [[84, 91], ["np_box_list.BoxList.get_field"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get_field"], ["", "def", "get", "(", "self", ")", ":", "\n", "        ", "\"\"\"Convenience function for accesssing box coordinates.\n\n        Returns:\n            a numpy array of shape [N, 4] representing box corners\n        \"\"\"", "\n", "return", "self", ".", "get_field", "(", "'boxes'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get_field": [[92, 109], ["np_box_list.BoxList.has_field", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.has_field"], ["", "def", "get_field", "(", "self", ",", "field", ")", ":", "\n", "        ", "\"\"\"Accesses data associated with the specified field in the box\n        collection.\n\n        Args:\n            field: a string parameter used to specify a related field to be\n                accessed.\n\n        Returns:\n            a numpy 1-d array representing data of an associated field\n\n        Raises:\n            ValueError: if invalid field\n        \"\"\"", "\n", "if", "not", "self", ".", "has_field", "(", "field", ")", ":", "\n", "            ", "raise", "ValueError", "(", "f'field {field} does not exist'", ")", "\n", "", "return", "self", ".", "data", "[", "field", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get_coordinates": [[110, 122], ["np_box_list.BoxList.get"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList.get"], ["", "def", "get_coordinates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get corner coordinates of boxes.\n\n        Returns:\n            a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n        \"\"\"", "\n", "box_coordinates", "=", "self", ".", "get", "(", ")", "\n", "y_min", "=", "box_coordinates", "[", ":", ",", "0", "]", "\n", "x_min", "=", "box_coordinates", "[", ":", ",", "1", "]", "\n", "y_max", "=", "box_coordinates", "[", ":", ",", "2", "]", "\n", "x_max", "=", "box_coordinates", "[", ":", ",", "3", "]", "\n", "return", "[", "y_min", ",", "x_min", ",", "y_max", ",", "x_max", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_list.BoxList._is_valid_boxes": [[123, 140], ["len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_is_valid_boxes", "(", "data", ")", ":", "\n", "        ", "\"\"\"Check whether data fulfills the format of N*[ymin, xmin, ymax,\n        xmin].\n\n        Args:\n            data: a numpy array of shape [N, 4] representing box coordinates\n\n        Returns:\n            a boolean indicating whether all ymax of boxes are equal or greater\n            than ymin, and all xmax of boxes are equal or greater than xmin.\n        \"\"\"", "\n", "if", "len", "(", "data", ")", "!=", "0", ":", "\n", "            ", "for", "v", "in", "data", ":", "\n", "                ", "if", "v", "[", "0", "]", ">", "v", "[", "2", "]", "or", "v", "[", "1", "]", ">", "v", "[", "3", "]", ":", "\n", "                    ", "return", "False", "\n", "", "", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_precision_recall": [[20, 66], ["numpy.argsort", "labels.astype.astype", "numpy.cumsum", "numpy.cumsum", "ValueError", "ValueError", "numpy.sum", "ValueError", "len", "len", "ValueError", "np.cumsum.astype", "np.cumsum.astype", "isinstance", "len", "isinstance", "len"], "function", ["None"], ["self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n", "\n", "", "", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "maxk", "=", "min", "(", "max", "(", "topk", ")", ",", "output", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "return", "[", "correct", "[", ":", "min", "(", "k", ",", "maxk", ")", "]", ".", "reshape", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ")", "*", "100.", "/", "batch_size", "for", "k", "in", "topk", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_average_precision": [[68, 117], ["numpy.concatenate", "numpy.concatenate", "range", "numpy.sum", "ValueError", "ValueError", "len", "len", "ValueError", "ValueError", "ValueError", "all", "ValueError", "numpy.maximum", "ValueError", "isinstance", "isinstance", "numpy.amin", "numpy.amax", "numpy.amin", "numpy.amax", "len", "numpy.where", "range", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_cor_loc": [[119, 143], ["numpy.errstate", "numpy.where"], "function", ["None"], []], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.area": [[25, 35], ["None"], "function", ["None"], ["def", "area", "(", "boxes", ")", ":", "\n", "    ", "\"\"\"Computes area of boxes.\n\n    Args:\n        boxes: Numpy array with shape [N, 4] holding N boxes\n\n    Returns:\n        a numpy array with shape [N*1] representing box areas\n    \"\"\"", "\n", "return", "(", "boxes", "[", ":", ",", "2", "]", "-", "boxes", "[", ":", ",", "0", "]", ")", "*", "(", "boxes", "[", ":", ",", "3", "]", "-", "boxes", "[", ":", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.intersection": [[37, 61], ["numpy.split", "numpy.split", "numpy.minimum", "numpy.maximum", "numpy.maximum", "numpy.minimum", "numpy.maximum", "numpy.maximum", "numpy.transpose", "numpy.transpose", "numpy.zeros", "numpy.transpose", "numpy.transpose", "numpy.zeros"], "function", ["None"], ["", "def", "intersection", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Compute pairwise intersection areas between boxes.\n\n    Args:\n        boxes1: a numpy array with shape [N, 4] holding N boxes\n        boxes2: a numpy array with shape [M, 4] holding M boxes\n\n    Returns:\n        a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"", "\n", "[", "y_min1", ",", "x_min1", ",", "y_max1", ",", "x_max1", "]", "=", "np", ".", "split", "(", "boxes1", ",", "4", ",", "axis", "=", "1", ")", "\n", "[", "y_min2", ",", "x_min2", ",", "y_max2", ",", "x_max2", "]", "=", "np", ".", "split", "(", "boxes2", ",", "4", ",", "axis", "=", "1", ")", "\n", "\n", "all_pairs_min_ymax", "=", "np", ".", "minimum", "(", "y_max1", ",", "np", ".", "transpose", "(", "y_max2", ")", ")", "\n", "all_pairs_max_ymin", "=", "np", ".", "maximum", "(", "y_min1", ",", "np", ".", "transpose", "(", "y_min2", ")", ")", "\n", "intersect_heights", "=", "np", ".", "maximum", "(", "\n", "np", ".", "zeros", "(", "all_pairs_max_ymin", ".", "shape", ")", ",", "\n", "all_pairs_min_ymax", "-", "all_pairs_max_ymin", ")", "\n", "all_pairs_min_xmax", "=", "np", ".", "minimum", "(", "x_max1", ",", "np", ".", "transpose", "(", "x_max2", ")", ")", "\n", "all_pairs_max_xmin", "=", "np", ".", "maximum", "(", "x_min1", ",", "np", ".", "transpose", "(", "x_min2", ")", ")", "\n", "intersect_widths", "=", "np", ".", "maximum", "(", "\n", "np", ".", "zeros", "(", "all_pairs_max_xmin", ".", "shape", ")", ",", "\n", "all_pairs_min_xmax", "-", "all_pairs_max_xmin", ")", "\n", "return", "intersect_heights", "*", "intersect_widths", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.iou": [[63, 80], ["np_box_ops.intersection", "np_box_ops.area", "np_box_ops.area", "numpy.expand_dims", "numpy.expand_dims"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.intersection", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.area", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.area"], ["", "def", "iou", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-union between box collections.\n\n    Args:\n        boxes1: a numpy array with shape [N, 4] holding N boxes.\n        boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n    Returns:\n        a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"", "\n", "intersect", "=", "intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "area1", "=", "area", "(", "boxes1", ")", "\n", "area2", "=", "area", "(", "boxes2", ")", "\n", "union", "=", "(", "\n", "np", ".", "expand_dims", "(", "area1", ",", "axis", "=", "1", ")", "+", "np", ".", "expand_dims", "(", "area2", ",", "axis", "=", "0", ")", "-", "\n", "intersect", ")", "\n", "return", "intersect", "/", "union", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.ioa": [[82, 99], ["np_box_ops.intersection", "numpy.expand_dims", "np_box_ops.area"], "function", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.intersection", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.np_box_ops.area"], ["", "def", "ioa", "(", "boxes1", ",", "boxes2", ")", ":", "\n", "    ", "\"\"\"Computes pairwise intersection-over-area between box collections.\n\n    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n    their intersection area over box2's area. Note that ioa is not symmetric,\n    that is, IOA(box1, box2) != IOA(box2, box1).\n\n    Args:\n        boxes1: a numpy array with shape [N, 4] holding N boxes.\n        boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n    Returns:\n        a numpy array with shape [N, M] representing pairwise ioa scores.\n    \"\"\"", "\n", "intersect", "=", "intersection", "(", "boxes1", ",", "boxes2", ")", "\n", "areas", "=", "np", ".", "expand_dims", "(", "area", "(", "boxes2", ")", ",", "axis", "=", "0", ")", "\n", "return", "intersect", "/", "areas", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.DetectionEvaluator.__init__": [[61, 72], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "categories", ")", ":", "\n", "        ", "\"\"\"Constructor.\n\n        Args:\n            categories: A list of dicts, each of which has the following keys -\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name e.g.,\n                    'cat', 'dog'.\n        \"\"\"", "\n", "self", ".", "_categories", "=", "categories", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.DetectionEvaluator.add_single_ground_truth_image_info": [[73, 82], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "add_single_ground_truth_image_info", "(", "self", ",", "image_id", ",", "groundtruth_dict", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            groundtruth_dict: A dictionary of groundtruth numpy arrays required\n                for evaluations.\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.DetectionEvaluator.add_single_detected_image_info": [[83, 92], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "add_single_detected_image_info", "(", "self", ",", "image_id", ",", "detections_dict", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            detections_dict: A dictionary of detection numpy arrays required\n                for evaluation.\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.DetectionEvaluator.evaluate": [[93, 96], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Evaluates detections and returns a dictionary of metrics.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.DetectionEvaluator.clear": [[97, 100], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "clear", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.__init__": [[105, 152], ["object_detection_evaluation.DetectionEvaluator.__init__", "max", "object_detection_evaluation.ObjectDetectionEvaluation", "set", "min", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["def", "__init__", "(", "self", ",", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "metric_prefix", "=", "None", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", "evaluate_masks", "=", "False", ")", ":", "\n", "        ", "\"\"\"Constructor.\n\n        Args:\n            categories: A list of dicts, each of which has the following keys -\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name e.g.,\n                    'cat', 'dog'.\n            matching_iou_threshold: IOU threshold to use for matching\n                groundtruth boxes to detection boxes.\n            evaluate_corlocs: (optional) boolean which determines if corloc\n                scores are to be returned or not.\n            metric_prefix: (optional) string prefix for metric name; if None,\n                no prefix is used.\n            use_weighted_mean_ap: (optional) boolean which determines if the\n                mean average precision is computed directly from the scores and\n                tp_fp_labels of all classes.\n            evaluate_masks: If False, evaluation will be performed based on\n                boxes. If True, mask evaluation will be performed instead.\n\n        Raises:\n            ValueError: If the category ids are not 1-indexed.\n        \"\"\"", "\n", "super", "(", "ObjectDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "categories", ")", "\n", "self", ".", "_num_classes", "=", "max", "(", "[", "cat", "[", "'id'", "]", "for", "cat", "in", "categories", "]", ")", "\n", "if", "min", "(", "cat", "[", "'id'", "]", "for", "cat", "in", "categories", ")", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "'Classes should be 1-indexed.'", ")", "\n", "", "self", ".", "_matching_iou_threshold", "=", "matching_iou_threshold", "\n", "self", ".", "_use_weighted_mean_ap", "=", "use_weighted_mean_ap", "\n", "self", ".", "_label_id_offset", "=", "1", "\n", "self", ".", "_evaluate_masks", "=", "evaluate_masks", "\n", "self", ".", "_evaluation", "=", "ObjectDetectionEvaluation", "(", "\n", "num_groundtruth_classes", "=", "self", ".", "_num_classes", ",", "\n", "matching_iou_threshold", "=", "self", ".", "_matching_iou_threshold", ",", "\n", "use_weighted_mean_ap", "=", "self", ".", "_use_weighted_mean_ap", ",", "\n", "label_id_offset", "=", "self", ".", "_label_id_offset", ",", "\n", ")", "\n", "self", ".", "_image_ids", "=", "set", "(", "[", "]", ")", "\n", "self", ".", "_evaluate_corlocs", "=", "evaluate_corlocs", "\n", "self", ".", "_metric_prefix", "=", "(", "metric_prefix", "+", "'_'", ")", "if", "metric_prefix", "else", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.add_single_ground_truth_image_info": [[153, 200], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.add_single_ground_truth_image_info", "object_detection_evaluation.ObjectDetectionEvaluator._image_ids.update", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update"], ["", "def", "add_single_ground_truth_image_info", "(", "self", ",", "image_id", ",", "groundtruth_dict", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            groundtruth_dict: A dictionary containing -\n                standard_fields.InputDataFields.groundtruth_boxes: float32\n                    numpy array of shape [num_boxes, 4] containing `num_boxes`\n                    groundtruth boxes of the format [ymin, xmin, ymax, xmax] in\n                    absolute image coordinates.\n                standard_fields.InputDataFields.groundtruth_classes: integer\n                    numpy array of shape [num_boxes] containing 1-indexed\n                    groundtruth classes for the boxes.\n                standard_fields.InputDataFields.groundtruth_instance_masks:\n                    Optional numpy array of shape [num_boxes, height, width]\n                    with values in {0, 1}.\n\n        Raises:\n            ValueError: On adding groundtruth for an image more than once. Will\n                also raise error if instance masks are not in groundtruth\n                dictionary.\n        \"\"\"", "\n", "if", "image_id", "in", "self", ".", "_image_ids", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Image with id {} already added.'", ".", "format", "(", "image_id", ")", ")", "\n", "\n", "", "groundtruth_classes", "=", "(", "\n", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_classes", "]", "-", "\n", "self", ".", "_label_id_offset", ")", "\n", "\n", "groundtruth_masks", "=", "None", "\n", "if", "self", ".", "_evaluate_masks", ":", "\n", "            ", "if", "(", "standard_fields", ".", "InputDataFields", ".", "groundtruth_instance_masks", "\n", "not", "in", "groundtruth_dict", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'Instance masks not in groundtruth dictionary.'", ")", "\n", "", "groundtruth_masks", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_instance_masks", "]", "\n", "", "self", ".", "_evaluation", ".", "add_single_ground_truth_image_info", "(", "\n", "image_key", "=", "image_id", ",", "\n", "groundtruth_boxes", "=", "groundtruth_dict", "[", "\n", "standard_fields", ".", "InputDataFields", ".", "groundtruth_boxes", "]", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_classes", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "self", ".", "_image_ids", ".", "update", "(", "[", "image_id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.add_single_detected_image_info": [[201, 244], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.add_single_detected_image_info", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info"], ["", "def", "add_single_detected_image_info", "(", "self", ",", "image_id", ",", "detections_dict", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            detections_dict: A dictionary containing -\n                standard_fields.DetectionResultFields.detection_boxes: float32\n                    numpy array of shape [num_boxes, 4] containing `num_boxes`\n                    detection boxes of the format [ymin, xmin, ymax, xmax] in\n                    absolute image coordinates.\n                standard_fields.DetectionResultFields.detection_scores: float32\n                    numpy array of shape [num_boxes] containing detection\n                    scores for the boxes.\n                standard_fields.DetectionResultFields.detection_classes:\n                    integer numpy array of shape [num_boxes] containing\n                    1-indexed detection classes for the boxes.\n                standard_fields.DetectionResultFields.detection_masks: uint8\n                    numpy array of shape [num_boxes, height, width] containing\n                    `num_boxes` masks of values ranging between 0 and 1.\n\n        Raises:\n            ValueError: If detection masks are not in detections dictionary.\n        \"\"\"", "\n", "detection_classes", "=", "(", "\n", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_classes", "]", "-", "\n", "self", ".", "_label_id_offset", ")", "\n", "detection_masks", "=", "None", "\n", "if", "self", ".", "_evaluate_masks", ":", "\n", "            ", "if", "(", "standard_fields", ".", "DetectionResultFields", ".", "detection_masks", "\n", "not", "in", "detections_dict", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'Detection masks not in detections dictionary.'", ")", "\n", "", "detection_masks", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_masks", "]", "\n", "", "self", ".", "_evaluation", ".", "add_single_detected_image_info", "(", "\n", "image_key", "=", "image_id", ",", "\n", "detected_boxes", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_boxes", "]", ",", "\n", "detected_scores", "=", "detections_dict", "[", "\n", "standard_fields", ".", "DetectionResultFields", ".", "detection_scores", "]", ",", "\n", "detected_class_labels", "=", "detection_classes", ",", "\n", "detected_masks", "=", "detection_masks", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.create_category_index": [[246, 266], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "create_category_index", "(", "categories", ")", ":", "\n", "        ", "\"\"\"Creates dictionary of COCO compatible categories keyed by category\n        id.\n\n        Args:\n            categories: a list of dicts, each of which has the following keys:\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name\n                    e.g., 'cat', 'dog', 'pizza'.\n\n        Returns:\n            category_index: a dict containing the same entries as categories,\n                but keyed by the 'id' field of each category.\n        \"\"\"", "\n", "category_index", "=", "{", "}", "\n", "for", "cat", "in", "categories", ":", "\n", "            ", "category_index", "[", "cat", "[", "'id'", "]", "]", "=", "cat", "\n", "", "return", "category_index", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.evaluate": [[267, 312], ["object_detection_evaluation.ObjectDetectionEvaluator._evaluation.evaluate", "object_detection_evaluation.ObjectDetectionEvaluator.create_category_index", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.create_category_index"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute evaluation result.\n\n        Returns:\n            A dictionary of metrics with the following fields -\n\n            1. summary_metrics:\n                'Precision/mAP@<matching_iou_threshold>IOU': mean average\n                precision at the specified IOU threshold\n\n            2. per_category_ap: category specific results with keys of the form\n               'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category'\n        \"\"\"", "\n", "(", "per_class_ap", ",", "mean_ap", ",", "_", ",", "_", ",", "per_class_corloc", ",", "\n", "mean_corloc", ")", "=", "self", ".", "_evaluation", ".", "evaluate", "(", ")", "\n", "\n", "metric", "=", "f'mAP@{self._matching_iou_threshold}IOU'", "\n", "pascal_metrics", "=", "{", "self", ".", "_metric_prefix", "+", "metric", ":", "mean_ap", "}", "\n", "if", "self", ".", "_evaluate_corlocs", ":", "\n", "            ", "pascal_metrics", "[", "self", ".", "_metric_prefix", "+", "\n", "'Precision/meanCorLoc@{}IOU'", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", ")", "]", "=", "mean_corloc", "\n", "", "category_index", "=", "self", ".", "create_category_index", "(", "self", ".", "_categories", ")", "\n", "for", "idx", "in", "range", "(", "per_class_ap", ".", "size", ")", ":", "\n", "            ", "if", "idx", "+", "self", ".", "_label_id_offset", "in", "category_index", ":", "\n", "                ", "display_name", "=", "(", "\n", "self", ".", "_metric_prefix", "+", "\n", "'PerformanceByCategory/AP@{}IOU/{}'", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", ",", "\n", "category_index", "[", "idx", "+", "self", ".", "_label_id_offset", "]", "[", "'name'", "]", ",", "\n", ")", ")", "\n", "pascal_metrics", "[", "display_name", "]", "=", "per_class_ap", "[", "idx", "]", "\n", "\n", "# Optionally add CorLoc metrics.classes", "\n", "if", "self", ".", "_evaluate_corlocs", ":", "\n", "                    ", "display_name", "=", "(", "\n", "self", ".", "_metric_prefix", "+", "\n", "'PerformanceByCategory/CorLoc@{}IOU/{}'", ".", "format", "(", "\n", "self", ".", "_matching_iou_threshold", ",", "\n", "category_index", "[", "idx", "+", "\n", "self", ".", "_label_id_offset", "]", "[", "'name'", "]", ",", "\n", ")", ")", "\n", "pascal_metrics", "[", "display_name", "]", "=", "per_class_corloc", "[", "idx", "]", "\n", "\n", "", "", "", "return", "pascal_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.clear": [[313, 322], ["object_detection_evaluation.ObjectDetectionEvaluation", "object_detection_evaluation.ObjectDetectionEvaluator._image_ids.clear"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluator.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"", "\n", "self", ".", "_evaluation", "=", "ObjectDetectionEvaluation", "(", "\n", "num_groundtruth_classes", "=", "self", ".", "_num_classes", ",", "\n", "matching_iou_threshold", "=", "self", ".", "_matching_iou_threshold", ",", "\n", "use_weighted_mean_ap", "=", "self", ".", "_use_weighted_mean_ap", ",", "\n", "label_id_offset", "=", "self", ".", "_label_id_offset", ",", "\n", ")", "\n", "self", ".", "_image_ids", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.PascalDetectionEvaluator.__init__": [[327, 333], ["object_detection_evaluation.ObjectDetectionEvaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__"], ["def", "__init__", "(", "self", ",", "categories", ",", "matching_iou_threshold", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "PascalDetectionEvaluator", ",", "self", ")", ".", "__init__", "(", "\n", "categories", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", "evaluate_corlocs", "=", "False", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.__init__": [[352, 378], ["per_image_evaluation.PerImageEvaluation", "numpy.zeros", "numpy.zeros", "object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections", "ValueError"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], ["def", "__init__", "(", "self", ",", "\n", "num_groundtruth_classes", ",", "\n", "matching_iou_threshold", "=", "0.5", ",", "\n", "nms_iou_threshold", "=", "1.0", ",", "\n", "nms_max_output_boxes", "=", "10000", ",", "\n", "use_weighted_mean_ap", "=", "False", ",", "\n", "label_id_offset", "=", "0", ")", ":", "\n", "        ", "if", "num_groundtruth_classes", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Need at least 1 groundtruth class for evaluation.'", ")", "\n", "\n", "", "self", ".", "per_image_eval", "=", "per_image_evaluation", ".", "PerImageEvaluation", "(", "\n", "num_groundtruth_classes", "=", "num_groundtruth_classes", ",", "\n", "matching_iou_threshold", "=", "matching_iou_threshold", ",", "\n", ")", "\n", "self", ".", "num_class", "=", "num_groundtruth_classes", "\n", "self", ".", "use_weighted_mean_ap", "=", "use_weighted_mean_ap", "\n", "self", ".", "label_id_offset", "=", "label_id_offset", "\n", "\n", "self", ".", "groundtruth_boxes", "=", "{", "}", "\n", "self", ".", "groundtruth_class_labels", "=", "{", "}", "\n", "self", ".", "groundtruth_masks", "=", "{", "}", "\n", "self", ".", "num_gt_instances_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ",", "dtype", "=", "int", ")", "\n", "self", ".", "num_gt_imgs_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ",", "dtype", "=", "int", ")", "\n", "\n", "self", ".", "_initialize_detections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections": [[379, 390], ["set", "numpy.zeros", "numpy.empty", "object_detection_evaluation.ObjectDetectionEvaluation.average_precision_per_class.fill", "numpy.ones", "range", "range"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.model_ema.ModelEmaV2.set"], ["", "def", "_initialize_detections", "(", "self", ")", ":", "\n", "        ", "self", ".", "detection_keys", "=", "set", "(", ")", "\n", "self", ".", "scores_per_class", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "num_class", ")", "]", "\n", "self", ".", "tp_fp_labels_per_class", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "num_class", ")", "]", "\n", "self", ".", "num_images_correctly_detected_per_class", "=", "np", ".", "zeros", "(", "self", ".", "num_class", ")", "\n", "self", ".", "average_precision_per_class", "=", "np", ".", "empty", "(", "\n", "self", ".", "num_class", ",", "dtype", "=", "float", ")", "\n", "self", ".", "average_precision_per_class", ".", "fill", "(", "np", ".", "nan", ")", "\n", "self", ".", "precisions_per_class", "=", "[", "]", "\n", "self", ".", "recalls_per_class", "=", "[", "]", "\n", "self", ".", "corloc_per_class", "=", "np", ".", "ones", "(", "self", ".", "num_class", ",", "dtype", "=", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.clear_detections": [[391, 393], ["object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._initialize_detections"], ["", "def", "clear_detections", "(", "self", ")", ":", "\n", "        ", "self", ".", "_initialize_detections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_ground_truth_image_info": [[394, 422], ["object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics"], ["", "def", "add_single_ground_truth_image_info", "(", "self", ",", "\n", "image_key", ",", "\n", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", ",", "\n", "groundtruth_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n        Args:\n            image_key: A unique string/integer identifier for the image.\n            groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n                containing `num_boxes` groundtruth boxes of the format\n                [ymin, xmin, ymax, xmax] in absolute image coordinates.\n            groundtruth_class_labels: integer numpy array of shape [num_boxes]\n                containing 0-indexed groundtruth classes for the boxes.\n            groundtruth_masks: uint8 numpy array of shape\n                [num_boxes, height, width] containing `num_boxes` groundtruth\n                masks. The mask values range from 0 to 1.\n        \"\"\"", "\n", "if", "image_key", "in", "self", ".", "groundtruth_boxes", ":", "\n", "            ", "warnings", ".", "warn", "(", "(", "'image %s has already been added to the ground '", "\n", "'truth database.'", ")", ",", "image_key", ")", "\n", "return", "\n", "\n", "", "self", ".", "groundtruth_boxes", "[", "image_key", "]", "=", "groundtruth_boxes", "\n", "self", ".", "groundtruth_class_labels", "[", "image_key", "]", "=", "groundtruth_class_labels", "\n", "self", ".", "groundtruth_masks", "[", "image_key", "]", "=", "groundtruth_masks", "\n", "\n", "self", ".", "_update_ground_truth_statistics", "(", "groundtruth_class_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.add_single_detected_image_info": [[423, 494], ["object_detection_evaluation.ObjectDetectionEvaluation.detection_keys.add", "object_detection_evaluation.ObjectDetectionEvaluation.per_image_eval.compute_object_detection_metrics", "range", "ValueError", "warnings.warn", "object_detection_evaluation.ObjectDetectionEvaluation.groundtruth_masks.pop", "numpy.empty", "numpy.array", "len", "len", "len", "len", "len", "len", "numpy.empty", "object_detection_evaluation.ObjectDetectionEvaluation.scores_per_class[].append", "object_detection_evaluation.ObjectDetectionEvaluation.tp_fp_labels_per_class[].append", "len"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.per_image_evaluation.PerImageEvaluation.compute_object_detection_metrics"], ["", "def", "add_single_detected_image_info", "(", "self", ",", "\n", "image_key", ",", "\n", "detected_boxes", ",", "\n", "detected_scores", ",", "\n", "detected_class_labels", ",", "\n", "detected_masks", "=", "None", ")", ":", "\n", "        ", "\"\"\"Adds detections for a single image to be used for evaluation.\n\n        Args:\n            image_key: A unique string/integer identifier for the image.\n            detected_boxes: float32 numpy array of shape [num_boxes, 4]\n                containing `num_boxes` detection boxes of the format\n                [ymin, xmin, ymax, xmax] in absolute image coordinates.\n            detected_scores: float32 numpy array of shape [num_boxes]\n                containing detection scores for the boxes.\n            detected_class_labels: integer numpy array of shape [num_boxes]\n                containing 0-indexed detection classes for the boxes.\n            detected_masks: np.uint8 numpy array of shape\n                [num_boxes, height, width] containing `num_boxes` detection\n                masks with values ranging between 0 and 1.\n\n        Raises:\n            ValueError: if the number of boxes, scores and class labels differ\n                in length.\n        \"\"\"", "\n", "if", "len", "(", "detected_boxes", ")", "!=", "len", "(", "detected_scores", ")", "or", "len", "(", "\n", "detected_boxes", ")", "!=", "len", "(", "detected_class_labels", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'detected_boxes, detected_scores and '", "\n", "'detected_class_labels should all have same lengths. Got'", "\n", "'[%d, %d, %d]'", "%", "len", "(", "detected_boxes", ")", ",", "\n", "len", "(", "detected_scores", ")", ",", "\n", "len", "(", "detected_class_labels", ")", ",", "\n", ")", "\n", "\n", "", "if", "image_key", "in", "self", ".", "detection_keys", ":", "\n", "            ", "warnings", ".", "warn", "(", "(", "'image %s has already been added to the ground '", "\n", "'truth database.'", ")", ",", "image_key", ")", "\n", "return", "\n", "\n", "", "self", ".", "detection_keys", ".", "add", "(", "image_key", ")", "\n", "if", "image_key", "in", "self", ".", "groundtruth_boxes", ":", "\n", "            ", "groundtruth_boxes", "=", "self", ".", "groundtruth_boxes", "[", "image_key", "]", "\n", "groundtruth_class_labels", "=", "self", ".", "groundtruth_class_labels", "[", "image_key", "]", "\n", "# Masks are popped instead of look up. The reason is that we do not", "\n", "# want to keep all masks in memory which can cause memory overflow.", "\n", "groundtruth_masks", "=", "self", ".", "groundtruth_masks", ".", "pop", "(", "image_key", ")", "\n", "", "else", ":", "\n", "            ", "groundtruth_boxes", "=", "np", ".", "empty", "(", "shape", "=", "[", "0", ",", "4", "]", ",", "dtype", "=", "float", ")", "\n", "groundtruth_class_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "int", ")", "\n", "if", "detected_masks", "is", "None", ":", "\n", "                ", "groundtruth_masks", "=", "None", "\n", "", "else", ":", "\n", "                ", "groundtruth_masks", "=", "np", ".", "empty", "(", "shape", "=", "[", "0", ",", "1", ",", "1", "]", ",", "dtype", "=", "float", ")", "\n", "", "", "(", "\n", "scores", ",", "\n", "tp_fp_labels", ",", "\n", ")", "=", "self", ".", "per_image_eval", ".", "compute_object_detection_metrics", "(", "\n", "detected_boxes", "=", "detected_boxes", ",", "\n", "detected_scores", "=", "detected_scores", ",", "\n", "detected_class_labels", "=", "detected_class_labels", ",", "\n", "groundtruth_boxes", "=", "groundtruth_boxes", ",", "\n", "groundtruth_class_labels", "=", "groundtruth_class_labels", ",", "\n", "detected_masks", "=", "detected_masks", ",", "\n", "groundtruth_masks", "=", "groundtruth_masks", ",", "\n", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "if", "scores", "[", "i", "]", ".", "shape", "[", "0", "]", ">", "0", ":", "\n", "                ", "self", ".", "scores_per_class", "[", "i", "]", ".", "append", "(", "scores", "[", "i", "]", ")", "\n", "self", ".", "tp_fp_labels_per_class", "[", "i", "]", ".", "append", "(", "tp_fp_labels", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation._update_ground_truth_statistics": [[495, 508], ["collections.defaultdict"], "methods", ["None"], ["", "", "", "def", "_update_ground_truth_statistics", "(", "self", ",", "groundtruth_class_labels", ")", ":", "\n", "        ", "\"\"\"Update grouth truth statitistics.\n\n        Args:\n            groundtruth_class_labels: An integer numpy array of length M,\n                representing M class labels of object instances in ground truth\n        \"\"\"", "\n", "count", "=", "defaultdict", "(", "lambda", ":", "0", ")", "\n", "for", "label", "in", "groundtruth_class_labels", ":", "\n", "            ", "count", "[", "label", "]", "+=", "1", "\n", "", "for", "k", "in", "count", ":", "\n", "            ", "self", ".", "num_gt_instances_per_class", "[", "k", "]", "+=", "count", "[", "k", "]", "\n", "self", ".", "num_gt_imgs_per_class", "[", "k", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.object_detection_evaluation.ObjectDetectionEvaluation.evaluate": [[509, 574], ["range", "metrics.compute_cor_loc", "numpy.nanmean", "ObjectDetectionEvalMetrics", "logging.info", "numpy.array", "numpy.array", "metrics.compute_precision_recall", "object_detection_evaluation.ObjectDetectionEvaluation.precisions_per_class.append", "object_detection_evaluation.ObjectDetectionEvaluation.recalls_per_class.append", "metrics.compute_average_precision", "numpy.sum", "metrics.compute_precision_recall", "metrics.compute_average_precision", "numpy.nanmean", "numpy.array", "numpy.array", "numpy.concatenate", "numpy.concatenate", "numpy.append", "numpy.append", "numpy.squeeze", "numpy.argwhere"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_cor_loc", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_precision_recall", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_average_precision", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_precision_recall", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.ava_evaluation.metrics.compute_average_precision"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute evaluation result.\n\n        Returns:\n            A named tuple with the following fields -\n                average_precision: float numpy array of average precision for\n                    each class.\n                mean_ap: mean average precision of all classes, float scalar\n                precisions: List of precisions, each precision is a float numpy\n                    array\n                recalls: List of recalls, each recall is a float numpy array\n                corloc: numpy float array\n                mean_corloc: Mean CorLoc score for each class, float scalar\n        \"\"\"", "\n", "if", "(", "self", ".", "num_gt_instances_per_class", "==", "0", ")", ".", "any", "(", ")", ":", "\n", "            ", "logging", ".", "info", "(", "\n", "'The following classes have no ground truth examples: %s'", ",", "\n", "np", ".", "squeeze", "(", "np", ".", "argwhere", "(", "self", ".", "num_gt_instances_per_class", "==", "0", ")", ")", "+", "\n", "self", ".", "label_id_offset", ")", "\n", "\n", "", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "            ", "all_scores", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", "\n", "all_tp_fp_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "\n", "", "for", "class_index", "in", "range", "(", "self", ".", "num_class", ")", ":", "\n", "            ", "if", "self", ".", "num_gt_instances_per_class", "[", "class_index", "]", "==", "0", ":", "\n", "                ", "continue", "\n", "", "if", "not", "self", ".", "scores_per_class", "[", "class_index", "]", ":", "\n", "                ", "scores", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "float", ")", "\n", "tp_fp_labels", "=", "np", ".", "array", "(", "[", "]", ",", "dtype", "=", "bool", ")", "\n", "", "else", ":", "\n", "                ", "scores", "=", "np", ".", "concatenate", "(", "self", ".", "scores_per_class", "[", "class_index", "]", ")", "\n", "tp_fp_labels", "=", "np", ".", "concatenate", "(", "\n", "self", ".", "tp_fp_labels_per_class", "[", "class_index", "]", ")", "\n", "", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "                ", "all_scores", "=", "np", ".", "append", "(", "all_scores", ",", "scores", ")", "\n", "all_tp_fp_labels", "=", "np", ".", "append", "(", "all_tp_fp_labels", ",", "tp_fp_labels", ")", "\n", "", "precision", ",", "recall", "=", "metrics", ".", "compute_precision_recall", "(", "\n", "scores", ",", "tp_fp_labels", ",", "\n", "self", ".", "num_gt_instances_per_class", "[", "class_index", "]", ")", "\n", "self", ".", "precisions_per_class", ".", "append", "(", "precision", ")", "\n", "self", ".", "recalls_per_class", ".", "append", "(", "recall", ")", "\n", "average_precision", "=", "metrics", ".", "compute_average_precision", "(", "\n", "precision", ",", "recall", ")", "\n", "self", ".", "average_precision_per_class", "[", "class_index", "]", "=", "average_precision", "\n", "\n", "", "self", ".", "corloc_per_class", "=", "metrics", ".", "compute_cor_loc", "(", "\n", "self", ".", "num_gt_imgs_per_class", ",", "\n", "self", ".", "num_images_correctly_detected_per_class", ")", "\n", "\n", "if", "self", ".", "use_weighted_mean_ap", ":", "\n", "            ", "num_gt_instances", "=", "np", ".", "sum", "(", "self", ".", "num_gt_instances_per_class", ")", "\n", "precision", ",", "recall", "=", "metrics", ".", "compute_precision_recall", "(", "\n", "all_scores", ",", "all_tp_fp_labels", ",", "num_gt_instances", ")", "\n", "mean_ap", "=", "metrics", ".", "compute_average_precision", "(", "precision", ",", "recall", ")", "\n", "", "else", ":", "\n", "            ", "mean_ap", "=", "np", ".", "nanmean", "(", "self", ".", "average_precision_per_class", ")", "\n", "", "mean_corloc", "=", "np", ".", "nanmean", "(", "self", ".", "corloc_per_class", ")", "\n", "return", "ObjectDetectionEvalMetrics", "(", "\n", "self", ".", "average_precision_per_class", ",", "\n", "mean_ap", ",", "\n", "self", ".", "precisions_per_class", ",", "\n", "self", ".", "recalls_per_class", ",", "\n", "self", ".", "corloc_per_class", ",", "\n", "mean_corloc", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.optimizer.tsm_optimizer_constructor.TSMOptimizerConstructor.add_params": [[22, 110], ["model.modules", "normal_weight.pop", "normal_bias.pop", "params.append", "params.append", "params.append", "params.append", "params.append", "params.append", "params.append", "isinstance", "lr5_weight.append", "lr10_bias.append", "normal_weight.append", "normal_bias.append", "list", "isinstance", "m.parameters", "first_conv_weight.append", "normal_weight.append", "list", "normal_weight.append", "isinstance", "len", "first_conv_bias.append", "len", "normal_bias.append", "m.parameters", "len", "normal_bias.append", "list", "m.parameters", "len", "bn.append", "len", "ValueError", "list", "m.parameters", "type"], "methods", ["None"], ["def", "add_params", "(", "self", ",", "params", ",", "model", ")", ":", "\n", "        ", "\"\"\"Add parameters and their corresponding lr and wd to the params.\n\n        Args:\n            params (list): The list to be modified, containing all parameter\n                groups and their corresponding lr and wd configurations.\n            model (nn.Module): The model to be trained with the optimizer.\n        \"\"\"", "\n", "# use fc_lr5 to determine whether to specify higher multi-factor", "\n", "# for fc layer weights and bias.", "\n", "fc_lr5", "=", "self", ".", "paramwise_cfg", "[", "'fc_lr5'", "]", "\n", "first_conv_weight", "=", "[", "]", "\n", "first_conv_bias", "=", "[", "]", "\n", "normal_weight", "=", "[", "]", "\n", "normal_bias", "=", "[", "]", "\n", "lr5_weight", "=", "[", "]", "\n", "lr10_bias", "=", "[", "]", "\n", "bn", "=", "[", "]", "\n", "\n", "conv_cnt", "=", "0", "\n", "\n", "for", "m", "in", "model", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "_ConvNd", ")", ":", "\n", "                ", "m_params", "=", "list", "(", "m", ".", "parameters", "(", ")", ")", "\n", "conv_cnt", "+=", "1", "\n", "if", "conv_cnt", "==", "1", ":", "\n", "                    ", "first_conv_weight", ".", "append", "(", "m_params", "[", "0", "]", ")", "\n", "if", "len", "(", "m_params", ")", "==", "2", ":", "\n", "                        ", "first_conv_bias", ".", "append", "(", "m_params", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "normal_weight", ".", "append", "(", "m_params", "[", "0", "]", ")", "\n", "if", "len", "(", "m_params", ")", "==", "2", ":", "\n", "                        ", "normal_bias", ".", "append", "(", "m_params", "[", "1", "]", ")", "\n", "", "", "", "elif", "isinstance", "(", "m", ",", "torch", ".", "nn", ".", "Linear", ")", ":", "\n", "                ", "m_params", "=", "list", "(", "m", ".", "parameters", "(", ")", ")", "\n", "normal_weight", ".", "append", "(", "m_params", "[", "0", "]", ")", "\n", "if", "len", "(", "m_params", ")", "==", "2", ":", "\n", "                    ", "normal_bias", ".", "append", "(", "m_params", "[", "1", "]", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "\n", "(", "_BatchNorm", ",", "SyncBatchNorm", ",", "torch", ".", "nn", ".", "GroupNorm", ")", ")", ":", "\n", "                ", "for", "param", "in", "list", "(", "m", ".", "parameters", "(", ")", ")", ":", "\n", "                    ", "if", "param", ".", "requires_grad", ":", "\n", "                        ", "bn", ".", "append", "(", "param", ")", "\n", "", "", "", "elif", "len", "(", "m", ".", "_modules", ")", "==", "0", ":", "\n", "                ", "if", "len", "(", "list", "(", "m", ".", "parameters", "(", ")", ")", ")", ">", "0", ":", "\n", "                    ", "raise", "ValueError", "(", "f'New atomic module type: {type(m)}. '", "\n", "'Need to give it a learning policy'", ")", "\n", "\n", "# pop the cls_head fc layer params", "\n", "", "", "", "last_fc_weight", "=", "normal_weight", ".", "pop", "(", ")", "\n", "last_fc_bias", "=", "normal_bias", ".", "pop", "(", ")", "\n", "if", "fc_lr5", ":", "\n", "            ", "lr5_weight", ".", "append", "(", "last_fc_weight", ")", "\n", "lr10_bias", ".", "append", "(", "last_fc_bias", ")", "\n", "", "else", ":", "\n", "            ", "normal_weight", ".", "append", "(", "last_fc_weight", ")", "\n", "normal_bias", ".", "append", "(", "last_fc_bias", ")", "\n", "\n", "", "params", ".", "append", "(", "{", "\n", "'params'", ":", "first_conv_weight", ",", "\n", "'lr'", ":", "self", ".", "base_lr", ",", "\n", "'weight_decay'", ":", "self", ".", "base_wd", "\n", "}", ")", "\n", "params", ".", "append", "(", "{", "\n", "'params'", ":", "first_conv_bias", ",", "\n", "'lr'", ":", "self", ".", "base_lr", "*", "2", ",", "\n", "'weight_decay'", ":", "0", "\n", "}", ")", "\n", "params", ".", "append", "(", "{", "\n", "'params'", ":", "normal_weight", ",", "\n", "'lr'", ":", "self", ".", "base_lr", ",", "\n", "'weight_decay'", ":", "self", ".", "base_wd", "\n", "}", ")", "\n", "params", ".", "append", "(", "{", "\n", "'params'", ":", "normal_bias", ",", "\n", "'lr'", ":", "self", ".", "base_lr", "*", "2", ",", "\n", "'weight_decay'", ":", "0", "\n", "}", ")", "\n", "params", ".", "append", "(", "{", "'params'", ":", "bn", ",", "'lr'", ":", "self", ".", "base_lr", ",", "'weight_decay'", ":", "0", "}", ")", "\n", "params", ".", "append", "(", "{", "\n", "'params'", ":", "lr5_weight", ",", "\n", "'lr'", ":", "self", ".", "base_lr", "*", "5", ",", "\n", "'weight_decay'", ":", "self", ".", "base_wd", "\n", "}", ")", "\n", "params", ".", "append", "(", "{", "\n", "'params'", ":", "lr10_bias", ",", "\n", "'lr'", ":", "self", ".", "base_lr", "*", "10", ",", "\n", "'weight_decay'", ":", "0", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.bbox.transforms.bbox2result": [[5, 38], ["bboxes.cpu().numpy.cpu().numpy", "labels.cpu().numpy.cpu().numpy", "range", "list", "isinstance", "len", "result.append", "numpy.zeros", "bboxes.cpu().numpy.cpu", "labels.cpu().numpy.cpu", "numpy.concatenate"], "function", ["None"], ["has_interpolation_mode", "=", "True", "\n", "", "except", "ImportError", ":", "\n", "    ", "has_interpolation_mode", "=", "False", "\n", "", "from", "PIL", "import", "Image", "\n", "import", "warnings", "\n", "import", "math", "\n", "import", "random", "\n", "import", "numpy", "as", "np", "\n", "\n", "\n", "class", "ToNumpy", ":", "\n", "\n", "    ", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "np_img", "\n", "\n", "\n", "", "", "class", "ToTensor", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "dtype", "=", "torch", ".", "float32", ")", ":", "\n", "        ", "self", ".", "dtype", "=", "dtype", "\n", "\n", "", "def", "__call__", "(", "self", ",", "pil_img", ")", ":", "\n", "        ", "np_img", "=", "np", ".", "array", "(", "pil_img", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "if", "np_img", ".", "ndim", "<", "3", ":", "\n", "            ", "np_img", "=", "np", ".", "expand_dims", "(", "np_img", ",", "axis", "=", "-", "1", ")", "\n", "", "np_img", "=", "np", ".", "rollaxis", "(", "np_img", ",", "2", ")", "# HWC to CHW", "\n", "return", "torch", ".", "from_numpy", "(", "np_img", ")", ".", "to", "(", "dtype", "=", "self", ".", "dtype", ")", "\n", "\n", "\n", "", "", "_pil_interpolation_to_str", "=", "{", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.bbox.bbox_target.bbox_target": [[6, 43], ["len", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "len", "len", "pos_bboxes.size", "neg_bboxes.size", "torch.pad", "pos_bboxes.new_zeros", "torch.cat.append", "torch.cat.append"], "function", ["None"], ["def", "bbox_target", "(", "pos_bboxes_list", ",", "neg_bboxes_list", ",", "gt_labels", ",", "cfg", ")", ":", "\n", "    ", "\"\"\"Generate classification targets for bboxes.\n\n    Args:\n        pos_bboxes_list (list[Tensor]): Positive bboxes list.\n        neg_bboxes_list (list[Tensor]): Negative bboxes list.\n        gt_labels (list[Tensor]): Groundtruth classification label list.\n        cfg (Config): RCNN config.\n\n    Returns:\n        (Tensor, Tensor): Label and label_weight for bboxes.\n    \"\"\"", "\n", "labels", ",", "label_weights", "=", "[", "]", ",", "[", "]", "\n", "pos_weight", "=", "1.0", "if", "cfg", ".", "pos_weight", "<=", "0", "else", "cfg", ".", "pos_weight", "\n", "\n", "assert", "len", "(", "pos_bboxes_list", ")", "==", "len", "(", "neg_bboxes_list", ")", "==", "len", "(", "gt_labels", ")", "\n", "length", "=", "len", "(", "pos_bboxes_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "        ", "pos_bboxes", "=", "pos_bboxes_list", "[", "i", "]", "\n", "neg_bboxes", "=", "neg_bboxes_list", "[", "i", "]", "\n", "gt_label", "=", "gt_labels", "[", "i", "]", "\n", "\n", "num_pos", "=", "pos_bboxes", ".", "size", "(", "0", ")", "\n", "num_neg", "=", "neg_bboxes", ".", "size", "(", "0", ")", "\n", "num_samples", "=", "num_pos", "+", "num_neg", "\n", "label", "=", "F", ".", "pad", "(", "gt_label", ",", "(", "0", ",", "0", ",", "0", ",", "num_neg", ")", ")", "\n", "label_weight", "=", "pos_bboxes", ".", "new_zeros", "(", "num_samples", ")", "\n", "label_weight", "[", ":", "num_pos", "]", "=", "pos_weight", "\n", "label_weight", "[", "-", "num_neg", ":", "]", "=", "1.", "\n", "\n", "labels", ".", "append", "(", "label", ")", "\n", "label_weights", ".", "append", "(", "label_weight", ")", "\n", "\n", "", "labels", "=", "torch", ".", "cat", "(", "labels", ",", "0", ")", "\n", "label_weights", "=", "torch", ".", "cat", "(", "label_weights", ",", "0", ")", "\n", "return", "labels", ",", "label_weights", "\n", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceDistSamplerSeedHook.before_epoch": [[21, 29], ["hasattr", "data_loader.sampler.set_epoch", "hasattr", "data_loader.batch_sampler.sampler.set_epoch"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.set_epoch", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.data.distributed_sampler.RepeatAugSampler.set_epoch"], ["    ", "def", "before_epoch", "(", "self", ",", "runner", ")", ":", "\n", "        ", "for", "data_loader", "in", "runner", ".", "data_loaders", ":", "\n", "            ", "if", "hasattr", "(", "data_loader", ".", "sampler", ",", "'set_epoch'", ")", ":", "\n", "# in case the data loader uses `SequentialSampler` in Pytorch", "\n", "                ", "data_loader", ".", "sampler", ".", "set_epoch", "(", "runner", ".", "epoch", ")", "\n", "", "elif", "hasattr", "(", "data_loader", ".", "batch_sampler", ".", "sampler", ",", "'set_epoch'", ")", ":", "\n", "# batch sampler in pytorch wraps the sampler as its attributes.", "\n", "                ", "data_loader", ".", "batch_sampler", ".", "sampler", ".", "set_epoch", "(", "runner", ".", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run_iter": [[38, 58], ["omnisource_runner.OmniSourceRunner.batch_processor", "isinstance", "TypeError", "omnisource_runner.OmniSourceRunner.log_buffer.update", "omnisource_runner.OmniSourceRunner.model.train_step", "omnisource_runner.OmniSourceRunner.model.val_step", "log_vars.items"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.utils.metrics.AverageMeter.update", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.train_step", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.localizers.base.BaseTAGClassifier.val_step"], ["def", "run_iter", "(", "self", ",", "data_batch", ",", "train_mode", ",", "source", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "batch_processor", "is", "not", "None", ":", "\n", "            ", "outputs", "=", "self", ".", "batch_processor", "(", "\n", "self", ".", "model", ",", "data_batch", ",", "train_mode", "=", "train_mode", ",", "**", "kwargs", ")", "\n", "", "elif", "train_mode", ":", "\n", "            ", "outputs", "=", "self", ".", "model", ".", "train_step", "(", "data_batch", ",", "self", ".", "optimizer", ",", "\n", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "model", ".", "val_step", "(", "data_batch", ",", "self", ".", "optimizer", ",", "**", "kwargs", ")", "\n", "", "if", "not", "isinstance", "(", "outputs", ",", "dict", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'\"batch_processor()\" or \"model.train_step()\"'", "\n", "'and \"model.val_step()\" must return a dict'", ")", "\n", "# Since we have multiple sources, we add a suffix to log_var names,", "\n", "# so that we can differentiate them.", "\n", "", "if", "'log_vars'", "in", "outputs", ":", "\n", "            ", "log_vars", "=", "outputs", "[", "'log_vars'", "]", "\n", "log_vars", "=", "{", "k", "+", "source", ":", "v", "for", "k", ",", "v", "in", "log_vars", ".", "items", "(", ")", "}", "\n", "self", ".", "log_buffer", ".", "update", "(", "log_vars", ",", "outputs", "[", "'num_samples'", "]", ")", "\n", "\n", "", "self", ".", "outputs", "=", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train": [[59, 102], ["omnisource_runner.OmniSourceRunner.model.train", "omnisource_runner.OmniSourceRunner.call_hook", "time.sleep", "enumerate", "omnisource_runner.OmniSourceRunner.call_hook", "omnisource_runner.cycle", "len", "kwargs.pop", "len", "omnisource_runner.OmniSourceRunner.call_hook", "omnisource_runner.OmniSourceRunner.run_iter", "omnisource_runner.OmniSourceRunner.call_hook", "enumerate", "range", "next", "omnisource_runner.OmniSourceRunner.call_hook", "omnisource_runner.OmniSourceRunner.run_iter", "omnisource_runner.OmniSourceRunner.call_hook"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.train", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.cycle", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run_iter", "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run_iter"], ["", "def", "train", "(", "self", ",", "data_loaders", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "mode", "=", "'train'", "\n", "self", ".", "data_loaders", "=", "data_loaders", "\n", "self", ".", "main_loader", "=", "self", ".", "data_loaders", "[", "0", "]", "\n", "# Add aliasing", "\n", "self", ".", "data_loader", "=", "self", ".", "main_loader", "\n", "self", ".", "aux_loaders", "=", "self", ".", "data_loaders", "[", "1", ":", "]", "\n", "self", ".", "aux_iters", "=", "[", "cycle", "(", "loader", ")", "for", "loader", "in", "self", ".", "aux_loaders", "]", "\n", "\n", "auxiliary_iter_times", "=", "[", "1", "]", "*", "len", "(", "self", ".", "aux_loaders", ")", "\n", "use_aux_per_niter", "=", "1", "\n", "if", "'train_ratio'", "in", "kwargs", ":", "\n", "            ", "train_ratio", "=", "kwargs", ".", "pop", "(", "'train_ratio'", ")", "\n", "use_aux_per_niter", "=", "train_ratio", "[", "0", "]", "\n", "auxiliary_iter_times", "=", "train_ratio", "[", "1", ":", "]", "\n", "\n", "", "self", ".", "_max_iters", "=", "self", ".", "_max_epochs", "*", "len", "(", "self", ".", "main_loader", ")", "\n", "\n", "self", ".", "call_hook", "(", "'before_train_epoch'", ")", "\n", "time", ".", "sleep", "(", "2", ")", "# Prevent possible deadlock during epoch transition", "\n", "\n", "for", "i", ",", "data_batch", "in", "enumerate", "(", "self", ".", "main_loader", ")", ":", "\n", "            ", "self", ".", "_inner_iter", "=", "i", "\n", "self", ".", "call_hook", "(", "'before_train_iter'", ")", "\n", "self", ".", "run_iter", "(", "data_batch", ",", "train_mode", "=", "True", ",", "source", "=", "''", ")", "\n", "self", ".", "call_hook", "(", "'after_train_iter'", ")", "\n", "\n", "if", "self", ".", "_iter", "%", "use_aux_per_niter", "!=", "0", ":", "\n", "                ", "self", ".", "_iter", "+=", "1", "\n", "continue", "\n", "\n", "", "for", "idx", ",", "n_times", "in", "enumerate", "(", "auxiliary_iter_times", ")", ":", "\n", "                ", "for", "_", "in", "range", "(", "n_times", ")", ":", "\n", "                    ", "data_batch", "=", "next", "(", "self", ".", "aux_iters", "[", "idx", "]", ")", "\n", "self", ".", "call_hook", "(", "'before_train_iter'", ")", "\n", "self", ".", "run_iter", "(", "\n", "data_batch", ",", "train_mode", "=", "True", ",", "source", "=", "f'/aux{idx}'", ")", "\n", "self", ".", "call_hook", "(", "'after_train_iter'", ")", "\n", "", "", "self", ".", "_iter", "+=", "1", "\n", "\n", "", "self", ".", "call_hook", "(", "'after_train_epoch'", ")", "\n", "self", ".", "_epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.val": [[104, 106], ["None"], "methods", ["None"], ["", "def", "val", "(", "self", ",", "data_loader", ",", "**", "kwargs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.OmniSourceRunner.run": [[107, 163], ["isinstance", "mmcv.is_list_of", "omnisource_runner.OmniSourceRunner.logger.info", "omnisource_runner.OmniSourceRunner.logger.info", "omnisource_runner.OmniSourceRunner.call_hook", "time.sleep", "omnisource_runner.OmniSourceRunner.call_hook", "warnings.warn", "len", "mmcv.runner.utils.get_host_info", "isinstance", "range", "len", "getattr", "TypeError", "getattr.", "hasattr", "ValueError"], "methods", ["None"], ["", "def", "run", "(", "self", ",", "data_loaders", ",", "workflow", ",", "max_epochs", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Start running.\n\n        Args:\n            data_loaders (list[:obj:`DataLoader`]): Dataloaders for training.\n                `data_loaders[0]` is the main data_loader, which contains\n                target datasets and determines the epoch length.\n                `data_loaders[1:]` are auxiliary data loaders, which contain\n                auxiliary web datasets.\n            workflow (list[tuple]): A list of (phase, epochs) to specify the\n                running order and epochs. E.g, [('train', 2)] means running 2\n                epochs for training iteratively. Note that val epoch is not\n                supported for this runner for simplicity.\n            max_epochs (int | None): The max epochs that training lasts,\n                deprecated now. Default: None.\n        \"\"\"", "\n", "assert", "isinstance", "(", "data_loaders", ",", "list", ")", "\n", "assert", "mmcv", ".", "is_list_of", "(", "workflow", ",", "tuple", ")", "\n", "assert", "len", "(", "workflow", ")", "==", "1", "and", "workflow", "[", "0", "]", "[", "0", "]", "==", "'train'", "\n", "if", "max_epochs", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "'setting max_epochs in run is deprecated, '", "\n", "'please set max_epochs in runner_config'", ",", "DeprecationWarning", ")", "\n", "self", ".", "_max_epochs", "=", "max_epochs", "\n", "\n", "", "assert", "self", ".", "_max_epochs", "is", "not", "None", ",", "(", "\n", "'max_epochs must be specified during instantiation'", ")", "\n", "\n", "mode", ",", "epochs", "=", "workflow", "[", "0", "]", "\n", "self", ".", "_max_iters", "=", "self", ".", "_max_epochs", "*", "len", "(", "data_loaders", "[", "0", "]", ")", "\n", "\n", "work_dir", "=", "self", ".", "work_dir", "if", "self", ".", "work_dir", "is", "not", "None", "else", "'NONE'", "\n", "self", ".", "logger", ".", "info", "(", "'Start running, host: %s, work_dir: %s'", ",", "\n", "get_host_info", "(", ")", ",", "work_dir", ")", "\n", "self", ".", "logger", ".", "info", "(", "'workflow: %s, max: %d epochs'", ",", "workflow", ",", "\n", "self", ".", "_max_epochs", ")", "\n", "self", ".", "call_hook", "(", "'before_run'", ")", "\n", "\n", "while", "self", ".", "epoch", "<", "self", ".", "_max_epochs", ":", "\n", "            ", "if", "isinstance", "(", "mode", ",", "str", ")", ":", "# self.train()", "\n", "                ", "if", "not", "hasattr", "(", "self", ",", "mode", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f'runner has no method named \"{mode}\" to run an '", "\n", "'epoch'", ")", "\n", "", "epoch_runner", "=", "getattr", "(", "self", ",", "mode", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "f'mode in workflow must be a str, but got {mode}'", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "epochs", ")", ":", "\n", "                ", "if", "mode", "==", "'train'", "and", "self", ".", "epoch", ">=", "self", ".", "_max_epochs", ":", "\n", "                    ", "break", "\n", "", "epoch_runner", "(", "data_loaders", ",", "**", "kwargs", ")", "\n", "\n", "", "", "time", ".", "sleep", "(", "1", ")", "# wait for some hooks like loggers to finish", "\n", "self", ".", "call_hook", "(", "'after_run'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.runner.omnisource_runner.cycle": [[10, 17], ["iter", "next", "iter"], "function", ["None"], ["def", "cycle", "(", "iterable", ")", ":", "\n", "    ", "iterator", "=", "iter", "(", "iterable", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "yield", "next", "(", "iterator", ")", "\n", "", "except", "StopIteration", ":", "\n", "            ", "iterator", "=", "iter", "(", "iterable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__init__": [[18, 24], ["output.OutputHook.register"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.register"], ["def", "__init__", "(", "self", ",", "module", ",", "outputs", "=", "None", ",", "as_tensor", "=", "False", ")", ":", "\n", "        ", "self", ".", "outputs", "=", "outputs", "\n", "self", ".", "as_tensor", "=", "as_tensor", "\n", "self", ".", "layer_outputs", "=", "{", "}", "\n", "self", ".", "handles", "=", "[", "]", "\n", "self", ".", "register", "(", "module", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.register": [[25, 49], ["isinstance", "output.OutputHook.handles.append", "isinstance", "warnings.warn", "output.rgetattr", "rgetattr.register_forward_hook", "output.detach().cpu().numpy", "output.OutputHook.register.hook_wrapper"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.rgetattr"], ["", "def", "register", "(", "self", ",", "module", ")", ":", "\n", "\n", "        ", "def", "hook_wrapper", "(", "name", ")", ":", "\n", "\n", "            ", "def", "hook", "(", "model", ",", "input", ",", "output", ")", ":", "\n", "                ", "if", "not", "isinstance", "(", "output", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "warnings", ".", "warn", "(", "f'Directly return the output from {name}, '", "\n", "f'since it is not a tensor'", ")", "\n", "self", ".", "layer_outputs", "[", "name", "]", "=", "output", "\n", "", "elif", "self", ".", "as_tensor", ":", "\n", "                    ", "self", ".", "layer_outputs", "[", "name", "]", "=", "output", "\n", "", "else", ":", "\n", "                    ", "self", ".", "layer_outputs", "[", "name", "]", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "", "return", "hook", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "outputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "for", "name", "in", "self", ".", "outputs", ":", "\n", "                ", "try", ":", "\n", "                    ", "layer", "=", "rgetattr", "(", "module", ",", "name", ")", "\n", "h", "=", "layer", ".", "register_forward_hook", "(", "hook_wrapper", "(", "name", ")", ")", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "raise", "AttributeError", "(", "f'Module {name} not found'", ")", "\n", "", "self", ".", "handles", ".", "append", "(", "h", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove": [[50, 53], ["h.remove"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove"], ["", "", "", "def", "remove", "(", "self", ")", ":", "\n", "        ", "for", "h", "in", "self", ".", "handles", ":", "\n", "            ", "h", ".", "remove", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__enter__": [[54, 56], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.__exit__": [[57, 59], ["output.OutputHook.remove"], "methods", ["home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.OutputHook.remove"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "self", ".", "remove", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ucsc-vlaa_image-pretraining-for-video.hooks.output.rgetattr": [[63, 69], ["functools.reduce", "getattr", "attr.split"], "function", ["None"], ["", "", "def", "rgetattr", "(", "obj", ",", "attr", ",", "*", "args", ")", ":", "\n", "\n", "    ", "def", "_getattr", "(", "obj", ",", "attr", ")", ":", "\n", "        ", "return", "getattr", "(", "obj", ",", "attr", ",", "*", "args", ")", "\n", "\n", "", "return", "functools", ".", "reduce", "(", "_getattr", ",", "[", "obj", "]", "+", "attr", ".", "split", "(", "'.'", ")", ")", "\n", "", ""]]}