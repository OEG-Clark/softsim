{"home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_perturbations.parse_args": [[13, 49], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit", "print", "exit", "print", "exit", "print", "exit", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Generate perturbation scores based on a set of heatmaps'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify the noised images'", ")", "\n", "parser", ".", "add_argument", "(", "'-a'", ",", "'--analyzer'", ",", "type", "=", "str", ",", "help", "=", "'analyzer that compute the heatmaps'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-z'", ",", "'--baseline'", ",", "type", "=", "str", ",", "help", "=", "'baseline type to compute IG'", ",", "default", "=", "'zero'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_noise'", ",", "type", "=", "int", ",", "help", "=", "'number of noise baselines used by IG'", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--noise_scale'", ",", "type", "=", "float", ",", "help", "=", "'magnitude of the noise scale used to noise the image'", ",", "default", "=", "5e-1", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--noise_percent'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use proportion of image as noise scale'", ")", "\n", "parser", ".", "add_argument", "(", "'-an'", ",", "'--adaptive_noise'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use adaptive noise'", ")", "\n", "parser", ".", "add_argument", "(", "'-af'", ",", "'--adaptive_function'", ",", "type", "=", "str", ",", "help", "=", "'objective function used'", ",", "default", "=", "'aupc'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--num_roots'", ",", "type", "=", "int", ",", "help", "=", "'number of noised images used'", ",", "default", "=", "150", ")", "\n", "parser", ".", "add_argument", "(", "'-k'", ",", "'--kernel_size'", ",", "type", "=", "int", ",", "help", "=", "'size of the window of each perturbation'", ",", "default", "=", "15", ")", "\n", "parser", ".", "add_argument", "(", "'-pt'", ",", "'--num_perturbs'", ",", "type", "=", "int", ",", "help", "=", "'number of random perturbations to evaluate'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--num_regions'", ",", "type", "=", "int", ",", "help", "=", "'number of regions to perturbate'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-d'", ",", "'--draw_mode'", ",", "type", "=", "int", ",", "help", "=", "'perturb draw mode: 0 - uniform; 1 - gaussian according to image stats'", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "analyzer", "not", "in", "ANALYZERS", ":", "\n", "        ", "print", "(", "'Invalid analyzer:'", ",", "args", ".", "analyzer", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "baseline", "not", "in", "IG_BASELINES", ":", "\n", "        ", "print", "(", "'Invalid IG baseline name:'", ",", "args", ".", "baseline", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "draw_mode", ">", "1", ":", "\n", "        ", "print", "(", "'Invalid draw mode:'", ",", "args", ".", "draw_mode", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "adaptive_function", "not", "in", "[", "'aupc'", ",", "'autvc'", "]", ":", "\n", "        ", "print", "(", "'Invalid adaptive objective function:'", ",", "args", ".", "adaptive_function", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_perturbations.run_perturbations_evaluation": [[51, 164], ["os.path.join", "os.path.join", "numpy.load", "os.path.join", "numpy.load", "os.path.join", "len", "enumerate", "os.path.join", "numpy.save", "os.path.exists", "print", "exit", "os.path.join", "os.path.exists", "print", "exit", "os.path.join", "os.path.isdir", "os.makedirs", "numpy.ones", "tqdm.tqdm", "os.path.basename", "os.path.join", "os.path.join", "numpy.load", "attribution.eval.compute_perturbations", "numpy.save", "str", "os.path.exists", "print", "numpy.load", "os.path.exists", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "str", "os.path.join", "os.path.join", "os.path.join", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_perturbations"], ["", "def", "run_perturbations_evaluation", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "analyzer", ",", "\n", "noise_scale", "=", "None", ",", "num_roots", "=", "None", ",", "\n", "noise_percent", "=", "False", ",", "baseline", "=", "None", ",", "num_noise", "=", "None", ",", "\n", "kernel_size", "=", "15", ",", "num_regions", "=", "30", ",", "\n", "num_perturbs", "=", "50", ",", "draw_mode", "=", "0", ",", "adaptive_noise", "=", "False", ",", "adaptive_function", "=", "None", ",", "overwrite", "=", "False", ")", ":", "\n", "# Read all scores and top10idxs for that model", "\n", "    ", "input_dir", "=", "os", ".", "path", ".", "join", "(", "'output/'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "input_dir", ")", ":", "\n", "        ", "print", "(", "'Model classification output not found for:'", ",", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_scores.npy'", ")", "\n", "all_scores", "=", "np", ".", "load", "(", "input_path", ")", "\n", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "all_top10_idxs", "=", "np", ".", "load", "(", "input_path", ")", "\n", "\n", "# Prepare input heatmaps directory", "\n", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "assert", "(", "noise_scale", ")", "\n", "assert", "(", "num_roots", ")", "\n", "if", "adaptive_noise", ":", "\n", "            ", "assert", "(", "adaptive_function", ")", "\n", "noise_folder", "=", "'adaptive/'", "+", "adaptive_function", "\n", "", "elif", "noise_percent", ":", "\n", "            ", "noise_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "", "else", ":", "\n", "            ", "noise_folder", "=", "'{:.1e}'", ".", "format", "(", "noise_scale", ")", "# convert to scientific notation", "\n", "", "num_roots_folder", "=", "str", "(", "num_roots", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "noise_folder", ",", "num_roots_folder", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "assert", "(", "baseline", ")", "\n", "if", "baseline", "==", "'zero'", ":", "\n", "            ", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "baseline", ")", "\n", "", "elif", "baseline", "==", "'noise'", ":", "\n", "            ", "assert", "(", "num_noise", ")", "\n", "num_noise_folder", "=", "str", "(", "num_noise", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "baseline", ",", "num_noise_folder", ")", "\n", "", "", "elif", "analyzer", "==", "'grad'", ":", "\n", "        ", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ")", "\n", "", "elif", "analyzer", "==", "'smooth-grad'", ":", "\n", "        ", "assert", "(", "num_noise", ")", "\n", "assert", "(", "noise_scale", ")", "\n", "noise_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "num_noise_folder", "=", "str", "(", "num_noise", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "noise_folder", ",", "num_noise_folder", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "heatmap_dir", ")", ":", "\n", "        ", "print", "(", "'Heatmaps folders missing:'", ",", "heatmap_dir", ")", "\n", "exit", "(", ")", "\n", "\n", "# Prepare output directory", "\n", "", "output_dir", "=", "os", ".", "path", ".", "join", "(", "'perturbations/'", ",", "analyzer", ",", "model_name", ")", "\n", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "noise_folder", ",", "num_roots_folder", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "if", "baseline", "==", "'zero'", ":", "\n", "            ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "baseline", ")", "\n", "", "elif", "baseline", "==", "'noise'", ":", "\n", "            ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "baseline", ",", "num_noise_folder", ")", "\n", "", "", "elif", "analyzer", "==", "'smooth-grad'", ":", "\n", "        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "noise_folder", ",", "num_noise_folder", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n", "# Initialize perturbation scores", "\n", "", "dataset_size", "=", "len", "(", "dataset", ")", "\n", "all_perturb_scores", "=", "np", ".", "ones", "(", "(", "dataset_size", ",", "num_regions", "+", "1", ")", ")", "*", "np", ".", "inf", "\n", "\n", "# Go through each image", "\n", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Check if perturbation scores exists", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "perturbation_filename", "=", "'perturbations_scores_{0}k_{1}p_{2}r_{3}d_{4}.npy'", ".", "format", "(", "\n", "kernel_size", ",", "num_perturbs", ",", "num_regions", ",", "draw_mode", ",", "img_filename", "\n", ")", "\n", "perturb_outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "perturbation_filename", ")", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "perturb_outpath", ")", ":", "# ignore if the perturbation done", "\n", "            ", "print", "(", "img_filename", ",", "'already has perturbation scores computed'", ")", "\n", "all_perturb_scores", "[", "img_idx", "]", "=", "np", ".", "load", "(", "perturb_outpath", ")", "\n", "continue", "\n", "\n", "# Load the image heatmap", "\n", "", "heatmap_filepath", "=", "os", ".", "path", ".", "join", "(", "heatmap_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "heatmap_filepath", ")", ":", "\n", "            ", "print", "(", "'Heatmaps not found for:'", ",", "heatmap_filepath", ")", "\n", "continue", "\n", "", "heatmap", "=", "np", ".", "load", "(", "heatmap_filepath", ")", "\n", "\n", "# Save top score as the starting perturb score as the first region", "\n", "img_input", "=", "dataset", "[", "img_idx", "]", "[", "'image'", "]", "\n", "predicted_class", "=", "all_top10_idxs", "[", "img_idx", ",", "0", "]", "\n", "all_perturb_scores", "[", "img_idx", ",", "0", "]", "=", "all_scores", "[", "img_idx", ",", "predicted_class", "]", "\n", "\n", "mean_perturb_scores", "=", "compute_perturbations", "(", "\n", "img_input", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "explained_class", "=", "predicted_class", ",", "\n", "heatmap", "=", "heatmap", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "draw_mode", "=", "draw_mode", ",", "\n", "num_regions", "=", "num_regions", ",", "\n", "num_perturbs", "=", "num_perturbs", "\n", ")", "\n", "\n", "# Update all perturb scores and save individual scores", "\n", "all_perturb_scores", "[", "img_idx", ",", "1", ":", "]", "=", "mean_perturb_scores", "\n", "np", ".", "save", "(", "perturb_outpath", ",", "all_perturb_scores", "[", "img_idx", "]", ")", "\n", "\n", "# Save all perturbation scores", "\n", "", "out_filename", "=", "'all_perturbations_scores_{0}k_{1}p_{2}r_{3}d.npy'", ".", "format", "(", "\n", "kernel_size", ",", "num_perturbs", ",", "num_regions", ",", "draw_mode", "\n", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "out_filename", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_perturb_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_grad.parse_args": [[13, 28], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Generate gradients and smooth grad heatmaps'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--smooth'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use smooth grad'", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--noise_scale'", ",", "type", "=", "float", ",", "help", "=", "'percentage noise scale'", ",", "default", "=", "15", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_noise'", ",", "type", "=", "int", ",", "help", "=", "'number of noise inputs to use'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_grad.run_grad_experiment": [[30, 94], ["os.path.join", "os.path.join", "numpy.load", "os.path.join", "numpy.load", "enumerate", "os.path.exists", "print", "exit", "os.path.join", "os.path.join", "os.path.join", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "os.path.basename", "os.path.join", "numpy.sum", "numpy.save", "str", "str", "os.path.exists", "print", "torch.stack", "range", "torch.utils.data.dataset.TensorDataset", "torch.utils.data.DataLoader", "attribution.get_gradients", "numpy.mean", "torch.utils.data.dataset.TensorDataset", "torch.utils.data.DataLoader", "torch.unsqueeze", "attribution.get_gradients", "torch.zeros_like", "numpy.max", "numpy.min", "range", "img_input.numpy", "img_input.numpy", "torch.randn_like"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.get_gradients", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.get_gradients"], ["", "def", "run_grad_experiment", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "smooth", ",", "\n", "num_noise", ",", "noise_scale", ",", "overwrite", "=", "False", ")", ":", "\n", "# Read all scores and top10idxs for that model", "\n", "    ", "input_dir", "=", "os", ".", "path", ".", "join", "(", "'output/'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "input_dir", ")", ":", "\n", "        ", "print", "(", "'Model classification output not found for:'", ",", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "all_top10_idxs", "=", "np", ".", "load", "(", "input_path", ")", "\n", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_scores.npy'", ")", "\n", "all_scores", "=", "np", ".", "load", "(", "input_path", ")", "\n", "\n", "# Check output directory", "\n", "if", "smooth", ":", "\n", "        ", "noise_scale_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "num_noise_folder", "=", "str", "(", "num_noise", ")", "+", "'N'", "\n", "noise_scale", "=", "noise_scale", "/", "100.", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/smooth-grad/'", ",", "model_name", ")", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "noise_scale_folder", ",", "num_noise_folder", ")", "\n", "", "else", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/grad/'", ",", "model_name", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "\n", "\n", "# Generate grad and heatmap for each image", "\n", "", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Initialize output filepath", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "outpath", ")", ":", "# ignore if the heatmap is already generated", "\n", "            ", "print", "(", "img_filepath", ",", "'already has heatmap generated'", ")", "\n", "continue", "\n", "\n", "# Retrieve the image data and predicted class", "\n", "", "predicted_class", "=", "all_top10_idxs", "[", "img_idx", ",", "0", "]", "\n", "img_input", "=", "dataset", "[", "img_idx", "]", "[", "'image'", "]", "\n", "input_score", "=", "all_scores", "[", "img_idx", ",", "predicted_class", "]", "\n", "# print('input score:', input_score)", "\n", "\n", "if", "smooth", ":", "\n", "# Generate noised dataset based on the original inputs with additive noise", "\n", "            ", "noised_inputs", "=", "torch", ".", "stack", "(", "[", "torch", ".", "zeros_like", "(", "img_input", ")", "for", "_", "in", "range", "(", "num_noise", ")", "]", ")", "\n", "img_noise_scale", "=", "noise_scale", "*", "(", "np", ".", "max", "(", "img_input", ".", "numpy", "(", ")", ")", "-", "np", ".", "min", "(", "img_input", ".", "numpy", "(", ")", ")", ")", "\n", "for", "i", "in", "range", "(", "num_noise", ")", ":", "\n", "                ", "noised_inputs", "[", "i", "]", "=", "img_input", "+", "img_noise_scale", "*", "torch", ".", "randn_like", "(", "img_input", ")", "\n", "", "noised_dataset", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "TensorDataset", "(", "noised_inputs", ")", "\n", "noised_data_loader", "=", "DataLoader", "(", "noised_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "# Compute the gradients w.r.t. explained class for each noised input", "\n", "gradients", "=", "get_gradients", "(", "noised_data_loader", ",", "model", ",", "explained_class", "=", "predicted_class", ")", "\n", "grad", "=", "np", ".", "mean", "(", "gradients", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "single_dataset", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "TensorDataset", "(", "torch", ".", "unsqueeze", "(", "img_input", ",", "0", ")", ")", "\n", "single_data_loader", "=", "DataLoader", "(", "single_dataset", ",", "batch_size", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "grad", "=", "get_gradients", "(", "\n", "data_loader", "=", "single_data_loader", ",", "\n", "model", "=", "model", ",", "\n", "explained_class", "=", "predicted_class", "\n", ")", "[", "0", "]", "\n", "\n", "# Save grad heatmap", "\n", "", "heatmap", "=", "np", ".", "sum", "(", "grad", ",", "axis", "=", "0", ")", "# aggregate along color channel", "\n", "np", ".", "save", "(", "outpath", ",", "heatmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_ig.parse_args": [[13, 31], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Generate integrated gradients'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-k'", ",", "'--steps'", ",", "type", "=", "int", ",", "help", "=", "'number of steps along path'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-z'", ",", "'--baseline_type'", ",", "type", "=", "str", ",", "help", "=", "'baseline type to use'", ",", "default", "=", "'zero'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_noise'", ",", "type", "=", "int", ",", "help", "=", "'number of noise baselines to use'", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "baseline_type", "not", "in", "IG_BASELINES", ":", "\n", "        ", "print", "(", "'Invalid IG baseline type:'", ",", "args", ".", "baseline_type", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_ig.run_ig_experiment": [[33, 101], ["os.path.join", "os.path.join", "numpy.load", "os.path.join", "numpy.load", "os.path.join", "enumerate", "os.path.exists", "print", "exit", "os.path.join", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "os.path.basename", "os.path.join", "numpy.sum", "numpy.save", "os.path.exists", "print", "torch.zeros_like", "NORMALIZE_TRANSFORM", "attribution.integrated_gradients", "str", "range", "numpy.mean", "torch.rand_like", "NORMALIZE_TRANSFORM", "attribution.integrated_gradients", "all_igs.append", "numpy.array"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients"], ["", "def", "run_ig_experiment", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "baseline_type", ",", "\n", "num_noise", ",", "steps", ",", "overwrite", "=", "False", ")", ":", "\n", "# Read all scores and top10idxs for that model", "\n", "    ", "input_dir", "=", "os", ".", "path", ".", "join", "(", "'output/'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "input_dir", ")", ":", "\n", "        ", "print", "(", "'Model classification output not found for:'", ",", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "all_top10_idxs", "=", "np", ".", "load", "(", "input_path", ")", "\n", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_scores.npy'", ")", "\n", "all_scores", "=", "np", ".", "load", "(", "input_path", ")", "\n", "\n", "# Check output directory", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/ig/'", ",", "model_name", ",", "baseline_type", ")", "\n", "if", "baseline_type", "==", "'noise'", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "str", "(", "num_noise", ")", "+", "'N'", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "\n", "# Generate IG and heatmap for each image", "\n", "", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Initialize output filepath", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "outpath", ")", ":", "# ignore if the heatmap is already generated", "\n", "            ", "print", "(", "img_filepath", ",", "'already has heatmap generated'", ")", "\n", "continue", "\n", "\n", "# Retrieve the image data and predicted class", "\n", "", "predicted_class", "=", "all_top10_idxs", "[", "img_idx", ",", "0", "]", "\n", "img_input", "=", "dataset", "[", "img_idx", "]", "[", "'image'", "]", "\n", "input_score", "=", "all_scores", "[", "img_idx", ",", "predicted_class", "]", "\n", "# print('input score:', input_score)", "\n", "\n", "if", "baseline_type", "==", "'zero'", ":", "\n", "            ", "baseline", "=", "torch", ".", "zeros_like", "(", "img_input", ")", "\n", "baseline", "=", "NORMALIZE_TRANSFORM", "(", "baseline", ")", "\n", "ig", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "predicted_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "# To check for completeness:", "\n", "# out = model(torch.unsqueeze(baseline, 0))", "\n", "# print('baseline score:',  out[:, explained_class].detach().numpy()[0])", "\n", "# print('IG sum:', np.sum(ig))", "\n", "", "elif", "baseline_type", "==", "'noise'", ":", "\n", "            ", "all_igs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_noise", ")", ":", "\n", "                ", "baseline", "=", "torch", ".", "rand_like", "(", "img_input", ")", "# uniform between 0 and 1", "\n", "baseline", "=", "NORMALIZE_TRANSFORM", "(", "baseline", ")", "\n", "ig", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "predicted_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "all_igs", ".", "append", "(", "ig", ")", "\n", "", "ig", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "all_igs", ")", ",", "axis", "=", "0", ")", "\n", "\n", "# Save IG heatmap", "\n", "", "heatmap", "=", "np", ".", "sum", "(", "ig", ",", "axis", "=", "0", ")", "# aggregate along color channel", "\n", "np", ".", "save", "(", "outpath", ",", "heatmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_total_variation.parse_args": [[13, 45], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit", "print", "exit", "print", "exit", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Generate multi-scaled average total variation based on a set of heatmaps'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify the noised images'", ")", "\n", "parser", ".", "add_argument", "(", "'-a'", ",", "'--analyzer'", ",", "type", "=", "str", ",", "help", "=", "'analyzer that compute the heatmaps'", ")", "\n", "parser", ".", "add_argument", "(", "'-z'", ",", "'--baseline'", ",", "type", "=", "str", ",", "help", "=", "'baseline type to compute IG'", ",", "default", "=", "'zero'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--num_noise'", ",", "type", "=", "int", ",", "help", "=", "'number of noise baselines used by IG'", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--noise_scale'", ",", "type", "=", "float", ",", "help", "=", "'magnitude of the noise scale used to noise the image'", ",", "default", "=", "5e-1", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--noise_percent'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use proportion of image as noise scale'", ")", "\n", "parser", ".", "add_argument", "(", "'-an'", ",", "'--adaptive_noise'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use adaptive noise'", ")", "\n", "parser", ".", "add_argument", "(", "'-af'", ",", "'--adaptive_function'", ",", "type", "=", "str", ",", "help", "=", "'objective function used'", ",", "default", "=", "'aupc'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--num_roots'", ",", "type", "=", "int", ",", "help", "=", "'number of noised images used'", ",", "default", "=", "150", ")", "\n", "parser", ".", "add_argument", "(", "'-ds'", ",", "'--downscale'", ",", "type", "=", "float", ",", "help", "=", "'factor to downscale heatmap'", ",", "default", "=", "1.5", ")", "\n", "parser", ".", "add_argument", "(", "'-wms'", ",", "'--width_min_size'", ",", "type", "=", "int", ",", "help", "=", "'minimum width dimension for downscale'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-hms'", ",", "'--height_min_size'", ",", "type", "=", "int", ",", "help", "=", "'minimum height dimension for downscale'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-lp'", ",", "'--lp_norm'", ",", "type", "=", "int", ",", "help", "=", "'norm to use to calculate TV'", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "analyzer", "not", "in", "ANALYZERS", ":", "\n", "        ", "print", "(", "'Invalid analyzer:'", ",", "args", ".", "analyzer", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "baseline", "not", "in", "IG_BASELINES", ":", "\n", "        ", "print", "(", "'Invalid IG baseline name:'", ",", "args", ".", "baseline", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "adaptive_function", "not", "in", "[", "'aupc'", ",", "'autvc'", "]", ":", "\n", "        ", "print", "(", "'Invalid adaptive objective function:'", ",", "args", ".", "adaptive_function", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_total_variation.run_total_variation_evaluation": [[47, 138], ["os.path.join", "enumerate", "os.path.join", "numpy.array", "numpy.save", "os.path.join", "os.path.exists", "print", "exit", "os.path.join", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "os.path.basename", "os.path.join", "os.path.join", "numpy.load", "attribution.eval.compute_multiscaled_atv", "np.array.append", "numpy.save", "str", "os.path.exists", "print", "np.array.append", "os.path.exists", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "numpy.load", "str", "os.path.join", "os.path.join", "os.path.join", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_multiscaled_atv"], ["", "def", "run_total_variation_evaluation", "(", "dataset", ",", "model_name", ",", "analyzer", ",", "\n", "noise_scale", "=", "None", ",", "num_roots", "=", "None", ",", "\n", "noise_percent", "=", "False", ",", "baseline", "=", "None", ",", "num_noise", "=", "None", ",", "\n", "downscale", "=", "1.5", ",", "min_size", "=", "(", "30", ",", "30", ")", ",", "lp_norm", "=", "1", ",", "\n", "adaptive_noise", "=", "False", ",", "adaptive_function", "=", "None", ",", "overwrite", "=", "False", ")", ":", "\n", "# Prepare input heatmaps directory", "\n", "    ", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "assert", "(", "noise_scale", ")", "\n", "assert", "(", "num_roots", ")", "\n", "if", "adaptive_noise", ":", "\n", "            ", "assert", "(", "adaptive_function", ")", "\n", "noise_folder", "=", "'adaptive/'", "+", "adaptive_function", "\n", "", "elif", "noise_percent", ":", "\n", "            ", "noise_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "", "else", ":", "\n", "            ", "noise_folder", "=", "'{:.1e}'", ".", "format", "(", "noise_scale", ")", "# convert to scientific notation", "\n", "", "num_roots_folder", "=", "str", "(", "num_roots", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "noise_folder", ",", "num_roots_folder", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "assert", "(", "baseline", ")", "\n", "if", "baseline", "==", "'zero'", ":", "\n", "            ", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "baseline", ")", "\n", "", "elif", "baseline", "==", "'noise'", ":", "\n", "            ", "assert", "(", "num_noise", ")", "\n", "num_noise_folder", "=", "str", "(", "num_noise", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "baseline", ",", "num_noise_folder", ")", "\n", "", "", "elif", "analyzer", "==", "'grad'", ":", "\n", "        ", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ")", "\n", "", "elif", "analyzer", "==", "'smooth-grad'", ":", "\n", "        ", "assert", "(", "num_noise", ")", "\n", "assert", "(", "noise_scale", ")", "\n", "noise_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "num_noise_folder", "=", "str", "(", "num_noise", ")", "+", "'N'", "\n", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "analyzer", ",", "model_name", ",", "noise_folder", ",", "num_noise_folder", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "heatmap_dir", ")", ":", "\n", "        ", "print", "(", "'Heatmaps folders missing:'", ",", "heatmap_dir", ")", "\n", "exit", "(", ")", "\n", "\n", "# Prepare output directory", "\n", "", "output_dir", "=", "os", ".", "path", ".", "join", "(", "'atv/'", ",", "analyzer", ",", "model_name", ")", "\n", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "noise_folder", ",", "num_roots_folder", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "if", "baseline", "==", "'zero'", ":", "\n", "            ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "baseline", ")", "\n", "", "elif", "baseline", "==", "'noise'", ":", "\n", "            ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "baseline", ",", "num_noise_folder", ")", "\n", "", "", "elif", "analyzer", "==", "'smooth-grad'", ":", "\n", "        ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "noise_folder", ",", "num_noise_folder", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "\n", "# Go through each image and compute ATV scores", "\n", "", "all_atv_scores", "=", "[", "]", "\n", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Check if ATV scores exists", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "atv_filename", "=", "'atv_scores_{0}ds_{1}ms_{2}l_{3}.npy'", ".", "format", "(", "\n", "downscale", ",", "min_size", ",", "lp_norm", ",", "img_filename", "\n", ")", "\n", "atv_outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "atv_filename", ")", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "atv_outpath", ")", ":", "# ignore if done", "\n", "            ", "print", "(", "img_filename", ",", "'already has ATV scores computed'", ")", "\n", "all_atv_scores", ".", "append", "(", "np", ".", "load", "(", "atv_outpath", ")", ")", "\n", "continue", "\n", "\n", "# Load the image heatmap", "\n", "", "heatmap_filepath", "=", "os", ".", "path", ".", "join", "(", "heatmap_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "heatmap_filepath", ")", ":", "\n", "            ", "print", "(", "'Heatmaps not found for:'", ",", "heatmap_filepath", ")", "\n", "continue", "\n", "", "heatmap", "=", "np", ".", "load", "(", "heatmap_filepath", ")", "\n", "\n", "multi_scaled_ATV", "=", "compute_multiscaled_atv", "(", "\n", "heatmap", "=", "heatmap", ",", "\n", "downscale", "=", "downscale", ",", "\n", "min_size", "=", "min_size", ",", "\n", "lp_norm", "=", "lp_norm", "\n", ")", "\n", "\n", "# Update all ATV scores and save individual scores", "\n", "all_atv_scores", ".", "append", "(", "multi_scaled_ATV", ")", "\n", "np", ".", "save", "(", "atv_outpath", ",", "multi_scaled_ATV", ")", "\n", "\n", "# Save all ATV scores", "\n", "", "out_filename", "=", "'all_ATV_scores_{0}ds_{1}ms_{2}l.npy'", ".", "format", "(", "\n", "downscale", ",", "min_size", ",", "lp_norm", "\n", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "out_filename", ")", "\n", "all_atv_scores", "=", "np", ".", "array", "(", "all_atv_scores", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_atv_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_classify.parse_args": [[12, 22], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Classify the image dataset and saves all prediction outputs'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "128", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_classify.run_classify_experiment": [[24, 96], ["torch.utils.data.DataLoader", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "len", "enumerate", "os.path.join", "numpy.save", "os.path.join", "numpy.save", "os.path.join", "numpy.save", "os.path.join", "numpy.save", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "print", "inputs.to.to", "labels.to.to", "torch.sum", "print", "print", "sample_batch[].size", "torch.no_grad", "model", "torch.max", "torch.topk", "top10idxs.cpu().numpy.cpu().numpy", "model.cpu().numpy", "enumerate", "float", "os.path.basename", "os.path.join", "numpy.save", "os.path.join", "numpy.save", "int", "top10idxs.cpu().numpy.cpu", "model.cpu", "epoch_acc.cpu().numpy", "running_acc.cpu().numpy", "epoch_acc.cpu", "running_acc.cpu"], "function", ["None"], ["", "def", "run_classify_experiment", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "num_classes", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        dataset (Dataset): Containing input images.\n        model (PyTorch model): Pre-trained Pytorch classifier.\n        model_name (str): Name of the model.\n        batch_size (int): Batch size to use during each epoch.\n        num_classes (int): Total number of classes.\n    \"\"\"", "\n", "# Load the dataset into a data loader", "\n", "data_loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "dataset_size", "=", "len", "(", "dataset", ")", "\n", "\n", "# Initialize all scores, top10 indexes and labels", "\n", "all_scores", "=", "np", ".", "zeros", "(", "(", "dataset_size", ",", "num_classes", ")", ")", "\n", "all_top10_idxs", "=", "np", ".", "zeros", "(", "(", "dataset_size", ",", "10", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "all_labels", "=", "np", ".", "zeros", "(", "(", "dataset_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "all_positives", "=", "np", ".", "zeros", "(", "(", "dataset_size", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "# Create output directory", "\n", "out_dir", "=", "'output/'", "+", "model_name", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "\n", "# Perform classification", "\n", "", "running_corrects", "=", "0.0", "\n", "total_epochs", "=", "len", "(", "data_loader", ")", "\n", "for", "i_batch", ",", "sample_batch", "in", "enumerate", "(", "tqdm", "(", "data_loader", ",", "desc", "=", "'Epoch'", ")", ")", ":", "\n", "        ", "print", "(", "'Epoch #%d/%d:'", "%", "(", "i_batch", "+", "1", ",", "total_epochs", ")", ",", "'batch_shape:'", ",", "sample_batch", "[", "'image'", "]", ".", "size", "(", ")", ")", "\n", "inputs", "=", "sample_batch", "[", "'image'", "]", "\n", "labels", "=", "sample_batch", "[", "'label'", "]", "\n", "fpaths", "=", "sample_batch", "[", "'filepath'", "]", "\n", "inputs", "=", "inputs", ".", "to", "(", "DEVICE", ")", "\n", "labels", "=", "labels", ".", "to", "(", "DEVICE", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "out", "=", "model", "(", "inputs", ")", "\n", "_", ",", "idx", "=", "torch", ".", "max", "(", "out", ",", "1", ")", "\n", "_", ",", "top10idxs", "=", "torch", ".", "topk", "(", "out", ",", "k", "=", "10", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ",", "out", "=", "None", ")", "\n", "\n", "# Save outputs into .npy files for each individual image", "\n", "top10idxs", "=", "top10idxs", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "scores", "=", "out", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "filepath", "in", "enumerate", "(", "fpaths", ")", ":", "\n", "                ", "filename", "=", "os", ".", "path", ".", "basename", "(", "filepath", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "filename", "+", "'_scores.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "scores", "[", "i", ",", ":", "]", ")", "\n", "all_scores", "[", "i_batch", "*", "batch_size", "+", "i", ",", ":", "]", "=", "scores", "[", "i", ",", ":", "]", "\n", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "filename", "+", "'_top10idxs.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "top10idxs", "[", "i", ",", ":", "]", ")", "\n", "all_top10_idxs", "[", "i_batch", "*", "batch_size", "+", "i", ",", ":", "]", "=", "top10idxs", "[", "i", ",", ":", "]", "\n", "\n", "all_labels", "[", "i_batch", "*", "batch_size", "+", "i", "]", "=", "labels", "[", "i", "]", "\n", "all_positives", "[", "i_batch", "*", "batch_size", "+", "i", "]", "=", "int", "(", "labels", "[", "i", "]", "==", "top10idxs", "[", "i", ",", "0", "]", ")", "\n", "\n", "", "", "num_correct", "=", "torch", ".", "sum", "(", "(", "idx", "==", "labels", ")", ".", "float", "(", ")", ")", "\n", "running_corrects", "+=", "num_correct", "\n", "epoch_acc", "=", "num_correct", "/", "float", "(", "batch_size", ")", "\n", "running_acc", "=", "running_corrects", "/", "dataset_size", "\n", "print", "(", "'Epoch Accuracy: %.2f%%'", "%", "(", "epoch_acc", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "*", "100", ")", ")", "\n", "print", "(", "'Running Accuracy: %.2f%%'", "%", "(", "running_acc", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "*", "100", ")", ")", "\n", "\n", "# Save all scores, all top10 indexes, all labels, and all positives", "\n", "", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'all_scores.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_scores", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_top10_idxs", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'all_labels.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_labels", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'all_positives.npy'", ")", "\n", "np", ".", "save", "(", "outpath", ",", "all_positives", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_adaptive_noising.parse_args": [[14, 43], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Perform adaptive noising per image for smooth integrated gradients'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--num_roots'", ",", "type", "=", "int", ",", "help", "=", "'number of noised images used'", ",", "default", "=", "150", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "'--obj_function'", ",", "type", "=", "str", ",", "help", "=", "'objective function to optimize'", ",", "default", "=", "'aupc'", ")", "\n", "parser", ".", "add_argument", "(", "'-ds'", ",", "'--downscale'", ",", "type", "=", "float", ",", "help", "=", "'factor to downscale heatmap'", ",", "default", "=", "1.5", ")", "\n", "parser", ".", "add_argument", "(", "'-wms'", ",", "'--width_min_size'", ",", "type", "=", "int", ",", "help", "=", "'minimum width dimension for downscale'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-hms'", ",", "'--height_min_size'", ",", "type", "=", "int", ",", "help", "=", "'minimum height dimension for downscale'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-lp'", ",", "'--lp_norm'", ",", "type", "=", "int", ",", "help", "=", "'norm to use to calculate TV'", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-k'", ",", "'--kernel_size'", ",", "type", "=", "int", ",", "help", "=", "'size of the window of each perturbation'", ",", "default", "=", "15", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--num_perturbs'", ",", "type", "=", "int", ",", "help", "=", "'number of random perturbations to evaluate'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--num_regions'", ",", "type", "=", "int", ",", "help", "=", "'number of regions to perturbate'", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "'-d'", ",", "'--draw_mode'", ",", "type", "=", "int", ",", "help", "=", "'perturb draw mode: 0 - uniform; 1 - gaussian according to image stats'", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'-lr'", ",", "'--learning_rate'", ",", "type", "=", "float", ",", "help", "=", "'learning rate for variable update'", ",", "default", "=", "0.1", ")", "\n", "parser", ".", "add_argument", "(", "'-y'", ",", "'--learning_decay'", ",", "type", "=", "float", ",", "help", "=", "'decay rate of learning rate'", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'-c'", ",", "'--max_stop_count'", ",", "type", "=", "int", ",", "help", "=", "'maximum stop count to terminate search'", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'-x'", ",", "'--max_iteration'", ",", "type", "=", "int", ",", "help", "=", "'maximum iterations to search'", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "if", "args", ".", "obj_function", "not", "in", "[", "'aupc'", ",", "'autvc'", "]", ":", "\n", "        ", "print", "(", "'Invalid objective function:'", ",", "args", ".", "obj_function", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_adaptive_noising.adapt_noise": [[45, 96], ["numpy.mean", "compute_auc", "numpy.abs", "math.fabs", "compute_auc", "print", "img_input.numpy", "math.fabs", "compute_auc", "math.fabs", "print", "print"], "function", ["None"], ["", "def", "adapt_noise", "(", "img_input", ",", "model", ",", "num_roots", ",", "learning_rate", ",", "learning_decay", ",", "\n", "max_stop_count", ",", "max_iteration", ",", "obj_function", ",", "params", ")", ":", "\n", "# Initialize values", "\n", "    ", "if", "obj_function", "==", "'aupc'", ":", "\n", "        ", "compute_auc", "=", "compute_aupc", "\n", "", "elif", "obj_function", "==", "'autvc'", ":", "\n", "        ", "compute_auc", "=", "compute_autvc", "\n", "", "current_noise", "=", "np", ".", "mean", "(", "np", ".", "abs", "(", "img_input", ".", "numpy", "(", ")", ")", ")", "# initialize noise = absolute mean", "\n", "params", "[", "'noise_scale'", "]", "=", "current_noise", "\n", "current_heatmap", ",", "current_score", ",", "current_auc", "=", "compute_auc", "(", "**", "params", ")", "\n", "# print('initial noise:', current_noise)", "\n", "# print('initial auc:', current_auc)", "\n", "best_noise", "=", "current_noise", "\n", "best_auc", "=", "current_auc", "\n", "best_score", "=", "current_score", "\n", "best_heatmap", "=", "current_heatmap", "\n", "stop_count", "=", "0", "\n", "lr", "=", "learning_rate", "\n", "iteration", "=", "1", "\n", "\n", "while", "iteration", "<=", "max_iteration", ":", "\n", "# Find direction", "\n", "        ", "params", "[", "'noise_scale'", "]", "=", "math", ".", "fabs", "(", "current_noise", "+", "lr", ")", "\n", "current_heatmap", ",", "current_score", ",", "search_auc", "=", "compute_auc", "(", "**", "params", ")", "\n", "if", "search_auc", ">", "current_auc", ":", "\n", "            ", "current_noise", "=", "math", ".", "fabs", "(", "current_noise", "-", "lr", ")", "\n", "params", "[", "'noise_scale'", "]", "=", "current_noise", "\n", "current_heatmap", ",", "current_score", ",", "search_auc", "=", "compute_auc", "(", "**", "params", ")", "\n", "", "else", ":", "\n", "            ", "current_noise", "=", "math", ".", "fabs", "(", "current_noise", "+", "lr", ")", "\n", "", "print", "(", "'update noise -'", ",", "'noise:'", ",", "current_noise", ")", "\n", "\n", "# Early stopping", "\n", "if", "search_auc", ">", "current_auc", ":", "# worse", "\n", "            ", "if", "stop_count", "<", "max_stop_count", ":", "\n", "                ", "lr", "=", "lr", "*", "learning_decay", "# reduce lr", "\n", "stop_count", "+=", "1", "\n", "", "else", ":", "\n", "                ", "print", "(", "'finished -'", ",", "'best auc:'", ",", "best_auc", ",", "'best noise:'", ",", "best_noise", ")", "\n", "break", "# exit the loop", "\n", "", "", "else", ":", "# improved", "\n", "            ", "stop_count", "=", "0", "\n", "if", "search_auc", "<", "best_auc", ":", "\n", "                ", "best_auc", "=", "search_auc", "\n", "best_noise", "=", "current_noise", "\n", "best_score", "=", "current_score", "\n", "best_heatmap", "=", "current_heatmap", "\n", "print", "(", "'update best -'", ",", "'best auc:'", ",", "best_auc", ",", "'best noise:'", ",", "best_noise", ")", "\n", "", "", "current_auc", "=", "search_auc", "\n", "iteration", "+=", "1", "\n", "", "return", "best_noise", ",", "best_heatmap", ",", "best_auc", ",", "best_score", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_adaptive_noising.run_adapt_noise_experiment": [[98, 254], ["os.path.join", "os.path.join", "numpy.load", "os.path.join", "numpy.load", "os.path.join", "os.path.join", "all_noise_file_format.format", "os.path.join", "all_scores_file_format.format", "os.path.join", "len", "numpy.zeros", "enumerate", "numpy.save", "numpy.save", "os.path.exists", "print", "exit", "os.path.isdir", "os.makedirs", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "os.path.basename", "noise_file_format.format", "os.path.join", "score_file_format.format", "os.path.join", "os.path.join", "experiment_adaptive_noising.adapt_noise", "numpy.save", "all_scores.append", "os.path.join", "numpy.save", "numpy.save", "numpy.array", "str", "os.path.exists", "os.path.exists", "os.path.exists", "print", "numpy.load", "all_scores.append", "numpy.load"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_adaptive_noising.adapt_noise"], ["", "def", "run_adapt_noise_experiment", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "transform", ",", "\n", "num_roots", ",", "obj_function", ",", "learning_rate", ",", "learning_decay", ",", "\n", "max_stop_count", ",", "max_iteration", ",", "downscale", ",", "min_size", ",", "lp_norm", ",", "\n", "kernel_size", ",", "num_regions", ",", "draw_mode", ",", "num_perturbs", ",", "\n", "percent", "=", "False", ",", "overwrite", "=", "False", ")", ":", "\n", "# Read all top10idxs and all scores for that model (only for aupc objective function)", "\n", "    ", "input_dir", "=", "os", ".", "path", ".", "join", "(", "'output/'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "input_dir", ")", ":", "\n", "        ", "print", "(", "'Model classification output not found for:'", ",", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_scores.npy'", ")", "\n", "all_model_scores", "=", "np", ".", "load", "(", "input_path", ")", "\n", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "all_top10_idxs", "=", "np", ".", "load", "(", "input_path", ")", "\n", "\n", "# Prepare heatmap output directory", "\n", "output_dir", "=", "os", ".", "path", ".", "join", "(", "'adaptive/'", ",", "model_name", ",", "obj_function", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "", "heatmap_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/'", ",", "'smooth-taylor'", ",", "model_name", ",", "'adaptive'", ",", "obj_function", ",", "str", "(", "num_roots", ")", "+", "'N'", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "heatmap_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "heatmap_dir", ")", "\n", "\n", "# Prepare output file formats", "\n", "", "all_scores", "=", "[", "]", "\n", "if", "obj_function", "==", "'aupc'", ":", "\n", "        ", "params_format", "=", "(", "'{num_roots}N_{learning_rate}lr_{learning_decay}ld_'", "\n", "'{max_stop_count}s_{kernel_size}k_{num_perturbs}p_'", "\n", "'{num_regions}r_{draw_mode}d'", ")", "\n", "single_params_format", "=", "params_format", "+", "'_{img_filename}'", "\n", "score_file_format", "=", "'perturbations_scores_'", "+", "single_params_format", "+", "'.npy'", "\n", "all_scores_file_format", "=", "'all_perturb_scores_'", "+", "params_format", "+", "'.npy'", "\n", "exp_params", "=", "{", "\n", "'num_roots'", ":", "num_roots", ",", "\n", "'learning_rate'", ":", "learning_rate", ",", "\n", "'learning_decay'", ":", "learning_decay", ",", "\n", "'max_stop_count'", ":", "max_stop_count", ",", "\n", "'kernel_size'", ":", "kernel_size", ",", "\n", "'num_perturbs'", ":", "num_perturbs", ",", "\n", "'num_regions'", ":", "num_regions", ",", "\n", "'draw_mode'", ":", "draw_mode", "\n", "}", "\n", "", "elif", "obj_function", "==", "'autvc'", ":", "\n", "        ", "params_format", "=", "(", "'{num_roots}N_{learning_rate}lr_{learning_decay}ld_'", "\n", "'{max_stop_count}s_{downscale}ds_{min_size}ms_{lp_norm}l'", ")", "\n", "single_params_format", "=", "params_format", "+", "'_{img_filename}'", "\n", "score_file_format", "=", "'atv_scores_'", "+", "single_params_format", "+", "'.npy'", "\n", "all_scores_file_format", "=", "'all_atv_scores_'", "+", "params_format", "+", "'.npy'", "\n", "exp_params", "=", "{", "\n", "'num_roots'", ":", "num_roots", ",", "\n", "'learning_rate'", ":", "learning_rate", ",", "\n", "'learning_decay'", ":", "learning_decay", ",", "\n", "'max_stop_count'", ":", "max_stop_count", ",", "\n", "'downscale'", ":", "downscale", ",", "\n", "'min_size'", ":", "min_size", ",", "\n", "'lp_norm'", ":", "lp_norm", "\n", "}", "\n", "", "noise_file_format", "=", "'noise_scores_'", "+", "single_params_format", "+", "'.npy'", "\n", "all_noise_file_format", "=", "'all_noise_'", "+", "params_format", "+", "'.npy'", "\n", "\n", "# Prepare overall output file paths", "\n", "all_noise_scores_filename", "=", "all_noise_file_format", ".", "format", "(", "**", "exp_params", ")", "\n", "all_noise_scores_filepath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "all_noise_scores_filename", ")", "\n", "all_scores_filename", "=", "all_scores_file_format", ".", "format", "(", "**", "exp_params", ")", "\n", "all_scores_filepath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "all_scores_filename", ")", "\n", "\n", "# Initialize noise scores", "\n", "dataset_size", "=", "len", "(", "dataset", ")", "\n", "all_noise_scores", "=", "np", ".", "zeros", "(", "(", "dataset_size", ",", "2", ")", ")", "# noise, AUC score", "\n", "\n", "# Go through each image", "\n", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Check if adaptive noise output already exists", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "exp_params", "[", "'img_filename'", "]", "=", "img_filename", "\n", "\n", "# Prepare noise score outpath", "\n", "noise_score_filename", "=", "noise_file_format", ".", "format", "(", "**", "exp_params", ")", "\n", "noise_score_outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "noise_score_filename", ")", "\n", "\n", "# Prepare score output", "\n", "scores_filename", "=", "score_file_format", ".", "format", "(", "**", "exp_params", ")", "\n", "scores_outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "scores_filename", ")", "\n", "\n", "# Prepare best heatmap output", "\n", "best_heatmap_outpath", "=", "os", ".", "path", ".", "join", "(", "heatmap_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "noise_score_outpath", ")", "and", "os", ".", "path", ".", "exists", "(", "scores_outpath", ")", "and", "os", ".", "path", ".", "exists", "(", "best_heatmap_outpath", ")", ":", "# ignore if already generated", "\n", "            ", "print", "(", "img_filename", ",", "'already has noise scores generated'", ")", "\n", "all_noise_scores", "[", "img_idx", "]", "=", "np", ".", "load", "(", "noise_score_outpath", ")", "\n", "all_scores", ".", "append", "(", "np", ".", "load", "(", "scores_outpath", ")", ")", "\n", "continue", "\n", "\n", "# Initialize parameters", "\n", "", "img_input", "=", "dataset", "[", "img_idx", "]", "[", "'image'", "]", "\n", "predicted_class", "=", "all_top10_idxs", "[", "img_idx", ",", "0", "]", "\n", "if", "obj_function", "==", "'aupc'", ":", "\n", "# Retrieve the image data, predicted class and score", "\n", "            ", "input_score", "=", "all_model_scores", "[", "img_idx", ",", "predicted_class", "]", "\n", "params", "=", "{", "\n", "'img_input'", ":", "img_input", ",", "\n", "'model'", ":", "model", ",", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'transform'", ":", "transform", ",", "\n", "'analyzer'", ":", "'smooth-taylor'", ",", "\n", "'explained_class'", ":", "predicted_class", ",", "\n", "'input_score'", ":", "input_score", ",", "\n", "'num_roots'", ":", "num_roots", ",", "\n", "'kernel_size'", ":", "kernel_size", ",", "\n", "'draw_mode'", ":", "draw_mode", ",", "\n", "'num_regions'", ":", "num_regions", ",", "\n", "'num_perturbs'", ":", "num_perturbs", ",", "\n", "}", "\n", "", "elif", "obj_function", "==", "'autvc'", ":", "\n", "            ", "params", "=", "{", "\n", "'img_input'", ":", "img_input", ",", "\n", "'model'", ":", "model", ",", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'transform'", ":", "transform", ",", "\n", "'analyzer'", ":", "'smooth-taylor'", ",", "\n", "'explained_class'", ":", "predicted_class", ",", "\n", "'num_roots'", ":", "num_roots", ",", "\n", "'downscale'", ":", "downscale", ",", "\n", "'min_size'", ":", "min_size", ",", "\n", "'lp_norm'", ":", "lp_norm", ",", "\n", "}", "\n", "\n", "# Find best noise scale", "\n", "", "best_noise", ",", "best_heatmap", ",", "best_auc", ",", "best_scores", "=", "adapt_noise", "(", "\n", "img_input", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "num_roots", "=", "num_roots", ",", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "learning_decay", "=", "learning_decay", ",", "\n", "max_stop_count", "=", "max_stop_count", ",", "\n", "max_iteration", "=", "max_iteration", ",", "\n", "obj_function", "=", "obj_function", ",", "\n", "params", "=", "params", "\n", ")", "\n", "\n", "# Save the best heatmap", "\n", "np", ".", "save", "(", "best_heatmap_outpath", ",", "best_heatmap", ")", "\n", "\n", "# Save the best noise and auc scores", "\n", "all_noise_scores", "[", "img_idx", ",", "0", "]", "=", "best_noise", "\n", "all_noise_scores", "[", "img_idx", ",", "1", "]", "=", "best_auc", "\n", "all_scores", ".", "append", "(", "best_scores", ")", "\n", "scores_outpath", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "scores_filename", ")", "\n", "np", ".", "save", "(", "scores_outpath", ",", "best_scores", ")", "\n", "np", ".", "save", "(", "noise_score_outpath", ",", "all_noise_scores", "[", "img_idx", "]", ")", "\n", "\n", "# Save all scores", "\n", "", "np", ".", "save", "(", "all_scores_filepath", ",", "np", ".", "array", "(", "all_scores", ")", ")", "\n", "np", ".", "save", "(", "all_noise_scores_filepath", ",", "all_noise_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args": [[13, 28], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Generate smooth taylor attributions'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model_name'", ",", "type", "=", "str", ",", "help", "=", "'name of the model used to classify'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--batch_size'", ",", "type", "=", "int", ",", "help", "=", "'batch size to use during each epoch'", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--noise_scale'", ",", "type", "=", "float", ",", "help", "=", "'magnitude of the noise scale to noise the image'", ",", "default", "=", "5e-1", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--num_roots'", ",", "type", "=", "int", ",", "help", "=", "'number of noised images to use'", ",", "default", "=", "150", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--num_image'", ",", "type", "=", "int", ",", "help", "=", "'number of image data to use from the first'", ",", "default", "=", "1000", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--noise_percent'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use proportion of image as noise scale'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--overwrite'", ",", "action", "=", "'store_true'", ",", "help", "=", "'overwrite the output'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "model_name", "not", "in", "MODELS", ":", "\n", "        ", "print", "(", "'Invalid model name:'", ",", "args", ".", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.None.experiment_smooth_taylor.run_smooth_taylor_experiment": [[30, 75], ["os.path.join", "os.path.join", "numpy.load", "os.path.join", "enumerate", "os.path.exists", "print", "exit", "str", "os.path.isdir", "os.makedirs", "tqdm.tqdm", "os.path.basename", "os.path.join", "attribution.smooth_taylor", "numpy.sum", "numpy.save", "str", "os.path.exists", "print"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.smooth_taylor"], ["", "def", "run_smooth_taylor_experiment", "(", "dataset", ",", "model", ",", "model_name", ",", "batch_size", ",", "noise_scale", ",", "num_roots", ",", "overwrite", "=", "False", ",", "noise_percent", "=", "True", ")", ":", "\n", "# Read all top10idxs for that model", "\n", "    ", "input_dir", "=", "os", ".", "path", ".", "join", "(", "'output/'", ",", "model_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "input_dir", ")", ":", "\n", "        ", "print", "(", "'Model classification output not found for:'", ",", "model_name", ")", "\n", "exit", "(", ")", "\n", "", "input_path", "=", "os", ".", "path", ".", "join", "(", "input_dir", ",", "'all_top10_idxs.npy'", ")", "\n", "all_top10_idxs", "=", "np", ".", "load", "(", "input_path", ")", "\n", "\n", "# Check output directory", "\n", "if", "noise_percent", ":", "\n", "        ", "noise_folder", "=", "str", "(", "noise_scale", ")", "+", "'%'", "\n", "noise_scale", "=", "noise_scale", "/", "100.", "\n", "", "else", ":", "\n", "        ", "noise_folder", "=", "'{:.1e}'", ".", "format", "(", "noise_scale", ")", "# convert to scientific notation", "\n", "", "roots_folder", "=", "str", "(", "num_roots", ")", "+", "'N'", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "'heatmaps/smooth-taylor/'", ",", "model_name", ",", "noise_folder", ",", "roots_folder", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "out_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out_dir", ")", "\n", "\n", "# Generate SmoothTaylor heatmap for each image", "\n", "", "for", "img_idx", ",", "img_filepath", "in", "enumerate", "(", "tqdm", "(", "dataset", ".", "img_filepaths", ",", "desc", "=", "'Image'", ")", ")", ":", "\n", "# Initialize output filepath", "\n", "        ", "img_filename", "=", "os", ".", "path", ".", "basename", "(", "img_filepath", ")", "\n", "outpath", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "img_filename", "+", "'_hm.npy'", ")", "\n", "if", "not", "overwrite", "and", "os", ".", "path", ".", "exists", "(", "outpath", ")", ":", "# ignore if already generated", "\n", "            ", "print", "(", "img_filename", ",", "'already has heatmap generated'", ")", "\n", "continue", "\n", "\n", "# Retrieve the image data and predicted class", "\n", "", "predicted_class", "=", "all_top10_idxs", "[", "img_idx", ",", "0", "]", "\n", "img_input", "=", "dataset", "[", "img_idx", "]", "[", "'image'", "]", "\n", "\n", "# Compute SmoothTaylor heatmaps", "\n", "attributions", "=", "smooth_taylor", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "noise_scale", "=", "noise_scale", ",", "\n", "num_roots", "=", "num_roots", ",", "\n", "explained_class", "=", "predicted_class", ",", "\n", "percent", "=", "noise_percent", "\n", ")", "\n", "heatmap", "=", "np", ".", "sum", "(", "attributions", ",", "axis", "=", "0", ")", "# sum across all channels", "\n", "np", ".", "save", "(", "outpath", ",", "heatmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.heatmap_normalize": [[12, 18], ["numpy.abs", "numpy.min", "numpy.percentile", "numpy.clip"], "function", ["None"], ["def", "heatmap_normalize", "(", "heatmap", ",", "percentile", "=", "99", ")", ":", "\n", "    ", "heatmap", "=", "np", ".", "abs", "(", "heatmap", ")", "\n", "vmin", "=", "np", ".", "min", "(", "heatmap", ")", "\n", "vmax", "=", "np", ".", "percentile", "(", "heatmap", ",", "percentile", ")", "\n", "heatmap", "=", "(", "heatmap", "-", "vmin", ")", "/", "(", "vmax", "-", "vmin", ")", "\n", "return", "np", ".", "clip", "(", "heatmap", ",", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.average_total_variation": [[20, 36], ["eval.heatmap_normalize", "numpy.abs", "numpy.abs", "numpy.power", "numpy.power", "numpy.power", "numpy.power", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.heatmap_normalize"], ["", "def", "average_total_variation", "(", "img", ",", "norm", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        img (array): image array with the shape (H, W)\n    \"\"\"", "\n", "img", "=", "heatmap_normalize", "(", "img", ")", "\n", "total_pixels", "=", "img", ".", "shape", "[", "0", "]", "*", "img", ".", "shape", "[", "1", "]", "\n", "x_diff", "=", "np", ".", "abs", "(", "img", "[", ":", ",", "1", ":", "]", "-", "img", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "y_diff", "=", "np", ".", "abs", "(", "img", "[", "1", ":", ",", ":", "]", "-", "img", "[", ":", "-", "1", ",", ":", "]", ")", "\n", "if", "norm", ">", "1", ":", "\n", "        ", "x_diff", "=", "np", ".", "power", "(", "x_diff", ",", "norm", ")", "\n", "y_diff", "=", "np", ".", "power", "(", "y_diff", ",", "norm", ")", "\n", "total", "=", "np", ".", "power", "(", "np", ".", "sum", "(", "x_diff", ")", ",", "1.", "/", "norm", ")", "+", "np", ".", "power", "(", "np", ".", "sum", "(", "y_diff", ")", ",", "1.", "/", "norm", ")", "\n", "", "else", ":", "\n", "        ", "total", "=", "np", ".", "sum", "(", "x_diff", ")", "+", "np", ".", "sum", "(", "y_diff", ")", "\n", "", "return", "total", "/", "total_pixels", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_autvc": [[38, 102], ["numpy.sum", "eval.compute_multiscaled_atv", "scipy.integrate.simps", "analyzer.smooth_taylor", "print", "exit", "torch.zeros_like", "transform", "analyzer.integrated_gradients", "range", "numpy.mean", "torch.rand_like", "transform", "analyzer.integrated_gradients", "all_igs.append", "numpy.array"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_multiscaled_atv", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.smooth_taylor", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients"], ["", "def", "compute_autvc", "(", "img_input", ",", "model", ",", "batch_size", ",", "transform", ",", "\n", "analyzer", ",", "explained_class", ",", "\n", "downscale", "=", "1.5", ",", "min_size", "=", "(", "30", ",", "30", ")", ",", "lp_norm", "=", "1", ",", "\n", "baseline_type", "=", "None", ",", "num_noise", "=", "None", ",", "steps", "=", "None", ",", "\n", "noise_scale", "=", "None", ",", "num_roots", "=", "None", ")", ":", "\n", "# Compute attribution map", "\n", "    ", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "assert", "(", "noise_scale", ")", "\n", "assert", "(", "num_roots", ")", "\n", "attributions", "=", "smooth_taylor", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "noise_scale", "=", "noise_scale", ",", "\n", "num_roots", "=", "num_roots", ",", "\n", "explained_class", "=", "explained_class", "\n", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "assert", "(", "steps", ")", "\n", "assert", "(", "baseline_type", ")", "\n", "if", "baseline_type", "==", "'zero'", ":", "\n", "            ", "baseline", "=", "torch", ".", "zeros_like", "(", "img_input", ")", "\n", "baseline", "=", "transform", "(", "baseline", ")", "\n", "attributions", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "explained_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "", "if", "baseline_type", "==", "'noise'", ":", "\n", "            ", "assert", "(", "num_noise", ")", "\n", "all_igs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_noise", ")", ":", "\n", "                ", "baseline", "=", "torch", ".", "rand_like", "(", "img_input", ")", "# uniform between 0 and 1", "\n", "baseline", "=", "transform", "(", "baseline", ")", "\n", "ig", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "explained_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "all_igs", ".", "append", "(", "ig", ")", "\n", "", "attributions", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "all_igs", ")", ",", "axis", "=", "0", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'Invalid analyzer:'", ",", "analyzer", ")", "\n", "exit", "(", ")", "\n", "\n", "# Create heatmap by summing across channels", "\n", "", "heatmap", "=", "np", ".", "sum", "(", "attributions", ",", "axis", "=", "0", ")", "\n", "\n", "# Compute multi-scaled averaged TVs", "\n", "multi_scaled_ATV", "=", "compute_multiscaled_atv", "(", "\n", "heatmap", "=", "heatmap", ",", "\n", "downscale", "=", "downscale", ",", "\n", "min_size", "=", "min_size", ",", "\n", "lp_norm", "=", "lp_norm", "\n", ")", "\n", "\n", "autvc", "=", "simps", "(", "multi_scaled_ATV", ",", "dx", "=", "1", ")", "\n", "return", "heatmap", ",", "multi_scaled_ATV", ",", "autvc", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_aupc": [[104, 176], ["numpy.sum", "eval.compute_perturbations", "scipy.integrate.simps", "analyzer.smooth_taylor", "print", "exit", "math.fabs", "torch.zeros_like", "transform", "analyzer.integrated_gradients", "range", "numpy.mean", "torch.rand_like", "transform", "analyzer.integrated_gradients", "all_igs.append", "numpy.array"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_perturbations", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.smooth_taylor", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients"], ["", "def", "compute_aupc", "(", "img_input", ",", "model", ",", "batch_size", ",", "transform", ",", "\n", "analyzer", ",", "explained_class", ",", "input_score", ",", "\n", "kernel_size", ",", "draw_mode", ",", "num_regions", ",", "num_perturbs", ",", "\n", "baseline_type", "=", "None", ",", "num_noise", "=", "None", ",", "steps", "=", "None", ",", "\n", "noise_scale", "=", "None", ",", "num_roots", "=", "None", ")", ":", "\n", "# Compute attribution map", "\n", "    ", "if", "analyzer", "==", "'smooth-taylor'", ":", "\n", "        ", "assert", "(", "noise_scale", ")", "\n", "assert", "(", "num_roots", ")", "\n", "attributions", "=", "smooth_taylor", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "noise_scale", "=", "noise_scale", ",", "\n", "num_roots", "=", "num_roots", ",", "\n", "explained_class", "=", "explained_class", "\n", ")", "\n", "", "elif", "analyzer", "==", "'ig'", ":", "\n", "        ", "assert", "(", "steps", ")", "\n", "assert", "(", "baseline_type", ")", "\n", "if", "baseline_type", "==", "'zero'", ":", "\n", "            ", "baseline", "=", "torch", ".", "zeros_like", "(", "img_input", ")", "\n", "baseline", "=", "transform", "(", "baseline", ")", "\n", "attributions", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "explained_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "", "if", "baseline_type", "==", "'noise'", ":", "\n", "            ", "assert", "(", "num_noise", ")", "\n", "all_igs", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_noise", ")", ":", "\n", "                ", "baseline", "=", "torch", ".", "rand_like", "(", "img_input", ")", "# uniform between 0 and 1", "\n", "baseline", "=", "transform", "(", "baseline", ")", "\n", "ig", "=", "integrated_gradients", "(", "\n", "inputs", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "baseline", "=", "baseline", ",", "\n", "explained_class", "=", "explained_class", ",", "\n", "steps", "=", "steps", "\n", ")", "\n", "all_igs", ".", "append", "(", "ig", ")", "\n", "", "attributions", "=", "np", ".", "mean", "(", "np", ".", "array", "(", "all_igs", ")", ",", "axis", "=", "0", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "'Invalid analyzer:'", ",", "analyzer", ")", "\n", "exit", "(", ")", "\n", "\n", "# Create heatmap by summing across channels", "\n", "", "heatmap", "=", "np", ".", "sum", "(", "attributions", ",", "axis", "=", "0", ")", "\n", "\n", "# Perform perturbation", "\n", "perturb_scores", "=", "compute_perturbations", "(", "\n", "img_input", "=", "img_input", ",", "\n", "model", "=", "model", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "explained_class", "=", "explained_class", ",", "\n", "heatmap", "=", "heatmap", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "draw_mode", "=", "draw_mode", ",", "\n", "num_regions", "=", "num_regions", ",", "\n", "num_perturbs", "=", "num_perturbs", "\n", ")", "\n", "\n", "# Prepare the perturb scores", "\n", "perturb_scores", "=", "[", "input_score", "]", "+", "perturb_scores", "# add original score", "\n", "perturb_scores", "=", "[", "x", "/", "math", ".", "fabs", "(", "input_score", ")", "for", "x", "in", "perturb_scores", "]", "# normalize into between 0 and 1", "\n", "aupc", "=", "simps", "(", "perturb_scores", ",", "dx", "=", "1", ")", "\n", "return", "heatmap", ",", "perturb_scores", ",", "aupc", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_multiscaled_atv": [[178, 194], ["enumerate", "numpy.array", "skimage.transform.pyramid_gaussian", "np.array.append", "eval.average_total_variation"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.average_total_variation"], ["", "def", "compute_multiscaled_atv", "(", "heatmap", ",", "downscale", ",", "min_size", ",", "lp_norm", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        heatmap (array): heatmap of the attribution (summed across channels)\n        downscale (float): proportion to reduce dimension at each step\n        min_size (tuple): minimum dimension to stop pyramid gaussian\n        lp_norm (int): lp normalization to compute total variation\n    \"\"\"", "\n", "multiscale_atvs", "=", "[", "]", "\n", "for", "(", "i", ",", "resized", ")", "in", "enumerate", "(", "pyramid_gaussian", "(", "heatmap", ",", "downscale", "=", "downscale", ")", ")", ":", "\n", "# if the image is too small, break from the loop", "\n", "        ", "if", "resized", ".", "shape", "[", "0", "]", "<", "min_size", "[", "0", "]", "or", "resized", ".", "shape", "[", "1", "]", "<", "min_size", "[", "1", "]", ":", "\n", "            ", "break", "\n", "", "multiscale_atvs", ".", "append", "(", "average_total_variation", "(", "resized", ",", "norm", "=", "lp_norm", ")", ")", "\n", "", "multiscale_atvs", "=", "np", ".", "array", "(", "multiscale_atvs", ")", "\n", "return", "multiscale_atvs", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.compute_perturbations": [[196, 300], ["range", "numpy.argsort", "INVERSE_TRANSFORM", "set", "range", "numpy.ones", "range", "numpy.zeros", "range", "enumerate", "torch.stack", "range", "torch.utils.data.dataset.TensorDataset", "torch.utils.data.DataLoader", "eval.classify_perturbations", "torch.Tensor", "perturb_scores.append", "numpy.mean", "numpy.mean", "numpy.std", "numpy.isnan", "torch.Tensor", "NORMALIZE_TRANSFORM", "perturbs.append", "numpy.abs", "int", "int", "range", "torch.zeros_like", "numpy.random.uniform", "math.floor", "range", "range", "numpy.zeros", "range", "numpy.maximum", "numpy.minimum", "print", "exit", "set.add", "numpy.random.normal", "numpy.zeros_like", "numpy.ones_like"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.classify_perturbations"], ["", "def", "compute_perturbations", "(", "img_input", ",", "model", ",", "batch_size", ",", "explained_class", ",", "heatmap", ",", "\n", "kernel_size", ",", "draw_mode", ",", "num_regions", ",", "num_perturbs", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        img_input (array): original image\n        model (PyTorch model): pretrained model\n        batch_size (int): batch size to run per epoch\n        explained_class (int): index of the class to be explained\n        heatmap (array): attribution heatmap for the image\n        kernel_size (int): size of the window of each perturbation\n        draw_mode (int): perturb draw mode: 0 - uniform; 1 - gaussian according to image stats\n        num_regions (int): number of regions to perturbate\n        num_perturbs (int): number of random perturbations to evaluate\n    \"\"\"", "\n", "# Find average pooling with kernel", "\n", "img_h", ",", "img_w", "=", "heatmap", ".", "shape", "\n", "avg_values", "=", "-", "np", ".", "inf", "*", "np", ".", "ones", "(", "img_h", "*", "img_w", ")", "\n", "\n", "for", "h", "in", "range", "(", "img_h", "-", "kernel_size", ")", ":", "\n", "        ", "for", "w", "in", "range", "(", "img_w", "-", "kernel_size", ")", ":", "\n", "            ", "avg_values", "[", "h", "+", "w", "*", "img_h", "]", "=", "np", ".", "mean", "(", "np", ".", "abs", "(", "heatmap", "[", "h", ":", "h", "+", "kernel_size", ",", "w", ":", "w", "+", "kernel_size", "]", ")", ")", "\n", "", "", "most_relevant_idxs", "=", "np", ".", "argsort", "(", "-", "avg_values", ")", "# most relevant first", "\n", "\n", "# Load original image", "\n", "img", "=", "INVERSE_TRANSFORM", "(", "img_input", ")", "# TODO: maybe take transform as input?", "\n", "if", "draw_mode", "==", "1", ":", "# from gaussian according to img stats", "\n", "# compute mean and std dev. from each channel", "\n", "        ", "channel_stats", "=", "np", ".", "zeros", "(", "(", "2", ",", "3", ")", ")", "\n", "for", "c", "in", "range", "(", "3", ")", ":", "\n", "            ", "channel_stats", "[", "0", ",", "c", "]", "=", "np", ".", "mean", "(", "img", "[", "c", ",", ":", ",", ":", "]", ")", "\n", "channel_stats", "[", "1", ",", "c", "]", "=", "np", ".", "std", "(", "img", "[", "c", ",", ":", ",", ":", "]", ")", "\n", "if", "np", ".", "isnan", "(", "channel_stats", "[", "1", ",", "c", "]", ")", ":", "\n", "                ", "channel_stats", "[", "1", ",", "c", "]", "=", "1e-3", "\n", "\n", "# Initialize perturb scores", "\n", "", "", "", "perturb_scores", "=", "[", "]", "\n", "\n", "# Select the most relevant point to perform the perturbation", "\n", "bad_idxs", "=", "set", "(", ")", "\n", "for", "region_idx", "in", "range", "(", "num_regions", ")", ":", "\n", "        ", "found", "=", "False", "\n", "for", "i", ",", "kernel_idx", "in", "enumerate", "(", "most_relevant_idxs", ")", ":", "\n", "            ", "if", "kernel_idx", "not", "in", "bad_idxs", ":", "\n", "# get coordinates of point", "\n", "                ", "width", "=", "int", "(", "math", ".", "floor", "(", "kernel_idx", "/", "img_h", ")", ")", "\n", "height", "=", "int", "(", "kernel_idx", "-", "width", "*", "img_h", ")", "\n", "\n", "# ignore if the point is beyond the boundaries of kernel", "\n", "if", "(", "img_h", "-", "height", ")", "<=", "kernel_size", "or", "(", "img_w", "-", "width", ")", "<=", "kernel_size", ":", "\n", "                    ", "continue", "\n", "\n", "# mark overlapping neighboring points as not to use in bad_idxs", "\n", "", "for", "h", "in", "range", "(", "-", "kernel_size", "+", "1", ",", "kernel_size", ")", ":", "\n", "                    ", "for", "w", "in", "range", "(", "-", "kernel_size", "+", "1", ",", "kernel_size", ")", ":", "\n", "                        ", "bad_idxs", ".", "add", "(", "(", "height", "+", "h", ")", "+", "(", "width", "+", "w", ")", "*", "img_h", ")", "\n", "\n", "", "", "found", "=", "True", "\n", "break", "\n", "", "", "if", "not", "found", ":", "# no more useful points", "\n", "            ", "break", "\n", "\n", "# Prepare perturbation copies", "\n", "", "perturbs", "=", "[", "]", "\n", "perturbs_imgs", "=", "torch", ".", "stack", "(", "[", "torch", ".", "zeros_like", "(", "img_input", ")", "for", "_", "in", "range", "(", "num_perturbs", ")", "]", ")", "\n", "\n", "# Compute the perturbation", "\n", "for", "i", "in", "range", "(", "num_perturbs", ")", ":", "\n", "            ", "if", "draw_mode", "==", "0", ":", "# uniform", "\n", "# draw randomly from uniform distribution, space is 0,255", "\n", "                ", "perturb", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "0", ",", "high", "=", "255", ",", "size", "=", "(", "3", ",", "kernel_size", ",", "kernel_size", ")", ")", "\n", "", "elif", "draw_mode", "==", "1", ":", "# from gaussian according to img stats", "\n", "                ", "perturb", "=", "np", ".", "zeros", "(", "(", "3", ",", "kernel_size", ",", "kernel_size", ")", ")", "\n", "for", "c", "in", "range", "(", "3", ")", ":", "\n", "                    ", "perturb", "[", "c", "]", "=", "np", ".", "random", ".", "normal", "(", "\n", "loc", "=", "channel_stats", "[", "0", ",", "c", "]", ",", "# mean", "\n", "scale", "=", "channel_stats", "[", "1", ",", "c", "]", ",", "# std dev", "\n", "size", "=", "(", "kernel_size", ",", "kernel_size", ")", "\n", ")", "\n", "# Ensure perturb does not exceed bounds", "\n", "", "perturb", "=", "np", ".", "maximum", "(", "perturb", ",", "np", ".", "zeros_like", "(", "perturb", ")", ")", "# element-wise max(p, 0)", "\n", "perturb", "=", "np", ".", "minimum", "(", "perturb", ",", "255", "*", "np", ".", "ones_like", "(", "perturb", ")", ")", "# element-wise min(p, 255)", "\n", "", "else", ":", "\n", "                ", "print", "(", "'Invalid perturb draw mode'", ")", "\n", "exit", "(", ")", "\n", "\n", "# Apply the perturbation to the current point", "\n", "", "perturbs_imgs", "[", "i", "]", "=", "img", "\n", "perturb", "=", "perturb", "/", "255.", "# normalize to 0 to 1", "\n", "perturbs_imgs", "[", "i", ",", ":", ",", "height", ":", "height", "+", "kernel_size", ",", "width", ":", "width", "+", "kernel_size", "]", "=", "torch", ".", "Tensor", "(", "perturb", ")", "\n", "perturbs_imgs", "[", "i", "]", "=", "NORMALIZE_TRANSFORM", "(", "perturbs_imgs", "[", "i", "]", ")", "# TODO: take transform as input?", "\n", "perturbs", ".", "append", "(", "perturb", ")", "\n", "\n", "# Perform the classification for the perturbations", "\n", "", "perturb_dataset", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "TensorDataset", "(", "perturbs_imgs", ")", "\n", "data_loader", "=", "DataLoader", "(", "perturb_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "actual_perturb_idx", ",", "mean_score", "=", "classify_perturbations", "(", "data_loader", ",", "model", ",", "explained_class", ")", "\n", "\n", "# Apply the actual perturbation on image", "\n", "actual_perturb", "=", "perturbs", "[", "actual_perturb_idx", "]", "\n", "img", "[", ":", ",", "height", ":", "height", "+", "kernel_size", ",", "width", ":", "width", "+", "kernel_size", "]", "=", "torch", ".", "Tensor", "(", "actual_perturb", ")", "\n", "\n", "# Save the mean scores of the perturbations", "\n", "perturb_scores", ".", "append", "(", "mean_score", ")", "\n", "", "return", "perturb_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.eval.classify_perturbations": [[302, 332], ["numpy.array", "numpy.concatenate", "numpy.mean", "numpy.median", "range", "inputs.to.to", "len", "math.fabs", "torch.no_grad", "model", "out[].cpu().numpy", "np.concatenate.append", "out[].cpu"], "function", ["None"], ["", "def", "classify_perturbations", "(", "data_loader", ",", "model", ",", "explained_class", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        data_loader (DataLoader): object that loads perturbated images\n        model (model): pre-trained PyTorch model\n        explained_class (int): index of the class to be explained\n    \"\"\"", "\n", "# Find the scores for all perturbation candidates", "\n", "all_scores", "=", "[", "]", "\n", "for", "sample_batch", "in", "data_loader", ":", "\n", "        ", "inputs", "=", "sample_batch", "[", "0", "]", "\n", "inputs", "=", "inputs", ".", "to", "(", "DEVICE", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "out", "=", "model", "(", "inputs", ")", "\n", "scores", "=", "out", "[", ":", ",", "explained_class", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "all_scores", ".", "append", "(", "scores", ")", "\n", "", "", "all_scores", "=", "np", ".", "array", "(", "all_scores", ")", "\n", "all_scores", "=", "np", ".", "concatenate", "(", "all_scores", ")", "\n", "\n", "# Find the index of the perturbation closest to the median", "\n", "mean_score", "=", "np", ".", "mean", "(", "all_scores", ")", "\n", "median", "=", "np", ".", "median", "(", "all_scores", ")", "\n", "best_idx", "=", "0", "\n", "best_val", "=", "np", ".", "inf", "\n", "for", "i", "in", "range", "(", "len", "(", "all_scores", ")", ")", ":", "# for every perturbation candidate", "\n", "        ", "value", "=", "math", ".", "fabs", "(", "all_scores", "[", "i", "]", "-", "median", ")", "# absolute distance to median", "\n", "if", "value", "<", "best_val", ":", "\n", "            ", "best_val", "=", "value", "\n", "best_idx", "=", "i", "\n", "", "", "return", "best_idx", ",", "mean_score", "\n", "", ""]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.__init__": [[10, 40], ["dataset.ImageNetValDataset.parse_synset", "os.walk", "enumerate", "sorted", "os.path.join", "dataset.ImageNetValDataset.img_filepaths.append", "dataset.ImageNetValDataset.labels.append", "print", "dataset.ImageNetValDataset.parse_label"], "methods", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.parse_synset", "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.parse_label"], ["    ", "def", "__init__", "(", "self", ",", "root_dir", ",", "label_dir", ",", "synset_filepath", ",", "max_num", "=", "None", ",", "transform", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            root_dir (string): Directory with all the raw images\n            label_dir (string): Directory with the ground truth label (in XML files)\n            synset_filepath (string): Filepath of synset file\n            max_num (int, optional): Optional maximum number of image to load\n            transform (callable, optional): Optional transform to be applied on a sample\n        \"\"\"", "\n", "self", ".", "root_dir", "=", "root_dir", "\n", "self", ".", "label_dir", "=", "label_dir", "\n", "self", ".", "synset_filepath", "=", "synset_filepath", "\n", "if", "transform", ":", "\n", "            ", "self", ".", "transform", "=", "transform", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform", "=", "DEFAULT_TRANSFORM", "\n", "\n", "", "self", ".", "parse_synset", "(", ")", "\n", "self", ".", "img_filepaths", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "\n", "# Prepare the images and labels", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "self", ".", "root_dir", ")", ":", "\n", "            ", "for", "i", ",", "filename", "in", "enumerate", "(", "sorted", "(", "files", ")", ")", ":", "\n", "                ", "if", "max_num", "and", "i", ">=", "(", "max_num", ")", ":", "\n", "                    ", "print", "(", "'Read data read limit reached:'", ",", "max_num", ")", "\n", "break", "\n", "", "img_filepath", "=", "os", ".", "path", ".", "join", "(", "root", ",", "filename", ")", "\n", "self", ".", "img_filepaths", ".", "append", "(", "img_filepath", ")", "\n", "self", ".", "labels", ".", "append", "(", "self", ".", "parse_label", "(", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.parse_synset": [[41, 52], ["open", "f.readlines", "enumerate", "enumerate", "line.split", "line.split"], "methods", ["None"], ["", "", "", "def", "parse_synset", "(", "self", ")", ":", "\n", "        ", "\"\"\" Prepare the following mappings:\n            idx2synset:  dict mapping of idx to synsets\n            synset2idx:  dict mapping of synset to idx\n            synset2class:  dict mapping of synset to class\n        \"\"\"", "\n", "with", "open", "(", "self", ".", "synset_filepath", ")", "as", "f", ":", "\n", "            ", "synsets", "=", "[", "(", "line", ".", "split", "(", ")", "[", "0", "]", ",", "' '", ".", "join", "(", "line", ".", "split", "(", ")", "[", "1", ":", "]", ")", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "]", "\n", "self", ".", "idx2synset", "=", "{", "i", ":", "x", "[", "0", "]", "for", "i", ",", "x", "in", "enumerate", "(", "synsets", ")", "}", "\n", "self", ".", "synset2idx", "=", "{", "x", "[", "0", "]", ":", "i", "for", "i", ",", "x", "in", "enumerate", "(", "synsets", ")", "}", "\n", "self", ".", "synset2class", "=", "{", "x", "[", "0", "]", ":", "x", "[", "1", "]", "for", "x", "in", "synsets", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.parse_label": [[53, 69], ["os.path.splitext", "xml.parse", "xml.parse.getroot", "set", "ET.parse.getroot.findall", "os.path.join", "obj.findall", "len", "print", "exit", "set.add"], "methods", ["None"], ["", "", "def", "parse_label", "(", "self", ",", "img_filename", ")", ":", "\n", "        ", "\"\"\"Read the XML annotation file and retrieve the label index.\"\"\"", "\n", "img_identifier", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "img_filename", ")", "\n", "label_filepath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "label_dir", ",", "img_identifier", ")", "+", "'.xml'", "\n", "tree", "=", "ET", ".", "parse", "(", "label_filepath", ")", "\n", "root", "=", "tree", ".", "getroot", "(", ")", "\n", "\n", "label_set", "=", "set", "(", ")", "\n", "for", "obj", "in", "root", ".", "findall", "(", "'object'", ")", ":", "\n", "            ", "for", "name", "in", "obj", ".", "findall", "(", "'name'", ")", ":", "\n", "                ", "idx", "=", "self", ".", "synset2idx", "[", "name", ".", "text", "]", "\n", "label_set", ".", "add", "(", "idx", ")", "\n", "", "", "if", "len", "(", "label_set", ")", "!=", "1", ":", "\n", "            ", "print", "(", "'ERR: More than 1 label is found for '", ",", "img_filename", ")", "\n", "exit", "(", ")", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.__len__": [[70, 72], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "img_filepaths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.dataset.ImageNetValDataset.__getitem__": [[73, 89], ["PIL.Image.open().convert", "dataset.ImageNetValDataset.transform", "image.repeat.repeat.repeat", "PIL.Image.open", "image.repeat.repeat.size"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            sample: dict that contains image data, label and image filepath of the indexed image\n        \"\"\"", "\n", "img_filepath", "=", "self", ".", "img_filepaths", "[", "idx", "]", "\n", "image", "=", "PIL", ".", "Image", ".", "open", "(", "img_filepath", ")", ".", "convert", "(", "'RGB'", ")", "\n", "label", "=", "self", ".", "labels", "[", "idx", "]", "\n", "\n", "if", "self", ".", "transform", ":", "\n", "            ", "image", "=", "self", ".", "transform", "(", "image", ")", "\n", "", "if", "image", ".", "size", "(", ")", "[", "1", "]", "==", "1", ":", "# if only 1 channel after transformation, repeat channel 3 times", "\n", "            ", "image", "=", "image", ".", "repeat", "(", "[", "1", ",", "3", ",", "1", ",", "1", "]", ")", "\n", "\n", "", "sample", "=", "{", "'image'", ":", "image", ",", "'label'", ":", "label", ",", "'filepath'", ":", "img_filepath", "}", "\n", "return", "sample", "\n", "", "", ""]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.integrated_gradients": [[8, 31], ["torch.stack", "torch.utils.data.dataset.TensorDataset", "torch.utils.data.DataLoader", "analyzer.get_gradients", "numpy.average", "diff.numpy", "range", "float"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.get_gradients"], ["def", "integrated_gradients", "(", "inputs", ",", "model", ",", "batch_size", ",", "baseline", ",", "explained_class", ",", "steps", "=", "50", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        inputs (array): original inputs\n        model (PyTorch model): pretrained model\n        batch_size (int): batch size to run per epoch\n        baseline (array): reference point\n        explained_class (int): index of the class to be explained\n        steps (int, optional): number of steps along path\n    \"\"\"", "\n", "# Scale the input with k/m progressive steps multiplier", "\n", "diff", "=", "inputs", "-", "baseline", "\n", "scaled_inputs", "=", "[", "baseline", "+", "(", "float", "(", "i", ")", "/", "steps", ")", "*", "diff", "for", "i", "in", "range", "(", "steps", "+", "1", ")", "]", "\n", "scaled_inputs", "=", "torch", ".", "stack", "(", "scaled_inputs", ",", "dim", "=", "0", ")", "# shape: (k + 1, C, H, W)", "\n", "scaled_dataset", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "TensorDataset", "(", "scaled_inputs", ")", "\n", "scaled_loader", "=", "DataLoader", "(", "scaled_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "# Get the gradients along this path", "\n", "gradients", "=", "get_gradients", "(", "scaled_loader", ",", "model", ",", "explained_class", "=", "explained_class", ")", "\n", "avg_grads", "=", "np", ".", "average", "(", "gradients", "[", "1", ":", "]", ",", "axis", "=", "0", ")", "# from step 1 onwards", "\n", "\n", "integrated_grad", "=", "diff", ".", "numpy", "(", ")", "*", "avg_grads", "\n", "return", "integrated_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.smooth_taylor": [[33, 60], ["torch.stack", "range", "torch.utils.data.dataset.TensorDataset", "torch.utils.data.DataLoader", "analyzer.get_gradients", "numpy.mean", "torch.zeros_like", "range", "numpy.max", "numpy.min", "torch.randn_like", "range", "inputs.numpy", "inputs.numpy"], "function", ["home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.get_gradients"], ["", "def", "smooth_taylor", "(", "inputs", ",", "model", ",", "batch_size", ",", "noise_scale", ",", "num_roots", ",", "explained_class", ",", "percent", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        inputs (array): original inputs\n        model (PyTorch model): pretrained model\n        batch_size (int): batch size to run per epoch\n        noise_scale (float): scale to noise the inputs\n        num_roots (int): number of noised inputs to generate\n        explained_class (int): index of the class to be explained\n        percent (bool, optional): use noise scale percentage\n    \"\"\"", "\n", "# Generate roots dataset based on the original inputs with additive noise", "\n", "roots", "=", "torch", ".", "stack", "(", "[", "torch", ".", "zeros_like", "(", "inputs", ")", "for", "_", "in", "range", "(", "num_roots", ")", "]", ")", "\n", "if", "percent", ":", "\n", "        ", "noise_scale", "=", "noise_scale", "*", "(", "np", ".", "max", "(", "inputs", ".", "numpy", "(", ")", ")", "-", "np", ".", "min", "(", "inputs", ".", "numpy", "(", ")", ")", ")", "\n", "", "for", "i", "in", "range", "(", "num_roots", ")", ":", "\n", "        ", "roots", "[", "i", "]", "=", "inputs", "+", "noise_scale", "*", "torch", ".", "randn_like", "(", "inputs", ")", "\n", "", "roots_dataset", "=", "torch", ".", "utils", ".", "data", ".", "dataset", ".", "TensorDataset", "(", "roots", ")", "\n", "roots_data_loader", "=", "DataLoader", "(", "roots_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", ")", "\n", "\n", "# Compute the gradients w.r.t. explained class for roots", "\n", "gradients", "=", "get_gradients", "(", "roots_data_loader", ",", "model", ",", "explained_class", "=", "explained_class", ")", "\n", "\n", "# Compute SmoothTaylor with contribution from roots", "\n", "attributions", "=", "np", ".", "mean", "(", "[", "(", "inputs", "-", "roots_dataset", "[", "i", "]", "[", "0", "]", ")", ".", "numpy", "(", ")", "*", "gradients", "[", "i", "]", "\n", "for", "i", "in", "range", "(", "num_roots", ")", "]", ",", "axis", "=", "0", ")", "\n", "return", "attributions", "\n", "\n"]], "home.repos.pwc.inspect_result.garygsw_smooth-taylor.attribution.analyzer.get_gradients": [[62, 85], ["numpy.array", "numpy.concatenate", "inputs.to.to", "model", "model.zero_grad", "torch.sum().backward", "torch.no_grad", "inputs.to.grad.detach().cpu().numpy", "np.concatenate.append", "torch.sum", "inputs.to.grad.detach().cpu", "inputs.to.grad.detach"], "function", ["None"], ["", "def", "get_gradients", "(", "data_loader", ",", "model", ",", "explained_class", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        data_loader (DataLoader): object that loads images data\n        model (model): pre-trained PyTorch model\n        explained_class: index of the class to be explained\n    \"\"\"", "\n", "gradients", "=", "[", "]", "\n", "for", "sample_batch", "in", "data_loader", ":", "\n", "        ", "inputs", "=", "sample_batch", "[", "0", "]", "\n", "inputs", "=", "inputs", ".", "to", "(", "DEVICE", ")", "\n", "inputs", ".", "requires_grad", "=", "True", "\n", "\n", "# Perform the backpropagation for the explained class", "\n", "out", "=", "model", "(", "inputs", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "torch", ".", "sum", "(", "out", "[", ":", ",", "explained_class", "]", ")", ".", "backward", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "gradient", "=", "inputs", ".", "grad", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "# retrieve the input gradients", "\n", "gradients", ".", "append", "(", "gradient", ")", "\n", "", "", "gradients", "=", "np", ".", "array", "(", "gradients", ")", "\n", "gradients", "=", "np", ".", "concatenate", "(", "gradients", ")", "\n", "return", "gradients", "\n", "", ""]]}