{"home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_retrieval_v0.item_resorting": [[26, 48], ["enumerate", "sorted", "item[].append", "item[].append", "len", "utils.common.doc_id_to_tokenized_text"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.doc_id_to_tokenized_text"], ["def", "item_resorting", "(", "d_list", ",", "top_k", "=", "None", ")", ":", "\n", "    ", "for", "item", "in", "d_list", ":", "\n", "        ", "t_claim", "=", "' '", ".", "join", "(", "item", "[", "'claim_tokens'", "]", ")", "\n", "item", "[", "'predicted_docids'", "]", "=", "[", "]", "\n", "# for it in item['prioritized_docids']:", "\n", "#     if '-LRB-' in it[0] and common.doc_id_to_tokenized_text(it[0]) in t_claim:", "\n", "#         item['predicted_docids'].append(it[0])", "\n", "\n", "# Reset Exact match", "\n", "# t_claim = ' '.join(item['claim_tokens'])", "\n", "# item['predicted_docids'] = []", "\n", "for", "k", ",", "it", "in", "enumerate", "(", "item", "[", "'prioritized_docids'", "]", ")", ":", "\n", "            ", "if", "'-LRB-'", "in", "it", "[", "0", "]", "and", "common", ".", "doc_id_to_tokenized_text", "(", "it", "[", "0", "]", ")", "in", "t_claim", ":", "\n", "                ", "item", "[", "'prioritized_docids'", "]", "[", "k", "]", "=", "[", "it", "[", "0", "]", ",", "5.0", "]", "\n", "item", "[", "'predicted_docids'", "]", ".", "append", "(", "it", "[", "0", "]", ")", "\n", "\n", "", "", "for", "it", "in", "sorted", "(", "item", "[", "'prioritized_docids'", "]", ",", "key", "=", "lambda", "x", ":", "(", "-", "x", "[", "1", "]", ",", "x", "[", "0", "]", ")", ")", ":", "\n", "            ", "if", "it", "[", "0", "]", "not", "in", "item", "[", "'predicted_docids'", "]", ":", "\n", "                ", "item", "[", "'predicted_docids'", "]", ".", "append", "(", "it", "[", "0", "]", ")", "\n", "\n", "", "", "if", "top_k", "is", "not", "None", "and", "len", "(", "item", "[", "'predicted_docids'", "]", ")", ">", "top_k", ":", "\n", "            ", "item", "[", "'predicted_docids'", "]", "=", "item", "[", "'predicted_docids'", "]", "[", ":", "top_k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_retrieval_v0.fever_retrieval_v0": [[50, 178], ["utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "wiki_util.title_entities_set.get_title_entity_set", "dict", "build_rindex.build_rvindex.load_from_file", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "enumerate", "print", "print", "print", "print", "print", "utils.common.save_jsonl", "evaluation.fever_scorer.fever_score", "utils.common.load_jsonl", "str", "build_rindex.rvindex_scoring.get_query_ngrams", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "hotpot_doc_retri.hotpot_doc_retri_v0.get_kw_matching_results", "hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "dict", "int", "r_list.append", "len_list.append", "collections.Counter().most_common", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "utils.common.load_jsonl", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "_MatchedObject", "flashtext.KeywordProcessor.add_keyword", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "sorted", "fever_utils.fever_db.reverse_convert_brc.replace", "fever_utils.fever_db.reverse_convert_brc", "modified_docids.append", "len", "utils.common.load_jsonl", "ValueError", "flashtext.KeywordProcessor.get_keyword", "_MatchedObject", "flashtext.KeywordProcessor.add_keyword", "hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "collections.Counter", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "title.startswith", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.get_kw_matching_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.reverse_convert_brc", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "", "", "def", "fever_retrieval_v0", "(", "term_retrieval_top_k", "=", "3", ",", "match_filtering_k", "=", "2", ",", "tag", "=", "'dev'", ")", ":", "\n", "# term_retrieval_top_k = 20", "\n", "# term_retrieval_top_k = 20", "\n", "\n", "# term_retrieval_top_k = 3", "\n", "# match_filtering_k = 2", "\n", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "d_tf_idf", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\n", "f\"doc_retri_results/term_based_methods_results/fever_tf_idf_{tag}.jsonl\"", ")", "\n", "\n", "tf_idf_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_tf_idf", ",", "'id'", ")", "\n", "\n", "r_list", "=", "[", "]", "\n", "\n", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "# matched_key_word is the original matched span. we need to save it for group ordering.", "\n", "            ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "{", "kw", ":", "'kwm'", "}", ")", "\n", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_matched_obj", ":", "_MatchedObject", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "disamb_kw", "not", "in", "existing_matched_obj", ".", "matched_keywords_info", ":", "\n", "                        ", "existing_matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "# new_dict = dict()", "\n", "                ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "dict", "(", ")", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "# new_dict[disamb_kw] = 'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "        ", "cur_id", "=", "str", "(", "item", "[", "'id'", "]", ")", "\n", "query", "=", "item", "[", "'claim'", "]", "\n", "\n", "query_terms", "=", "get_query_ngrams", "(", "query", ")", "\n", "valid_query_terms", "=", "[", "term", "for", "term", "in", "query_terms", "if", "term", "in", "g_score_dict", "]", "\n", "\n", "retrieved_set", "=", "RetrievedSet", "(", ")", "\n", "# print(tf_idf_doc_list)", "\n", "get_kw_matching_results", "(", "query", ",", "valid_query_terms", ",", "retrieved_set", ",", "match_filtering_k", ",", "\n", "g_score_dict", ",", "keyword_processor", ",", "keyword_processor_disamb", ")", "\n", "\n", "tf_idf_doc_list", "=", "tf_idf_dict", "[", "cur_id", "]", "[", "'retrieved_list'", "]", "\n", "added_count", "=", "0", "\n", "for", "score", ",", "title", "in", "sorted", "(", "\n", "tf_idf_doc_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "term_retrieval_top_k", "+", "3", "]", ":", "\n", "            ", "if", "not", "filter_word", "(", "title", ")", "and", "not", "filter_document_id", "(", "title", ")", "and", "not", "title", ".", "startswith", "(", "'List of '", ")", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "RetrievedItem", "(", "title", ",", "'tf-idf'", ")", ")", "\n", "added_count", "+=", "1", "\n", "if", "term_retrieval_top_k", "is", "not", "None", "and", "added_count", ">=", "term_retrieval_top_k", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "predicted_docids", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "# print(retrieved_set)", "\n", "# print(item['claim'], predicted_docids)", "\n", "\n", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'id'", "]", "=", "int", "(", "cur_id", ")", "\n", "r_item", "[", "'claim'", "]", "=", "item", "[", "'claim'", "]", "\n", "r_item", "[", "'predicted_docids'", "]", "=", "predicted_docids", "\n", "if", "tag", "!=", "'test'", ":", "\n", "            ", "r_item", "[", "'label'", "]", "=", "item", "[", "'label'", "]", "\n", "", "r_list", ".", "append", "(", "r_item", ")", "\n", "\n", "# r_list = common.load_jsonl('dev-debug.jsonl')", "\n", "\n", "# We need to modify the existing retrieved document for naming consistency", "\n", "", "for", "i", ",", "item", "in", "enumerate", "(", "r_list", ")", ":", "\n", "        ", "predicted_docids", "=", "item", "[", "'predicted_docids'", "]", "\n", "modified_docids", "=", "[", "]", "\n", "for", "docid", "in", "predicted_docids", ":", "\n", "            ", "docid", "=", "docid", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "docid", "=", "reverse_convert_brc", "(", "docid", ")", "\n", "modified_docids", ".", "append", "(", "docid", ")", "\n", "", "item", "[", "'predicted_docids'", "]", "=", "modified_docids", "\n", "# Modify finished", "\n", "\n", "# print(r_list[0:10])", "\n", "", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "r_list", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", "[", "'predicted_docids'", "]", ")", ")", "\n", "\n", "", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "\n", "print", "(", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "r_list", ",", "f'fever_term_based_retri_results_'", "\n", "f'{tag}_term_topk:{term_retrieval_top_k}_match_filtering_k:{match_filtering_k}.jsonl'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "False", ",", "'check_doc_id_correct'", ":", "True", "}", "\n", "# fever_scorer.fever_score_analysis(r_list, d_list, mode=mode, max_evidence=None)", "\n", "fever_scorer", ".", "fever_score", "(", "r_list", ",", "d_list", ",", "mode", "=", "mode", ",", "max_evidence", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_retrieval_v0.merge_results_with_haonao_module": [[180, 247], ["utils.common.load_jsonl", "utils.common.load_jsonl", "fever_retrieval_v0.item_resorting", "utils.list_dict_data_tool.list_to_dict", "enumerate", "tqdm.tqdm", "print", "print", "print", "print", "print", "utils.common.load_jsonl", "int", "set.union", "list", "evaluation.fever_scorer.fever_score_analysis", "print", "utils.common.save_jsonl", "len_list.append", "collections.Counter().most_common", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "utils.common.load_jsonl", "fever_utils.fever_db.reverse_convert_brc.replace", "fever_utils.fever_db.reverse_convert_brc", "modified_docids.append", "set", "set", "len", "utils.common.load_jsonl", "ValueError", "collections.Counter", "fever_utils.fever_db.reverse_convert_brc.startswith"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_retrieval_v0.item_resorting", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score_analysis", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.reverse_convert_brc", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "merge_results_with_haonao_module", "(", "term_retrieval_top_k", "=", "3", ",", "match_filtering_k", "=", "2", ",", "haonan_topk", "=", "10", ",", "tag", "=", "'dev'", ",", "\n", "save", "=", "False", ")", ":", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "task_name", "=", "'shared_task_dev'", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "task_name", "=", "'train'", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "task_name", "=", "'shared_task_test'", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "# r_list = common.load_jsonl(config.RESULT_PATH / f'doc_retri_results/fever_results/standard_term_based_results/'", "\n", "# f'fever_term_based_retri_results_{tag}_term_topk:{term_retrieval_top_k}_match_filtering_k:{match_filtering_k}.jsonl')", "\n", "\n", "", "r_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "f'doc_retri_results/fever_results/standard_term_based_results/'", "\n", "f'fever_term_based_retri_results_{tag}_term_topk:{term_retrieval_top_k}_match_filtering_k:{match_filtering_k}.jsonl'", ")", "\n", "\n", "old_result_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\n", "f\"doc_retri_results/fever_results/haonans_results/dr_{tag}.jsonl\"", ")", "\n", "item_resorting", "(", "old_result_list", ",", "top_k", "=", "haonan_topk", ")", "\n", "\n", "old_result_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "old_result_list", ",", "'id'", ")", "\n", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "r_list", ")", ":", "\n", "        ", "predicted_docids", "=", "item", "[", "'predicted_docids'", "]", "\n", "modified_docids", "=", "[", "]", "\n", "for", "docid", "in", "predicted_docids", ":", "\n", "            ", "docid", "=", "docid", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "docid", "=", "reverse_convert_brc", "(", "docid", ")", "\n", "modified_docids", ".", "append", "(", "docid", ")", "\n", "", "item", "[", "'predicted_docids'", "]", "=", "modified_docids", "\n", "# item['predicted_docids'] = []", "\n", "\n", "", "merged_result_list", "=", "[", "]", "\n", "for", "item", "in", "tqdm", "(", "r_list", ")", ":", "\n", "        ", "cur_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "old_retrieval_doc", "=", "old_result_dict", "[", "cur_id", "]", "[", "'predicted_docids'", "]", "\n", "new_retrieval_doc", "=", "item", "[", "'predicted_docids'", "]", "\n", "m_predicted_docids", "=", "set", ".", "union", "(", "set", "(", "old_retrieval_doc", ")", ",", "set", "(", "new_retrieval_doc", ")", ")", "\n", "# print(m_predicted_docids)", "\n", "m_predicted_docids", "=", "[", "docid", "for", "docid", "in", "m_predicted_docids", "if", "not", "docid", ".", "startswith", "(", "'List_of_'", ")", "]", "\n", "item", "[", "'predicted_docids'", "]", "=", "list", "(", "m_predicted_docids", ")", "\n", "# print(item['predicted_docids'])", "\n", "\n", "", "mode", "=", "{", "'standard'", ":", "False", ",", "'check_doc_id_correct'", ":", "True", "}", "\n", "if", "tag", "!=", "'test'", ":", "\n", "        ", "fever_scorer", ".", "fever_score_analysis", "(", "r_list", ",", "d_list", ",", "mode", "=", "mode", ",", "max_evidence", "=", "None", ")", "\n", "\n", "", "if", "save", ":", "\n", "        ", "print", "(", "\"Saved to:\"", ")", "\n", "common", ".", "save_jsonl", "(", "r_list", ",", "config", ".", "RESULT_PATH", "/", "\n", "f\"doc_retri_results/fever_results/merged_doc_results/m_doc_{tag}.jsonl\"", ")", "\n", "\n", "# States information.", "\n", "", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "r_list", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", "[", "'predicted_docids'", "]", ")", ")", "\n", "\n", "", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "\n", "print", "(", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_tf_idf_doc_retri.process_fever_item_multiprocessing": [[22, 44], ["dict", "str", "build_rindex.rvindex_scoring.get_query_ngrams", "redis_score_index.get_candidate_set_from_batched_terms", "redis_score_index.get_scores_from_batched_term_doc_pairs", "redis_score_index.scored_dict_ranking"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_candidate_set_from_batched_terms", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_scores_from_batched_term_doc_pairs", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.scored_dict_ranking"], ["def", "process_fever_item_multiprocessing", "(", "item", ",", "top_k", ",", "query_field", "=", "'claim'", ",", "id_field", "=", "'id'", ")", ":", "\n", "# For multiprocess we set some global variable, but be careful and don't give memory intensive global variable.", "\n", "# The variable will be copied to other processes", "\n", "    ", "global", "redis_score_index", "\n", "\n", "results_item", "=", "dict", "(", ")", "\n", "query", "=", "item", "[", "query_field", "]", "\n", "qid", "=", "str", "(", "item", "[", "id_field", "]", ")", "\n", "\n", "query_ngrams", "=", "get_query_ngrams", "(", "query", ")", "\n", "\n", "candidate_doc_list", ",", "valid_set_list", ",", "valid_terms", "=", "redis_score_index", ".", "get_candidate_set_from_batched_terms", "(", "query_ngrams", ")", "\n", "# print(candidate_doc_list)", "\n", "scored_dict", "=", "redis_score_index", ".", "get_scores_from_batched_term_doc_pairs", "(", "valid_terms", ",", "valid_set_list", ")", "\n", "# print(scored_dict)", "\n", "doc_list", "=", "redis_score_index", ".", "scored_dict_ranking", "(", "candidate_doc_list", ",", "scored_dict", ",", "top_k", "=", "top_k", ")", "\n", "\n", "results_item", "[", "query_field", "]", "=", "query", "\n", "results_item", "[", "id_field", "]", "=", "qid", "\n", "results_item", "[", "'retrieved_list'", "]", "=", "doc_list", "\n", "\n", "return", "results_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_tf_idf_doc_retri.process_fever_item_with_score_dict": [[46, 65], ["dict", "str", "build_rindex.rvindex_scoring.get_query_ngrams", "build_rindex.rvindex_scoring.get_candidate_page_list", "build_rindex.rvindex_scoring.get_ranked_score"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_candidate_page_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_ranked_score"], ["", "def", "process_fever_item_with_score_dict", "(", "item", ",", "top_k", ",", "global_score_dict", ",", "query_field", "=", "'claim'", ",", "id_field", "=", "'id'", ")", ":", "\n", "    ", "results_item", "=", "dict", "(", ")", "\n", "# question = item['question']", "\n", "query", "=", "item", "[", "query_field", "]", "\n", "qid", "=", "str", "(", "item", "[", "id_field", "]", ")", "\n", "\n", "query_ngrams", "=", "get_query_ngrams", "(", "query", ")", "\n", "\n", "v_terms", ",", "v_docids", ",", "candidate_doc_list", "=", "get_candidate_page_list", "(", "query_ngrams", ",", "global_score_dict", ")", "\n", "# print(candidate_doc_list)", "\n", "doc_list", "=", "get_ranked_score", "(", "v_terms", ",", "v_docids", ",", "candidate_doc_list", ",", "top_k", ",", "global_score_dict", ")", "\n", "# print(scored_dict)", "\n", "# doc_list = redis_score_index.scored_dict_ranking(candidate_doc_list, scored_dict, top_k=top_k)", "\n", "\n", "results_item", "[", "query_field", "]", "=", "query", "\n", "results_item", "[", "id_field", "]", "=", "qid", "\n", "results_item", "[", "'retrieved_list'", "]", "=", "doc_list", "\n", "\n", "return", "results_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_tf_idf_doc_retri.single_process_fever_with_dict": [[67, 126], ["dict", "build_rindex.build_rvindex.load_from_file", "print", "print", "print", "print", "incr_file.is_file", "save_path.is_file", "print", "utils.common.save_jsonl", "utils.common.load_jsonl", "len", "len", "len", "print", "print", "open", "functools.partial", "tqdm.tqdm", "len", "utils.common.load_jsonl", "functools.partial.", "r_item_list.append", "out_f.write", "out_f.flush", "utils.common.load_jsonl", "ValueError", "len", "json.dumps"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "single_process_fever_with_dict", "(", "start", "=", "0", ",", "end", "=", "None", ",", "tag", "=", "'dev'", ")", ":", "\n", "    ", "task_name", "=", "'fever'", "\n", "debug", "=", "False", "\n", "top_k", "=", "20", "\n", "\n", "query_fieldname", "=", "'claim'", "\n", "id_fieldname", "=", "'id'", "\n", "debug_name", "=", "'debug'", "if", "debug", "else", "\"\"", "\n", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "g_score_dict", "=", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "# Important Set this number !!!", "\n", "", "print", "(", "\"Total length:\"", ",", "len", "(", "d_list", ")", ")", "\n", "# start, end = 0, len(d_list)", "\n", "# Important End !!!", "\n", "\n", "print", "(", "f\"Task:{task_name}, Tag:{tag}, TopK:{top_k}, Start/End:{start}/{end}\"", ")", "\n", "d_list", "=", "d_list", "[", "start", ":", "end", "]", "\n", "\n", "print", "(", "\"Data length:\"", ",", "len", "(", "d_list", ")", ")", "\n", "if", "debug", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "10", "]", "\n", "start", ",", "end", "=", "0", ",", "10", "\n", "", "print", "(", "\"Data length (Pos-filtering):\"", ",", "len", "(", "d_list", ")", ")", "\n", "\n", "r_item_list", "=", "[", "]", "\n", "\n", "incr_file", "=", "config", ".", "RESULT_PATH", "/", "f\"doc_retri_results/term_based_methods_results/{task_name}_tf_idf_{tag}_incr_({start},{end})_{debug_name}.jsonl\"", "\n", "if", "incr_file", ".", "is_file", "(", ")", ":", "\n", "        ", "print", "(", "\"Warning save file exists.\"", ")", "\n", "\n", "", "save_path", ":", "Path", "=", "config", ".", "RESULT_PATH", "/", "f\"doc_retri_results/term_based_methods_results/{task_name}_tf_idf_{tag}_({start},{end})_{debug_name}.jsonl\"", "\n", "if", "save_path", ".", "is_file", "(", ")", ":", "\n", "        ", "print", "(", "\"Warning save file exists.\"", ")", "\n", "\n", "", "with", "open", "(", "incr_file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "process_func", "=", "partial", "(", "process_fever_item_with_score_dict", ",", "\n", "top_k", "=", "top_k", ",", "query_field", "=", "query_fieldname", ",", "id_field", "=", "id_fieldname", ",", "\n", "global_score_dict", "=", "g_score_dict", ")", "\n", "\n", "for", "item", "in", "tqdm", "(", "d_list", ",", "total", "=", "len", "(", "d_list", ")", ")", ":", "\n", "            ", "r_item", "=", "process_func", "(", "item", ")", "\n", "r_item_list", ".", "append", "(", "r_item", ")", "\n", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ")", "+", "'\\n'", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "", "", "print", "(", "len", "(", "r_item_list", ")", ")", "\n", "common", ".", "save_jsonl", "(", "r_item_list", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_doc_retri.fever_tf_idf_doc_retri.multi_process": [[128, 187], ["print", "print", "print", "print", "print", "incr_file.is_file", "save_path.is_file", "print", "utils.common.save_jsonl", "multiprocessing.cpu_count", "utils.common.load_jsonl", "len", "len", "len", "print", "print", "open", "len", "utils.common.load_jsonl", "multiprocessing.Pool", "functools.partial", "pool.imap_unordered", "tqdm.tqdm", "utils.common.load_jsonl", "ValueError", "r_list.append", "out_f.write", "out_f.flush", "len", "json.dumps"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "multi_process", "(", "start", "=", "0", ",", "end", "=", "None", ",", "tag", "=", "'dev'", ")", ":", "\n", "    ", "task_name", "=", "'fever'", "\n", "debug", "=", "False", "\n", "top_k", "=", "20", "\n", "num_process", "=", "3", "\n", "query_fieldname", "=", "'claim'", "\n", "id_fieldname", "=", "'id'", "\n", "debug_name", "=", "'debug'", "if", "debug", "else", "\"\"", "\n", "\n", "# print(multiprocessing.cpu_count())", "\n", "print", "(", "\"CPU Count:\"", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "print", "(", "\"Total length:\"", ",", "len", "(", "d_list", ")", ")", "\n", "# Important Set this number !!!", "\n", "# start, end = 0, None", "\n", "# Important End !!!", "\n", "\n", "print", "(", "f\"Task:{task_name}, Tag:{tag}, TopK:{top_k}, Start/End:{start}/{end}\"", ")", "\n", "d_list", "=", "d_list", "[", "start", ":", "end", "]", "\n", "\n", "print", "(", "\"Data length:\"", ",", "len", "(", "d_list", ")", ")", "\n", "if", "debug", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "10", "]", "\n", "start", ",", "end", "=", "0", ",", "10", "\n", "", "print", "(", "\"Data length (Pos-filtering):\"", ",", "len", "(", "d_list", ")", ")", "\n", "\n", "r_list", "=", "[", "]", "\n", "\n", "incr_file", "=", "config", ".", "RESULT_PATH", "/", "f\"doc_retri_results/term_based_methods_results/{task_name}_tf_idf_{tag}_incr_({start},{end})_{debug_name}.jsonl\"", "\n", "if", "incr_file", ".", "is_file", "(", ")", ":", "\n", "        ", "print", "(", "\"Warning save file exists.\"", ")", "\n", "\n", "", "save_path", ":", "Path", "=", "config", ".", "RESULT_PATH", "/", "f\"doc_retri_results/term_based_methods_results/{task_name}_tf_idf_{tag}_({start},{end})_{debug_name}.jsonl\"", "\n", "if", "save_path", ".", "is_file", "(", ")", ":", "\n", "        ", "print", "(", "\"Warning save file exists.\"", ")", "\n", "\n", "", "with", "open", "(", "incr_file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "with", "Pool", "(", "processes", "=", "num_process", ",", "maxtasksperchild", "=", "1000", ")", "as", "pool", ":", "\n", "\n", "            ", "process_func", "=", "partial", "(", "process_fever_item_multiprocessing", ",", "\n", "top_k", "=", "top_k", ",", "query_field", "=", "query_fieldname", ",", "id_field", "=", "id_fieldname", ")", "\n", "\n", "p_item_list", "=", "pool", ".", "imap_unordered", "(", "process_func", ",", "d_list", ")", "\n", "for", "item", "in", "tqdm", "(", "p_item_list", ",", "total", "=", "len", "(", "d_list", ")", ")", ":", "\n", "                ", "r_list", ".", "append", "(", "item", ")", "\n", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ")", "+", "'\\n'", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "\n", "", "", "", "print", "(", "len", "(", "r_list", ")", ")", "\n", "common", ".", "save_jsonl", "(", "r_list", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.__init__": [[10, 15], ["object.__init__", "dict", "save_tool.ScoreLogger.score_tracker.update"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["    ", "def", "__init__", "(", "self", ",", "init_tracking_dict", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "logging_item_list", "=", "[", "]", "\n", "self", ".", "score_tracker", "=", "dict", "(", ")", "\n", "self", ".", "score_tracker", ".", "update", "(", "init_tracking_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results": [[16, 30], ["score_dict.keys", "score_dict.items", "save_tool.ScoreLogger.logging_item_list.append", "len", "len", "score_dict.keys", "save_tool.ScoreLogger.score_tracker.keys"], "methods", ["None"], ["", "def", "incorporate_results", "(", "self", ",", "score_dict", ",", "save_key", ",", "item", "=", "None", ")", "->", "bool", ":", "\n", "        ", "assert", "len", "(", "score_dict", ".", "keys", "(", ")", ")", "==", "len", "(", "self", ".", "score_tracker", ".", "keys", "(", ")", ")", "\n", "for", "fieldname", "in", "score_dict", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "fieldname", "in", "self", ".", "score_tracker", "\n", "\n", "", "valid_improvement", "=", "False", "\n", "for", "fieldname", ",", "value", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "score_dict", "[", "fieldname", "]", ">=", "self", ".", "score_tracker", "[", "fieldname", "]", ":", "\n", "                ", "self", ".", "score_tracker", "[", "fieldname", "]", "=", "score_dict", "[", "fieldname", "]", "\n", "valid_improvement", "=", "True", "\n", "\n", "", "", "self", ".", "logging_item_list", ".", "append", "(", "{", "'k'", ":", "save_key", ",", "'v'", ":", "item", "}", ")", "\n", "\n", "return", "valid_improvement", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file": [[31, 44], ["pathlib.Path().is_file", "utils.common.save_json", "utils.common.load_json", "set", "pathlib.Path", "set.add", "ValueError"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "logging_to_file", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "Path", "(", "filename", ")", ".", "is_file", "(", ")", ":", "\n", "            ", "old_logging_list", "=", "common", ".", "load_json", "(", "filename", ")", "\n", "current_saved_key", "=", "set", "(", ")", "\n", "\n", "for", "item", "in", "self", ".", "logging_item_list", ":", "\n", "                ", "current_saved_key", ".", "add", "(", "item", "[", "'k'", "]", ")", "\n", "\n", "", "for", "item", "in", "old_logging_list", ":", "\n", "                ", "if", "item", "[", "'k'", "]", "not", "in", "current_saved_key", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Previous logged item can not be found!\"", ")", "\n", "\n", "", "", "", "common", ".", "save_json", "(", "self", ".", "logging_item_list", ",", "filename", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix": [[46, 52], ["os.path.join", "datetime.datetime.now().strftime", "os.path.exists", "os.makedirs", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "gen_file_prefix", "(", "model_name", ",", "date", "=", "None", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d-%H:%M:%S\"", ")", "if", "not", "date", "else", "date", "\n", "file_path", "=", "os", ".", "path", ".", "join", "(", "config", ".", "PRO_ROOT", "/", "'saved_models'", "/", "'_'", ".", "join", "(", "(", "date_now", ",", "model_name", ")", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "file_path", ")", "\n", "", "return", "file_path", ",", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.get_cur_time_str": [[54, 57], ["datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["", "def", "get_cur_time_str", "(", ")", ":", "\n", "    ", "date_now", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d[%H:%M:%S]\"", ")", "\n", "return", "date_now", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.JsonableObjectEncoder.default": [[23, 30], ["isinstance", "d.update", "super().default", "vars", "type"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.JsonableObjectEncoder.default"], ["    ", "def", "default", "(", "self", ",", "o", ")", ":", "\n", "        ", "if", "isinstance", "(", "o", ",", "JsonableObj", ")", ":", "\n", "            ", "d", "=", "{", "'_jcls_'", ":", "type", "(", "o", ")", ".", "__name__", "}", "\n", "d", ".", "update", "(", "vars", "(", "o", ")", ")", "\n", "return", "d", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "default", "(", "o", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.DocIdDict.__init__": [[87, 89], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "tokenized_doc_id_dict", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.DocIdDict.load_dict": [[90, 93], ["json.load", "open"], "methods", ["None"], ["", "def", "load_dict", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tokenized_doc_id_dict", "is", "None", ":", "\n", "            ", "self", ".", "tokenized_doc_id_dict", "=", "json", ".", "load", "(", "open", "(", "config", ".", "FEVER_TOKENIZED_DOC_ID", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.DocIdDict.clean": [[94, 96], ["None"], "methods", ["None"], ["", "", "def", "clean", "(", "self", ")", ":", "\n", "        ", "self", ".", "tokenized_doc_id_dict", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.register_class": [[12, 16], ["registered_jsonabl_classes.update"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["def", "register_class", "(", "cls", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "if", "cls", "not", "in", "registered_jsonabl_classes", ":", "\n", "        ", "registered_jsonabl_classes", ".", "update", "(", "{", "cls", ".", "__name__", ":", "cls", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.unserialize_JsonableObject": [[32, 43], ["d.pop", "cls.__new__", "d.items", "setattr"], "function", ["None"], ["", "", "", "def", "unserialize_JsonableObject", "(", "d", ")", ":", "\n", "    ", "global", "registered_jsonabl_classes", "\n", "classname", "=", "d", ".", "pop", "(", "'_jcls_'", ",", "None", ")", "\n", "if", "classname", ":", "\n", "        ", "cls", "=", "registered_jsonabl_classes", "[", "classname", "]", "\n", "obj", "=", "cls", ".", "__new__", "(", "cls", ")", "# Make instance without calling __init__", "\n", "for", "key", ",", "value", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "obj", ",", "key", ",", "value", ")", "\n", "", "return", "obj", "\n", "", "else", ":", "\n", "        ", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.json_dumps": [[45, 47], ["json.dumps"], "function", ["None"], ["", "", "def", "json_dumps", "(", "item", ")", ":", "\n", "    ", "return", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.json_loads": [[49, 51], ["json.loads"], "function", ["None"], ["", "def", "json_loads", "(", "item_str", ")", ":", "\n", "    ", "return", "json", ".", "loads", "(", "item_str", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl": [[55, 60], ["print", "open", "out_f.write", "json.dumps"], "function", ["None"], ["", "def", "save_jsonl", "(", "d_list", ",", "filename", ")", ":", "\n", "    ", "print", "(", "\"Save to Jsonl:\"", ",", "filename", ")", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "for", "item", "in", "d_list", ":", "\n", "            ", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ",", "cls", "=", "JsonableObjectEncoder", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl": [[62, 73], ["open", "print", "tqdm.tqdm", "json.loads", "d_list.append", "line.strip", "len"], "function", ["None"], ["", "", "", "def", "load_jsonl", "(", "filename", ",", "debug_num", "=", "None", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "print", "(", "\"Load Jsonl:\"", ",", "filename", ")", "\n", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ",", "object_hook", "=", "unserialize_JsonableObject", ")", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "if", "debug_num", "is", "not", "None", "and", "0", "<", "debug_num", "==", "len", "(", "d_list", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json": [[75, 78], ["open", "json.load"], "function", ["None"], ["", "def", "load_json", "(", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "return", "json", ".", "load", "(", "in_f", ",", "object_hook", "=", "unserialize_JsonableObject", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json": [[80, 84], ["open", "json.dump", "out_f.close"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close"], ["", "", "def", "save_json", "(", "obj", ",", "filename", ",", "**", "kwargs", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "obj", ",", "out_f", ",", "cls", "=", "JsonableObjectEncoder", ",", "**", "kwargs", ")", "\n", "out_f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.doc_id_to_tokenized_text": [[103, 115], ["global_doc_id_object.load_dict", "json.load", "open"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.DocIdDict.load_dict"], ["def", "doc_id_to_tokenized_text", "(", "doc_id", ",", "including_lemmas", "=", "False", ")", ":", "\n", "# global tokenized_doc_id_dict", "\n", "    ", "global_doc_id_object", ".", "load_dict", "(", ")", "\n", "tokenized_doc_id_dict", "=", "global_doc_id_object", ".", "tokenized_doc_id_dict", "\n", "\n", "if", "tokenized_doc_id_dict", "is", "None", ":", "\n", "        ", "tokenized_doc_id_dict", "=", "json", ".", "load", "(", "open", "(", "config", ".", "FEVER_TOKENIZED_DOC_ID", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", ")", "\n", "\n", "", "if", "including_lemmas", ":", "\n", "        ", "return", "tokenized_doc_id_dict", "[", "doc_id", "]", "[", "'words'", "]", ",", "tokenized_doc_id_dict", "[", "doc_id", "]", "[", "'lemmas'", "]", "\n", "\n", "", "return", "' '", ".", "join", "(", "tokenized_doc_id_dict", "[", "doc_id", "]", "[", "'words'", "]", ")", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict": [[5, 11], ["dict"], "function", ["None"], ["def", "list_to_dict", "(", "d_list", ",", "key_fields", ")", ":", "#   '_id' or 'pid'", "\n", "    ", "d_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "assert", "key_fields", "in", "item", "\n", "d_dict", "[", "item", "[", "key_fields", "]", "]", "=", "item", "\n", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list": [[13, 26], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "if", "key", "in", "d_dict", ":", "\n", "            ", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "key", "]", "[", "append_fieldname", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style": [[28, 41], ["isinstance", "print"], "function", ["None"], ["", "def", "append_item_from_dict_to_list_hotpot_style", "(", "d_list", ",", "d_dict", ",", "key_fieldname", ",", "append_fieldnames", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "append_fieldnames", ",", "list", ")", ":", "\n", "        ", "append_fieldnames", "=", "[", "append_fieldnames", "]", "\n", "", "for", "item", "in", "d_list", ":", "\n", "        ", "key", "=", "item", "[", "key_fieldname", "]", "\n", "for", "append_fieldname", "in", "append_fieldnames", ":", "\n", "            ", "if", "key", "in", "d_dict", "[", "append_fieldname", "]", ":", "\n", "                ", "item", "[", "append_fieldname", "]", "=", "d_dict", "[", "append_fieldname", "]", "[", "key", "]", "\n", "", "else", ":", "\n", "                ", "print", "(", "f\"Potential Error: {key} not in scored_dict. Maybe bc all forward items are empty.\"", ")", "\n", "# for append_fieldname in append_fieldnames:", "\n", "item", "[", "append_fieldname", "]", "=", "[", "]", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict": [[43, 80], ["d_dict.keys", "dict", "print"], "function", ["None"], ["", "def", "append_subfield_from_list_to_dict", "(", "subf_list", ",", "d_dict", ",", "o_key_field_name", ",", "subfield_key_name", ",", "\n", "subfield_name", "=", "'merged_field'", ",", "check", "=", "False", ")", ":", "\n", "# Often times, we will need to split the one data point to multiple items to be feeded into neural networks", "\n", "# and after we obtain the results we will need to map the results back to original data point with some keys.", "\n", "\n", "# This method is used for this purpose.", "\n", "# The method can be invoke multiple times, (in practice usually one batch per time.)", "\n", "    ", "\"\"\"\n    :param subf_list:               The forward list.\n    :param d_dict:                  The dict that contain keys mapping to original data point.\n    :param o_key_field_name:        The fieldname of original data point key. 'pid'\n    :param subfield_key_name:       The fieldname of the sub item. 'fid'\n    :param subfield_name:           The merge field name.       'merged_field'\n    :param check:\n    :return:\n    \"\"\"", "\n", "for", "key", "in", "d_dict", ".", "keys", "(", ")", ":", "\n", "        ", "d_dict", "[", "key", "]", "[", "subfield_name", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "item", "in", "subf_list", ":", "\n", "        ", "assert", "o_key_field_name", "in", "item", "\n", "assert", "subfield_key_name", "in", "item", "\n", "map_id", "=", "item", "[", "o_key_field_name", "]", "\n", "sub_filed_id", "=", "item", "[", "subfield_key_name", "]", "\n", "assert", "map_id", "in", "d_dict", "\n", "\n", "# if subfield_name not in d_dict[map_id]:", "\n", "#     d_dict[map_id][subfield_name] = dict()", "\n", "\n", "if", "sub_filed_id", "not", "in", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", ":", "\n", "            ", "if", "check", ":", "\n", "                ", "assert", "item", "[", "o_key_field_name", "]", "==", "map_id", "\n", "", "d_dict", "[", "map_id", "]", "[", "subfield_name", "]", "[", "sub_filed_id", "]", "=", "item", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Duplicate forward item with key:\"", ",", "sub_filed_id", ")", "\n", "\n", "", "", "return", "d_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.text_process_tool.spacy_tokenize": [[12, 19], ["text_process_tool.normalize", "nlp", "c_token.text.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "spacy_tokenize", "(", "text", ",", "uncase", "=", "False", ")", ":", "\n", "    ", "text", "=", "normalize", "(", "text", ")", "\n", "t_text", "=", "nlp", "(", "text", ")", "\n", "if", "not", "uncase", ":", "\n", "        ", "return", "[", "c_token", ".", "text", "for", "c_token", "in", "t_text", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "c_token", ".", "text", ".", "lower", "(", ")", "for", "c_token", "in", "t_text", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.text_process_tool.normalize": [[21, 24], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "", "def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.squad_models.bert_span_v0.BertSpan.__init__": [[40, 45], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "bert_span_v0.init_bert_weights"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights"], ["", "def", "__init__", "(", "self", ",", "bert_encoder", ")", ":", "\n", "        ", "super", "(", "BertSpan", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ",", "2", ")", "# Should we have dropout here? Later?", "\n", "init_bert_weights", "(", "self", ".", "qa_outputs", ",", "initializer_range", "=", "0.02", ")", "# Hard code this value", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.squad_models.bert_span_v0.BertSpan.forward": [[46, 74], ["bert_span_v0.BertSpan.bert_encoder", "bert_span_v0.BertSpan.qa_outputs", "flint.span_select", "flint.span_select", "allennlp.nn.util.get_mask_from_sequence_lengths", "allennlp.nn.util.replace_masked_values", "allennlp.nn.util.replace_masked_values", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "allennlp.nn.util.masked_log_softmax", "gt_start.squeeze", "allennlp.nn.util.masked_log_softmax", "gt_end.squeeze"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.span_select", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.span_select"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "context_span", "=", "None", ",", "\n", "gt_span", "=", "None", ",", "max_context_length", "=", "0", ",", "mode", "=", "ForwardMode", ".", "TRAIN", ")", ":", "\n", "# Precomputing of the max_context_length is important", "\n", "# because we want the same value to be shared to different GPUs, dynamic calculating is not feasible.", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert_encoder", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "joint_seq_logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "context_logits", ",", "context_length", "=", "span_util", ".", "span_select", "(", "joint_seq_logits", ",", "context_span", ",", "max_context_length", ")", "\n", "context_mask", "=", "allen_util", ".", "get_mask_from_sequence_lengths", "(", "context_length", ",", "max_context_length", ")", "\n", "\n", "# The following line is from AllenNLP bidaf.", "\n", "start_logits", "=", "allen_util", ".", "replace_masked_values", "(", "context_logits", "[", ":", ",", ":", ",", "0", "]", ",", "context_mask", ",", "-", "1e18", ")", "\n", "# B, T, 2", "\n", "end_logits", "=", "allen_util", ".", "replace_masked_values", "(", "context_logits", "[", ":", ",", ":", ",", "1", "]", ",", "context_mask", ",", "-", "1e18", ")", "\n", "\n", "if", "mode", "==", "BertSpan", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "assert", "gt_span", "is", "not", "None", "\n", "gt_start", "=", "gt_span", "[", ":", ",", "0", "]", "# gt_span: [B, 2]", "\n", "gt_end", "=", "gt_span", "[", ":", ",", "1", "]", "\n", "\n", "start_loss", "=", "nll_loss", "(", "allen_util", ".", "masked_log_softmax", "(", "start_logits", ",", "context_mask", ")", ",", "gt_start", ".", "squeeze", "(", "-", "1", ")", ")", "\n", "end_loss", "=", "nll_loss", "(", "allen_util", ".", "masked_log_softmax", "(", "end_logits", ",", "context_mask", ")", ",", "gt_end", ".", "squeeze", "(", "-", "1", ")", ")", "\n", "\n", "loss", "=", "start_loss", "+", "end_loss", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", ",", "context_length", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.squad_models.bert_span_v0.init_bert_weights": [[21, 33], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "function", ["None"], ["def", "init_bert_weights", "(", "module", ",", "initializer_range", ")", ":", "\n", "    ", "\"\"\" Initialize the weights.\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "        ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "        ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.squad_models.bert_span_v0.non_answer_filter": [[76, 79], ["None"], "function", ["None"], ["", "", "", "def", "non_answer_filter", "(", "fitem", ")", ":", "\n", "    ", "if", "fitem", "[", "'start_position'", "]", "==", "-", "1", "or", "fitem", "[", "'end_position'", "]", "==", "-", "1", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.squad_models.bert_span_v0.go_model": [[81, 183], ["print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "utils.common.load_json", "utils.common.load_json", "span_prediction_task_utils.squad_utils.preprocessing_squad", "span_prediction_task_utils.squad_utils.preprocessing_squad", "span_prediction_task_utils.squad_utils.eitems_to_fitems", "span_prediction_task_utils.squad_utils.eitems_to_fitems", "print", "data_utils.readers.span_pred_reader.BertSpanPredReader", "data_utils.readers.span_pred_reader.BertSpanPredReader.read", "data_utils.readers.span_pred_reader.BertSpanPredReader.read", "print", "allennlp.data.iterators.BasicIterator", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_span_v0.BertSpan", "torch.device", "torch.device", "BertSpan.to", "list", "pytorch_pretrained_bert.BertAdam", "tqdm.tqdm", "len", "len", "BertSpan.named_parameters", "range", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "flint.get_length_and_mask", "max", "paired_sequence.to.to", "paired_segments_ids.to.to", "att_mask.to.to", "seq_context_span.to.to", "gt_span.to.to", "BertSpan.", "model.backward", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "any"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocessing_squad", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocessing_squad", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "", "def", "go_model", "(", ")", ":", "\n", "    ", "bert_model_name", "=", "\"bert-base-uncased\"", "\n", "do_lower_case", "=", "True", "\n", "batch_size", "=", "32", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_optimization_steps", "=", "200", "\n", "debug", "=", "True", "\n", "warmup_rate", "=", "0.1", "\n", "max_pre_context_length", "=", "200", "\n", "max_query_length", "=", "64", "\n", "lazy", "=", "False", "\n", "\n", "print", "(", "\"Potential total length:\"", ",", "max_pre_context_length", "+", "max_query_length", "+", "3", ")", "\n", "# Important: \"max_pre_context_length + max_query_length + 3\" is total length", "\n", "\n", "# debug = False", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "squad_train_v11", "=", "common", ".", "load_json", "(", "config", ".", "SQUAD_TRAIN_1_1", ")", "\n", "squad_dev_v11", "=", "common", ".", "load_json", "(", "config", ".", "SQUAD_DEV_1_1", ")", "\n", "\n", "train_eitem_list", "=", "preprocessing_squad", "(", "squad_train_v11", ")", "\n", "dev_eitem_list", "=", "preprocessing_squad", "(", "squad_dev_v11", ")", "\n", "\n", "if", "debug", ":", "\n", "        ", "train_eitem_list", "=", "[", "train_eitem_list", "[", "0", "]", ",", "train_eitem_list", "[", "100", "]", ",", "train_eitem_list", "[", "200", "]", ",", "\n", "train_eitem_list", "[", "300", "]", ",", "train_eitem_list", "[", "400", "]", "]", "\n", "\n", "", "train_fitem_dict", ",", "train_fitem_list", "=", "eitems_to_fitems", "(", "train_eitem_list", ",", "tokenizer", ",", "is_training", "=", "True", ",", "\n", "max_tokens_for_doc", "=", "max_pre_context_length", ")", "\n", "dev_fitem_dict", ",", "dev_fitem_list", "=", "eitems_to_fitems", "(", "dev_eitem_list", ",", "tokenizer", ",", "is_training", "=", "False", ",", "\n", "max_tokens_for_doc", "=", "max_pre_context_length", ")", "\n", "# Something test", "\n", "\n", "if", "debug", ":", "\n", "        ", "train_fitem_list", "=", "train_fitem_list", "[", ":", "5", "]", "\n", "\n", "", "print", "(", "\"Total train fitems:\"", ",", "len", "(", "train_fitem_list", ")", ")", "\n", "\n", "span_pred_reader", "=", "BertSpanPredReader", "(", "tokenizer", ",", "max_query_length", "=", "max_query_length", ",", "lazy", "=", "lazy", ",", "\n", "example_filter", "=", "non_answer_filter", ")", "\n", "train_instances", "=", "span_pred_reader", ".", "read", "(", "train_fitem_list", ")", "\n", "dev_instances", "=", "span_pred_reader", ".", "read", "(", "dev_fitem_list", ")", "\n", "\n", "print", "(", "\"Total train instances:\"", ",", "len", "(", "train_instances", ")", ")", "\n", "\n", "iterator", "=", "BasicIterator", "(", "batch_size", "=", "batch_size", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertSpan", "(", "bert_encoder", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "model", ".", "to", "(", "device", ")", "# sinlge gpu", "\n", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_rate", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "for", "iteration", "in", "tqdm", "(", "range", "(", "200", ")", ")", ":", "\n", "        ", "t_iter", "=", "iterator", "(", "train_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "tqdm", "(", "t_iter", ")", ":", "\n", "# print(batch['paired_sequence'])", "\n", "# print(span_util.span_select(batch['paired_sequence'], batch['bert_s1_span']))", "\n", "# print(span_util.span_select(batch['paired_sequence'], batch['bert_s2_span']))", "\n", "\n", "            ", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "seq_context_span", "=", "batch", "[", "'bert_s2_span'", "]", "# Context span is s2.", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "b_max_context_length", "=", "max", "(", "[", "end", "-", "start", "for", "(", "start", ",", "end", ")", "in", "batch", "[", "'bert_s2_span'", "]", "]", ")", "# THis is a int", "\n", "gt_span", "=", "batch", "[", "'gt_span'", "]", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "seq_context_span", "=", "seq_context_span", ".", "to", "(", "device", ")", "\n", "gt_span", "=", "gt_span", ".", "to", "(", "device", ")", "\n", "\n", "# b_fids = batch['fid']", "\n", "# b_uids = batch['uid']", "\n", "# print(gt_span)", "\n", "\n", "loss", "=", "model", "(", "mode", "=", "BertSpan", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "input_ids", "=", "paired_sequence", ",", "\n", "token_type_ids", "=", "paired_segments_ids", ",", "\n", "attention_mask", "=", "att_mask", ",", "\n", "context_span", "=", "seq_context_span", ",", "\n", "max_context_length", "=", "b_max_context_length", ",", "\n", "gt_span", "=", "gt_span", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__init__": [[9, 18], ["set", "sorted", "set.add"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["    ", "def", "__init__", "(", "self", ",", "evidences", ")", ":", "\n", "        ", "evidences_set", "=", "set", "(", ")", "\n", "for", "doc_id", ",", "line_num", "in", "evidences", ":", "\n", "            ", "if", "doc_id", "is", "not", "None", "and", "line_num", "is", "not", "None", ":", "\n", "                ", "evidences_set", ".", "add", "(", "(", "doc_id", ",", "line_num", ")", ")", "\n", "\n", "", "", "evidences_list", "=", "sorted", "(", "evidences_set", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "# print(evidences_list)", "\n", "self", ".", "evidences_list", "=", "evidences_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.add_sent": [[19, 24], ["set", "sorted.add", "sorted", "list"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "add_sent", "(", "self", ",", "sent", ",", "ln", ")", ":", "\n", "        ", "o_set", "=", "set", "(", "self", ".", "evidences_list", ")", "\n", "o_set", ".", "add", "(", "(", "sent", ",", "ln", ")", ")", "\n", "o_set", "=", "sorted", "(", "o_set", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "self", ".", "evidences_list", "=", "list", "(", "o_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__eq__": [[25, 39], ["zip", "isinstance", "len", "len"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "o", ":", "object", ")", "->", "bool", ":", "\n", "        ", "if", "not", "isinstance", "(", "o", ",", "Evidences", ")", ":", "\n", "            ", "return", "False", "\n", "\n", "", "if", "len", "(", "o", ".", "evidences_list", ")", "!=", "len", "(", "self", ".", "evidences_list", ")", ":", "\n", "            ", "return", "False", "\n", "\n", "", "is_eq", "=", "True", "\n", "for", "o", ",", "_e", "in", "zip", "(", "o", ".", "evidences_list", ",", "self", ".", "evidences_list", ")", ":", "\n", "            ", "if", "o", "!=", "_e", ":", "\n", "                ", "is_eq", "=", "False", "\n", "break", "\n", "\n", "", "", "return", "is_eq", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__hash__": [[40, 46], ["hash_str.__hash__", "hash_str_list.append"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__hash__"], ["", "def", "__hash__", "(", "self", ")", "->", "int", ":", "\n", "        ", "hash_str_list", "=", "[", "]", "\n", "for", "doc_id", ",", "line_num", "in", "self", ".", "evidences_list", ":", "\n", "            ", "hash_str_list", ".", "append", "(", "f'{doc_id}###{line_num}'", ")", "\n", "", "hash_str", "=", "'@'", ".", "join", "(", "hash_str_list", ")", "\n", "return", "hash_str", ".", "__hash__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__repr__": [[47, 49], ["check_sentences.Evidences.evidences_list.__repr__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{Evidences: '", "+", "self", ".", "evidences_list", ".", "__repr__", "(", ")", "+", "'}'", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__len__": [[50, 52], ["check_sentences.Evidences.evidences_list.__len__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.__len__"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "evidences_list", ".", "__len__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.__iter__": [[53, 55], ["check_sentences.Evidences.evidences_list.__iter__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__iter__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "evidences_list", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.load_data": [[57, 65], ["open", "json.loads", "d_list.append", "line.strip"], "function", ["None"], ["", "", "def", "load_data", "(", "file", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "with", "open", "(", "file", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "in_f", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "\n", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence": [[67, 82], ["set", "check_sentences.Evidences", "set.add", "cleaned_one_annotator_evidences_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "check_and_clean_evidence", "(", "item", ")", ":", "\n", "    ", "whole_annotators_evidences", "=", "item", "[", "'evidence'", "]", "\n", "# print(evidences)", "\n", "evidences_list_set", "=", "set", "(", ")", "\n", "for", "one_annotator_evidences_list", "in", "whole_annotators_evidences", ":", "\n", "        ", "cleaned_one_annotator_evidences_list", "=", "[", "]", "\n", "for", "evidence", "in", "one_annotator_evidences_list", ":", "\n", "            ", "docid", ",", "sent_num", "=", "evidence", "[", "-", "2", "]", ",", "evidence", "[", "-", "1", "]", "\n", "# print(docid, sent_num)", "\n", "cleaned_one_annotator_evidences_list", ".", "append", "(", "(", "docid", ",", "sent_num", ")", ")", "\n", "\n", "", "one_annotator_evidences", "=", "Evidences", "(", "cleaned_one_annotator_evidences_list", ")", "\n", "evidences_list_set", ".", "add", "(", "one_annotator_evidences", ")", "\n", "\n", "", "return", "evidences_list_set", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_evidence_in_db": [[84, 94], ["cursor.execute", "cursor.fetchone"], "function", ["None"], ["", "def", "check_evidence_in_db", "(", "cursor", ",", "doc_id", ",", "line_num", ")", ":", "\n", "    ", "key", "=", "f'{doc_id}(-.-){line_num}'", "\n", "# print(\"SELECT * FROM sentences WHERE id = \\\"%s\\\"\" % key)", "\n", "cursor", ".", "execute", "(", "\"SELECT * FROM sentences WHERE id=?\"", ",", "(", "key", ",", ")", ")", "\n", "fetched_data", "=", "cursor", ".", "fetchone", "(", ")", "\n", "if", "fetched_data", "is", "not", "None", ":", "\n", "        ", "_id", ",", "text", ",", "h_links", ",", "doc_id", "=", "fetched_data", "\n", "", "else", ":", "\n", "        ", "_id", ",", "text", ",", "h_links", ",", "doc_id", "=", "None", ",", "None", ",", "None", ",", "None", "\n", "", "return", "_id", ",", "text", ",", "h_links", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_evidence": [[96, 110], ["check_sentences.check_and_clean_evidence", "print", "print", "enumerate", "print", "check_sentences.check_evidence_in_db", "doc_ids.append", "texts.append", "h_linkss.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_evidence_in_db"], ["", "def", "check_evidence", "(", "item", ",", "cursor", ")", ":", "\n", "    ", "e_list", "=", "check_and_clean_evidence", "(", "item", ")", "\n", "print", "(", "\"claim:\"", ",", "item", "[", "'claim'", "]", ")", "\n", "print", "(", "\"label:\"", ",", "item", "[", "'label'", "]", ")", "\n", "for", "i", ",", "evidences", "in", "enumerate", "(", "e_list", ")", ":", "\n", "        ", "doc_ids", "=", "[", "]", "\n", "texts", "=", "[", "]", "\n", "h_linkss", "=", "[", "]", "\n", "for", "docid", ",", "line_num", "in", "evidences", ":", "\n", "            ", "_id", ",", "text", ",", "h_links", "=", "check_evidence_in_db", "(", "cursor", ",", "docid", ",", "line_num", ")", "\n", "doc_ids", ".", "append", "(", "_id", ")", "\n", "texts", ".", "append", "(", "text", ")", "\n", "h_linkss", ".", "append", "(", "h_links", ")", "\n", "", "print", "(", "i", ",", "doc_ids", ",", "texts", ",", "h_linkss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.normalize": [[8, 11], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.read_data": [[13, 20], ["open", "json.loads", "d_list.append", "line.strip"], "function", ["None"], ["", "def", "read_data", "(", "file", ")", ":", "\n", "    ", "d_list", "=", "[", "]", "\n", "with", "open", "(", "file", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "in_f", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.rule_split": [[22, 64], ["lines_b.find", "lines_b.find", "lines.append", "lines.append", "print", "print", "print", "lines_b.find", "lines_b.find", "lines_b.find"], "function", ["None"], ["", "def", "rule_split", "(", "lines_b", ")", ":", "\n", "    ", "'''\n    Important function!\n    This function clean the lines in original preprocessed wiki Pages.\n    :param lines_b:\n    :return: a list of sentence (responded to each sentence)\n    '''", "\n", "lines", "=", "[", "]", "\n", "\n", "i", "=", "0", "\n", "while", "True", ":", "\n", "        ", "start", "=", "f'\\n{i}\\t'", "\n", "end", "=", "f'\\n{i + 1}\\t'", "\n", "start_index", "=", "lines_b", ".", "find", "(", "start", ")", "\n", "end_index", "=", "lines_b", ".", "find", "(", "end", ")", "\n", "\n", "if", "end_index", "==", "-", "1", ":", "\n", "            ", "extra", "=", "f'\\n{i + 2}\\t'", "\n", "extra_1", "=", "f'\\n{i + 3}\\t'", "\n", "extra_2", "=", "f'\\n{i + 4}\\t'", "\n", "\n", "# print(lines_b[start_index:])", "\n", "lines", ".", "append", "(", "lines_b", "[", "start_index", ":", "]", ")", "\n", "# print('What?')", "\n", "# print(lines_b)", "\n", "# print(extra, extra_1)", "\n", "# print(lines_b.find(extra))", "\n", "# print(lines_b.find(extra_1))", "\n", "# print(not (lines_b.find(extra) == -1 and lines_b.find(extra_1) == -1))", "\n", "\n", "if", "not", "(", "lines_b", ".", "find", "(", "extra", ")", "==", "-", "1", "\n", "and", "lines_b", ".", "find", "(", "extra_1", ")", "==", "-", "1", "\n", "and", "lines_b", ".", "find", "(", "extra_2", ")", "==", "-", "1", ")", ":", "\n", "                ", "print", "(", "lines_b", ")", "\n", "print", "(", "extra", ",", "extra_1", ")", "\n", "print", "(", "'Error'", ")", "\n", "", "break", "\n", "\n", "", "lines", ".", "append", "(", "lines_b", "[", "start_index", ":", "end_index", "]", ")", "\n", "i", "+=", "1", "\n", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.lines_to_items": [[66, 131], ["enumerate", "json.dumps", "dict", "line.split", "line_item_list[].strip", "lines_list.append", "len", "int", "int.isdigit", "print", "print", "print", "print", "int", "int", "print", "print", "print", "print", "len", "lines_list.append", "list", "print", "print", "print", "print", "print", "print", "int", "filter", "len", "print", "len"], "function", ["None"], ["", "def", "lines_to_items", "(", "page_id", ",", "lines", ")", ":", "\n", "    ", "lines_list", "=", "[", "]", "\n", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "line_item", "=", "dict", "(", ")", "\n", "\n", "line_item_list", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "\n", "line_num", "=", "line_item_list", "[", "0", "]", "\n", "if", "not", "line_num", ".", "isdigit", "(", ")", ":", "\n", "            ", "print", "(", "\"None digit\"", ")", "\n", "print", "(", "page_id", ")", "\n", "\n", "print", "(", "lines", ")", "\n", "print", "(", "k", ")", "\n", "", "else", ":", "\n", "            ", "line_num", "=", "int", "(", "line_num", ")", "\n", "\n", "", "if", "int", "(", "line_num", ")", "!=", "i", ":", "\n", "            ", "print", "(", "\"Line num mismath\"", ")", "\n", "print", "(", "int", "(", "line_num", ")", ",", "i", ")", "\n", "print", "(", "page_id", ")", "\n", "\n", "print", "(", "k", ")", "\n", "\n", "", "line_item", "[", "'line_num'", "]", "=", "line_num", "\n", "line_item", "[", "'sentences'", "]", "=", "[", "]", "\n", "line_item", "[", "'h_links'", "]", "=", "[", "]", "\n", "\n", "if", "len", "(", "line_item_list", ")", "<=", "1", ":", "\n", "            ", "lines_list", ".", "append", "(", "line_item", ")", "\n", "continue", "\n", "\n", "", "sent", "=", "line_item_list", "[", "1", "]", ".", "strip", "(", ")", "\n", "h_links", "=", "line_item_list", "[", "2", ":", "]", "\n", "\n", "if", "'thumb'", "in", "h_links", ":", "\n", "            ", "h_links", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "h_links", "=", "list", "(", "filter", "(", "lambda", "x", ":", "len", "(", "x", ")", ">", "0", ",", "h_links", ")", ")", "\n", "\n", "", "line_item", "[", "'sentences'", "]", "=", "sent", "\n", "line_item", "[", "'h_links'", "]", "=", "h_links", "\n", "# print(line_num, sent)", "\n", "# print(len(h_links))", "\n", "# print(sent)", "\n", "# assert sent[-1] == '.'", "\n", "\n", "if", "len", "(", "h_links", ")", "%", "2", "!=", "0", ":", "\n", "            ", "print", "(", "page_id", ")", "\n", "for", "w", "in", "lines", ":", "\n", "                ", "print", "(", "w", ")", "\n", "", "print", "(", "\"Term mod 2 != 0\"", ")", "\n", "\n", "print", "(", "\"List:\"", ",", "line_item_list", ")", "\n", "print", "(", "line_num", ",", "sent", ")", "\n", "print", "(", "h_links", ")", "\n", "print", "(", ")", "\n", "\n", "", "lines_list", ".", "append", "(", "line_item", ")", "\n", "\n", "# print(len(lines_list), lines_list[-1]['line_num'] + 1)", "\n", "", "assert", "len", "(", "lines_list", ")", "==", "int", "(", "lines_list", "[", "-", "1", "]", "[", "'line_num'", "]", "+", "1", ")", "\n", "string_line_item", "=", "json", ".", "dumps", "(", "lines_list", ")", "\n", "return", "string_line_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.parse_pages_checks": [[133, 155], ["dict", "wiki_term_builder.normalize", "wiki_term_builder.normalize", "wiki_term_builder.normalize", "wiki_term_builder.rule_split", "wiki_term_builder.lines_to_items", "item[].strip", "item[].strip", "line.strip().replace", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.rule_split", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.lines_to_items"], ["", "def", "parse_pages_checks", "(", "k", ",", "item", ")", ":", "\n", "    ", "this_item", "=", "dict", "(", ")", "\n", "page_id", "=", "normalize", "(", "item", "[", "'id'", "]", ".", "strip", "(", ")", ")", "\n", "text", "=", "normalize", "(", "item", "[", "'text'", "]", ".", "strip", "(", ")", ")", "\n", "\n", "lines_b", "=", "normalize", "(", "item", "[", "'lines'", "]", ")", "\n", "lines_b", "=", "'\\n'", "+", "lines_b", "\n", "lines", "=", "rule_split", "(", "lines_b", ")", "\n", "\n", "this_item", "[", "'id'", "]", "=", "page_id", "\n", "this_item", "[", "'text'", "]", "=", "text", "\n", "\n", "lines", "=", "[", "line", ".", "strip", "(", ")", ".", "replace", "(", "'\\n'", ",", "''", ")", "for", "line", "in", "lines", "]", "\n", "\n", "if", "len", "(", "lines", ")", "==", "1", "and", "lines", "[", "0", "]", "==", "''", ":", "\n", "        ", "lines", "=", "[", "'0'", "]", "\n", "\n", "", "string_lines", "=", "lines_to_items", "(", "page_id", ",", "lines", ")", "\n", "\n", "this_item", "[", "'lines'", "]", "=", "string_lines", "\n", "\n", "return", "this_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_cursor": [[12, 16], ["str", "sqlite3.connect", "sqlite3.connect.cursor", "str"], "function", ["None"], ["def", "get_cursor", "(", "save_path", "=", "str", "(", "config", ".", "FEVER_DB", ")", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "str", "(", "save_path", ")", ")", "\n", "cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "return", "cursor", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_evidence": [[18, 28], ["cursor.execute", "cursor.fetchone"], "function", ["None"], ["", "def", "get_evidence", "(", "cursor", ",", "doc_id", ",", "line_num", ")", ":", "\n", "    ", "key", "=", "f'{doc_id}(-.-){line_num}'", "\n", "# print(\"SELECT * FROM sentences WHERE id = \\\"%s\\\"\" % key)", "\n", "cursor", ".", "execute", "(", "\"SELECT * FROM sentences WHERE id=?\"", ",", "(", "key", ",", ")", ")", "\n", "fetched_data", "=", "cursor", ".", "fetchone", "(", ")", "\n", "if", "fetched_data", "is", "not", "None", ":", "\n", "        ", "_id", ",", "text", ",", "h_links", ",", "doc_id", "=", "fetched_data", "\n", "", "else", ":", "\n", "        ", "_id", ",", "text", ",", "h_links", ",", "doc_id", "=", "None", ",", "None", ",", "None", ",", "None", "\n", "", "return", "_id", ",", "text", ",", "h_links", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_all_sent_by_doc_id": [[30, 47], ["cursor.execute", "cursor.fetchall", "r_list.append", "id_list.append", "h_links_list.append", "json.loads"], "function", ["None"], ["", "def", "get_all_sent_by_doc_id", "(", "cursor", ",", "doc_id", ",", "with_h_links", "=", "False", ")", ":", "\n", "    ", "cursor", ".", "execute", "(", "\"SELECT * FROM sentences WHERE doc_id=?\"", ",", "(", "doc_id", ",", ")", ")", "\n", "fetched_data", "=", "cursor", ".", "fetchall", "(", ")", "\n", "r_list", "=", "[", "]", "\n", "id_list", "=", "[", "]", "\n", "h_links_list", "=", "[", "]", "\n", "for", "id", ",", "text", ",", "h_links", ",", "doc_id", "in", "fetched_data", ":", "\n", "# print(id, text, h_li", "\n", "# nks, doc_id)", "\n", "        ", "r_list", ".", "append", "(", "text", ")", "\n", "id_list", ".", "append", "(", "id", ")", "\n", "h_links_list", ".", "append", "(", "json", ".", "loads", "(", "h_links", ")", ")", "\n", "\n", "", "if", "with_h_links", ":", "\n", "        ", "return", "r_list", ",", "id_list", ",", "h_links_list", "\n", "", "else", ":", "\n", "        ", "return", "r_list", ",", "id_list", "\n", "# API Ends", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc": [[51, 60], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub"], "function", ["None"], ["", "", "def", "convert_brc", "(", "string", ")", ":", "\n", "    ", "string", "=", "re", ".", "sub", "(", "'-LRB-'", ",", "'('", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-RRB-'", ",", "')'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-LSB-'", ",", "'['", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-RSB-'", ",", "']'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-LCB-'", ",", "'{'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-RCB-'", ",", "'}'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'-COLON-'", ",", "':'", ",", "string", ")", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.reverse_convert_brc": [[62, 71], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub"], "function", ["None"], ["", "def", "reverse_convert_brc", "(", "string", ")", ":", "\n", "    ", "string", "=", "re", ".", "sub", "(", "'\\('", ",", "'-LRB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\)'", ",", "'-RRB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'\\['", ",", "'-LSB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "']'", ",", "'-RSB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'{'", ",", "'-LCB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "'}'", ",", "'-RCB-'", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "':'", ",", "'-COLON-'", ",", "string", ")", "\n", "return", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.create_db": [[73, 77], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute"], "function", ["None"], ["", "def", "create_db", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\"CREATE TABLE documents (id PRIMARY KEY, text, lines_json);\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.create_sent_db": [[79, 84], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "conn.cursor.execute"], "function", ["None"], ["", "def", "create_sent_db", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\"CREATE TABLE sentences (id PRIMARY KEY, text, h_links, doc_id);\"", ")", "\n", "c", ".", "execute", "(", "\"CREATE INDEX doc_id_index ON sentences(doc_id);\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.insert_many": [[86, 88], ["cursor.executemany"], "function", ["None"], ["", "def", "insert_many", "(", "cursor", ",", "items", ")", ":", "\n", "    ", "cursor", ".", "executemany", "(", "\"INSERT INTO documents VALUES (?,?,?)\"", ",", "items", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.save_file_to_db": [[90, 102], ["fever_utils.wiki_term_builder.read_data", "range", "len", "fever_db.insert_many", "fever_utils.wiki_term_builder.parse_pages_checks", "buffer_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.read_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.insert_many", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.wiki_term_builder.parse_pages_checks"], ["", "def", "save_file_to_db", "(", "file", ",", "cursor", ")", ":", "\n", "    ", "d_list", "=", "read_data", "(", "file", ")", "\n", "batch_size", "=", "20", "\n", "for", "start", "in", "range", "(", "0", ",", "len", "(", "d_list", ")", ",", "batch_size", ")", ":", "\n", "        ", "end", "=", "start", "+", "batch_size", "\n", "buffer_list", "=", "[", "]", "\n", "for", "one_entity", "in", "d_list", "[", "start", ":", "end", "]", ":", "\n", "            ", "this_item", "=", "parse_pages_checks", "(", "0", ",", "one_entity", ")", "\n", "buffer_list", ".", "append", "(", "(", "this_item", "[", "'id'", "]", ",", "this_item", "[", "'text'", "]", ",", "this_item", "[", "'lines'", "]", ")", ")", "\n", "\n", "", "insert_many", "(", "cursor", ",", "buffer_list", ")", "\n", "del", "buffer_list", "\n", "# print(buffer_list)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.save_wiki_pages": [[105, 113], ["sqlite3.connect", "sqlite3.connect.cursor", "tqdm.tqdm", "sqlite3.connect.close", "config.WIKI_PAGE_PATH.iterdir", "print", "fever_db.save_file_to_db", "sqlite3.connect.commit"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.save_file_to_db"], ["", "", "def", "save_wiki_pages", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "for", "file", "in", "tqdm", "(", "config", ".", "WIKI_PAGE_PATH", ".", "iterdir", "(", ")", ")", ":", "\n", "        ", "print", "(", "file", ")", "\n", "save_file_to_db", "(", "file", ",", "cursor", "=", "c", ")", "\n", "conn", ".", "commit", "(", ")", "\n", "", "conn", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.iter_over_db": [[115, 132], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "tqdm.tqdm", "print", "pid.strip().split", "print", "len", "len", "json.loads", "pid.strip"], "function", ["None"], ["", "def", "iter_over_db", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\"SELECT * from documents\"", ")", "\n", "count", "=", "0", "\n", "for", "pid", ",", "text", ",", "lines", "in", "tqdm", "(", "c", ",", "total", "=", "5416537", ")", ":", "\n", "        ", "pid_words", "=", "pid", ".", "strip", "(", ")", ".", "split", "(", "'_'", ")", "\n", "print", "(", "pid_words", ",", "len", "(", "pid_words", ")", ")", "\n", "\n", "if", "len", "(", "text", ")", ">", "1", ":", "\n", "            ", "lines_items", "=", "json", ".", "loads", "(", "lines", ")", "\n", "for", "line", "in", "lines_items", ":", "\n", "# print(line['sentences'])", "\n", "                ", "if", "line", "[", "'sentences'", "]", ":", "\n", "                    ", "count", "+=", "1", "\n", "\n", "", "", "", "", "print", "(", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_all_doc_ids": [[134, 156], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "tqdm.tqdm", "text.strip.strip", "id_list.append", "len", "len"], "function", ["None"], ["", "def", "get_all_doc_ids", "(", "save_path", ",", "max_ind", "=", "None", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\"SELECT * from documents\"", ")", "\n", "id_list", "=", "[", "]", "\n", "count", "=", "0", "\n", "for", "pid", ",", "text", ",", "lines", "in", "tqdm", "(", "c", ",", "total", "=", "5416537", ")", ":", "\n", "        ", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "len", "(", "text", ")", ">", "1", "and", "len", "(", "lines", ")", ">", "0", ":", "# len(text) > 1 will leave out some error documents", "\n", "            ", "id_list", ".", "append", "(", "pid", ")", "\n", "count", "+=", "1", "\n", "\n", "", "if", "max_ind", "is", "not", "None", "and", "count", ">", "max_ind", ":", "\n", "            ", "break", "\n", "# if len(text) == 1:", "\n", "#     print(pid, text)", "\n", "\n", "# print(convert_brc(pid))", "\n", "\n", "# print(len(id_list))", "\n", "# print(len(set(id_list)))", "\n", "", "", "return", "id_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.build_sentences_table": [[158, 181], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "sqlite3.connect.cursor", "tqdm.tqdm", "print", "sqlite3.connect.commit", "sqlite3.connect.close", "len", "json.loads", "json.dumps", "fever_db.insert_one_sent", "str"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.insert_one_sent"], ["", "def", "build_sentences_table", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c_doc", "=", "conn", ".", "cursor", "(", ")", "\n", "c_doc", ".", "execute", "(", "\"SELECT * from documents\"", ")", "\n", "\n", "c_sent", "=", "conn", ".", "cursor", "(", ")", "\n", "count", "=", "0", "\n", "\n", "for", "pid", ",", "text", ",", "lines", "in", "tqdm", "(", "c_doc", ",", "total", "=", "5416537", ")", ":", "\n", "        ", "if", "len", "(", "text", ")", ">", "1", ":", "\n", "            ", "lines_items", "=", "json", ".", "loads", "(", "lines", ")", "\n", "for", "line", "in", "lines_items", ":", "\n", "# print(line['sentences'])", "\n", "                ", "if", "line", "[", "'sentences'", "]", ":", "\n", "                    ", "count", "+=", "1", "\n", "sent_pid", "=", "pid", "+", "'(-.-)'", "+", "str", "(", "line", "[", "'line_num'", "]", ")", "\n", "sent", "=", "line", "[", "'sentences'", "]", "\n", "h_links", "=", "json", ".", "dumps", "(", "line", "[", "'h_links'", "]", ")", "\n", "insert_one_sent", "(", "c_sent", ",", "(", "sent_pid", ",", "sent", ",", "h_links", ",", "pid", ")", ")", "\n", "\n", "", "", "", "", "print", "(", "count", ")", "\n", "conn", ".", "commit", "(", ")", "\n", "conn", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.insert_one_sent": [[183, 185], ["cursor.execute"], "function", ["None"], ["", "def", "insert_one_sent", "(", "cursor", ",", "item", ")", ":", "\n", "    ", "cursor", ".", "execute", "(", "\"INSERT INTO sentences VALUES (?,?,?,?)\"", ",", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.check_document_id": [[187, 203], ["sqlite3.connect", "sqlite3.connect.cursor", "conn.cursor.execute", "tqdm.tqdm", "print", "pid.strip().replace", "re.search", "print", "fever_utils.text_clean.check_arabic", "pid.strip", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.check_arabic"], ["", "def", "check_document_id", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "c", "=", "conn", ".", "cursor", "(", ")", "\n", "c", ".", "execute", "(", "\"SELECT * from documents\"", ")", "\n", "count", "=", "0", "\n", "for", "pid", ",", "text", ",", "lines", "in", "tqdm", "(", "c", ",", "total", "=", "5416537", ")", ":", "\n", "        ", "pid_words", "=", "pid", ".", "strip", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "match", "=", "re", ".", "search", "(", "'[a-zA-Z]'", ",", "pid_words", ")", "\n", "if", "match", "is", "None", ":", "\n", "            ", "print", "(", "pid_words", ")", "\n", "", "elif", "text_clean", ".", "check_arabic", "(", "pid_words", ")", ":", "\n", "            ", "print", "(", "'arabic:'", ",", "pid_words", ")", "\n", "", "else", ":", "\n", "            ", "count", "+=", "1", "\n", "\n", "", "", "print", "(", "count", ")", "\n", "# print(pid_words, len(pid_words))", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.hlink_sanity_check": [[215, 239], ["fever_db.get_cursor", "fever_db.get_cursor", "get_cursor.execute", "tqdm.tqdm", "json.loads", "enumerate", "json.loads.remove", "print", "fever_db.reverse_convert_brc", "get_cursor.execute", "get_cursor.fetchall", "len", "reverse_convert_brc.replace", "len", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.reverse_convert_brc"], ["", "def", "hlink_sanity_check", "(", ")", ":", "\n", "    ", "cursor", "=", "get_cursor", "(", ")", "\n", "doc_c", "=", "get_cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\"SELECT * FROM sentences\"", ")", "\n", "for", "id", ",", "text", ",", "h_links", ",", "doc_id", "in", "tqdm", "(", "cursor", ",", "total", "=", "25_247_794", ")", ":", "\n", "# print(h_links)", "\n", "\n", "# print(len(h_links))", "\n", "        ", "h_links", "=", "json", ".", "loads", "(", "h_links", ")", "\n", "if", "id", "==", "'Hurlingham_Polo_Association(-.-)1'", ":", "\n", "            ", "h_links", ".", "remove", "(", "'Asociacion Argentina de Polo'", ")", "\n", "\n", "", "if", "len", "(", "h_links", ")", "%", "2", "!=", "0", ":", "\n", "            ", "print", "(", "id", ",", "text", ",", "h_links", ")", "\n", "\n", "", "for", "i", ",", "h_link_doc_id", "in", "enumerate", "(", "h_links", "[", "1", ":", ":", "2", "]", ")", ":", "\n", "            ", "h_link_doc_id", "=", "reverse_convert_brc", "(", "h_link_doc_id", ".", "replace", "(", "' '", ",", "'_'", ")", ")", "\n", "# print(h_link_doc_id)", "\n", "\n", "doc_c", ".", "execute", "(", "\"SELECT * FROM documents where id=(?) COLLATE NOCASE\"", ",", "(", "h_link_doc_id", ",", ")", ")", "\n", "doc", "=", "doc_c", ".", "fetchall", "(", ")", "\n", "# print(h_link_doc_id, doc)", "\n", "if", "len", "(", "doc", ")", "==", "0", ":", "\n", "                ", "print", "(", "h_link_doc_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.text_clean.normalize": [[26, 29], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.text_clean.filter_word": [[31, 39], ["text_clean.normalize", "regex.match", "normalize.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "def", "filter_word", "(", "text", ")", ":", "\n", "    ", "\"\"\"Take out english stopwords, punctuation, and compound endings.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "if", "regex", ".", "match", "(", "r'^\\p{P}+$'", ",", "text", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "STOPWORDS", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.text_clean.filter_ngram": [[41, 60], ["text_clean.filter_word", "any", "all", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word"], ["", "def", "filter_ngram", "(", "gram", ",", "mode", "=", "'any'", ")", ":", "\n", "    ", "\"\"\"Decide whether to keep or discard an n-gram.\n\n    Args:\n        gram: list of tokens (length N)\n        mode: Option to throw out ngram if\n          'any': any single token passes filter_word\n          'all': all tokens pass filter_word\n          'ends': book-ended by filterable tokens\n    \"\"\"", "\n", "filtered", "=", "[", "filter_word", "(", "w", ")", "for", "w", "in", "gram", "]", "\n", "if", "mode", "==", "'any'", ":", "\n", "        ", "return", "any", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'all'", ":", "\n", "        ", "return", "all", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'ends'", ":", "\n", "        ", "return", "filtered", "[", "0", "]", "or", "filtered", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Invalid mode: %s'", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.text_clean.check_arabic": [[62, 72], ["re.findall", "len"], "function", ["None"], ["", "", "def", "check_arabic", "(", "input_string", ")", ":", "\n", "    ", "res", "=", "re", ".", "findall", "(", "\n", "r'[\\U00010E60-\\U00010E7F]|[\\U0001EE00-\\U0001EEFF]|[\\u0750-\\u077F]|[\\u08A0-\\u08FF]|[\\uFB50-\\uFDFF]|[\\uFE70-\\uFEFF]|[\\u0600-\\u06FF]'", ",", "input_string", ")", "\n", "# [\\U00010E60-\\U00010E7F]+|[\\U0001EE00-\\U0001EEFF]+", "\n", "\n", "# print(res)", "\n", "if", "len", "(", "res", ")", "!=", "0", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.text_clean.filter_document_id": [[74, 83], ["input_string.strip().replace", "re.search", "text_clean.check_arabic", "input_string.strip"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.check_arabic"], ["", "", "def", "filter_document_id", "(", "input_string", ")", ":", "\n", "    ", "pid_words", "=", "input_string", ".", "strip", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "match", "=", "re", ".", "search", "(", "'[a-zA-Z]'", ",", "pid_words", ")", "\n", "if", "match", "is", "None", ":", "\n", "        ", "return", "True", "\n", "", "elif", "check_arabic", "(", "pid_words", ")", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_tf_idf_doc_retri.toy_init_results": [[10, 38], ["utils.common.load_json", "print", "build_rindex.build_rvindex.IndexDB", "build_rindex.build_rvindex.IndexDB.load_from_file", "print", "build_rindex.build_rvindex.IndexDB.inverted_index.build_Nt_table", "dict", "build_rindex.build_rvindex.load_from_file", "tqdm.tqdm", "utils.common.save_jsonl", "len", "len", "dict", "build_rindex.rvindex_scoring.get_top_ranked_tf_idf_doc", "saved_items.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.build_Nt_table", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_top_ranked_tf_idf_doc"], ["def", "toy_init_results", "(", ")", ":", "\n", "    ", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "# Load rindex file", "\n", "abs_rindexdb", "=", "IndexDB", "(", ")", "\n", "abs_rindexdb", ".", "load_from_file", "(", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb\"", ")", "\n", "print", "(", "\"Number of terms:\"", ",", "len", "(", "abs_rindexdb", ".", "inverted_index", ".", "index", ")", ")", "\n", "abs_rindexdb", ".", "inverted_index", ".", "build_Nt_table", "(", ")", "\n", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", "=", "dict", "(", ")", "\n", "load_from_file", "(", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "# Load rindex finished", "\n", "\n", "saved_items", "=", "[", "]", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "saved_tfidf_item", "=", "dict", "(", ")", "\n", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "doc_list", "=", "get_top_ranked_tf_idf_doc", "(", "question", ",", "abs_rindexdb", ",", "top_k", "=", "50", ")", "\n", "saved_tfidf_item", "[", "'question'", "]", "=", "question", "\n", "saved_tfidf_item", "[", "'qid'", "]", "=", "qid", "\n", "saved_tfidf_item", "[", "'doc_list'", "]", "=", "doc_list", "\n", "\n", "saved_items", ".", "append", "(", "saved_tfidf_item", ")", "\n", "\n", "", "common", ".", "save_jsonl", "(", "saved_items", ",", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_tf_idf_doc_retri.load_and_eval": [[40, 58], ["utils.common.load_jsonl", "tqdm.tqdm", "utils.common.load_json", "evaluation.ext_hotpot_eval.eval", "dict", "sorted"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "load_and_eval", "(", ")", ":", "\n", "    ", "top_k", "=", "50", "\n", "value_thrsehold", "=", "None", "\n", "tf_idf_dev_results", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "tf_idf_dev_results", ")", ":", "\n", "        ", "sorted_scored_list", "=", "sorted", "(", "item", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "pred_list", "=", "[", "docid", "for", "_", ",", "docid", "in", "sorted_scored_list", "[", ":", "top_k", "]", "]", "\n", "# print(sorted_scored_list)", "\n", "\n", "qid", "=", "item", "[", "'qid'", "]", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "pred_list", "\n", "\n", "# break", "\n", "\n", "", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict", ",", "dev_fullwiki_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.filter_disamb_doc": [[23, 28], ["None"], "function", ["None"], ["def", "filter_disamb_doc", "(", "input_string", ")", ":", "\n", "    ", "if", "' (disambiguation)'", "in", "input_string", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.check_arabic": [[30, 39], ["re.findall", "len"], "function", ["None"], ["", "", "def", "check_arabic", "(", "input_string", ")", ":", "\n", "    ", "res", "=", "re", ".", "findall", "(", "\n", "r'[\\U00010E60-\\U00010E7F]|[\\U0001EE00-\\U0001EEFF]|[\\u0750-\\u077F]|[\\u08A0-\\u08FF]|[\\uFB50-\\uFDFF]|[\\uFE70-\\uFEFF]|[\\u0600-\\u06FF]'", ",", "\n", "input_string", ")", "\n", "\n", "if", "len", "(", "res", ")", "!=", "0", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.filter_document_id": [[41, 53], ["input_string.strip().replace", "re.search", "hotpot_preliminary_doc_retri.check_arabic", "input_string.strip", "hotpot_preliminary_doc_retri.filter_disamb_doc"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.check_arabic", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_disamb_doc"], ["", "", "def", "filter_document_id", "(", "input_string", ",", "remove_disambiguation_doc", "=", "True", ")", ":", "\n", "    ", "pid_words", "=", "input_string", ".", "strip", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "match", "=", "re", ".", "search", "(", "'[a-zA-Z]'", ",", "pid_words", ")", "\n", "if", "match", "is", "None", ":", "# filter id that contains no alphabets characters", "\n", "        ", "return", "True", "\n", "", "elif", "check_arabic", "(", "pid_words", ")", ":", "# remove id that contain arabic characters.", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "if", "remove_disambiguation_doc", ":", "\n", "            ", "if", "filter_disamb_doc", "(", "input_string", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.normalize": [[76, 79], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.filter_word": [[81, 89], ["hotpot_preliminary_doc_retri.normalize", "regex.match", "normalize.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "def", "filter_word", "(", "text", ")", ":", "\n", "    ", "\"\"\"Take out english stopwords, punctuation, and compound endings.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "if", "regex", ".", "match", "(", "r'^\\p{P}+$'", ",", "text", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "STOPWORDS", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results": [[91, 124], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "flashtext.KeywordProcessor.extract_keywords", "set", "sorted", "list", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "isinstance", "set.union", "list", "kw.lower", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results", "(", ")", ":", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", "}", ")", "\n", "\n", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "finded_keys", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ")", "\n", "finded_keys_set", "=", "set", "(", ")", "\n", "if", "isinstance", "(", "finded_keys", ",", "list", ")", "and", "len", "(", "finded_keys", ")", "!=", "0", ":", "\n", "            ", "finded_keys_set", "=", "set", ".", "union", "(", "*", "finded_keys", ")", "\n", "\n", "# Addons cut retrieved document to contain only two", "\n", "", "finded_keys_set", "=", "sorted", "(", "list", "(", "finded_keys_set", ")", ",", "key", "=", "lambda", "x", ":", "len", "(", "x", ")", ",", "reverse", "=", "True", ")", "\n", "top_n", "=", "2", "\n", "finded_keys_set", "=", "finded_keys_set", "[", ":", "top_n", "]", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "finded_keys_set", ")", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"toy_doc_rm_stopword_top2_pred_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v1": [[126, 194], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "dict", "flashtext.KeywordProcessor.extract_keywords", "set", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "isinstance", "set.union", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.add_item", "kw.lower", "len", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "flatten_hyperlinks.extend", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "retrieval_utils.RetrievedSet.add_item", "hyperlinked_title.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_preliminary_doc_retri.filter_document_id", "potential_title.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v1", "(", ")", ":", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", "}", ")", "\n", "\n", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "finded_keys", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ")", "\n", "finded_keys_set", "=", "set", "(", ")", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "# .1 We first find the raw matching.", "\n", "if", "isinstance", "(", "finded_keys", ",", "list", ")", "and", "len", "(", "finded_keys", ")", "!=", "0", ":", "\n", "            ", "finded_keys_set", "=", "set", ".", "union", "(", "*", "finded_keys", ")", "\n", "\n", "", "for", "page_name", "in", "finded_keys_set", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm'", ")", ")", "\n", "\n", "# .2 Then we add more disambiguation titles.    # Comment out this to remove this function.", "\n", "", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "if", "keyword", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "                ", "for", "page_name", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "keyword", "]", ":", "\n", "                    ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb'", ")", ")", "\n", "# finded_keys_set = set.union(finded_keys_set, wiki_util.title_entities_set.disambiguation_group[keyword])", "\n", "\n", "", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "hyperlinked_title", "=", "[", "]", "\n", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "potential_title", ".", "lower", "(", ")", "not", "in", "STOPWORDS", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "                    ", "hyperlinked_title", ".", "append", "(", "potential_title", ")", "\n", "# finded_keys_set.add(potential_title)", "\n", "\n", "# finded_keys_set = set.union(set(hyperlinked_title), finded_keys_set)", "\n", "", "", "", "for", "page_name", "in", "hyperlinked_title", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v2": [[196, 278], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "dict", "flashtext.KeywordProcessor.extract_keywords", "set", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "flashtext.KeywordProcessor.extract_keywords", "set", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "isinstance", "set.union", "retrieval_utils.RetrievedSet.add_item", "isinstance", "set.union", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.add_item", "kw.lower", "kw.lower", "len", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "len", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "flatten_hyperlinks.extend", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hyperlinked_title.append", "hotpot_preliminary_doc_retri.filter_document_id", "potential_title.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v2", "(", ")", ":", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", "}", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ")", "\n", "\n", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "finded_keys", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ")", "\n", "finded_keys_set", "=", "set", "(", ")", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "# .1 We first find the raw matching.", "\n", "if", "isinstance", "(", "finded_keys", ",", "list", ")", "and", "len", "(", "finded_keys", ")", "!=", "0", ":", "\n", "            ", "finded_keys_set", "=", "set", ".", "union", "(", "*", "finded_keys", ")", "\n", "\n", "", "for", "page_name", "in", "finded_keys_set", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm'", ")", ")", "\n", "\n", "# del keyword_processor", "\n", "\n", "# .2 Then we add more disambiguation titles.    # Comment out this to remove this function.", "\n", "", "finded_keys_disamb", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ")", "\n", "finded_keys_disamb_set", "=", "set", "(", ")", "\n", "if", "isinstance", "(", "finded_keys_disamb", ",", "list", ")", "and", "len", "(", "finded_keys_disamb", ")", "!=", "0", ":", "\n", "            ", "finded_keys_disamb_set", "=", "set", ".", "union", "(", "*", "finded_keys_disamb", ")", "\n", "\n", "", "for", "page_name", "in", "finded_keys_disamb_set", ":", "\n", "# There will be duplicate pages, then we just ignore.", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb'", ")", ")", "\n", "\n", "# del keyword_processor_disamb", "\n", "\n", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "hyperlinked_title", "=", "[", "]", "\n", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "potential_title", ".", "lower", "(", ")", "not", "in", "STOPWORDS", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "                    ", "hyperlinked_title", ".", "append", "(", "potential_title", ")", "\n", "# finded_keys_set.add(potential_title)", "\n", "\n", "# finded_keys_set = set.union(set(hyperlinked_title), finded_keys_set)", "\n", "", "", "", "for", "page_name", "in", "hyperlinked_title", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_v2_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v3": [[280, 384], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "dict", "flashtext.KeywordProcessor.extract_keywords", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_document_id", "finded_key.items", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.add_item", "kw.lower", "kw.lower", "flashtext.KeywordProcessor.get_keyword", "dict", "flashtext.KeywordProcessor.add_keyword", "finded_keys_list.append", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.add_item", "flatten_hyperlinks.extend", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hyperlinked_title.append", "hotpot_preliminary_doc_retri.filter_document_id", "potential_title.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v3", "(", ")", ":", "\n", "# 2019 - 03 - 27", "\n", "# We want to merge raw key word matching and disambiguration group.", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "# keyword_processor = KeywordProcessor(case_sensitive=True)", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "False", ")", "\n", "# The structure for keyword_processor is {keyword: str: dict{kw: str: method: str} }", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", ":", "'kwm'", "}", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_dict", ":", "Dict", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "disamb_kw", "not", "in", "existing_dict", ":", "\n", "                        ", "existing_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "\n", "                ", "new_dict", "=", "dict", "(", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "new_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "new_dict", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "finded_keys", ":", "List", "[", "Dict", "[", "str", ":", "str", "]", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", "=", "[", "]", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "\n", "for", "finded_key", "in", "finded_keys", ":", "\n", "            ", "for", "title", ",", "method", "in", "finded_key", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "title", ",", "method", ")", ")", "\n", "\n", "# .1 We first find the raw matching.", "\n", "", "", "for", "title", ",", "method", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "title", ",", "method", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm_disamb'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "\n", "# .2 Then we add more disambiguation titles.", "\n", "# Since we merge the two dictionary, we don't need to processing again.", "\n", "# Comment out this to remove this function.", "\n", "# finded_keys_disamb = keyword_processor_disamb.extract_keywords(question)", "\n", "# finded_keys_disamb_set = set()", "\n", "# if isinstance(finded_keys_disamb, list) and len(finded_keys_disamb) != 0:", "\n", "#     finded_keys_disamb_set = set.union(*finded_keys_disamb)", "\n", "#", "\n", "# for page_name in finded_keys_disamb_set:", "\n", "#     # There will be duplicate pages, then we just ignore.", "\n", "#     retrieved_set.add_item(retrieval_utils.RetrievedItem(page_name, 'kwm_disamb'))", "\n", "\n", "# del keyword_processor_disamb", "\n", "\n", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "hyperlinked_title", "=", "[", "]", "\n", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "potential_title", ".", "lower", "(", ")", "not", "in", "STOPWORDS", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "                    ", "hyperlinked_title", ".", "append", "(", "potential_title", ")", "\n", "# finded_keys_set.add(potential_title)", "\n", "\n", "# finded_keys_set = set.union(set(hyperlinked_title), finded_keys_set)", "\n", "", "", "", "for", "page_name", "in", "hyperlinked_title", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_uncased_v3_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v4": [[386, 493], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "dict", "flashtext.KeywordProcessor.extract_keywords", "flashtext.KeywordProcessor.extract_keywords", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_document_id", "range", "finded_key.items", "range", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.add_item", "kw.lower", "kw.lower", "flashtext.KeywordProcessor.get_keyword", "dict", "flashtext.KeywordProcessor.add_keyword", "finded_keys_list.append", "finded_key.items", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.add_item", "flatten_hyperlinks.extend", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "finded_keys_list.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hyperlinked_title.append", "hotpot_preliminary_doc_retri.filter_document_id", "potential_title.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v4", "(", ")", ":", "\n", "# 2019-03-28", "\n", "# We first do raw key word matching and then disambiguation and", "\n", "# remove the overlapping span of kw and disambiguating.", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", ":", "'kwm'", "}", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "kw", ".", "lower", "(", ")", "in", "STOPWORDS", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_dict", ":", "Dict", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "disamb_kw", "not", "in", "existing_dict", ":", "\n", "                        ", "existing_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "                ", "new_dict", "=", "dict", "(", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "new_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "new_dict", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "# 1. First retrieve raw key word matching.", "\n", "finded_keys_kwm", ":", "List", "[", "Dict", "[", "str", ":", "str", "]", ",", "int", ",", "int", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ",", "span_info", "=", "True", ")", "\n", "finded_keys_kwm_disamb", ":", "List", "[", "Dict", "[", "str", ":", "str", "]", ",", "int", ",", "int", "]", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ",", "\n", "span_info", "=", "True", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "\n", "whole_span", "=", "[", "False", "for", "_", "in", "question", "]", "\n", "\n", "for", "finded_key", ",", "start", ",", "end", "in", "finded_keys_kwm", ":", "\n", "            ", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                ", "whole_span", "[", "i", "]", "=", "True", "# We mark the span as extracted by key word matching.", "\n", "", "for", "title", ",", "method", "in", "finded_key", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "title", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "", "for", "finded_key", ",", "start", ",", "end", "in", "finded_keys_kwm_disamb", ":", "\n", "            ", "valid", "=", "True", "\n", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                ", "if", "whole_span", "[", "i", "]", ":", "\n", "                    ", "valid", "=", "False", "# If we want a span overlapping, we just ignore this item.", "\n", "break", "\n", "", "", "if", "valid", ":", "\n", "                ", "for", "title", ",", "method", "in", "finded_key", ".", "items", "(", ")", ":", "\n", "                    ", "finded_keys_list", ".", "append", "(", "(", "title", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "# .1 We first find the raw matching.", "\n", "", "", "", "for", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm_disamb'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "\n", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "hyperlinked_title", "=", "[", "]", "\n", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "potential_title", ".", "lower", "(", ")", "not", "in", "STOPWORDS", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "                    ", "hyperlinked_title", ".", "append", "(", "potential_title", ")", "\n", "# finded_keys_set.add(potential_title)", "\n", "\n", "# finded_keys_set = set.union(set(hyperlinked_title), finded_keys_set)", "\n", "", "", "", "for", "page_name", "in", "hyperlinked_title", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_v4_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v5": [[495, 605], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "len", "dict", "dict", "flashtext.KeywordProcessor.extract_keywords", "flashtext.KeywordProcessor.extract_keywords", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "range", "finded_key.items", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.add_item", "flashtext.KeywordProcessor.get_keyword", "dict", "flashtext.KeywordProcessor.add_keyword", "all_finded_span.append", "finded_keys_list.append", "finded_key.items", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.add_item", "flatten_hyperlinks.extend", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "finded_keys_list.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hyperlinked_title.append", "hotpot_preliminary_doc_retri.filter_document_id", "potential_title.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v5", "(", ")", ":", "\n", "# 2019-03-28", "\n", "# We first do raw key word matching and then disambiguation and", "\n", "# remove the overlapping span of kw and disambiguating.", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "# if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "{", "kw", ":", "'kwm'", "}", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "# if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_dict", ":", "Dict", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "disamb_kw", "not", "in", "existing_dict", ":", "\n", "                        ", "existing_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "                ", "new_dict", "=", "dict", "(", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "new_dict", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "new_dict", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "# 1. First retrieve raw key word matching.", "\n", "finded_keys_kwm", ":", "List", "[", "Dict", "[", "str", ":", "str", "]", ",", "int", ",", "int", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ",", "span_info", "=", "True", ")", "\n", "finded_keys_kwm_disamb", ":", "List", "[", "Dict", "[", "str", ":", "str", "]", ",", "int", ",", "int", "]", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ",", "\n", "span_info", "=", "True", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "\n", "all_finded_span", "=", "[", "]", "\n", "\n", "for", "finded_key", ",", "start", ",", "end", "in", "finded_keys_kwm", ":", "\n", "            ", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                ", "all_finded_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "for", "title", ",", "method", "in", "finded_key", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "title", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "", "for", "finded_key", ",", "start", ",", "end", "in", "finded_keys_kwm_disamb", ":", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span", ":", "\n", "                ", "if", "e_start", "<=", "start", "and", "e_end", ">=", "end", ":", "\n", "                    ", "not_valid", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "not_valid", ":", "\n", "                ", "for", "title", ",", "method", "in", "finded_key", ".", "items", "(", ")", ":", "\n", "                    ", "finded_keys_list", ".", "append", "(", "(", "title", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "# .1 We first find the raw matching.", "\n", "", "", "", "for", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "            ", "if", "method", "==", "'kwm_disamb'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "\n", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "hyperlinked_title", "=", "[", "]", "\n", "for", "keyword", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "potential_title", ".", "lower", "(", ")", "not", "in", "STOPWORDS", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "                    ", "hyperlinked_title", ".", "append", "(", "potential_title", ")", "\n", "\n", "", "", "", "for", "page_name", "in", "hyperlinked_title", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "page_name", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_withiout_hyperlinked_v5_file.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v6": [[607, 782], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "dict", "build_rindex.build_rvindex.load_from_file", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "collections.namedtuple", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "utils.common.load_json", "evaluation.ext_hotpot_eval.eval", "len", "dict", "dict", "build_rindex.rvindex_scoring.get_query_ngrams", "flashtext.KeywordProcessor.extract_keywords", "flashtext.KeywordProcessor.extract_keywords", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "retrieval_utils.RetrievedSet.to_id_list", "list", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "collections.namedtuple.", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "range", "finded_matched_obj.matched_keywords_info.items", "retrieval_utils.RetrievedSet.sort_and_filter", "flashtext.KeywordProcessor.get_keyword", "collections.namedtuple.", "flashtext.KeywordProcessor.add_keyword", "all_finded_span.append", "all_finded_span_2.append", "finded_keys_list.append", "finded_matched_obj.matched_keywords_info.items", "retrieval_utils.RetrievedSet.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieval_utils.RetrievedSet.score_item", "set.add", "retrieval_utils.RetrievedSet.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieval_utils.RetrievedSet.score_item", "set.add", "hotpot_preliminary_doc_retri.filter_document_id", "hotpot_preliminary_doc_retri.filter_document_id", "finded_keys_list.append", "all_finded_span_2.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "toy_init_results_v6", "(", ")", ":", "\n", "# 2019-03-28", "\n", "# We first do raw key word matching and then disambiguation and", "\n", "# remove the overlapping span of kw and disambiguating.", "\n", "    ", "match_filtering_k", "=", "3", "\n", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "# Load tf-idf_score function:", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "_MatchedObject", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"MatchedObject\"", ",", "[", "\"matched_key_word\"", ",", "\"matched_keywords_info\"", "]", ")", "\n", "# Extracted key word is the key word in the database, matched word is the word in the input question.", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "# if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "{", "kw", ":", "'kwm'", "}", ")", "\n", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "# if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_matched_obj", ":", "_MatchedObject", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "disamb_kw", "not", "in", "existing_matched_obj", ".", "matched_keywords_info", ":", "\n", "                        ", "existing_matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "# new_dict = dict()", "\n", "                ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "dict", "(", ")", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "# new_dict[disamb_kw] = 'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "query_terms", "=", "get_query_ngrams", "(", "question", ")", "\n", "valid_query_terms", "=", "[", "term", "for", "term", "in", "query_terms", "if", "term", "in", "g_score_dict", "]", "\n", "\n", "# 1. First retrieve raw key word matching.", "\n", "finded_keys_kwm", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ",", "span_info", "=", "True", ")", "\n", "finded_keys_kwm_disamb", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ",", "\n", "span_info", "=", "True", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "str", ",", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "\n", "all_finded_span", "=", "[", "]", "\n", "all_finded_span_2", "=", "[", "]", "\n", "\n", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm", ":", "\n", "            ", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                ", "all_finded_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "# for matched_obj in finded_matched_obj.:", "\n", "", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm_disamb", ":", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span", ":", "\n", "                ", "if", "e_start", "<=", "start", "and", "e_end", ">=", "end", ":", "\n", "                    ", "not_valid", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "not_valid", ":", "\n", "                ", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "                    ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "", "", "all_raw_matched_word", "=", "set", "(", ")", "\n", "# .1 We first find the raw matching.", "\n", "\n", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "                ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                    ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "                ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "                ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                    ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "                ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm_disamb'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "", "", "for", "matched_word", "in", "all_raw_matched_word", ":", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "matched_word", ",", "top_k", "=", "match_filtering_k", ")", "\n", "\n", "# We don't worry about the hyperlink so far.", "\n", "\n", "# finded_keys_set = set(", "\n", "#     retrieved_set.to_id_list())  # for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# # .3 We then add some hyperlinked title", "\n", "# db_cursor = wiki_db_tool.get_cursor(config.WHOLE_WIKI_DB)", "\n", "# hyperlinked_title = []", "\n", "# for keyword in finded_keys_set:", "\n", "#     flatten_hyperlinks = []", "\n", "#     hyperlinks = wiki_db_tool.get_first_paragraph_hyperlinks(db_cursor, keyword)", "\n", "#     for hls in hyperlinks:", "\n", "#         flatten_hyperlinks.extend(hls)", "\n", "#", "\n", "#     for hl in flatten_hyperlinks:", "\n", "#         potential_title = hl.href", "\n", "#         if potential_title in ner_set and potential_title.lower() not in STOPWORDS or not filter_document_id(", "\n", "#                 potential_title):", "\n", "#             hyperlinked_title.append(potential_title)", "\n", "#", "\n", "# for page_name in hyperlinked_title:", "\n", "#     retrieved_set.add_item(retrieval_utils.RetrievedItem(page_name, 'kwm_disamb_hlinked'))", "\n", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "", "retrieved_list", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "\n", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_list", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_withiout_hyperlinked_v6_file_debug_4_redo_0.json\"", ")", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict", ",", "dev_fullwiki_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v7_pre": [[784, 1022], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "utils.common.load_jsonl", "dict", "dict", "build_rindex.build_rvindex.load_from_file", "utils.common.load_json", "tqdm.tqdm", "utils.common.save_json", "utils.common.load_json", "evaluation.ext_hotpot_eval.eval", "len", "build_rindex.rvindex_scoring.get_query_ngrams", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieved_set.to_id_list", "sorted", "retrieved_set.add_item", "retrieved_set.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieved_set.sort_and_filter", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "flatten_hyperlinks.extend", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieved_set.add_item", "retrieved_set.score_item", "hotpot_preliminary_doc_retri.filter_document_id", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_preliminary_doc_retri.filter_word"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word"], ["", "def", "toy_init_results_v7_pre", "(", ")", ":", "\n", "# 2019-03-28", "\n", "# We first do raw key word matching and then disambiguation and", "\n", "# remove the overlapping span of kw and disambiguating.", "\n", "\n", "# match_filtering_k = 3", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "term_retrieval_top_k", "=", "5", "\n", "multihop_retrieval_top_k", "=", "None", "\n", "#", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "terms_based_results", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "\n", "terms_based_results_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "terms_based_results", ":", "\n", "        ", "terms_based_results_dict", "[", "item", "[", "'qid'", "]", "]", "=", "item", "\n", "# print(item)", "\n", "\n", "# Load tf-idf_score function:", "\n", "", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "# keyword_processor = KeywordProcessor(case_sensitive=True)", "\n", "# keyword_processor_disamb = KeywordProcessor(case_sensitive=True)", "\n", "\n", "# _MatchedObject = collections.namedtuple(  # pylint: disable=invalid-name", "\n", "#     \"MatchedObject\", [\"matched_key_word\", \"matched_keywords_info\"])", "\n", "# Extracted key word is the key word in the database, matched word is the word in the input question.", "\n", "\n", "# print(\"Build Processor\")", "\n", "# for kw in tqdm(ner_set):", "\n", "#     if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "# if filter_word(kw) or filter_document_id(kw):", "\n", "#     continue  # if the keyword is filtered by above function or is stopwords", "\n", "# else:", "\n", "#     matched_obj = _MatchedObject(matched_key_word=kw, matched_keywords_info={kw: 'kwm'})", "\n", "#     keyword_processor.add_keyword(kw, matched_obj)", "\n", "\n", "# for kw in wiki_util.title_entities_set.disambiguation_group:", "\n", "#     # if kw.lower() in STOPWORDS or filter_document_id(kw):", "\n", "#     if filter_word(kw) or filter_document_id(kw):", "\n", "#         continue  # if the keyword is filtered by above function or is stopwords", "\n", "#     else:", "\n", "#         if kw in keyword_processor:", "\n", "#             # if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "#             existing_matched_obj: _MatchedObject = keyword_processor.get_keyword(kw)", "\n", "#             for disamb_kw in wiki_util.title_entities_set.disambiguation_group[kw]:", "\n", "#                 if filter_document_id(disamb_kw):", "\n", "#                     continue", "\n", "#                 if disamb_kw not in existing_matched_obj.matched_keywords_info:", "\n", "#                     existing_matched_obj.matched_keywords_info[disamb_kw] = 'kwm_disamb'", "\n", "#         else:   # If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "#             # new_dict = dict()", "\n", "#             matched_obj = _MatchedObject(matched_key_word=kw, matched_keywords_info=dict())", "\n", "#             for disamb_kw in wiki_util.title_entities_set.disambiguation_group[kw]:", "\n", "#                 if filter_document_id(disamb_kw):", "\n", "#                     continue", "\n", "#                 matched_obj.matched_keywords_info[disamb_kw] = 'kwm_disamb'", "\n", "#                 # new_dict[disamb_kw] = 'kwm_disamb'", "\n", "#             keyword_processor_disamb.add_keyword(kw, matched_obj)", "\n", "#", "\n", "# doc_pred_dict = {'sp_doc': dict(), 'raw_retrieval_set': dict()}", "\n", "\n", "# Load some preobtained results.", "\n", "doc_pred_dict", "=", "common", ".", "load_json", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/doc_retrieval_debug_v6/doc_raw_matching_with_disamb_withiout_hyperlinked_v6_file_debug_4.json\"", ")", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "retrieved_set", "=", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "\n", "# print(type(retrieved_set))", "\n", "\n", "query_terms", "=", "get_query_ngrams", "(", "question", ")", "\n", "valid_query_terms", "=", "[", "term", "for", "term", "in", "query_terms", "if", "term", "in", "g_score_dict", "]", "\n", "\n", "new_sent_from_tf_idf", "=", "[", "]", "\n", "for", "score", ",", "title", "in", "sorted", "(", "\n", "terms_based_results_dict", "[", "qid", "]", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "term_retrieval_top_k", "]", ":", "\n", "# doc_pred_dict['sp_doc'][qid].append(title)", "\n", "            ", "retrieved_set", ".", "add_item", "(", "RetrievedItem", "(", "title", ",", "'tf-idf'", ")", ")", "\n", "\n", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "\n", "# hyperlinked_title = []", "\n", "# keyword_group = []", "\n", "\n", "for", "keyword_group", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword_group", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "# if potential_title in ner_set and potential_title.lower() not in STOPWORDS or not filter_document_id(", "\n", "if", "potential_title", "in", "ner_set", "and", "not", "filter_word", "(", "potential_title", ")", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "# hyperlinked_title.append(potential_title)", "\n", "\n", "# if not filter_document_id(potential_title):", "\n", "                    ", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "potential_title", ",", "g_score_dict", ")", "\n", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "potential_title", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "retrieved_set", ".", "score_item", "(", "potential_title", ",", "score", ",", "namespace", "=", "keyword_group", "+", "'-2-hop'", ")", "\n", "#                                         g_score_dict)   # A function to compute between title and query", "\n", "\n", "", "", "", "for", "keyword_group", "in", "finded_keys_set", ":", "# Group ordering and filtering", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "keyword_group", "+", "'-2-hop'", ",", "top_k", "=", "multihop_retrieval_top_k", ")", "\n", "\n", "# for page_name in hyperlinked_title:", "\n", "#     retrieved_set.add_item(retrieval_utils.RetrievedItem(page_name, 'kwm_disamb_hlinked'))", "\n", "\n", "", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "#", "\n", "# for item in tqdm(dev_fullwiki_list):", "\n", "#     question = item['question']", "\n", "#     qid = item['_id']", "\n", "#", "\n", "#     query_terms = get_query_ngrams(question)", "\n", "#     valid_query_terms = [term for term in query_terms if term in g_score_dict]", "\n", "#", "\n", "#     # 1. First retrieve raw key word matching.", "\n", "#     finded_keys_kwm: List[_MatchedObject, int, int] = keyword_processor.extract_keywords(question, span_info=True)", "\n", "#     finded_keys_kwm_disamb: List[_MatchedObject, int, int] = keyword_processor_disamb.extract_keywords(question,", "\n", "#                                                                                                        span_info=True)", "\n", "#     finded_keys_list: List[Tuple[str, str, str, int, int]] = []", "\n", "#     retrieved_set = retrieval_utils.RetrievedSet()", "\n", "#", "\n", "#     all_finded_span = []", "\n", "#     all_finded_span_2 = []", "\n", "#", "\n", "#     for finded_matched_obj, start, end in finded_keys_kwm:", "\n", "#         for i in range(start, end):", "\n", "#             all_finded_span.append((start, end))", "\n", "#             all_finded_span_2.append((start, end))", "\n", "#", "\n", "#         # for matched_obj in finded_matched_obj.:", "\n", "#         matched_words = finded_matched_obj.matched_key_word", "\n", "#         for extracted_keyword, method in finded_matched_obj.matched_keywords_info.items():", "\n", "#             finded_keys_list.append((matched_words, extracted_keyword, method, start, end))", "\n", "#", "\n", "#     for finded_matched_obj, start, end in finded_keys_kwm_disamb:", "\n", "#         not_valid = False", "\n", "#         for e_start, e_end in all_finded_span:", "\n", "#             if e_start <= start and e_end >= end:", "\n", "#                 not_valid = True", "\n", "#                 break", "\n", "#", "\n", "#         if not not_valid:", "\n", "#             matched_words = finded_matched_obj.matched_key_word", "\n", "#             for extracted_keyword, method in finded_matched_obj.matched_keywords_info.items():", "\n", "#                 finded_keys_list.append((matched_words, extracted_keyword, method, start, end))", "\n", "#                 all_finded_span_2.append((start, end))", "\n", "#", "\n", "#     all_raw_matched_word = set()", "\n", "#     # .1 We first find the raw matching.", "\n", "#", "\n", "#     for matched_word, title, method, start, end in finded_keys_list:", "\n", "#         # add after debug_2", "\n", "#         not_valid = False", "\n", "#         for e_start, e_end in all_finded_span_2:", "\n", "#             if (e_start < start and e_end >= end) or (e_start <= start and e_end > end):", "\n", "#                 not_valid = True    # Skip this match bc this match is already contained in some other match.", "\n", "#                 break", "\n", "#", "\n", "#         if not_valid:", "\n", "#             continue", "\n", "#         # add finished", "\n", "#", "\n", "#         if method == 'kwm':", "\n", "#             retrieved_set.add_item(retrieval_utils.RetrievedItem(title, 'kwm'))", "\n", "#             score = get_query_doc_score(valid_query_terms, title,", "\n", "#                                         g_score_dict)   # A function to compute between title and query", "\n", "#             retrieved_set.score_item(title, score, namespace=matched_word)", "\n", "#             all_raw_matched_word.add(matched_word)", "\n", "#", "\n", "#     # .2 Then, we find the raw matching.", "\n", "#     for matched_word, title, method, start, end in finded_keys_list:", "\n", "#         # add after debug_2", "\n", "#         not_valid = False", "\n", "#         for e_start, e_end in all_finded_span_2:", "\n", "#             if (e_start < start and e_end >= end) or (e_start <= start and e_end > end):", "\n", "#                 not_valid = True    # Skip this match bc this match is already contained in some other match.", "\n", "#                 break", "\n", "#", "\n", "#         if not_valid:", "\n", "#             continue", "\n", "#         # add finished", "\n", "#", "\n", "#         if method == 'kwm_disamb':", "\n", "#             retrieved_set.add_item(retrieval_utils.RetrievedItem(title, 'kwm_disamb'))", "\n", "#             score = get_query_doc_score(valid_query_terms, title,", "\n", "#                                         g_score_dict)  # A function to compute between title and query", "\n", "#             retrieved_set.score_item(title, score, namespace=matched_word)", "\n", "#             all_raw_matched_word.add(matched_word)", "\n", "#", "\n", "#     for matched_word in all_raw_matched_word:", "\n", "#         retrieved_set.sort_and_filter(matched_word, top_k=match_filtering_k)", "\n", "\n", "# We don't worry about the hyperlink so far.", "\n", "\n", "# finded_keys_set = set(", "\n", "#     retrieved_set.to_id_list())  # for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# # .3 We then add some hyperlinked title", "\n", "# db_cursor = wiki_db_tool.get_cursor(config.WHOLE_WIKI_DB)", "\n", "# hyperlinked_title = []", "\n", "# for keyword in finded_keys_set:", "\n", "#     flatten_hyperlinks = []", "\n", "#     hyperlinks = wiki_db_tool.get_first_paragraph_hyperlinks(db_cursor, keyword)", "\n", "#     for hls in hyperlinks:", "\n", "#         flatten_hyperlinks.extend(hls)", "\n", "#", "\n", "#     for hl in flatten_hyperlinks:", "\n", "#         potential_title = hl.href", "\n", "#         if potential_title in ner_set and potential_title.lower() not in STOPWORDS or not filter_document_id(", "\n", "#                 potential_title):", "\n", "#             hyperlinked_title.append(potential_title)", "\n", "#", "\n", "# for page_name in hyperlinked_title:", "\n", "#     retrieved_set.add_item(retrieval_utils.RetrievedItem(page_name, 'kwm_disamb_hlinked'))", "\n", "\n", "# Addons cut retrieved document to contain only two", "\n", "# finded_keys_set = sorted(list(finded_keys_set), key=lambda x: len(x), reverse=True)", "\n", "# top_n = 2", "\n", "# finded_keys_set = finded_keys_set[:top_n]", "\n", "# retrieved_list = retrieved_set.to_id_list()", "\n", "#", "\n", "# doc_pred_dict['sp_doc'][qid] = list(retrieved_list)", "\n", "# doc_pred_dict['raw_retrieval_set'][qid] = retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_v7_file_debug_top_none.json\"", ")", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict", ",", "dev_fullwiki_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_results_v7": [[1024, 1226], ["wiki_util.title_entities_set.get_title_entity_set", "utils.common.load_json", "print", "utils.common.load_jsonl", "dict", "dict", "build_rindex.build_rvindex.load_from_file", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "collections.namedtuple", "print", "tqdm.tqdm", "tqdm.tqdm", "utils.common.save_json", "utils.common.save_json", "utils.common.load_json", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "len", "dict", "dict", "dict", "dict", "build_rindex.rvindex_scoring.get_query_ngrams", "flashtext.KeywordProcessor.extract_keywords", "flashtext.KeywordProcessor.extract_keywords", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "set", "retrieval_utils.RetrievedSet.to_id_list", "set", "wiki_util.wiki_db_tool.get_cursor", "retrieval_utils.RetrievedSet.to_id_list", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "collections.namedtuple.", "flashtext.KeywordProcessor.add_keyword", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "range", "finded_matched_obj.matched_keywords_info.items", "retrieval_utils.RetrievedSet.sort_and_filter", "sorted", "retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "retrieval_utils.RetrievedSet.sort_and_filter", "flashtext.KeywordProcessor.get_keyword", "collections.namedtuple.", "flashtext.KeywordProcessor.add_keyword", "all_finded_span.append", "all_finded_span_2.append", "finded_keys_list.append", "finded_matched_obj.matched_keywords_info.items", "retrieval_utils.RetrievedSet.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieval_utils.RetrievedSet.score_item", "set.add", "retrieval_utils.RetrievedSet.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieval_utils.RetrievedSet.score_item", "set.add", "retrieval_utils.RetrievedSet.add_item", "flatten_hyperlinks.extend", "hotpot_preliminary_doc_retri.filter_document_id", "hotpot_preliminary_doc_retri.filter_document_id", "finded_keys_list.append", "all_finded_span_2.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_preliminary_doc_retri.filter_word", "hotpot_preliminary_doc_retri.filter_document_id", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieval_utils.RetrievedSet.add_item", "retrieval_utils.RetrievedSet.score_item", "dict", "hotpot_preliminary_doc_retri.filter_document_id", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_preliminary_doc_retri.filter_word"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word"], ["", "def", "toy_init_results_v7", "(", ")", ":", "\n", "# 2019-04-05", "\n", "# The complete v7 version of retrieval", "\n", "# We first do raw key word matching and then disambiguation and", "\n", "# remove the overlapping span of kw and disambiguating.", "\n", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "match_filtering_k", "=", "3", "\n", "term_retrieval_top_k", "=", "5", "\n", "multihop_retrieval_top_k", "=", "None", "\n", "\n", "#", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "print", "(", "len", "(", "dev_fullwiki_list", ")", ")", "\n", "\n", "# We load term-based results", "\n", "terms_based_results", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "\n", "terms_based_results_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "terms_based_results", ":", "\n", "        ", "terms_based_results_dict", "[", "item", "[", "'qid'", "]", "]", "=", "item", "\n", "\n", "# Load tf-idf_score function:", "\n", "", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "_MatchedObject", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"MatchedObject\"", ",", "[", "\"matched_key_word\"", ",", "\"matched_keywords_info\"", "]", ")", "\n", "# Extracted key word is the key word in the database, matched word is the word in the input question.", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "# matched_key_word is the original matched span. we need to save it for group ordering.", "\n", "            ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "{", "kw", ":", "'kwm'", "}", ")", "\n", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "#", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_matched_obj", ":", "_MatchedObject", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "disamb_kw", "not", "in", "existing_matched_obj", ".", "matched_keywords_info", ":", "\n", "                        ", "existing_matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "# new_dict = dict()", "\n", "                ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "dict", "(", ")", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "# new_dict[disamb_kw] = 'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "doc_pred_dict_p1", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "tqdm", "(", "dev_fullwiki_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "query_terms", "=", "get_query_ngrams", "(", "question", ")", "\n", "valid_query_terms", "=", "[", "term", "for", "term", "in", "query_terms", "if", "term", "in", "g_score_dict", "]", "\n", "\n", "# 1. First retrieve raw key word matching.", "\n", "finded_keys_kwm", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ",", "span_info", "=", "True", ")", "\n", "finded_keys_kwm_disamb", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ",", "\n", "span_info", "=", "True", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "str", ",", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "retrieved_set", "=", "retrieval_utils", ".", "RetrievedSet", "(", ")", "\n", "\n", "all_finded_span", "=", "[", "]", "\n", "all_finded_span_2", "=", "[", "]", "\n", "\n", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm", ":", "\n", "            ", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "                ", "all_finded_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "# for matched_obj in finded_matched_obj.:", "\n", "", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm_disamb", ":", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span", ":", "\n", "                ", "if", "e_start", "<=", "start", "and", "e_end", ">=", "end", ":", "\n", "                    ", "not_valid", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "not_valid", ":", "\n", "                ", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "                    ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "", "", "all_raw_matched_word", "=", "set", "(", ")", "\n", "# .1 We first find the raw matching.", "\n", "\n", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "                ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                    ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "                ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "            ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "                ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                    ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "                ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm_disamb'", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "", "", "for", "matched_word", "in", "all_raw_matched_word", ":", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "matched_word", ",", "top_k", "=", "match_filtering_k", ")", "\n", "\n", "", "doc_pred_dict_p1", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "doc_pred_dict_p1", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "# Then we add term-based matching results", "\n", "added_count", "=", "0", "\n", "for", "score", ",", "title", "in", "sorted", "(", "\n", "terms_based_results_dict", "[", "qid", "]", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "term_retrieval_top_k", "+", "3", "]", ":", "\n", "            ", "if", "not", "filter_word", "(", "title", ")", "and", "not", "filter_document_id", "(", "title", ")", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "RetrievedItem", "(", "title", ",", "'tf-idf'", ")", ")", "\n", "added_count", "+=", "1", "\n", "if", "term_retrieval_top_k", "is", "not", "None", "and", "added_count", ">=", "term_retrieval_top_k", ":", "\n", "                    ", "break", "\n", "\n", "# Add hyperlinked pages:", "\n", "", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "\n", "for", "keyword_group", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword_group", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "not", "filter_word", "(", "potential_title", ")", "or", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "\n", "# hyperlinked_title.append(potential_title)", "\n", "\n", "# if not filter_document_id(potential_title):", "\n", "                    ", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "potential_title", ",", "g_score_dict", ")", "\n", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "potential_title", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "retrieved_set", ".", "score_item", "(", "potential_title", ",", "score", ",", "namespace", "=", "keyword_group", "+", "'-2-hop'", ")", "\n", "\n", "", "", "", "for", "keyword_group", "in", "finded_keys_set", ":", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "keyword_group", "+", "'-2-hop'", ",", "top_k", "=", "multihop_retrieval_top_k", ")", "\n", "\n", "", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "common", ".", "save_json", "(", "doc_pred_dict", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_v7_file_pipeline_top_none_redo_0.json\"", ")", "\n", "common", ".", "save_json", "(", "doc_pred_dict_p1", ",", "\"doc_raw_matching_with_disamb_with_hyperlinked_v7_file_pipeline_top_none_debug_p1.json\"", ")", "\n", "\n", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict", ",", "dev_fullwiki_list", ")", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict_p1", ",", "dev_fullwiki_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_preliminary_doc_retri.toy_init_pos_results_v7": [[1228, 1256], ["utils.common.load_json", "pred_dict[].keys", "utils.common.load_json", "evaluation.ext_hotpot_eval.eval", "dict", "dict", "set", "retrieved_set.retrieved_dict.values", "retrieved_set.to_id_list", "item.scores_dict.keys", "retrieved_set.sort_and_filter", "keyword_gourp_name.endswith", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "toy_init_pos_results_v7", "(", ")", ":", "\n", "    ", "hyperlinked_top_k", "=", "None", "\n", "\n", "pred_dict", "=", "common", ".", "load_json", "(", "config", ".", "PRO_ROOT", "/", "\"src/doc_retri/doc_raw_matching_with_disamb_with_hyperlinked_v7_file_debug_top_none.json\"", ")", "\n", "\n", "new_doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "key", "in", "pred_dict", "[", "'raw_retrieval_set'", "]", ".", "keys", "(", ")", ":", "\n", "        ", "qid", "=", "key", "\n", "retrieved_set", ":", "RetrievedSet", "=", "pred_dict", "[", "'raw_retrieval_set'", "]", "[", "key", "]", "\n", "\n", "hyperlinked_keyword_group", "=", "set", "(", ")", "\n", "for", "item", "in", "retrieved_set", ".", "retrieved_dict", ".", "values", "(", ")", ":", "\n", "            ", "for", "keyword_gourp_name", "in", "item", ".", "scores_dict", ".", "keys", "(", ")", ":", "\n", "                ", "if", "keyword_gourp_name", ".", "endswith", "(", "'-2-hop'", ")", ":", "\n", "                    ", "hyperlinked_keyword_group", ".", "add", "(", "keyword_gourp_name", ")", "\n", "# If the current scored one is 2-hop retrieval", "\n", "\n", "", "", "", "for", "keyword_group", "in", "hyperlinked_keyword_group", ":", "# The group already has '-2-hop' in the end", "\n", "# retrieved_set.sort_and_filter(keyword_group + '-2-hop', top_k=hyperlinked_top_k)", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "keyword_group", ",", "top_k", "=", "hyperlinked_top_k", ")", "\n", "\n", "", "new_doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "new_doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "# common.save_json(new_doc_pred_dict, \"doc_raw_matching_with_disamb_with_hyperlinked_v7_file_pipeline_top_none.json\")", "\n", "", "dev_fullwiki_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "ext_hotpot_eval", ".", "eval", "(", "new_doc_pred_dict", ",", "dev_fullwiki_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedItem.__init__": [[13, 19], ["utils.common.JsonableObj.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "item_id", ":", "str", ",", "retrieval_method", ":", "str", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "item_id", "=", "item_id", "# The item identifier.", "\n", "self", ".", "scores_dict", ":", "Dict", "[", "str", ",", "float", "]", "=", "dict", "(", ")", "\n", "# self.types: List = []             # The type of this item.", "\n", "self", ".", "method", "=", "retrieval_method", "# The top level method that retrieve this item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedItem.__repr__": [[20, 22], ["utils.common.json_dumps"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.json_dumps"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "common", ".", "json_dumps", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedItem.set_score": [[23, 25], ["None"], "methods", ["None"], ["", "def", "set_score", "(", "self", ",", "score", ":", "float", ",", "namespace", ":", "str", ")", ":", "\n", "        ", "self", ".", "scores_dict", "[", "namespace", "]", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedItem.__eq__": [[26, 30], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "self", ".", "item_id", "==", "other", ".", "item_id", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.__init__": [[48, 51], ["utils.common.JsonableObj.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "retrieved_dict", ":", "Dict", "[", "str", ",", "RetrievedItem", "]", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item": [[52, 58], ["retrieval_utils.RetrievedSet.retrieved_dict.keys"], "methods", ["None"], ["", "def", "add_item", "(", "self", ",", "retri_item", ":", "RetrievedItem", ")", ":", "\n", "        ", "if", "retri_item", ".", "item_id", "in", "self", ".", "retrieved_dict", ".", "keys", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "retrieved_dict", "[", "retri_item", ".", "item_id", "]", "=", "retri_item", "\n", "return", "retri_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item": [[59, 64], ["retrieval_utils.RetrievedSet.retrieved_dict.keys", "retrieval_utils.RetrievedSet.retrieved_dict[].set_score"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedItem.set_score"], ["", "", "def", "score_item", "(", "self", ",", "item_id", ":", "str", ",", "score", ":", "float", ",", "namespace", ":", "str", ")", ":", "\n", "        ", "if", "item_id", "not", "in", "self", ".", "retrieved_dict", ".", "keys", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "retrieved_dict", "[", "item_id", "]", ".", "set_score", "(", "score", ",", "namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.remove_item": [[65, 70], ["retrieval_utils.RetrievedSet.retrieved_dict.keys"], "methods", ["None"], ["", "", "def", "remove_item", "(", "self", ",", "item_id", ":", "str", ")", ":", "\n", "        ", "if", "item_id", "not", "in", "self", ".", "retrieved_dict", ".", "keys", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "del", "self", ".", "retrieved_dict", "[", "item_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter": [[71, 100], ["retrieval_utils.RetrievedSet.retrieved_dict.items", "sorted", "isinstance", "list", "all_item_meet_requirements.append", "filter", "retrieval_utils.get_sorted_top_k", "retrieval_utils.RetrievedSet.remove_item"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.get_sorted_top_k", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.remove_item"], ["", "", "def", "sort_and_filter", "(", "self", ",", "namespace", ":", "str", ",", "methods", ":", "Union", "[", "str", ",", "List", "[", "str", "]", "]", "=", "'_all_'", ",", "\n", "top_k", ":", "int", "=", "None", ",", "filter_value", ":", "float", "=", "None", ",", "strict_mode", "=", "False", ")", ":", "\n", "# Sort all the item by the score in the give namespace, with specific methods.", "\n", "        ", "if", "not", "isinstance", "(", "methods", ",", "list", ")", ":", "\n", "            ", "methods", "=", "[", "methods", "]", "\n", "\n", "# We only select those meet the method and contain the score namespace.", "\n", "", "all_item_meet_requirements", ":", "List", "[", "RetrievedItem", "]", "=", "[", "]", "\n", "for", "key", ",", "item", "in", "self", ".", "retrieved_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "item", ".", "method", "in", "methods", "or", "methods", "==", "[", "'_all_'", "]", "and", "namespace", "in", "item", ".", "scores_dict", ":", "# important.", "\n", "                ", "all_item_meet_requirements", ".", "append", "(", "item", ")", "\n", "\n", "", "", "sorted_item_list", "=", "sorted", "(", "all_item_meet_requirements", ",", "key", "=", "lambda", "x", ":", "(", "x", ".", "scores_dict", "[", "namespace", "]", ",", "x", ".", "item_id", ")", ",", "\n", "reverse", "=", "True", ")", "# sort, we use id as second key to eliminate randomness.", "\n", "if", "filter_value", "is", "not", "None", ":", "\n", "            ", "sorted_item_list", "=", "list", "(", "filter", "(", "lambda", "x", ":", "x", ".", "scores_dict", "[", "namespace", "]", ">=", "filter_value", ",", "sorted_item_list", ")", ")", "\n", "# filter by value", "\n", "", "if", "strict_mode", ":", "\n", "            ", "sorted_item_list", "=", "sorted_item_list", "[", ":", "top_k", "]", "if", "top_k", "is", "not", "None", "else", "sorted_item_list", "\n", "", "else", ":", "\n", "# Important update 2019-04-06 to keep the data consistancy after sorting, we just keep all the value with same score.", "\n", "            ", "sorted_item_list", "=", "get_sorted_top_k", "(", "sorted_item_list", ",", "top_k", ",", "key", "=", "lambda", "x", ":", "x", ".", "scores_dict", "[", "namespace", "]", ")", "if", "top_k", "is", "not", "None", "else", "sorted_item_list", "\n", "# filter by count", "\n", "\n", "", "for", "aitem", "in", "all_item_meet_requirements", ":", "\n", "            ", "if", "aitem", "in", "sorted_item_list", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "self", ".", "remove_item", "(", "aitem", ".", "item_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list": [[101, 103], ["list", "retrieval_utils.RetrievedSet.retrieved_dict.keys"], "methods", ["None"], ["", "", "", "def", "to_id_list", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "self", ".", "retrieved_dict", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.__len__": [[104, 106], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "retrieved_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.__repr__": [[107, 109], ["utils.common.json_dumps"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.json_dumps"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "common", ".", "json_dumps", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.get_sorted_top_k": [[32, 45], ["range", "len", "key", "len", "key", "init_list.append"], "function", ["None"], ["", "", "def", "get_sorted_top_k", "(", "d_l", ",", "top_k", ",", "key", "=", "None", ")", ":", "\n", "    ", "if", "top_k", ">=", "len", "(", "d_l", ")", ":", "\n", "        ", "return", "d_l", "[", ":", "top_k", "]", "\n", "", "value", "=", "d_l", "[", "top_k", "-", "1", "]", "if", "key", "is", "None", "else", "key", "(", "d_l", "[", "top_k", "-", "1", "]", ")", "\n", "init_list", "=", "d_l", "[", ":", "top_k", "]", "# First select top_k", "\n", "for", "i", "in", "range", "(", "top_k", ",", "len", "(", "d_l", ")", ")", ":", "\n", "        ", "cur_val", "=", "d_l", "[", "i", "]", "if", "key", "is", "None", "else", "key", "(", "d_l", "[", "i", "]", ")", "\n", "if", "cur_val", "==", "value", ":", "\n", "            ", "init_list", ".", "append", "(", "d_l", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "init_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.append_baseline_context": [[25, 35], ["list", "provided_title.append", "set.union", "set", "set"], "function", ["None"], ["def", "append_baseline_context", "(", "doc_results", ",", "baseline_data_list", ")", ":", "\n", "    ", "data_list", "=", "baseline_data_list", "\n", "for", "item", "in", "data_list", ":", "\n", "        ", "key", "=", "item", "[", "'_id'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "provided_title", "=", "[", "]", "\n", "for", "title", ",", "paragraph", "in", "contexts", ":", "\n", "            ", "provided_title", ".", "append", "(", "title", ")", "\n", "\n", "", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", "=", "list", "(", "set", ".", "union", "(", "set", "(", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", ")", ",", "set", "(", "provided_title", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_disamb_doc": [[37, 42], ["None"], "function", ["None"], ["", "", "def", "filter_disamb_doc", "(", "input_string", ")", ":", "\n", "    ", "if", "' (disambiguation)'", "in", "input_string", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.check_arabic": [[44, 53], ["re.findall", "len"], "function", ["None"], ["", "", "def", "check_arabic", "(", "input_string", ")", ":", "\n", "    ", "res", "=", "re", ".", "findall", "(", "\n", "r'[\\U00010E60-\\U00010E7F]|[\\U0001EE00-\\U0001EEFF]|[\\u0750-\\u077F]|[\\u08A0-\\u08FF]|[\\uFB50-\\uFDFF]|[\\uFE70-\\uFEFF]|[\\u0600-\\u06FF]'", ",", "\n", "input_string", ")", "\n", "\n", "if", "len", "(", "res", ")", "!=", "0", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id": [[55, 67], ["input_string.strip().replace", "re.search", "hotpot_doc_retri_v0.check_arabic", "input_string.strip", "hotpot_doc_retri_v0.filter_disamb_doc"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.check_arabic", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_disamb_doc"], ["", "", "def", "filter_document_id", "(", "input_string", ",", "remove_disambiguation_doc", "=", "True", ")", ":", "\n", "    ", "pid_words", "=", "input_string", ".", "strip", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "match", "=", "re", ".", "search", "(", "'[a-zA-Z]'", ",", "pid_words", ")", "\n", "if", "match", "is", "None", ":", "# filter id that contains no alphabets characters", "\n", "        ", "return", "True", "\n", "", "elif", "check_arabic", "(", "pid_words", ")", ":", "# remove id that contain arabic characters.", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "if", "remove_disambiguation_doc", ":", "\n", "            ", "if", "filter_disamb_doc", "(", "input_string", ")", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.normalize": [[90, 93], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_word": [[95, 103], ["hotpot_doc_retri_v0.normalize", "regex.match", "normalize.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "def", "filter_word", "(", "text", ")", ":", "\n", "    ", "\"\"\"Take out english stopwords, punctuation, and compound endings.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "if", "regex", ".", "match", "(", "r'^\\p{P}+$'", ",", "text", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "STOPWORDS", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.get_kw_matching_results": [[110, 192], ["keyword_processor.extract_keywords", "keyword_processor_disamb.extract_keywords", "set", "range", "finded_matched_obj.matched_keywords_info.items", "retrieved_set.sort_and_filter", "all_finded_span.append", "all_finded_span_2.append", "finded_keys_list.append", "finded_matched_obj.matched_keywords_info.items", "retrieved_set.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieved_set.score_item", "set.add", "retrieved_set.add_item", "build_rindex.rvindex_scoring.get_query_doc_score", "retrieved_set.score_item", "set.add", "finded_keys_list.append", "all_finded_span_2.append", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "hotpot_doc_retri.retrieval_utils.RetrievedItem"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["def", "get_kw_matching_results", "(", "question", ",", "valid_query_terms", ",", "retrieved_set", ",", "match_filtering_k", ",", "\n", "g_score_dict", ",", "keyword_processor", ",", "keyword_processor_disamb", ")", ":", "\n", "# 1. First retrieve raw key word matching.", "\n", "\n", "    ", "finded_keys_kwm", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor", ".", "extract_keywords", "(", "question", ",", "span_info", "=", "True", ")", "\n", "finded_keys_kwm_disamb", ":", "List", "[", "_MatchedObject", ",", "int", ",", "int", "]", "=", "keyword_processor_disamb", ".", "extract_keywords", "(", "question", ",", "\n", "span_info", "=", "True", ")", "\n", "finded_keys_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "str", ",", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "# retrieved_set = retrieval_utils.RetrievedSet()", "\n", "\n", "all_finded_span", "=", "[", "]", "\n", "all_finded_span_2", "=", "[", "]", "\n", "\n", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm", ":", "\n", "        ", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "            ", "all_finded_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "# for matched_obj in finded_matched_obj.:", "\n", "", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "            ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "", "for", "finded_matched_obj", ",", "start", ",", "end", "in", "finded_keys_kwm_disamb", ":", "\n", "        ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span", ":", "\n", "            ", "if", "e_start", "<=", "start", "and", "e_end", ">=", "end", ":", "\n", "                ", "not_valid", "=", "True", "\n", "break", "\n", "\n", "", "", "if", "not", "not_valid", ":", "\n", "            ", "matched_words", "=", "finded_matched_obj", ".", "matched_key_word", "\n", "for", "extracted_keyword", ",", "method", "in", "finded_matched_obj", ".", "matched_keywords_info", ".", "items", "(", ")", ":", "\n", "                ", "finded_keys_list", ".", "append", "(", "(", "matched_words", ",", "extracted_keyword", ",", "method", ",", "start", ",", "end", ")", ")", "\n", "all_finded_span_2", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "", "", "all_raw_matched_word", "=", "set", "(", ")", "\n", "# .1 We first find the raw matching.", "\n", "\n", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "        ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "            ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "            ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm'", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "# .2 Then, we find the raw matching.", "\n", "", "", "for", "matched_word", ",", "title", ",", "method", ",", "start", ",", "end", "in", "finded_keys_list", ":", "\n", "# add after debug_2", "\n", "        ", "not_valid", "=", "False", "\n", "for", "e_start", ",", "e_end", "in", "all_finded_span_2", ":", "\n", "            ", "if", "(", "e_start", "<", "start", "and", "e_end", ">=", "end", ")", "or", "(", "e_start", "<=", "start", "and", "e_end", ">", "end", ")", ":", "\n", "                ", "not_valid", "=", "True", "# Skip this match bc this match is already contained in some other match.", "\n", "break", "\n", "\n", "", "", "if", "not_valid", ":", "\n", "            ", "continue", "\n", "# add finished", "\n", "\n", "", "if", "method", "==", "'kwm_disamb'", ":", "\n", "            ", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "title", ",", "'kwm_disamb'", ")", ")", "\n", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "title", ",", "\n", "g_score_dict", ")", "# A function to compute between title and query", "\n", "retrieved_set", ".", "score_item", "(", "title", ",", "score", ",", "namespace", "=", "matched_word", ")", "\n", "all_raw_matched_word", ".", "add", "(", "matched_word", ")", "\n", "\n", "", "", "for", "matched_word", "in", "all_raw_matched_word", ":", "\n", "        ", "retrieved_set", ".", "sort_and_filter", "(", "matched_word", ",", "top_k", "=", "match_filtering_k", ")", "\n", "\n", "", "return", "retrieved_set", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.init_results_v8": [[194, 311], ["wiki_util.title_entities_set.get_title_entity_set", "print", "print", "print", "dict", "flashtext.KeywordProcessor", "flashtext.KeywordProcessor", "print", "tqdm.tqdm", "tqdm.tqdm", "len", "dict", "dict", "build_rindex.rvindex_scoring.get_query_ngrams", "hotpot_doc_retri.retrieval_utils.RetrievedSet", "hotpot_doc_retri_v0.get_kw_matching_results", "set", "wiki_util.wiki_db_tool.get_cursor", "hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "evaluation.ext_hotpot_eval.eval", "hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri_v0.filter_document_id", "_MatchedObject", "flashtext.KeywordProcessor.add_keyword", "hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri_v0.filter_document_id", "sorted", "hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "flashtext.KeywordProcessor.get_keyword", "_MatchedObject", "flashtext.KeywordProcessor.add_keyword", "hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "flatten_hyperlinks.extend", "hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri.retrieval_utils.RetrievedItem", "build_rindex.rvindex_scoring.get_query_doc_score", "hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "dict", "hotpot_doc_retri_v0.filter_word", "hotpot_doc_retri_v0.filter_document_id", "hotpot_doc_retri.retrieval_utils.RetrievedItem"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.get_kw_matching_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.add_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.score_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.filter_document_id"], ["", "def", "init_results_v8", "(", "data_list", ",", "gt_data_list", ",", "\n", "terms_based_resutls", ",", "g_score_dict", ",", "\n", "match_filtering_k", "=", "3", ",", "\n", "term_retrieval_top_k", "=", "5", ",", "\n", "multihop_retrieval_top_k", "=", "None", ")", ":", "\n", "# 2019-04-06", "\n", "# The complete v7 version of retrieval", "\n", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "# dev_fullwiki_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "print", "(", "\"Total data length:\"", ")", "\n", "print", "(", "len", "(", "data_list", ")", ")", "\n", "\n", "# We load term-based results", "\n", "print", "(", "\"Load term-based results.\"", ")", "\n", "terms_based_results_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "terms_based_resutls", ":", "\n", "        ", "terms_based_results_dict", "[", "item", "[", "'qid'", "]", "]", "=", "item", "\n", "\n", "# Load tf-idf_score function:", "\n", "# g_score_dict = dict()", "\n", "# load_from_file(g_score_dict,", "\n", "#                config.PDATA_ROOT / \"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\")", "\n", "\n", "", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "keyword_processor_disamb", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "\n", "print", "(", "\"Build Processor\"", ")", "\n", "for", "kw", "in", "tqdm", "(", "ner_set", ")", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "# matched_key_word is the original matched span. we need to save it for group ordering.", "\n", "            ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "{", "kw", ":", "'kwm'", "}", ")", "\n", "keyword_processor", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "#", "\n", "", "", "for", "kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", ":", "\n", "        ", "if", "filter_word", "(", "kw", ")", "or", "filter_document_id", "(", "kw", ")", ":", "\n", "            ", "continue", "# if the keyword is filtered by above function or is stopwords", "\n", "", "else", ":", "\n", "            ", "if", "kw", "in", "keyword_processor", ":", "\n", "# if the kw existed in the kw_processor, we update its dict to add more disamb items", "\n", "                ", "existing_matched_obj", ":", "_MatchedObject", "=", "keyword_processor", ".", "get_keyword", "(", "kw", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "disamb_kw", "not", "in", "existing_matched_obj", ".", "matched_keywords_info", ":", "\n", "                        ", "existing_matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "", "", "", "else", ":", "# If not we add it to the keyword_processor_disamb, which is set to be lower priority", "\n", "# new_dict = dict()", "\n", "                ", "matched_obj", "=", "_MatchedObject", "(", "matched_key_word", "=", "kw", ",", "matched_keywords_info", "=", "dict", "(", ")", ")", "\n", "for", "disamb_kw", "in", "wiki_util", ".", "title_entities_set", ".", "disambiguation_group", "[", "kw", "]", ":", "\n", "                    ", "if", "filter_document_id", "(", "disamb_kw", ")", ":", "\n", "                        ", "continue", "\n", "", "matched_obj", ".", "matched_keywords_info", "[", "disamb_kw", "]", "=", "'kwm_disamb'", "\n", "# new_dict[disamb_kw] = 'kwm_disamb'", "\n", "", "keyword_processor_disamb", ".", "add_keyword", "(", "kw", ",", "matched_obj", ")", "\n", "\n", "", "", "", "doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "# doc_pred_dict_p1 = {'sp_doc': dict(), 'raw_retrieval_set': dict()}", "\n", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "question", "=", "item", "[", "'question'", "]", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "\n", "query_terms", "=", "get_query_ngrams", "(", "question", ")", "\n", "valid_query_terms", "=", "[", "term", "for", "term", "in", "query_terms", "if", "term", "in", "g_score_dict", "]", "\n", "\n", "retrieved_set", "=", "RetrievedSet", "(", ")", "\n", "\n", "# This method will add the keyword match results in-place to retrieved_set.", "\n", "get_kw_matching_results", "(", "question", ",", "valid_query_terms", ",", "retrieved_set", ",", "match_filtering_k", ",", "\n", "g_score_dict", ",", "keyword_processor", ",", "keyword_processor_disamb", ")", "\n", "\n", "# Then we add term-based matching results", "\n", "added_count", "=", "0", "\n", "for", "score", ",", "title", "in", "sorted", "(", "\n", "terms_based_results_dict", "[", "qid", "]", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "term_retrieval_top_k", "+", "3", "]", ":", "\n", "            ", "if", "not", "filter_word", "(", "title", ")", "and", "not", "filter_document_id", "(", "title", ")", ":", "\n", "                ", "retrieved_set", ".", "add_item", "(", "RetrievedItem", "(", "title", ",", "'tf-idf'", ")", ")", "\n", "added_count", "+=", "1", "\n", "if", "term_retrieval_top_k", "is", "not", "None", "and", "added_count", ">=", "term_retrieval_top_k", ":", "\n", "                    ", "break", "\n", "\n", "# Add hyperlinked pages:", "\n", "", "", "", "finded_keys_set", "=", "set", "(", "\n", "retrieved_set", ".", "to_id_list", "(", ")", ")", "# for finding hyperlinked pages we do for both keyword matching and disambiguration group.", "\n", "# .3 We then add some hyperlinked title", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_DB", ")", "\n", "\n", "for", "keyword_group", "in", "finded_keys_set", ":", "\n", "            ", "flatten_hyperlinks", "=", "[", "]", "\n", "hyperlinks", "=", "wiki_db_tool", ".", "get_first_paragraph_hyperlinks", "(", "db_cursor", ",", "keyword_group", ")", "\n", "for", "hls", "in", "hyperlinks", ":", "\n", "                ", "flatten_hyperlinks", ".", "extend", "(", "hls", ")", "\n", "\n", "", "for", "hl", "in", "flatten_hyperlinks", ":", "\n", "                ", "potential_title", "=", "hl", ".", "href", "\n", "if", "potential_title", "in", "ner_set", "and", "not", "filter_word", "(", "potential_title", ")", "and", "not", "filter_document_id", "(", "\n", "potential_title", ")", ":", "# important bug fixing 'or' to 'and'", "\n", "# hyperlinked_title.append(potential_title)", "\n", "\n", "# if not filter_document_id(potential_title):", "\n", "                    ", "score", "=", "get_query_doc_score", "(", "valid_query_terms", ",", "potential_title", ",", "g_score_dict", ")", "\n", "retrieved_set", ".", "add_item", "(", "retrieval_utils", ".", "RetrievedItem", "(", "potential_title", ",", "'kwm_disamb_hlinked'", ")", ")", "\n", "retrieved_set", ".", "score_item", "(", "potential_title", ",", "score", ",", "namespace", "=", "keyword_group", "+", "'-2-hop'", ")", "\n", "\n", "", "", "", "for", "keyword_group", "in", "finded_keys_set", ":", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "keyword_group", "+", "'-2-hop'", ",", "top_k", "=", "multihop_retrieval_top_k", ")", "\n", "\n", "", "doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "if", "gt_data_list", "is", "not", "None", ":", "\n", "        ", "ext_hotpot_eval", ".", "eval", "(", "doc_pred_dict", ",", "gt_data_list", ")", "\n", "", "return", "doc_pred_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering": [[313, 339], ["wiki_util.title_entities_set.get_title_entity_set", "pred_dict[].keys", "dict", "dict", "set", "retrieved_set.retrieved_dict.values", "retrieved_set.to_id_list", "item.scores_dict.keys", "retrieved_set.sort_and_filter", "keyword_gourp_name.endswith", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.to_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.retrieval_utils.RetrievedSet.sort_and_filter", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "results_multihop_filtering", "(", "pred_dict", ",", "multihop_retrieval_top_k", "=", "3", ",", "strict_mode", "=", "False", ")", ":", "\n", "    ", "ner_set", "=", "get_title_entity_set", "(", ")", "\n", "\n", "new_doc_pred_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'raw_retrieval_set'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "key", "in", "pred_dict", "[", "'raw_retrieval_set'", "]", ".", "keys", "(", ")", ":", "\n", "        ", "qid", "=", "key", "\n", "retrieved_set", ":", "RetrievedSet", "=", "pred_dict", "[", "'raw_retrieval_set'", "]", "[", "key", "]", "\n", "\n", "hyperlinked_keyword_group", "=", "set", "(", ")", "\n", "for", "item", "in", "retrieved_set", ".", "retrieved_dict", ".", "values", "(", ")", ":", "\n", "            ", "for", "keyword_gourp_name", "in", "item", ".", "scores_dict", ".", "keys", "(", ")", ":", "\n", "                ", "if", "keyword_gourp_name", ".", "endswith", "(", "'-2-hop'", ")", ":", "\n", "                    ", "hyperlinked_keyword_group", ".", "add", "(", "keyword_gourp_name", ")", "\n", "# If the current scored one is 2-hop retrieval", "\n", "\n", "", "", "", "for", "keyword_group", "in", "hyperlinked_keyword_group", ":", "# The group already has '-2-hop' in the end", "\n", "# if keyword_group not in ner_set:", "\n", "#     continue    # Important update 2019-04-07, we skip the one not in wiki title set.", "\n", "# retrieved_set.sort_and_filter(keyword_group + '-2-hop', top_k=hyperlinked_top_k)", "\n", "            ", "retrieved_set", ".", "sort_and_filter", "(", "keyword_group", ",", "top_k", "=", "multihop_retrieval_top_k", ",", "strict_mode", "=", "strict_mode", ")", "\n", "\n", "", "new_doc_pred_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "retrieved_set", ".", "to_id_list", "(", ")", "\n", "new_doc_pred_dict", "[", "'raw_retrieval_set'", "]", "[", "qid", "]", "=", "retrieved_set", "\n", "\n", "", "return", "new_doc_pred_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.experiment_dev_full_wiki": [[341, 388], ["utils.common.load_json", "utils.common.load_jsonl", "dict", "build_rindex.build_rvindex.load_from_file", "hotpot_doc_retri_v0.init_results_v8", "doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "utils.common.save_json", "hotpot_doc_retri_v0.results_multihop_filtering", "print", "new_doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "evaluation.ext_hotpot_eval.eval", "utils.common.save_json", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len", "len", "collections.Counter", "collections.Counter"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.init_results_v8", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json"], ["", "def", "experiment_dev_full_wiki", "(", ")", ":", "\n", "    ", "multihop_retrieval_top_k", "=", "3", "\n", "match_filtering_k", "=", "3", "\n", "term_retrieval_top_k", "=", "5", "\n", "\n", "data_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "terms_based_results_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "doc_retri_pred_dict", "=", "init_results_v8", "(", "data_list", ",", "data_list", ",", "terms_based_results_list", ",", "g_score_dict", ",", "\n", "match_filtering_k", "=", "match_filtering_k", ",", "\n", "term_retrieval_top_k", "=", "term_retrieval_top_k", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results without filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "common", ".", "save_json", "(", "doc_retri_pred_dict", ",", "\"hotpot_dev_doc_retrieval_v8_before_multihop_filtering.json\"", ")", "\n", "\n", "# Filtering", "\n", "new_doc_retri_pred_dict", "=", "results_multihop_filtering", "(", "doc_retri_pred_dict", ",", "\n", "multihop_retrieval_top_k", "=", "multihop_retrieval_top_k", ")", "\n", "print", "(", "\"Results with filtering:\"", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "new_doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results with filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "ext_hotpot_eval", ".", "eval", "(", "new_doc_retri_pred_dict", ",", "data_list", ")", "\n", "common", ".", "save_json", "(", "new_doc_retri_pred_dict", ",", "\"hotpot_dev_doc_retrieval_v8.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.experiment_test_full_wiki": [[390, 438], ["utils.common.load_json", "utils.common.load_jsonl", "dict", "build_rindex.build_rvindex.load_from_file", "hotpot_doc_retri_v0.init_results_v8", "doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "utils.common.save_json", "hotpot_doc_retri_v0.results_multihop_filtering", "print", "new_doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "utils.common.save_json", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len", "len", "collections.Counter", "collections.Counter"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.init_results_v8", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json"], ["", "def", "experiment_test_full_wiki", "(", ")", ":", "\n", "    ", "multihop_retrieval_top_k", "=", "3", "\n", "match_filtering_k", "=", "3", "\n", "term_retrieval_top_k", "=", "5", "\n", "\n", "data_list", "=", "common", ".", "load_json", "(", "config", ".", "TEST_FULLWIKI_FILE", ")", "\n", "terms_based_results_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_test.jsonl\"", ")", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "# WE need to give gt data None.", "\n", "doc_retri_pred_dict", "=", "init_results_v8", "(", "data_list", ",", "None", ",", "terms_based_results_list", ",", "g_score_dict", ",", "\n", "match_filtering_k", "=", "match_filtering_k", ",", "\n", "term_retrieval_top_k", "=", "term_retrieval_top_k", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results without filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "common", ".", "save_json", "(", "doc_retri_pred_dict", ",", "\"hotpot_test_doc_retrieval_v8_before_multihop_filtering.json\"", ")", "\n", "\n", "# Filtering", "\n", "new_doc_retri_pred_dict", "=", "results_multihop_filtering", "(", "doc_retri_pred_dict", ",", "\n", "multihop_retrieval_top_k", "=", "multihop_retrieval_top_k", ")", "\n", "print", "(", "\"Results with filtering:\"", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "new_doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results with filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "# ext_hotpot_eval.eval(new_doc_retri_pred_dict, data_list)", "\n", "common", ".", "save_json", "(", "new_doc_retri_pred_dict", ",", "\"hotpot_test_doc_retrieval_v8.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.experiment_train_full_wiki": [[440, 497], ["utils.common.load_json", "utils.common.load_jsonl", "dict", "build_rindex.build_rvindex.load_from_file", "hotpot_doc_retri_v0.init_results_v8", "doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "utils.common.save_json", "hotpot_doc_retri_v0.results_multihop_filtering", "print", "new_doc_retri_pred_dict[].values", "print", "print", "print", "print", "print", "print", "print", "evaluation.ext_hotpot_eval.eval", "utils.common.save_json", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len", "len", "collections.Counter", "collections.Counter"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.init_results_v8", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json"], ["", "def", "experiment_train_full_wiki", "(", ")", ":", "\n", "    ", "multihop_retrieval_top_k", "=", "3", "\n", "match_filtering_k", "=", "3", "\n", "term_retrieval_top_k", "=", "5", "\n", "multihop_strict_mode", "=", "True", "\n", "debug_mode", "=", "None", "\n", "\n", "# data_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "data_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "\n", "if", "debug_mode", "is", "not", "None", ":", "\n", "        ", "data_list", "=", "data_list", "[", ":", "debug_mode", "]", "\n", "\n", "", "terms_based_results_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_train.jsonl\"", ")", "\n", "\n", "g_score_dict", "=", "dict", "(", ")", "\n", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "doc_retri_pred_dict", "=", "init_results_v8", "(", "data_list", ",", "data_list", ",", "terms_based_results_list", ",", "g_score_dict", ",", "\n", "match_filtering_k", "=", "match_filtering_k", ",", "term_retrieval_top_k", "=", "term_retrieval_top_k", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results without filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "# common.save_json(doc_retri_pred_dict, f\"hotpot_doc_retrieval_v8_before_multihop_filtering_{debug_mode}.json\")", "\n", "common", ".", "save_json", "(", "doc_retri_pred_dict", ",", "f\"hotpot_train_doc_retrieval_v8_before_multihop_filtering.json\"", ")", "\n", "\n", "# Filtering", "\n", "new_doc_retri_pred_dict", "=", "results_multihop_filtering", "(", "doc_retri_pred_dict", ",", "\n", "multihop_retrieval_top_k", "=", "multihop_retrieval_top_k", ",", "\n", "strict_mode", "=", "multihop_strict_mode", ")", "\n", "print", "(", "\"Results with filtering:\"", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "new_doc_retri_pred_dict", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results with filtering:\"", ")", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "ext_hotpot_eval", ".", "eval", "(", "new_doc_retri_pred_dict", ",", "data_list", ")", "\n", "# common.save_json(new_doc_retri_pred_dict, f\"hotpot_doc_retrieval_v8_{debug_mode}.json\")", "\n", "common", ".", "save_json", "(", "new_doc_retri_pred_dict", ",", "f\"hotpot_train_doc_retrieval_v8.json\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.sp_doc_analysis": [[15, 30], ["None"], "function", ["None"], ["def", "sp_doc_analysis", "(", "item", ")", ":", "\n", "# if item['type'] == 'comparison':", "\n", "# if item['type'] == 'bridge':", "\n", "#     return True", "\n", "\n", "    ", "if", "item", "[", "'doc_recall'", "]", "!=", "1.0", "and", "item", "[", "'type'", "]", "==", "'bridge'", ":", "\n", "        ", "return", "True", "\n", "# if item['doc_recall'] == 0.0 and item['type'] == 'comparison':", "\n", "# if item['doc_prec'] == 0.0 and item['type'] == 'comparison':", "\n", "# if item['doc_recall'] == 0.0 and item['type'] == 'bridge':", "\n", "# if item['doc_prec'] == 0.0 and item['type'] == 'bridge':", "\n", "# if item['doc_recall'] != 1.0 and item['type'] == 'bridge':", "\n", "# if 1 >= item['doc_recall'] >= 0.0 and item['type'] == 'comparison':", "\n", "#     return True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.sp_position_analysis": [[32, 38], ["counter.update", "sent_numbers.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["", "def", "sp_position_analysis", "(", "item", ",", "counter", ")", ":", "\n", "    ", "supoorting_fact", "=", "item", "[", "'supporting_facts'", "]", "\n", "sent_numbers", "=", "[", "]", "\n", "for", "doc", ",", "sent_number", "in", "supoorting_fact", ":", "\n", "        ", "sent_numbers", ".", "append", "(", "sent_number", ")", "\n", "", "counter", ".", "update", "(", "sent_numbers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.filter_analysis": [[40, 66], ["score_tracker.items", "print", "filter_func", "print", "dict", "print", "ValueError"], "function", ["None"], ["", "def", "filter_analysis", "(", "score_tracker", ",", "filter_func", ",", "max_count", "=", "None", ",", "show_info", "=", "None", ",", "additional_item", "=", "None", ")", ":", "\n", "    ", "count", "=", "0", "\n", "total_count", "=", "0", "\n", "for", "k", ",", "v", "in", "score_tracker", ".", "items", "(", ")", ":", "\n", "        ", "if", "filter_func", "(", "v", ")", ":", "\n", "            ", "count", "+=", "1", "\n", "if", "show_info", "is", "None", ":", "\n", "                ", "print", "(", "v", ")", "\n", "", "else", ":", "\n", "# print out information that is needed", "\n", "                ", "pinfo", "=", "dict", "(", ")", "\n", "for", "key", "in", "show_info", ":", "\n", "                    ", "if", "key", "in", "v", ":", "\n", "                        ", "pinfo", "[", "key", "]", "=", "v", "[", "key", "]", "\n", "", "elif", "additional_item", "is", "not", "None", "and", "key", "in", "additional_item", ":", "\n", "                        ", "pinfo", "[", "key", "]", "=", "additional_item", "[", "key", "]", "[", "k", "]", "\n", "", "else", ":", "\n", "                        ", "raise", "ValueError", "(", "f\"Key Value {key} is not presented in both score tracker and additaional_item.\"", ")", "\n", "", "", "print", "(", "pinfo", ")", "\n", "\n", "", "", "total_count", "+=", "1", "\n", "\n", "if", "max_count", "is", "not", "None", "and", "count", "==", "max_count", ":", "\n", "            ", "break", "\n", "\n", "", "", "print", "(", "count", ",", "total_count", ",", "count", "/", "total_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.counter_analysis": [[68, 73], ["collections.Counter", "score_tracker.items", "print", "hotpot_doc_retri_analysis.sp_position_analysis"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.sp_position_analysis"], ["", "def", "counter_analysis", "(", "score_tracker", ")", ":", "\n", "    ", "c", "=", "Counter", "(", ")", "\n", "for", "k", ",", "v", "in", "score_tracker", ".", "items", "(", ")", ":", "\n", "        ", "sp_position_analysis", "(", "v", ",", "c", ")", "\n", "", "print", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.get_sp_position_count": [[75, 82], ["utils.common.load_json", "collections.Counter", "print", "hotpot_doc_retri_analysis.sp_position_analysis"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_analysis.sp_position_analysis"], ["", "def", "get_sp_position_count", "(", ")", ":", "\n", "    ", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "c", "=", "Counter", "(", ")", "\n", "for", "item", "in", "train_list", ":", "\n", "        ", "sp_position_analysis", "(", "item", ",", "c", ")", "\n", "\n", "", "print", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.IdField.__str__": [[15, 17], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"IdField with id: {self.metadata}.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.__init__": [[26, 29], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "array", ":", "numpy", ".", "ndarray", ",", "padding_value", ":", "int", "=", "0", ")", "->", "None", ":", "\n", "        ", "self", ".", "array", "=", "array", "\n", "self", ".", "padding_value", "=", "padding_value", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.get_padding_lengths": [[30, 34], ["str", "enumerate"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "\"dimension_\"", "+", "str", "(", "i", ")", ":", "shape", "\n", "for", "i", ",", "shape", "in", "enumerate", "(", "self", ".", "array", ".", "shape", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.as_tensor": [[35, 52], ["numpy.asarray", "list", "tuple", "torch.from_numpy", "len", "len", "range", "numpy.ones", "slice", "len", "range", "len", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "max_shape", "=", "[", "padding_lengths", "[", "\"dimension_{}\"", ".", "format", "(", "i", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "padding_lengths", ")", ")", "]", "\n", "\n", "# Convert explicitly to an ndarray just in case it's an scalar (it'd end up not being an ndarray otherwise)", "\n", "return_array", "=", "numpy", ".", "asarray", "(", "numpy", ".", "ones", "(", "max_shape", ",", "\"int64\"", ")", "*", "self", ".", "padding_value", ")", "\n", "\n", "# If the tensor has a different shape from the largest tensor, pad dimensions with zeros to", "\n", "# form the right shaped list of slices for insertion into the final tensor.", "\n", "slicing_shape", "=", "list", "(", "self", ".", "array", ".", "shape", ")", "\n", "if", "len", "(", "self", ".", "array", ".", "shape", ")", "<", "len", "(", "max_shape", ")", ":", "\n", "            ", "slicing_shape", "=", "slicing_shape", "+", "[", "0", "for", "_", "in", "range", "(", "len", "(", "max_shape", ")", "-", "len", "(", "self", ".", "array", ".", "shape", ")", ")", "]", "\n", "", "slices", "=", "tuple", "(", "[", "slice", "(", "0", ",", "x", ")", "for", "x", "in", "slicing_shape", "]", ")", "\n", "return_array", "[", "slices", "]", "=", "self", ".", "array", "\n", "tensor", "=", "torch", ".", "from_numpy", "(", "return_array", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.sequence_length": [[54, 56], ["None"], "methods", ["None"], ["", "def", "sequence_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "array", ".", "shape", "[", "0", "]", "# The first shape", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.empty_field": [[57, 62], ["customized_field.BertIndexField", "numpy.array"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "# pylint: disable=no-self-use", "\n", "# Pass the padding_value, so that any outer field, e.g., `ListField[ArrayField]` uses the", "\n", "# same padding_value in the padded ArrayFields", "\n", "        ", "return", "BertIndexField", "(", "numpy", ".", "array", "(", "[", "]", ",", "dtype", "=", "\"int64\"", ")", ",", "padding_value", "=", "self", ".", "padding_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.customized_field.BertIndexField.__str__": [[63, 65], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"ArrayField with shape: {self.array.shape}.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._NamespaceDependentDefaultDict.__init__": [[78, 92], ["oov_token.replace", "collections.defaultdict.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "non_padded_namespaces", ":", "Sequence", "[", "str", "]", ",", "\n", "padded_function", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "\n", "non_padded_function", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "\n", "padding_token", ":", "str", "=", "DEFAULT_PADDING_TOKEN", ",", "\n", "oov_token", ":", "str", "=", "DEFAULT_OOV_TOKEN", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "_non_padded_namespaces", "=", "non_padded_namespaces", "\n", "self", ".", "padding_token", "=", "padding_token", "\n", "self", ".", "oov_token", "=", "oov_token", ".", "replace", "(", "'@'", ",", "''", ")", "\n", "\n", "self", ".", "_padded_function", "=", "padded_function", "\n", "self", ".", "_non_padded_function", "=", "non_padded_function", "\n", "super", "(", "_NamespaceDependentDefaultDict", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._NamespaceDependentDefaultDict.__missing__": [[93, 100], ["any", "dict.__setitem__", "exvocab._NamespaceDependentDefaultDict._non_padded_function", "exvocab._NamespaceDependentDefaultDict._padded_function", "allennlp.common.util.namespace_match"], "methods", ["None"], ["", "def", "__missing__", "(", "self", ",", "key", ":", "str", ")", ":", "\n", "        ", "if", "any", "(", "namespace_match", "(", "pattern", ",", "key", ")", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", ")", ":", "\n", "            ", "value", "=", "self", ".", "_non_padded_function", "(", ")", "\n", "", "else", ":", "\n", "            ", "value", "=", "self", ".", "_padded_function", "(", ")", "\n", "", "dict", ".", "__setitem__", "(", "self", ",", "key", ",", "value", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._NamespaceDependentDefaultDict.initialize_dictionary": [[101, 121], ["any", "dict.__setitem__", "exvocab.RandomHashDict", "RandomHashDict.update", "exvocab.RandomHashDict.add_unk_tokens", "dict.__setitem__", "any", "allennlp.common.util.namespace_match", "dict.__setitem__", "range", "dict.__setitem__", "allennlp.common.util.namespace_match", "len", "str"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.add_unk_tokens"], ["", "def", "initialize_dictionary", "(", "self", ",", "namespace", ":", "str", ",", "unk_num", ":", "int", ",", "mode", ":", "MappingMode", ")", ":", "\n", "        ", "if", "mode", "==", "MappingMode", ".", "token2index", ":", "\n", "            ", "if", "any", "(", "namespace_match", "(", "pattern", ",", "namespace", ")", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", ")", ":", "\n", "                ", "dict", ".", "__setitem__", "(", "self", ",", "namespace", ",", "{", "}", ")", "\n", "", "else", ":", "\n", "                ", "init_namespace_dictionary", "=", "RandomHashDict", "(", "unk_num", "=", "unk_num", ",", "oov_token", "=", "self", ".", "oov_token", ")", "\n", "init_namespace_dictionary", ".", "update", "(", "{", "self", ".", "padding_token", ":", "0", "}", ")", "\n", "init_namespace_dictionary", ".", "add_unk_tokens", "(", ")", "\n", "\n", "dict", ".", "__setitem__", "(", "self", ",", "namespace", ",", "init_namespace_dictionary", ")", "\n", "\n", "", "", "elif", "mode", "==", "MappingMode", ".", "index2token", ":", "\n", "            ", "if", "any", "(", "namespace_match", "(", "pattern", ",", "namespace", ")", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", ")", ":", "\n", "                ", "dict", ".", "__setitem__", "(", "self", ",", "namespace", ",", "{", "}", ")", "\n", "", "else", ":", "\n", "                ", "init_namespace_dictionary", "=", "{", "0", ":", "self", ".", "padding_token", "}", "\n", "for", "i", "in", "range", "(", "unk_num", ")", ":", "\n", "                    ", "init_namespace_dictionary", "[", "len", "(", "init_namespace_dictionary", ")", "]", "=", "f\"@@{self.oov_token}#{str(i)}@@\"", "\n", "\n", "", "dict", ".", "__setitem__", "(", "self", ",", "namespace", ",", "init_namespace_dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.__init__": [[124, 128], ["dict.__init__", "oov_token.replace"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "unk_num", ":", "int", "=", "1", ",", "oov_token", ":", "str", "=", "DEFAULT_OOV_TOKEN", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "unk_num", "=", "unk_num", "\n", "self", ".", "oov_token", "=", "oov_token", ".", "replace", "(", "'@'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.add_unk_tokens": [[129, 132], ["range", "exvocab.RandomHashDict.__setitem__", "exvocab.RandomHashDict.__len__", "str"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.__len__"], ["", "def", "add_unk_tokens", "(", "self", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "unk_num", ")", ":", "\n", "            ", "self", ".", "__setitem__", "(", "f\"@@{self.oov_token}#{str(i)}@@\"", ",", "self", ".", "__len__", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.__getitem__": [[133, 139], ["dict.__getitem__", "exvocab.RandomHashDict.hash_string", "dict.__getitem__", "super().keys"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.__getitem__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.hash_string", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.__getitem__"], ["", "", "def", "__getitem__", "(", "self", ",", "k", ")", ":", "\n", "        ", "if", "self", ".", "unk_num", "==", "0", "or", "k", "in", "super", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "return", "super", "(", ")", ".", "__getitem__", "(", "k", ")", "\n", "", "else", ":", "\n", "            ", "k", "=", "self", ".", "hash_string", "(", "k", ",", "self", ".", "unk_num", ")", "\n", "return", "super", "(", ")", ".", "__getitem__", "(", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.hash_string": [[140, 144], ["int", "hashlib.sha1().hexdigest", "str", "hashlib.sha1", "input_str.encode"], "methods", ["None"], ["", "", "def", "hash_string", "(", "self", ",", "input_str", ",", "unk_num", ")", ":", "\n", "        ", "hcode", "=", "int", "(", "hashlib", ".", "sha1", "(", "input_str", ".", "encode", "(", "'utf-8'", ")", ")", ".", "hexdigest", "(", ")", ",", "16", ")", "\n", "hcode", "%=", "unk_num", "\n", "return", "f\"@@{self.oov_token}#{str(hcode)}@@\"", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.RandomHashDict.__str__": [[145, 147], ["dict.__str__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.__str__"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "super", "(", ")", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.__init__": [[160, 202], ["exvocab.ExVocabulary._initialize_dictionary", "isinstance", "collections.defaultdict", "list", "exvocab.ExVocabulary.unk_token_num.keys", "list", "list.sort", "exvocab._read_pretrained_words", "counter[].items", "min_count.get", "exvocab.ExVocabulary.add_token_to_namespace", "exvocab.ExVocabulary.add_token_to_namespace", "exvocab.ExVocabulary.add_token_to_namespace", "min_count.get", "min_count.get"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary._initialize_dictionary", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._read_pretrained_words", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace"], ["    ", "def", "__init__", "(", "self", ",", "\n", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Sequence", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "unk_token_num", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "_padding_token", "=", "DEFAULT_PADDING_TOKEN", "\n", "self", ".", "_oov_token", "=", "DEFAULT_OOV_TOKEN", "\n", "if", "not", "isinstance", "(", "max_vocab_size", ",", "dict", ")", ":", "\n", "            ", "int_max_vocab_size", "=", "max_vocab_size", "\n", "max_vocab_size", "=", "defaultdict", "(", "lambda", ":", "int_max_vocab_size", ")", "# type: ignore", "\n", "", "self", ".", "_non_padded_namespaces", "=", "non_padded_namespaces", "\n", "self", ".", "unk_token_num", "=", "unk_token_num", "\n", "\n", "self", ".", "_initialize_dictionary", "(", "list", "(", "self", ".", "unk_token_num", ".", "keys", "(", ")", ")", ",", "non_padded_namespaces", ",", "\n", "self", ".", "_padding_token", ",", "self", ".", "_oov_token", ")", "\n", "\n", "min_count", "=", "min_count", "or", "{", "}", "\n", "pretrained_files", "=", "pretrained_files", "or", "{", "}", "\n", "if", "counter", "is", "not", "None", ":", "\n", "            ", "for", "namespace", "in", "counter", ":", "\n", "                ", "if", "namespace", "in", "pretrained_files", ":", "\n", "                    ", "pretrained_list", "=", "_read_pretrained_words", "(", "pretrained_files", "[", "namespace", "]", ")", "\n", "", "else", ":", "\n", "                    ", "pretrained_list", "=", "None", "\n", "", "token_counts", "=", "list", "(", "counter", "[", "namespace", "]", ".", "items", "(", ")", ")", "\n", "token_counts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "max_vocab", "=", "max_vocab_size", "[", "namespace", "]", "\n", "if", "max_vocab", ":", "\n", "                    ", "token_counts", "=", "token_counts", "[", ":", "max_vocab", "]", "\n", "", "for", "token", ",", "count", "in", "token_counts", ":", "\n", "                    ", "if", "pretrained_list", "is", "not", "None", ":", "\n", "                        ", "if", "only_include_pretrained_words", ":", "\n", "                            ", "if", "token", "in", "pretrained_list", "and", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "", "elif", "token", "in", "pretrained_list", "or", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                            ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "", "elif", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                        ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary._initialize_dictionary": [[203, 227], ["exvocab._NamespaceDependentDefaultDict", "exvocab._NamespaceDependentDefaultDict", "collections.defaultdict", "exvocab.ExVocabulary.unk_token_num.get", "exvocab.ExVocabulary._token_to_index.initialize_dictionary", "exvocab.ExVocabulary._index_to_token.initialize_dictionary"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._NamespaceDependentDefaultDict.initialize_dictionary", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._NamespaceDependentDefaultDict.initialize_dictionary"], ["", "", "", "", "", "def", "_initialize_dictionary", "(", "self", ",", "namespaces", ":", "List", "[", "str", "]", ",", "non_padded_namespaces", ",", "padding_token", ",", "oov_token", ")", ":", "\n", "\n", "        ", "self", ".", "_token_to_index", "=", "_NamespaceDependentDefaultDict", "(", "\n", "non_padded_namespaces", ",", "\n", "lambda", ":", "{", "padding_token", ":", "0", ",", "oov_token", ":", "1", "}", ",", "\n", "lambda", ":", "{", "}", ",", "\n", "padding_token", ",", "\n", "oov_token", "\n", ")", "\n", "\n", "self", ".", "_index_to_token", "=", "_NamespaceDependentDefaultDict", "(", "\n", "non_padded_namespaces", ",", "\n", "lambda", ":", "{", "0", ":", "padding_token", ",", "1", ":", "oov_token", "}", ",", "\n", "lambda", ":", "{", "}", ",", "\n", "padding_token", ",", "\n", "oov_token", "\n", ")", "\n", "\n", "if", "self", ".", "unk_token_num", "is", "None", ":", "\n", "            ", "self", ".", "unk_token_num", "=", "defaultdict", "(", "lambda", ":", "1", ")", "\n", "", "for", "namespace", "in", "namespaces", ":", "\n", "            ", "cur_unk_num", "=", "self", ".", "unk_token_num", ".", "get", "(", "namespace", ",", "1", ")", "\n", "self", ".", "_token_to_index", ".", "initialize_dictionary", "(", "namespace", ",", "cur_unk_num", ",", "MappingMode", ".", "token2index", ")", "\n", "self", ".", "_index_to_token", ".", "initialize_dictionary", "(", "namespace", ",", "cur_unk_num", ",", "MappingMode", ".", "index2token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.save_to_files": [[228, 263], ["os.makedirs", "os.listdir", "exvocab.ExVocabulary._index_to_token.items", "logging.warning", "codecs.open", "codecs.open", "exvocab.ExVocabulary.unk_token_num.items", "os.path.join", "print", "os.path.join", "print", "codecs.open", "len", "range", "len", "len", "os.path.join", "print", "str", "str", "mapping[].replace"], "methods", ["None"], ["", "", "def", "save_to_files", "(", "self", ",", "directory", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Persist this Vocabulary to files so it can be reloaded later.\n        Each namespace corresponds to one file.\n\n        Parameters\n        ----------\n        directory : ``str``\n            The directory where we save the serialized vocabulary.\n        \"\"\"", "\n", "os", ".", "makedirs", "(", "directory", ",", "exist_ok", "=", "True", ")", "\n", "if", "os", ".", "listdir", "(", "directory", ")", ":", "\n", "            ", "logging", ".", "warning", "(", "\"vocabulary serialization directory %s is not empty\"", ",", "directory", ")", "\n", "\n", "", "with", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "NAMESPACE_PADDING_FILE", ")", ",", "'w'", ",", "'utf-8'", ")", "as", "namespace_file", ":", "\n", "            ", "for", "namespace_str", "in", "self", ".", "_non_padded_namespaces", ":", "\n", "                ", "print", "(", "namespace_str", ",", "file", "=", "namespace_file", ")", "\n", "\n", "", "", "with", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "UNK_COUNT_FILE", ")", ",", "'w'", ",", "'utf-8'", ")", "as", "unk_count_file", ":", "\n", "            ", "for", "namespace_str", ",", "unk_count_value", "in", "self", ".", "unk_token_num", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "namespace_str", "+", "'###'", "+", "str", "(", "unk_count_value", ")", ",", "file", "=", "unk_count_file", ")", "\n", "\n", "# assert len(self._token_to_index) == len(self._index_to_token)", "\n", "\n", "", "", "for", "namespace", ",", "mapping", "in", "self", ".", "_index_to_token", ".", "items", "(", ")", ":", "\n", "            ", "if", "namespace", "in", "self", ".", "_token_to_index", ":", "\n", "                ", "assert", "len", "(", "self", ".", "_token_to_index", "[", "namespace", "]", ")", "==", "len", "(", "self", ".", "_index_to_token", "[", "namespace", "]", ")", "\n", "# Each namespace gets written to its own file, in index order.", "\n", "", "with", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "namespace", "+", "'.txt'", ")", ",", "'w'", ",", "'utf-8'", ")", "as", "token_file", ":", "\n", "                ", "num_tokens", "=", "len", "(", "mapping", ")", "\n", "for", "i", "in", "range", "(", "num_tokens", ")", ":", "\n", "                    ", "if", "namespace", "in", "self", ".", "_token_to_index", ":", "\n", "                        ", "assert", "mapping", "[", "i", "]", "==", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "mapping", "[", "i", "]", "]", "]", "\n", "\n", "", "print", "(", "mapping", "[", "i", "]", ".", "replace", "(", "'\\n'", ",", "'@@NEWLINE@@'", ")", "+", "TOKEN_INDEX_SEP", "+", "str", "(", "i", ")", ",", "file", "=", "token_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_files": [[264, 301], ["logger.info", "dict", "exvocab.ExVocabulary", "os.listdir", "codecs.open", "codecs.open", "namespace_filename.replace", "os.path.join", "exvocab.ExVocabulary.set_from_file", "os.path.join", "namespace_str.strip", "os.path.join", "int", "unk_count.split", "unk_count.split"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.set_from_file"], ["", "", "", "", "@", "classmethod", "\n", "def", "from_files", "(", "cls", ",", "directory", ":", "str", ")", "->", "'ExVocabulary'", ":", "\n", "        ", "\"\"\"\n        Loads a ``Vocabulary`` that was serialized using ``save_to_files``.\n\n        Parameters\n        ----------\n        directory : ``str``\n            The directory containing the serialized vocabulary.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Loading token dictionary from %s.\"", ",", "directory", ")", "\n", "with", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "NAMESPACE_PADDING_FILE", ")", ",", "'r'", ",", "'utf-8'", ")", "as", "namespace_file", ":", "\n", "            ", "non_padded_namespaces", "=", "[", "namespace_str", ".", "strip", "(", ")", "for", "namespace_str", "in", "namespace_file", "]", "\n", "\n", "", "unk_token_num", "=", "dict", "(", ")", "\n", "with", "codecs", ".", "open", "(", "os", ".", "path", ".", "join", "(", "directory", ",", "UNK_COUNT_FILE", ")", ",", "'r'", ",", "'utf-8'", ")", "as", "unk_count_file", ":", "\n", "            ", "for", "unk_count", "in", "unk_count_file", ":", "\n", "                ", "namespace_str", ",", "unk_count_value", "=", "unk_count", ".", "split", "(", "'###'", ")", "[", "0", "]", ",", "int", "(", "unk_count", ".", "split", "(", "'###'", ")", "[", "1", "]", ")", "\n", "unk_token_num", "[", "namespace_str", "]", "=", "unk_count_value", "\n", "\n", "", "", "vocab", "=", "ExVocabulary", "(", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "unk_token_num", "=", "unk_token_num", ")", "\n", "\n", "# Check every file in the directory.", "\n", "for", "namespace_filename", "in", "os", ".", "listdir", "(", "directory", ")", ":", "\n", "            ", "if", "namespace_filename", "==", "NAMESPACE_PADDING_FILE", ":", "\n", "                ", "continue", "\n", "", "elif", "namespace_filename", "==", "UNK_COUNT_FILE", ":", "\n", "                ", "continue", "\n", "", "elif", "namespace_filename", "==", "'weights'", ":", "\n", "                ", "continue", "# This is used for save weights", "\n", "\n", "", "namespace", "=", "namespace_filename", ".", "replace", "(", "'.txt'", ",", "''", ")", "\n", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "namespace_filename", ")", "\n", "vocab", ".", "set_from_file", "(", "filename", ",", "namespace", "=", "namespace", ")", "\n", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.set_from_file": [[302, 352], ["codecs.open", "input_file.read().split", "enumerate", "line.split", "token.replace.replace.replace", "input_file.read", "int", "len", "len"], "methods", ["None"], ["", "def", "set_from_file", "(", "self", ",", "\n", "filename", ":", "str", ",", "\n", "namespace", ":", "str", "=", "\"tokens\"", ")", ":", "\n", "        ", "\"\"\"\n        If you already have a vocabulary file for a trained model somewhere, and you really want to\n        use that vocabulary file instead of just setting the vocabulary from a dataset, for\n        whatever reason, you can do that with this method.  You must specify the namespace to use,\n        and we assume that you want to use padding and OOV tokens for this.\n\n        Parameters\n        ----------\n        filename : ``str``\n            The file containing the vocabulary to load.  It should be formatted as one token per\n            line, with nothing else in the line.  The index we assign to the token is the line\n            number in the file (1-indexed if ``is_padded``, 0-indexed otherwise).  Note that this\n            file should contain the OOV token string!\n        is_padded : ``bool``, optional (default=True)\n            Is this vocabulary padded?  For token / word / character vocabularies, this should be\n            ``True``; while for tag or label vocabularies, this should typically be ``False``.  If\n            ``True``, we add a padding token with index 0, and we enforce that the ``oov_token`` is\n            present in the file.\n        oov_token : ``str``, optional (default=DEFAULT_OOV_TOKEN)\n            What token does this vocabulary use to represent out-of-vocabulary characters?  This\n            must show up as a line in the vocabulary file.  When we find it, we replace\n            ``oov_token`` with ``self._oov_token``, because we only use one OOV token across\n            namespaces.\n        namespace : ``str``, optional (default=\"tokens\")\n            What namespace should we overwrite with this vocab file?\n        \"\"\"", "\n", "# self._token_to_index[namespace] = {}", "\n", "# self._index_to_token[namespace] = {}", "\n", "\n", "with", "codecs", ".", "open", "(", "filename", ",", "'r'", ",", "'utf-8'", ")", "as", "input_file", ":", "\n", "            ", "lines", "=", "input_file", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", "\n", "# Be flexible about having final newline or not", "\n", "if", "lines", "and", "lines", "[", "-", "1", "]", "==", "''", ":", "\n", "                ", "lines", "=", "lines", "[", ":", "-", "1", "]", "\n", "", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "                ", "row", "=", "line", ".", "split", "(", "TOKEN_INDEX_SEP", ")", "\n", "token", ",", "index", "=", "row", "[", "0", "]", ",", "int", "(", "row", "[", "1", "]", ")", "\n", "token", "=", "token", ".", "replace", "(", "'@@NEWLINE@@'", ",", "'\\n'", ")", "\n", "\n", "if", "token", "not", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "                    ", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "=", "index", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "=", "token", "\n", "", "else", ":", "\n", "                    ", "assert", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "==", "index", "\n", "assert", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "==", "token", "\n", "\n", "", "assert", "len", "(", "self", ".", "_token_to_index", ")", "==", "len", "(", "self", ".", "_index_to_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_instances": [[353, 397], ["logger.info", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "print", "namespace_token_counts.items", "exvocab.ExVocabulary", "allennlp.data.instance.count_vocab_items", "collections.defaultdict", "len", "print", "dict", "dict"], "methods", ["None"], ["", "", "", "@", "classmethod", "\n", "def", "from_instances", "(", "cls", ",", "\n", "instances", ":", "Iterable", "[", "'adi.Instance'", "]", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Sequence", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "unk_token_num", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "exclude_namespaces", "=", "None", ",", "\n", "include_namespaces", "=", "None", ")", "->", "'ExVocabulary'", ":", "\n", "        ", "\"\"\"\n        Constructs a vocabulary given a collection of `Instances` and some parameters.\n        We count all of the vocabulary items in the instances, then pass those counts\n        and the other parameters, to :func:`__init__`.  See that method for a description\n        of what the other parameters do.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Fitting token dictionary from dataset.\"", ")", "\n", "namespace_token_counts", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "            ", "instance", ".", "count_vocab_items", "(", "namespace_token_counts", ")", "\n", "\n", "", "if", "exclude_namespaces", "is", "not", "None", ":", "\n", "            ", "for", "namespace", "in", "namespace_token_counts", ":", "\n", "                ", "if", "namespace", "in", "exclude_namespaces", ":", "\n", "                    ", "namespace_token_counts", "[", "namespace", "]", "=", "dict", "(", ")", "\n", "\n", "", "if", "include_namespaces", "is", "not", "None", ":", "\n", "# If include namespaces is not None, we only include those namespaces.", "\n", "                    ", "if", "namespace", "not", "in", "include_namespaces", ":", "\n", "                        ", "namespace_token_counts", "[", "namespace", "]", "=", "dict", "(", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Start counting for namespaces:\"", ")", "\n", "for", "namespace", ",", "counter", "in", "namespace_token_counts", ".", "items", "(", ")", ":", "\n", "            ", "if", "len", "(", "counter", ")", "!=", "0", ":", "\n", "                ", "print", "(", "namespace", ")", "\n", "\n", "", "", "return", "ExVocabulary", "(", "counter", "=", "namespace_token_counts", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "unk_token_num", "=", "unk_token_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_params": [[398, 441], ["params.pop", "params.pop", "params.pop_int", "params.pop", "params.pop", "params.pop_bool", "params.assert_empty", "exvocab.ExVocabulary.from_instances", "allennlp.common.checks.ConfigurationError", "logger.info", "params.assert_empty", "Vocabulary.from_files"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_instances", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_files"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ",", "instances", ":", "Iterable", "[", "'adi.Instance'", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        There are two possible ways to build a vocabulary; from a\n        collection of instances, using :func:`Vocabulary.from_instances`, or\n        from a pre-saved vocabulary, using :func:`Vocabulary.from_files`.\n        This method wraps both of these options, allowing their specification\n        from a ``Params`` object, generated from a JSON configuration file.\n\n        Parameters\n        ----------\n        params: Params, required.\n        dataset: Dataset, optional.\n            If ``params`` doesn't contain a ``vocabulary_directory`` key,\n            the ``Vocabulary`` can be built directly from a ``Dataset``.\n\n        Returns\n        -------\n        A ``Vocabulary``.\n        \"\"\"", "\n", "vocabulary_directory", "=", "params", ".", "pop", "(", "\"directory_path\"", ",", "None", ")", "\n", "if", "not", "vocabulary_directory", "and", "not", "instances", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"You must provide either a Params object containing a \"", "\n", "\"vocab_directory key or a Dataset to build a vocabulary from.\"", ")", "\n", "", "if", "vocabulary_directory", "and", "instances", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading Vocab from files instead of dataset.\"", ")", "\n", "\n", "", "if", "vocabulary_directory", ":", "\n", "            ", "params", ".", "assert_empty", "(", "\"Vocabulary - from files\"", ")", "\n", "return", "Vocabulary", ".", "from_files", "(", "vocabulary_directory", ")", "\n", "\n", "", "min_count", "=", "params", ".", "pop", "(", "\"min_count\"", ",", "None", ")", "\n", "max_vocab_size", "=", "params", ".", "pop_int", "(", "\"max_vocab_size\"", ",", "None", ")", "\n", "non_padded_namespaces", "=", "params", ".", "pop", "(", "\"non_padded_namespaces\"", ",", "DEFAULT_NON_PADDED_NAMESPACES", ")", "\n", "pretrained_files", "=", "params", ".", "pop", "(", "\"pretrained_files\"", ",", "{", "}", ")", "\n", "only_include_pretrained_words", "=", "params", ".", "pop_bool", "(", "\"only_include_pretrained_words\"", ",", "False", ")", "\n", "params", ".", "assert_empty", "(", "\"Vocabulary - from dataset\"", ")", "\n", "return", "ExVocabulary", ".", "from_instances", "(", "instances", "=", "instances", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.is_padded": [[442, 447], ["None"], "methods", ["None"], ["", "def", "is_padded", "(", "self", ",", "namespace", ":", "str", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns whether or not there are padding and OOV tokens added to the given namepsace.\n        \"\"\"", "\n", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "0", "]", "==", "self", ".", "_padding_token", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace": [[448, 463], ["isinstance", "ValueError", "len", "repr", "type"], "methods", ["None"], ["", "def", "add_token_to_namespace", "(", "self", ",", "token", ":", "str", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Adds ``token`` to the index, if it is not already present.  Either way, we return the index of\n        the token.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "token", ",", "str", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Vocabulary tokens must be strings, or saving and loading will break.\"", "\n", "\"  Got %s (with type %s)\"", "%", "(", "repr", "(", "token", ")", ",", "type", "(", "token", ")", ")", ")", "\n", "", "if", "token", "not", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "            ", "index", "=", "len", "(", "self", ".", "_token_to_index", "[", "namespace", "]", ")", "\n", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "=", "index", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "=", "token", "\n", "return", "index", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace": [[464, 487], ["isinstance", "ValueError", "ValueError", "print", "exvocab.ExVocabulary._index_to_token[].pop", "repr", "type"], "methods", ["None"], ["", "", "def", "change_token_with_index_to_namespace", "(", "self", ",", "token", ":", "str", ",", "index", ":", "int", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Adds ``token`` to the index, if it is not already present.  Either way, we return the index of\n        the token.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "token", ",", "str", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Vocabulary tokens must be strings, or saving and loading will break.\"", "\n", "\"  Got %s (with type %s)\"", "%", "(", "repr", "(", "token", ")", ",", "type", "(", "token", ")", ")", ")", "\n", "\n", "", "if", "index", "in", "self", ".", "_index_to_token", "[", "namespace", "]", "or", "index", ">=", "0", ":", "\n", "            ", "if", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "==", "index", "and", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "==", "token", ":", "\n", "                    ", "return", "0", "# Already changed", "\n", "\n", "", "raise", "ValueError", "(", "f\"Index: {index} already exists or is invalid in the {namespace}, \"", "\n", "f\"can not set special Token: {token}\"", ")", "\n", "\n", "", "if", "token", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "            ", "print", "(", "f\"Change index to for an existing token: {token}\"", ")", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", ".", "pop", "(", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", ")", "\n", "\n", "", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "=", "index", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "=", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_index_to_token_vocabulary": [[488, 490], ["None"], "methods", ["None"], ["", "def", "get_index_to_token_vocabulary", "(", "self", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "Dict", "[", "int", ",", "str", "]", ":", "\n", "        ", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_token_to_index_vocabulary": [[491, 493], ["None"], "methods", ["None"], ["", "def", "get_token_to_index_vocabulary", "(", "self", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_token_index": [[494, 506], ["exvocab.ExVocabulary.unk_token_num.keys", "logger.error", "logger.error"], "methods", ["None"], ["", "def", "get_token_index", "(", "self", ",", "token", ":", "str", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "int", ":", "\n", "        ", "if", "token", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "            ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "\n", "", "elif", "namespace", "in", "self", ".", "unk_token_num", ".", "keys", "(", ")", ":", "# If we specify the unk token.", "\n", "            ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "self", ".", "_oov_token", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "logger", ".", "error", "(", "'Namespace: %s'", ",", "namespace", ")", "\n", "logger", ".", "error", "(", "'Token: %s'", ",", "token", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_token_from_index": [[507, 509], ["None"], "methods", ["None"], ["", "", "", "def", "get_token_from_index", "(", "self", ",", "index", ":", "int", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_vocab_size": [[510, 512], ["len"], "methods", ["None"], ["", "def", "get_vocab_size", "(", "self", ",", "namespace", ":", "str", "=", "'tokens'", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_token_to_index", "[", "namespace", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.__eq__": [[513, 517], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.__str__": [[518, 524], ["exvocab.ExVocabulary.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_vocab_size"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "base_string", "=", "f\"Vocabulary with namespaces:\\n\"", "\n", "non_padded_namespaces", "=", "f\"\\tNon Padded Namespaces: {self._non_padded_namespaces}\\n\"", "\n", "namespaces", "=", "[", "f\"\\tNamespace: {name}, Size: {self.get_vocab_size(name)} \\n\"", "\n", "for", "name", "in", "self", ".", "_index_to_token", "]", "\n", "return", "\" \"", ".", "join", "(", "[", "base_string", ",", "non_padded_namespaces", "]", "+", "namespaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab._read_pretrained_words": [[149, 157], ["set", "gzip.open", "allennlp.common.file_utils.cached_path", "line.decode().strip().split", "set.add", "line.decode().strip", "line.decode"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "", "def", "_read_pretrained_words", "(", "embeddings_filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "words", "=", "set", "(", ")", "\n", "with", "gzip", ".", "open", "(", "cached_path", "(", "embeddings_filename", ")", ",", "'rb'", ")", "as", "embeddings_file", ":", "\n", "        ", "for", "line", "in", "embeddings_file", ":", "\n", "            ", "fields", "=", "line", ".", "decode", "(", "'utf-8'", ")", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "word", "=", "fields", "[", "0", "]", "\n", "words", ".", "add", "(", "word", ")", "\n", "", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.read_normal_embedding_file": [[526, 581], ["set", "vocab.get_vocab_size", "logger.info", "numpy.asarray", "float", "float", "logger.info", "torch.FloatTensor().normal_", "allennlp.common.tqdm.Tqdm.tqdm", "vocab.get_index_to_token_vocabulary().values", "open", "ValueError", "list", "numpy.mean", "numpy.std", "range", "vocab.get_token_from_index", "line.decode().rstrip().split", "embeddings.values", "torch.FloatTensor", "torch.FloatTensor", "logger.debug", "vocab.get_index_to_token_vocabulary", "logger.warning", "numpy.asarray", "line.decode().rstrip", "len", "len", "line.decode"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_vocab_size", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_token_from_index", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.get_index_to_token_vocabulary"], ["", "", "def", "read_normal_embedding_file", "(", "embeddings_filename", ":", "str", ",", "# pylint: disable=invalid-name", "\n", "embedding_dim", ":", "int", ",", "\n", "vocab", ":", "ExVocabulary", ",", "\n", "namespace", ":", "str", "=", "\"tokens\"", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "    ", "words_to_keep", "=", "set", "(", "vocab", ".", "get_index_to_token_vocabulary", "(", "namespace", ")", ".", "values", "(", ")", ")", "\n", "vocab_size", "=", "vocab", ".", "get_vocab_size", "(", "namespace", ")", "\n", "embeddings", "=", "{", "}", "\n", "\n", "# First we read the embeddings from the file, only keeping vectors for the words we need.", "\n", "logger", ".", "info", "(", "\"Reading embeddings from file\"", ")", "\n", "with", "open", "(", "embeddings_filename", ",", "'rb'", ")", "as", "embeddings_file", ":", "\n", "        ", "for", "line", "in", "embeddings_file", ":", "\n", "            ", "fields", "=", "line", ".", "decode", "(", "'utf-8'", ")", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "fields", ")", "-", "1", "!=", "embedding_dim", ":", "\n", "# Sometimes there are funny unicode parsing problems that lead to different", "\n", "# fields lengths (e.g., a word with a unicode space character that splits", "\n", "# into more than one column).  We skip those lines.  Note that if you have", "\n", "# some kind of long header, this could result in all of your lines getting", "\n", "# skipped.  It's hard to check for that here; you just have to look in the", "\n", "# embedding_misses_file and at the model summary to make sure things look", "\n", "# like they are supposed to.", "\n", "                ", "logger", ".", "warning", "(", "\"Found line with wrong number of dimensions (expected %d, was %d): %s\"", ",", "\n", "embedding_dim", ",", "len", "(", "fields", ")", "-", "1", ",", "line", ")", "\n", "continue", "\n", "", "word", "=", "fields", "[", "0", "]", "\n", "if", "word", "in", "words_to_keep", ":", "\n", "                ", "vector", "=", "numpy", ".", "asarray", "(", "fields", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "embeddings", "[", "word", "]", "=", "vector", "\n", "\n", "", "", "", "if", "not", "embeddings", ":", "\n", "        ", "raise", "ValueError", "(", "\"No embeddings of correct dimension found; you probably \"", "\n", "\"misspecified your embedding_dim parameter, or didn't \"", "\n", "\"pre-populate your Vocabulary\"", ")", "\n", "\n", "", "all_embeddings", "=", "numpy", ".", "asarray", "(", "list", "(", "embeddings", ".", "values", "(", ")", ")", ")", "\n", "embeddings_mean", "=", "float", "(", "numpy", ".", "mean", "(", "all_embeddings", ")", ")", "\n", "embeddings_std", "=", "float", "(", "numpy", ".", "std", "(", "all_embeddings", ")", ")", "\n", "# Now we initialize the weight matrix for an embedding layer, starting with random vectors,", "\n", "# then filling in the word vectors we just read.", "\n", "logger", ".", "info", "(", "\"Initializing pre-trained embedding layer\"", ")", "\n", "embedding_matrix", "=", "torch", ".", "FloatTensor", "(", "vocab_size", ",", "embedding_dim", ")", ".", "normal_", "(", "embeddings_mean", ",", "\n", "embeddings_std", ")", "\n", "\n", "for", "i", "in", "Tqdm", ".", "tqdm", "(", "range", "(", "0", ",", "vocab_size", ")", ")", ":", "\n", "        ", "word", "=", "vocab", ".", "get_token_from_index", "(", "i", ",", "namespace", ")", "\n", "\n", "# If we don't have a pre-trained vector for this word, we'll just leave this row alone,", "\n", "# so the word has a random initialization.", "\n", "if", "word", "in", "embeddings", ":", "\n", "            ", "embedding_matrix", "[", "i", "]", "=", "torch", ".", "FloatTensor", "(", "embeddings", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "debug", "(", "\"Word %s was not found in the embedding file. Initialising randomly.\"", ",", "word", ")", "\n", "\n", "# The weight matrix is initialized, so we construct and return the actual Embedding.", "\n", "", "", "return", "embedding_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.build_vocab_embeddings": [[583, 609], ["zip", "isinstance", "exvocab.read_normal_embedding_file", "weight_list.append", "vocab.save_to_files", "pathlib.Path().mkdir", "zip", "len", "isinstance", "len", "len", "torch.save", "len", "len", "len", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.read_normal_embedding_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.save_to_files"], ["", "def", "build_vocab_embeddings", "(", "vocab", ":", "ExVocabulary", ",", "external_embd_path", ",", "embd_dim", ",", "namespaces", "=", "None", ",", "saved_path", "=", "None", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "external_embd_path", ",", "list", ")", ":", "\n", "        ", "external_embd_path", "=", "[", "external_embd_path", "]", "\n", "\n", "", "weight_list", "=", "[", "]", "\n", "if", "namespaces", "is", "None", ":", "\n", "        ", "namespaces", "=", "[", "'tokens'", "]", "*", "len", "(", "external_embd_path", ")", "\n", "", "elif", "not", "isinstance", "(", "namespaces", ",", "list", ")", ":", "\n", "        ", "namespaces", "=", "[", "namespaces", "]", "*", "len", "(", "external_embd_path", ")", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "namespaces", ")", "==", "len", "(", "external_embd_path", ")", "\n", "\n", "", "for", "external_embd_file", ",", "namespace", "in", "zip", "(", "external_embd_path", ",", "namespaces", ")", ":", "\n", "        ", "weight", "=", "read_normal_embedding_file", "(", "external_embd_file", ",", "\n", "embd_dim", ",", "vocab", ",", "namespace", "=", "namespace", ")", "\n", "weight_list", ".", "append", "(", "weight", ")", "\n", "\n", "", "if", "saved_path", "is", "not", "None", ":", "\n", "        ", "vocab", ".", "save_to_files", "(", "saved_path", ")", "\n", "Path", "(", "saved_path", "/", "\"weights\"", ")", ".", "mkdir", "(", "parents", "=", "True", ")", "\n", "\n", "assert", "len", "(", "weight_list", ")", "==", "len", "(", "external_embd_path", ")", "\n", "\n", "for", "weight", ",", "external_embd_file", "in", "zip", "(", "weight_list", ",", "external_embd_path", ")", ":", "\n", "            ", "embd_name", "=", "Path", "(", "external_embd_file", ")", ".", "stem", "\n", "torch", ".", "save", "(", "weight", ",", "saved_path", "/", "\"weights\"", "/", "embd_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.load_vocab_embeddings": [[611, 619], ["exvocab.ExVocabulary.from_files", "pathlib.Path().iterdir", "torch.load", "pathlib.Path", "str"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.from_files"], ["", "", "", "def", "load_vocab_embeddings", "(", "saved_path", ")", ":", "\n", "    ", "vocab", "=", "ExVocabulary", ".", "from_files", "(", "saved_path", ")", "\n", "weight_dict", "=", "{", "}", "\n", "for", "file", "in", "Path", "(", "saved_path", "/", "\"weights\"", ")", ".", "iterdir", "(", ")", ":", "\n", "        ", "weight_dict", "[", "str", "(", "file", ".", "name", ")", "]", "=", "torch", ".", "load", "(", "file", ")", "\n", "# assert vocab.get_vocab_size('tokens') == int(weight_dict[str(file.name)].size(0))", "\n", "\n", "", "return", "vocab", ",", "weight_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_reader.BertReaderFeverNLI.__init__": [[51, 66], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "s1_l", "=", "256", ",", "\n", "s2_l", "=", "64", ",", "\n", "max_l", "=", "300", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "s1_l", "=", "s1_l", "# This is the max_length of whole sequence", "\n", "self", ".", "s2_l", "=", "s2_l", "# This is the max_length of whole sequence", "\n", "self", ".", "max_l", "=", "max_l", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_reader.BertReaderFeverNLI._read": [[67, 94], ["logger.info", "str", "bert_fever_reader.BertReaderFeverNLI._example_filter", "len", "bert_fever_reader.BertReaderFeverNLI.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query sentence instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "label", "=", "example", "[", "\"label\"", "]", "\n", "\n", "if", "label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "evid", "=", "example", "[", "'evid'", "]", "\n", "sentence1", "=", "' '", ".", "join", "(", "e", "[", "0", "]", "for", "e", "in", "evid", ")", "\n", "sentence2", ":", "str", "=", "example", "[", "\"claim\"", "]", "# Question go first according to BERT paper.", "\n", "# sentence2: str = example[\"evid\"]", "\n", "\n", "if", "len", "(", "sentence1", ")", "==", "0", ":", "\n", "                ", "sentence1", "=", "\"EEMMPPTTYY\"", "\n", "\n", "", "pid", "=", "str", "(", "example", "[", "'id'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "sentence1", ",", "sentence2", ",", "pid", ",", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_reader.BertReaderFeverNLI.text_to_instance": [[95, 135], ["bert_fever_reader.BertReaderFeverNLI.bert_tokenizer.tokenize", "bert_fever_reader.BertReaderFeverNLI.bert_tokenizer.tokenize", "bert_fever_reader.BertReaderFeverNLI.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "allennlp.data.fields.LabelField", "data_utils.customized_field.IdField", "len", "len", "len", "range", "range"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "sent1", ":", "str", ",", "# Important type information", "\n", "sent2", ":", "str", ",", "\n", "pid", ":", "str", "=", "None", ",", "\n", "label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "sent1", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "sent2", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "s1_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text1", ")", ")", "]", "\n", "\n", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "+", "[", "'[SEP]'", "]", "+", "tokenized_text2", "+", "[", "'[SEP]'", "]", "\n", "text1_len", "=", "len", "(", "tokenized_text1", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "tokenized_text2", ")", "+", "1", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "if", "label", ":", "\n", "            ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "if", "pid", ":", "\n", "            ", "fields", "[", "'pid'", "]", "=", "IdField", "(", "pid", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_reader._truncate_seq_pair": [[29, 44], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_nli.BertFeverNLIReader.__init__": [[30, 49], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "query_l", "=", "64", ",", "\n", "context_l", "=", "300", ",", "\n", "max_l", "=", "128", ",", "\n", "is_paired", "=", "True", ",", "\n", "pair_order", "=", "'qc'", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "query_l", "=", "query_l", "# This is the max_length of whole sequence", "\n", "self", ".", "context_l", "=", "context_l", "# This is the max_length of whole sequence", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "self", ".", "is_paired", "=", "is_paired", "\n", "self", ".", "max_l", "=", "max_l", "\n", "self", ".", "pair_order", "=", "pair_order", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_nli.BertFeverNLIReader._read": [[50, 88], ["logger.info", "str", "str", "bert_reader_nli.BertFeverNLIReader._example_filter", "len", "len", "len", "bert_reader_nli.BertFeverNLIReader.text_to_instance", "ValueError", "bert_reader_nli.BertFeverNLIReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query-context instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "selection_label", "=", "example", "[", "\"label\"", "]", "\n", "\n", "if", "selection_label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "query", ":", "str", "=", "example", "[", "\"query\"", "]", "# Question go first according to BERT paper.", "\n", "# truncate premise", "\n", "context", ":", "str", "=", "example", "[", "\"context\"", "]", "\n", "\n", "if", "len", "(", "context", ")", "==", "0", ":", "\n", "                ", "context", "=", "'empty'", "\n", "\n", "# print(example)", "\n", "", "retain_item", "=", "None", "# Change this to some other name for flexibility.", "\n", "\n", "assert", "len", "(", "query", ")", "!=", "0", "\n", "assert", "len", "(", "context", ")", "!=", "0", "\n", "\n", "fid", "=", "str", "(", "example", "[", "'fid'", "]", ")", "\n", "oid", "=", "str", "(", "example", "[", "'cid'", "]", ")", "\n", "\n", "if", "self", ".", "pair_order", "==", "'qc'", ":", "\n", "                ", "yield", "self", ".", "text_to_instance", "(", "query", ",", "context", ",", "fid", ",", "oid", ",", "retain_item", ",", "selection_label", ")", "\n", "", "elif", "self", ".", "pair_order", "==", "'cq'", ":", "\n", "                ", "yield", "self", ".", "text_to_instance", "(", "context", ",", "query", ",", "fid", ",", "oid", ",", "retain_item", ",", "selection_label", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"No valid pair ordering. {self.pair_order}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_nli.BertFeverNLIReader.text_to_instance": [[89, 176], ["bert_reader_nli.BertFeverNLIReader.bert_tokenizer.tokenize", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.tokenize", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.tokenize", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.tokenize", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.convert_tokens_to_ids", "bert_reader_nli.BertFeverNLIReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "allennlp.data.fields.LabelField", "numpy.asarray", "numpy.asarray", "len", "len", "allennlp.data.fields.LabelField", "ValueError", "len", "len", "range", "range", "len", "len"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "seq1", ":", "str", ",", "# Important type information", "\n", "seq2", ":", "str", ",", "\n", "fid", ":", "str", "=", "None", ",", "\n", "oid", ":", "str", "=", "None", ",", "\n", "retain_item", ":", "str", "=", "None", ",", "\n", "selection_label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "if", "self", ".", "is_paired", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "seq1", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "seq2", ")", "\n", "\n", "if", "self", ".", "pair_order", "==", "'qc'", ":", "\n", "                ", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text1", ")", ")", "]", "\n", "", "elif", "self", ".", "pair_order", "==", "'cq'", ":", "\n", "                ", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text2", ")", ")", "]", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "f\"No valid pair ordering. {self.pair_order}\"", ")", "\n", "\n", "", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "+", "[", "'[SEP]'", "]", "+", "tokenized_text2", "+", "[", "'[SEP]'", "]", "\n", "text1_len", "=", "len", "(", "tokenized_text1", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "tokenized_text2", ")", "+", "1", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "oid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'oid'", "]", "=", "IdField", "(", "oid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "seq1", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "seq2", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "context_l", "]", "\n", "\n", "s1_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "\n", "s2_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text2", "\n", "\n", "s1_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s1_tokens_seq", ")", "\n", "s2_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s2_tokens_seq", ")", "\n", "\n", "fields", "[", "'s1_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s1_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'s2_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s2_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "1", ",", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'s1_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'s2_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "oid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'oid'", "]", "=", "IdField", "(", "oid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_context_selection.BertContentSelectionReader.__init__": [[34, 47], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "query_l", "=", "64", ",", "\n", "context_l", "=", "300", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "query_l", "=", "query_l", "# This is the max_length of whole sequence", "\n", "self", ".", "context_l", "=", "context_l", "# This is the max_length of whole sequence", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_context_selection.BertContentSelectionReader._read": [[48, 75], ["logger.info", "str", "str", "bert_reader_context_selection.BertContentSelectionReader._example_filter", "len", "len", "bert_reader_context_selection.BertContentSelectionReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query-context instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "selection_label", "=", "example", "[", "\"selection_label\"", "]", "\n", "\n", "if", "selection_label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "query", ":", "str", "=", "example", "[", "\"query\"", "]", "# Question go first according to BERT paper.", "\n", "# truncate premise", "\n", "context", ":", "str", "=", "example", "[", "\"context\"", "]", "\n", "\n", "assert", "len", "(", "query", ")", "!=", "0", "\n", "assert", "len", "(", "context", ")", "!=", "0", "\n", "\n", "fid", "=", "str", "(", "example", "[", "'fid'", "]", ")", "\n", "qid", "=", "str", "(", "example", "[", "'qid'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "query", ",", "context", ",", "fid", ",", "qid", ",", "selection_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_context_selection.BertContentSelectionReader.text_to_instance": [[76, 122], ["bert_reader_context_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_context_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_context_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "bert_reader_context_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "numpy.asarray", "numpy.asarray", "len", "len", "allennlp.data.fields.LabelField"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "query", ":", "str", ",", "# Important type information", "\n", "context", ":", "str", ",", "\n", "fid", ":", "str", "=", "None", ",", "\n", "qid", ":", "str", "=", "None", ",", "\n", "selection_label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "query", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "context", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "context_l", "]", "\n", "\n", "s1_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "\n", "s2_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text2", "\n", "\n", "# text1_len = len(tokenized_text1) + 1", "\n", "# text2_len = len(tokenized_text2) + 1", "\n", "\n", "# segments_ids = [0 for _ in range(text1_len)] + [1 for _ in range(text2_len)]", "\n", "\n", "s1_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s1_tokens_seq", ")", "\n", "s2_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s2_tokens_seq", ")", "\n", "\n", "fields", "[", "'s1_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s1_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'s2_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s2_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "1", ",", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'s1_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'s2_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "            ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "qid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'qid'", "]", "=", "IdField", "(", "qid", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.span_pred_reader.BertSpanPredReader.__init__": [[29, 40], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "max_query_length", "=", "64", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "self", ".", "max_query_length", "=", "max_query_length", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.span_pred_reader.BertSpanPredReader._read": [[41, 54], ["logger.info", "span_pred_reader.BertSpanPredReader.bert_tokenizer.tokenize", "span_pred_reader.BertSpanPredReader._example_filter", "span_pred_reader.BertSpanPredReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query sentence instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "query_tokens", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "example", "[", "'o_query'", "]", ")", "\n", "query_tokens", "=", "query_tokens", "[", ":", "self", ".", "max_query_length", "]", "\n", "example", "[", "'query_c_tokens'", "]", "=", "query_tokens", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "example", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.span_pred_reader.BertSpanPredReader.text_to_instance": [[55, 92], ["span_pred_reader.BertSpanPredReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "len", "len", "range", "range"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "example", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "example", "[", "'query_c_tokens'", "]", "+", "[", "'[SEP]'", "]", "+", "example", "[", "'context_c_tokens'", "]", "+", "[", "'[SEP]'", "]", "\n", "assert", "len", "(", "joint_tokens_seq", ")", "<", "512", "\n", "\n", "text1_len", "=", "len", "(", "example", "[", "'query_c_tokens'", "]", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "example", "[", "'context_c_tokens'", "]", ")", "+", "1", "\n", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "# This text span is begin inclusive and end exclusive.", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "example", "[", "'query_c_tokens'", "]", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "example", "[", "'context_c_tokens'", "]", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "# fields['bert_s2_span'] = SpanField(text2_span)", "\n", "# fields['bert_s1_span'] = MetadataField(text1_span)", "\n", "# fields['bert_s2_span'] = MetadataField(text2_span)", "\n", "\n", "# However, the ground truth span is begin and end both inclusive", "\n", "fields", "[", "'gt_span'", "]", "=", "SpanField", "(", "example", "[", "'start_position'", "]", ",", "example", "[", "'end_position'", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "example", "[", "'fid'", "]", ")", "\n", "fields", "[", "'uid'", "]", "=", "IdField", "(", "example", "[", "'uid'", "]", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader.__init__": [[34, 47], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "s1_l", "=", "256", ",", "\n", "s2_l", "=", "64", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "s1_l", "=", "s1_l", "# This is the max_length of whole sequence", "\n", "self", ".", "s2_l", "=", "s2_l", "# This is the max_length of whole sequence", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader._read": [[48, 75], ["logger.info", "str", "bert_fever_verification_separate_seq.BertSeparateSeqReader._example_filter", "len", "bert_fever_verification_separate_seq.BertSeparateSeqReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query-context instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "label", "=", "example", "[", "\"label\"", "]", "\n", "\n", "if", "label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "evid", "=", "example", "[", "'evid'", "]", "\n", "sentence1", "=", "' '", ".", "join", "(", "e", "[", "0", "]", "for", "e", "in", "evid", ")", "\n", "sentence2", ":", "str", "=", "example", "[", "\"claim\"", "]", "\n", "# sentence2: str = example[\"evid\"]", "\n", "\n", "if", "len", "(", "sentence1", ")", "==", "0", ":", "\n", "                ", "sentence1", "=", "\"EEMMPPTTYY\"", "\n", "\n", "", "pid", "=", "str", "(", "example", "[", "'id'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "sentence1", ",", "sentence2", ",", "pid", ",", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader.text_to_instance": [[76, 121], ["bert_fever_verification_separate_seq.BertSeparateSeqReader.bert_tokenizer.tokenize", "bert_fever_verification_separate_seq.BertSeparateSeqReader.bert_tokenizer.tokenize", "bert_fever_verification_separate_seq.BertSeparateSeqReader.bert_tokenizer.convert_tokens_to_ids", "bert_fever_verification_separate_seq.BertSeparateSeqReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "numpy.asarray", "numpy.asarray", "len", "len", "allennlp.data.fields.LabelField"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "s1", ":", "str", ",", "# Important type information", "\n", "s2", ":", "str", ",", "\n", "pid", ":", "str", ",", "\n", "selection_label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "s1", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "s2", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "s1_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "s2_l", "]", "\n", "\n", "s1_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "\n", "s2_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text2", "\n", "\n", "# text1_len = len(tokenized_text1) + 1", "\n", "# text2_len = len(tokenized_text2) + 1", "\n", "\n", "# segments_ids = [0 for _ in range(text1_len)] + [1 for _ in range(text2_len)]", "\n", "\n", "s1_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s1_tokens_seq", ")", "\n", "s2_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s2_tokens_seq", ")", "\n", "\n", "fields", "[", "'s1_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s1_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'s2_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s2_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "1", ",", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'s1_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'s2_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "            ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "# assert fid is not None", "\n", "", "assert", "pid", "is", "not", "None", "\n", "# fields['fid'] = IdField(fid)", "\n", "fields", "[", "'pid'", "]", "=", "IdField", "(", "pid", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_fever_sent_selection.BertContentSelectionReader.__init__": [[30, 47], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "query_l", "=", "64", ",", "\n", "context_l", "=", "300", ",", "\n", "max_l", "=", "128", ",", "\n", "is_paired", "=", "True", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "query_l", "=", "query_l", "# This is the max_length of whole sequence", "\n", "self", ".", "context_l", "=", "context_l", "# This is the max_length of whole sequence", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "self", ".", "is_paired", "=", "is_paired", "\n", "self", ".", "max_l", "=", "max_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_fever_sent_selection.BertContentSelectionReader._read": [[48, 78], ["logger.info", "str", "str", "bert_reader_fever_sent_selection.BertContentSelectionReader._example_filter", "len", "len", "bert_reader_fever_sent_selection.BertContentSelectionReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query-context instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "selection_label", "=", "example", "[", "\"s_labels\"", "]", "\n", "\n", "if", "selection_label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "query", ":", "str", "=", "example", "[", "\"query\"", "]", "# Question go first according to BERT paper.", "\n", "# truncate premise", "\n", "context", ":", "str", "=", "example", "[", "\"context\"", "]", "\n", "\n", "# print(example)", "\n", "retain_item", "=", "example", "[", "\"sid\"", "]", "# Change this to some other name for flexibility.", "\n", "\n", "assert", "len", "(", "query", ")", "!=", "0", "\n", "assert", "len", "(", "context", ")", "!=", "0", "\n", "\n", "fid", "=", "str", "(", "example", "[", "'fid'", "]", ")", "\n", "oid", "=", "str", "(", "example", "[", "'cid'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "query", ",", "context", ",", "fid", ",", "oid", ",", "retain_item", ",", "selection_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_fever_sent_selection.BertContentSelectionReader.text_to_instance": [[79, 160], ["bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "bert_reader_fever_sent_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "allennlp.data.fields.LabelField", "numpy.asarray", "numpy.asarray", "len", "len", "allennlp.data.fields.LabelField", "len", "len", "len", "range", "range"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "query", ":", "str", ",", "# Important type information", "\n", "context", ":", "str", ",", "\n", "fid", ":", "str", "=", "None", ",", "\n", "oid", ":", "str", "=", "None", ",", "\n", "retain_item", ":", "str", "=", "None", ",", "\n", "selection_label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "if", "self", ".", "is_paired", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "query", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "context", ")", "\n", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text1", ")", ")", "]", "\n", "\n", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "+", "[", "'[SEP]'", "]", "+", "tokenized_text2", "+", "[", "'[SEP]'", "]", "\n", "text1_len", "=", "len", "(", "tokenized_text1", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "tokenized_text2", ")", "+", "1", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "oid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'oid'", "]", "=", "IdField", "(", "oid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "query", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "context", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "context_l", "]", "\n", "\n", "s1_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "\n", "s2_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text2", "\n", "\n", "s1_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s1_tokens_seq", ")", "\n", "s2_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s2_tokens_seq", ")", "\n", "\n", "fields", "[", "'s1_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s1_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'s2_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s2_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "1", ",", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'s1_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'s2_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "oid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'oid'", "]", "=", "IdField", "(", "oid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_content_selection.BertContentSelectionReader.__init__": [[30, 49], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "query_l", "=", "64", ",", "\n", "context_l", "=", "300", ",", "\n", "max_l", "=", "364", ",", "\n", "is_paired", "=", "True", ",", "\n", "element_fieldname", "=", "'doc_t'", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "query_l", "=", "query_l", "# This is the max_length of whole sequence", "\n", "self", ".", "context_l", "=", "context_l", "# This is the max_length of whole sequence", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "self", ".", "is_paired", "=", "is_paired", "\n", "self", ".", "max_l", "=", "max_l", "\n", "self", ".", "element_fieldname", "=", "element_fieldname", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_content_selection.BertContentSelectionReader._read": [[50, 80], ["logger.info", "str", "str", "bert_reader_content_selection.BertContentSelectionReader._example_filter", "len", "len", "bert_reader_content_selection.BertContentSelectionReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query-context instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "selection_label", "=", "example", "[", "\"s_labels\"", "]", "\n", "\n", "if", "selection_label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "query", ":", "str", "=", "example", "[", "\"query\"", "]", "# Question go first according to BERT paper.", "\n", "# truncate premise", "\n", "context", ":", "str", "=", "example", "[", "\"context\"", "]", "\n", "\n", "# print(example)", "\n", "retain_item", "=", "example", "[", "self", ".", "element_fieldname", "]", "# Change this to some other name for flexibility.", "\n", "\n", "assert", "len", "(", "query", ")", "!=", "0", "\n", "assert", "len", "(", "context", ")", "!=", "0", "\n", "\n", "fid", "=", "str", "(", "example", "[", "'fid'", "]", ")", "\n", "qid", "=", "str", "(", "example", "[", "'qid'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "query", ",", "context", ",", "fid", ",", "qid", ",", "retain_item", ",", "selection_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_content_selection.BertContentSelectionReader.text_to_instance": [[81, 162], ["bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.tokenize", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "bert_reader_content_selection.BertContentSelectionReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "allennlp.data.fields.LabelField", "numpy.asarray", "numpy.asarray", "len", "len", "allennlp.data.fields.LabelField", "len", "len", "len", "range", "range"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "query", ":", "str", ",", "# Important type information", "\n", "context", ":", "str", ",", "\n", "fid", ":", "str", "=", "None", ",", "\n", "qid", ":", "str", "=", "None", ",", "\n", "retain_item", ":", "str", "=", "None", ",", "\n", "selection_label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "if", "self", ".", "is_paired", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "query", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "context", ")", "\n", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text1", ")", ")", "]", "\n", "\n", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "+", "[", "'[SEP]'", "]", "+", "tokenized_text2", "+", "[", "'[SEP]'", "]", "\n", "text1_len", "=", "len", "(", "tokenized_text1", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "tokenized_text2", ")", "+", "1", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "qid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'qid'", "]", "=", "IdField", "(", "qid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n", "", "else", ":", "\n", "            ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "query", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "context", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "self", ".", "context_l", "]", "\n", "\n", "s1_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "\n", "s2_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text2", "\n", "\n", "s1_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s1_tokens_seq", ")", "\n", "s2_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "s2_tokens_seq", ")", "\n", "\n", "fields", "[", "'s1_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s1_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'s2_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "s2_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "1", ",", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "SpanField", "(", "text1_span", "[", "0", "]", ",", "text1_span", "[", "1", "]", ",", "fields", "[", "'s1_sequence'", "]", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "SpanField", "(", "text2_span", "[", "0", "]", ",", "text2_span", "[", "1", "]", ",", "fields", "[", "'s2_sequence'", "]", ")", "\n", "\n", "if", "selection_label", ":", "\n", "                ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "selection_label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "assert", "fid", "is", "not", "None", "\n", "assert", "qid", "is", "not", "None", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "fid", ")", "\n", "fields", "[", "'qid'", "]", "=", "IdField", "(", "qid", ")", "\n", "fields", "[", "'item'", "]", "=", "IdField", "(", "retain_item", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_sent_selection.BertReaderSentM.__init__": [[51, 66], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "example_filter", "=", "None", ",", "\n", "query_l", "=", "64", ",", "\n", "context_l", "=", "256", ",", "\n", "max_l", "=", "300", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "self", ".", "query_l", "=", "query_l", "# This is the max_length of whole sequence", "\n", "self", ".", "context_l", "=", "context_l", "# This is the max_length of whole sequence", "\n", "self", ".", "max_l", "=", "max_l", "\n", "self", ".", "bert_tokenizer", ":", "BertTokenizer", "=", "bert_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_sent_selection.BertReaderSentM._read": [[67, 93], ["logger.info", "str", "bert_reader_sent_selection.BertReaderSentM._example_filter", "len", "bert_reader_sent_selection.BertReaderSentM.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query sentence instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "label", "=", "example", "[", "\"label\"", "]", "\n", "\n", "if", "label", "==", "'-'", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "# We use binary parse here", "\n", "# first element is the sentence and the second is the upstream semantic relatedness score.", "\n", "\n", "", "sentence1", ":", "str", "=", "example", "[", "\"question\"", "]", "# Question go first according to BERT paper.", "\n", "# truncate premise", "\n", "sentence2", ":", "str", "=", "example", "[", "\"sentence\"", "]", "\n", "\n", "if", "len", "(", "sentence1", ")", "==", "0", ":", "\n", "                ", "sentence1", "=", "\"EEMMPPTTYY\"", "\n", "\n", "", "pid", "=", "str", "(", "example", "[", "'selection_id'", "]", ")", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "sentence1", ",", "sentence2", ",", "pid", ",", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_sent_selection.BertReaderSentM.text_to_instance": [[94, 134], ["bert_reader_sent_selection.BertReaderSentM.bert_tokenizer.tokenize", "bert_reader_sent_selection.BertReaderSentM.bert_tokenizer.tokenize", "bert_reader_sent_selection.BertReaderSentM.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.MetadataField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "len", "len", "len", "len", "numpy.asarray", "numpy.asarray", "allennlp.data.fields.LabelField", "data_utils.customized_field.IdField", "len", "len", "len", "range", "range"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "sent1", ":", "str", ",", "# Important type information", "\n", "sent2", ":", "str", ",", "\n", "pid", ":", "str", "=", "None", ",", "\n", "label", ":", "str", "=", "None", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokenized_text1", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "sent1", ")", "\n", "tokenized_text2", "=", "self", ".", "bert_tokenizer", ".", "tokenize", "(", "sent2", ")", "\n", "\n", "# _truncate_seq_pair(tokenized_text1, tokenized_text2, self.max_l)", "\n", "tokenized_text1", "=", "tokenized_text1", "[", ":", "self", ".", "query_l", "]", "\n", "tokenized_text2", "=", "tokenized_text2", "[", ":", "(", "self", ".", "max_l", "-", "len", "(", "tokenized_text1", ")", ")", "]", "\n", "\n", "joint_tokens_seq", "=", "[", "'[CLS]'", "]", "+", "tokenized_text1", "+", "[", "'[SEP]'", "]", "+", "tokenized_text2", "+", "[", "'[SEP]'", "]", "\n", "text1_len", "=", "len", "(", "tokenized_text1", ")", "+", "2", "\n", "text2_len", "=", "len", "(", "tokenized_text2", ")", "+", "1", "\n", "segments_ids", "=", "[", "0", "for", "_", "in", "range", "(", "text1_len", ")", "]", "+", "[", "1", "for", "_", "in", "range", "(", "text2_len", ")", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "text1_span", "=", "(", "1", ",", "1", "+", "len", "(", "tokenized_text1", ")", ")", "# End is exclusive (important for later use)", "\n", "text2_span", "=", "(", "text1_span", "[", "1", "]", "+", "1", ",", "text1_span", "[", "1", "]", "+", "1", "+", "len", "(", "tokenized_text2", ")", ")", "\n", "\n", "fields", "[", "'bert_s1_span'", "]", "=", "MetadataField", "(", "text1_span", ")", "\n", "fields", "[", "'bert_s2_span'", "]", "=", "MetadataField", "(", "text2_span", ")", "\n", "\n", "if", "label", ":", "\n", "            ", "fields", "[", "'label'", "]", "=", "LabelField", "(", "label", ",", "label_namespace", "=", "'labels'", ")", "\n", "\n", "", "if", "pid", ":", "\n", "            ", "fields", "[", "'pid'", "]", "=", "IdField", "(", "pid", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.bert_reader_sent_selection._truncate_seq_pair": [[29, 44], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.__init__": [[29, 38], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["def", "__init__", "(", "self", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", "bert_tokenizer", ":", "BertTokenizer", "=", "None", ",", "\n", "example_filter", "=", "None", ")", "->", "None", ":", "\n", "\n", "# max_l indicate the max length of each individual sentence.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", "=", "lazy", ")", "\n", "self", ".", "bert_tokenizer", "=", "bert_tokenizer", "\n", "self", ".", "_example_filter", "=", "example_filter", "# If filter equals True, then we delete this example", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader._read": [[39, 48], ["logger.info", "paired_span_pred_reader.BertPairedSpanPredReader._example_filter", "paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "data_list", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading query sentence instances from upstream sampler\"", ")", "\n", "for", "example", "in", "data_list", ":", "\n", "\n", "            ", "if", "self", ".", "_example_filter", "is", "not", "None", "and", "self", ".", "_example_filter", "(", "example", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "yield", "self", ".", "text_to_instance", "(", "example", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.readers.paired_span_pred_reader.BertPairedSpanPredReader.text_to_instance": [[49, 83], ["paired_span_pred_reader.BertPairedSpanPredReader.bert_tokenizer.convert_tokens_to_ids", "data_utils.customized_field.BertIndexField", "data_utils.customized_field.BertIndexField", "allennlp.data.fields.SpanField", "data_utils.customized_field.IdField", "data_utils.customized_field.IdField", "allennlp.data.instance.Instance", "len", "len", "len", "numpy.asarray", "numpy.asarray"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "example", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "joint_tokens_seq", "=", "example", "[", "'paired_c_tokens'", "]", "\n", "assert", "len", "(", "joint_tokens_seq", ")", "<=", "512", "\n", "\n", "segments_ids", "=", "example", "[", "'segment_ids'", "]", "\n", "\n", "joint_tokens_ids", "=", "self", ".", "bert_tokenizer", ".", "convert_tokens_to_ids", "(", "joint_tokens_seq", ")", "\n", "assert", "len", "(", "joint_tokens_ids", ")", "==", "len", "(", "segments_ids", ")", "\n", "\n", "fields", "[", "'paired_sequence'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "joint_tokens_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "fields", "[", "'paired_segments_ids'", "]", "=", "BertIndexField", "(", "np", ".", "asarray", "(", "segments_ids", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "# This text span is begin inclusive and end exclusive.", "\n", "# text1_span = (1, 1 + len(example['query_c_tokens'])) # End is exclusive (important for later use)", "\n", "# text2_span = (text1_span[1] + 1, text1_span[1] + 1 + len(example['context_c_tokens']))", "\n", "\n", "# fields['bert_s1_span'] = SpanField(text1_span[0], text1_span[1], fields['paired_sequence'])", "\n", "# fields['bert_s2_span'] = SpanField(text2_span[0], text2_span[1], fields['paired_sequence'])", "\n", "# fields['bert_s2_span'] = SpanField(text2_span)", "\n", "# fields['bert_s1_span'] = MetadataField(text1_span)", "\n", "# fields['bert_s2_span'] = MetadataField(text2_span)", "\n", "\n", "# However, the ground truth span is begin and end both inclusive", "\n", "fields", "[", "'gt_span'", "]", "=", "SpanField", "(", "example", "[", "'start_position'", "]", ",", "example", "[", "'end_position'", "]", ",", "fields", "[", "'paired_sequence'", "]", ")", "\n", "\n", "fields", "[", "'fid'", "]", "=", "IdField", "(", "example", "[", "'fid'", "]", ")", "\n", "fields", "[", "'uid'", "]", "=", "IdField", "(", "example", "[", "'uid'", "]", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.BertMultiLayerPooler.__init__": [[42, 47], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "act_type", "=", "\"gelu\"", ",", "num_of_pooling_layer", "=", "4", ")", ":", "\n", "        ", "super", "(", "BertMultiLayerPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_of_pooling_layer", "=", "num_of_pooling_layer", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "hidden_size", "*", "num_of_pooling_layer", ",", "hidden_size", "*", "num_of_pooling_layer", ")", "\n", "self", ".", "activation", "=", "ACT2FN", "[", "act_type", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.BertMultiLayerPooler.forward": [[48, 63], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_multilayer_output.BertMultiLayerPooler.dense", "bert_multilayer_output.BertMultiLayerPooler.activation", "first_token_list.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking multiple layers of the hidden state corresponding", "\n", "# to the first token.", "\n", "# first_token_tensor = hidden_states[:, 0]", "\n", "        ", "pooled_layer_list", "=", "hidden_states", "[", "-", "self", ".", "num_of_pooling_layer", ":", "]", "\n", "first_token_list", "=", "[", "]", "\n", "\n", "for", "selected_layer", "in", "pooled_layer_list", ":", "\n", "            ", "first_token_list", ".", "append", "(", "selected_layer", "[", ":", ",", "0", "]", ")", "\n", "\n", "", "concatenated_first_token_output", "=", "torch", ".", "cat", "(", "first_token_list", ",", "dim", "=", "1", ")", "\n", "\n", "pooled_output", "=", "self", ".", "dense", "(", "concatenated_first_token_output", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification.__init__": [[70, 98], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "bert_multilayer_output.BertMultiLayerSeqClassification.classifier.apply", "bert_multilayer_output.BertMultiLayerPooler", "bert_multilayer_output.BertMultiLayerSeqClassification.multilayer_pooler.apply", "ValueError"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.apply", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.apply"], ["", "def", "__init__", "(", "self", ",", "bert_encoder", ":", "BertModel", ",", "num_labels", ",", "num_of_pooling_layer", "=", "4", ",", "act_type", "=", "'gelu'", ",", "\n", "use_pretrained_pooler", "=", "False", ",", "use_sigmoid", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", "BertMultiLayerSeqClassification", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "self", ".", "use_pretrained_pooler", "=", "use_pretrained_pooler", "\n", "if", "not", "self", ".", "use_pretrained_pooler", ":", "\n", "            ", "self", ".", "multilayer_pooler", "=", "BertMultiLayerPooler", "(", "bert_encoder", ".", "config", ".", "hidden_size", ",", "act_type", ",", "\n", "num_of_pooling_layer", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_of_pooling_layer", "=", "1", "# If we use pretrained pooler, we can only use the last layer", "\n", "self", ".", "multilayer_pooler", "=", "self", ".", "bert_encoder", ".", "pooler", "\n", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "bert_encoder", ".", "config", ".", "hidden_size", "*", "num_of_pooling_layer", ",", "num_labels", ")", "\n", "\n", "# Apply init value for model except bert_encoder", "\n", "if", "not", "self", ".", "use_pretrained_pooler", ":", "\n", "            ", "self", ".", "multilayer_pooler", ".", "apply", "(", "init_bert_weights", ")", "\n", "\n", "", "self", ".", "classifier", ".", "apply", "(", "init_bert_weights", ")", "\n", "\n", "self", ".", "use_sigmoid", "=", "False", "\n", "if", "num_labels", "==", "1", "and", "use_sigmoid", ":", "\n", "            ", "self", ".", "use_sigmoid", "=", "use_sigmoid", "\n", "", "elif", "num_labels", "!=", "1", "and", "use_sigmoid", ":", "\n", "            ", "raise", "ValueError", "(", "\"Can not use sigmoid when number of labels is 1.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification.forward": [[99, 126], ["bert_multilayer_output.BertMultiLayerSeqClassification.bert_encoder", "bert_multilayer_output.BertMultiLayerSeqClassification.dropout", "bert_multilayer_output.BertMultiLayerSeqClassification.classifier", "bert_multilayer_output.BertMultiLayerSeqClassification.multilayer_pooler", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss.", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "labels.unsqueeze().float", "bert_multilayer_output.BertMultiLayerSeqClassification.view", "labels.view", "labels.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "mode", "=", "ForwardMode", ".", "TRAIN", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert_encoder", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ")", "\n", "\n", "if", "not", "self", ".", "use_pretrained_pooler", ":", "\n", "            ", "multilayer_pooled_output", "=", "self", ".", "multilayer_pooler", "(", "sequence_output", ")", "\n", "", "else", ":", "\n", "            ", "multilayer_pooled_output", "=", "pooled_output", "\n", "\n", "", "dr_pooled_output", "=", "self", ".", "dropout", "(", "multilayer_pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "dr_pooled_output", ")", "\n", "\n", "if", "mode", "==", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "if", "self", ".", "use_sigmoid", ":", "\n", "                ", "assert", "labels", "is", "not", "None", "\n", "loss_fn", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "loss", "=", "loss_fn", "(", "logits", ",", "labels", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "# use softmax", "\n", "                ", "assert", "labels", "is", "not", "None", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "\n", "", "", "elif", "mode", "==", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.gelu": [[11, 17], ["torch.erf", "torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.swish": [[19, 21], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_multilayer_output.init_bert_weights": [[26, 39], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "function", ["None"], ["def", "init_bert_weights", "(", "module", ")", ":", "\n", "    ", "\"\"\" Initialize the weights.\n    \"\"\"", "\n", "initializer_range", "=", "0.02", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "        ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "        ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher.__init__": [[50, 77], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "ValueError", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["", "def", "__init__", "(", "self", ",", "bert_encoder", ":", "BertModel", ",", "num_of_class", ",", "act_type", "=", "\"gelu\"", ",", "dropout_rate", "=", "None", ",", "num_of_out_layers", "=", "4", ",", "\n", "use_sigmoid", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertPairMaxOutMatcher", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "self", ".", "num_of_out_layers", "=", "num_of_out_layers", "\n", "self", ".", "num_of_class", "=", "num_of_class", "\n", "\n", "self", ".", "matching_hidden_dimension", "=", "4", "*", "self", ".", "num_of_out_layers", "*", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", "\n", "self", ".", "matching_intermediate_dimension", "=", "self", ".", "bert_encoder", ".", "config", ".", "intermediate_size", "\n", "\n", "if", "dropout_rate", "is", "None", ":", "\n", "            ", "dropout_rate", "=", "self", ".", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", "\n", "\n", "", "self", ".", "matching_layer1", "=", "nn", ".", "Linear", "(", "self", ".", "matching_hidden_dimension", ",", "self", ".", "matching_intermediate_dimension", ")", "\n", "self", ".", "matching_layer2", "=", "nn", ".", "Linear", "(", "self", ".", "matching_intermediate_dimension", ",", "num_of_class", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "self", ".", "activation", "=", "ACT2FN", "[", "act_type", "]", "\n", "\n", "self", ".", "match_layers", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "matching_layer1", ",", "nn", ".", "ReLU", "(", ")", ",", "self", ".", "dropout", ",", "self", ".", "matching_layer2", "]", ")", "\n", "\n", "self", ".", "use_sigmoid", "=", "False", "\n", "if", "self", ".", "num_of_class", "==", "1", "and", "use_sigmoid", ":", "\n", "            ", "self", ".", "use_sigmoid", "=", "use_sigmoid", "\n", "", "elif", "self", ".", "num_of_class", "!=", "1", "and", "use_sigmoid", ":", "\n", "            ", "raise", "ValueError", "(", "\"Can not use sigmoid when number of labels is 1.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher.span_maxpool": [[78, 83], ["flint.span_select", "flint.span_select", "flint.max_along_time", "flint.max_along_time"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.span_select", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.span_select", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.max_along_time", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.max_along_time"], ["", "", "@", "staticmethod", "\n", "def", "span_maxpool", "(", "input_seq", ",", "span", ")", ":", "# [B, T, D]", "\n", "        ", "selected_seq", ",", "selected_length", "=", "span_util", ".", "span_select", "(", "input_seq", ",", "span", ")", "# [B, T, D]", "\n", "maxout_r", "=", "torch_util", ".", "max_along_time", "(", "selected_seq", ",", "selected_length", ")", "\n", "return", "maxout_r", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher.forward": [[84, 116], ["bert_maxout_clf.BertPairMaxOutMatcher.bert_encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_maxout_clf.BertPairMaxOutMatcher.span_maxpool", "bert_maxout_clf.BertPairMaxOutMatcher.span_maxpool", "bert_maxout_clf.BertPairMaxOutMatcher.dropout", "bert_maxout_clf.BertPairMaxOutMatcher.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_maxout_clf.BertPairMaxOutMatcher.dropout", "bert_maxout_clf.BertPairMaxOutMatcher.matching_layer2", "bert_maxout_clf.BertPairMaxOutMatcher.activation", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "bert_maxout_clf.BertPairMaxOutMatcher.matching_layer1", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss.", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "labels.unsqueeze().float", "bert_maxout_clf.BertPairMaxOutMatcher.view", "labels.view", "labels.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher.span_maxpool", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher.span_maxpool"], ["", "def", "forward", "(", "self", ",", "seq", ",", "token_type_ids", ",", "attention_mask", ",", "s1_span", ",", "s2_span", ",", "mode", ",", "labels", "=", "None", ")", ":", "\n", "# Something", "\n", "        ", "encoded_layers", ",", "_", "=", "self", ".", "bert_encoder", "(", "seq", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ")", "\n", "selected_output_layers", "=", "encoded_layers", "[", "-", "self", ".", "num_of_out_layers", ":", "]", "# [[B, T, D]]   0, 1, 2", "\n", "# context_length = att_mask.sum(dim=1)", "\n", "selected_output", "=", "torch", ".", "cat", "(", "selected_output_layers", ",", "dim", "=", "2", ")", "# Concat at last layer.", "\n", "\n", "s1_out", "=", "self", ".", "span_maxpool", "(", "selected_output", ",", "s1_span", ")", "\n", "s2_out", "=", "self", ".", "span_maxpool", "(", "selected_output", ",", "s2_span", ")", "\n", "\n", "s1_out", "=", "self", ".", "dropout", "(", "s1_out", ")", "\n", "s2_out", "=", "self", ".", "dropout", "(", "s2_out", ")", "\n", "paired_out", "=", "torch", ".", "cat", "(", "[", "s1_out", ",", "s2_out", ",", "torch", ".", "abs", "(", "s1_out", "-", "s2_out", ")", ",", "s1_out", "*", "s2_out", "]", ",", "dim", "=", "1", ")", "\n", "\n", "paired_out_1", "=", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "matching_layer1", "(", "paired_out", ")", ")", ")", "\n", "logits", "=", "self", ".", "matching_layer2", "(", "paired_out_1", ")", "\n", "\n", "if", "mode", "==", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "if", "self", ".", "use_sigmoid", ":", "\n", "                ", "assert", "labels", "is", "not", "None", "\n", "loss_fn", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "loss", "=", "loss_fn", "(", "logits", ",", "labels", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "                ", "assert", "labels", "is", "not", "None", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_of_class", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertSupervisedVecClassifier.__init__": [[123, 139], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["", "def", "__init__", "(", "self", ",", "bert_content2vec_model", ",", "num_of_class", ",", "dropout_rate", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertSupervisedVecClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_content2vec_model", "=", "bert_content2vec_model", "\n", "self", ".", "num_of_class", "=", "num_of_class", "\n", "\n", "self", ".", "matching_hidden_dimension", "=", "4", "*", "self", ".", "bert_content2vec_model", ".", "num_of_out_layers", "*", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "hidden_size", "\n", "self", ".", "matching_intermediate_dimension", "=", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "intermediate_size", "\n", "if", "dropout_rate", "is", "None", ":", "\n", "            ", "dropout_rate", "=", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", "\n", "\n", "", "self", ".", "matching_layer1", "=", "nn", ".", "Linear", "(", "self", ".", "matching_hidden_dimension", ",", "self", ".", "matching_intermediate_dimension", ")", "\n", "self", ".", "matching_layer2", "=", "nn", ".", "Linear", "(", "self", ".", "matching_intermediate_dimension", ",", "self", ".", "num_of_class", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "self", ".", "match_layers", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "matching_layer1", ",", "nn", ".", "ReLU", "(", ")", ",", "self", ".", "dropout", ",", "self", ".", "matching_layer2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.BertSupervisedVecClassifier.forward": [[140, 156], ["bert_maxout_clf.BertSupervisedVecClassifier.bert_content2vec_model", "bert_maxout_clf.BertSupervisedVecClassifier.bert_content2vec_model", "bert_maxout_clf.BertSupervisedVecClassifier.dropout", "bert_maxout_clf.BertSupervisedVecClassifier.dropout", "bert_maxout_clf.BertSupervisedVecClassifier.match_layers", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "bert_maxout_clf.BertSupervisedVecClassifier.view", "labels.view", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "s1_seq", ",", "s1_mask", ",", "s2_seq", ",", "s2_mask", ",", "mode", ",", "labels", "=", "None", ")", ":", "\n", "        ", "s1_out", "=", "self", ".", "bert_content2vec_model", "(", "s1_seq", ",", "attention_mask", "=", "s1_mask", ")", "\n", "s2_out", "=", "self", ".", "bert_content2vec_model", "(", "s2_seq", ",", "attention_mask", "=", "s2_mask", ")", "\n", "\n", "s1_out", "=", "self", ".", "dropout", "(", "s1_out", ")", "\n", "s2_out", "=", "self", ".", "dropout", "(", "s2_out", ")", "\n", "\n", "logits", "=", "self", ".", "match_layers", "(", "torch", ".", "cat", "(", "[", "s1_out", ",", "s2_out", ",", "torch", ".", "abs", "(", "s1_out", "-", "s2_out", ")", ",", "s1_out", "*", "s2_out", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "if", "mode", "==", "BertSupervisedVecClassifier", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "assert", "labels", "is", "not", "None", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_of_class", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.gelu": [[15, 21], ["torch.erf", "torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.swish": [[23, 25], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.bert_model_variances.bert_maxout_clf.init_bert_weights": [[30, 43], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "function", ["None"], ["def", "init_bert_weights", "(", "module", ")", ":", "\n", "    ", "\"\"\" Initialize the weights.\n    \"\"\"", "\n", "initializer_range", "=", "0.02", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "        ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "        ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.full_wiki_baseline_upperbound": [[7, 57], ["utils.common.load_json", "dict", "dict", "dict", "dict", "evaluation.ext_hotpot_eval.eval", "dict", "all", "enumerate", "dict", "upperbound_pred_doc.append", "upperbound_pred_sp.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["def", "full_wiki_baseline_upperbound", "(", ")", ":", "\n", "    ", "dev_fullwiki", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "# dev_fullwiki = common.load_json(config.DEV_DISTRACTOR_FILE)", "\n", "upperbound_pred_file", "=", "dict", "(", ")", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "=", "dict", "(", ")", "\n", "\n", "# print(dev_fullwiki)", "\n", "for", "item", "in", "dev_fullwiki", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "# supporting_doc = set([fact[0] for fact in item['supporting_facts']])", "\n", "\n", "# retrieved_doc_dict = set([context[0] for context in contexts])", "\n", "retrieved_doc_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc_title", ",", "context_sents", "in", "contexts", ":", "\n", "            ", "if", "doc_title", "not", "in", "retrieved_doc_dict", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "i", ",", "sent", "in", "enumerate", "(", "context_sents", ")", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "[", "i", "]", "=", "sent", "\n", "\n", "", "", "upperbound_pred_doc", "=", "[", "]", "\n", "upperbound_pred_sp", "=", "[", "]", "\n", "\n", "found_answer", "=", "False", "\n", "for", "sp_doc", ",", "sp_fact_line_num", "in", "supporting_facts", ":", "\n", "            ", "if", "sp_doc", "in", "retrieved_doc_dict", "and", "sp_fact_line_num", "in", "retrieved_doc_dict", "[", "sp_doc", "]", ":", "\n", "                ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "if", "answer", "in", "retrieved_doc_dict", "[", "sp_doc", "]", "[", "sp_fact_line_num", "]", ":", "\n", "                    ", "found_answer", "=", "True", "\n", "\n", "", "", "", "p_answer", "=", "answer", "if", "found_answer", "else", "\"\"", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "[", "qid", "]", "=", "upperbound_pred_sp", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "upperbound_pred_doc", "\n", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "p_answer", "\n", "\n", "if", "all", "(", "[", "gt_fact", "in", "upperbound_pred_sp", "for", "gt_fact", "in", "supporting_facts", "]", ")", ":", "\n", "# If we find all the evidence, to add additional yes/no answer.", "\n", "            ", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "answer", "\n", "\n", "", "", "ext_hotpot_eval", ".", "eval", "(", "upperbound_pred_file", ",", "dev_fullwiki", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.doc_retrie_v5_upperbound": [[59, 122], ["utils.common.load_json", "utils.common.load_json", "dict", "dict", "dict", "dict", "evaluation.ext_hotpot_eval.eval", "dict", "all", "enumerate", "dict", "upperbound_pred_doc.append", "upperbound_pred_sp.append", "upperbound_pred_doc.append", "upperbound_pred_sp.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "doc_retrie_v5_upperbound", "(", ")", ":", "\n", "    ", "dev_fullwiki", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "pred_dev", "=", "common", ".", "load_json", "(", "\n", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/doc_raw_matching_with_disamb_with_hyperlinked_v5_file.json\"", ")", "\n", "\n", "pred_v5_sp_doc", "=", "pred_dev", "[", "'sp_doc'", "]", "\n", "# dev_fullwiki = common.load_json(config.DEV_DISTRACTOR_FILE)", "\n", "upperbound_pred_file", "=", "dict", "(", ")", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "=", "dict", "(", ")", "\n", "\n", "# print(dev_fullwiki)", "\n", "for", "item", "in", "dev_fullwiki", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "\n", "v5_retrieved_doc", "=", "pred_v5_sp_doc", "[", "qid", "]", "\n", "# print(v5_retrieved_doc)", "\n", "# supporting_doc = set([fact[0] for fact in item['supporting_facts']])", "\n", "\n", "# retrieved_doc_dict = set([context[0] for context in contexts])", "\n", "retrieved_doc_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc_title", ",", "context_sents", "in", "contexts", ":", "\n", "            ", "if", "doc_title", "not", "in", "retrieved_doc_dict", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "i", ",", "sent", "in", "enumerate", "(", "context_sents", ")", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "[", "i", "]", "=", "sent", "\n", "\n", "", "", "upperbound_pred_doc", "=", "[", "]", "\n", "upperbound_pred_sp", "=", "[", "]", "\n", "\n", "found_answer", "=", "False", "\n", "for", "sp_doc", ",", "sp_fact_line_num", "in", "supporting_facts", ":", "\n", "            ", "if", "sp_doc", "in", "retrieved_doc_dict", "and", "sp_fact_line_num", "in", "retrieved_doc_dict", "[", "sp_doc", "]", ":", "\n", "                ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "if", "answer", "in", "retrieved_doc_dict", "[", "sp_doc", "]", "[", "sp_fact_line_num", "]", ":", "\n", "                    ", "found_answer", "=", "True", "\n", "\n", "", "", "elif", "sp_doc", "in", "v5_retrieved_doc", ":", "\n", "                ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "# if answer in retrieved_doc_dict[sp_doc][sp_fact_line_num]:", "\n", "#     found_answer = True", "\n", "\n", "", "", "p_answer", "=", "answer", "if", "found_answer", "else", "\"\"", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "[", "qid", "]", "=", "upperbound_pred_sp", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "upperbound_pred_doc", "\n", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "p_answer", "\n", "\n", "if", "all", "(", "[", "gt_fact", "in", "upperbound_pred_sp", "for", "gt_fact", "in", "supporting_facts", "]", ")", ":", "\n", "# If we find all the evidence, to add additional yes/no answer.", "\n", "            ", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "answer", "\n", "\n", "", "", "ext_hotpot_eval", ".", "eval", "(", "upperbound_pred_file", ",", "dev_fullwiki", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.doc_retrie_v5_reimpl_tf_idf_upperbound": [[124, 216], ["utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "dict", "dict", "dict", "dict", "dict", "evaluation.ext_hotpot_eval.eval", "sorted", "set", "dict", "all", "enumerate", "dict", "upperbound_pred_doc.append", "upperbound_pred_doc.append", "upperbound_pred_sp.append", "upperbound_pred_sp.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "doc_retrie_v5_reimpl_tf_idf_upperbound", "(", ")", ":", "\n", "    ", "top_k", "=", "10", "\n", "dev_fullwiki", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "pred_dev", "=", "common", ".", "load_json", "(", "\n", "# config.RESULT_PATH / \"doc_retri_results/doc_raw_matching_with_disamb_with_hyperlinked_v5_file.json\")", "\n", "# config.RESULT_PATH / \"doc_retri_results/doc_raw_matching_file.json\")", "\n", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/doc_retrieval_debug_v6/doc_raw_matching_with_disamb_withiout_hyperlinked_v6_file_debug_4.json\"", ")", "\n", "# config.RESULT_PATH / \"doc_retri_results/doc_raw_matching_with_disamb_withiout_hyperlinked_v5_file.json\")", "\n", "\n", "tf_idf_dev_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\"", ")", "\n", "\n", "tf_idf_scored_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "tf_idf_dev_results", ":", "\n", "        ", "sorted_scored_list", "=", "sorted", "(", "item", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "pred_list", "=", "[", "docid", "for", "_", ",", "docid", "in", "sorted_scored_list", "[", ":", "top_k", "]", "]", "\n", "qid", "=", "item", "[", "'qid'", "]", "\n", "tf_idf_scored_dict", "[", "qid", "]", "=", "pred_list", "\n", "\n", "", "pred_v5_sp_doc", "=", "pred_dev", "[", "'sp_doc'", "]", "\n", "# dev_fullwiki = common.load_json(config.DEV_DISTRACTOR_FILE)", "\n", "upperbound_pred_file", "=", "dict", "(", ")", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "=", "dict", "(", ")", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "=", "dict", "(", ")", "\n", "\n", "# print(dev_fullwiki", "\n", "\n", "for", "item", "in", "dev_fullwiki", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "\n", "tf_idf_docs", "=", "tf_idf_scored_dict", "[", "qid", "]", "\n", "\n", "v5_retrieved_doc", "=", "pred_v5_sp_doc", "[", "qid", "]", "\n", "# print(v5_retrieved_doc)", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", "\n", "\n", "# retrieved_doc_dict = set([context[0] for context in contexts])", "\n", "retrieved_doc_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc_title", ",", "context_sents", "in", "contexts", ":", "\n", "            ", "if", "doc_title", "not", "in", "retrieved_doc_dict", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "i", ",", "sent", "in", "enumerate", "(", "context_sents", ")", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "[", "i", "]", "=", "sent", "\n", "\n", "", "", "upperbound_pred_doc", "=", "[", "]", "\n", "upperbound_pred_sp", "=", "[", "]", "\n", "\n", "found_answer", "=", "False", "\n", "for", "sp_doc", "in", "tf_idf_docs", ":", "\n", "            ", "if", "sp_doc", "in", "supporting_doc", ":", "\n", "                ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "for", "gt_sp_doc", ",", "sp_fact_line_num", "in", "supporting_facts", ":", "\n", "                    ", "if", "gt_sp_doc", "==", "sp_doc", ":", "\n", "                        ", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "# if answer in retrieved_doc_dict[sp_doc][sp_fact_line_num]:", "\n", "found_answer", "=", "True", "\n", "\n", "", "", "", "", "for", "sp_doc", "in", "v5_retrieved_doc", ":", "\n", "            ", "if", "sp_doc", "not", "in", "upperbound_pred_doc", ":", "\n", "                ", "if", "sp_doc", "in", "supporting_doc", ":", "\n", "                    ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "for", "gt_sp_doc", ",", "sp_fact_line_num", "in", "supporting_facts", ":", "\n", "                        ", "if", "gt_sp_doc", "==", "sp_doc", ":", "\n", "                            ", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "# if answer in retrieved_doc_dict[sp_doc][sp_fact_line_num]:", "\n", "found_answer", "=", "True", "\n", "\n", "\n", "# upperbound_pred_sp.append([sp_doc, sp_fact_line_num])", "\n", "# if answer in retrieved_doc_dict[sp_doc][sp_fact_line_num]:", "\n", "#     found_answer = True", "\n", "\n", "", "", "", "", "", "p_answer", "=", "answer", "if", "found_answer", "else", "\"\"", "\n", "\n", "upperbound_pred_file", "[", "'sp'", "]", "[", "qid", "]", "=", "upperbound_pred_sp", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "upperbound_pred_doc", "\n", "\n", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "p_answer", "\n", "\n", "if", "all", "(", "[", "gt_fact", "in", "upperbound_pred_sp", "for", "gt_fact", "in", "supporting_facts", "]", ")", ":", "\n", "# If we find all the evidence, to add additional yes/no answer.", "\n", "            ", "upperbound_pred_file", "[", "'p_answer'", "]", "[", "qid", "]", "=", "answer", "\n", "\n", "", "", "ext_hotpot_eval", ".", "eval", "(", "upperbound_pred_file", ",", "dev_fullwiki", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.full_wiki_baseline_score": [[218, 268], ["utils.common.load_json", "dict", "dict", "evaluation.ext_hotpot_eval.eval", "dict", "list", "enumerate", "dict.keys", "dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "full_wiki_baseline_score", "(", ")", ":", "\n", "    ", "dev_fullwiki", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "# dev_fullwiki = common.load_json(config.DEV_DISTRACTOR_FILE)", "\n", "upperbound_pred_file", "=", "dict", "(", ")", "\n", "\n", "# upperbound_pred_file['sp'] = dict()", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "=", "dict", "(", ")", "\n", "# upperbound_pred_file['p_answer'] = dict()", "\n", "\n", "# print(dev_fullwiki)", "\n", "for", "item", "in", "dev_fullwiki", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "# supporting_doc = set([fact[0] for fact in item['supporting_facts']])", "\n", "\n", "# retrieved_doc_dict = set([context[0] for context in contexts])", "\n", "retrieved_doc_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc_title", ",", "context_sents", "in", "contexts", ":", "\n", "            ", "if", "doc_title", "not", "in", "retrieved_doc_dict", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "i", ",", "sent", "in", "enumerate", "(", "context_sents", ")", ":", "\n", "                ", "retrieved_doc_dict", "[", "doc_title", "]", "[", "i", "]", "=", "sent", "\n", "\n", "", "", "upperbound_pred_doc", "=", "[", "]", "\n", "# upperbound_pred_sp = []", "\n", "\n", "# found_answer = False", "\n", "# for sp_doc, sp_fact_line_num in supporting_facts:", "\n", "#     if sp_doc in retrieved_doc_dict and sp_fact_line_num in retrieved_doc_dict[sp_doc]:", "\n", "#         upperbound_pred_doc.append(sp_doc)", "\n", "#         upperbound_pred_sp.append([sp_doc, sp_fact_line_num])", "\n", "#         if answer in retrieved_doc_dict[sp_doc][sp_fact_line_num]:", "\n", "#             found_answer = True", "\n", "#", "\n", "# p_answer = answer if found_answer else \"\"", "\n", "\n", "# upperbound_pred_file['sp'][qid] = upperbound_pred_sp", "\n", "upperbound_pred_file", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "list", "(", "retrieved_doc_dict", ".", "keys", "(", ")", ")", "\n", "\n", "# upperbound_pred_file['p_answer'][qid] = p_answer", "\n", "\n", "# if all([gt_fact in upperbound_pred_sp for gt_fact in supporting_facts]):", "\n", "# If we find all the evidence, to add additional yes/no answer.", "\n", "# upperbound_pred_file['p_answer'][qid] = answer", "\n", "\n", "", "ext_hotpot_eval", ".", "eval", "(", "upperbound_pred_file", ",", "dev_fullwiki", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri": [[270, 308], ["dict", "dict", "dict", "set", "all", "upperbound_pred_doc.append", "upperbound_pred_sp.append"], "function", ["None"], ["", "def", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "results_dict", ",", "d_list", ")", ":", "\n", "\n", "    ", "upper_bound_results_dict", "=", "{", "'sp'", ":", "dict", "(", ")", ",", "'sp_doc'", ":", "dict", "(", ")", ",", "'p_answer'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "# contexts = item['context']", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", "\n", "\n", "found_answer", "=", "False", "\n", "\n", "retrieved_doc", "=", "results_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "\n", "\n", "upperbound_pred_doc", "=", "[", "]", "\n", "upperbound_pred_sp", "=", "[", "]", "\n", "\n", "for", "sp_doc", "in", "retrieved_doc", ":", "\n", "            ", "if", "sp_doc", "in", "supporting_doc", ":", "\n", "                ", "upperbound_pred_doc", ".", "append", "(", "sp_doc", ")", "\n", "for", "gt_sp_doc", ",", "sp_fact_line_num", "in", "supporting_facts", ":", "\n", "                    ", "if", "gt_sp_doc", "==", "sp_doc", ":", "\n", "                        ", "upperbound_pred_sp", ".", "append", "(", "[", "sp_doc", ",", "sp_fact_line_num", "]", ")", "\n", "found_answer", "=", "True", "\n", "\n", "", "", "", "", "p_answer", "=", "answer", "if", "found_answer", "else", "\"\"", "\n", "\n", "upper_bound_results_dict", "[", "'sp'", "]", "[", "qid", "]", "=", "upperbound_pred_sp", "\n", "upper_bound_results_dict", "[", "'sp_doc'", "]", "[", "qid", "]", "=", "upperbound_pred_doc", "\n", "\n", "upper_bound_results_dict", "[", "'p_answer'", "]", "[", "qid", "]", "=", "p_answer", "\n", "\n", "if", "all", "(", "[", "gt_fact", "in", "upperbound_pred_sp", "for", "gt_fact", "in", "supporting_facts", "]", ")", ":", "\n", "# If we find all the evidence, to add additional yes/no answer.", "\n", "            ", "upper_bound_results_dict", "[", "'p_answer'", "]", "[", "qid", "]", "=", "answer", "\n", "\n", "", "", "return", "upper_bound_results_dict", "\n", "# ext_hotpot_eval.eval(upperbound_pred_file, dev_fullwiki)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_eval.eval_p_level.eval_p_level": [[9, 27], ["utils.common.load_jsonl", "utils.common.load_json", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_content_selection.bert_p_level_v1.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["def", "eval_p_level", "(", ")", ":", "\n", "    ", "cur_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", "\n", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "print", "(", "metrics_top5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_eval.eval_s_level.eval_hotpot_s": [[9, 45], ["utils.common.load_jsonl", "utils.common.load_json", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "print", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["def", "eval_hotpot_s", "(", ")", ":", "\n", "    ", "cur_dev_eval_results_list_out", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpot_p_level_effects/hotpot_s_level_dev_results_top_k_doc_100.jsonl\"", "\n", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_dev_eval_results_list_out", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# 0.5", "\n", "cur_results_dict_v05", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "# cur_results_dict_v02 = select_top_k_and_to_results_dict(copied_dev_o_dict, top_k=5,", "\n", "#                                                         score_field_name='prob',", "\n", "#                                                         filter_value=0.2,", "\n", "#                                                         result_field='sp')", "\n", "\n", "_", ",", "metrics_v5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v05", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "# _, metrics_v2 = ext_hotpot_eval.eval(cur_results_dict_v02, dev_list, verbose=False)", "\n", "\n", "logging_item", "=", "{", "\n", "# 'v02': metrics_v2,", "\n", "'v05'", ":", "metrics_v5", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "f1", "=", "metrics_v5", "[", "'sp_f1'", "]", "\n", "em", "=", "metrics_v5", "[", "'sp_em'", "]", "\n", "pr", "=", "metrics_v5", "[", "'sp_prec'", "]", "\n", "rec", "=", "metrics_v5", "[", "'sp_recall'", "]", "\n", "\n", "print", "(", "em", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.pre_compute_abs_if_idf_scores": [[19, 29], ["build_rindex.build_rvindex.IndexDB", "build_rindex.build_rvindex.IndexDB.load_from_file", "print", "build_rindex.build_rvindex.IndexDB.inverted_index.build_Nt_table", "build_rindex.build_rvindex.IndexDB.pre_compute_scores", "build_rindex.build_rvindex.save_to_file", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.build_Nt_table", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.pre_compute_scores", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file"], ["def", "pre_compute_abs_if_idf_scores", "(", ")", ":", "\n", "    ", "abs_rindexdb", "=", "IndexDB", "(", ")", "\n", "abs_rindexdb", ".", "load_from_file", "(", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb\"", ")", "\n", "print", "(", "\"Number of terms:\"", ",", "len", "(", "abs_rindexdb", ".", "inverted_index", ".", "index", ")", ")", "\n", "abs_rindexdb", ".", "inverted_index", ".", "build_Nt_table", "(", ")", "\n", "# exit(0)", "\n", "\n", "abs_rindexdb", ".", "pre_compute_scores", "(", ")", "\n", "save_to_file", "(", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.compute_abs_score": [[31, 49], ["build_rindex.build_rvindex.IndexDB", "build_rindex.build_rvindex.IndexDB.load_from_file", "print", "build_rindex.build_rvindex.IndexDB.inverted_index.build_Nt_table", "len", "build_rindex.build_rvindex.IndexDB.pre_compute_scores", "build_rindex.build_rvindex.save_to_file", "build_rindex.build_rvindex.IndexDB.pre_compute_scores_iteratively", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.build_Nt_table", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.pre_compute_scores", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.pre_compute_scores_iteratively"], ["", "def", "compute_abs_score", "(", "db_path", ",", "score_path", "=", "\"scored_db/default-tf-idf.score.txt\"", ",", "with_int_type", "=", "False", ",", "\n", "memory_efficient", "=", "False", ",", "iteratively", "=", "False", ")", ":", "\n", "    ", "abs_rindexdb", "=", "IndexDB", "(", ")", "\n", "abs_rindexdb", ".", "load_from_file", "(", "db_path", ",", "with_int_type", ",", "memory_saving", "=", "memory_efficient", ")", "\n", "print", "(", "\"Number of terms:\"", ",", "len", "(", "abs_rindexdb", ".", "inverted_index", ".", "index", ")", ")", "\n", "abs_rindexdb", ".", "inverted_index", ".", "build_Nt_table", "(", ")", "\n", "\n", "if", "not", "iteratively", ":", "\n", "        ", "abs_rindexdb", ".", "pre_compute_scores", "(", ")", "\n", "if", "not", "(", "Path", "(", "db_path", ")", "/", "score_path", ")", ".", "parent", ".", "is_dir", "(", ")", ":", "\n", "            ", "(", "Path", "(", "db_path", ")", "/", "score_path", ")", ".", "parent", ".", "mkdir", "(", ")", "\n", "\n", "", "save_to_file", "(", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", ",", "\n", "Path", "(", "db_path", ")", "/", "score_path", ",", "memory_efficient", "=", "memory_efficient", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "(", "Path", "(", "db_path", ")", "/", "score_path", ")", ".", "parent", ".", "is_dir", "(", ")", ":", "\n", "            ", "(", "Path", "(", "db_path", ")", "/", "score_path", ")", ".", "parent", ".", "mkdir", "(", ")", "\n", "", "abs_rindexdb", ".", "pre_compute_scores_iteratively", "(", "Path", "(", "db_path", ")", "/", "score_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_ngrams": [[51, 57], ["build_rindex.build_wiki_rindex.get_ngrams", "nlp", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams"], ["", "", "def", "get_query_ngrams", "(", "query", ")", ":", "\n", "    ", "tokens", "=", "[", "t", ".", "text", "for", "t", "in", "nlp", "(", "query", ")", "]", "\n", "query_ngrams", "=", "get_ngrams", "(", "tokens", ",", "None", ",", "3", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "None", ")", "\n", "return", "query_ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_query_doc_score": [[59, 66], ["None"], "function", ["None"], ["", "def", "get_query_doc_score", "(", "query_ngrams", ",", "doc", ",", "score_dict", ")", ":", "\n", "    ", "score", "=", "0", "\n", "for", "term", "in", "query_ngrams", ":", "\n", "        ", "if", "term", "in", "score_dict", ":", "\n", "            ", "if", "doc", "in", "score_dict", "[", "term", "]", ":", "\n", "                ", "score", "+=", "score_dict", "[", "term", "]", "[", "doc", "]", "\n", "", "", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_candidate_page_list": [[68, 81], ["set", "list", "v_terms.append", "list", "v_docids.append", "score_dict[].keys", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "get_candidate_page_list", "(", "terms", ",", "score_dict", ")", ":", "\n", "    ", "candidate_doc_set", "=", "set", "(", ")", "\n", "v_terms", "=", "[", "]", "\n", "v_docids", "=", "[", "]", "\n", "for", "term", "in", "terms", ":", "\n", "        ", "if", "term", "in", "score_dict", ":", "\n", "            ", "v_terms", ".", "append", "(", "term", ")", "\n", "cur_v_docids", "=", "list", "(", "score_dict", "[", "term", "]", ".", "keys", "(", ")", ")", "\n", "v_docids", ".", "append", "(", "cur_v_docids", ")", "\n", "for", "v_docid", "in", "cur_v_docids", ":", "\n", "                ", "candidate_doc_set", ".", "add", "(", "v_docid", ")", "\n", "\n", "", "", "", "return", "v_terms", ",", "v_docids", ",", "list", "(", "candidate_doc_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_ranked_score": [[83, 111], ["dict", "zip", "dict", "scored_doc.append", "heapq.heappushpop", "heapq.heappush", "len"], "function", ["None"], ["", "def", "get_ranked_score", "(", "v_terms", ",", "v_docids", ",", "candidate_doc_list", ",", "top_k", ",", "score_dict", ")", ":", "\n", "    ", "cached_scored_results", "=", "dict", "(", ")", "\n", "\n", "# We first access the global dict to cached a local score dict.", "\n", "for", "term", ",", "mset", "in", "zip", "(", "v_terms", ",", "v_docids", ")", ":", "\n", "        ", "cached_scored_results", "[", "term", "]", "=", "dict", "(", ")", "\n", "for", "docid", "in", "mset", ":", "\n", "            ", "cached_scored_results", "[", "term", "]", "[", "docid", "]", "=", "score_dict", "[", "term", "]", "[", "docid", "]", "\n", "\n", "", "", "scored_doc", "=", "[", "]", "\n", "\n", "for", "cur_doc", "in", "candidate_doc_list", ":", "\n", "        ", "cur_doc_score", "=", "0", "\n", "for", "cur_term", "in", "v_terms", ":", "\n", "            ", "if", "cur_doc", "not", "in", "cached_scored_results", "[", "cur_term", "]", ":", "\n", "                ", "cur_doc_score", "+=", "0", "\n", "", "else", ":", "\n", "                ", "cur_doc_score", "+=", "cached_scored_results", "[", "cur_term", "]", "[", "cur_doc", "]", "\n", "\n", "", "", "if", "top_k", "is", "None", ":", "\n", "            ", "scored_doc", ".", "append", "(", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "top_k", "is", "not", "None", "and", "0", "<=", "top_k", "==", "len", "(", "scored_doc", ")", ":", "\n", "                ", "heapq", ".", "heappushpop", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "", "else", ":", "\n", "                ", "heapq", ".", "heappush", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "\n", "", "", "", "return", "scored_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.get_top_ranked_tf_idf_doc": [[113, 130], ["build_rindex.build_wiki_rindex.get_ngrams", "set", "rindexdb.get_relevant_document", "rindexdb.inverted_index.get_containing_document", "nlp", "functools.partial", "valid_terms.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.get_relevant_document", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_containing_document"], ["", "def", "get_top_ranked_tf_idf_doc", "(", "query", ",", "rindexdb", ",", "top_k", ")", ":", "\n", "    ", "tokens", "=", "[", "t", ".", "text", "for", "t", "in", "nlp", "(", "query", ")", "]", "\n", "query_ngrams", "=", "get_ngrams", "(", "tokens", ",", "None", ",", "3", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "None", ")", "\n", "\n", "candidate_pages_set", "=", "set", "(", ")", "\n", "valid_terms", "=", "[", "]", "\n", "for", "q_ngram", "in", "query_ngrams", ":", "\n", "        ", "candidate_pages", "=", "rindexdb", ".", "inverted_index", ".", "get_containing_document", "(", "q_ngram", ")", "\n", "if", "candidate_pages", "is", "not", "None", ":", "\n", "            ", "valid_terms", ".", "append", "(", "q_ngram", ")", "\n", "candidate_pages_set", "|=", "candidate_pages", "\n", "\n", "", "", "doc_list", "=", "rindexdb", ".", "get_relevant_document", "(", "candidate_pages_set", ",", "valid_terms", ",", "top_k", "=", "top_k", ")", "\n", "\n", "return", "doc_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rvindex_scoring.sanity_check": [[132, 174], ["build_rindex.build_rvindex.IndexDB", "build_rindex.build_rvindex.IndexDB.load_from_file", "print", "build_rindex.build_rvindex.IndexDB.inverted_index.build_Nt_table", "dict", "build_rindex.build_rvindex.load_from_file", "build_rindex.build_wiki_rindex.get_ngrams", "set", "print", "print", "build_rindex.build_rvindex.IndexDB.get_relevant_document", "print", "print", "print", "len", "build_rindex.build_rvindex.IndexDB.inverted_index.get_containing_document", "build_rindex.build_rvindex.IndexDB.get_relevant_document", "len", "nlp", "functools.partial", "valid_terms.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.build_Nt_table", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.get_relevant_document", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_containing_document", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.get_relevant_document"], ["", "def", "sanity_check", "(", ")", ":", "\n", "# pre_compute_abs_if_idf_scores()", "\n", "#", "\n", "    ", "abs_rindexdb", "=", "IndexDB", "(", ")", "\n", "abs_rindexdb", ".", "load_from_file", "(", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb\"", ")", "\n", "print", "(", "\"Number of terms:\"", ",", "len", "(", "abs_rindexdb", ".", "inverted_index", ".", "index", ")", ")", "\n", "abs_rindexdb", ".", "inverted_index", ".", "build_Nt_table", "(", ")", "\n", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", "=", "dict", "(", ")", "\n", "load_from_file", "(", "abs_rindexdb", ".", "score_db", "[", "'default-tf-idf'", "]", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "# # exit(0)", "\n", "#", "\n", "# abs_rindexdb.pre_compute_scores()", "\n", "# save_to_file(abs_rindexdb.score_db['default-tf-idf'], config.PDATA_ROOT / \"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\")", "\n", "\n", "# exit(0)", "\n", "\n", "query", "=", "\"What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\"", "\n", "tokens", "=", "[", "t", ".", "text", "for", "t", "in", "nlp", "(", "query", ")", "]", "\n", "# poss = [t.text for t in nlp(query)]", "\n", "query_ngrams", "=", "get_ngrams", "(", "tokens", ",", "None", ",", "3", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "None", ")", "\n", "\n", "# print(query_ngram)", "\n", "candidate_pages_set", "=", "set", "(", ")", "\n", "valid_terms", "=", "[", "]", "\n", "for", "q_ngram", "in", "query_ngrams", ":", "\n", "        ", "candidate_pages", "=", "abs_rindexdb", ".", "inverted_index", ".", "get_containing_document", "(", "q_ngram", ")", "\n", "if", "candidate_pages", "is", "not", "None", ":", "\n", "            ", "valid_terms", ".", "append", "(", "q_ngram", ")", "\n", "candidate_pages_set", "|=", "candidate_pages", "\n", "\n", "", "", "print", "(", "'Animorphs'", "in", "candidate_pages_set", ")", "\n", "print", "(", "abs_rindexdb", ".", "get_relevant_document", "(", "[", "'Animorphs'", "]", ",", "valid_terms", ")", ")", "\n", "doc_list", "=", "abs_rindexdb", ".", "get_relevant_document", "(", "candidate_pages_set", ",", "valid_terms", ",", "top_k", "=", "100", ")", "\n", "\n", "# print(candidate_pages_set)", "\n", "print", "(", "query_ngrams", ")", "\n", "print", "(", "len", "(", "candidate_pages_set", ")", ")", "\n", "print", "(", "doc_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.__init__": [[7, 11], ["str", "sqlite3.connect", "persistent_index_db.IndexingDB.conn.cursor", "str"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "db_path", ")", ":", "\n", "        ", "self", ".", "db_path", "=", "str", "(", "db_path", ")", "\n", "self", ".", "conn", "=", "sqlite3", ".", "connect", "(", "str", "(", "self", ".", "db_path", ")", ")", "\n", "self", ".", "cursor", "=", "self", ".", "conn", ".", "cursor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.create_tables": [[12, 30], ["c.execute", "c.execute", "c.execute", "c.execute", "c.execute", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "create_tables", "(", "self", ")", ":", "\n", "        ", "table_name", "=", "'term_title_table'", "\n", "c", "=", "self", ".", "cursor", "\n", "c", ".", "execute", "(", "f\"CREATE TABLE {table_name} (\"", "\n", "f\"term TEXT NOT NULL, \"", "\n", "f\"article_title TEXT NOT NULL, \"", "\n", "f\"occurrence INTEGER NOT NULL, \"", "\n", "f\"tf_idf_score REAL\"", "\n", "f\");\"", ")", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX term_index ON {table_name}(term);\"", ")", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX article_title_index ON {table_name}(article_title);\"", ")", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX term_title_index ON {table_name}(term, article_title);\"", ")", "\n", "\n", "article_table_name", "=", "'title_table'", "\n", "c", ".", "execute", "(", "f\"CREATE TABLE {article_table_name} (\"", "\n", "f\"article_title TEXT NOT NULL PRIMARY KEY, \"", "\n", "f\"gram_number INTEGER NOT NULL);\"", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_item": [[31, 36], ["persistent_index_db.IndexingDB.cursor.execute", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "insert_item", "(", "self", ",", "term", ":", "str", ",", "article_title", ":", "str", ",", "occurrence", ":", "int", ")", ":", "\n", "        ", "table_name", "=", "'term_title_table'", "\n", "self", ".", "cursor", ".", "execute", "(", "f\"INSERT INTO {table_name}(term, article_title, occurrence) VALUES (?, ?, ?)\"", ",", "\n", "(", "term", ",", "article_title", ",", "occurrence", ")", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_items": [[37, 42], ["persistent_index_db.IndexingDB.cursor.executemany", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "insert_many_items", "(", "self", ",", "items", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "table_name", "=", "'term_title_table'", "\n", "self", ".", "cursor", ".", "executemany", "(", "f\"INSERT INTO {table_name}(term, article_title, occurrence) VALUES (?, ?, ?)\"", ",", "\n", "items", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_article": [[43, 48], ["persistent_index_db.IndexingDB.cursor.execute", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "insert_article", "(", "self", ",", "article_title", ":", "str", ",", "gram_number", ":", "int", ")", ":", "\n", "        ", "table_name", "=", "'title_table'", "\n", "self", ".", "cursor", ".", "execute", "(", "f\"INSERT INTO {table_name}(article_title, gram_number) VALUES (?, ?)\"", ",", "\n", "(", "article_title", ",", "gram_number", ")", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_articles": [[49, 54], ["persistent_index_db.IndexingDB.cursor.executemany", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "insert_many_articles", "(", "self", ",", "items", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "table_name", "=", "'title_table'", "\n", "self", ".", "cursor", ".", "executemany", "(", "f\"INSERT INTO {table_name}(article_title, gram_number) VALUES (?, ?)\"", ",", "\n", "items", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.assign_score": [[55, 60], ["persistent_index_db.IndexingDB.cursor.execute", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "assign_score", "(", "self", ",", "term", ":", "str", ",", "article_title", ":", "str", ",", "\n", "score", ":", "float", ",", "score_field", ":", "str", "=", "'tf_idf_score'", ")", ":", "\n", "        ", "self", ".", "cursor", ".", "execute", "(", "f\"UPDATE term_title_table SET {score_field}=? where term=? AND article_title=?\"", "\n", ",", "(", "score", ",", "term", ",", "article_title", ")", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.assign_many_scores": [[61, 67], ["persistent_index_db.IndexingDB.cursor.executemany", "persistent_index_db.IndexingDB.conn.commit"], "methods", ["None"], ["", "def", "assign_many_scores", "(", "self", ",", "items", ":", "List", "[", "Tuple", "[", "float", ",", "str", ",", "str", "]", "]", ",", "score_field", ":", "str", "=", "'tf_idf_score'", ")", ":", "\n", "        ", "self", ".", "cursor", ".", "executemany", "(", "f\"UPDATE term_title_table SET {score_field}=? where term=? AND article_title=?\"", "\n", ",", "items", ")", "\n", "# for value in self.cursor:", "\n", "#     print(value)", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close": [[68, 72], ["persistent_index_db.IndexingDB.cursor.close", "persistent_index_db.IndexingDB.conn.commit", "persistent_index_db.IndexingDB.conn.close"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "cursor", ".", "close", "(", ")", "\n", "self", ".", "conn", ".", "commit", "(", ")", "\n", "self", ".", "conn", ".", "close", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.spacy_get_pos": [[18, 26], ["spacy.tokens.doc.Doc", "spacy.tokens.doc.Doc", "proc"], "function", ["None"], ["def", "spacy_get_pos", "(", "tokens", ")", ":", "\n", "    ", "doc", "=", "spacy", ".", "tokens", ".", "doc", ".", "Doc", "(", "\n", "nlp", ".", "vocab", ",", "words", "=", "tokens", ")", "\n", "\n", "for", "name", ",", "proc", "in", "nlp", ".", "pipeline", ":", "\n", "        ", "proc", "(", "doc", ")", "\n", "\n", "", "return", "[", "token", ".", "pos_", "for", "token", "in", "doc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.get_sentence_tokens": [[28, 45], ["sentence_offsets.append", "len", "tokens.append"], "function", ["None"], ["", "def", "get_sentence_tokens", "(", "texts", ",", "charoffsets", ")", ":", "\n", "    ", "whole_text", "=", "\"\"", ".", "join", "(", "texts", ")", "\n", "tokens", "=", "[", "]", "\n", "sentence_offsets", "=", "[", "]", "\n", "\n", "start_t", "=", "0", "\n", "end_t", "=", "0", "\n", "for", "offset_list", "in", "charoffsets", ":", "\n", "        ", "end_t", "=", "start_t", "\n", "for", "start", ",", "end", "in", "offset_list", ":", "\n", "            ", "cur_token", "=", "whole_text", "[", "start", ":", "end", "]", "\n", "if", "len", "(", "cur_token", ")", ">", "0", ":", "\n", "                ", "tokens", ".", "append", "(", "cur_token", ")", "\n", "end_t", "+=", "1", "\n", "", "", "sentence_offsets", ".", "append", "(", "(", "start_t", ",", "end_t", ")", ")", "\n", "start_t", "=", "end_t", "\n", "", "return", "tokens", ",", "sentence_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.iterative_abs": [[47, 61], ["open", "tqdm.tqdm", "json.loads", "rindex.get_sentence_tokens", "rindex.spacy_get_pos", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.get_sentence_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.spacy_get_pos"], ["", "def", "iterative_abs", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "with", "open", "(", "config", ".", "ABS_WIKI_FILE", ",", "'rb'", ")", "as", "abs_file", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "abs_file", ",", "total", "=", "total_doc_num", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "# print(item.keys())", "\n", "# print()", "\n", "tokens", ",", "sent_offset", "=", "get_sentence_tokens", "(", "item", "[", "'text'", "]", ",", "item", "[", "'charoffset'", "]", ")", "\n", "poss", "=", "spacy_get_pos", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "poss", ")", "\n", "print", "(", "tokens", ")", "\n", "print", "(", "sent_offset", ")", "\n", "# print(poss)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.iterative_abs_save_info": [[64, 94], ["open", "sqlitedict.SqliteDict", "tqdm.tqdm", "abs_rindex_db.commit", "abs_rindex_db.close", "str", "json.loads", "rindex.get_sentence_tokens", "rindex.spacy_get_pos", "len", "len", "abs_rindex_db.commit"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.get_sentence_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.spacy_get_pos"], ["", "", "", "def", "iterative_abs_save_info", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "with", "open", "(", "config", ".", "ABS_WIKI_FILE", ",", "'rb'", ")", "as", "abs_file", ":", "\n", "        ", "with", "SqliteDict", "(", "str", "(", "config", ".", "ABS_PROCESS_FOR_RINDEX_DB", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "abs_rindex_db", ":", "\n", "            ", "for", "line", "in", "tqdm", "(", "abs_file", ",", "total", "=", "total_doc_num", ")", ":", "\n", "                ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "# print(item.keys())", "\n", "# print()", "\n", "if", "item", "[", "'title'", "]", "in", "abs_rindex_db", ":", "\n", "                    ", "continue", "\n", "\n", "", "tokens", ",", "sent_offset", "=", "get_sentence_tokens", "(", "item", "[", "'text'", "]", ",", "item", "[", "'charoffset'", "]", ")", "\n", "poss", "=", "spacy_get_pos", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "poss", ")", "\n", "# print(tokens)", "\n", "# print(sent_offset)", "\n", "abs_rindex_db", "[", "item", "[", "'title'", "]", "]", "=", "{", "\n", "'tokens'", ":", "tokens", ",", "\n", "'poss'", ":", "poss", ",", "\n", "'sentence_offset'", ":", "sent_offset", "\n", "}", "\n", "cur_count", "+=", "1", "\n", "\n", "if", "cur_count", "%", "5000", "==", "0", ":", "\n", "                    ", "abs_rindex_db", ".", "commit", "(", ")", "\n", "\n", "", "", "abs_rindex_db", ".", "commit", "(", ")", "\n", "abs_rindex_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.iterative_abs_save_random_batching": [[96, 147], ["range", "open", "tqdm.tqdm", "len", "sqlitedict.SqliteDict", "tqdm.tqdm", "abs_rindex_db.commit", "abs_rindex_db.close", "lines.append", "str", "json.loads", "rindex.get_sentence_tokens", "rindex.spacy_get_pos", "batch_list.append", "len", "len", "len", "abs_rindex_db.commit"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex.get_sentence_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.spacy_get_pos"], ["", "", "", "def", "iterative_abs_save_random_batching", "(", "batch_size", "=", "10000", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "\n", "\n", "with", "open", "(", "config", ".", "ABS_WIKI_FILE", ",", "'rb'", ")", "as", "abs_file", ":", "\n", "        ", "lines", "=", "[", "]", "\n", "for", "line", "in", "tqdm", "(", "abs_file", ",", "total", "=", "total_doc_num", ")", ":", "\n", "            ", "lines", ".", "append", "(", "line", ")", "\n", "# if len(lines) == 100000:", "\n", "#     break", "\n", "\n", "", "", "random_per", "=", "range", "(", "len", "(", "lines", ")", ")", "\n", "# random_per = np.random.permutation(len(lines))", "\n", "# random.shuffle(lines)", "\n", "\n", "# existing_title_set = set()", "\n", "\n", "batch_list", "=", "[", "]", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "ABS_PROCESS_FOR_RINDEX_DB", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "abs_rindex_db", ":", "\n", "        ", "for", "index", "in", "tqdm", "(", "random_per", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "lines", "[", "index", "]", ")", "\n", "# print(item.keys())", "\n", "# print()", "\n", "if", "item", "[", "'title'", "]", "in", "abs_rindex_db", ":", "\n", "                ", "continue", "\n", "\n", "", "tokens", ",", "sent_offset", "=", "get_sentence_tokens", "(", "item", "[", "'text'", "]", ",", "item", "[", "'charoffset'", "]", ")", "\n", "poss", "=", "spacy_get_pos", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "poss", ")", "\n", "# print(tokens)", "\n", "# print(sent_offset)", "\n", "rindex_item", "=", "{", "\n", "'tokens'", ":", "tokens", ",", "\n", "'poss'", ":", "poss", ",", "\n", "'sentence_offset'", ":", "sent_offset", "\n", "}", "\n", "\n", "batch_list", ".", "append", "(", "(", "item", "[", "'title'", "]", ",", "rindex_item", ")", ")", "\n", "\n", "if", "len", "(", "batch_list", ")", "==", "batch_size", ":", "\n", "                ", "for", "title", ",", "rindex_item", "in", "batch_list", ":", "\n", "                    ", "abs_rindex_db", "[", "title", "]", "=", "rindex_item", "\n", "", "abs_rindex_db", ".", "commit", "(", ")", "\n", "batch_list", "=", "[", "]", "\n", "\n", "# Commit last one", "\n", "", "", "for", "title", ",", "rindex_item", "in", "batch_list", ":", "\n", "            ", "abs_rindex_db", "[", "title", "]", "=", "rindex_item", "\n", "\n", "", "abs_rindex_db", ".", "commit", "(", ")", "\n", "abs_rindex_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize": [[43, 46], ["unicodedata.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["def", "normalize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Resolve different type of unicode encodings.\"\"\"", "\n", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word": [[48, 56], ["build_wiki_rindex.normalize", "regex.match", "normalize.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "def", "filter_word", "(", "text", ")", ":", "\n", "    ", "\"\"\"Take out english stopwords, punctuation, and compound endings.\"\"\"", "\n", "text", "=", "normalize", "(", "text", ")", "\n", "if", "regex", ".", "match", "(", "r'^\\p{P}+$'", ",", "text", ")", ":", "\n", "        ", "return", "True", "\n", "", "if", "text", ".", "lower", "(", ")", "in", "STOPWORDS", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_ngram": [[58, 76], ["build_wiki_rindex.filter_word", "any", "all", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.filter_word"], ["", "def", "filter_ngram", "(", "gram", ",", "mode", "=", "'any'", ")", ":", "\n", "    ", "\"\"\"Decide whether to keep or discard an n-gram.\n    Args:\n        gram: list of tokens (length N)\n        mode: Option to throw out ngram if\n          'any': any single token passes filter_word\n          'all': all tokens pass filter_word\n          'ends': book-ended by filterable tokens\n    \"\"\"", "\n", "filtered", "=", "[", "filter_word", "(", "w", ")", "for", "w", "in", "gram", "]", "\n", "if", "mode", "==", "'any'", ":", "\n", "        ", "return", "any", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'all'", ":", "\n", "        ", "return", "all", "(", "filtered", ")", "\n", "", "elif", "mode", "==", "'ends'", ":", "\n", "        ", "return", "filtered", "[", "0", "]", "or", "filtered", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Invalid mode: %s'", "%", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.get_ngrams": [[78, 113], ["filter_fn", "range", "range", "any", "len", "min", "build_wiki_rindex.get_ngrams._skip"], "function", ["None"], ["", "", "def", "get_ngrams", "(", "terms", ",", "poss", "=", "None", ",", "n", "=", "1", ",", "filter_fn", "=", "None", ",", "included_tags", "=", "None", ",", "as_strings", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "\"\"\"Returns a list of all ngrams from length 1 to n.\n    \"\"\"", "\n", "\n", "def", "_skip", "(", "gram", ")", ":", "\n", "        ", "if", "not", "filter_fn", ":", "\n", "            ", "return", "False", "\n", "", "return", "filter_fn", "(", "gram", ")", "\n", "\n", "", "ngrams", "=", "[", "(", "s", ",", "e", "+", "1", ")", "\n", "for", "s", "in", "range", "(", "len", "(", "terms", ")", ")", "\n", "for", "e", "in", "range", "(", "s", ",", "min", "(", "s", "+", "n", ",", "len", "(", "terms", ")", ")", ")", "\n", "if", "not", "_skip", "(", "terms", "[", "s", ":", "e", "+", "1", "]", ")", "]", "\n", "\n", "if", "poss", "is", "not", "None", "and", "included_tags", "is", "not", "None", ":", "# We do filtering according to pos.", "\n", "        ", "filtered_ngram", "=", "[", "]", "\n", "for", "(", "s", ",", "e", ")", "in", "ngrams", ":", "\n", "            ", "if", "any", "(", "[", "poss", "[", "i", "]", "in", "included_tags", "for", "i", "in", "range", "(", "s", ",", "e", ")", "]", ")", ":", "\n", "                ", "filtered_ngram", ".", "append", "(", "(", "s", ",", "e", ")", ")", "\n", "\n", "", "", "ngrams", "=", "filtered_ngram", "\n", "\n", "# Concatenate into strings", "\n", "", "if", "as_strings", ":", "\n", "        ", "r_list", "=", "[", "]", "\n", "for", "(", "s", ",", "e", ")", "in", "ngrams", ":", "\n", "            ", "if", "lower", ":", "\n", "                ", "r_list", ".", "append", "(", "' '", ".", "join", "(", "terms", "[", "s", ":", "e", "]", ")", ".", "lower", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "r_list", ".", "append", "(", "' '", ".", "join", "(", "terms", "[", "s", ":", "e", "]", ")", ")", "\n", "\n", "", "", "return", "r_list", "\n", "\n", "", "else", ":", "\n", "        ", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing": [[115, 205], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "build_rindex.build_rvindex.IndexDB", "sqlitedict.SqliteDict", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "str", "json.loads", "inspect_wikidump.inspect_whole_file.get_first_paragraph_index", "enumerate", "build_wiki_rindex.get_ngrams", "build_wiki_rindex.get_ngrams", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "len", "len", "zip", "zip", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "functools.partial", "functools.partial", "title_term_list.extend", "title_poss_list.extend", "article_term_list.extend", "article_poss_list.extend", "abstract_term_list.extend", "abstract_poss_list.extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "", "def", "whole_wiki_pages_title_raw_indexing", "(", ")", ":", "\n", "    ", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "title_abs_raw_indexdb", "=", "IndexDB", "(", ")", "\n", "abs_file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb\"", "\n", "\n", "content_indexdb", "=", "IndexDB", "(", ")", "\n", "content_index_file_name", "=", "''", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ",", "flag", "=", "'r'", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_wiki_db", ":", "\n", "        ", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "            ", "valid_page", "=", "True", "\n", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "# print(item)", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "abs_index", "=", "get_first_paragraph_index", "(", "whole_wiki_db", "[", "article_title", "]", ")", "\n", "\n", "if", "abs_index", "==", "-", "1", ":", "\n", "                ", "valid_page", "=", "False", "\n", "\n", "# print(whole_wiki_db[article_title])", "\n", "# This pages is not valid.", "\n", "\n", "", "article_term_list", "=", "[", "]", "\n", "article_poss_list", "=", "[", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "abstract_term_list", "=", "[", "]", "\n", "abstract_poss_list", "=", "[", "]", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "                ", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                    ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                        ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "\n", "                        ", "if", "p_i", "==", "abs_index", ":", "# If the terms are in abstract", "\n", "                            ", "abstract_term_list", ".", "extend", "(", "sent_text", ")", "\n", "abstract_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "article_term_list", ".", "extend", "(", "sent_text", ")", "\n", "article_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "# print(\"Title:\", title_term_list, title_poss_list)", "\n", "\n", "", "", "", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "3", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "abs_ngram", "=", "get_ngrams", "(", "abstract_term_list", ",", "abstract_poss_list", ",", "3", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "# print(article_title)", "\n", "# print(title_ngram)", "\n", "# print(abs_ngram)", "\n", "\n", "added_terms_num", "=", "0", "\n", "for", "added_term", "in", "title_ngram", "+", "abs_ngram", ":", "\n", "                ", "title_abs_raw_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "article_title", ")", "\n", "added_terms_num", "+=", "1", "\n", "\n", "", "title_abs_raw_indexdb", ".", "document_length_table", ".", "add", "(", "article_title", ",", "added_terms_num", ")", "\n", "# break", "\n", "\n", "#     content_t_ngram = get_ngrams(title_term_list, title_poss_list, 3,", "\n", "#                                  filter_fn=partial(filter_ngram, mode='any'),", "\n", "#                                  included_tags=POS_INCLUDED)", "\n", "#", "\n", "#     content_c_ngram = get_ngrams(abstract_term_list, abstract_poss_list, 3,", "\n", "#                                  filter_fn=partial(filter_ngram, mode='any'),", "\n", "#                                  included_tags=POS_INCLUDED)", "\n", "#", "\n", "#     added_terms_num = 0", "\n", "#     for added_term in content_t_ngram + content_c_ngram:", "\n", "#         content_indexdb.inverted_index.add(added_term, article_title)", "\n", "#         added_terms_num += 1", "\n", "#", "\n", "#     content_indexdb.document_length_table.add(article_title, added_terms_num)", "\n", "#", "\n", "", "title_abs_raw_indexdb", ".", "save_to_file", "(", "abs_file_name", ")", "\n", "# print(title_term_list)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_paragraph_level": [[215, 294], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "set", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "build_rindex.term_manage.load_wiki_abstract_terms", "json.loads", "enumerate", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "key_separator.join", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "build_wiki_rindex.get_ngrams", "len", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "str", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "functools.partial", "build_rindex.build_rvindex.IndexDB.inverted_index.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.term_manage.load_wiki_abstract_terms", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "", "def", "whole_wiki_pages_title_raw_indexing_paragraph_level", "(", "limited_terms", "=", "True", ")", ":", "\n", "    ", "key_separator", "=", "'/'", "\n", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "wiki_p_level_indexdb", "=", "IndexDB", "(", ")", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_p_level_limited_gram_rindexdb\"", "\n", "\n", "count", "=", "0", "\n", "\n", "if", "limited_terms", ":", "\n", "        ", "limited_terms_set", "=", "load_wiki_abstract_terms", "(", "config", ".", "PRO_ROOT", "/", "\"data/processed/wiki_abs_3gram_terms.txt\"", ")", "\n", "", "else", ":", "\n", "        ", "limited_terms_set", "=", "[", "]", "\n", "\n", "", "limited_terms_set", "=", "set", "(", "limited_terms_set", ")", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "", "if", "p_i", ">=", "100", ":", "\n", "                ", "break", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "added_terms_num", "=", "0", "\n", "\n", "paragraph_key", "=", "key_separator", ".", "join", "(", "(", "article_title", ",", "str", "(", "p_i", ")", ")", ")", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "paragraph_ngram", ":", "\n", "                ", "if", "added_term", "in", "limited_terms_set", ":", "\n", "                    ", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "paragraph_key", ")", "\n", "added_terms_num", "+=", "1", "\n", "", "elif", "' '", "not", "in", "added_term", ":", "\n", "                    ", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "paragraph_key", ")", "\n", "added_terms_num", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "document_length_table", ".", "add", "(", "paragraph_key", ",", "added_terms_num", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "# if count >= 1000:", "\n", "#     break", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "save_to_file", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_paragraph_level_unigram": [[296, 375], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "json.loads", "enumerate", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "key_separator.join", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "build_wiki_rindex.get_ngrams", "len", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "str", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "whole_wiki_pages_title_raw_indexing_paragraph_level_unigram", "(", ")", ":", "\n", "    ", "key_separator", "=", "'/'", "\n", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "wiki_p_level_indexdb", "=", "IndexDB", "(", ")", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_p_level_unigram_rindexdb\"", "\n", "\n", "count", "=", "0", "\n", "\n", "# if limited_terms:", "\n", "#     limited_terms_set = load_wiki_abstract_terms(config.PRO_ROOT / \"data/processed/wiki_abs_3gram_terms.txt\")", "\n", "# else:", "\n", "#     limited_terms_set = []", "\n", "#", "\n", "# limited_terms_set = set(limited_terms_set)", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "", "if", "p_i", ">=", "100", ":", "\n", "                ", "break", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "added_terms_num", "=", "0", "\n", "\n", "paragraph_key", "=", "key_separator", ".", "join", "(", "(", "article_title", ",", "str", "(", "p_i", ")", ")", ")", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "paragraph_ngram", ":", "\n", "# if added_term in limited_terms_set:", "\n", "#     wiki_p_level_indexdb.inverted_index.add(added_term, paragraph_key)", "\n", "#     added_terms_num += 1", "\n", "# elif ' ' not in added_term:", "\n", "                ", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "paragraph_key", ")", "\n", "added_terms_num", "+=", "1", "\n", "# else:", "\n", "#     pass", "\n", "\n", "", "wiki_p_level_indexdb", ".", "document_length_table", ".", "add", "(", "paragraph_key", ",", "added_terms_num", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "", "if", "count", ">=", "1000", ":", "\n", "            ", "break", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "save_to_file", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_paragraph_level_unigram_size_limited": [[377, 458], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "json.loads", "enumerate", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "key_separator.join", "build_wiki_rindex.hash", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "build_wiki_rindex.get_ngrams", "len", "build_wiki_rindex.hash", "build_wiki_rindex.hash", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "str", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "whole_wiki_pages_title_raw_indexing_paragraph_level_unigram_size_limited", "(", "hash_size", "=", "2", "**", "24", ")", ":", "\n", "    ", "key_separator", "=", "'/'", "\n", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "wiki_p_level_indexdb", "=", "IndexDB", "(", ")", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_p_level_unigram_rindexdb_hash_size_limited\"", "\n", "\n", "count", "=", "0", "\n", "# if limited_terms:", "\n", "#     limited_terms_set = load_wiki_abstract_terms(config.PRO_ROOT / \"data/processed/wiki_abs_3gram_terms.txt\")", "\n", "# else:", "\n", "#     limited_terms_set = []", "\n", "#", "\n", "# limited_terms_set = set(limited_terms_set)", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "", "if", "p_i", ">=", "100", ":", "\n", "                ", "break", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "added_terms_num", "=", "0", "\n", "\n", "paragraph_key", "=", "key_separator", ".", "join", "(", "(", "article_title", ",", "str", "(", "p_i", ")", ")", ")", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "paragraph_ngram", ":", "\n", "# if added_term in limited_terms_set:", "\n", "#     wiki_p_level_indexdb.inverted_index.add(added_term, paragraph_key)", "\n", "#     added_terms_num += 1", "\n", "# elif ' ' not in added_term:", "\n", "                ", "hash_value_added_term", "=", "hash", "(", "added_term", ",", "hash_size", ")", "\n", "hash_value_paragraph_key", "=", "hash", "(", "paragraph_key", ")", "\n", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "hash_value_added_term", ",", "hash_value_paragraph_key", ")", "\n", "added_terms_num", "+=", "1", "\n", "# else:", "\n", "#     pass", "\n", "\n", "", "hash_value_paragraph_key", "=", "hash", "(", "paragraph_key", ")", "\n", "wiki_p_level_indexdb", ".", "document_length_table", ".", "add", "(", "hash_value_paragraph_key", ",", "added_terms_num", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "", "if", "count", ">=", "1000", ":", "\n", "            ", "break", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "save_to_file", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_paragraph_level_unigram_size_limited_memory_saving": [[462, 543], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "json.loads", "enumerate", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "key_separator.join", "build_wiki_rindex.hash", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "build_wiki_rindex.get_ngrams", "len", "build_wiki_rindex.hash", "build_wiki_rindex.hash", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "str", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "whole_wiki_pages_title_raw_indexing_paragraph_level_unigram_size_limited_memory_saving", "(", ")", ":", "\n", "    ", "key_separator", "=", "'/'", "\n", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "wiki_p_level_indexdb", "=", "IndexDB", "(", ")", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_p_level_unigram_rindexdb\"", "\n", "\n", "count", "=", "0", "\n", "# if limited_terms:", "\n", "#     limited_terms_set = load_wiki_abstract_terms(config.PRO_ROOT / \"data/processed/wiki_abs_3gram_terms.txt\")", "\n", "# else:", "\n", "#     limited_terms_set = []", "\n", "#", "\n", "# limited_terms_set = set(limited_terms_set)", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "", "if", "p_i", ">=", "100", ":", "\n", "                ", "break", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "added_terms_num", "=", "0", "\n", "\n", "paragraph_key", "=", "key_separator", ".", "join", "(", "(", "article_title", ",", "str", "(", "p_i", ")", ")", ")", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "paragraph_ngram", ":", "\n", "# if added_term in limited_terms_set:", "\n", "#     wiki_p_level_indexdb.inverted_index.add(added_term, paragraph_key)", "\n", "#     added_terms_num += 1", "\n", "# elif ' ' not in added_term:", "\n", "                ", "hash_value_added_term", "=", "hash", "(", "added_term", ")", "\n", "hash_value_paragraph_key", "=", "hash", "(", "paragraph_key", ")", "\n", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "hash_value_added_term", ",", "hash_value_paragraph_key", ")", "\n", "added_terms_num", "+=", "1", "\n", "# else:", "\n", "#     pass", "\n", "\n", "", "hash_value_paragraph_key", "=", "hash", "(", "paragraph_key", ")", "\n", "wiki_p_level_indexdb", ".", "document_length_table", ".", "add", "(", "hash_value_paragraph_key", ",", "added_terms_num", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "# if count >= 1000:", "\n", "#     break", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "save_to_file", "(", "file_name", ",", "memory_saving", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_paragraph_level_to_indexdb": [[545, 634], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.persistent_index_db.IndexingDB", "build_rindex.persistent_index_db.IndexingDB.create_tables", "tqdm.tqdm", "build_rindex.persistent_index_db.IndexingDB.insert_many_items", "build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "build_rindex.persistent_index_db.IndexingDB.close", "json.loads", "dict", "dict", "enumerate", "paragraph_term_title_dict.items", "paragraph_title_dict.items", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "key_separator.join", "term_title_items_buffer_list.append", "title_items_buffer_list.append", "len", "build_rindex.persistent_index_db.IndexingDB.insert_many_items", "build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "build_wiki_rindex.get_ngrams", "len", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "str", "paragraph_term_title_dict.get", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.create_tables", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_items", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_items", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams"], ["", "def", "whole_wiki_pages_title_raw_indexing_paragraph_level_to_indexdb", "(", ")", ":", "\n", "    ", "key_separator", "=", "'/'", "\n", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "# wiki_p_level_indexdb = IndexDB()", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_p_level_persistent_indexdb.db\"", "\n", "index_db", "=", "IndexingDB", "(", "file_name", ")", "\n", "index_db", ".", "create_tables", "(", ")", "\n", "\n", "count", "=", "0", "\n", "\n", "term_title_items_buffer_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "title_items_buffer_list", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "paragraph_term_title_dict", ":", "Dict", "[", "Tuple", "[", "str", ",", "str", "]", ",", "int", "]", "=", "dict", "(", ")", "\n", "paragraph_title_dict", ":", "Dict", "[", "str", ",", "int", "]", "=", "dict", "(", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "continue", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "added_terms_num", "=", "0", "\n", "\n", "paragraph_key", "=", "key_separator", ".", "join", "(", "(", "article_title", ",", "str", "(", "p_i", ")", ")", ")", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "paragraph_ngram", ":", "\n", "                ", "paragraph_term_title_dict", "[", "(", "added_term", ",", "paragraph_key", ")", "]", "=", "paragraph_term_title_dict", ".", "get", "(", "(", "added_term", ",", "paragraph_key", ")", ",", "0", ")", "+", "1", "\n", "added_terms_num", "+=", "1", "\n", "\n", "", "paragraph_title_dict", "[", "paragraph_key", "]", "=", "added_terms_num", "\n", "count", "+=", "1", "\n", "\n", "if", "p_i", ">=", "60", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "count", ">=", "5000", ":", "\n", "            ", "break", "\n", "\n", "", "for", "(", "term", ",", "paragraph_key", ")", ",", "ovalue", "in", "paragraph_term_title_dict", ".", "items", "(", ")", ":", "\n", "            ", "term_title_items_buffer_list", ".", "append", "(", "(", "term", ",", "paragraph_key", ",", "ovalue", ")", ")", "\n", "\n", "", "for", "paragraph_title", ",", "ovalue", "in", "paragraph_title_dict", ".", "items", "(", ")", ":", "\n", "            ", "title_items_buffer_list", ".", "append", "(", "(", "paragraph_title", ",", "ovalue", ")", ")", "\n", "\n", "", "if", "len", "(", "term_title_items_buffer_list", ")", ">=", "1000", ":", "# Flush", "\n", "            ", "index_db", ".", "insert_many_items", "(", "term_title_items_buffer_list", ")", "\n", "index_db", ".", "insert_many_articles", "(", "title_items_buffer_list", ")", "\n", "term_title_items_buffer_list", "=", "[", "]", "\n", "title_items_buffer_list", "=", "[", "]", "\n", "\n", "", "", "index_db", ".", "insert_many_items", "(", "term_title_items_buffer_list", ")", "\n", "index_db", ".", "insert_many_articles", "(", "title_items_buffer_list", ")", "\n", "index_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_article_level_to_indexdb": [[636, 726], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.persistent_index_db.IndexingDB", "build_rindex.persistent_index_db.IndexingDB.create_tables", "tqdm.tqdm", "build_rindex.persistent_index_db.IndexingDB.insert_many_items", "build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "build_rindex.persistent_index_db.IndexingDB.close", "dict", "dict", "json.loads", "enumerate", "article_term_title_dict.items", "article_title_dict.items", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "article_ngram.extend", "term_title_items_buffer_list.append", "title_items_buffer_list.append", "len", "build_rindex.persistent_index_db.IndexingDB.insert_many_items", "build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "build_wiki_rindex.get_ngrams", "len", "article_term_title_dict.get", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.create_tables", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_items", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_items", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.insert_many_articles", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams"], ["", "def", "whole_wiki_pages_title_raw_indexing_article_level_to_indexdb", "(", ")", ":", "\n", "    ", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "# wiki_p_level_indexdb = IndexDB()", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_a_level_persistent_indexdb.db\"", "\n", "index_db", "=", "IndexingDB", "(", "file_name", ")", "\n", "index_db", ".", "create_tables", "(", ")", "\n", "\n", "count", "=", "0", "\n", "\n", "term_title_items_buffer_list", ":", "List", "[", "Tuple", "[", "str", ",", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "title_items_buffer_list", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "article_term_title_dict", ":", "Dict", "[", "Tuple", "[", "str", ",", "str", "]", ",", "int", "]", "=", "dict", "(", ")", "\n", "article_title_dict", ":", "Dict", "[", "str", ",", "int", "]", "=", "dict", "(", ")", "\n", "\n", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "article_ngram", "=", "[", "]", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "continue", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "2", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "article_ngram", ".", "extend", "(", "paragraph_ngram", ")", "\n", "\n", "if", "p_i", ">=", "60", ":", "\n", "                ", "break", "\n", "\n", "", "", "added_terms_num", "=", "0", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "article_ngram", ":", "\n", "            ", "article_term_title_dict", "[", "(", "added_term", ",", "article_title", ")", "]", "=", "article_term_title_dict", ".", "get", "(", "(", "added_term", ",", "article_title", ")", ",", "0", ")", "+", "1", "\n", "added_terms_num", "+=", "1", "\n", "\n", "", "article_title_dict", "[", "article_title", "]", "=", "added_terms_num", "\n", "count", "+=", "1", "\n", "\n", "if", "count", ">=", "200", ":", "\n", "            ", "break", "\n", "\n", "", "for", "(", "term", ",", "article_title", ")", ",", "ovalue", "in", "article_term_title_dict", ".", "items", "(", ")", ":", "\n", "            ", "term_title_items_buffer_list", ".", "append", "(", "(", "term", ",", "article_title", ",", "ovalue", ")", ")", "\n", "\n", "", "for", "article_title", ",", "ovalue", "in", "article_title_dict", ".", "items", "(", ")", ":", "\n", "            ", "title_items_buffer_list", ".", "append", "(", "(", "article_title", ",", "ovalue", ")", ")", "\n", "\n", "", "if", "len", "(", "term_title_items_buffer_list", ")", ">=", "1000", ":", "# Flush", "\n", "            ", "index_db", ".", "insert_many_items", "(", "term_title_items_buffer_list", ")", "\n", "index_db", ".", "insert_many_articles", "(", "title_items_buffer_list", ")", "\n", "term_title_items_buffer_list", "=", "[", "]", "\n", "title_items_buffer_list", "=", "[", "]", "\n", "\n", "", "", "index_db", ".", "insert_many_items", "(", "term_title_items_buffer_list", ")", "\n", "index_db", ".", "insert_many_articles", "(", "title_items_buffer_list", ")", "\n", "index_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.whole_wiki_pages_title_raw_indexing_article_level": [[728, 808], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "build_rindex.build_rvindex.IndexDB", "set", "tqdm.tqdm", "build_rindex.build_rvindex.IndexDB.save_to_file", "build_rindex.term_manage.load_wiki_abstract_terms", "json.loads", "enumerate", "build_rindex.build_rvindex.IndexDB.document_length_table.add", "len", "len", "zip", "zip", "build_wiki_rindex.get_ngrams", "article_ngram.extend", "build_wiki_rindex.get_ngrams", "len", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "title_term_list.extend", "title_poss_list.extend", "paragraph_term_list.extend", "paragraph_poss_list.extend", "functools.partial", "build_rindex.build_rvindex.IndexDB.inverted_index.add", "functools.partial"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.term_manage.load_wiki_abstract_terms", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "whole_wiki_pages_title_raw_indexing_article_level", "(", "limited_terms", "=", "True", ")", ":", "\n", "    ", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "wiki_p_level_indexdb", "=", "IndexDB", "(", ")", "\n", "file_name", "=", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/wiki_a_level_limited_gram_rindexdb\"", "\n", "\n", "if", "limited_terms", ":", "\n", "        ", "limited_terms_set", "=", "load_wiki_abstract_terms", "(", "config", ".", "PRO_ROOT", "/", "\"data/processed/wiki_abs_3gram_terms.txt\"", ")", "\n", "", "else", ":", "\n", "        ", "limited_terms_set", "=", "[", "]", "\n", "\n", "", "limited_terms_set", "=", "set", "(", "limited_terms_set", ")", "\n", "\n", "count", "=", "0", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "whole_tokenized_db_cursor", ",", "total", "=", "TOTAL_NUM_DOC", ")", ":", "\n", "        ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "title_ngram", "=", "None", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "# article_term_list = []", "\n", "# article_poss_list = []", "\n", "article_ngram", "=", "[", "]", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "            ", "paragraph_term_list", "=", "[", "]", "\n", "paragraph_poss_list", "=", "[", "]", "\n", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                    ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "# p_i != 0", "\n", "                    ", "paragraph_term_list", ".", "extend", "(", "sent_text", ")", "\n", "paragraph_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "if", "p_i", "==", "0", "and", "title_ngram", "is", "None", ":", "\n", "                ", "title_ngram", "=", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "continue", "\n", "\n", "", "paragraph_ngram", "=", "get_ngrams", "(", "paragraph_term_list", ",", "paragraph_poss_list", ",", "1", ",", "\n", "filter_fn", "=", "partial", "(", "filter_ngram", ",", "mode", "=", "'any'", ")", ",", "\n", "included_tags", "=", "POS_INCLUDED", ")", "\n", "if", "len", "(", "paragraph_ngram", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "article_ngram", ".", "extend", "(", "paragraph_ngram", ")", "\n", "\n", "if", "p_i", ">=", "80", ":", "\n", "                ", "break", "\n", "\n", "", "", "added_terms_num", "=", "0", "\n", "\n", "for", "added_term", "in", "title_ngram", "+", "article_ngram", ":", "\n", "            ", "if", "added_term", "in", "limited_terms_set", ":", "\n", "                ", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "article_title", ")", "\n", "added_terms_num", "+=", "1", "\n", "", "elif", "' '", "not", "in", "added_term", ":", "\n", "                ", "wiki_p_level_indexdb", ".", "inverted_index", ".", "add", "(", "added_term", ",", "article_title", ")", "\n", "added_terms_num", "+=", "1", "\n", "\n", "", "", "wiki_p_level_indexdb", ".", "document_length_table", ".", "add", "(", "article_title", ",", "added_terms_num", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "# if count >= 5000:", "\n", "#     break", "\n", "\n", "", "wiki_p_level_indexdb", ".", "save_to_file", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.hash": [[810, 816], ["sklearn.utils.murmurhash3_32", "sklearn.utils.murmurhash3_32"], "function", ["None"], ["", "def", "hash", "(", "token", ",", "num_buckets", "=", "None", ")", ":", "\n", "    ", "\"\"\"Unsigned 32 bit murmurhash for feature hashing.\"\"\"", "\n", "if", "num_buckets", "is", "None", ":", "\n", "        ", "return", "murmurhash3_32", "(", "token", ",", "positive", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "return", "murmurhash3_32", "(", "token", ",", "positive", "=", "True", ")", "%", "num_buckets", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams": [[20, 44], ["range", "range", "any", "len", "min", "filtered_ngram.append", "len", "range"], "function", ["None"], ["def", "get_ngrams", "(", "terms", ",", "poss", "=", "None", ",", "n", "=", "1", ",", "included_tags", "=", "None", ",", "as_strings", "=", "True", ")", ":", "\n", "    ", "\"\"\"Returns a list of all ngrams from length 1 to n.\n    \"\"\"", "\n", "ngrams", "=", "[", "(", "s", ",", "e", "+", "1", ")", "\n", "for", "s", "in", "range", "(", "len", "(", "terms", ")", ")", "\n", "for", "e", "in", "range", "(", "s", ",", "min", "(", "s", "+", "n", ",", "len", "(", "terms", ")", ")", ")", "]", "\n", "\n", "if", "poss", "is", "not", "None", "and", "included_tags", "is", "not", "None", ":", "# We do filtering according to pos.", "\n", "# ngrampos = [(s, e + 1)", "\n", "#             for s in range(len(poss))", "\n", "#             for e in range(s, min(s + n, len(poss)))]", "\n", "\n", "        ", "filtered_ngram", "=", "[", "]", "\n", "for", "(", "s", ",", "e", ")", "in", "ngrams", ":", "\n", "            ", "if", "any", "(", "[", "poss", "[", "i", "]", "in", "included_tags", "for", "i", "in", "range", "(", "s", ",", "e", ")", "]", ")", ":", "\n", "                ", "filtered_ngram", ".", "append", "(", "(", "s", ",", "e", ")", ")", "\n", "\n", "", "", "ngrams", "=", "filtered_ngram", "\n", "\n", "# Concatenate into strings", "\n", "", "if", "as_strings", ":", "\n", "        ", "ngrams", "=", "[", "'{}'", ".", "format", "(", "' '", ".", "join", "(", "terms", "[", "s", ":", "e", "]", ")", ")", "for", "(", "s", ",", "e", ")", "in", "ngrams", "]", "\n", "\n", "", "return", "ngrams", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.whole_wiki_pages_analysis": [[161, 209], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "sqlitedict.SqliteDict", "str", "json.loads", "inspect_wikidump.inspect_whole_file.get_first_paragraph_index", "enumerate", "print", "print", "len", "len", "zip", "zip", "document_analysis.get_ngrams", "title_term_list.extend", "title_poss_list.extend", "article_term_list.extend", "article_poss_list.extend", "abstract_term_list.extend", "abstract_poss_list.extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.document_analysis.get_ngrams"], ["def", "whole_wiki_pages_analysis", "(", ")", ":", "\n", "    ", "whole_tokenized_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "whole_tokenized_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ",", "flag", "=", "'r'", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_wiki_db", ":", "\n", "        ", "for", "key", ",", "value", "in", "whole_tokenized_db_cursor", ":", "\n", "            ", "valid_page", "=", "True", "\n", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "# print(item)", "\n", "article_title", "=", "item", "[", "'title'", "]", "\n", "article_clean_text", "=", "item", "[", "'clean_text'", "]", "\n", "article_poss", "=", "item", "[", "'poss'", "]", "\n", "\n", "abs_index", "=", "get_first_paragraph_index", "(", "whole_wiki_db", "[", "article_title", "]", ")", "\n", "\n", "if", "abs_index", "==", "-", "1", ":", "\n", "                ", "valid_page", "=", "False", "\n", "# print(whole_wiki_db[article_title])", "\n", "# This pages is not valid.", "\n", "\n", "", "article_term_list", "=", "[", "]", "\n", "article_poss_list", "=", "[", "]", "\n", "\n", "title_term_list", "=", "[", "]", "\n", "title_poss_list", "=", "[", "]", "\n", "\n", "abstract_term_list", "=", "[", "]", "\n", "abstract_poss_list", "=", "[", "]", "\n", "\n", "assert", "len", "(", "article_clean_text", ")", "==", "len", "(", "article_poss", ")", "\n", "\n", "for", "p_i", ",", "(", "paragraph_text", ",", "paragraph_poss", ")", "in", "enumerate", "(", "zip", "(", "article_clean_text", ",", "article_poss", ")", ")", ":", "\n", "                ", "for", "sent_text", ",", "sent_poss", "in", "zip", "(", "paragraph_text", ",", "paragraph_poss", ")", ":", "\n", "                    ", "if", "p_i", "==", "0", ":", "# In title.", "\n", "                        ", "title_term_list", ".", "extend", "(", "sent_text", ")", "\n", "title_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "continue", "# If the terms are in title, we don't those terms in abstract and article term.", "\n", "", "else", ":", "\n", "                        ", "if", "p_i", "==", "abs_index", ":", "# If the terms are in abstract", "\n", "                            ", "abstract_term_list", ".", "extend", "(", "sent_text", ")", "\n", "abstract_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "article_term_list", ".", "extend", "(", "sent_text", ")", "\n", "article_poss_list", ".", "extend", "(", "sent_poss", ")", "\n", "\n", "", "", "", "print", "(", "\"Title:\"", ",", "title_term_list", ",", "title_poss_list", ")", "\n", "\n", "print", "(", "\"Title:(ngram):\"", ",", "get_ngrams", "(", "title_term_list", ",", "title_poss_list", ",", "3", ",", "included_tags", "=", "POS_INCLUDED", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndexOld.__init__": [[16, 18], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "redis_db", ":", "redis", ".", "Redis", ")", ":", "\n", "        ", "self", ".", "redis_db", ":", "redis", ".", "Redis", "=", "redis_db", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndexOld.get_containing_document": [[19, 25], ["redis_index.RedisScoreIndexOld.redis_db.get", "json.loads().keys", "json.loads"], "methods", ["None"], ["", "def", "get_containing_document", "(", "self", ",", "term", ")", ":", "\n", "        ", "item", "=", "self", ".", "redis_db", ".", "get", "(", "term", ")", "\n", "if", "item", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "json", ".", "loads", "(", "item", ")", ".", "keys", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndexOld.get_score": [[26, 36], ["redis_index.RedisScoreIndexOld.redis_db.get", "json.loads"], "methods", ["None"], ["", "", "def", "get_score", "(", "self", ",", "term", ",", "docid", ")", ":", "\n", "        ", "item", "=", "self", ".", "redis_db", ".", "get", "(", "term", ")", "\n", "if", "item", "is", "None", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "item", ")", "\n", "if", "docid", "not", "in", "item", ":", "\n", "                ", "return", "0", "\n", "", "else", ":", "\n", "                ", "return", "item", "[", "docid", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndexOld.get_score_item": [[37, 43], ["redis_index.RedisScoreIndexOld.redis_db.get", "json.loads"], "methods", ["None"], ["", "", "", "def", "get_score_item", "(", "self", ",", "term", ")", ":", "\n", "        ", "item", "=", "self", ".", "redis_db", ".", "get", "(", "term", ")", "\n", "if", "item", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "json", ".", "loads", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndexOld.save_scored_index": [[44, 51], ["print", "tqdm.tqdm.tqdm", "redis_index.RedisScoreIndexOld.redis_db.save", "scored_index.keys", "redis_index.RedisScoreIndexOld.redis_db.set", "json.dumps"], "methods", ["None"], ["", "", "def", "save_scored_index", "(", "self", ",", "scored_index", ")", ":", "\n", "        ", "print", "(", "\"Save scored term-doc index to Redis.\"", ")", "\n", "for", "key", "in", "tqdm", "(", "scored_index", ".", "keys", "(", ")", ")", ":", "\n", "            ", "item", "=", "scored_index", "[", "key", "]", "\n", "self", ".", "redis_db", ".", "set", "(", "key", ",", "json", ".", "dumps", "(", "item", ")", ")", "\n", "\n", "", "self", ".", "redis_db", ".", "save", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.scored_dict_ranking": [[61, 80], ["scored_dict.keys", "heapq.heappushpop", "heapq.heappush", "len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "scored_dict_ranking", "(", "candidate_doc_list", ",", "scored_dict", ",", "top_k", ")", ":", "\n", "        ", "scored_doc", "=", "[", "]", "\n", "v_terms", "=", "scored_dict", ".", "keys", "(", ")", "\n", "\n", "for", "cur_doc", "in", "candidate_doc_list", ":", "\n", "            ", "cur_doc_score", "=", "0", "\n", "for", "cur_term", "in", "v_terms", ":", "\n", "                ", "if", "cur_doc", "not", "in", "scored_dict", "[", "cur_term", "]", ":", "\n", "                    ", "cur_doc_score", "+=", "0", "\n", "", "else", ":", "\n", "                    ", "cur_doc_score", "+=", "scored_dict", "[", "cur_term", "]", "[", "cur_doc", "]", "\n", "\n", "", "", "if", "top_k", "is", "not", "None", "and", "0", "<=", "top_k", "==", "len", "(", "scored_doc", ")", ":", "\n", "                ", "heapq", ".", "heappushpop", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "", "else", ":", "\n", "                ", "heapq", ".", "heappush", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "\n", "", "", "return", "scored_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.__init__": [[81, 83], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "redis_db", ":", "redis", ".", "Redis", ")", ":", "\n", "        ", "self", ".", "redis_db", ":", "redis", ".", "Redis", "=", "redis_db", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_containing_document": [[84, 91], ["redis_index.RedisScoreIndex.redis_db.smembers"], "methods", ["None"], ["", "def", "get_containing_document", "(", "self", ",", "term", ")", ":", "\n", "        ", "key", "=", "self", ".", "TERM_PREFIX", "+", "self", ".", "SEP_SYB", "+", "term", "\n", "item", "=", "self", ".", "redis_db", ".", "smembers", "(", "key", ")", "\n", "if", "item", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_score": [[92, 99], ["redis_index.RedisScoreIndex.SEP_SYB.join", "redis_index.RedisScoreIndex.redis_db.get", "float"], "methods", ["None"], ["", "", "def", "get_score", "(", "self", ",", "term", ",", "docid", ")", ":", "\n", "        ", "key", "=", "self", ".", "SEP_SYB", ".", "join", "(", "[", "self", ".", "SCORE_PREFIX", ",", "term", ",", "docid", "]", ")", "\n", "item", "=", "self", ".", "redis_db", ".", "get", "(", "key", ")", "\n", "if", "item", "is", "None", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "float", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_candidate_set_from_batched_terms": [[100, 118], ["redis_index.RedisScoreIndex.redis_db.pipeline", "redis_index.RedisScoreIndex.execute", "zip", "redis_index.RedisScoreIndex.smembers", "list", "len", "valid_terms.append", "valid_set_list.append", "set.union"], "methods", ["None"], ["", "", "def", "get_candidate_set_from_batched_terms", "(", "self", ",", "terms", ")", ":", "\n", "        ", "pipe", "=", "self", ".", "redis_db", ".", "pipeline", "(", ")", "\n", "\n", "valid_terms", "=", "[", "]", "\n", "valid_set_list", "=", "[", "]", "\n", "\n", "for", "term", "in", "terms", ":", "\n", "            ", "key", "=", "self", ".", "TERM_PREFIX", "+", "self", ".", "SEP_SYB", "+", "term", "\n", "pipe", ".", "smembers", "(", "key", ")", "\n", "\n", "", "result_set_list", "=", "pipe", ".", "execute", "(", ")", "\n", "\n", "for", "term", ",", "mset", "in", "zip", "(", "terms", ",", "result_set_list", ")", ":", "\n", "            ", "if", "len", "(", "mset", ")", ">", "0", ":", "\n", "                ", "valid_terms", ".", "append", "(", "term", ")", "\n", "valid_set_list", ".", "append", "(", "mset", ")", "\n", "\n", "", "", "return", "list", "(", "set", ".", "union", "(", "*", "valid_set_list", ")", ")", ",", "valid_set_list", ",", "valid_terms", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.get_scores_from_batched_term_doc_pairs": [[119, 137], ["dict", "zip", "redis_index.RedisScoreIndex.redis_db.pipeline", "redis_index.RedisScoreIndex.execute", "dict", "redis_index.RedisScoreIndex.SEP_SYB.join", "redis_index.RedisScoreIndex.get", "float"], "methods", ["None"], ["", "def", "get_scores_from_batched_term_doc_pairs", "(", "self", ",", "terms", ":", "List", ",", "valid_set_list", ":", "List", ")", ":", "\n", "        ", "scored_results", "=", "dict", "(", ")", "\n", "\n", "# Remember order matters:", "\n", "for", "term", ",", "mset", "in", "zip", "(", "terms", ",", "valid_set_list", ")", ":", "\n", "            ", "pipe", "=", "self", ".", "redis_db", ".", "pipeline", "(", ")", "\n", "for", "docid", "in", "mset", ":", "\n", "                ", "key", "=", "self", ".", "SEP_SYB", ".", "join", "(", "[", "self", ".", "SCORE_PREFIX", ",", "term", ",", "docid", "]", ")", "\n", "pipe", ".", "get", "(", "key", ")", "\n", "", "ritems", "=", "pipe", ".", "execute", "(", ")", "\n", "\n", "scored_results", "[", "term", "]", "=", "dict", "(", ")", "\n", "cur_ptr", "=", "0", "\n", "for", "docid", "in", "mset", ":", "\n", "                ", "scored_results", "[", "term", "]", "[", "docid", "]", "=", "float", "(", "ritems", "[", "cur_ptr", "]", ")", "\n", "cur_ptr", "+=", "1", "\n", "\n", "", "", "return", "scored_results", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.save_scored_index": [[138, 150], ["print", "tqdm.tqdm.tqdm", "scored_index.keys", "redis_index.RedisScoreIndex.redis_db.pipeline", "scored_index[].keys", "item.items", "redis_index.RedisScoreIndex.sadd", "redis_index.RedisScoreIndex.execute", "redis_index.RedisScoreIndex.SEP_SYB.join", "redis_index.RedisScoreIndex.set"], "methods", ["None"], ["", "def", "save_scored_index", "(", "self", ",", "scored_index", ")", ":", "\n", "        ", "print", "(", "\"Save scored term-doc index to Redis.\"", ")", "\n", "for", "term", "in", "tqdm", "(", "scored_index", ".", "keys", "(", ")", ")", ":", "\n", "            ", "pipe", "=", "self", ".", "redis_db", ".", "pipeline", "(", ")", "\n", "item", "=", "scored_index", "[", "term", "]", "\n", "doc_set", "=", "scored_index", "[", "term", "]", ".", "keys", "(", ")", "\n", "term_key", "=", "self", ".", "TERM_PREFIX", "+", "self", ".", "SEP_SYB", "+", "term", "\n", "for", "docid", ",", "score", "in", "item", ".", "items", "(", ")", ":", "\n", "                ", "score_key", "=", "self", ".", "SEP_SYB", ".", "join", "(", "[", "self", ".", "SCORE_PREFIX", ",", "term", ",", "docid", "]", ")", "\n", "pipe", ".", "set", "(", "score_key", ",", "score", ")", "\n", "", "pipe", ".", "sadd", "(", "term_key", ",", "*", "doc_set", ")", "\n", "pipe", ".", "execute", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.load_tf_idf_score_to_redis_cache": [[154, 166], ["redis.StrictRedis", "redis_index.RedisScoreIndex", "dict", "build_rindex.build_rvindex.load_from_file", "redis_index.RedisScoreIndex.save_scored_index"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.redis_index.RedisScoreIndex.save_scored_index"], ["", "", "", "def", "load_tf_idf_score_to_redis_cache", "(", ")", ":", "\n", "    ", "tf_idf_score_redis", "=", "redis", ".", "StrictRedis", "(", "host", "=", "'localhost'", ",", "port", "=", "6379", ",", "db", "=", "0", ",", "decode_responses", "=", "True", ")", "\n", "redis_score_index", "=", "RedisScoreIndex", "(", "tf_idf_score_redis", ")", "\n", "# abs_rindexdb = IndexDB()", "\n", "# abs_rindexdb.load_from_file(config.PDATA_ROOT / \"reverse_indexing/abs_rindexdb\")", "\n", "# print(\"Number of terms:\", len(abs_rindexdb.inverted_index.index))", "\n", "# abs_rindexdb.inverted_index.build_Nt_table()", "\n", "score_db", "=", "dict", "(", ")", "\n", "load_from_file", "(", "score_db", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "\n", "redis_score_index", ".", "save_scored_index", "(", "score_db", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.bm25.score_BM25": [[9, 15], ["bm25.compute_K", "math.log"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.bm25.compute_K"], ["def", "score_BM25", "(", "n", ",", "f", ",", "qf", ",", "r", ",", "N", ",", "dl", ",", "avdl", ")", ":", "\n", "    ", "K", "=", "compute_K", "(", "dl", ",", "avdl", ")", "\n", "first", "=", "log", "(", "(", "(", "r", "+", "0.5", ")", "/", "(", "R", "-", "r", "+", "0.5", ")", ")", "/", "(", "(", "n", "-", "r", "+", "0.5", ")", "/", "(", "N", "-", "n", "-", "R", "+", "r", "+", "0.5", ")", ")", ")", "\n", "second", "=", "(", "(", "k1", "+", "1", ")", "*", "f", ")", "/", "(", "K", "+", "f", ")", "\n", "third", "=", "(", "(", "k2", "+", "1", ")", "*", "qf", ")", "/", "(", "k2", "+", "qf", ")", "\n", "return", "first", "*", "second", "*", "third", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.bm25.compute_K": [[17, 19], ["float", "float"], "function", ["None"], ["", "def", "compute_K", "(", "dl", ",", "avdl", ")", ":", "\n", "    ", "return", "k1", "*", "(", "(", "1", "-", "b", ")", "+", "b", "*", "(", "float", "(", "dl", ")", "/", "float", "(", "avdl", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.bm25.get_query_result": [[21, 36], ["dict", "doc_dict.iteritems", "bm25.score_BM25", "len", "dlt.get_length"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.bm25.score_BM25", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.get_length"], ["", "def", "get_query_result", "(", "terms", ",", "idx", ",", "dlt", ",", "dlt_total_length", ",", "dlt_avdl", ")", ":", "\n", "# dlt_total_length = len(dlt)", "\n", "# dlt_avdl = dlt.get_average_length()", "\n", "    ", "query_result", "=", "dict", "(", ")", "\n", "for", "term", "in", "terms", ":", "\n", "        ", "if", "term", "in", "idx", ".", "index", ":", "\n", "            ", "doc_dict", "=", "idx", ".", "index", "[", "term", "]", "# retrieve index entry", "\n", "for", "docid", ",", "freq", "in", "doc_dict", ".", "iteritems", "(", ")", ":", "# for each document and its word frequency", "\n", "                ", "score", "=", "score_BM25", "(", "n", "=", "len", "(", "doc_dict", ")", ",", "f", "=", "freq", ",", "qf", "=", "1", ",", "r", "=", "0", ",", "N", "=", "dlt_total_length", ",", "\n", "dl", "=", "dlt", ".", "get_length", "(", "docid", ")", ",", "avdl", "=", "dlt_avdl", ")", "# calculate score", "\n", "if", "docid", "in", "query_result", ":", "# this document has already been scored once", "\n", "                    ", "query_result", "[", "docid", "]", "+=", "score", "\n", "", "else", ":", "\n", "                    ", "query_result", "[", "docid", "]", "=", "score", "\n", "", "", "", "", "return", "query_result", "\n", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.term_manage.save_wiki_abstract_terms": [[6, 13], ["dict", "build_rindex.build_rvindex.load_from_file", "open", "tqdm.tqdm", "build_rindex.build_rvindex.load_from_file.keys", "out_f.write"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file"], ["def", "save_wiki_abstract_terms", "(", "save_file", ")", ":", "\n", "    ", "g_score_dict", "=", "dict", "(", ")", "\n", "g_score_dict", "=", "load_from_file", "(", "g_score_dict", ",", "\n", "config", ".", "PDATA_ROOT", "/", "\"reverse_indexing/abs_rindexdb/scored_db/default-tf-idf.score.txt\"", ")", "\n", "with", "open", "(", "save_file", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "for", "term", "in", "tqdm", "(", "g_score_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "out_f", ".", "write", "(", "term", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.term_manage.load_wiki_abstract_terms": [[15, 22], ["open", "tqdm.tqdm", "abs_terms.append", "line.strip"], "function", ["None"], ["", "", "", "def", "load_wiki_abstract_terms", "(", "save_file", ")", ":", "\n", "    ", "abs_terms", "=", "[", "]", "\n", "with", "open", "(", "save_file", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "            ", "abs_terms", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "return", "abs_terms", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.remove_all_anchor": [[16, 18], ["re.sub"], "function", ["None"], ["def", "remove_all_anchor", "(", "input_str", ")", ":", "\n", "    ", "return", "re", ".", "sub", "(", "anchor_pattern", ",", "\"\"", ",", "input_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.get_raw_text": [[20, 33], ["article_list.append", "paragraph_sent_list.append", "raw_text_db.remove_all_anchor"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.remove_all_anchor"], ["", "def", "get_raw_text", "(", "item", ")", ":", "\n", "    ", "article_list", "=", "[", "]", "# article level hyperlinks", "\n", "\n", "# Article level", "\n", "for", "paragraph", "in", "item", "[", "'text'", "]", ":", "# Paragraph level", "\n", "# paragraph is a list of strings (each indicates one sentence), and paragraph_offsets is a list of tuples.", "\n", "        ", "paragraph_sent_list", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "paragraph", ":", "\n", "            ", "paragraph_sent_list", ".", "append", "(", "remove_all_anchor", "(", "sentence", ")", ")", "\n", "\n", "", "article_list", ".", "append", "(", "paragraph_sent_list", ")", "# End article level", "\n", "", "return", "article_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.create_raw_text_tables": [[35, 47], ["sqlite3.connect", "sqlite3.connect.cursor", "c.execute", "c.execute", "c.execute", "str"], "function", ["None"], ["", "def", "create_raw_text_tables", "(", "db_path", ",", "table_name", "=", "'raw_text'", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "str", "(", "db_path", ")", ")", "\n", "cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "\n", "c", "=", "cursor", "\n", "c", ".", "execute", "(", "f\"CREATE TABLE {table_name} (\"", "\n", "f\"article_title TEXT NOT NULL, \"", "\n", "f\"p_num INTEGER NOT NULL, \"", "\n", "f\"value TEXT NOT NULL);\"", ")", "\n", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX article_title_index ON {table_name}(article_title);\"", ")", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX paragraph_index ON {table_name}(article_title, p_num);\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.insert_many_raw_text_table": [[49, 51], ["cursor.executemany"], "function", ["None"], ["", "def", "insert_many_raw_text_table", "(", "cursor", ":", "sqlite3", ".", "Cursor", ",", "data", ",", "table_name", "=", "'raw_text'", ")", ":", "\n", "    ", "cursor", ".", "executemany", "(", "f\"INSERT INTO {table_name} (article_title, p_num, value) VALUES (?, ?, ?)\"", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.query_raw_text": [[53, 63], ["cursor.execute", "cursor.execute", "r_list.append"], "function", ["None"], ["", "def", "query_raw_text", "(", "cursor", ",", "article_title", ",", "p_num", "=", "-", "1", ",", "table_name", "=", "'raw_text'", ")", ":", "\n", "    ", "if", "p_num", "<=", "-", "1", ":", "\n", "        ", "cursor", ".", "execute", "(", "f\"SELECT * FROM {table_name} WHERE article_title=?\"", ",", "(", "article_title", ",", ")", ")", "\n", "", "else", ":", "\n", "# print(f\"SELECT * FROM {table_name} WHERE article_title={article_title} AND p_num={p_num}\")", "\n", "        ", "cursor", ".", "execute", "(", "f\"SELECT * FROM {table_name} WHERE article_title=? AND p_num=?\"", ",", "(", "article_title", ",", "p_num", ")", ")", "\n", "", "r_list", "=", "[", "]", "\n", "for", "element", "in", "cursor", ":", "\n", "        ", "r_list", ".", "append", "(", "element", ")", "\n", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.iterative_build_raw_text": [[65, 99], ["wiki_util.wiki_db_tool.get_cursor", "wiki_util.wiki_db_tool.get_cursor.execute", "sqlite3.connect", "sqlite3.connect.cursor", "tqdm.tqdm", "print", "raw_text_db.insert_many_raw_text_table", "sqlite3.connect.commit", "sqlite3.connect.close", "str", "str", "json.loads", "raw_text_db.get_raw_text", "enumerate", "isinstance", "json.dumps", "insert_data_list.append", "sqlite3.connect.commit", "len", "raw_text_db.insert_many_raw_text_table"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.insert_many_raw_text_table", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.get_raw_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.insert_many_raw_text_table"], ["", "def", "iterative_build_raw_text", "(", ")", ":", "\n", "    ", "wiki_whole_db_cursor", "=", "get_cursor", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ")", "\n", "wiki_whole_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "total_count", "=", "0", "\n", "insert_data_list", "=", "[", "]", "\n", "\n", "db_path", "=", "config", ".", "WHOLE_WIKI_RAW_TEXT", "\n", "conn", "=", "sqlite3", ".", "connect", "(", "str", "(", "db_path", ")", ")", "\n", "saving_cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "\n", "for", "key", ",", "value", "in", "tqdm", "(", "wiki_whole_db_cursor", ",", "total", "=", "TOTAL_ARTICLE_COUNT", ")", ":", "\n", "        ", "cur_item", "=", "json", ".", "loads", "(", "value", ")", "\n", "raw_text", "=", "get_raw_text", "(", "cur_item", ")", "\n", "\n", "article_title", "=", "cur_item", "[", "'title'", "]", "\n", "\n", "for", "p_num", ",", "paragraph", "in", "enumerate", "(", "raw_text", ")", ":", "\n", "            ", "assert", "isinstance", "(", "paragraph", ",", "list", ")", "\n", "p_str", "=", "json", ".", "dumps", "(", "paragraph", ")", "\n", "insert_data_list", ".", "append", "(", "(", "article_title", ",", "p_num", ",", "p_str", ")", ")", "\n", "total_count", "+=", "1", "\n", "conn", ".", "commit", "(", ")", "\n", "\n", "", "if", "len", "(", "insert_data_list", ")", ">=", "5000", ":", "\n", "            ", "insert_many_raw_text_table", "(", "saving_cursor", ",", "insert_data_list", ")", "\n", "insert_data_list", "=", "[", "]", "\n", "\n", "# if total_count >= 10000:", "\n", "#     break", "\n", "\n", "", "", "print", "(", "total_count", ")", "\n", "insert_many_raw_text_table", "(", "saving_cursor", ",", "insert_data_list", ")", "\n", "conn", ".", "commit", "(", ")", "\n", "conn", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex_analysis.build_processor": [[16, 21], ["flashtext.KeywordProcessor", "tqdm.tqdm", "idx.index.keys", "flashtext.KeywordProcessor.add_keyword"], "function", ["None"], ["def", "build_processor", "(", "idx", ")", ":", "\n", "    ", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "for", "word", "in", "tqdm", "(", "idx", ".", "index", ".", "keys", "(", ")", ")", ":", "\n", "        ", "keyword_processor", ".", "add_keyword", "(", "word", ")", "\n", "", "return", "keyword_processor", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex_analysis.query_get_terms": [[23, 27], ["kw_processor.extract_keywords", "nlp"], "function", ["None"], ["", "def", "query_get_terms", "(", "query", ",", "kw_processor", ")", ":", "\n", "    ", "tokenized_query", "=", "' '", ".", "join", "(", "[", "t", ".", "text", "for", "t", "in", "nlp", "(", "query", ")", "]", ")", "\n", "terms", "=", "kw_processor", ".", "extract_keywords", "(", "tokenized_query", ")", "\n", "return", "terms", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex_analysis.save_terms": [[29, 33], ["open", "idx.index.keys", "out_f.write"], "function", ["None"], ["", "def", "save_terms", "(", "idx", ":", "InvertedIndex", ",", "file", ")", ":", "\n", "    ", "with", "open", "(", "file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "for", "w", "in", "idx", ".", "index", ".", "keys", "(", ")", ":", "\n", "            ", "out_f", ".", "write", "(", "w", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex_analysis.load_terms": [[35, 42], ["set", "open", "set.add", "line.strip"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "", "", "def", "load_terms", "(", "file", ")", ":", "\n", "    ", "terms", "=", "set", "(", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "in_f", ":", "\n", "            ", "terms", ".", "add", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "return", "terms", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.iterative_build": [[15, 66], ["wiki_util.wiki_db_tool.get_cursor", "wiki_util.wiki_db_tool.get_cursor.execute", "str", "sqlitedict.SqliteDict", "tqdm.tqdm", "whole_rindex_db.commit", "whole_rindex_db.close", "str", "json.loads", "wiki_util.hyperlink.item_get_clean_text", "dict", "enumerate", "build_rindex.rindex.spacy_get_pos", "enumerate", "enumerate", "enumerate", "article_poss.append", "whole_rindex_db.commit", "flatten_article_tokens.extend", "paragraph_poss.append", "sentence_poss.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.item_get_clean_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.spacy_get_pos"], ["def", "iterative_build", "(", ")", ":", "\n", "# wiki_abs_db_cursor = get_cursor(str(config.ABS_WIKI_DB))", "\n", "    ", "wiki_whole_db_cursor", "=", "get_cursor", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ")", "\n", "wiki_whole_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "total_count", "=", "0", "\n", "cur_count", "=", "0", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_rindex_db", ":", "\n", "        ", "for", "key", ",", "value", "in", "tqdm", "(", "wiki_whole_db_cursor", ",", "total", "=", "TOTAL_ARTICLE_COUNT", ")", ":", "\n", "            ", "cur_item", "=", "json", ".", "loads", "(", "value", ")", "\n", "# print(cur_item)", "\n", "clean_text", "=", "item_get_clean_text", "(", "cur_item", ")", "\n", "# print(clean_text)", "\n", "new_item", "=", "dict", "(", ")", "\n", "new_item", "[", "'title'", "]", "=", "cur_item", "[", "'title'", "]", "\n", "\n", "flatten_article_tokens", "=", "[", "]", "\n", "\n", "for", "p_i", ",", "paragraph", "in", "enumerate", "(", "clean_text", ")", ":", "\n", "# flatten_paragraph_tokens = []", "\n", "# paragraph_poss = []", "\n", "                ", "for", "s_i", ",", "sentence", "in", "enumerate", "(", "paragraph", ")", ":", "\n", "                    ", "flatten_article_tokens", ".", "extend", "(", "sentence", ")", "\n", "\n", "# flatten_article_tokens.extend(flatten_paragraph_tokens)", "\n", "\n", "", "", "flatten_article_poss", "=", "spacy_get_pos", "(", "flatten_article_tokens", ")", "\n", "\n", "cur_ptr", "=", "0", "\n", "article_poss", "=", "[", "]", "\n", "for", "p_i", ",", "paragraph", "in", "enumerate", "(", "clean_text", ")", ":", "\n", "                ", "paragraph_poss", "=", "[", "]", "\n", "for", "s_i", ",", "sentence", "in", "enumerate", "(", "paragraph", ")", ":", "\n", "                    ", "sentence_poss", "=", "[", "]", "\n", "for", "_", "in", "sentence", ":", "\n", "                        ", "sentence_poss", ".", "append", "(", "flatten_article_poss", "[", "cur_ptr", "]", ")", "\n", "cur_ptr", "+=", "1", "\n", "", "paragraph_poss", ".", "append", "(", "sentence_poss", ")", "\n", "", "article_poss", ".", "append", "(", "paragraph_poss", ")", "\n", "\n", "", "new_item", "[", "'clean_text'", "]", "=", "clean_text", "\n", "new_item", "[", "'poss'", "]", "=", "article_poss", "\n", "whole_rindex_db", "[", "new_item", "[", "'title'", "]", "]", "=", "new_item", "\n", "\n", "cur_count", "+=", "1", "\n", "\n", "if", "cur_count", "%", "5000", "==", "0", ":", "\n", "                ", "whole_rindex_db", ".", "commit", "(", ")", "\n", "\n", "", "", "whole_rindex_db", ".", "commit", "(", ")", "\n", "whole_rindex_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.remove_all_anchor": [[71, 73], ["re.sub"], "function", ["None"], ["def", "remove_all_anchor", "(", "input_str", ")", ":", "\n", "    ", "return", "re", ".", "sub", "(", "anchor_pattern", ",", "\"\"", ",", "input_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.get_raw_text": [[75, 89], ["article_list.append", "paragraph_sent_list.append", "iterative_building.remove_all_anchor"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.remove_all_anchor"], ["", "def", "get_raw_text", "(", "item", ")", ":", "\n", "# assert len(item['text']) == len(item[\"charoffset\"])", "\n", "    ", "article_list", "=", "[", "]", "# article level hyperlinks", "\n", "\n", "# Article level", "\n", "for", "paragraph", "in", "item", "[", "'text'", "]", ":", "# Paragraph level", "\n", "# paragraph is a list of strings (each indicates one sentence), and paragraph_offsets is a list of tuples.", "\n", "        ", "paragraph_sent_list", "=", "[", "]", "\n", "\n", "for", "sentence", "in", "paragraph", ":", "\n", "            ", "paragraph_sent_list", ".", "append", "(", "remove_all_anchor", "(", "sentence", ")", ")", "\n", "\n", "", "article_list", ".", "append", "(", "paragraph_sent_list", ")", "# End article level", "\n", "", "return", "article_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.create_raw_text_tables": [[91, 103], ["sqlite3.connect", "sqlite3.connect.cursor", "c.execute", "c.execute", "c.execute", "str"], "function", ["None"], ["", "def", "create_raw_text_tables", "(", "db_path", ",", "table_name", "=", "'raw_text'", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "str", "(", "db_path", ")", ")", "\n", "cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "\n", "c", "=", "cursor", "\n", "c", ".", "execute", "(", "f\"CREATE TABLE {table_name} (\"", "\n", "f\"article_title TEXT NOT NULL, \"", "\n", "f\"p_num INTEGER NOT NULL, \"", "\n", "f\"value TEXT NOT NULL);\"", ")", "\n", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX article_title_index ON {table_name}(article_title);\"", ")", "\n", "c", ".", "execute", "(", "f\"CREATE INDEX paragraph_index ON {table_name}(article_title, p_num);\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.insert_many_raw_text_table": [[105, 107], ["cursor.executemany"], "function", ["None"], ["", "def", "insert_many_raw_text_table", "(", "cursor", ":", "sqlite3", ".", "Cursor", ",", "data", ",", "table_name", "=", "'raw_text'", ")", ":", "\n", "    ", "cursor", ".", "executemany", "(", "f\"INSERT INTO {table_name} (article_title, p_num, value) VALUES (?, ?, ?)\"", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.iterative_build_raw_text": [[109, 133], ["wiki_util.wiki_db_tool.get_cursor", "wiki_util.wiki_db_tool.get_cursor.execute", "str", "sqlitedict.SqliteDict", "tqdm.tqdm", "whole_rindex_db.commit", "whole_rindex_db.close", "str", "json.loads", "iterative_building.get_raw_text", "dict", "whole_rindex_db.commit"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.iterative_building.get_raw_text"], ["", "def", "iterative_build_raw_text", "(", ")", ":", "\n", "    ", "wiki_whole_db_cursor", "=", "get_cursor", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ")", "\n", "wiki_whole_db_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "total_count", "=", "0", "\n", "cur_count", "=", "0", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_WIKI_RAW_TEXT", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_rindex_db", ":", "\n", "        ", "for", "key", ",", "value", "in", "tqdm", "(", "wiki_whole_db_cursor", ",", "total", "=", "TOTAL_ARTICLE_COUNT", ")", ":", "\n", "            ", "cur_item", "=", "json", ".", "loads", "(", "value", ")", "\n", "raw_text", "=", "get_raw_text", "(", "cur_item", ")", "\n", "\n", "new_item", "=", "dict", "(", ")", "\n", "new_item", "[", "'title'", "]", "=", "cur_item", "[", "'title'", "]", "\n", "new_item", "[", "'raw_text'", "]", "=", "raw_text", "\n", "whole_rindex_db", "[", "new_item", "[", "'title'", "]", "]", "=", "new_item", "\n", "\n", "cur_count", "+=", "1", "\n", "\n", "if", "cur_count", "%", "5000", "==", "0", ":", "\n", "                ", "whole_rindex_db", ".", "commit", "(", ")", "\n", "# break", "\n", "\n", "", "", "whole_rindex_db", ".", "commit", "(", ")", "\n", "whole_rindex_db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.__init__": [[70, 75], ["object.__init__", "build_rvindex.InvertedIndex", "build_rvindex.DocumentLengthTable", "dict"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inverted_index", "=", "InvertedIndex", "(", ")", "\n", "self", ".", "document_length_table", "=", "DocumentLengthTable", "(", ")", "\n", "self", ".", "score_db", "=", "dict", "(", ")", "# This is a important score_db", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.save_to_file": [[76, 81], ["build_rvindex.IndexDB.inverted_index.save_to_file", "build_rvindex.IndexDB.document_length_table.save_to_file", "filename.exists", "filename.mkdir"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file"], ["", "def", "save_to_file", "(", "self", ",", "filename", ":", "Path", ",", "memory_saving", "=", "False", ")", ":", "\n", "        ", "if", "not", "filename", ".", "exists", "(", ")", ":", "\n", "            ", "filename", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "False", ")", "\n", "", "self", ".", "inverted_index", ".", "save_to_file", "(", "filename", "/", "\"inverted_index.txt\"", ",", "memory_efficient", "=", "memory_saving", ")", "\n", "self", ".", "document_length_table", ".", "save_to_file", "(", "filename", "/", "\"doc_length_table.txt\"", ")", "\n", "# score_file_name = filename / \"scored_db\"", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.load_from_file": [[87, 93], ["build_rvindex.IndexDB.inverted_index.load_from_file", "build_rvindex.IndexDB.document_length_table.load_from_file", "filename.is_dir", "FileNotFoundError"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file"], ["", "def", "load_from_file", "(", "self", ",", "filename", ":", "Path", ",", "with_int_type", "=", "False", ",", "memory_saving", "=", "False", ")", ":", "\n", "        ", "if", "not", "filename", ".", "is_dir", "(", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "f\"{filename} is not a valid indexDB directory.\"", ")", "\n", "", "self", ".", "inverted_index", ".", "load_from_file", "(", "filename", "/", "\"inverted_index.txt\"", ",", "with_int_type", "=", "with_int_type", ",", "\n", "memory_efficient", "=", "memory_saving", ")", "\n", "self", ".", "document_length_table", ".", "load_from_file", "(", "filename", "/", "\"doc_length_table.txt\"", ",", "with_int_type", "=", "with_int_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.pre_compute_scores": [[100, 116], ["dict", "print", "tqdm.tqdm.tqdm", "build_rvindex.IndexDB.inverted_index.index.keys", "build_rvindex.IndexDB.inverted_index.index[].keys", "build_rvindex.IndexDB.inverted_index.get_tf", "build_rvindex.IndexDB.inverted_index.get_Nt", "len", "scoring_fn", "dict"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_tf", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_Nt"], ["", "def", "pre_compute_scores", "(", "self", ",", "scoring_fn", "=", "default_scoring_fn", ",", "score_name", "=", "'default-tf-idf'", ")", ":", "\n", "        ", "self", ".", "score_db", "[", "score_name", "]", "=", "dict", "(", ")", "\n", "\n", "the_cur_score_db", "=", "self", ".", "score_db", "[", "score_name", "]", "\n", "\n", "print", "(", "f\"Pre computing scores for {score_name}.\"", ")", "\n", "for", "cur_term", "in", "tqdm", "(", "self", ".", "inverted_index", ".", "index", ".", "keys", "(", ")", ")", ":", "\n", "            ", "for", "cur_doc", "in", "self", ".", "inverted_index", ".", "index", "[", "cur_term", "]", ".", "keys", "(", ")", ":", "\n", "                ", "tf", "=", "self", ".", "inverted_index", ".", "get_tf", "(", "cur_term", ",", "cur_doc", ")", "\n", "Nt", "=", "self", ".", "inverted_index", ".", "get_Nt", "(", "cur_term", ")", "\n", "N", "=", "len", "(", "self", ".", "document_length_table", ")", "\n", "score", "=", "scoring_fn", "(", "tf", "=", "tf", ",", "Nt", "=", "Nt", ",", "N", "=", "N", ")", "\n", "if", "cur_term", "not", "in", "the_cur_score_db", ":", "\n", "                    ", "the_cur_score_db", "[", "cur_term", "]", "=", "dict", "(", ")", "\n", "\n", "", "the_cur_score_db", "[", "cur_term", "]", "[", "cur_doc", "]", "=", "score", "\n", "# break", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.pre_compute_scores_iteratively": [[118, 128], ["print", "open", "tqdm.tqdm.tqdm", "build_rvindex.IndexDB.inverted_index.index.keys", "build_rvindex.IndexDB.inverted_index.index[].keys", "build_rvindex.IndexDB.inverted_index.get_tf", "build_rvindex.IndexDB.inverted_index.get_Nt", "len", "scoring_fn", "out_f.write", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_tf", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_Nt"], ["", "", "", "def", "pre_compute_scores_iteratively", "(", "self", ",", "save_path", ",", "scoring_fn", "=", "default_scoring_fn", ",", "score_name", "=", "'default-tf-idf'", ")", ":", "\n", "        ", "print", "(", "f\"Pre computing scores for {score_name}.\"", ")", "\n", "with", "open", "(", "save_path", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "            ", "for", "cur_term", "in", "tqdm", "(", "self", ".", "inverted_index", ".", "index", ".", "keys", "(", ")", ")", ":", "\n", "                ", "for", "cur_doc", "in", "self", ".", "inverted_index", ".", "index", "[", "cur_term", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "tf", "=", "self", ".", "inverted_index", ".", "get_tf", "(", "cur_term", ",", "cur_doc", ")", "\n", "Nt", "=", "self", ".", "inverted_index", ".", "get_Nt", "(", "cur_term", ")", "\n", "N", "=", "len", "(", "self", ".", "document_length_table", ")", "\n", "score", "=", "scoring_fn", "(", "tf", "=", "tf", ",", "Nt", "=", "Nt", ",", "N", "=", "N", ")", "\n", "out_f", ".", "write", "(", "'/'", ".", "join", "(", "[", "str", "(", "cur_term", ")", ",", "str", "(", "cur_doc", ")", ",", "str", "(", "score", ")", "]", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.IndexDB.get_relevant_document": [[129, 159], ["heapq.heappushpop", "heapq.heappush", "build_rvindex.IndexDB.inverted_index.get_tf", "build_rvindex.IndexDB.inverted_index.get_Nt", "len", "scoring_fn", "len"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_tf", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_Nt"], ["", "", "", "", "def", "get_relevant_document", "(", "self", ",", "candidate_doc", ",", "valid_terms", ",", "score_name", "=", "'default-tf-idf'", ",", "\n", "scoring_fn", "=", "default_scoring_fn", ",", "top_k", "=", "None", ")", ":", "\n", "        ", "scored_doc", "=", "[", "]", "\n", "\n", "for", "cur_doc", "in", "candidate_doc", ":", "\n", "            ", "cur_doc_score", "=", "0", "\n", "for", "cur_term", "in", "valid_terms", ":", "\n", "\n", "                ", "if", "score_name", "in", "self", ".", "score_db", ":", "\n", "# print(\"Found precomputed_value.\")", "\n", "                    ", "if", "cur_term", "not", "in", "self", ".", "score_db", "[", "score_name", "]", ":", "\n", "                        ", "score", "=", "0", "\n", "", "elif", "cur_doc", "not", "in", "self", ".", "score_db", "[", "score_name", "]", "[", "cur_term", "]", ":", "\n", "                        ", "score", "=", "0", "\n", "", "else", ":", "\n", "                        ", "score", "=", "self", ".", "score_db", "[", "score_name", "]", "[", "cur_term", "]", "[", "cur_doc", "]", "\n", "", "", "else", ":", "\n", "                    ", "tf", "=", "self", ".", "inverted_index", ".", "get_tf", "(", "cur_term", ",", "cur_doc", ")", "\n", "Nt", "=", "self", ".", "inverted_index", ".", "get_Nt", "(", "cur_term", ")", "\n", "N", "=", "len", "(", "self", ".", "document_length_table", ")", "\n", "score", "=", "scoring_fn", "(", "tf", "=", "tf", ",", "Nt", "=", "Nt", ",", "N", "=", "N", ")", "\n", "\n", "", "cur_doc_score", "+=", "score", "\n", "\n", "", "if", "top_k", "is", "not", "None", "and", "0", "<=", "top_k", "==", "len", "(", "scored_doc", ")", ":", "\n", "                ", "heapq", ".", "heappushpop", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "", "else", ":", "\n", "                ", "heapq", ".", "heappush", "(", "scored_doc", ",", "(", "cur_doc_score", ",", "cur_doc", ")", ")", "\n", "\n", "", "", "return", "scored_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.__init__": [[166, 169], ["dict", "dict"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "index", "=", "dict", "(", ")", "\n", "self", ".", "Nt_table", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.__contains__": [[170, 172], ["None"], "methods", ["None"], ["", "def", "__contains__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "item", "in", "self", ".", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.__getitem__": [[173, 175], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "index", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.build_Nt_table": [[176, 180], ["print", "tqdm.tqdm.tqdm", "build_rvindex.InvertedIndex.index.keys", "len"], "methods", ["None"], ["", "def", "build_Nt_table", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"Build Nt table.\"", ")", "\n", "for", "term", "in", "tqdm", "(", "self", ".", "index", ".", "keys", "(", ")", ")", ":", "\n", "            ", "self", ".", "Nt_table", "[", "term", "]", "=", "len", "(", "self", ".", "index", "[", "term", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.add": [[181, 191], ["dict"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "term", ",", "docid", ")", ":", "\n", "        ", "if", "term", "in", "self", ".", "index", ":", "\n", "            ", "if", "docid", "in", "self", ".", "index", "[", "term", "]", ":", "\n", "                ", "self", ".", "index", "[", "term", "]", "[", "docid", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "self", ".", "index", "[", "term", "]", "[", "docid", "]", "=", "1", "\n", "", "", "else", ":", "\n", "            ", "d", "=", "dict", "(", ")", "\n", "d", "[", "docid", "]", "=", "1", "\n", "self", ".", "index", "[", "term", "]", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.save_to_file": [[192, 204], ["print", "open", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "out_f.write", "term_row.items", "out_f.write", "json.dumps", "str", "str", "str"], "methods", ["None"], ["", "", "def", "save_to_file", "(", "self", ",", "file", ",", "memory_efficient", "=", "False", ")", ":", "\n", "        ", "print", "(", "f\"Save indexing file to {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "            ", "if", "not", "memory_efficient", ":", "\n", "                ", "for", "term", "in", "tqdm", "(", "self", ".", "index", ")", ":", "\n", "                    ", "item", "=", "{", "'t'", ":", "term", ",", "'i'", ":", "self", ".", "index", "[", "term", "]", "}", "\n", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ")", "+", "\"\\n\"", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "term", "in", "tqdm", "(", "self", ".", "index", ")", ":", "\n", "                    ", "term_row", "=", "self", ".", "index", "[", "term", "]", "\n", "for", "key", ",", "value", "in", "term_row", ".", "items", "(", ")", ":", "\n", "                        ", "out_f", ".", "write", "(", "'/'", ".", "join", "(", "[", "str", "(", "term", ")", ",", "str", "(", "key", ")", ",", "str", "(", "value", ")", "]", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.load_from_file": [[205, 232], ["print", "open", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "line.strip.strip.strip", "line.strip.strip.split", "int", "int", "int", "json.loads", "json.loads", "dict", "item[].items", "int", "int", "int"], "methods", ["None"], ["", "", "", "", "", "def", "load_from_file", "(", "self", ",", "file", ",", "with_int_type", "=", "False", ",", "memory_efficient", "=", "False", ")", ":", "\n", "        ", "print", "(", "f\"Load indexing file from {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "in_f", ":", "\n", "            ", "if", "not", "memory_efficient", ":", "\n", "                ", "if", "not", "with_int_type", ":", "\n", "                    ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                        ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "self", ".", "index", "[", "item", "[", "'t'", "]", "]", "=", "item", "[", "'i'", "]", "\n", "del", "item", "\n", "", "", "else", ":", "\n", "                    ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                        ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "self", ".", "index", "[", "int", "(", "item", "[", "'t'", "]", ")", "]", "=", "dict", "(", ")", "\n", "for", "key", ",", "value", "in", "item", "[", "'i'", "]", ".", "items", "(", ")", ":", "\n", "                            ", "self", ".", "index", "[", "int", "(", "item", "[", "'t'", "]", ")", "]", "[", "int", "(", "key", ")", "]", "=", "value", "\n", "", "del", "item", "\n", "", "", "", "else", ":", "\n", "                ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "sp_line", "=", "line", ".", "split", "(", "'/'", ")", "\n", "term", "=", "int", "(", "sp_line", "[", "0", "]", ")", "\n", "doc", "=", "int", "(", "sp_line", "[", "1", "]", ")", "\n", "value", "=", "int", "(", "sp_line", "[", "2", "]", ")", "\n", "if", "term", "not", "in", "self", ".", "index", ":", "\n", "                        ", "self", ".", "index", "[", "term", "]", "=", "{", "doc", ":", "value", "}", "\n", "", "else", ":", "\n", "                        ", "self", ".", "index", "[", "term", "]", "[", "doc", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_containing_document": [[234, 239], ["set", "build_rvindex.InvertedIndex.index[].keys"], "methods", ["None"], ["", "", "", "", "", "def", "get_containing_document", "(", "self", ",", "term", ")", ":", "\n", "        ", "if", "term", "not", "in", "self", ".", "index", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "set", "(", "self", ".", "index", "[", "term", "]", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_tf": [[240, 247], ["None"], "methods", ["None"], ["", "", "def", "get_tf", "(", "self", ",", "term", ",", "docid", ")", ":", "\n", "        ", "if", "term", "not", "in", "self", ".", "index", ":", "\n", "            ", "return", "0", "\n", "", "elif", "docid", "not", "in", "self", ".", "index", "[", "term", "]", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "index", "[", "term", "]", "[", "docid", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.InvertedIndex.get_Nt": [[248, 255], ["Exception", "len"], "methods", ["None"], ["", "", "def", "get_Nt", "(", "self", ",", "term", ")", ":", "\n", "        ", "if", "self", ".", "Nt_table", "is", "None", "or", "len", "(", "self", ".", "Nt_table", ")", "==", "0", ":", "\n", "            ", "raise", "Exception", "(", "\"Nt table not built.\"", ")", "\n", "", "if", "term", "not", "in", "self", ".", "Nt_table", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "Nt_table", "[", "term", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.__init__": [[275, 278], ["dict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "table", "=", "dict", "(", ")", "\n", "self", ".", "total_doc_num", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.__len__": [[279, 285], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "total_doc_num", "!=", "-", "1", ":", "\n", "            ", "return", "self", ".", "total_doc_num", "\n", "", "else", ":", "\n", "            ", "self", ".", "total_doc_num", "=", "len", "(", "self", ".", "table", ")", "\n", "return", "self", ".", "total_doc_num", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add": [[286, 288], ["None"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "docid", ",", "length", ")", ":", "\n", "        ", "self", ".", "table", "[", "docid", "]", "=", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.get_length": [[289, 294], ["LookupError", "str"], "methods", ["None"], ["", "def", "get_length", "(", "self", ",", "docid", ")", ":", "\n", "        ", "if", "docid", "in", "self", ".", "table", ":", "\n", "            ", "return", "self", ".", "table", "[", "docid", "]", "\n", "", "else", ":", "\n", "            ", "raise", "LookupError", "(", "'%s not found in table'", "%", "str", "(", "docid", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.get_average_length": [[295, 300], ["build_rvindex.DocumentLengthTable.table.values", "float", "float", "len"], "methods", ["None"], ["", "", "def", "get_average_length", "(", "self", ")", ":", "\n", "        ", "sum", "=", "0", "\n", "for", "length", "in", "self", ".", "table", ".", "values", "(", ")", ":", "\n", "            ", "sum", "+=", "length", "\n", "", "return", "float", "(", "sum", ")", "/", "float", "(", "len", "(", "self", ".", "table", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.save_to_file": [[301, 306], ["print", "open", "tqdm.tqdm.tqdm", "out_f.write", "str", "str"], "methods", ["None"], ["", "def", "save_to_file", "(", "self", ",", "file", ")", ":", "\n", "        ", "print", "(", "f\"Save Document lengths file to {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "            ", "for", "title", "in", "tqdm", "(", "self", ".", "table", ")", ":", "\n", "                ", "out_f", ".", "write", "(", "str", "(", "title", ")", "+", "'<#.#>'", "+", "str", "(", "self", ".", "table", "[", "title", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.load_from_file": [[307, 322], ["print", "len", "open", "line.strip.strip.strip", "line.strip.strip.split", "int", "line.strip.strip.strip", "line.strip.strip.split", "int", "int"], "methods", ["None"], ["", "", "", "def", "load_from_file", "(", "self", ",", "file", ",", "with_int_type", "=", "False", ")", ":", "\n", "        ", "print", "(", "f\"Load Document lengths file from {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "in_f", ":", "\n", "            ", "if", "not", "with_int_type", ":", "\n", "                ", "for", "line", "in", "in_f", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "title", ",", "lens_str", "=", "line", ".", "split", "(", "'<#.#>'", ")", "\n", "self", ".", "table", "[", "title", "]", "=", "int", "(", "lens_str", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "line", "in", "in_f", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "title", ",", "lens_str", "=", "line", ".", "split", "(", "'<#.#>'", ")", "\n", "self", ".", "table", "[", "int", "(", "title", ")", "]", "=", "int", "(", "lens_str", ")", "\n", "\n", "", "", "", "self", ".", "total_doc_num", "=", "len", "(", "self", ".", "table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.default_scoring_fn": [[16, 22], ["math.log", "math.log"], "function", ["None"], ["def", "default_scoring_fn", "(", "**", "kwargs", ")", ":", "\n", "    ", "tf", "=", "kwargs", "[", "'tf'", "]", "\n", "Nt", "=", "kwargs", "[", "'Nt'", "]", "\n", "N", "=", "kwargs", "[", "'N'", "]", "\n", "score", "=", "math", ".", "log", "(", "tf", "+", "1", ")", "*", "math", ".", "log", "(", "(", "N", "-", "Nt", "+", "0.5", ")", "/", "(", "Nt", "+", "0.5", ")", ")", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.save_to_file": [[24, 36], ["print", "open", "tqdm.tqdm", "tqdm.tqdm", "dict_db.keys", "out_f.write", "term_row.items", "out_f.write", "json.dumps", "str", "str", "str"], "function", ["None"], ["", "def", "save_to_file", "(", "dict_db", ",", "file", ",", "memory_efficient", "=", "False", ")", ":", "\n", "    ", "print", "(", "f\"Save scored file to {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "if", "not", "memory_efficient", ":", "\n", "            ", "for", "term", "in", "tqdm", "(", "dict_db", ".", "keys", "(", ")", ")", ":", "\n", "                ", "item", "=", "{", "'t'", ":", "term", ",", "'v'", ":", "dict_db", "[", "term", "]", "}", "\n", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "item", ")", "+", "\"\\n\"", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "term", "in", "tqdm", "(", "dict_db", ")", ":", "\n", "                ", "term_row", "=", "dict_db", "[", "term", "]", "\n", "for", "key", ",", "value", "in", "term_row", ".", "items", "(", ")", ":", "\n", "                    ", "out_f", ".", "write", "(", "'/'", ".", "join", "(", "[", "str", "(", "term", ")", ",", "str", "(", "key", ")", ",", "str", "(", "value", ")", "]", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.load_from_file": [[38, 67], ["print", "open", "tqdm.tqdm", "tqdm.tqdm", "tqdm.tqdm", "line.strip.strip", "line.strip.split", "int", "int", "json.loads", "json.loads", "dict", "item[].items", "int", "float", "int", "int", "int"], "function", ["None"], ["", "", "", "", "", "def", "load_from_file", "(", "dict_db", ",", "file", ",", "with_int_type", "=", "False", ",", "memory_efficient", "=", "False", ",", "value_type", "=", "'int'", ")", ":", "\n", "    ", "print", "(", "f\"Load scored file from {file}\"", ")", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "in_f", ":", "\n", "        ", "if", "not", "memory_efficient", ":", "\n", "            ", "if", "not", "with_int_type", ":", "\n", "                ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                    ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "dict_db", "[", "item", "[", "'t'", "]", "]", "=", "item", "[", "'v'", "]", "\n", "", "", "else", ":", "\n", "                ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                    ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "dict_db", "[", "int", "(", "item", "[", "'t'", "]", ")", "]", "=", "dict", "(", ")", "\n", "for", "key", ",", "value", "in", "item", "[", "'v'", "]", ".", "items", "(", ")", ":", "\n", "                        ", "dict_db", "[", "int", "(", "item", "[", "'t'", "]", ")", "]", "[", "int", "(", "key", ")", "]", "=", "value", "\n", "", "", "", "", "else", ":", "\n", "            ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "sp_line", "=", "line", ".", "split", "(", "'/'", ")", "\n", "term", "=", "int", "(", "sp_line", "[", "0", "]", ")", "\n", "doc", "=", "int", "(", "sp_line", "[", "1", "]", ")", "\n", "if", "value_type", "==", "'int'", ":", "\n", "                    ", "value", "=", "int", "(", "sp_line", "[", "2", "]", ")", "\n", "", "elif", "value_type", "==", "'float'", ":", "\n", "                    ", "value", "=", "float", "(", "sp_line", "[", "2", "]", ")", "\n", "", "if", "term", "not", "in", "dict_db", ":", "\n", "                    ", "dict_db", "[", "term", "]", "=", "{", "doc", ":", "value", "}", "\n", "", "else", ":", "\n", "                    ", "dict_db", "[", "term", "]", "[", "doc", "]", "=", "value", "\n", "", "", "", "", "return", "dict_db", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.term_matching.get_kwterm_matching": [[9, 29], ["list", "len", "range", "print", "flashtext.KeywordProcessor", "tqdm.tqdm", "tqdm.tqdm", "flashtext.KeywordProcessor.add_keyword", "build_rindex.rindex_analysis.query_get_terms", "item[].extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.rindex_analysis.query_get_terms"], ["def", "get_kwterm_matching", "(", "kw_terms", ",", "d_list", ",", "chuck_size", "=", "10_000_000", ")", ":", "\n", "    ", "kw_terms", "=", "list", "(", "kw_terms", ")", "\n", "kw_terms_total_size", "=", "len", "(", "kw_terms", ")", "\n", "for", "start", "in", "range", "(", "0", ",", "kw_terms_total_size", ",", "chuck_size", ")", ":", "\n", "        ", "print", "(", "start", ",", "start", "+", "chuck_size", ")", "\n", "current_kw_terms", "=", "kw_terms", "[", "start", ":", "start", "+", "chuck_size", "]", "\n", "keyword_processor", "=", "KeywordProcessor", "(", "case_sensitive", "=", "True", ")", "\n", "for", "word", "in", "tqdm", "(", "current_kw_terms", ")", ":", "\n", "            ", "keyword_processor", ".", "add_keyword", "(", "word", ")", "\n", "\n", "", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "            ", "query", "=", "item", "[", "'question'", "]", "\n", "terms", "=", "query_get_terms", "(", "query", ",", "keyword_processor", ")", "\n", "if", "'kw_matches'", "not", "in", "item", ":", "\n", "                ", "item", "[", "'kw_matches'", "]", "=", "[", "]", "\n", "", "item", "[", "'kw_matches'", "]", ".", "extend", "(", "terms", ")", "\n", "\n", "", "del", "keyword_processor", "\n", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v1.eval_model": [[35, 114], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "model", "model", "list", "list", "list", "[].view().tolist", "model.tolist", "y_probs_list.extend", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ",", "\n", "feed_input_span", "=", "False", ")", ":", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "tqdm_disable", "=", "not", "show_progress", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "tqdm_disable", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "eval_s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "eval_s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "if", "not", "feed_input_span", ":", "\n", "                ", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "s1_span", "=", "eval_s1_span", ",", "s2_span", "=", "eval_s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'oid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'oid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "# r_item['probs'] =", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v1.get_inference_pair": [[116, 163], ["utils.list_dict_data_tool.list_to_dict", "isinstance", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_models.sentence_selection.bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "fever_utils.fever_db.get_cursor", "fever_sampler.nli_new_sampler.build_nli_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "isinstance", "utils.common.load_jsonl", "ValueError", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.build_nli_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_inference_pair", "(", "tag", ",", "is_training", ",", "sent_result_path", ",", "debug_num", "=", "None", ",", "evidence_filtering_threshold", "=", "0.01", ")", ":", "\n", "# sent_result_path = \"\"", "\n", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug_num", "is", "not", "None", ":", "\n", "# d_list = d_list[:10]", "\n", "        ", "d_list", "=", "d_list", "[", ":", "50", "]", "\n", "# d_list = d_list[:200]", "\n", "\n", "", "d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'id'", ")", "\n", "\n", "threshold_value", "=", "evidence_filtering_threshold", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\")", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\")", "\n", "\n", "# debug_num = None if not debug else 2971", "\n", "# debug_num = None", "\n", "\n", "if", "isinstance", "(", "sent_result_path", ",", "Path", ")", ":", "\n", "        ", "sent_list", "=", "common", ".", "load_jsonl", "(", "sent_result_path", ",", "debug_num", ")", "\n", "", "elif", "isinstance", "(", "sent_result_path", ",", "list", ")", ":", "\n", "        ", "sent_list", "=", "sent_result_path", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"{sent_result_path} is not of a valid argument type which should be [list, Path].\"", ")", "\n", "\n", "", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "sent_list", ",", "d_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "filltered_sent_dict", "=", "select_top_k_and_to_results_dict", "(", "d_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "threshold_value", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "d_list", ",", "filltered_sent_dict", ",", "\n", "'id'", ",", "[", "'predicted_evidence'", ",", "'predicted_scored_evidence'", "]", ")", "\n", "fever_db_cursor", "=", "fever_db", ".", "get_cursor", "(", "config", ".", "FEVER_DB", ")", "\n", "forward_items", "=", "build_nli_forward_item", "(", "d_list", ",", "is_training", "=", "is_training", ",", "db_cursor", "=", "fever_db_cursor", ")", "\n", "\n", "return", "forward_items", ",", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v1.model_go": [[165, 368], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "bert_fever_nli_v1.get_inference_pair", "bert_fever_nli_v1.get_inference_pair", "len", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "bert_fever_nli_v1.get_inference_pair", "random.shuffle", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "loss.mean.backward", "os.path.join", "it.read", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "print", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v1.eval_model", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "any", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'fever_v1_nli_maxout'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "16", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "eval_frequency", "=", "2000", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "# debug_mode = True", "\n", "debug_mode", "=", "False", "\n", "maxout_model", "=", "True", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "3", "\n", "# num_train_optimization_steps", "\n", "\n", "train_sent_filtering_prob", "=", "0.35", "\n", "dev_sent_filtering_prob", "=", "0.01", "\n", "\n", "dev_sent_results_file", "=", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\"", "\n", "train_sent_results_file", "=", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\"", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "# train_fitems_list = get_inference_pair('train', True, train_sent_results_file, debug_mode, train_sent_filtering_prob)", "\n", "dev_debug_num", "=", "2481", "if", "debug_mode", "else", "None", "\n", "dev_fitems_list", ",", "dev_list", "=", "get_inference_pair", "(", "'dev'", ",", "False", ",", "dev_sent_results_file", ",", "dev_debug_num", ",", "\n", "dev_sent_filtering_prob", ")", "\n", "# = common.load_jsonl(config.FEVER_DEV)", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "50", "]", "\n", "eval_frequency", "=", "1", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "train_debug_num", "=", "2971", "if", "debug_mode", "else", "None", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "est_datasize", "=", "len", "(", "train_fitems_list", ")", "\n", "\n", "# dev_o_dict = list_dict_data_tool.list_to_dict(dev_list, 'id')", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "364", ",", "pair_order", "=", "pair_order", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "if", "not", "maxout_model", ":", "\n", "        ", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertPairMaxOutMatcher", "(", "bert_encoder", ",", "num_of_class", "=", "num_class", ",", "act_type", "=", "\"gelu\"", ",", "num_of_out_layers", "=", "2", ")", "\n", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "random", ".", "shuffle", "(", "train_fitems_list", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "if", "not", "maxout_model", ":", "\n", "                ", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "s1_span", "=", "s1_span", ",", "s2_span", "=", "s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ")", "\n", "\n", "results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_fitems_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|ss({strict_score})|ac({acc_score})|pr({pr})|rec({rec})|f1({f1})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_nli_results.json\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v1.model_go_with_old_data": [[372, 567], ["torch.manual_seed", "torch.manual_seed", "int", "format_convert", "format_convert", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "bert_fever_nli_v1.get_inference_pair", "bert_fever_nli_v1.get_inference_pair", "len", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "bert_fever_nli_v1.get_inference_pair", "random.shuffle", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "print", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v1.eval_model", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "any", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.format_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.format_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "", "", "", "", "def", "model_go_with_old_data", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'fever_v1_nli'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "16", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "eval_frequency", "=", "2000", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "# debug_mode = True", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "3", "\n", "# num_train_optimization_steps", "\n", "\n", "train_sent_filtering_prob", "=", "0.35", "\n", "dev_sent_filtering_prob", "=", "0.1", "\n", "\n", "# dev_sent_results_file = config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\"", "\n", "# train_sent_results_file = config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\"", "\n", "from", "utest", ".", "utest_format_converter_for_old_sent", ".", "tool", "import", "format_convert", "\n", "dev_sent_results_file", "=", "format_convert", "(", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/sent_results/old_sent_data_by_NSMN/4-15-dev_sent_pred_scores_old_format.jsonl\"", ")", "\n", "train_sent_results_file", "=", "format_convert", "(", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/sent_results/old_sent_data_by_NSMN/train_sent_scores_old_format.jsonl\"", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "# train_fitems_list = get_inference_pair('train', True, train_sent_results_file, debug_mode, train_sent_filtering_prob)", "\n", "dev_debug_num", "=", "2481", "if", "debug_mode", "else", "None", "\n", "dev_fitems_list", ",", "dev_list", "=", "get_inference_pair", "(", "'dev'", ",", "False", ",", "dev_sent_results_file", ",", "dev_debug_num", ",", "\n", "dev_sent_filtering_prob", ")", "\n", "# = common.load_jsonl(config.FEVER_DEV)", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "50", "]", "\n", "eval_frequency", "=", "1", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "train_debug_num", "=", "2971", "if", "debug_mode", "else", "None", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "est_datasize", "=", "len", "(", "train_fitems_list", ")", "\n", "\n", "# dev_o_dict = list_dict_data_tool.list_to_dict(dev_list, 'id')", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "364", ",", "pair_order", "=", "pair_order", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "False", ")", "\n", "#", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "random", ".", "shuffle", "(", "train_fitems_list", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ")", "\n", "\n", "results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_fitems_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|ss({strict_score})|ac({acc_score})|pr({pr})|rec({rec})|f1({f1})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_nli_results.json\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_full.hidden_eval": [[28, 110], ["print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "enumerate", "range", "print", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_att_mask.to.to", "model", "y_id_list.extend", "y_pred_list.extend", "model.size", "len", "len", "len", "len", "len", "list", "[].view().tolist", "y_logits_list.extend", "y_probs_list.extend", "str", "str", "len", "model.tolist", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "hidden_eval", "(", "model", ",", "data_iter", ",", "dev_data_list", ",", "device", ",", "with_logits", "=", "False", ",", "with_probs", "=", "False", ")", ":", "\n", "# SUPPORTS < (-.-) > 0", "\n", "# REFUTES < (-.-) > 1", "\n", "# NOT ENOUGH INFO < (-.-) > 2", "\n", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_id_list", "=", "[", "]", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "# if append_text:", "\n", "# y_premise = []", "\n", "# y_hypothesis = []", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "# eval_labels_ids = batch['label']", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "# eval_labels_ids = eval_labels_ids.to(device)", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "y_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "\n", "# if append_text:", "\n", "# y_premise.extend(list(batch['text']))", "\n", "# y_hypothesis.extend(list(batch['query']))", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_logits", ":", "\n", "                ", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "y_id_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "dev_data_list", ")", ")", ":", "\n", "            ", "assert", "str", "(", "y_id_list", "[", "i", "]", ")", "==", "str", "(", "dev_data_list", "[", "i", "]", "[", "'id'", "]", ")", "\n", "\n", "# Matching id", "\n", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "if", "with_logits", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'probs'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "# Reset neural set", "\n", "", "if", "len", "(", "dev_data_list", "[", "i", "]", "[", "'predicted_sentids'", "]", ")", "==", "0", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "\"NOT ENOUGH INFO\"", "\n", "\n", "# if append_text:", "\n", "#     dev_data_list[i]['premise'] = y_premise[i]", "\n", "#     dev_data_list[i]['hypothesis'] = y_hypothesis[i]", "\n", "\n", "", "", "print", "(", "'total_size:'", ",", "totoal_size", ")", "\n", "\n", "", "return", "dev_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_full.eval_model": [[112, 150], ["model.eval", "print", "zip", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_labels_ids.to.to", "eval_att_mask.to.to", "model", "model.size", "output_pred_list.extend", "output_logits_list.extend", "output_id_list.extend", "y_pred_list.extend", "datetime.datetime.now", "[].view().tolist", "model.tolist", "list", "eval_labels_ids.to.tolist", "[].view", "model.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "eval_iter", ",", "device", ")", ":", "\n", "    ", "output_logits_list", "=", "[", "]", "\n", "output_id_list", "=", "[", "]", "\n", "output_pred_list", "=", "[", "]", "\n", "y_pred_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "print", "(", "f\"Start Eval ({datetime.datetime.now()}):\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "eval_iter", ")", ":", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "eval_labels_ids", "=", "eval_labels_ids", ".", "to", "(", "device", ")", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "eval_logits", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "labels", "=", "None", ")", "\n", "total_size", "+=", "eval_logits", ".", "size", "(", "0", ")", "\n", "\n", "output_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "eval_logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "eval_logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "output_logits_list", ".", "extend", "(", "eval_logits", ".", "tolist", "(", ")", ")", "\n", "output_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "y_pred_list", ".", "extend", "(", "eval_labels_ids", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "output_pred_list", ")", "\n", "correct", "=", "0", "\n", "for", "pred", ",", "y", "in", "zip", "(", "output_pred_list", ",", "y_pred_list", ")", ":", "\n", "        ", "if", "pred", "==", "y", ":", "\n", "            ", "correct", "+=", "1", "\n", "\n", "", "", "print", "(", "correct", ",", "total_size", ",", "correct", "/", "total_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_full.model_go": [[152, 396], ["int", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertForSequenceClassification.from_pretrained", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "nn.DataParallel.train", "range", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "print", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.adv_simi_sample_with_prob_v1_1", "random.shuffle", "print", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "len", "tqdm.tqdm", "flint.torch_util.get_length_and_mask", "paired_sequence.to.to", "paired_segments_ids.to.to", "labels_ids.to.to", "att_mask.to.to", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_v0_full.hidden_eval", "bert_v0_full.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_v0_full.hidden_eval", "bert_v0_full.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "do_ema", "=", "False", "\n", "dev_prob_threshold", "=", "0.1", "\n", "train_prob_threshold", "=", "0.35", "\n", "debug_mode", "=", "False", "\n", "\n", "experiment_name", "=", "f\"bert_fever_nli_baseline_on_fulldata\"", "\n", "\n", "training_file", "=", "config", ".", "FEVER_TRAIN", "\n", "\n", "train_sample_top_k", "=", "8", "\n", "\n", "est_datasize", "=", "208_346", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\"", ")", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DEV", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DEV", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# print(dev_data_list[0])", "\n", "# exit(0)", "\n", "\n", "train_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/train_sent_scores.jsonl\"", ")", "\n", "# Finished loading standardized sentence file.", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertReaderFeverNLI", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "# Load training model", "\n", "model_clf", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "bert_model_name", ",", "num_labels", "=", "num_class", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "        ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "        ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "model_clf", ".", "train", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "2_000", "# Change this to real evaluation.", "\n", "best_fever_score", "=", "-", "1", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Resampling...\"", ")", "\n", "train_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "training_file", ",", "\n", "train_upstream_sent_list", ",", "\n", "train_prob_threshold", ",", "\n", "top_n", "=", "train_sample_top_k", ")", "\n", "#", "\n", "train_data_list", "=", "fever_nli_sampler", ".", "adv_simi_sample_with_prob_v1_1", "(", "\n", "training_file", ",", "\n", "train_sent_after_threshold_filter", ",", "\n", "None", ",", "\n", "tokenized", "=", "True", ")", "\n", "\n", "train_data_list", "=", "train_data_list", "\n", "\n", "random", ".", "shuffle", "(", "train_data_list", ")", "\n", "print", "(", "\"Sample data length:\"", ",", "len", "(", "train_data_list", ")", ")", "\n", "sampled_train_instances", "=", "bert_fever_reader", ".", "read", "(", "train_data_list", ")", "\n", "#", "\n", "train_iter", "=", "biterator", "(", "sampled_train_instances", ",", "shuffle", "=", "True", ",", "num_epochs", "=", "1", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "            ", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "labels_ids", "=", "labels_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model_clf", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                    ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                        ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                            ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                        ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                        ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "            ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "            ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_full.model_eval": [[398, 520], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "zip", "evaluation.fever_scorer.fever_score", "print", "print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertForSequenceClassification.from_pretrained", "nn.DataParallel.load_state_dict", "nn.DataParallel.to", "nn.DataParallel.eval", "allennlp.data.iterators.BasicIterator.", "bert_v0_full.hidden_eval", "utils.common.save_jsonl", "evaluation.fever_scorer.fever_score", "print", "print", "torch.load", "torch.load", "torch.load", "torch.load", "torch.DataParallel", "utils.common.load_jsonl", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "", "", "def", "model_eval", "(", "model_save_path", ")", ":", "\n", "    ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "# dev_prob_threshold = 0.05", "\n", "dev_prob_threshold", "=", "0.1", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "# dev_upstream_sent_list = common.load_jsonl(config.RESULT_PATH /", "\n", "#                                            \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\")", "\n", "\n", "# dev_upstream_sent_list = common.load_jsonl(config.DATA_ROOT /", "\n", "# \"utest_data/dev_sent_score_2_shared_task_dev.jsonl\")", "\n", "# \"utest_data/dev_sent_score_1_shared_task_dev_docnum(10)_ensembled.jsonl\")", "\n", "\n", "# dev_upstream_sent_list = common.load_jsonl(config.FEVER_DATA_ROOT /", "\n", "#                                            \"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\")", "\n", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/4-15-dev_sent_pred_scores.jsonl\"", ")", "\n", "#", "\n", "# dev_upstream_sent_list = common.load_jsonl(config.FEVER_DATA_ROOT /", "\n", "#                                            \"upstream_sentence_selection_Feb16/4-15-test_sent_pred_scores.jsonl\")", "\n", "\n", "# dev_upstream_sent_list = common.load_jsonl(config.FEVER_DATA_ROOT /", "\n", "#                                            \"upstream_sentence_selection_Feb16/n_dev_sent_pred_scores.jsonl\")", "\n", "\n", "\n", "# dev_sent_after_threshold_filter = fever_ss_sampler.threshold_sampler_insure_unique_new_format(", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DEV", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DEV", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# dev_sent_after_threshold_filter = fever_ss_sampler.threshold_sampler_insure_unique(", "\n", "#     config.FEVER_TEST,", "\n", "#     dev_upstream_sent_list,", "\n", "#     prob_threshold=dev_prob_threshold, top_n=5)", "\n", "#", "\n", "# dev_data_list = fever_nli_sampler.select_sent_with_prob_for_eval(", "\n", "#     config.FEVER_TEST, dev_sent_after_threshold_filter,", "\n", "#     None, tokenized=True, pipeline=True)", "\n", "\n", "for", "item", "in", "dev_data_list", ":", "\n", "        ", "item", "[", "'label'", "]", "=", "'hidden'", "\n", "\n", "", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "\n", "for", "a", ",", "b", "in", "zip", "(", "dev_list", ",", "dev_data_list", ")", ":", "\n", "        ", "del", "b", "[", "'label'", "]", "\n", "b", "[", "'predicted_label'", "]", "=", "a", "[", "'label'", "]", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "dev_list", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertReaderFeverNLI", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "# Load training model", "\n", "model_clf", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "bert_model_name", ",", "num_labels", "=", "num_class", ")", "\n", "\n", "model_clf", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_save_path", ")", ")", "\n", "\n", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "model_clf", ".", "eval", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "# for item in dev_data_list:", "\n", "\n", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "dev_data_list", ",", "config", ".", "PRO_ROOT", "/", "\"data/fever/upstream_sentence_selection_Feb16/4-15-dev_nli_results.jsonl\"", ")", "\n", "\n", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.eval_model": [[36, 115], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "model", "model", "list", "list", "list", "[].view().tolist", "model.tolist", "y_probs_list.extend", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ",", "\n", "feed_input_span", "=", "False", ")", ":", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "tqdm_disable", "=", "not", "show_progress", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "tqdm_disable", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "eval_s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "eval_s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "if", "not", "feed_input_span", ":", "\n", "                ", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "s1_span", "=", "eval_s1_span", ",", "s2_span", "=", "eval_s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'oid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'oid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "# r_item['probs'] =", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.get_inference_pair": [[117, 164], ["utils.list_dict_data_tool.list_to_dict", "isinstance", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_models.sentence_selection.bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "fever_utils.fever_db.get_cursor", "fever_sampler.nli_new_sampler.build_nli_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "isinstance", "utils.common.load_jsonl", "ValueError", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.build_nli_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_inference_pair", "(", "tag", ",", "is_training", ",", "sent_result_path", ",", "debug_num", "=", "None", ",", "evidence_filtering_threshold", "=", "0.01", ")", ":", "\n", "# sent_result_path = \"\"", "\n", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug_num", "is", "not", "None", ":", "\n", "# d_list = d_list[:10]", "\n", "        ", "d_list", "=", "d_list", "[", ":", "50", "]", "\n", "# d_list = d_list[:200]", "\n", "\n", "", "d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'id'", ")", "\n", "\n", "threshold_value", "=", "evidence_filtering_threshold", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\")", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\")", "\n", "\n", "# debug_num = None if not debug else 2971", "\n", "# debug_num = None", "\n", "\n", "if", "isinstance", "(", "sent_result_path", ",", "Path", ")", ":", "\n", "        ", "sent_list", "=", "common", ".", "load_jsonl", "(", "sent_result_path", ",", "debug_num", ")", "\n", "", "elif", "isinstance", "(", "sent_result_path", ",", "list", ")", ":", "\n", "        ", "sent_list", "=", "sent_result_path", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"{sent_result_path} is not of a valid argument type which should be [list, Path].\"", ")", "\n", "\n", "", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "sent_list", ",", "d_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "filltered_sent_dict", "=", "select_top_k_and_to_results_dict", "(", "d_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "threshold_value", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "d_list", ",", "filltered_sent_dict", ",", "\n", "'id'", ",", "[", "'predicted_evidence'", ",", "'predicted_scored_evidence'", "]", ")", "\n", "fever_db_cursor", "=", "fever_db", ".", "get_cursor", "(", "config", ".", "FEVER_DB", ")", "\n", "forward_items", "=", "build_nli_forward_item", "(", "d_list", ",", "is_training", "=", "is_training", ",", "db_cursor", "=", "fever_db_cursor", ")", "\n", "\n", "return", "forward_items", ",", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.model_go": [[166, 427], ["torch.manual_seed", "torch.manual_seed", "int", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.nli_new_sampler.get_nli_pair", "fever_sampler.nli_new_sampler.get_nli_pair", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "len", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "list", "print", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher", "neural_modules.model_EMA.EMA", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "fever_sampler.nli_new_sampler.get_nli_pair", "random.shuffle", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "loss.mean.backward", "os.path.join", "it.read", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "neural_modules.model_EMA.EMA.", "print", "any", "hasattr", "updated_model.named_parameters", "neural_modules.model_EMA.EMA.get_inference_model", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "torch.nn.DataParallel", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v2.eval_model", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", "th_filter_prob", "=", "0.2", ",", "top_k_sent", "=", "5", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "32", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "# schedule_type = 'warmup_constant'", "\n", "# 'warmup_cosine': warmup_cosine,", "\n", "# 'warmup_constant': warmup_constant,", "\n", "# 'warmup_linear': warmup_linear,", "\n", "schedule_type", "=", "'warmup_linear'", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "4000", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "# debug_mode = True", "\n", "# debug_mode = True", "\n", "debug_mode", "=", "False", "\n", "do_ema", "=", "True", "\n", "\n", "maxout_model", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "3", "\n", "# num_train_optimization_steps", "\n", "top_k", "=", "top_k_sent", "\n", "\n", "train_sent_filtering_prob", "=", "th_filter_prob", "\n", "dev_sent_filtering_prob", "=", "th_filter_prob", "\n", "experiment_name", "=", "f'fever_v2_nli_th{train_sent_filtering_prob}_tk{top_k}'", "\n", "\n", "# Data dataset and upstream sentence results.", "\n", "dev_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_dev_results.jsonl\"", ")", "\n", "train_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_train_results.jsonl\"", ")", "\n", "\n", "dev_fitems", ",", "dev_list", "=", "get_nli_pair", "(", "'dev'", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "dev_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "top_k_sent", ",", "sent_filter_value", "=", "dev_sent_filtering_prob", ")", "\n", "train_fitems", ",", "train_list", "=", "get_nli_pair", "(", "'train'", ",", "is_training", "=", "True", ",", "\n", "sent_level_results_list", "=", "train_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "top_k_sent", ",", "sent_filter_value", "=", "train_sent_filtering_prob", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "train_list", "=", "train_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "\n", "", "est_datasize", "=", "len", "(", "train_fitems", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "384", ",", "pair_order", "=", "pair_order", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "if", "not", "maxout_model", ":", "\n", "        ", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertPairMaxOutMatcher", "(", "bert_encoder", ",", "num_of_class", "=", "num_class", ",", "act_type", "=", "\"gelu\"", ",", "num_of_out_layers", "=", "2", ")", "\n", "\n", "", "ema", "=", "None", "\n", "if", "do_ema", ":", "\n", "        ", "ema", "=", "EMA", "(", "model", ",", "model", ".", "named_parameters", "(", ")", ",", "device_num", "=", "1", ")", "\n", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "print", "(", "\"Do EMA:\"", ",", "do_ema", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ",", "\n", "schedule", "=", "schedule_type", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "\n", "train_fitems_list", ",", "_", "=", "get_nli_pair", "(", "'train'", ",", "is_training", "=", "True", ",", "\n", "sent_level_results_list", "=", "train_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "5", ",", "sent_filter_value", "=", "train_sent_filtering_prob", ")", "\n", "\n", "random", ".", "shuffle", "(", "train_fitems_list", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "if", "not", "maxout_model", ":", "\n", "                ", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "s1_span", "=", "s1_span", ",", "s2_span", "=", "s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "if", "ema", "is", "not", "None", "and", "do_ema", ":", "\n", "                    ", "updated_model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "ema", "(", "updated_model", ".", "named_parameters", "(", ")", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "# dev_iter = biterator(dev_instances, num_epochs=1, shuffle=False)", "\n", "#", "\n", "# cur_eval_results_list = eval_model(model, dev_iter, device_num, with_probs=True, make_int=True,", "\n", "#                                    feed_input_span=maxout_model)", "\n", "#", "\n", "# ema_results_dict = list_dict_data_tool.list_to_dict(cur_eval_results_list, 'oid')", "\n", "# copied_dev_list = copy.deepcopy(dev_list)", "\n", "# list_dict_data_tool.append_item_from_dict_to_list(copied_dev_list, ema_results_dict,", "\n", "#                                                   'id', 'predicted_label')", "\n", "#", "\n", "# mode = {'standard': True}", "\n", "# strict_score, acc_score, pr, rec, f1 = fever_scorer.fever_score(copied_dev_list, dev_list,", "\n", "#                                                                 mode=mode, max_evidence=5)", "\n", "# logging_item = {", "\n", "#     'ss': strict_score, 'ac': acc_score,", "\n", "#     'pr': pr, 'rec': rec, 'f1': f1,", "\n", "# }", "\n", "#", "\n", "# if not debug_mode:", "\n", "#     save_file_name = f'i({update_step})|e({epoch_i})' \\", "\n", "#         f'|ss({strict_score})|ac({acc_score})|pr({pr})|rec({rec})|f1({f1})' \\", "\n", "#         f'|seed({seed})'", "\n", "#", "\n", "#     common.save_jsonl(copied_dev_list, Path(file_path_prefix) /", "\n", "#                       f\"{save_file_name}_dev_nli_results.json\")", "\n", "#", "\n", "#     # print(save_file_name)", "\n", "#     logging_agent.incorporate_results({}, save_file_name, logging_item)", "\n", "#     logging_agent.logging_to_file(Path(file_path_prefix) / \"log.json\")", "\n", "#", "\n", "#     model_to_save = model.module if hasattr(model, 'module') else model", "\n", "#     output_model_file = Path(file_path_prefix) / save_file_name", "\n", "#     torch.save(model_to_save.state_dict(), str(output_model_file))", "\n", "\n", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "                        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "ema_device_num", "=", "0", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "device", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_ema_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "ema_device_num", ",", "with_probs", "=", "True", ",", "\n", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_ema_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "ema_logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "                            ", "save_file_name", "=", "f'ema_i({update_step})|e({epoch_i})'", "f'|ss({strict_score})|ac({acc_score})|pr({pr})|rec({rec})|f1({f1})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_nli_results.json\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "ema_logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.model_eval": [[429, 548], ["utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.nli_new_sampler.get_nli_pair", "fever_sampler.nli_new_sampler.get_nli_pair", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "torch.nn.DataParallel.load_state_dict", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "torch.cuda.is_available", "torch.cuda.is_available", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v2.eval_model", "utils.common.save_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "utils.common.save_jsonl", "evaluation.fever_scorer.fever_score", "print", "torch.cuda.is_available", "torch.cuda.is_available", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v2.eval_model", "utils.common.save_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "utils.common.save_jsonl"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "", "", "", "", "", "", "def", "model_eval", "(", "model_path", ")", ":", "\n", "    ", "bert_model_name", "=", "'bert-base-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "32", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "debug_mode", "=", "False", "\n", "\n", "maxout_model", "=", "False", "\n", "\n", "num_class", "=", "3", "\n", "\n", "tag", "=", "'test'", "\n", "train_sent_filtering_prob", "=", "0.2", "\n", "dev_sent_filtering_prob", "=", "0.2", "\n", "test_sent_filtering_prob", "=", "0.2", "\n", "\n", "# Data dataset and upstream sentence results.", "\n", "dev_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_dev_results.jsonl\"", ")", "\n", "# train_sent_results_list = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_train_results.jsonl\")", "\n", "test_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_test_results.jsonl\"", ")", "\n", "\n", "dev_fitems", ",", "dev_list", "=", "get_nli_pair", "(", "'dev'", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "dev_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "5", ",", "sent_filter_value", "=", "dev_sent_filtering_prob", ")", "\n", "# train_fitems, train_list = get_nli_pair('train', is_training=True,", "\n", "#                                         sent_level_results_list=train_sent_results_list, debug=debug_mode,", "\n", "#                                         sent_top_k=5, sent_filter_value=train_sent_filtering_prob)", "\n", "test_fitems", ",", "test_list", "=", "get_nli_pair", "(", "'test'", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "test_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "5", ",", "sent_filter_value", "=", "test_sent_filtering_prob", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "# train_list = train_list[:100]", "\n", "test_list", "=", "test_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "\n", "# est_datasize = len(train_fitems)", "\n", "\n", "", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "384", ",", "pair_order", "=", "pair_order", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "if", "not", "maxout_model", ":", "\n", "        ", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertPairMaxOutMatcher", "(", "bert_encoder", ",", "num_of_class", "=", "num_class", ",", "act_type", "=", "\"gelu\"", ",", "num_of_out_layers", "=", "2", ")", "\n", "\n", "", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "# train_instances = bert_cs_reader.read(train_fitems)", "\n", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"nli_{tag}_label_results_th{dev_sent_filtering_prob}.jsonl\"", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "f\"nli_{tag}_cp_results_th{dev_sent_filtering_prob}.jsonl\"", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ",", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"nli_{tag}_label_results_th{test_sent_filtering_prob}.jsonl\"", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_test_list", "=", "copy", ".", "deepcopy", "(", "test_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_test_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_test_list", ",", "f\"nli_{tag}_cp_results_th{test_sent_filtering_prob}.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.model_eval_ablation": [[550, 675], ["print", "print", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.nli_new_sampler.get_nli_pair", "fever_sampler.nli_new_sampler.get_nli_pair", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "torch.nn.DataParallel.load_state_dict", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "torch.cuda.is_available", "torch.cuda.is_available", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v2.eval_model", "utils.common.save_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "utils.common.save_jsonl", "evaluation.fever_scorer.fever_score", "print", "utils.common.save_json", "torch.cuda.is_available", "torch.cuda.is_available", "allennlp.data.iterators.BasicIterator.", "bert_fever_nli_v2.eval_model", "utils.common.save_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "utils.common.save_jsonl"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "", "def", "model_eval_ablation", "(", "model_path", ",", "filter_value", "=", "0.2", ",", "top_k_sent", "=", "5", ")", ":", "\n", "    ", "bert_model_name", "=", "'bert-base-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "32", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "debug_mode", "=", "False", "\n", "\n", "maxout_model", "=", "False", "\n", "\n", "num_class", "=", "3", "\n", "\n", "tag", "=", "'dev'", "\n", "exp", "=", "'no_re_train'", "\n", "print", "(", "\"Filter value:\"", ",", "filter_value", ")", "\n", "print", "(", "\"top_k_sent:\"", ",", "top_k_sent", ")", "\n", "train_sent_filtering_prob", "=", "0.2", "\n", "dev_sent_filtering_prob", "=", "filter_value", "\n", "test_sent_filtering_prob", "=", "0.2", "\n", "\n", "# Data dataset and upstream sentence results.", "\n", "dev_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_dev_results.jsonl\"", ")", "\n", "# train_sent_results_list = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_train_results.jsonl\")", "\n", "test_sent_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_test_results.jsonl\"", ")", "\n", "\n", "dev_fitems", ",", "dev_list", "=", "get_nli_pair", "(", "'dev'", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "dev_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "top_k_sent", ",", "sent_filter_value", "=", "dev_sent_filtering_prob", ")", "\n", "# train_fitems, train_list = get_nli_pair('train', is_training=True,", "\n", "#                                         sent_level_results_list=train_sent_results_list, debug=debug_mode,", "\n", "#                                         sent_top_k=5, sent_filter_value=train_sent_filtering_prob)", "\n", "test_fitems", ",", "test_list", "=", "get_nli_pair", "(", "'test'", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "test_sent_results_list", ",", "debug", "=", "debug_mode", ",", "\n", "sent_top_k", "=", "top_k_sent", ",", "sent_filter_value", "=", "test_sent_filtering_prob", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "# train_list = train_list[:100]", "\n", "test_list", "=", "test_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "\n", "# est_datasize = len(train_fitems)", "\n", "\n", "", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "384", ",", "pair_order", "=", "pair_order", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "if", "not", "maxout_model", ":", "\n", "        ", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertPairMaxOutMatcher", "(", "bert_encoder", ",", "num_of_class", "=", "num_class", ",", "act_type", "=", "\"gelu\"", ",", "num_of_out_layers", "=", "2", ")", "\n", "\n", "", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "# train_instances = bert_cs_reader.read(train_fitems)", "\n", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"nli_{tag}_label_results_th{dev_sent_filtering_prob}_{exp}.jsonl\"", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "f\"nli_{tag}_cp_results_th{dev_sent_filtering_prob}_{exp}.jsonl\"", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "common", ".", "save_json", "(", "logging_item", ",", "\n", "f\"nli_th{dev_sent_filtering_prob}_{exp}_ss:{strict_score}_ac:{acc_score}_pr:{pr}_rec:{rec}_f1:{f1}.jsonl\"", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "\n", "feed_input_span", "=", "maxout_model", ",", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"nli_{tag}_label_results_th{test_sent_filtering_prob}.jsonl\"", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_test_list", "=", "copy", ".", "deepcopy", "(", "test_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_test_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_test_list", ",", "f\"nli_{tag}_cp_results_th{test_sent_filtering_prob}.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_fever_nli_v2.ensemble_results": [[677, 679], ["None"], "function", ["None"], ["", "", "def", "ensemble_results", "(", ")", ":", "\n", "    ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_encoder_maxout_model.hidden_eval": [[34, 119], ["print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "enumerate", "range", "print", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "flint.torch_util.get_length_and_mask", "model", "y_id_list.extend", "y_pred_list.extend", "model.size", "len", "len", "len", "len", "len", "list", "[].view().tolist", "y_logits_list.extend", "y_probs_list.extend", "str", "str", "len", "model.tolist", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "hidden_eval", "(", "model", ",", "data_iter", ",", "dev_data_list", ",", "device", ",", "with_logits", "=", "False", ",", "with_probs", "=", "False", ",", "dev_num", "=", "-", "1", ")", ":", "\n", "# SUPPORTS < (-.-) > 0", "\n", "# REFUTES < (-.-) > 1", "\n", "# NOT ENOUGH INFO < (-.-) > 2", "\n", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_id_list", "=", "[", "]", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "# if append_text:", "\n", "# y_premise = []", "\n", "# y_hypothesis = []", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "cuda_device", "=", "dev_num", ")", "\n", "s1_sequence", "=", "batch", "[", "'s1_sequence'", "]", "\n", "s1_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "s1_sequence", ")", "\n", "s2_sequence", "=", "batch", "[", "'s2_sequence'", "]", "\n", "s2_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "s2_sequence", ")", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "\n", "out", "=", "model", "(", "s1_sequence", ",", "s1_att_mask", ",", "s2_sequence", ",", "s2_att_mask", ",", "\n", "mode", "=", "BertSupervisedVecClassifier", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "# out = model(eval_paired_sequence, token_type_ids=eval_paired_segments_ids,", "\n", "#             attention_mask=eval_att_mask,", "\n", "#             s1_span=eval_s1_span, s2_span=eval_s2_span,", "\n", "#             mode=BertPairMaxOutMatcher.ForwardMode.EVAL,", "\n", "#             labels=None)", "\n", "\n", "y_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "\n", "# if append_text:", "\n", "# y_premise.extend(list(batch['text']))", "\n", "# y_hypothesis.extend(list(batch['query']))", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_logits", ":", "\n", "                ", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "y_id_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "dev_data_list", ")", ")", ":", "\n", "            ", "assert", "str", "(", "y_id_list", "[", "i", "]", ")", "==", "str", "(", "dev_data_list", "[", "i", "]", "[", "'id'", "]", ")", "\n", "\n", "# Matching id", "\n", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "if", "with_logits", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'probs'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "# Reset neural set", "\n", "", "if", "len", "(", "dev_data_list", "[", "i", "]", "[", "'predicted_sentids'", "]", ")", "==", "0", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "\"NOT ENOUGH INFO\"", "\n", "\n", "# if append_text:", "\n", "#     dev_data_list[i]['premise'] = y_premise[i]", "\n", "#     dev_data_list[i]['hypothesis'] = y_hypothesis[i]", "\n", "\n", "", "", "print", "(", "'total_size:'", ",", "totoal_size", ")", "\n", "\n", "", "return", "dev_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_encoder_maxout_model.eval_model": [[121, 159], ["model.eval", "print", "zip", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_labels_ids.to.to", "eval_att_mask.to.to", "model", "model.size", "output_pred_list.extend", "output_logits_list.extend", "output_id_list.extend", "y_pred_list.extend", "datetime.datetime.now", "[].view().tolist", "model.tolist", "list", "eval_labels_ids.to.tolist", "[].view", "model.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "eval_iter", ",", "device", ")", ":", "\n", "    ", "output_logits_list", "=", "[", "]", "\n", "output_id_list", "=", "[", "]", "\n", "output_pred_list", "=", "[", "]", "\n", "y_pred_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "print", "(", "f\"Start Eval ({datetime.datetime.now()}):\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "eval_iter", ")", ":", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "eval_labels_ids", "=", "eval_labels_ids", ".", "to", "(", "device", ")", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "eval_logits", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "labels", "=", "None", ")", "\n", "total_size", "+=", "eval_logits", ".", "size", "(", "0", ")", "\n", "\n", "output_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "eval_logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "eval_logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "output_logits_list", ".", "extend", "(", "eval_logits", ".", "tolist", "(", ")", ")", "\n", "output_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "y_pred_list", ".", "extend", "(", "eval_labels_ids", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "output_pred_list", ")", "\n", "correct", "=", "0", "\n", "for", "pred", ",", "y", "in", "zip", "(", "output_pred_list", ",", "y_pred_list", ")", ":", "\n", "        ", "if", "pred", "==", "y", ":", "\n", "            ", "correct", "+=", "1", "\n", "\n", "", "", "print", "(", "correct", ",", "total_size", ",", "correct", "/", "total_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_encoder_maxout_model.model_go": [[161, 424], ["int", "int", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader", "data_utils.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertModel.from_pretrained", "hotpot_content_selection.bert_para2vec_rank.BertContent2Vec", "bert_model_variances.bert_maxout_clf.BertSupervisedVecClassifier", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "print", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.adv_simi_sample_with_prob_v1_1", "random.shuffle", "print", "data_utils.readers.bert_fever_verification_separate_seq.BertSeparateSeqReader.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "utils.common.load_json", "random.shuffle", "len", "tqdm.tqdm", "nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "flint.torch_util.get_length_and_mask", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_encoder_maxout_model.hidden_eval", "bert_encoder_maxout_model.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_encoder_maxout_model.hidden_eval", "bert_encoder_maxout_model.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "for", "some_params", "in", "[", "0", "]", ":", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "        ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "4", "\n", "do_ema", "=", "False", "\n", "dev_prob_threshold", "=", "0.1", "\n", "train_prob_threshold", "=", "0.35", "\n", "debug_mode", "=", "False", "\n", "experiment_name", "=", "f\"fever_nli_bert_encoder_maxout_l4_on_fulldata\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_fulldata_aug_the_same_gt_mrate({some_params})\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_10p_aug_the_same_gt_mrate({some_params})\"", "\n", "\n", "# data_aug = True", "\n", "data_aug", "=", "False", "\n", "data_aug_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"qa_aug/squad_train_turker_groundtruth.json\"", "\n", "# data_aug_size = int(21_015 * some_params)   # 10p", "\n", "data_aug_size", "=", "int", "(", "208_346", "*", "some_params", ")", "\n", "\n", "# training_file = config.FEVER_DATA_ROOT / \"fever_1.0/train_10.jsonl\"", "\n", "training_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/train.jsonl\"", "\n", "\n", "train_sample_top_k", "=", "8", "\n", "\n", "# est_datasize = 208_346    # full", "\n", "# est_datasize = 14_544", "\n", "# est_datasize = 21_015 + data_aug_size   # 10p", "\n", "est_datasize", "=", "208_346", "+", "data_aug_size", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "dev_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\"", ")", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# print(dev_data_list[0])", "\n", "# exit(0)", "\n", "\n", "train_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/train_sent_scores.jsonl\"", ")", "\n", "# Finished loading standardized sentence file.", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertSeparateSeqReader", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model_content2vec", "=", "BertContent2Vec", "(", "bert_encoder", ",", "num_of_out_layers", "=", "4", ")", "\n", "model_clf", "=", "BertSupervisedVecClassifier", "(", "model_content2vec", ",", "num_class", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "            ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "            ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "2_000", "# Change this to real evaluation.", "\n", "best_fever_score", "=", "-", "1", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "            ", "print", "(", "\"Resampling...\"", ")", "\n", "train_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "training_file", ",", "\n", "train_upstream_sent_list", ",", "\n", "train_prob_threshold", ",", "\n", "top_n", "=", "train_sample_top_k", ")", "\n", "#", "\n", "train_data_list", "=", "fever_nli_sampler", ".", "adv_simi_sample_with_prob_v1_1", "(", "\n", "training_file", ",", "\n", "train_sent_after_threshold_filter", ",", "\n", "None", ",", "\n", "tokenized", "=", "True", ")", "\n", "\n", "aug_d_list", "=", "[", "]", "\n", "if", "data_aug", ":", "\n", "                ", "aug_d_list", "=", "common", ".", "load_json", "(", "data_aug_file", ")", "\n", "random", ".", "shuffle", "(", "aug_d_list", ")", "\n", "aug_d_list", "=", "aug_d_list", "[", ":", "data_aug_size", "]", "\n", "\n", "", "train_data_list", "=", "train_data_list", "+", "aug_d_list", "\n", "\n", "random", ".", "shuffle", "(", "train_data_list", ")", "\n", "print", "(", "\"Sample data length:\"", ",", "len", "(", "train_data_list", ")", ")", "\n", "sampled_train_instances", "=", "bert_fever_reader", ".", "read", "(", "train_data_list", ")", "\n", "#", "\n", "train_iter", "=", "biterator", "(", "sampled_train_instances", ",", "shuffle", "=", "True", ",", "num_epochs", "=", "1", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "                ", "model_clf", ".", "train", "(", ")", "\n", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "cuda_device", "=", "dev_num", ")", "\n", "s1_sequence", "=", "batch", "[", "'s1_sequence'", "]", "\n", "s1_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "s1_sequence", ")", "\n", "s2_sequence", "=", "batch", "[", "'s2_sequence'", "]", "\n", "s2_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "s2_sequence", ")", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "\n", "loss", "=", "model_clf", "(", "s1_sequence", ",", "s1_att_mask", ",", "s2_sequence", ",", "s2_att_mask", ",", "\n", "mode", "=", "BertSupervisedVecClassifier", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                        ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                        ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ",", "dev_num", "=", "dev_num", ")", "\n", "", "else", ":", "\n", "                            ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ",", "dev_num", "=", "dev_num", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                            ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ",", "dev_num", ")", "\n", "", "else", ":", "\n", "                ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ",", "dev_num", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_multilayer_seq_pairing.hidden_eval": [[31, 118], ["print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "enumerate", "range", "print", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_att_mask.to.to", "eval_s1_span.to.to", "eval_s2_span.to.to", "model", "y_id_list.extend", "y_pred_list.extend", "model.size", "len", "len", "len", "len", "len", "list", "[].view().tolist", "y_logits_list.extend", "y_probs_list.extend", "str", "str", "len", "model.tolist", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "hidden_eval", "(", "model", ",", "data_iter", ",", "dev_data_list", ",", "device", ",", "with_logits", "=", "False", ",", "with_probs", "=", "False", ")", ":", "\n", "# SUPPORTS < (-.-) > 0", "\n", "# REFUTES < (-.-) > 1", "\n", "# NOT ENOUGH INFO < (-.-) > 2", "\n", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_id_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "# if append_text:", "\n", "# y_premise = []", "\n", "# y_hypothesis = []", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "# eval_labels_ids = batch['label']", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "eval_s1_span", ",", "eval_s2_span", "=", "batch", "[", "'bert_s1_span'", "]", ",", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "# eval_labels_ids = eval_labels_ids.to(device)", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "eval_s1_span", "=", "eval_s1_span", ".", "to", "(", "device", ")", "\n", "eval_s2_span", "=", "eval_s2_span", ".", "to", "(", "device", ")", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "y_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "\n", "# if append_text:", "\n", "# y_premise.extend(list(batch['text']))", "\n", "# y_hypothesis.extend(list(batch['query']))", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_logits", ":", "\n", "                ", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "y_id_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "dev_data_list", ")", ")", ":", "\n", "            ", "assert", "str", "(", "y_id_list", "[", "i", "]", ")", "==", "str", "(", "dev_data_list", "[", "i", "]", "[", "'id'", "]", ")", "\n", "\n", "# Matching id", "\n", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "if", "with_logits", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'probs'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "# Reset neural set", "\n", "", "if", "len", "(", "dev_data_list", "[", "i", "]", "[", "'predicted_sentids'", "]", ")", "==", "0", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "\"NOT ENOUGH INFO\"", "\n", "\n", "# if append_text:", "\n", "#     dev_data_list[i]['premise'] = y_premise[i]", "\n", "#     dev_data_list[i]['hypothesis'] = y_hypothesis[i]", "\n", "\n", "", "", "print", "(", "'total_size:'", ",", "totoal_size", ")", "\n", "\n", "", "return", "dev_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_multilayer_seq_pairing.model_go": [[120, 387], ["int", "int", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "range", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "print", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.adv_simi_sample_with_prob_v1_1", "random.shuffle", "print", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "utils.common.load_json", "random.shuffle", "len", "tqdm.tqdm", "nn.DataParallel.train", "flint.torch_util.get_length_and_mask", "paired_sequence.to.to", "paired_segments_ids.to.to", "labels_ids.to.to", "att_mask.to.to", "s1_span.to.to", "s2_span.to.to", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_multilayer_seq_pairing.hidden_eval", "bert_multilayer_seq_pairing.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_multilayer_seq_pairing.hidden_eval", "bert_multilayer_seq_pairing.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "model_go", "(", ")", ":", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "    ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "do_ema", "=", "False", "\n", "dev_prob_threshold", "=", "0.1", "\n", "train_prob_threshold", "=", "0.35", "\n", "debug_mode", "=", "False", "\n", "experiment_name", "=", "f\"fever_nli_bert_multilayer_matching_pretrained_pooler\"", "\n", "num_of_pooling_layer", "=", "1", "\n", "use_pretrained_pooler", "=", "True", "\n", "\n", "data_aug", "=", "False", "\n", "data_aug_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"qa_aug/squad_train_turker_groundtruth.json\"", "\n", "# data_aug_size = int(21_015 * some_params)   # 10p", "\n", "# data_aug_size = int(208_346 * 0)", "\n", "data_aug_size", "=", "int", "(", "0", ")", "\n", "\n", "# training_file = config.FEVER_DATA_ROOT / \"fever_1.0/train_10.jsonl\"", "\n", "training_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/train.jsonl\"", "\n", "\n", "train_sample_top_k", "=", "8", "\n", "\n", "# est_datasize = 208_346    # full", "\n", "# est_datasize = 14_544", "\n", "# est_datasize = 21_015 + data_aug_size   # 10p", "\n", "est_datasize", "=", "208_346", "+", "data_aug_size", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\"", ")", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# print(dev_data_list[0])", "\n", "# exit(0)", "\n", "\n", "train_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/train_sent_scores.jsonl\"", ")", "\n", "# Finished loading standardized sentence file.", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertReaderFeverNLI", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# Load training model", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model_clf", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "\n", "act_type", "=", "'tanh'", ",", "\n", "num_of_pooling_layer", "=", "num_of_pooling_layer", ",", "\n", "use_pretrained_pooler", "=", "use_pretrained_pooler", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "        ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "        ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "2_000", "# Change this to real evaluation.", "\n", "best_fever_score", "=", "-", "1", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Resampling...\"", ")", "\n", "train_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "training_file", ",", "\n", "train_upstream_sent_list", ",", "\n", "train_prob_threshold", ",", "\n", "top_n", "=", "train_sample_top_k", ")", "\n", "#", "\n", "train_data_list", "=", "fever_nli_sampler", ".", "adv_simi_sample_with_prob_v1_1", "(", "\n", "training_file", ",", "\n", "train_sent_after_threshold_filter", ",", "\n", "None", ",", "\n", "tokenized", "=", "True", ")", "\n", "\n", "aug_d_list", "=", "[", "]", "\n", "if", "data_aug", ":", "\n", "            ", "aug_d_list", "=", "common", ".", "load_json", "(", "data_aug_file", ")", "\n", "random", ".", "shuffle", "(", "aug_d_list", ")", "\n", "aug_d_list", "=", "aug_d_list", "[", ":", "data_aug_size", "]", "\n", "\n", "", "train_data_list", "=", "train_data_list", "+", "aug_d_list", "\n", "\n", "random", ".", "shuffle", "(", "train_data_list", ")", "\n", "print", "(", "\"Sample data length:\"", ",", "len", "(", "train_data_list", ")", ")", "\n", "sampled_train_instances", "=", "bert_fever_reader", ".", "read", "(", "train_data_list", ")", "\n", "#", "\n", "train_iter", "=", "biterator", "(", "sampled_train_instances", ",", "shuffle", "=", "True", ",", "num_epochs", "=", "1", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "            ", "model_clf", ".", "train", "(", ")", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "labels_ids", "=", "labels_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "s1_span", "=", "s1_span", ".", "to", "(", "device", ")", "\n", "s2_span", "=", "s2_span", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model_clf", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                    ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                        ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                            ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                        ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                        ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "            ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "            ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_1.BertSeqClassifier.__init__": [[35, 41], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "bert_v0_1.init_bert_weights"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights"], ["", "def", "__init__", "(", "self", ",", "bert_encoder", ")", ":", "\n", "        ", "super", "(", "BertSeqClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ",", "2", ")", "# Should we have dropout here? Later?", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "init_bert_weights", "(", "self", ".", "qa_outputs", ",", "initializer_range", "=", "0.02", ")", "# Hard code this value", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_1.BertSeqClassifier.forward": [[42, 58], ["bert_v0_1.BertSeqClassifier.bert_encoder", "bert_v0_1.BertSeqClassifier.dropout", "bert_v0_1.BertSeqClassifier.final_linear", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "bert_v0_1.BertSeqClassifier.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "mode", "=", "ForwardMode", ".", "TRAIN", ",", "labels", "=", "None", ")", ":", "\n", "# Precomputing of the max_context_length is important", "\n", "# because we want the same value to be shared to different GPUs, dynamic calculating is not feasible.", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert_encoder", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "final_linear", "(", "pooled_output", ")", "\n", "\n", "if", "mode", "==", "BertSeqClassifier", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "assert", "labels", "is", "not", "None", "\n", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_v0_1.init_bert_weights": [[16, 28], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "function", ["None"], ["def", "init_bert_weights", "(", "module", ",", "initializer_range", ")", ":", "\n", "    ", "\"\"\" Initialize the weights.\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "        ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "        ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval": [[31, 118], ["print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "enumerate", "range", "print", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_att_mask.to.to", "eval_s1_span.to.to", "eval_s2_span.to.to", "model", "y_id_list.extend", "y_pred_list.extend", "model.size", "len", "len", "len", "len", "len", "list", "[].view().tolist", "y_logits_list.extend", "y_probs_list.extend", "str", "str", "len", "model.tolist", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "hidden_eval", "(", "model", ",", "data_iter", ",", "dev_data_list", ",", "device", ",", "with_logits", "=", "False", ",", "with_probs", "=", "False", ")", ":", "\n", "# SUPPORTS < (-.-) > 0", "\n", "# REFUTES < (-.-) > 1", "\n", "# NOT ENOUGH INFO < (-.-) > 2", "\n", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_id_list", "=", "[", "]", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "# if append_text:", "\n", "# y_premise = []", "\n", "# y_hypothesis = []", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "data_iter", ")", ":", "\n", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "# eval_labels_ids = batch['label']", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "eval_s1_span", ",", "eval_s2_span", "=", "batch", "[", "'bert_s1_span'", "]", ",", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "# eval_labels_ids = eval_labels_ids.to(device)", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "eval_s1_span", "=", "eval_s1_span", ".", "to", "(", "device", ")", "\n", "eval_s2_span", "=", "eval_s2_span", ".", "to", "(", "device", ")", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "s1_span", "=", "eval_s1_span", ",", "s2_span", "=", "eval_s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "None", ")", "\n", "\n", "y_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "\n", "# if append_text:", "\n", "# y_premise.extend(list(batch['text']))", "\n", "# y_hypothesis.extend(list(batch['query']))", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_logits", ":", "\n", "                ", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "y_id_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "dev_data_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "dev_data_list", ")", ")", ":", "\n", "            ", "assert", "str", "(", "y_id_list", "[", "i", "]", ")", "==", "str", "(", "dev_data_list", "[", "i", "]", "[", "'id'", "]", ")", "\n", "\n", "# Matching id", "\n", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "if", "with_logits", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "\n", "", "if", "with_probs", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'probs'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "# Reset neural set", "\n", "", "if", "len", "(", "dev_data_list", "[", "i", "]", "[", "'predicted_sentids'", "]", ")", "==", "0", ":", "\n", "                ", "dev_data_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "\"NOT ENOUGH INFO\"", "\n", "\n", "# if append_text:", "\n", "#     dev_data_list[i]['premise'] = y_premise[i]", "\n", "#     dev_data_list[i]['hypothesis'] = y_hypothesis[i]", "\n", "\n", "", "", "print", "(", "'total_size:'", ",", "totoal_size", ")", "\n", "\n", "", "return", "dev_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.eval_model": [[120, 158], ["model.eval", "print", "zip", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_labels_ids.to.to", "eval_att_mask.to.to", "model", "model.size", "output_pred_list.extend", "output_logits_list.extend", "output_id_list.extend", "y_pred_list.extend", "datetime.datetime.now", "[].view().tolist", "model.tolist", "list", "eval_labels_ids.to.tolist", "[].view", "model.size", "torch.max", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "eval_iter", ",", "device", ")", ":", "\n", "    ", "output_logits_list", "=", "[", "]", "\n", "output_id_list", "=", "[", "]", "\n", "output_pred_list", "=", "[", "]", "\n", "y_pred_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "print", "(", "f\"Start Eval ({datetime.datetime.now()}):\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "eval_iter", ")", ":", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "eval_labels_ids", "=", "eval_labels_ids", ".", "to", "(", "device", ")", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "eval_logits", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "labels", "=", "None", ")", "\n", "total_size", "+=", "eval_logits", ".", "size", "(", "0", ")", "\n", "\n", "output_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "eval_logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "eval_logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "output_logits_list", ".", "extend", "(", "eval_logits", ".", "tolist", "(", ")", ")", "\n", "output_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "y_pred_list", ".", "extend", "(", "eval_labels_ids", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "output_pred_list", ")", "\n", "correct", "=", "0", "\n", "for", "pred", ",", "y", "in", "zip", "(", "output_pred_list", ",", "y_pred_list", ")", ":", "\n", "        ", "if", "pred", "==", "y", ":", "\n", "            ", "correct", "+=", "1", "\n", "\n", "", "", "print", "(", "correct", ",", "total_size", ",", "correct", "/", "total_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.model_go": [[160, 431], ["int", "int", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_maxout_clf.BertPairMaxOutMatcher", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "nn.DataParallel.train", "range", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "print", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.adv_simi_sample_with_prob_v1_1", "random.shuffle", "print", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "utils.common.load_json", "random.shuffle", "len", "tqdm.tqdm", "flint.torch_util.get_length_and_mask", "paired_sequence.to.to", "paired_segments_ids.to.to", "labels_ids.to.to", "att_mask.to.to", "s1_span.to.to", "s2_span.to.to", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_layermaxout_v0.hidden_eval", "bert_layermaxout_v0.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_layermaxout_v0.hidden_eval", "bert_layermaxout_v0.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "for", "some_params", "in", "[", "0", "]", ":", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "        ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "4", "\n", "do_ema", "=", "False", "\n", "dev_prob_threshold", "=", "0.1", "\n", "train_prob_threshold", "=", "0.35", "\n", "debug_mode", "=", "False", "\n", "experiment_name", "=", "f\"fever_nli_bert_maxout_l4_on_fulldata\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_fulldata_aug_the_same_gt_mrate({some_params})\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_10p_aug_the_same_gt_mrate({some_params})\"", "\n", "\n", "# data_aug = True", "\n", "data_aug", "=", "False", "\n", "data_aug_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"qa_aug/squad_train_turker_groundtruth.json\"", "\n", "# data_aug_size = int(21_015 * some_params)   # 10p", "\n", "data_aug_size", "=", "int", "(", "208_346", "*", "some_params", ")", "\n", "\n", "# training_file = config.FEVER_DATA_ROOT / \"fever_1.0/train_10.jsonl\"", "\n", "training_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/train.jsonl\"", "\n", "\n", "train_sample_top_k", "=", "8", "\n", "\n", "# est_datasize = 208_346    # full", "\n", "# est_datasize = 14_544", "\n", "# est_datasize = 21_015 + data_aug_size   # 10p", "\n", "est_datasize", "=", "208_346", "+", "data_aug_size", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\"", ")", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# print(dev_data_list[0])", "\n", "# exit(0)", "\n", "\n", "train_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/train_sent_scores.jsonl\"", ")", "\n", "# Finished loading standardized sentence file.", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertReaderFeverNLI", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "# Load training model", "\n", "# model_clf = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_class)", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model_clf", "=", "BertPairMaxOutMatcher", "(", "bert_encoder", ",", "num_of_class", "=", "3", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "            ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "model_clf", ".", "train", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "2_000", "# Change this to real evaluation.", "\n", "best_fever_score", "=", "-", "1", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "            ", "print", "(", "\"Resampling...\"", ")", "\n", "train_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "training_file", ",", "\n", "train_upstream_sent_list", ",", "\n", "train_prob_threshold", ",", "\n", "top_n", "=", "train_sample_top_k", ")", "\n", "#", "\n", "train_data_list", "=", "fever_nli_sampler", ".", "adv_simi_sample_with_prob_v1_1", "(", "\n", "training_file", ",", "\n", "train_sent_after_threshold_filter", ",", "\n", "None", ",", "\n", "tokenized", "=", "True", ")", "\n", "\n", "aug_d_list", "=", "[", "]", "\n", "if", "data_aug", ":", "\n", "                ", "aug_d_list", "=", "common", ".", "load_json", "(", "data_aug_file", ")", "\n", "random", ".", "shuffle", "(", "aug_d_list", ")", "\n", "aug_d_list", "=", "aug_d_list", "[", ":", "data_aug_size", "]", "\n", "\n", "", "train_data_list", "=", "train_data_list", "+", "aug_d_list", "\n", "\n", "random", ".", "shuffle", "(", "train_data_list", ")", "\n", "print", "(", "\"Sample data length:\"", ",", "len", "(", "train_data_list", ")", ")", "\n", "sampled_train_instances", "=", "bert_fever_reader", ".", "read", "(", "train_data_list", ")", "\n", "#", "\n", "train_iter", "=", "biterator", "(", "sampled_train_instances", ",", "shuffle", "=", "True", ",", "num_epochs", "=", "1", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "                ", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "labels_ids", "=", "labels_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "s1_span", "=", "s1_span", ".", "to", "(", "device", ")", "\n", "s2_span", "=", "s2_span", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model_clf", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "s1_span", "=", "s1_span", ",", "s2_span", "=", "s2_span", ",", "\n", "mode", "=", "BertPairMaxOutMatcher", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                        ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                        ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                            ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                            ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.model_go_pure_aug": [[433, 701], ["int", "int", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.select_sent_with_prob_for_eval", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertForSequenceClassification.from_pretrained", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "nn.DataParallel.train", "range", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "print", "fever_sampler.threshold_sampler_insure_unique", "fever_sampler.adv_simi_sample_with_prob_v1_1", "random.shuffle", "print", "data_utils.readers.bert_fever_reader.BertReaderFeverNLI.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "fever_sampler.qa_aug_sampler.get_sample_data", "random.shuffle", "len", "tqdm.tqdm", "flint.torch_util.get_length_and_mask", "paired_sequence.to.to", "paired_segments_ids.to.to", "labels_ids.to.to", "att_mask.to.to", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_layermaxout_v0.hidden_eval", "bert_layermaxout_v0.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "evaluation.fever_scorer.fever_score", "print", "print", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_layermaxout_v0.hidden_eval", "bert_layermaxout_v0.hidden_eval", "utils.common.load_jsonl", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.qa_aug_sampler.get_sample_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_layermaxout_v0.hidden_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "", "", "", "def", "model_go_pure_aug", "(", ")", ":", "\n", "# for some_params in [0.25, 0.25, 0.25]:", "\n", "    ", "for", "some_params", "in", "[", "0.25", ",", "0.25", ",", "0.25", "]", ":", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "        ", "seed", "=", "6", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "do_ema", "=", "False", "\n", "dev_prob_threshold", "=", "0.1", "\n", "train_prob_threshold", "=", "0.35", "\n", "debug_mode", "=", "False", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_fulldata\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_fulldata_aug_the_same_gt_mrate({some_params})\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_on_10p_aug_ratio({some_params})\"", "\n", "experiment_name", "=", "f\"bert_fever_nli_baseline_on_fulldata_aug_ratio({some_params})\"", "\n", "# experiment_name = f\"bert_fever_nli_baseline_pure_aug\"", "\n", "\n", "data_aug", "=", "True", "\n", "# data_aug_file = config.FEVER_DATA_ROOT / \"qa_aug/squad_train_turker_groundtruth.json\"", "\n", "# data_aug_size = int(21_015 * some_params)   # 10p", "\n", "# data_aug_size = int(208_346 * some_params)", "\n", "\n", "# training_file = config.FEVER_DATA_ROOT / \"fever_1.0/train_10.jsonl\"", "\n", "training_file", "=", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/train.jsonl\"", "\n", "\n", "train_sample_top_k", "=", "8", "\n", "\n", "# est_datasize = 208_346    # full", "\n", "# est_datasize = 14_544", "\n", "# est_datasize = 21_015 + data_aug_size   # 10p", "\n", "aug_size", "=", "int", "(", "208_346", "*", "some_params", ")", "\n", "est_datasize", "=", "208_346", "+", "aug_size", "\n", "# est_datasize = 208_346 + data_aug_size", "\n", "\n", "num_class", "=", "3", "\n", "\n", "# num_train_optimization_steps", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "# Finished build vocabulary.", "\n", "\n", "# Load standardized sentence file", "\n", "dev_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/dev_sent_pred_scores.jsonl\"", ")", "\n", "dev_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "\n", "dev_upstream_sent_list", ",", "\n", "prob_threshold", "=", "dev_prob_threshold", ",", "top_n", "=", "5", ")", "\n", "\n", "dev_data_list", "=", "fever_nli_sampler", ".", "select_sent_with_prob_for_eval", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ",", "dev_sent_after_threshold_filter", ",", "\n", "None", ",", "tokenized", "=", "True", ")", "\n", "\n", "# print(dev_data_list[0])", "\n", "# exit(0)", "\n", "\n", "train_upstream_sent_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\n", "\"upstream_sentence_selection_Feb16/train_sent_scores.jsonl\"", ")", "\n", "# Finished loading standardized sentence file.", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "bert_fever_reader", "=", "BertReaderFeverNLI", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_fever_reader", ".", "read", "(", "dev_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "# Load training model", "\n", "model_clf", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "bert_model_name", ",", "num_labels", "=", "num_class", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "            ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "model_clf", ".", "train", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "2_000", "# Change this to real evaluation.", "\n", "best_fever_score", "=", "-", "1", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "            ", "print", "(", "\"Resampling...\"", ")", "\n", "train_sent_after_threshold_filter", "=", "fever_ss_sampler", ".", "threshold_sampler_insure_unique", "(", "training_file", ",", "\n", "train_upstream_sent_list", ",", "\n", "train_prob_threshold", ",", "\n", "top_n", "=", "train_sample_top_k", ")", "\n", "#", "\n", "train_data_list", "=", "fever_nli_sampler", ".", "adv_simi_sample_with_prob_v1_1", "(", "\n", "training_file", ",", "\n", "train_sent_after_threshold_filter", ",", "\n", "None", ",", "\n", "tokenized", "=", "True", ")", "\n", "\n", "aug_d_list", "=", "[", "]", "\n", "if", "data_aug", ":", "\n", "                ", "aug_d_list", "=", "get_sample_data", "(", "-", "1", ")", "\n", "random", ".", "shuffle", "(", "aug_d_list", ")", "\n", "aug_d_list", "=", "aug_d_list", "[", ":", "aug_size", "]", "\n", "\n", "", "train_data_list", "=", "train_data_list", "+", "aug_d_list", "\n", "\n", "random", ".", "shuffle", "(", "train_data_list", ")", "\n", "# train_data_list = get_sample_data(-1)", "\n", "print", "(", "\"Sample data length:\"", ",", "len", "(", "train_data_list", ")", ")", "\n", "sampled_train_instances", "=", "bert_fever_reader", ".", "read", "(", "train_data_list", ")", "\n", "#", "\n", "train_iter", "=", "biterator", "(", "sampled_train_instances", ",", "shuffle", "=", "True", ",", "num_epochs", "=", "1", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "                ", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "labels_ids", "=", "labels_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model_clf", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                        ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                        ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                            ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                            ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_data_list", "=", "hidden_eval", "(", "ema_model_copy", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "", "else", ":", "\n", "                ", "dev_data_list", "=", "hidden_eval", "(", "model_clf", ",", "dev_iter", ",", "dev_data_list", ",", "device", ")", "\n", "\n", "", "eval_mode", "=", "{", "'check_sent_id_correct'", ":", "True", ",", "'standard'", ":", "True", "}", "\n", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "dev_data_list", ",", "\n", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"fever_1.0/shared_task_dev.jsonl\"", ")", ",", "\n", "mode", "=", "eval_mode", ",", "\n", "verbose", "=", "False", ")", "\n", "print", "(", "\"Fever Score(FScore/LScore:/Precision/Recall/F1):\"", ",", "fever_score", ",", "label_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "print", "(", "f\"Dev:{fever_score}/{label_score}\"", ")", "\n", "\n", "if", "best_fever_score", "<", "fever_score", ":", "\n", "                ", "print", "(", "\"New Best FScore\"", ")", "\n", "best_fever_score", "=", "fever_score", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'i({update_step})_epoch({n_epoch})_dev({fever_score})_lacc({label_score})_seed({seed})'", "\n", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.eval_model": [[33, 105], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.tolist", "y_probs_list.extend", "torch.softmax().tolist", "[].view", "model.size", "torch.softmax", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", "\n", "}", "\n", "\n", "print", "(", "\"Evaluating ...\"", ")", "\n", "tqdm_disable", "=", "not", "show_progress", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "tqdm_disable", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "# mode=BertMultiLayerSeqClassification.ForwardMode.EVAL,", "\n", "labels", "=", "None", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'oid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "F", ".", "softmax", "(", "out", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'oid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'logits'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "# r_item['probs'] =", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "r_item", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_pred_list", "[", "i", "]", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair": [[107, 149], ["utils.list_dict_data_tool.list_to_dict", "utils.common.load_jsonl", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_models.sentence_selection.bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "fever_utils.fever_db.get_cursor", "fever_sampler.nli_new_sampler.build_nli_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.build_nli_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_inference_pair", "(", "tag", ",", "is_training", ",", "sent_result_path", ",", "debug_num", "=", "None", ",", "evidence_filtering_threshold", "=", "0.01", ")", ":", "\n", "# sent_result_path = \"\"", "\n", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug_num", "is", "not", "None", ":", "\n", "# d_list = d_list[:10]", "\n", "        ", "d_list", "=", "d_list", "[", ":", "50", "]", "\n", "# d_list = d_list[:200]", "\n", "\n", "", "d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'id'", ")", "\n", "\n", "threshold_value", "=", "evidence_filtering_threshold", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\")", "\n", "# sent_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\")", "\n", "\n", "# debug_num = None if not debug else 2971", "\n", "# debug_num = None", "\n", "\n", "sent_list", "=", "common", ".", "load_jsonl", "(", "sent_result_path", ",", "debug_num", ")", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "sent_list", ",", "d_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "filltered_sent_dict", "=", "select_top_k_and_to_results_dict", "(", "d_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "threshold_value", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "d_list", ",", "filltered_sent_dict", ",", "\n", "'id'", ",", "[", "'predicted_evidence'", ",", "'predicted_scored_evidence'", "]", ")", "\n", "fever_db_cursor", "=", "fever_db", ".", "get_cursor", "(", "config", ".", "FEVER_DB", ")", "\n", "forward_items", "=", "build_nli_forward_item", "(", "d_list", ",", "is_training", "=", "is_training", ",", "db_cursor", "=", "fever_db_cursor", ")", "\n", "\n", "return", "forward_items", ",", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.model_go": [[151, 344], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "bert_hf_fever_nli_v1.get_inference_pair", "bert_hf_fever_nli_v1.get_inference_pair", "len", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_nli.BertFeverNLIReader", "pytorch_pretrained_bert.BertForSequenceClassification.from_pretrained", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "bert_hf_fever_nli_v1.get_inference_pair", "random.shuffle", "data_utils.readers.bert_reader_nli.BertFeverNLIReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "print", "allennlp.data.iterators.BasicIterator.", "bert_hf_fever_nli_v1.eval_model", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "any", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.bert_hf_fever_nli_v1.get_inference_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'fever_v1_nli'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "32", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "eval_frequency", "=", "2000", "\n", "do_lower_case", "=", "True", "\n", "pair_order", "=", "'cq'", "\n", "# debug_mode = True", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "3", "\n", "# num_train_optimization_steps", "\n", "\n", "train_sent_filtering_prob", "=", "0.35", "\n", "dev_sent_filtering_prob", "=", "0.01", "\n", "dev_sent_results_file", "=", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/i(5000)|e(0)|s01(0.9170917091709171)|s05(0.8842384238423843)|seed(12)_dev_sent_results.json\"", "\n", "train_sent_results_file", "=", "config", ".", "RESULT_PATH", "/", "\"doc_retri_results/fever_results/sent_results/4-14-sent_results_v0/train_sent_results.jsonl\"", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'SUPPORTS'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'REFUTES'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "'NOT ENOUGH INFO'", ",", "namespace", "=", "'labels'", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "# train_fitems_list = get_inference_pair('train', True, train_sent_results_file, debug_mode, train_sent_filtering_prob)", "\n", "dev_debug_num", "=", "2481", "if", "debug_mode", "else", "None", "\n", "dev_fitems_list", ",", "dev_list", "=", "get_inference_pair", "(", "'dev'", ",", "False", ",", "dev_sent_results_file", ",", "dev_debug_num", ",", "\n", "dev_sent_filtering_prob", ")", "\n", "# = common.load_jsonl(config.FEVER_DEV)", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "50", "]", "\n", "eval_frequency", "=", "1", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "train_debug_num", "=", "2971", "if", "debug_mode", "else", "None", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "est_datasize", "=", "len", "(", "train_fitems_list", ")", "\n", "\n", "# dev_o_dict = list_dict_data_tool.list_to_dict(dev_list, 'id')", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertFeverNLIReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "query_l", "=", "64", ",", "\n", "example_filter", "=", "None", ",", "max_l", "=", "364", ",", "pair_order", "=", "pair_order", ")", "\n", "#", "\n", "# bert_encoder = BertModel.from_pretrained(bert_model_name)", "\n", "# model = BertMultiLayerSeqClassification(bert_encoder, num_labels=num_class, num_of_pooling_layer=1,", "\n", "#                                         act_type='tanh', use_pretrained_pooler=True, use_sigmoid=False)", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "bert_model_name", ",", "num_labels", "=", "num_class", ")", "\n", "\n", "#", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "\n", "train_fitems_list", ",", "_", "=", "get_inference_pair", "(", "'train'", ",", "True", ",", "train_sent_results_file", ",", "train_debug_num", ",", "\n", "train_sent_filtering_prob", ")", "\n", "random", ".", "shuffle", "(", "train_fitems_list", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "# mode=BertMultiLayerSeqClassification.ForwardMode.TRAIN,", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ")", "\n", "\n", "results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_fitems_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|ss({strict_score})|ac({acc_score})|pr({pr})|rec({rec})|f1({f1})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "copied_dev_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_nli_results.json\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.ensemble_nli_results": [[11, 41], ["len", "copy.deepcopy", "range", "numpy.stack", "numpy.argmax", "range", "numpy.stack", "numpy.mean", "logits_list.append", "len", "len", "logits_current_logits_list.append", "numpy.asarray"], "function", ["None"], ["def", "ensemble_nli_results", "(", "nli_r_list", ")", ":", "\n", "    ", "id2label", "=", "{", "\n", "0", ":", "\"SUPPORTS\"", ",", "\n", "1", ":", "\"REFUTES\"", ",", "\n", "2", ":", "\"NOT ENOUGH INFO\"", ",", "\n", "}", "\n", "\n", "r_len", "=", "len", "(", "nli_r_list", "[", "0", "]", ")", "\n", "for", "nli_r", "in", "nli_r_list", ":", "\n", "        ", "assert", "len", "(", "nli_r", ")", "==", "r_len", "\n", "\n", "", "new_list", "=", "copy", ".", "deepcopy", "(", "nli_r_list", "[", "0", "]", ")", "\n", "logits_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "r_len", ")", ":", "\n", "        ", "logits_current_logits_list", "=", "[", "]", "\n", "for", "nli_r", "in", "nli_r_list", ":", "\n", "            ", "assert", "nli_r", "[", "i", "]", "[", "'oid'", "]", "==", "new_list", "[", "i", "]", "[", "'oid'", "]", "\n", "logits_current_logits_list", ".", "append", "(", "np", ".", "asarray", "(", "nli_r", "[", "i", "]", "[", "'logits'", "]", ",", "dtype", "=", "np", ".", "float32", ")", ")", "# [(3)]", "\n", "", "logits_current_logits", "=", "np", ".", "stack", "(", "logits_current_logits_list", ",", "axis", "=", "0", ")", "# [num, 3]", "\n", "current_mean_logits", "=", "np", ".", "mean", "(", "logits_current_logits", ",", "axis", "=", "0", ")", "# [3]", "\n", "logits_list", ".", "append", "(", "current_mean_logits", ")", "\n", "\n", "", "logits", "=", "np", ".", "stack", "(", "logits_list", ",", "axis", "=", "0", ")", "# (len, 3)", "\n", "y_", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", "# (len)", "\n", "assert", "y_", ".", "shape", "[", "0", "]", "==", "len", "(", "new_list", ")", "\n", "\n", "for", "i", "in", "range", "(", "r_len", ")", ":", "\n", "        ", "new_list", "[", "i", "]", "[", "'predicted_label'", "]", "=", "id2label", "[", "y_", "[", "i", "]", "]", "\n", "\n", "", "return", "new_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.build_submission_file": [[43, 52], ["open", "dict", "int", "out_f.write", "json.dumps"], "function", ["None"], ["", "def", "build_submission_file", "(", "d_list", ",", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "for", "item", "in", "d_list", ":", "\n", "            ", "instance_item", "=", "dict", "(", ")", "\n", "instance_item", "[", "'id'", "]", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "instance_item", "[", "'claim'", "]", "=", "item", "[", "'claim'", "]", "\n", "instance_item", "[", "'predicted_label'", "]", "=", "item", "[", "'predicted_label'", "]", "\n", "instance_item", "[", "'predicted_evidence'", "]", "=", "item", "[", "'predicted_evidence'", "]", "\n", "out_f", ".", "write", "(", "json", ".", "dumps", "(", "instance_item", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.delete_unused_evidence": [[54, 58], ["None"], "function", ["None"], ["", "", "", "def", "delete_unused_evidence", "(", "d_list", ")", ":", "\n", "    ", "for", "item", "in", "d_list", ":", "\n", "        ", "if", "item", "[", "'predicted_label'", "]", "==", "'NOT ENOUGH INFO'", ":", "\n", "            ", "item", "[", "'predicted_evidence'", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.evidence_adjustment": [[60, 87], ["utils.common.load_jsonl", "fever_sampler.nli_new_sampler.get_nli_pair", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score"], ["", "", "", "def", "evidence_adjustment", "(", "tag", ",", "sent_file", ",", "label_file", ",", "filter_prob", "=", "0.2", ",", "top_k", "=", "5", ")", ":", "\n", "    ", "dev_sent_filtering_prob", "=", "filter_prob", "\n", "\n", "# dev_list = common.load_jsonl(config.FEVER_DEV)", "\n", "dev_sent_results_list", "=", "common", ".", "load_jsonl", "(", "sent_file", ")", "\n", "\n", "dev_fitems", ",", "dev_list", "=", "get_nli_pair", "(", "tag", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "dev_sent_results_list", ",", "debug", "=", "False", ",", "\n", "sent_top_k", "=", "top_k", ",", "sent_filter_value", "=", "dev_sent_filtering_prob", ")", "\n", "\n", "cur_eval_results_list", "=", "common", ".", "load_jsonl", "(", "label_file", ")", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "cur_eval_results_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "# delete_unused_evidence(copied_dev_list)", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.eval_ensemble": [[89, 126], ["utils.common.load_jsonl", "fever_sampler.nli_new_sampler.get_nli_pair", "evidence_adjustment.ensemble_nli_results", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_item_from_dict_to_list", "utils.common.load_jsonl", "evaluation.fever_scorer.fever_score", "print", "utils.common.load_jsonl"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.nli.evidence_adjustment.ensemble_nli_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "eval_ensemble", "(", ")", ":", "\n", "    ", "sent_file", "=", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_sentence_level/04-24-00-11-19_fever_v0_slevel_retri_(ignore_non_verifiable-True)/fever_s_level_dev_results.jsonl\"", "\n", "dev_sent_filtering_prob", "=", "0.01", "\n", "tag", "=", "'dev'", "\n", "top_k", "=", "5", "\n", "\n", "# dev_list = common.load_jsonl(config.FEVER_DEV)", "\n", "dev_sent_results_list", "=", "common", ".", "load_jsonl", "(", "sent_file", ")", "\n", "\n", "dev_fitems", ",", "dev_list", "=", "get_nli_pair", "(", "tag", ",", "is_training", "=", "False", ",", "\n", "sent_level_results_list", "=", "dev_sent_results_list", ",", "debug", "=", "False", ",", "\n", "sent_top_k", "=", "top_k", ",", "sent_filter_value", "=", "dev_sent_filtering_prob", ")", "\n", "\n", "pred_file_list", "=", "[", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_nli/04-25-22:02:53_fever_v2_nli_th0.2/ema_i(20000)|e(3)|ss(0.7002700270027002)|ac(0.746024602460246)|pr(0.6141389138913633)|rec(0.8627362736273627)|f1(0.7175148212089147)|seed(12)/nli_dev_label_results_th0.2.jsonl\"", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_nli/04-26-10:15:39_fever_v2_nli_th0.2/ema_i(14000)|e(2)|ss(0.6991199119911992)|ac(0.7492249224922493)|pr(0.7129412941294097)|rec(0.8338583858385838)|f1(0.7686736484619933)|seed(12)/nli_dev_label_results_th0.2.jsonl\"", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_nli/04-27-10:03:27_fever_v2_nli_th0.2/ema_i(26000)|e(3)|ss(0.6958695869586958)|ac(0.7447744774477447)|pr(0.7129412941294097)|rec(0.8338583858385838)|f1(0.7686736484619933)|seed(12)/nli_dev_label_results_th0.2.jsonl\"", ",", "\n", "]", "\n", "pred_d_list", "=", "[", "common", ".", "load_jsonl", "(", "file", ")", "for", "file", "in", "pred_file_list", "]", "\n", "final_list", "=", "ensemble_nli_results", "(", "pred_d_list", ")", "\n", "pred_list", "=", "final_list", "\n", "\n", "ema_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "pred_list", ",", "'oid'", ")", "\n", "copied_dev_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_list", ",", "ema_results_dict", ",", "\n", "'id'", ",", "'predicted_label'", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.eval_model": [[29, 94], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "torch.sigmoid().view", "model.size", "torch.max", "torch.max", "torch.sigmoid", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "tqdm_disable", "=", "not", "show_progress", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "tqdm_disable", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'oid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'oid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.select_top_k_and_to_results_dict": [[96, 148], ["dict", "scored_dict.items", "fitems_dict.values", "sorted", "dict", "scored_element_list.append", "len", "len", "[].append", "[].append", "evidence_sid.append", "scored_evidence_sid.append", "int", "int", "evidence_sid.append", "scored_evidence_sid.append", "sid.split", "sid.split", "sid.split", "sid.split"], "function", ["None"], ["", "def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'prob'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "None", ")", ":", "\n", "    ", "results_dict", "=", "dict", "(", ")", "\n", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", "not", "in", "results_dict", ":", "\n", "            ", "results_dict", "[", "key", "]", "=", "dict", "(", ")", "\n", "\n", "# if merged_field_name not in value:", "\n", "#     results_dict[key]['scored_results'] = []", "\n", "#     results_dict[key]['predicated_evidence'] = []", "\n", "#     continue", "\n", "\n", "", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "key", "]", "[", "'scored_results'", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "evidence_sid", "=", "[", "]", "\n", "scored_evidence_sid", "=", "[", "]", "\n", "for", "s", ",", "e", "in", "sorted_e_list", ":", "\n", "            ", "if", "threshold", "is", "not", "None", ":", "\n", "                ", "if", "s", ">=", "threshold", ":", "\n", "                    ", "evidence_sid", ".", "append", "(", "e", ")", "\n", "scored_evidence_sid", ".", "append", "(", "[", "s", ",", "e", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "evidence_sid", ".", "append", "(", "e", ")", "\n", "scored_evidence_sid", ".", "append", "(", "[", "s", ",", "e", "]", ")", "\n", "", "", "evidence_sid", "=", "evidence_sid", "[", ":", "top_k", "]", "\n", "scored_evidence_sid", "=", "scored_evidence_sid", "[", ":", "top_k", "]", "\n", "\n", "assert", "len", "(", "evidence_sid", ")", "==", "len", "(", "scored_evidence_sid", ")", "\n", "\n", "results_dict", "[", "key", "]", "[", "'predicted_evidence'", "]", "=", "[", "]", "\n", "for", "sid", "in", "evidence_sid", ":", "\n", "            ", "doc_id", ",", "ln", "=", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "0", "]", ",", "int", "(", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "1", "]", ")", "\n", "results_dict", "[", "key", "]", "[", "'predicted_evidence'", "]", ".", "append", "(", "[", "doc_id", ",", "ln", "]", ")", "\n", "\n", "", "results_dict", "[", "key", "]", "[", "'predicted_scored_evidence'", "]", "=", "[", "]", "\n", "for", "score", ",", "sid", "in", "scored_evidence_sid", ":", "\n", "            ", "doc_id", ",", "ln", "=", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "0", "]", ",", "int", "(", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "1", "]", ")", "\n", "results_dict", "[", "key", "]", "[", "'predicted_scored_evidence'", "]", ".", "append", "(", "(", "score", ",", "[", "doc_id", ",", "ln", "]", ")", ")", "\n", "\n", "# predicted_sentids", "\n", "# results_dict[key]['predicted_sentids'] = results_dict[key]['predicted_evidence']", "\n", "\n", "", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.get_sentences": [[150, 172], ["utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "fever_utils.fever_db.get_cursor", "fever_sampler.ss_sampler.build_full_wiki_document_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.build_full_wiki_document_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_sentences", "(", "tag", ",", "is_training", ",", "debug", "=", "False", ")", ":", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "# d_list = d_list[:10]", "\n", "        ", "d_list", "=", "d_list", "[", ":", "50", "]", "\n", "# d_list = d_list[:200]", "\n", "\n", "", "doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "RESULT_PATH", "/", "f\"doc_retri_results/fever_results/merged_doc_results/m_doc_{tag}.jsonl\"", ")", "\n", "doc_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "doc_results", ",", "'id'", ")", "\n", "fever_db_cursor", "=", "fever_db", ".", "get_cursor", "(", "config", ".", "FEVER_DB", ")", "\n", "forward_items", "=", "build_full_wiki_document_forward_item", "(", "doc_results_dict", ",", "d_list", ",", "is_training", "=", "is_training", ",", "\n", "db_cursor", "=", "fever_db_cursor", ")", "\n", "return", "forward_items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.set_gt_nli_label": [[174, 180], ["None"], "function", ["None"], ["", "def", "set_gt_nli_label", "(", "d_list", ",", "delete_label", "=", "False", ")", ":", "\n", "    ", "for", "item", "in", "d_list", ":", "\n", "        ", "item", "[", "'predicted_label'", "]", "=", "item", "[", "'label'", "]", "\n", "if", "delete_label", ":", "\n", "            ", "del", "item", "[", "'label'", "]", "\n", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.model_go": [[182, 464], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "bert_v1.set_gt_nli_label", "bert_v1.get_sentences", "bert_v1.get_sentences", "fever_sampler.ss_sampler.down_sample_neg", "len", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_fever_sent_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_fever_sent_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "int", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "fever_sampler.ss_sampler.down_sample_neg", "data_utils.readers.bert_reader_fever_sent_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "print", "allennlp.data.iterators.BasicIterator.", "bert_v1.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "print", "bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print", "bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "pathlib.Path", "pathlib.Path", "len", "any", "print", "allennlp.data.iterators.BasicIterator.", "bert_v1.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "print", "bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print", "bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "any", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.set_gt_nli_label", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.get_sentences", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.get_sentences", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'fever_v0_cs_ratio_001'", "\n", "# lazy = False", "\n", "lazy", "=", "True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "20000", "\n", "pos_ratio", "=", "0.01", "\n", "do_lower_case", "=", "True", "\n", "\n", "# debug_mode = True", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "# train_list = common.load_jsonl(config.FEVER_TRAIN)", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "set_gt_nli_label", "(", "dev_list", ")", "\n", "\n", "# dev_fitems_list = common.load_jsonl(", "\n", "#     config.PDATA_ROOT / \"content_selection_forward\" / \"hotpot_dev_p_level_unlabeled.jsonl\")", "\n", "# train_fitems_list = common.load_jsonl(", "\n", "#     config.PDATA_ROOT / \"content_selection_forward\" / \"hotpot_train_p_level_labeled.jsonl\")", "\n", "\n", "dev_fitems_list", "=", "get_sentences", "(", "'dev'", ",", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ")", "\n", "train_fitems_list", "=", "get_sentences", "(", "'train'", ",", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "50", "]", "\n", "eval_frequency", "=", "1", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "sampled_train_list", "=", "down_sample_neg", "(", "train_fitems_list", ",", "ratio", "=", "pos_ratio", ")", "\n", "est_datasize", "=", "len", "(", "sampled_train_list", ")", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "128", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "#", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# # # Create Log File", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "sampled_train_list", "=", "down_sample_neg", "(", "train_fitems_list", ",", "ratio", "=", "pos_ratio", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "sampled_train_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "print", "(", "\"Threshold 0.5:\"", ")", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "0.5", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_d_list", ",", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", ",", "'check_sent_id_correct'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'as'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "\"Threshold 0.1:\"", ")", "\n", "cur_results_dict_th0_1", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "0.1", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_d_list", ",", "cur_results_dict_th0_1", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", ",", "'check_sent_id_correct'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "score_01", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'as'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'score_01'", ":", "score_01", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s01_ss_score", "=", "score_01", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "#", "\n", "# exit(0)", "\n", "\n", "# print(logging_item)", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|s01({s01_ss_score})|s05({s05_ss_score})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_sent_results.json\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "# print(logging_agent.logging_item_list)", "\n", "\n", "# Epoch eval:", "\n", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "print", "(", "\"Threshold 0.5:\"", ")", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "0.5", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_d_list", ",", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", ",", "'check_sent_id_correct'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'as'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "\"Threshold 0.1:\"", ")", "\n", "cur_results_dict_th0_1", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "0.1", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_d_list", ",", "cur_results_dict_th0_1", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", ",", "'check_sent_id_correct'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "score_01", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'as'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'score_01'", ":", "score_01", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s01_ss_score", "=", "score_01", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "#", "\n", "# exit(0)", "\n", "\n", "# print(logging_item)", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|s01({s01_ss_score})|s05({s05_ss_score})'", "f'|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_sent_results.jsonl\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.eval_trainset_for_train_nli": [[466, 551], ["torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "bert_v1.get_sentences", "len", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_fever_sent_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "print", "print", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.nn.DataParallel.to", "data_utils.readers.bert_reader_fever_sent_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "bert_v1.eval_model", "utils.common.save_jsonl", "torch.cuda.is_available", "torch.cuda.is_available", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "utils.common.load_jsonl", "bert_v1.set_gt_nli_label", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "print", "bert_v1.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list", "evaluation.fever_scorer.fever_score", "print", "torch.cuda.is_available", "torch.cuda.is_available", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.get_sentences", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.bert_v1.set_gt_nli_label", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score"], ["", "", "def", "eval_trainset_for_train_nli", "(", "model_path", ")", ":", "\n", "    ", "tag", "=", "'test'", "\n", "is_training", "=", "False", "\n", "\n", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "# batch_size = 192", "\n", "batch_size", "=", "128", "\n", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "# debug_mode = True", "\n", "\n", "num_class", "=", "1", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "\n", "train_fitems_list", "=", "get_sentences", "(", "tag", ",", "is_training", "=", "is_training", ",", "debug", "=", "debug_mode", ")", "\n", "est_datasize", "=", "len", "(", "train_fitems_list", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "128", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "\n", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Estimated forward steps:\"", ",", "est_datasize", "/", "forward_size", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "train_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "make_int", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "train_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "train_list", "=", "train_list", "[", ":", "50", "]", "\n", "set_gt_nli_label", "(", "train_list", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'id'", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "train_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "train_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'oid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "print", "(", "\"Threshold 0.5:\"", ")", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "top_k", "=", "5", ",", "threshold", "=", "0.1", ")", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list", "(", "copied_dev_d_list", ",", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", ",", "'check_sent_id_correct'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "copied_dev_d_list", ",", "train_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "print", "(", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", ")", "\n", "\n", "", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f'{tag}_sent_results_labeled:{is_training}.jsonl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_p_level_v1.eval_model": [[33, 97], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "torch.sigmoid().view", "model.size", "torch.max", "torch.max", "torch.sigmoid", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_p_level_v1.model_go": [[119, 344], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "fever_sampler.fever_p_level_sampler.down_sample_neg", "len", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "random.shuffle", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "len", "any", "print", "allennlp.data.iterators.BasicIterator.", "fever_bert_p_level_v1.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "any", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "64", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "5000", "\n", "do_lower_case", "=", "True", "\n", "ignore_non_verifiable", "=", "True", "\n", "experiment_name", "=", "f'fever_v0_plevel_retri_(ignore_non_verifiable:{ignore_non_verifiable})'", "\n", "\n", "debug_mode", "=", "False", "\n", "max_l", "=", "264", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_train.jsonl\"", ")", "\n", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_dev.jsonl\"", ")", "\n", "\n", "# train_list = common.load_json(config.TRAIN_FILE)", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "\n", "train_fitems", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'train'", ",", "train_ruleterm_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "True", ")", "\n", "dev_fitems", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'dev'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "\n", "# Just to show the information", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "train_fitems", ",", "None", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "dev_fitems", ",", "None", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "est_datasize", "=", "len", "(", "train_fitems", ")", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "# # # Create Log File", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "random", ".", "shuffle", "(", "train_fitems", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "                        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_p_level_v1.eval_model_for_downstream": [[348, 563], ["torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "fever_sampler.fever_p_level_sampler.down_sample_neg", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "allennlp.data.iterators.BasicIterator.", "fever_bert_p_level_v1.eval_model", "utils.common.save_jsonl", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "torch.cuda.is_available", "torch.cuda.is_available", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "exit", "json.dumps", "torch.cuda.is_available", "torch.cuda.is_available", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "NotImplemented", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair"], ["", "", "", "", "", "", "def", "eval_model_for_downstream", "(", "model_saved_path", ")", ":", "\n", "    ", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "64", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "max_l", "=", "264", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "tag", "=", "'test'", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "# train_ruleterm_doc_results = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"results/doc_retri_results/fever_results/merged_doc_results/m_doc_train.jsonl\")", "\n", "# dev_ruleterm_doc_results = train_ruleterm_doc_results", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_dev.jsonl\"", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "\n", "dev_fitems", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'dev'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_train.jsonl\"", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "\n", "dev_fitems", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'train'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_test.jsonl\"", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "\n", "dev_fitems", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'test'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n", "# dev_fitems = fever_p_level_sampler.get_paragraph_forward_pair('train', dev_ruleterm_doc_results,", "\n", "#                                                               is_training=True, debug=debug_mode,", "\n", "#                                                               ignore_non_verifiable=False)", "\n", "\n", "# Just to show the information", "\n", "", "fever_p_level_sampler", ".", "down_sample_neg", "(", "dev_fitems", ",", "None", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "\n", "", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_p_level_{tag}_results.jsonl\"", ")", "\n", "\n", "if", "tag", "==", "'test'", ":", "\n", "        ", "exit", "(", "0", ")", "\n", "# common.save_jsonl(cur_eval_results_list, \"fever_p_level_train_results_1.jsonl\")", "\n", "\n", "", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_1", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.1", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_1", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_01", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th00_1", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.01", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th00_1", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_001", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th000_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.005", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th000_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_0005", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'score_0005'", ":", "score_0005", ",", "\n", "'score_001'", ":", "score_001", ",", "\n", "'score_01'", ":", "score_01", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "}", "\n", "\n", "print", "(", "json", ".", "dumps", "(", "logging_item", ",", "indent", "=", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_s_level_v1.eval_model": [[33, 97], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "torch.sigmoid().view", "model.size", "torch.max", "torch.max", "torch.sigmoid", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_s_level_v1.model_go": [[119, 381], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "fever_sampler.fever_p_level_sampler.down_sample_neg", "len", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "random.shuffle", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "len", "any", "print", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "print", "any", "json.dumps", "utils.common.save_jsonl", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "3", "\n", "eval_frequency", "=", "5000", "\n", "do_lower_case", "=", "True", "\n", "ignore_non_verifiable", "=", "True", "\n", "doc_filter_value", "=", "0.005", "\n", "doc_top_k", "=", "5", "\n", "experiment_name", "=", "f'fever_v0_slevel_retri_(ignore_non_verifiable:{ignore_non_verifiable})'", "\n", "\n", "debug_mode", "=", "False", "\n", "max_l", "=", "128", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_train_results.jsonl\"", "\n", ")", "\n", "\n", "dev_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_dev_results.jsonl\"", "\n", ")", "\n", "\n", "# train_list = common.load_json(config.TRAIN_FILE)", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "\n", "train_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'train'", ",", "train_upstream_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "ignore_non_verifiable", ",", "\n", "top_k", "=", "doc_top_k", ",", "filter_value", "=", "doc_filter_value", ")", "\n", "\n", "dev_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'dev'", ",", "dev_upstream_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "ignore_non_verifiable", ",", "\n", "top_k", "=", "doc_top_k", ",", "filter_value", "=", "doc_filter_value", ")", "\n", "\n", "# Just to show the information", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "train_fitems", ",", "None", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "dev_fitems", ",", "None", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "est_datasize", "=", "len", "(", "train_fitems", ")", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "# # # Create Log File", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "random", ".", "shuffle", "(", "train_fitems", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_1", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.1", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_1", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_01", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'score_01'", ":", "score_01", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "}", "\n", "\n", "print", "(", "json", ".", "dumps", "(", "logging_item", ",", "indent", "=", "2", ")", ")", "\n", "\n", "s01_ss_score", "=", "score_01", "[", "'ss'", "]", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "                        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v01_ofever({s01_ss_score})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "Path", "(", "file_path_prefix", ")", "/", "\n", "f\"{save_file_name}_dev_s_level_results.jsonl\"", ")", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_s_level_v1.eval_model_for_downstream": [[386, 600], ["torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.cuda.is_available", "torch.cuda.is_available", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "print", "torch.cuda.is_available", "torch.cuda.is_available", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "print", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only"], ["", "", "", "", "", "", "def", "eval_model_for_downstream", "(", "model_saved_path", ")", ":", "\n", "    ", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "# batch_size = 128", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "max_l", "=", "128", "\n", "# est_datasize = 900_000", "\n", "tag", "=", "'dev'", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_train_results.jsonl\"", "\n", ")", "\n", "\n", "dev_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_dev_results.jsonl\"", "\n", ")", "\n", "\n", "test_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_test_results.jsonl\"", "\n", ")", "\n", "\n", "train_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "test_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "# dev_list = common.load_jsonl(config.FEVER_DEV)", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'dev'", ",", "dev_upstream_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.005", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "dev_fitems", ",", "None", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "train_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'train'", ",", "train_upstream_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.005", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "train_fitems", ",", "None", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'test'", ",", "test_upstream_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.005", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "test_fitems", ",", "None", ")", "\n", "\n", "# Just to show the information", "\n", "\n", "", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "test_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "test_list", ",", "'id'", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "\n", "", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results.jsonl\"", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "score_05", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results.jsonl\"", ")", "\n", "\n", "# copied_test_o_dict = copy.deepcopy(test_o_dict)", "\n", "# copied_test_d_list = copy.deepcopy(test_list)", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_test_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_5 = select_top_k_and_to_results_dict(copied_test_o_dict,", "\n", "#                                                           score_field_name='prob',", "\n", "#                                                           top_k=5, filter_value=0.5,", "\n", "#                                                           result_field='predicted_evidence')", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_test_d_list,", "\n", "#                                                                cur_results_dict_th0_5,", "\n", "#                                                                'id', 'predicted_evidence')", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "\n", "\n", "# copied_train_o_dict = copy.deepcopy(train_o_dict)", "\n", "# copied_train_d_list = copy.deepcopy(train_list)", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_train_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_5 = select_top_k_and_to_results_dict(copied_train_o_dict,", "\n", "#                                                           score_field_name='prob',", "\n", "#                                                           top_k=5, filter_value=0.5,", "\n", "#                                                           result_field='predicted_evidence')", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_train_d_list,", "\n", "#                                                                cur_results_dict_th0_5,", "\n", "#                                                                'id', 'predicted_evidence')", "\n", "# # mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "# strict_score, pr, rec, f1 = fever_scorer.fever_sent_only(copied_train_d_list, train_list,", "\n", "#                                                          max_evidence=5)", "\n", "# score_05 = {", "\n", "#     'ss': strict_score,", "\n", "#     'pr': pr, 'rec': rec, 'f1': f1,", "\n", "# }", "\n", "#", "\n", "# print(score_05)", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "train_instances", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "\n", "train_iter", "=", "biterator", "(", "train_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "train_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results.jsonl\"", ")", "\n", "\n", "copied_train_o_dict", "=", "copy", ".", "deepcopy", "(", "train_o_dict", ")", "\n", "copied_train_d_list", "=", "copy", ".", "deepcopy", "(", "train_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_train_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_train_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_train_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_train_d_list", ",", "train_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "score_05", ")", "\n", "# dev_instances = bert_cs_reader.read(dev_fitems)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.sentence_selection.fever_bert_s_level_v1.eval_model_for_downstream_ablation": [[603, 821], ["torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "torch.cuda.is_available", "torch.cuda.is_available", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "print", "print", "utils.common.save_json", "torch.cuda.is_available", "torch.cuda.is_available", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "fever_sampler.fever_p_level_sampler.down_sample_neg", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "fever_bert_s_level_v1.eval_model", "utils.common.save_jsonl", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_sent_only", "print", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only"], ["", "", "def", "eval_model_for_downstream_ablation", "(", "model_saved_path", ",", "top_k_doc", ")", ":", "\n", "    ", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "# batch_size = 128", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "max_l", "=", "128", "\n", "# est_datasize = 900_000", "\n", "tag", "=", "'dev'", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_train_results.jsonl\"", "\n", ")", "\n", "\n", "dev_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_dev_results.jsonl\"", "\n", ")", "\n", "\n", "test_upstream_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/fever_paragraph_level/04-22-15:05:45_fever_v0_plevel_retri_(ignore_non_verifiable:True)/\"", "\n", "\"i(5000)|e(0)|v02_ofever(0.8947894789478947)|v05_ofever(0.8555355535553555)|seed(12)/fever_p_level_test_results.jsonl\"", "\n", ")", "\n", "\n", "train_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "test_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "# dev_list = common.load_jsonl(config.FEVER_DEV)", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'dev'", ",", "dev_upstream_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "top_k_doc", ",", "filter_value", "=", "0.00000", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "dev_fitems", ",", "None", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "train_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'train'", ",", "train_upstream_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "top_k_doc", ",", "filter_value", "=", "0.00000", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "train_fitems", ",", "None", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_fitems", "=", "fever_s_level_sampler", ".", "get_sentence_forward_pair", "(", "'test'", ",", "test_upstream_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "top_k_doc", ",", "filter_value", "=", "0.00000", ")", "\n", "fever_p_level_sampler", ".", "down_sample_neg", "(", "test_fitems", ",", "None", ")", "\n", "\n", "# Just to show the information", "\n", "\n", "", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'id'", ")", "\n", "test_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "test_list", ",", "'id'", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "\n", "", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results_top_k_doc_{top_k_doc}.jsonl\"", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'top_k_doc'", ":", "top_k_doc", ",", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "\"Top_k doc:\"", ",", "top_k_doc", ")", "\n", "print", "(", "score_05", ")", "\n", "common", ".", "save_json", "(", "score_05", ",", "f\"top_k_doc:{top_k_doc}_ss:{strict_score}_pr:{pr}_rec:{rec}_f1:{f1}\"", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results.jsonl\"", ")", "\n", "\n", "# copied_test_o_dict = copy.deepcopy(test_o_dict)", "\n", "# copied_test_d_list = copy.deepcopy(test_list)", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_test_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_5 = select_top_k_and_to_results_dict(copied_test_o_dict,", "\n", "#                                                           score_field_name='prob',", "\n", "#                                                           top_k=5, filter_value=0.5,", "\n", "#                                                           result_field='predicted_evidence')", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_test_d_list,", "\n", "#                                                                cur_results_dict_th0_5,", "\n", "#                                                                'id', 'predicted_evidence')", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "\n", "\n", "# copied_train_o_dict = copy.deepcopy(train_o_dict)", "\n", "# copied_train_d_list = copy.deepcopy(train_list)", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_train_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_5 = select_top_k_and_to_results_dict(copied_train_o_dict,", "\n", "#                                                           score_field_name='prob',", "\n", "#                                                           top_k=5, filter_value=0.5,", "\n", "#                                                           result_field='predicted_evidence')", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_train_d_list,", "\n", "#                                                                cur_results_dict_th0_5,", "\n", "#                                                                'id', 'predicted_evidence')", "\n", "# # mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "# strict_score, pr, rec, f1 = fever_scorer.fever_sent_only(copied_train_d_list, train_list,", "\n", "#                                                          max_evidence=5)", "\n", "# score_05 = {", "\n", "#     'ss': strict_score,", "\n", "#     'pr': pr, 'rec': rec, 'f1': f1,", "\n", "# }", "\n", "#", "\n", "# print(score_05)", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "train_instances", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "\n", "train_iter", "=", "biterator", "(", "train_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "train_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "f\"fever_s_level_{tag}_results.jsonl\"", ")", "\n", "\n", "copied_train_o_dict", "=", "copy", ".", "deepcopy", "(", "train_o_dict", ")", "\n", "copied_train_d_list", "=", "copy", ".", "deepcopy", "(", "train_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_train_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "select_top_k_and_to_results_dict", "(", "copied_train_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_train_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_evidence'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_sent_only", "(", "copied_train_d_list", ",", "train_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "score_05", ")", "\n", "# dev_instances = bert_cs_reader.read(dev_fitems)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_s_level_v1.eval_model": [[32, 96], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "torch.sigmoid().view", "model.size", "torch.max", "torch.max", "torch.sigmoid", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_s_level_v1.model_go": [[118, 396], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "len", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "utils.save_tool.gen_file_prefix", "os.path.basename", "range", "torch.cuda.is_available", "torch.cuda.is_available", "neural_modules.model_EMA.EMA", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "print", "random.shuffle", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "os.path.join", "it.read", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "len", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "bert_s_level_v1.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "any", "hasattr", "updated_model.named_parameters", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "neural_modules.model_EMA.EMA.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "torch.nn.DataParallel", "allennlp.data.iterators.BasicIterator.", "bert_s_level_v1.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "pathlib.Path", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "2000", "\n", "pos_ratio", "=", "0.2", "\n", "do_lower_case", "=", "True", "\n", "document_top_k", "=", "2", "\n", "experiment_name", "=", "f'hotpot_v0_slevel_retri_(doc_top_k:{document_top_k})'", "\n", "\n", "debug_mode", "=", "False", "\n", "do_ema", "=", "True", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "# train_fitems = sentence_level_sampler.get_train_sentence_pair(document_top_k, True, debug_mode)", "\n", "# dev_fitems = sentence_level_sampler.get_dev_sentence_pair(document_top_k, False, debug_mode)", "\n", "\n", "# Load train eval results list", "\n", "cur_train_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/train_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "cur_dev_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "train_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "train_list", ",", "cur_train_eval_results_list", ",", "is_training", "=", "True", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "\n", "dev_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "dev_list", ",", "cur_dev_eval_results_list", ",", "is_training", "=", "False", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "", "est_datasize", "=", "len", "(", "train_fitems", ")", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "128", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "ema", "=", "None", "\n", "if", "do_ema", ":", "\n", "        ", "ema", "=", "EMA", "(", "model", ",", "model", ".", "named_parameters", "(", ")", ",", "device_num", "=", "1", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "        ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "random", ".", "shuffle", "(", "train_fitems", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "if", "ema", "is", "not", "None", "and", "do_ema", ":", "\n", "                    ", "updated_model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "ema", "(", "updated_model", ".", "named_parameters", "(", ")", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# 0.5", "\n", "cur_results_dict_v05", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "cur_results_dict_v02", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "_", ",", "metrics_v5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v05", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_v2", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v02", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "v02_sp_f1", "=", "metrics_v2", "[", "'sp_f1'", "]", "\n", "v02_sp_recall", "=", "metrics_v2", "[", "'sp_recall'", "]", "\n", "v02_sp_prec", "=", "metrics_v2", "[", "'sp_prec'", "]", "\n", "\n", "v05_sp_f1", "=", "metrics_v5", "[", "'sp_f1'", "]", "\n", "v05_sp_recall", "=", "metrics_v5", "[", "'sp_recall'", "]", "\n", "v05_sp_prec", "=", "metrics_v5", "[", "'sp_prec'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'v02'", ":", "metrics_v2", ",", "\n", "'v05'", ":", "metrics_v5", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "# print(logging_item)", "\n", "if", "not", "debug_mode", ":", "\n", "                        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_f1({v02_sp_f1})|v02_recall({v02_sp_recall})'", "f'|v05_f1({v05_sp_f1})|v05_recall({v05_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "                        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "1", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# 0.5", "\n", "cur_results_dict_v05", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "cur_results_dict_v02", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "_", ",", "metrics_v5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v05", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_v2", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v02", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "v02_sp_f1", "=", "metrics_v2", "[", "'sp_f1'", "]", "\n", "v02_sp_recall", "=", "metrics_v2", "[", "'sp_recall'", "]", "\n", "v02_sp_prec", "=", "metrics_v2", "[", "'sp_prec'", "]", "\n", "\n", "v05_sp_f1", "=", "metrics_v5", "[", "'sp_f1'", "]", "\n", "v05_sp_recall", "=", "metrics_v5", "[", "'sp_recall'", "]", "\n", "v05_sp_prec", "=", "metrics_v5", "[", "'sp_prec'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'v02'", ":", "metrics_v2", ",", "\n", "'v05'", ":", "metrics_v5", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "                            ", "save_file_name", "=", "f'ema_i({update_step})|e({epoch_i})'", "f'|v02_f1({v02_sp_f1})|v02_recall({v02_sp_recall})'", "f'|v05_f1({v05_sp_f1})|v05_recall({v05_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_s_level_v1.eval_model_for_downstream": [[398, 548], ["torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "print", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "torch.cuda.is_available", "torch.cuda.is_available", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "exit", "torch.cuda.is_available", "torch.cuda.is_available", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "len", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "len", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "", "", "", "", "", "", "def", "eval_model_for_downstream", "(", "model_saved_path", ",", "doc_top_k", "=", "2", ",", "tag", "=", "'dev'", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "# lazy = False", "\n", "lazy", "=", "True", "\n", "# forward_size = 256", "\n", "forward_size", "=", "128", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "do_lower_case", "=", "True", "\n", "document_top_k", "=", "doc_top_k", "\n", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "test_list", "=", "common", ".", "load_json", "(", "config", ".", "TEST_FULLWIKI_FILE", ")", "\n", "\n", "# Load train eval results list", "\n", "cur_train_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/train_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "cur_dev_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "cur_test_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/test_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "if", "tag", "==", "'train'", ":", "\n", "        ", "train_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "train_list", ",", "cur_train_eval_results_list", ",", "is_training", "=", "True", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "dev_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "dev_list", ",", "cur_dev_eval_results_list", ",", "is_training", "=", "False", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "test_list", ",", "cur_test_eval_results_list", ",", "is_training", "=", "False", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "\n", "", "if", "debug_mode", ":", "\n", "        ", "eval_frequency", "=", "2", "\n", "\n", "#     dev_list = dev_list[:10]", "\n", "#     dev_fitems_list = dev_fitems_list[:296]", "\n", "#     train_fitems_list = train_fitems_list[:300]", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'_id'", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "128", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "if", "tag", "==", "'train'", ":", "\n", "        ", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "if", "tag", "==", "'train'", ":", "\n", "        ", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "train_fitems", ")", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "dev_fitems", ")", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "test_fitems", ")", ")", "\n", "\n", "", "print", "(", "\"Forward size:\"", ",", "forward_size", ")", "\n", "\n", "if", "tag", "==", "'train'", ":", "\n", "        ", "cur_train_eval_results_list_out", "=", "eval_model", "(", "model", ",", "train_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_train_eval_results_list_out", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/train_s_level_bert_v1_results.jsonl\"", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "cur_dev_eval_results_list_out", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_dev_eval_results_list_out", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/dev_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "cur_test_eval_results_list_out", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_test_eval_results_list_out", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/test_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "", "if", "tag", "==", "'train'", "or", "tag", "==", "'test'", ":", "\n", "        ", "exit", "(", "0", ")", "\n", "\n", "", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_dev_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# 0.5", "\n", "cur_results_dict_v05", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "cur_results_dict_v02", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "_", ",", "metrics_v5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v05", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_v2", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v02", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "logging_item", "=", "{", "\n", "'v02'", ":", "metrics_v2", ",", "\n", "'v05'", ":", "metrics_v5", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_s_level_v1.eval_model_for_downstream_ablation": [[550, 709], ["print", "torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "print", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "utils.common.save_json", "torch.cuda.is_available", "torch.cuda.is_available", "hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "exit", "torch.cuda.is_available", "torch.cuda.is_available", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "len", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "len", "allennlp.data.iterators.BasicIterator.", "print", "bert_s_level_v1.eval_model", "utils.common.save_jsonl", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "def", "eval_model_for_downstream_ablation", "(", "model_saved_path", ",", "doc_top_k", "=", "2", ",", "tag", "=", "'dev'", ")", ":", "\n", "    ", "print", "(", "f\"Run doc_top_k:{doc_top_k}\"", ")", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "# lazy = False", "\n", "lazy", "=", "True", "\n", "# forward_size = 256", "\n", "forward_size", "=", "256", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "do_lower_case", "=", "True", "\n", "document_top_k", "=", "doc_top_k", "\n", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "test_list", "=", "common", ".", "load_json", "(", "config", ".", "TEST_FULLWIKI_FILE", ")", "\n", "\n", "# Load train eval results list", "\n", "# cur_train_eval_results_list = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "#                       \"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/train_p_level_bert_v1_results.jsonl\")", "\n", "\n", "cur_dev_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "# cur_test_eval_results_list = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "#                       \"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/test_p_level_bert_v1_results.jsonl\")", "\n", "\n", "# if tag == 'train':", "\n", "#     train_fitems = get_sentence_pair(document_top_k, train_list, cur_train_eval_results_list, is_training=True,", "\n", "#                                      debug_mode=debug_mode)", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_fitems", "=", "get_sentence_pair", "(", "document_top_k", ",", "dev_list", ",", "cur_dev_eval_results_list", ",", "is_training", "=", "False", ",", "\n", "debug_mode", "=", "debug_mode", ")", "\n", "\n", "# elif tag == 'test':", "\n", "#     test_fitems = get_sentence_pair(document_top_k, test_list, cur_test_eval_results_list, is_training=False,", "\n", "#                                     debug_mode=debug_mode)", "\n", "\n", "", "if", "debug_mode", ":", "\n", "        ", "eval_frequency", "=", "2", "\n", "\n", "#     dev_list = dev_list[:10]", "\n", "#     dev_fitems_list = dev_fitems_list[:296]", "\n", "#     train_fitems_list = train_fitems_list[:300]", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'_id'", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "128", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "if", "tag", "==", "'train'", ":", "\n", "        ", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems", ")", "\n", "\n", "", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "if", "tag", "==", "'train'", ":", "\n", "        ", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "train_fitems", ")", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "dev_fitems", ")", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "len", "(", "test_fitems", ")", ")", "\n", "\n", "", "print", "(", "\"Forward size:\"", ",", "forward_size", ")", "\n", "\n", "if", "tag", "==", "'train'", ":", "\n", "        ", "cur_train_eval_results_list_out", "=", "eval_model", "(", "model", ",", "train_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_train_eval_results_list_out", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/train_s_level_bert_v1_results.jsonl\"", ")", "\n", "", "elif", "tag", "==", "'dev'", ":", "\n", "        ", "cur_dev_eval_results_list_out", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_dev_eval_results_list_out", ",", "f\"hotpot_s_level_{tag}_results_top_k_doc_{document_top_k}.jsonl\"", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "cur_test_eval_results_list_out", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "\n", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_test_eval_results_list_out", ",", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/test_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "", "if", "tag", "==", "'train'", "or", "tag", "==", "'test'", ":", "\n", "        ", "exit", "(", "0", ")", "\n", "\n", "", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_dev_eval_results_list_out", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# 0.5", "\n", "cur_results_dict_v05", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.5", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "cur_results_dict_v02", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "_", ",", "metrics_v5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v05", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_v2", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v02", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "logging_item", "=", "{", "\n", "'v02'", ":", "metrics_v2", ",", "\n", "'v05'", ":", "metrics_v5", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "f1", "=", "metrics_v5", "[", "'sp_f1'", "]", "\n", "em", "=", "metrics_v5", "[", "'sp_em'", "]", "\n", "pr", "=", "metrics_v5", "[", "'sp_prec'", "]", "\n", "rec", "=", "metrics_v5", "[", "'sp_recall'", "]", "\n", "common", ".", "save_json", "(", "logging_item", ",", "f\"top_k_doc:{document_top_k}_em:{em}_pr:{pr}_rec:{rec}_f1:{f1}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_v0.convert_sent_list_to_prediction_file": [[32, 46], ["dict", "sid.split", "int", "[].append"], "function", ["None"], ["def", "convert_sent_list_to_prediction_file", "(", "sent_list", ")", ":", "\n", "    ", "pred_dict", "=", "{", "'sp'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "sent_item", "in", "sent_list", ":", "\n", "        ", "sid", "=", "sent_item", "[", "'selection_id'", "]", "\n", "oid", ",", "title", ",", "sent_num", "=", "sid", ".", "split", "(", "ID_SEPARATOR", ")", "\n", "sent_num", "=", "int", "(", "sent_num", ")", "\n", "# Change this to other later", "\n", "if", "oid", "not", "in", "pred_dict", "[", "'sp'", "]", ":", "\n", "            ", "pred_dict", "[", "'sp'", "]", "[", "oid", "]", "=", "[", "]", "\n", "", "if", "sent_item", "[", "'pred_label'", "]", "==", "'true'", ":", "\n", "            ", "pred_dict", "[", "'sp'", "]", "[", "oid", "]", ".", "append", "(", "[", "title", ",", "sent_num", "]", ")", "\n", "\n", "", "", "return", "pred_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_v0.eval_model": [[48, 94], ["model.eval", "print", "zip", "print", "enumerate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "len", "len", "len", "flint.torch_util.get_length_and_mask", "eval_paired_sequence.to.to", "eval_paired_segments_ids.to.to", "eval_labels_ids.to.to", "eval_att_mask.to.to", "model", "model.size", "output_pred_list.extend", "output_logits_list.extend", "output_id_list.extend", "y_pred_list.extend", "datetime.datetime.now", "[].view().tolist", "model.tolist", "list", "eval_labels_ids.to.tolist", "datetime.datetime.now", "[].view", "model.size", "torch.max", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "eval_iter", ",", "device", ",", "data_item_list", ")", ":", "\n", "    ", "output_logits_list", "=", "[", "]", "\n", "output_id_list", "=", "[", "]", "\n", "output_pred_list", "=", "[", "]", "\n", "y_pred_list", "=", "[", "]", "\n", "total_size", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "print", "(", "f\"Start Eval ({datetime.datetime.now()}):\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", ",", "batch", "in", "enumerate", "(", "eval_iter", ")", ":", "\n", "            ", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "\n", "eval_paired_sequence", "=", "eval_paired_sequence", ".", "to", "(", "device", ")", "\n", "eval_paired_segments_ids", "=", "eval_paired_segments_ids", ".", "to", "(", "device", ")", "\n", "eval_labels_ids", "=", "eval_labels_ids", ".", "to", "(", "device", ")", "\n", "eval_att_mask", "=", "eval_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "eval_logits", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "\n", "attention_mask", "=", "eval_att_mask", ",", "\n", "labels", "=", "None", ")", "\n", "total_size", "+=", "eval_logits", ".", "size", "(", "0", ")", "\n", "\n", "output_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "eval_logits", ",", "1", ")", "[", "1", "]", ".", "view", "(", "eval_logits", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "output_logits_list", ".", "extend", "(", "eval_logits", ".", "tolist", "(", ")", ")", "\n", "output_id_list", ".", "extend", "(", "list", "(", "batch", "[", "'pid'", "]", ")", ")", "\n", "y_pred_list", ".", "extend", "(", "eval_labels_ids", ".", "tolist", "(", ")", ")", "\n", "\n", "", "", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "output_pred_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "data_item_list", ")", "\n", "correct", "=", "0", "\n", "for", "pred", ",", "y", "in", "zip", "(", "output_pred_list", ",", "y_pred_list", ")", ":", "\n", "        ", "if", "pred", "==", "y", ":", "\n", "            ", "correct", "+=", "1", "\n", "", "", "print", "(", "f\"Finish Eval ({datetime.datetime.now()}):\"", ")", "\n", "# print(correct, total_size, correct / total_size)", "\n", "\n", "for", "i", ",", "item", "in", "enumerate", "(", "data_item_list", ")", ":", "\n", "        ", "assert", "item", "[", "'selection_id'", "]", "==", "output_id_list", "[", "i", "]", "\n", "item", "[", "'pred_label'", "]", "=", "id2label", "[", "output_pred_list", "[", "i", "]", "]", "\n", "# item", "\n", "\n", "", "return", "data_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_v0.model_go": [[96, 309], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_list", "hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_list", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_sent_selection.BertReaderSentM", "data_utils.readers.bert_reader_sent_selection.BertReaderSentM.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "pytorch_pretrained_bert.BertForSequenceClassification.from_pretrained", "nn.DataParallel.to", "list", "print", "pytorch_pretrained_bert.BertAdam", "utils.save_tool.gen_file_prefix", "os.path.basename", "nn.DataParallel.train", "range", "neural_modules.model_EMA.EMA", "copy.deepcopy", "nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "torch.DataParallel", "random.shuffle", "hotpot_fact_selection_sampler.sampler_from_distractor.downsample_negative_examples", "print", "data_utils.readers.bert_reader_sent_selection.BertReaderSentM.read", "allennlp.data.iterators.BasicIterator.", "enumerate", "print", "allennlp.data.iterators.BasicIterator.", "print", "bert_v0.convert_sent_list_to_prediction_file", "evaluation.ext_hotpot_eval.eval", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nn.DataParallel.named_parameters", "os.path.join", "it.read", "len", "tqdm.tqdm", "flint.torch_util.get_length_and_mask", "paired_sequence.to.to", "paired_segments_ids.to.to", "labels_ids.to.to", "att_mask.to.to", "nn.DataParallel.", "loss.mean.backward", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_v0.eval_model", "bert_v0.eval_model", "hasattr", "model_to_save.state_dict", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "torch.DataParallel", "any", "neural_modules.model_EMA.EMA.", "print", "allennlp.data.iterators.BasicIterator.", "bert_v0.convert_sent_list_to_prediction_file", "evaluation.ext_hotpot_eval.eval", "any", "hasattr", "model_to_track.named_parameters", "print", "neural_modules.model_EMA.EMA.load_ema_to_model", "nn.DataParallel.to", "bert_v0.eval_model", "bert_v0.eval_model", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "print", "os.path.join", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.DataParallel", "hasattr", "model_to_save.state_dict", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.downsample_negative_examples", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_v0.convert_sent_list_to_prediction_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_v0.convert_sent_list_to_prediction_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'bert_v0_ss'", "\n", "lazy", "=", "True", "\n", "forward_size", "=", "16", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "\n", "do_ema", "=", "False", "\n", "debug_mode", "=", "False", "\n", "\n", "do_lower_case", "=", "True", "\n", "\n", "# est_datasize = 650_000", "\n", "est_datasize", "=", "900_000", "\n", "\n", "num_class", "=", "2", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_DISTRACTOR_FILE", ")", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "\n", "", "train_sent_data_list", "=", "build_sent_match_data_from_distractor_list", "(", "train_list", ",", "is_training", "=", "True", ")", "\n", "dev_sent_data_list", "=", "build_sent_match_data_from_distractor_list", "(", "dev_list", ",", "is_training", "=", "False", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "bert_mnli_reader", "=", "BertReaderSentM", "(", "bert_tokenizer", ",", "lazy", "=", "lazy", ")", "\n", "\n", "dev_instances", "=", "bert_mnli_reader", ".", "read", "(", "dev_sent_data_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# print(list(mnli_dev_instances))", "\n", "\n", "# Load training model", "\n", "model_clf", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "bert_model_name", ",", "num_labels", "=", "num_class", ")", "\n", "\n", "ema_tracker", "=", "None", "\n", "ema_model_copy", "=", "None", "\n", "if", "do_ema", "and", "ema_tracker", "is", "None", ":", "\n", "        ", "ema_tracker", "=", "EMA", "(", "model_clf", ".", "named_parameters", "(", ")", ",", "on_cpu", "=", "True", ")", "\n", "ema_model_copy", "=", "copy", ".", "deepcopy", "(", "model_clf", ")", "\n", "\n", "", "model_clf", ".", "to", "(", "device", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model_clf", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# optimizer = optim.Adam(optimizer_grouped_parameters, lr=learning_rate)", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "        ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "model_clf", ".", "train", "(", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model_clf", "=", "nn", ".", "DataParallel", "(", "model_clf", ")", "\n", "\n", "", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "eval_iter_num", "=", "50_000", "# Change this to real evaluation.", "\n", "best_f1", "=", "0", "\n", "best_em", "=", "0", "\n", "\n", "for", "n_epoch", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "train_sent_data_list", ")", "\n", "sampled_train_sent_data_list", "=", "downsample_negative_examples", "(", "train_sent_data_list", ",", "0.1", ",", "1", ")", "\n", "print", "(", "\"Current Sample Size:\"", ",", "len", "(", "sampled_train_sent_data_list", ")", ")", "\n", "train_instances", "=", "bert_mnli_reader", ".", "read", "(", "sampled_train_sent_data_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iter", ")", ")", ":", "\n", "            ", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "\n", "paired_sequence", "=", "paired_sequence", ".", "to", "(", "device", ")", "\n", "paired_segments_ids", "=", "paired_segments_ids", ".", "to", "(", "device", ")", "\n", "labels_ids", "=", "labels_ids", ".", "to", "(", "device", ")", "\n", "att_mask", "=", "att_mask", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model_clf", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "if", "do_ema", "and", "ema_tracker", "is", "not", "None", ":", "\n", "# if model_clf is DataParallel, then we use model_clf.module", "\n", "                    ", "model_to_track", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "ema_tracker", "(", "model_to_track", ".", "named_parameters", "(", ")", ")", "# Whenever we do update, the do ema update", "\n", "\n", "", "if", "update_step", "%", "eval_iter_num", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "                        ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                            ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_sent_data_list", "=", "eval_model", "(", "ema_model_copy", ",", "dev_iter", ",", "device", ",", "dev_sent_data_list", ")", "\n", "", "else", ":", "\n", "                        ", "dev_sent_data_list", "=", "eval_model", "(", "model_clf", ",", "dev_iter", ",", "device", ",", "dev_sent_data_list", ")", "\n", "\n", "", "sp_pred_format", "=", "convert_sent_list_to_prediction_file", "(", "dev_sent_data_list", ")", "\n", "score_tracker", ",", "metrics", "=", "ext_hotpot_eval", ".", "eval", "(", "sp_pred_format", ",", "dev_list", ")", "\n", "sp_em", ",", "sp_f1", ",", "sp_prec", ",", "sp_recall", "=", "metrics", "[", "'sp_em'", "]", ",", "metrics", "[", "'sp_f1'", "]", ",", "metrics", "[", "'sp_prec'", "]", ",", "metrics", "[", "\n", "'sp_recall'", "]", "\n", "\n", "if", "best_f1", "<", "sp_f1", ":", "\n", "                        ", "print", "(", "\"New Best F1\"", ")", "\n", "best_f1", "=", "sp_f1", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'({update_step})_epoch({n_epoch})_sp_em({sp_em})_sp_f1({sp_f1})_p({sp_prec})_r({sp_recall})_seed({seed})'", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "if", "best_em", "<", "sp_em", ":", "\n", "                        ", "print", "(", "\"New Best F1\"", ")", "\n", "best_em", "=", "sp_em", "\n", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'({update_step})_epoch({n_epoch})_sp_em({sp_em})_sp_f1({sp_f1})_p({sp_prec})_r({sp_recall})_seed({seed})'", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "", "", "", "", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "print", "(", "\"Epoch Evaluation\"", ")", "\n", "if", "do_ema", "and", "ema_model_copy", "is", "not", "None", "and", "ema_tracker", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"EMA evaluation.\"", ")", "\n", "EMA", ".", "load_ema_to_model", "(", "ema_model_copy", ",", "ema_tracker", ")", "\n", "ema_model_copy", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "ema_model_copy", "=", "nn", ".", "DataParallel", "(", "ema_model_copy", ")", "\n", "", "dev_sent_data_list", "=", "eval_model", "(", "ema_model_copy", ",", "dev_iter", ",", "device", ",", "dev_sent_data_list", ")", "\n", "", "else", ":", "\n", "            ", "dev_sent_data_list", "=", "eval_model", "(", "model_clf", ",", "dev_iter", ",", "device", ",", "dev_sent_data_list", ")", "\n", "\n", "", "sp_pred_format", "=", "convert_sent_list_to_prediction_file", "(", "dev_sent_data_list", ")", "\n", "score_tracker", ",", "metrics", "=", "ext_hotpot_eval", ".", "eval", "(", "sp_pred_format", ",", "dev_list", ")", "\n", "sp_em", ",", "sp_f1", ",", "sp_prec", ",", "sp_recall", "=", "metrics", "[", "'sp_em'", "]", ",", "metrics", "[", "'sp_f1'", "]", ",", "metrics", "[", "'sp_prec'", "]", ",", "metrics", "[", "\n", "'sp_recall'", "]", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "file_path_prefix", ",", "\n", "f'({update_step})_epoch({n_epoch})_sp_em({sp_em})_sp_f1({sp_f1})_p({sp_prec})_r({sp_recall})_seed({seed})'", ")", "\n", "model_to_save", "=", "model_clf", ".", "module", "if", "hasattr", "(", "model_clf", ",", "\n", "'module'", ")", "else", "model_clf", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "save_path", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertContent2Vec.__init__": [[21, 25], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_encoder", ",", "num_of_out_layers", "=", "4", ")", ":", "\n", "        ", "super", "(", "BertContent2Vec", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "self", ".", "num_of_out_layers", "=", "num_of_out_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertContent2Vec.forward": [[30, 45], ["bert_para2vec_rank.BertContent2Vec.bert_encoder", "attention_mask.sum", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "output_layer_list.append", "flint.max_along_time", "flint.max_along_time"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.max_along_time", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.max_along_time"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ")", ":", "\n", "# Precomputing of the max_context_length is important", "\n", "# because we want the same value to be shared to different GPUs, dynamic calculating is not feasible.", "\n", "        ", "encoded_layers", ",", "pooled_output", "=", "self", ".", "bert_encoder", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ")", "\n", "selected_output_layers", "=", "encoded_layers", "[", "-", "self", ".", "num_of_out_layers", ":", "]", "\n", "context_length", "=", "attention_mask", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "output_layer_list", "=", "[", "]", "\n", "for", "i", ",", "output_layer", "in", "enumerate", "(", "selected_output_layers", ")", ":", "\n", "            ", "output_layer_list", ".", "append", "(", "torch_util", ".", "max_along_time", "(", "output_layer", ",", "context_length", ")", ")", "# [B, T, D] -> [B, D]", "\n", "\n", "", "packed_output", "=", "torch", ".", "cat", "(", "output_layer_list", ",", "dim", "=", "1", ")", "\n", "\n", "return", "packed_output", "\n", "# context_mask = allen_util.get_mask_from_sequence_lengths(context_length, max_context_length)", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertSupervisedVecMatcher.__init__": [[56, 71], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["", "def", "__init__", "(", "self", ",", "bert_content2vec_model", ",", "dropout_rate", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertSupervisedVecMatcher", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_content2vec_model", "=", "bert_content2vec_model", "\n", "\n", "self", ".", "matching_hidden_dimension", "=", "4", "*", "self", ".", "bert_content2vec_model", ".", "num_of_out_layers", "*", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "hidden_size", "\n", "self", ".", "matching_intermediate_dimension", "=", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "intermediate_size", "\n", "if", "dropout_rate", "is", "None", ":", "\n", "            ", "dropout_rate", "=", "self", ".", "bert_content2vec_model", ".", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", "\n", "\n", "", "self", ".", "matching_layer1", "=", "nn", ".", "Linear", "(", "self", ".", "matching_hidden_dimension", ",", "self", ".", "matching_intermediate_dimension", ")", "\n", "self", ".", "matching_layer2", "=", "nn", ".", "Linear", "(", "self", ".", "matching_intermediate_dimension", ",", "1", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "self", ".", "match_layers", "=", "nn", ".", "Sequential", "(", "*", "[", "self", ".", "matching_layer1", ",", "nn", ".", "ReLU", "(", ")", ",", "self", ".", "dropout", ",", "self", ".", "matching_layer2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertSupervisedVecMatcher.forward": [[72, 91], ["bert_para2vec_rank.BertSupervisedVecMatcher.bert_content2vec_model", "bert_para2vec_rank.BertSupervisedVecMatcher.bert_content2vec_model", "bert_para2vec_rank.BertSupervisedVecMatcher.dropout", "bert_para2vec_rank.BertSupervisedVecMatcher.dropout", "bert_para2vec_rank.BertSupervisedVecMatcher.match_layers", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss.", "labels.unsqueeze().float", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "labels.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "s1_seq", ",", "s1_mask", ",", "s2_seq", ",", "s2_mask", ",", "mode", ",", "labels", "=", "None", ")", ":", "\n", "        ", "s1_out", "=", "self", ".", "bert_content2vec_model", "(", "s1_seq", ",", "attention_mask", "=", "s1_mask", ")", "\n", "s2_out", "=", "self", ".", "bert_content2vec_model", "(", "s2_seq", ",", "attention_mask", "=", "s2_mask", ")", "\n", "\n", "s1_out", "=", "self", ".", "dropout", "(", "s1_out", ")", "\n", "s2_out", "=", "self", ".", "dropout", "(", "s2_out", ")", "\n", "\n", "logits", "=", "self", ".", "match_layers", "(", "torch", ".", "cat", "(", "[", "s1_out", ",", "s2_out", ",", "torch", ".", "abs", "(", "s1_out", "-", "s2_out", ")", ",", "s1_out", "*", "s2_out", "]", ",", "dim", "=", "1", ")", ")", "\n", "\n", "if", "mode", "==", "BertSupervisedVecMatcher", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "assert", "labels", "is", "not", "None", "\n", "loss_fn", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "# batch_size = logits.size(0)", "\n", "# labels_logits = logits.new_zeros(batch_size, 2)", "\n", "# labels_logits.scatter_(1, labels.unsqueeze(-1), 1)", "\n", "loss", "=", "loss_fn", "(", "logits", ",", "labels", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertVecMatcher.__init__": [[95, 98], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_content2vec_model", ")", ":", "\n", "        ", "super", "(", "BertVecMatcher", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_content2vec_model", "=", "bert_content2vec_model", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_para2vec_rank.BertVecMatcher.forward": [[99, 108], ["bert_para2vec_rank.BertVecMatcher.bert_content2vec_model", "bert_para2vec_rank.BertVecMatcher.bert_content2vec_model", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "s1_seq", ",", "s1_mask", ",", "s2_seq", ",", "s2_mask", ")", ":", "\n", "        ", "s1_out", "=", "self", ".", "bert_content2vec_model", "(", "s1_seq", ",", "attention_mask", "=", "s1_mask", ")", "\n", "s2_out", "=", "self", ".", "bert_content2vec_model", "(", "s2_seq", ",", "attention_mask", "=", "s2_mask", ")", "\n", "cosine_simi_score", "=", "F", ".", "cosine_similarity", "(", "s1_out", ",", "s2_out", ")", "\n", "# batch_size = s1_out.size(0)", "\n", "# hidden_size = s1_out.size(1)", "\n", "# m_scores = torch.bmm(s1_out.view(batch_size, 1, hidden_size), s2_out.view(batch_size, hidden_size, 1))", "\n", "# return m_scores", "\n", "return", "cosine_simi_score", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_p_level_v1.eval_model": [[27, 91], ["print", "range", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "torch.sigmoid().view", "model.size", "torch.max", "torch.max", "torch.sigmoid", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_p_level_v1.select_top_k_and_to_results_dict": [[93, 110], ["scored_dict.items", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append"], "function", ["None"], ["", "def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ")", ":", "\n", "    ", "results_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'scored_results'", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "'scored_results'", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "results_dict", "[", "'sp_doc'", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "\n", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_p_level_v1.model_go": [[112, 311], ["torch.manual_seed", "torch.manual_seed", "int", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "len", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "utils.save_tool.gen_file_prefix", "os.path.basename", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "open", "open", "out_f.write", "out_f.flush", "print", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "os.path.join", "it.read", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "len", "any", "print", "allennlp.data.iterators.BasicIterator.", "bert_p_level_v1.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "bert_p_level_v1.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "bert_p_level_v1.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "any", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "experiment_name", "=", "'hotpot_v0_cs'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "16", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "5000", "\n", "pos_ratio", "=", "0.2", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "dev_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_dev_p_level_unlabeled.jsonl\"", ")", "\n", "train_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_train_p_level_labeled.jsonl\"", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "10", "]", "\n", "dev_fitems_list", "=", "dev_fitems_list", "[", ":", "296", "]", "\n", "train_fitems_list", "=", "train_fitems_list", "[", ":", "300", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "sampled_train_list", "=", "down_sample_neg", "(", "train_fitems_list", ",", "ratio", "=", "pos_ratio", ")", "\n", "est_datasize", "=", "len", "(", "sampled_train_list", ")", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "# print(dev_o_dict)", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "286", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "        ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "sampled_train_list", "=", "down_sample_neg", "(", "train_fitems_list", ",", "ratio", "=", "pos_ratio", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "sampled_train_list", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "# top5_doc_f1, top5_UB_sp_f1, top10_doc_f1, top10_Ub_sp_f1", "\n", "# top5_doc_f1 = metrics_top5['doc_f1']", "\n", "# top5_UB_sp_f1 = metrics_top5_UB['sp_f1']", "\n", "# top10_doc_f1 = metrics_top10['doc_f1']", "\n", "# top10_Ub_sp_f1 = metrics_top10_UB['sp_f1']", "\n", "\n", "top5_doc_recall", "=", "metrics_top5", "[", "'doc_recall'", "]", "\n", "top5_UB_sp_recall", "=", "metrics_top5_UB", "[", "'sp_recall'", "]", "\n", "top10_doc_recall", "=", "metrics_top10", "[", "'doc_recall'", "]", "\n", "top10_Ub_sp_recall", "=", "metrics_top10_UB", "[", "'sp_recall'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "}", "\n", "\n", "# print(logging_item)", "\n", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|t5_doc_recall({top5_doc_recall})|t5_sp_recall({top5_UB_sp_recall})'", "f'|t10_doc_recall({top10_doc_recall})|t5_sp_recall({top10_Ub_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_content_selection.bert_p_level_v1.eval_model_for_downstream": [[315, 447], ["torch.manual_seed", "torch.manual_seed", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "allennlp.data.iterators.BasicIterator.", "print", "print", "print", "bert_p_level_v1.eval_model", "utils.common.save_jsonl", "print", "exit", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "bert_p_level_v1.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "bert_p_level_v1.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "bert_p_level_v1.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "utils.common.save_jsonl", "utils.common.save_jsonl", "torch.cuda.is_available", "torch.cuda.is_available", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "len", "len", "len", "torch.cuda.is_available", "torch.cuda.is_available", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "", "", "", "", "def", "eval_model_for_downstream", "(", "model_saved_path", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "# lazy = False", "\n", "lazy", "=", "True", "\n", "forward_size", "=", "32", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug_mode", "=", "False", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Dataset", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "dev_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_dev_p_level_unlabeled.jsonl\"", ")", "\n", "train_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_train_p_level_labeled.jsonl\"", ")", "\n", "test_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_test_p_level_unlabeled.jsonl\"", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "10", "]", "\n", "dev_fitems_list", "=", "dev_fitems_list", "[", ":", "296", "]", "\n", "train_fitems_list", "=", "train_fitems_list", "[", ":", "300", "]", "\n", "eval_frequency", "=", "2", "\n", "# print(dev_list[-1]['_id'])", "\n", "# exit(0)", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "train_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "train_list", ",", "'_id'", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "286", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_saved_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "dev_instances", "=", "bert_cs_reader", ".", "read", "(", "dev_fitems_list", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "train_fitems_list", ")", "\n", "test_instances", "=", "bert_cs_reader", ".", "read", "(", "test_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# train_iter = biterator(train_instance, num_epochs=1, shuffle=False)", "\n", "# dev_iter = biterator(dev_instances, num_epochs=1, shuffle=False)", "\n", "test_iter", "=", "biterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "print", "(", "len", "(", "dev_fitems_list", ")", ")", "\n", "print", "(", "len", "(", "test_fitems_list", ")", ")", "\n", "print", "(", "len", "(", "train_fitems_list", ")", ")", "\n", "\n", "# cur_dev_eval_results_list = eval_model(model, dev_iter, device_num, with_probs=True, show_progress=True)", "\n", "# cur_train_eval_results_list = eval_model(model, train_iter, device_num, with_probs=True, show_progress=True)", "\n", "\n", "cur_test_eval_results_list", "=", "eval_model", "(", "model", ",", "test_iter", ",", "device_num", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "common", ".", "save_jsonl", "(", "cur_test_eval_results_list", ",", "\"test_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "print", "(", "\"Test write finished.\"", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_dev_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_3", "\n", "cur_results_dict_top3", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "3", ")", "\n", "upperbound_results_dict_top3", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top3", ",", "\n", "dev_list", ")", "\n", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top3", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top3", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top3_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top3", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "logging_item", "=", "{", "\n", "'top3'", ":", "metrics_top3", ",", "\n", "'top3_UB'", ":", "metrics_top3_UB", ",", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_train_eval_results_list", ",", "\"train_p_level_bert_v1_results.jsonl\"", ")", "\n", "common", ".", "save_jsonl", "(", "cur_dev_eval_results_list", ",", "\"dev_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_processing": [[44, 65], ["common_utils.is_whitespace", "char_to_word_offset.append", "doc_tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.is_whitespace"], ["def", "w_processing", "(", "context", ")", ":", "\n", "# This is the method of white-space tokenization.", "\n", "# We process the context to get w_tokens and char_to_word_offset", "\n", "# char_to_word_offset have length equals the total number of characters in the context: each element indicate the", "\n", "# position index of the word (in the w-tokens) to which the current character belongs to.", "\n", "    ", "paragraph_text", "=", "context", "\n", "doc_tokens", "=", "[", "]", "# Here doc_tokens indicate w-tokens", "\n", "char_to_word_offset", "=", "[", "]", "\n", "prev_is_whitespace", "=", "True", "\n", "for", "c", "in", "paragraph_text", ":", "\n", "        ", "if", "is_whitespace", "(", "c", ")", ":", "\n", "            ", "prev_is_whitespace", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "prev_is_whitespace", ":", "\n", "                ", "doc_tokens", ".", "append", "(", "c", ")", "\n", "", "else", ":", "\n", "                ", "doc_tokens", "[", "-", "1", "]", "+=", "c", "\n", "", "prev_is_whitespace", "=", "False", "\n", "", "char_to_word_offset", ".", "append", "(", "len", "(", "doc_tokens", ")", "-", "1", ")", "\n", "\n", "", "return", "doc_tokens", ",", "char_to_word_offset", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.pair_w_tokens_with_ground_truth_span": [[67, 133], ["len", "common_utils.whitespace_tokenize", "logger.warning", "logger.warning", "actual_text.find"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.whitespace_tokenize"], ["", "def", "pair_w_tokens_with_ground_truth_span", "(", "w_tokens", ",", "w_char_to_word_offset", ",", "ground_truth_answer_text", ",", "span_offset_start", ",", "\n", "no_answer", "=", "False", ",", "is_yes_no_question", "=", "False", ",", "is_training", "=", "False", ",", "\n", "do_checking", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param w_tokens:                    The white-space tokenzied input context.\n    :param w_char_to_word_offset:       Total length is the character length of the context.\n                                        Each element is the context char to word index in w-tokens.\n    :param ground_truth_answer_text:    The GT answer span.\n    :param span_offset_start:           The GT span start in the original context.\n    :param no_answer:                   If the question is non-answerable.\n    :param is_yes_no_question:\n    :param is_training:\n    :return:\n    w_token: Same as input,\n    start_position: The start position of the answer span in w.r.t. w-tokens. (Inclusive)\n    end_position:   The end position of the answer span in w.r.t. w-tokens. (Inclusive)\n    \"\"\"", "\n", "# Here we just try to pair the w_tokens with the ground truth span", "\n", "# The output will be the w-tokens and adjusted start and end position of the ground truth span.", "\n", "# If we could not find the answer, we will return None.", "\n", "\n", "# If the answer is non-answerable, give the ground_truth_span = None", "\n", "doc_tokens", "=", "w_tokens", "\n", "char_to_word_offset", "=", "w_char_to_word_offset", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "orig_answer_text", "=", "ground_truth_answer_text", "\n", "\n", "if", "no_answer", "or", "is_yes_no_question", ":", "\n", "# Non-answerable question", "\n", "        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "elif", "orig_answer_text", "is", "not", "None", "and", "not", "no_answer", "and", "not", "is_yes_no_question", "and", "is_training", ":", "\n", "# if the orig_answer_text is None, which means the span is None or Non-answerable", "\n", "        ", "answer_offset", "=", "span_offset_start", "\n", "answer_length", "=", "len", "(", "orig_answer_text", ")", "\n", "start_position", "=", "char_to_word_offset", "[", "answer_offset", "]", "\n", "end_position", "=", "char_to_word_offset", "[", "answer_offset", "+", "answer_length", "-", "1", "]", "\n", "# Only add answers where the text can be exactly recovered from the", "\n", "# document. If this CAN'T happen it's likely due to weird Unicode", "\n", "# stuff so we will just skip the example.", "\n", "#", "\n", "# Note that this means for training mode, every example is NOT", "\n", "# guaranteed to be preserved.", "\n", "actual_text", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "start_position", ":", "(", "end_position", "+", "1", ")", "]", ")", "\n", "cleaned_answer_text", "=", "\" \"", ".", "join", "(", "\n", "whitespace_tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "if", "actual_text", ".", "find", "(", "cleaned_answer_text", ")", "==", "-", "1", "and", "is_training", ":", "\n", "            ", "logger", ".", "warning", "(", "\"Can not find answer span from whitespace-tokenized context, there might be a problem\"", ")", "\n", "logger", ".", "warning", "(", "\"Could not find answer: '%s' vs. '%s'\"", ",", "\n", "actual_text", ",", "cleaned_answer_text", ")", "\n", "# If we could not find answer, we return None", "\n", "if", "do_checking", ":", "\n", "                ", "return", "None", ",", "None", ",", "None", "\n", "", "else", ":", "\n", "                ", "return", "doc_tokens", ",", "start_position", ",", "end_position", "\n", "", "", "", "else", ":", "\n", "# Non-answerable question", "\n", "        ", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "orig_answer_text", "=", "\"\"", "\n", "\n", "", "return", "doc_tokens", ",", "start_position", ",", "end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.is_whitespace": [[135, 140], ["ord"], "function", ["None"], ["", "def", "is_whitespace", "(", "c", ")", ":", "\n", "    ", "if", "c", "==", "\" \"", "or", "c", "==", "\"\\t\"", "or", "c", "==", "\"\\r\"", "or", "c", "==", "\"\\n\"", "or", "ord", "(", "c", ")", "==", "0x202F", "or", "c", "==", "'\\u200e'", "or", "c", "in", "all_white_space", "or", "c", "==", "'\u0650'", "or", "c", "==", "'\u20bf'", "or", "c", "==", "'\u0084'", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.whitespace_tokenize": [[142, 149], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_tokens2c_tokens": [[151, 210], ["enumerate", "w_to_c_index.append", "tokenizer.tokenize", "common_utils._improve_answer_span", "len", "len", "ValueError", "c_to_w_index.append", "c_tokens.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils._improve_answer_span"], ["", "def", "w_tokens2c_tokens", "(", "w_tokens", ",", "w_start_position", ",", "w_end_position", ",", "gt_answer_text", ",", "\n", "tokenizer", ",", "no_answer", "=", "False", ",", "is_yes_no_question", "=", "False", ",", "is_training", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    This method convert w-tokens to c-tokens (customized tokenzied tokens.) depend on the given tokenizer.\n\n    :param w_tokens:                The w-tokens\n    :param w_start_position:        Start position of w.r.t. w-tokens\n    :param w_end_position:          End position of w.r.t. w-tokens\n    :param gt_answer_text:          Ground truth answer text\n    :param tokenizer:               The tokenizer\n    :param no_answer:               Whether the context contains the answer.\n    :param is_training:\n    :return:\n\n        c_tokens,               The c-tokens on top of w-tokens with customized tokenizer.\n        c_to_w_index,           The index from c-tokens to w-tokens.\n        w_to_c_index,           The start index from w-tokens to c-tokens.\n        c_start_position,       Start position of w.r.t. c-tokens\n        c_end_position          End position of w.r.t. c-tokens\n    \"\"\"", "\n", "c_to_w_index", "=", "[", "]", "# Total length: len(c_tokens)", "\n", "w_to_c_index", "=", "[", "]", "# Total length: len(w_tokens)", "\n", "c_tokens", "=", "[", "]", "\n", "\n", "# print(w_tokens)", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "w_tokens", ")", ":", "\n", "        ", "w_to_c_index", ".", "append", "(", "len", "(", "c_tokens", ")", ")", "\n", "sub_tokens", "=", "tokenizer", ".", "tokenize", "(", "token", ")", "\n", "# print(w_tokens)", "\n", "# print(token)", "\n", "# print(sub_tokens)", "\n", "if", "len", "(", "sub_tokens", ")", "==", "0", ":", "\n", "            ", "return", "None", ",", "None", ",", "None", ",", "None", ",", "None", "# Error skip this example, change later?", "\n", "raise", "ValueError", "(", "\"Number of sub-tokens should be greater than 0.\"", ")", "\n", "", "for", "sub_token", "in", "sub_tokens", ":", "# Make sure that token will not be empty", "\n", "            ", "c_to_w_index", ".", "append", "(", "i", ")", "\n", "c_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "c_start_position", "=", "None", "\n", "c_end_position", "=", "None", "\n", "if", "is_training", "and", "no_answer", ":", "\n", "        ", "c_start_position", "=", "-", "1", "\n", "c_end_position", "=", "-", "1", "\n", "\n", "", "if", "is_training", "and", "is_yes_no_question", ":", "\n", "        ", "c_start_position", "=", "-", "1", "\n", "c_end_position", "=", "-", "1", "\n", "\n", "", "if", "is_training", "and", "not", "no_answer", ":", "\n", "        ", "c_start_position", "=", "w_to_c_index", "[", "w_start_position", "]", "\n", "if", "w_end_position", "<", "len", "(", "w_tokens", ")", "-", "1", ":", "\n", "            ", "c_end_position", "=", "w_to_c_index", "[", "w_end_position", "+", "1", "]", "-", "1", "\n", "", "else", ":", "\n", "            ", "c_end_position", "=", "len", "(", "c_tokens", ")", "-", "1", "\n", "", "(", "c_start_position", ",", "c_end_position", ")", "=", "_improve_answer_span", "(", "\n", "c_tokens", ",", "c_start_position", ",", "c_end_position", ",", "tokenizer", ",", "\n", "gt_answer_text", ")", "\n", "\n", "", "return", "c_tokens", ",", "c_to_w_index", ",", "w_to_c_index", ",", "c_start_position", ",", "c_end_position", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils._improve_answer_span": [[212, 248], ["range", "tokenizer.tokenize", "range"], "function", ["None"], ["", "def", "_improve_answer_span", "(", "doc_tokens", ",", "input_start", ",", "input_end", ",", "tokenizer", ",", "\n", "orig_answer_text", ")", ":", "\n", "# The code is originally from hugging face squad preprocessing.", "\n", "# We will need to use the code when we use tokenizer to obtain subtokens and rematch with ground truth answer span", "\n", "    ", "\"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"", "\n", "# The SQuAD annotations are character based. We first project them to", "\n", "# whitespace-tokenized words. But then after WordPiece tokenization, we can", "\n", "# often find a \"better match\". For example:", "\n", "#", "\n", "#   Question: What year was John Smith born?", "\n", "#   Context: The leader was John Smith (1895-1943).", "\n", "#   Answer: 1895", "\n", "#", "\n", "# The original whitespace-tokenized answer will be \"(1895-1943).\". However", "\n", "# after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match", "\n", "# the exact answer, 1895.", "\n", "#", "\n", "# However, this is not always possible. Consider the following:", "\n", "#", "\n", "#   Question: What country is the top exporter of electornics?", "\n", "#   Context: The Japanese electronics industry is the lagest in the world.", "\n", "#   Answer: Japan", "\n", "#", "\n", "# In this case, the annotator chose \"Japan\" as a character sub-span of", "\n", "# the word \"Japanese\". Since our WordPiece tokenizer does not split", "\n", "# \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare", "\n", "# in SQuAD, but does happen.", "\n", "tok_answer_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_answer_text", ")", ")", "\n", "\n", "for", "new_start", "in", "range", "(", "input_start", ",", "input_end", "+", "1", ")", ":", "\n", "        ", "for", "new_end", "in", "range", "(", "input_end", ",", "new_start", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "text_span", "=", "\" \"", ".", "join", "(", "doc_tokens", "[", "new_start", ":", "(", "new_end", "+", "1", ")", "]", ")", "\n", "if", "text_span", "==", "tok_answer_text", ":", "\n", "                ", "return", "new_start", ",", "new_end", "\n", "\n", "", "", "", "return", "input_start", ",", "input_end", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.logits_to_span_score": [[250, 286], ["start_logits.size", "start_logits.tolist", "end_logits.tolist", "context_length.tolist", "range", "range", "sorted", "b_scored_span_list.append", "len", "range", "min", "sorted.append", "len"], "function", ["None"], ["", "def", "logits_to_span_score", "(", "start_logits", ",", "end_logits", ",", "context_length", ",", "max_span_length", "=", "30", ")", ":", "\n", "    ", "\"\"\"\n    This is batched operation that convert start logits and end logtis to a list of scored, span.\n    Each example of the two logits will also depends on the context length.\n\n    :param start_logits:        # [B, T]\n    :param end_logits:          # [B, T]\n    :param context_length:      # [B]\n    :return:\n    \"\"\"", "\n", "# pass", "\n", "batch_size", "=", "start_logits", ".", "size", "(", "0", ")", "\n", "s_logits", "=", "start_logits", ".", "tolist", "(", ")", "\n", "e_logits", "=", "end_logits", ".", "tolist", "(", ")", "\n", "c_l", "=", "context_length", ".", "tolist", "(", ")", "\n", "b_scored_span_list", ":", "List", "[", "List", "[", "Tuple", "[", "float", ",", "float", ",", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "[", "]", "\n", "if", "max_span_length", "is", "None", ":", "\n", "        ", "max_span_length", "=", "1000", "\n", "\n", "", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "cur_l", "=", "c_l", "[", "b_i", "]", "\n", "cur_s_logits", "=", "s_logits", "[", "b_i", "]", "[", ":", "cur_l", "]", "\n", "cur_e_logits", "=", "e_logits", "[", "b_i", "]", "[", ":", "cur_l", "]", "\n", "cur_span_list", ":", "List", "[", "Tuple", "[", "float", ",", "float", ",", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "\n", "for", "s_i", "in", "range", "(", "len", "(", "cur_s_logits", ")", ")", ":", "\n", "            ", "for", "e_i", "in", "range", "(", "s_i", ",", "min", "(", "len", "(", "cur_e_logits", ")", ",", "s_i", "+", "max_span_length", ")", ")", ":", "\n", "                ", "s_score", "=", "cur_s_logits", "[", "s_i", "]", "\n", "e_score", "=", "cur_e_logits", "[", "e_i", "]", "\n", "assert", "s_i", "<=", "e_i", "\n", "cur_span_list", ".", "append", "(", "(", "s_score", ",", "e_score", ",", "(", "s_i", ",", "e_i", ")", ")", ")", "\n", "\n", "", "", "cur_span_list", "=", "sorted", "(", "cur_span_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", "+", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "# Start + End logits", "\n", "b_scored_span_list", ".", "append", "(", "cur_span_list", ")", "\n", "\n", "", "return", "b_scored_span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.get_final_text": [[288, 384], ["pytorch_pretrained_bert.BasicTokenizer", "tok_text.find", "common_utils.get_final_text._strip_spaces"], "function", ["None"], ["", "def", "get_final_text", "(", "pred_text", ",", "orig_text", ",", "do_lower_case", ",", "verbose_logging", "=", "False", ")", ":", "\n", "    ", "\"\"\"Project the tokenized prediction back to the original text.\"\"\"", "\n", "\n", "# This method is borrowed from hugging-face BERT codebase.", "\n", "\n", "# When we created the data, we kept track of the alignment between original", "\n", "# (whitespace tokenized) tokens and our WordPiece tokenized tokens. So", "\n", "# now `orig_text` contains the span of our original text corresponding to the", "\n", "# span that we predicted.", "\n", "#", "\n", "# However, `orig_text` may contain extra characters that we don't want in", "\n", "# our prediction.", "\n", "#", "\n", "# For example, let's say:", "\n", "#   pred_text = steve smith", "\n", "#   orig_text = Steve Smith's", "\n", "#", "\n", "# We don't want to return `orig_text` because it contains the extra \"'s\".", "\n", "#", "\n", "# We don't want to return `pred_text` because it's already been normalized", "\n", "# (the SQuAD eval script also does punctuation stripping/lower casing but", "\n", "# our tokenizer does additional normalization like stripping accent", "\n", "# characters).", "\n", "#", "\n", "# What we really want to return is \"Steve Smith\".", "\n", "#", "\n", "# Therefore, we have to apply a semi-complicated alignment heruistic between", "\n", "# `pred_text` and `orig_text` to get a character-to-charcter alignment. This", "\n", "# can fail in certain cases in which case we just return `orig_text`.", "\n", "\n", "def", "_strip_spaces", "(", "text", ")", ":", "\n", "        ", "ns_chars", "=", "[", "]", "\n", "ns_to_s_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "(", "i", ",", "c", ")", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "if", "c", "==", "\" \"", ":", "\n", "                ", "continue", "\n", "", "ns_to_s_map", "[", "len", "(", "ns_chars", ")", "]", "=", "i", "\n", "ns_chars", ".", "append", "(", "c", ")", "\n", "", "ns_text", "=", "\"\"", ".", "join", "(", "ns_chars", ")", "\n", "return", "(", "ns_text", ",", "ns_to_s_map", ")", "\n", "\n", "# We first tokenize `orig_text`, strip whitespace from the result", "\n", "# and `pred_text`, and check if they are the same length. If they are", "\n", "# NOT the same length, the heuristic has failed. If they are the same", "\n", "# length, we assume the characters are one-to-one aligned.", "\n", "", "tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "\n", "tok_text", "=", "\" \"", ".", "join", "(", "tokenizer", ".", "tokenize", "(", "orig_text", ")", ")", "\n", "\n", "start_position", "=", "tok_text", ".", "find", "(", "pred_text", ")", "\n", "if", "start_position", "==", "-", "1", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Unable to find text: '%s' in '%s'\"", "%", "(", "pred_text", ",", "orig_text", ")", ")", "\n", "", "return", "orig_text", "\n", "", "end_position", "=", "start_position", "+", "len", "(", "pred_text", ")", "-", "1", "\n", "\n", "(", "orig_ns_text", ",", "orig_ns_to_s_map", ")", "=", "_strip_spaces", "(", "orig_text", ")", "\n", "(", "tok_ns_text", ",", "tok_ns_to_s_map", ")", "=", "_strip_spaces", "(", "tok_text", ")", "\n", "\n", "if", "len", "(", "orig_ns_text", ")", "!=", "len", "(", "tok_ns_text", ")", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Length not equal after stripping spaces: '%s' vs '%s'\"", ",", "\n", "orig_ns_text", ",", "tok_ns_text", ")", "\n", "", "return", "orig_text", "\n", "\n", "# We then project the characters in `pred_text` back to `orig_text` using", "\n", "# the character-to-character alignment.", "\n", "", "tok_s_to_ns_map", "=", "{", "}", "\n", "for", "(", "i", ",", "tok_index", ")", "in", "tok_ns_to_s_map", ".", "items", "(", ")", ":", "\n", "        ", "tok_s_to_ns_map", "[", "tok_index", "]", "=", "i", "\n", "\n", "", "orig_start_position", "=", "None", "\n", "if", "start_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_start_position", "=", "tok_s_to_ns_map", "[", "start_position", "]", "\n", "if", "ns_start_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_start_position", "=", "orig_ns_to_s_map", "[", "ns_start_position", "]", "\n", "\n", "", "", "if", "orig_start_position", "is", "None", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map start position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "orig_end_position", "=", "None", "\n", "if", "end_position", "in", "tok_s_to_ns_map", ":", "\n", "        ", "ns_end_position", "=", "tok_s_to_ns_map", "[", "end_position", "]", "\n", "if", "ns_end_position", "in", "orig_ns_to_s_map", ":", "\n", "            ", "orig_end_position", "=", "orig_ns_to_s_map", "[", "ns_end_position", "]", "\n", "\n", "", "", "if", "orig_end_position", "is", "None", ":", "\n", "        ", "if", "verbose_logging", ":", "\n", "            ", "logger", ".", "info", "(", "\"Couldn't map end position\"", ")", "\n", "", "return", "orig_text", "\n", "\n", "", "output_text", "=", "orig_text", "[", "orig_start_position", ":", "(", "orig_end_position", "+", "1", ")", "]", "\n", "return", "output_text", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.merge_predicted_fitem_to_eitem": [[386, 445], ["fitem_dict.items", "eitem_dict.items", "list", "[].extend", "sorted", "eitem_dict[].update", "min", "max", "max"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["", "def", "merge_predicted_fitem_to_eitem", "(", "fitem_dict", ",", "pre_etiem_dict", "=", "None", ",", "pred_no_answer", "=", "False", ",", "no_answer_threshold", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"\n    This method helps merge the forward item to prediction file.\n    Because the sliding window natural, one context-question pair might be converted to multiple forward item passing through the network.\n    We need to merge them before we do evaluation. This usually require some comparing method to choose the best prediction along all the forward outputs..\n\n    This method can be generally useful to many scenario.\n    :param fitem_dict:\n    :return:\n    \"\"\"", "\n", "eitem_dict", "=", "{", "}", "\n", "for", "k", ",", "fitem", "in", "fitem_dict", ".", "items", "(", ")", ":", "\n", "        ", "uid", "=", "fitem", "[", "'uid'", "]", "\n", "if", "uid", "in", "eitem_dict", ":", "\n", "            ", "eitem_dict", "[", "uid", "]", "[", "'top_spans'", "]", ".", "extend", "(", "fitem", "[", "'top_spans'", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "pre_etiem_dict", "is", "not", "None", ":", "\n", "                ", "eitem_dict", "[", "uid", "]", ".", "update", "(", "pre_etiem_dict", "[", "uid", "]", ")", "\n", "", "else", ":", "\n", "                ", "eitem_dict", "[", "uid", "]", "=", "{", "}", "\n", "eitem_dict", "[", "uid", "]", "[", "'top_spans'", "]", "=", "fitem", "[", "'top_spans'", "]", "\n", "\n", "", "", "", "eval_dict", "=", "{", "}", "\n", "for", "k", ",", "eitem", "in", "eitem_dict", ".", "items", "(", ")", ":", "\n", "        ", "eitem_dict", "[", "k", "]", "[", "'top_spans'", "]", "=", "list", "(", "sorted", "(", "eitem", "[", "'top_spans'", "]", ",", "key", "=", "lambda", "x", ":", "(", "x", ".", "start_logit", "+", "x", ".", "end_logit", ")", ",", "\n", "reverse", "=", "True", ")", ")", "\n", "# yes_item, no_item, no_answer_item = None, None, None", "\n", "no_answer_min_score", "=", "1_000_000", "\n", "best_yes_score", "=", "-", "1_000_000", "\n", "best_no_score", "=", "-", "1_000_000", "\n", "for", "cur_span", "in", "eitem_dict", "[", "k", "]", "[", "'top_spans'", "]", ":", "\n", "# \"text\", \"c_token_start\", \"c_token_end\", \"start_logit\", \"end_logit\"", "\n", "            ", "if", "cur_span", ".", "text", "==", "\"\"", "and", "cur_span", ".", "c_token_start", "==", "cur_span", ".", "c_token_end", "and", "cur_span", ".", "c_token_start", "==", "0", ":", "\n", "                ", "sum_score", "=", "cur_span", ".", "start_logit", "+", "cur_span", ".", "end_logit", "\n", "no_answer_min_score", "=", "min", "(", "sum_score", ",", "no_answer_min_score", ")", "\n", "", "elif", "cur_span", ".", "text", "==", "\"yes\"", "and", "cur_span", ".", "c_token_start", "==", "cur_span", ".", "c_token_end", "and", "cur_span", ".", "c_token_start", "==", "1", ":", "\n", "                ", "best_yes_score", "=", "max", "(", "best_yes_score", ",", "cur_span", ".", "start_logit", "+", "cur_span", ".", "end_logit", ")", "\n", "", "elif", "cur_span", ".", "text", "==", "\"no\"", "and", "cur_span", ".", "c_token_start", "==", "cur_span", ".", "c_token_end", "and", "cur_span", ".", "c_token_start", "==", "2", ":", "\n", "                ", "best_no_score", "=", "max", "(", "best_no_score", ",", "cur_span", ".", "start_logit", "+", "cur_span", ".", "end_logit", ")", "\n", "\n", "", "", "assert", "no_answer_min_score", "!=", "1_000_000", "# no_answer_min_score need to be updated", "\n", "assert", "best_yes_score", "!=", "-", "1_000_000", "# yes_score need to be updated", "\n", "assert", "best_no_score", "!=", "-", "1_000_000", "# no_score need to be updated", "\n", "\n", "eitem_dict", "[", "k", "]", "[", "'best_yes_score'", "]", "=", "best_yes_score", "\n", "eitem_dict", "[", "k", "]", "[", "'best_no_score'", "]", "=", "best_no_score", "\n", "eitem_dict", "[", "k", "]", "[", "'best_pred_span'", "]", "=", "eitem", "[", "'top_spans'", "]", "[", "0", "]", ".", "text", "\n", "eitem_dict", "[", "k", "]", "[", "'best_pred_span_score'", "]", "=", "eitem", "[", "'top_spans'", "]", "[", "0", "]", ".", "start_logit", "+", "eitem", "[", "'top_spans'", "]", "[", "0", "]", ".", "end_logit", "\n", "eitem_dict", "[", "k", "]", "[", "'min_no_answer_score'", "]", "=", "no_answer_min_score", "\n", "\n", "if", "pred_no_answer", "and", "no_answer_min_score", "-", "eitem_dict", "[", "k", "]", "[", "'best_pred_span_score'", "]", ">", "no_answer_threshold", ":", "\n", "            ", "eval_dict", "[", "k", "]", "=", "\"\"", "# Predict no answer", "\n", "", "else", ":", "\n", "            ", "eval_dict", "[", "k", "]", "=", "eitem_dict", "[", "k", "]", "[", "'best_pred_span'", "]", "\n", "\n", "", "", "return", "eitem_dict", ",", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.write_to_predicted_fitem": [[447, 589], ["common_utils.logits_to_span_score", "start_logits.size", "collections.namedtuple", "range", "int", "sorted", "set", "int", "int", "c_cat_text.strip.replace", "c_cat_text.strip.replace", "c_cat_text.strip.strip", "common_utils.get_final_text", "output_top_spans.append", "set.add", "special_scored_span.append", "token_is_max_context.get", "filtered_scored_span.append", "c_cat_text.strip.split", "collections.namedtuple.", "special_position_mapping.keys", "output_top_spans.append", "set.add", "collections.namedtuple.", "output_top_spans.append", "set.add", "collections.namedtuple.", "output_top_spans.append", "set.add", "ValueError", "collections.namedtuple."], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.logits_to_span_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.get_final_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "write_to_predicted_fitem", "(", "start_logits", ",", "end_logits", ",", "context_length", ",", "b_fids", ",", "b_uids", ",", "gt_span", ",", "\n", "fitem_dict", ",", "output_fitem_dict", ",", "do_lower_case", ",", "top_n_span", "=", "30", ")", ":", "\n", "    ", "\"\"\"\n    We convert the output of networks together with the original fitem to new output fitem.\n    This new output fitem will later to be mapped to original examples using \"merge_predicted_fitem_to_eitem\" method above.\n\n    :param start_logits:            A batch of start logits.\n    :param end_logits:              A batch of end logits.\n    :param context_length:          A batch of int indicating the length of the context.\n    :param b_fids:                  A batch of fids.\n    :param b_uids:                  A batch of uids.\n    :param gt_span:                 A batch of ground truth spans.\n\n        All the parameter above should be order preserved.\n\n    :param fitem_dict:              A the fitem we built before passing through the networks. This is just for reference.\n    :param output_fitem_dict:       The output fitem dict we want to write to\n    :return:\n    \"\"\"", "\n", "\n", "# b_fids = batch['fid']", "\n", "# b_uids = batch['uid']", "\n", "# print(gt_span)", "\n", "\n", "b_scored_spans", "=", "logits_to_span_score", "(", "start_logits", ",", "end_logits", ",", "context_length", ")", "\n", "# print(b_scored_spans[:5])", "\n", "\n", "batch_size", "=", "start_logits", ".", "size", "(", "0", ")", "\n", "\n", "NbestSpanPrediction", "=", "collections", ".", "namedtuple", "(", "# pylint: disable=invalid-name", "\n", "\"NbestSpanPrediction\"", ",", "[", "\"text\"", ",", "\"c_token_start\"", ",", "\"c_token_end\"", ",", "\"start_logit\"", ",", "\"end_logit\"", "]", ")", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "new_fitem", "=", "{", "}", "\n", "\n", "fid", "=", "b_fids", "[", "b_i", "]", "\n", "uid", "=", "b_uids", "[", "b_i", "]", "\n", "assert", "uid", "==", "fitem_dict", "[", "fid", "]", "[", "'uid'", "]", "# Check mapping correctness.", "\n", "\n", "scored_span", "=", "b_scored_spans", "[", "b_i", "]", "\n", "\n", "gt_c_start", ",", "gt_c_end", "=", "-", "1", ",", "-", "1", "\n", "\n", "if", "gt_span", "is", "not", "None", ":", "\n", "            ", "b_gt_span", "=", "gt_span", "[", "b_i", "]", "\n", "gt_c_start", "=", "int", "(", "b_gt_span", "[", "0", "]", ")", "\n", "gt_c_end", "=", "int", "(", "b_gt_span", "[", "1", "]", ")", "\n", "\n", "", "c_l", "=", "int", "(", "context_length", "[", "b_i", "]", ")", "\n", "fitem", "=", "fitem_dict", "[", "fid", "]", "\n", "special_position_mapping", "=", "fitem", "[", "'special_position_mapping'", "]", "\n", "fctoken_to_wtoken_map", "=", "fitem", "[", "'fctoken_to_wtoken_map'", "]", "\n", "paired_c_tokens", "=", "fitem", "[", "'paired_c_tokens'", "]", "\n", "w_token_context", "=", "fitem", "[", "'w_token_context'", "]", "\n", "\n", "# We do the max context filtering here to prevent any further confusion.", "\n", "token_is_max_context", "=", "fitem", "[", "'token_is_max_context'", "]", "\n", "filtered_scored_span", "=", "[", "]", "\n", "special_scored_span", "=", "[", "]", "\n", "\n", "for", "start_s", ",", "end_s", ",", "(", "c_token_start", ",", "c_token_end", ")", "in", "scored_span", ":", "\n", "            ", "if", "c_token_start", "==", "c_token_end", "and", "c_token_start", "in", "special_position_mapping", ":", "\n", "                ", "special_scored_span", ".", "append", "(", "(", "start_s", ",", "end_s", ",", "(", "c_token_start", ",", "c_token_end", ")", ")", ")", "\n", "# We append (0,0), (1,1), (2,2)", "\n", "continue", "\n", "\n", "", "if", "token_is_max_context", ".", "get", "(", "c_token_start", ",", "False", ")", "and", "c_token_start", "in", "fctoken_to_wtoken_map", "and", "c_token_end", "in", "fctoken_to_wtoken_map", ":", "\n", "                ", "filtered_scored_span", ".", "append", "(", "(", "start_s", ",", "end_s", ",", "(", "c_token_start", ",", "c_token_end", ")", ")", ")", "\n", "\n", "", "", "scored_span", "=", "filtered_scored_span", "\n", "# Max context filtering end", "\n", "\n", "scored_span", "=", "sorted", "(", "scored_span", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", "+", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "# Start + End logits", "\n", "top30_scored_span", "=", "scored_span", "[", ":", "top_n_span", "]", "+", "special_scored_span", "\n", "\n", "output_top_spans", "=", "[", "]", "\n", "seen_final_text", "=", "set", "(", ")", "\n", "\n", "for", "cur_span", "in", "top30_scored_span", ":", "\n", "# top_span = top30_scored_span[0]", "\n", "            ", "start_logits", "=", "cur_span", "[", "0", "]", "\n", "end_logits", "=", "cur_span", "[", "1", "]", "\n", "c_token_start", ",", "c_token_end", "=", "cur_span", "[", "2", "]", "\n", "\n", "if", "c_token_start", "==", "c_token_end", "and", "c_token_start", "in", "special_position_mapping", ".", "keys", "(", ")", ":", "\n", "                ", "if", "c_token_start", "==", "0", ":", "\n", "                    ", "final_text", "=", "\"\"", "\n", "output_top_spans", ".", "append", "(", "\n", "NbestSpanPrediction", "(", "final_text", ",", "c_token_start", ",", "c_token_end", ",", "start_logits", ",", "end_logits", ")", ")", "\n", "seen_final_text", ".", "add", "(", "final_text", ")", "\n", "", "elif", "c_token_start", "==", "1", ":", "\n", "                    ", "final_text", "=", "\"yes\"", "\n", "output_top_spans", ".", "append", "(", "\n", "NbestSpanPrediction", "(", "final_text", ",", "c_token_start", ",", "c_token_end", ",", "start_logits", ",", "end_logits", ")", ")", "\n", "seen_final_text", ".", "add", "(", "final_text", ")", "\n", "", "elif", "c_token_end", "==", "2", ":", "\n", "                    ", "final_text", "=", "\"no\"", "\n", "output_top_spans", ".", "append", "(", "\n", "NbestSpanPrediction", "(", "final_text", ",", "c_token_start", ",", "c_token_end", ",", "start_logits", ",", "end_logits", ")", ")", "\n", "seen_final_text", ".", "add", "(", "final_text", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "f\"Unexpected special position value {c_token_start}.\"", ")", "\n", "", "continue", "\n", "# print(c_token_start, c_token_end)", "\n", "# fctoken_to_wtoken_map = fitem['fctoken_to_wtoken_map']", "\n", "\n", "", "c_token_pred_answer_span", "=", "paired_c_tokens", "[", "c_token_start", ":", "c_token_end", "+", "1", "]", "\n", "w_token_start", "=", "fctoken_to_wtoken_map", "[", "c_token_start", "]", "\n", "w_token_end", "=", "fctoken_to_wtoken_map", "[", "c_token_end", "]", "\n", "w_token_pred_answer_span", "=", "w_token_context", "[", "w_token_start", ":", "w_token_end", "+", "1", "]", "\n", "gt_c_answer_span", "=", "paired_c_tokens", "[", "gt_c_start", ":", "gt_c_end", "+", "1", "]", "\n", "\n", "c_cat_text", "=", "\" \"", ".", "join", "(", "c_token_pred_answer_span", ")", "\n", "\n", "# De-tokenize WordPieces that have been split off.", "\n", "c_cat_text", "=", "c_cat_text", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "c_cat_text", "=", "c_cat_text", ".", "replace", "(", "\"##\"", ",", "\"\"", ")", "\n", "\n", "# Clean whitespace", "\n", "c_cat_text", "=", "c_cat_text", ".", "strip", "(", ")", "\n", "c_cat_text", "=", "\" \"", ".", "join", "(", "c_cat_text", ".", "split", "(", ")", ")", "\n", "w_cat_text", "=", "\" \"", ".", "join", "(", "w_token_pred_answer_span", ")", "\n", "\n", "final_text", "=", "get_final_text", "(", "c_cat_text", ",", "w_cat_text", ",", "do_lower_case", ")", "\n", "\n", "# if final_text not in seen_final_text:", "\n", "output_top_spans", ".", "append", "(", "\n", "NbestSpanPrediction", "(", "final_text", ",", "c_token_start", ",", "c_token_end", ",", "start_logits", ",", "end_logits", ")", ")", "\n", "seen_final_text", ".", "add", "(", "final_text", ")", "\n", "\n", "", "new_fitem", "[", "'top_spans'", "]", "=", "output_top_spans", "\n", "new_fitem", "[", "'fid'", "]", "=", "fitem", "[", "'fid'", "]", "\n", "new_fitem", "[", "'uid'", "]", "=", "fitem", "[", "'uid'", "]", "\n", "if", "'additional_fields'", "in", "fitem", ":", "\n", "            ", "new_fitem", "[", "'additional_fields'", "]", "=", "fitem", "[", "'additional_fields'", "]", "\n", "\n", "# new_fitem.update(fitem)", "\n", "# assert fid not in new_fitem", "\n", "", "output_fitem_dict", "[", "fid", "]", "=", "new_fitem", "\n", "\n", "", "return", "output_fitem_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.check_pred_output": [[591, 633], ["common_utils.logits_to_span_score", "start_logits.size", "range", "int", "print", "print", "print", "print", "print", "int", "int"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.logits_to_span_score"], ["", "def", "check_pred_output", "(", "start_logits", ",", "end_logits", ",", "context_length", ",", "b_fids", ",", "b_uids", ",", "gt_span", ",", "fitem_dict", ")", ":", "\n", "# b_fids = batch['fid']", "\n", "# b_uids = batch['uid']", "\n", "# print(gt_span)", "\n", "\n", "    ", "b_scored_spans", "=", "logits_to_span_score", "(", "start_logits", ",", "end_logits", ",", "context_length", ")", "\n", "# print(b_scored_spans[:5])", "\n", "\n", "batch_size", "=", "start_logits", ".", "size", "(", "0", ")", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "fid", "=", "b_fids", "[", "b_i", "]", "\n", "scored_span", "=", "b_scored_spans", "[", "b_i", "]", "\n", "\n", "gt_c_start", ",", "gt_c_end", "=", "-", "1", ",", "-", "1", "\n", "\n", "if", "gt_span", "is", "not", "None", ":", "\n", "            ", "b_gt_span", "=", "gt_span", "[", "b_i", "]", "\n", "gt_c_start", "=", "int", "(", "b_gt_span", "[", "0", "]", ")", "\n", "gt_c_end", "=", "int", "(", "b_gt_span", "[", "1", "]", ")", "\n", "\n", "", "c_l", "=", "int", "(", "context_length", "[", "b_i", "]", ")", "\n", "fitem", "=", "fitem_dict", "[", "fid", "]", "\n", "top10_scored_span", "=", "scored_span", "[", ":", "10", "]", "\n", "top_span", "=", "top10_scored_span", "[", "0", "]", "\n", "c_token_start", ",", "c_token_end", "=", "top_span", "[", "2", "]", "\n", "# print(c_token_start, c_token_end)", "\n", "context_c_tokens", "=", "fitem", "[", "'context_c_tokens'", "]", "\n", "context_w_tokens", "=", "fitem", "[", "'context_w_tokens'", "]", "\n", "fctoken_to_wtoken_map", "=", "fitem", "[", "'fctoken_to_wtoken_map'", "]", "\n", "token_is_max_context", "=", "fitem", "[", "'token_is_max_context'", "]", "\n", "\n", "c_token_pred_answer_span", "=", "context_c_tokens", "[", "c_token_start", ":", "c_token_end", "+", "1", "]", "\n", "w_token_start", "=", "fctoken_to_wtoken_map", "[", "c_token_start", "]", "\n", "w_token_end", "=", "fctoken_to_wtoken_map", "[", "c_token_end", "]", "\n", "w_token_pred_answer_span", "=", "context_w_tokens", "[", "w_token_start", ":", "w_token_end", "+", "1", "]", "\n", "gt_c_answer_span", "=", "context_c_tokens", "[", "gt_c_start", ":", "gt_c_end", "+", "1", "]", "\n", "\n", "print", "(", "\"C Pred Answer:\"", ",", "c_token_pred_answer_span", ")", "\n", "print", "(", "\"W Pred Answer:\"", ",", "w_token_pred_answer_span", ")", "\n", "print", "(", "\"C GT Answer:\"", ",", "gt_c_answer_span", ")", "\n", "print", "(", "\"GT span:\"", ",", "gt_c_start", ",", "gt_c_end", ")", "\n", "print", "(", "\"Pred span:\"", ",", "c_token_start", ",", "c_token_end", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_preprocess_tool._check_is_max_context": [[11, 46], ["enumerate", "min"], "function", ["None"], ["def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "# cf. hugging face.", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_preprocess_tool.preprocssing_span_prediction_item_paired": [[48, 198], ["span_prediction_task_utils.w_tokens2c_tokens", "dict", "enumerate", "tokenizer.tokenize", "collections.namedtuple", "enumerate", "len", "len", "doc_spans.append", "min", "fc_token.append", "segment_ids.append", "range", "fc_token.append", "segment_ids.append", "str", "dict", "ready_training_list.append", "len", "collections.namedtuple.", "len", "fc_token.append", "segment_ids.append", "fc_token.append", "segment_ids.append", "span_preprocess_tool._check_is_max_context", "fc_token.append", "segment_ids.append", "uuid.uuid4", "item[].lower", "len", "len", "item[].lower", "ValueError", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_tokens2c_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils._check_is_max_context"], ["", "def", "preprocssing_span_prediction_item_paired", "(", "item", ",", "tokenizer", ",", "is_training", ",", "\n", "max_tokens_for_doc", ",", "max_query_length", ",", "doc_stride", ",", "\n", "appended_special_token_list", "=", "SPECIAL_TOKEN_LIST", ")", ":", "\n", "    ", "ready_training_list", "=", "[", "]", "\n", "\n", "c_tokens", ",", "c2w_index", ",", "w2c_index", ",", "c_start_position", ",", "c_end_position", "=", "span_utils", ".", "w_tokens2c_tokens", "(", "\n", "item", "[", "'w_token_context'", "]", ",", "item", "[", "'answer_start'", "]", ",", "item", "[", "'answer_end'", "]", ",", "\n", "item", "[", "'gt_answer_text'", "]", ",", "tokenizer", ",", "item", "[", "'no_answer'", "]", ",", "item", "[", "'is_yes_no_question'", "]", ",", "is_training", "\n", ")", "\n", "\n", "if", "c_tokens", "is", "None", "and", "c2w_index", "is", "None", "and", "w2c_index", "is", "None", "and", "c_start_position", "is", "None", "and", "c_end_position", "is", "None", ":", "\n", "        ", "return", "ready_training_list", "\n", "\n", "", "special_mapping", "=", "dict", "(", ")", "\n", "for", "i", ",", "token_str", "in", "enumerate", "(", "appended_special_token_list", ")", ":", "\n", "        ", "special_mapping", "[", "i", "]", "=", "token_str", "\n", "\n", "", "query", "=", "item", "[", "'query'", "]", "\n", "query_ctokens", "=", "tokenizer", ".", "tokenize", "(", "query", ")", "\n", "\n", "if", "len", "(", "query_ctokens", ")", ">", "max_query_length", ":", "\n", "        ", "query_ctokens", "=", "query_ctokens", "[", "0", ":", "max_query_length", "]", "\n", "\n", "", "no_answer", "=", "item", "[", "'no_answer'", "]", "\n", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "c_tokens", ")", ":", "\n", "        ", "length", "=", "len", "(", "c_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "            ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "c_tokens", ")", ":", "\n", "            ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "# We first append special tokens and query tokens", "\n", "        ", "fc_token", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "for", "token", "in", "appended_special_token_list", ":", "\n", "            ", "fc_token", ".", "append", "(", "token", ")", "# [CLS] yes no", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# Then we append all query tokens", "\n", "", "for", "token", "in", "query_ctokens", ":", "\n", "            ", "fc_token", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "fc_token", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "# query token append finished.", "\n", "\n", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "\n", "fctoken_to_wtoken_map", "=", "{", "}", "# we need a map to the original w-tokens", "\n", "\n", "# This the the code from Google BERT where we keep track of the score of current token.", "\n", "# This is only for sliding window approach.", "\n", "token_is_max_context", "=", "{", "}", "\n", "\n", "# We do this for all the document. Bc this is not that time-consuming. 2019-03-14", "\n", "# if not is_training:     # We only need to do this during inference  # But maybe not. Check laters", "\n", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "            ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "fctoken_to_wtoken_map", "[", "len", "(", "fc_token", ")", "]", "=", "c2w_index", "[", "split_token_index", "]", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "len", "(", "fc_token", ")", "]", "=", "is_max_context", "\n", "fc_token", ".", "append", "(", "c_tokens", "[", "split_token_index", "]", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "# The index offset is 0.", "\n", "# the_index = doc_span.start + i", "\n", "# fctoken_to_wtoken_map[i] = c2w_index[doc_span.start + i]", "\n", "# split_token_index = doc_span.start + i", "\n", "# is_max_context = _check_is_max_context(doc_spans, doc_span_index,", "\n", "#                                        split_token_index)", "\n", "# token_is_max_context[i] = is_max_context", "\n", "\n", "", "fc_token", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "1", ")", "\n", "\n", "if", "is_training", "and", "not", "item", "[", "'no_answer'", "]", "and", "not", "item", "[", "'is_yes_no_question'", "]", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "            ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "c_start_position", ">=", "doc_start", "and", "c_end_position", "<=", "doc_end", ")", ":", "\n", "                ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "                ", "no_answer", "=", "True", "\n", "start_position", "=", "0", "\n", "end_position", "=", "0", "# if out of span then, no_answers", "\n", "", "else", ":", "\n", "# doc_offset = len(query_tokens) + 2", "\n", "                ", "no_answer", "=", "False", "\n", "doc_offset", "=", "len", "(", "appended_special_token_list", ")", "+", "len", "(", "query_ctokens", ")", "+", "1", "\n", "# special CLS, yes, no, [query], seq", "\n", "start_position", "=", "c_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "c_end_position", "-", "doc_start", "+", "doc_offset", "\n", "# We change this from original code base, here the doc_offset will only w.r.t. the context.", "\n", "\n", "", "", "if", "is_training", "and", "item", "[", "'no_answer'", "]", ":", "\n", "            ", "no_answer", "=", "True", "\n", "start_position", "=", "0", "\n", "end_position", "=", "0", "\n", "\n", "", "if", "is_training", "and", "'is_yes_no_question'", "in", "item", "and", "item", "[", "'is_yes_no_question'", "]", ":", "\n", "            ", "no_answer", "=", "False", "\n", "if", "item", "[", "'gt_answer_text'", "]", ".", "lower", "(", ")", "==", "'yes'", ":", "\n", "                ", "start_position", "=", "1", "\n", "end_position", "=", "1", "\n", "", "elif", "item", "[", "'gt_answer_text'", "]", ".", "lower", "(", ")", "==", "'no'", ":", "\n", "                ", "start_position", "=", "2", "\n", "end_position", "=", "2", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Answers of Yes/No Question can only be yes or no.'", ")", "\n", "\n", "", "", "if", "not", "is_training", ":", "\n", "# no_answer = True", "\n", "            ", "start_position", "=", "-", "1", "# -2 indicates that the start and end position is hidden.", "\n", "end_position", "=", "-", "1", "\n", "\n", "", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "# This is unique forward id for each of the doc that will be feeded into net.", "\n", "uid", "=", "item", "[", "'uid'", "]", "\n", "\n", "f_item", "=", "dict", "(", ")", "\n", "f_item", "[", "'fid'", "]", "=", "fid", "\n", "f_item", "[", "'uid'", "]", "=", "uid", "\n", "f_item", "[", "'doc_span_index'", "]", "=", "doc_span_index", "\n", "f_item", "[", "'w_token_context'", "]", "=", "item", "[", "'w_token_context'", "]", "\n", "f_item", "[", "'paired_c_tokens'", "]", "=", "fc_token", "\n", "f_item", "[", "'segment_ids'", "]", "=", "segment_ids", "\n", "f_item", "[", "'no_answer'", "]", "=", "no_answer", "\n", "f_item", "[", "'start_position'", "]", "=", "start_position", "\n", "f_item", "[", "'end_position'", "]", "=", "end_position", "\n", "f_item", "[", "'fctoken_to_wtoken_map'", "]", "=", "fctoken_to_wtoken_map", "\n", "f_item", "[", "'token_is_max_context'", "]", "=", "token_is_max_context", "\n", "f_item", "[", "'special_position_mapping'", "]", "=", "special_mapping", "\n", "if", "'additional_fields'", "in", "item", ":", "\n", "            ", "f_item", "[", "'additional_fields'", "]", "=", "item", "[", "'additional_fields'", "]", "\n", "# if f_item['doc_span_index'] == 1:", "\n", "#     pass", "\n", "# The item will not include query.", "\n", "", "ready_training_list", ".", "append", "(", "f_item", ")", "\n", "# each item in the list don't have query, we append the query latter outside the function.", "\n", "", "return", "ready_training_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_preprocess_tool.eitems_to_fitems": [[200, 214], ["tqdm.tqdm", "utils.list_dict_data_tool.list_to_dict", "span_preprocess_tool.preprocssing_span_prediction_item_paired", "fitem_list.extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_preprocess_tool.preprocssing_span_prediction_item_paired"], ["", "def", "eitems_to_fitems", "(", "eitem_list", ",", "tokenizer", ",", "is_training", ",", "max_tokens_for_doc", "=", "320", ",", "max_query_length", "=", "64", ",", "doc_stride", "=", "128", ",", "debug", "=", "False", ")", ":", "\n", "    ", "fitem_list", "=", "[", "]", "# The output of all fitems", "\n", "if", "debug", ":", "\n", "        ", "eitem_list", "=", "eitem_list", "[", ":", "100", "]", "\n", "\n", "", "for", "item", "in", "tqdm", "(", "eitem_list", ")", ":", "\n", "        ", "f_items", "=", "preprocssing_span_prediction_item_paired", "(", "item", ",", "tokenizer", ",", "is_training", ",", "\n", "max_tokens_for_doc", ",", "max_query_length", ",", "\n", "doc_stride", ")", "\n", "fitem_list", ".", "extend", "(", "f_items", ")", "\n", "\n", "", "fitem_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "fitem_list", ",", "'fid'", ")", "\n", "\n", "return", "fitem_dict", ",", "fitem_list", "\n", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.__init__": [[6, 14], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "left_em", ",", "right_em", ",", "span_list", ",", "original_answer", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "left_em", "=", "left_em", "\n", "self", ".", "right_em", "=", "right_em", "\n", "self", ".", "span_list", "=", "span_list", "\n", "self", ".", "original_answer", "=", "original_answer", "\n", "self", ".", "overlap_ratio", "=", "0", "\n", "self", ".", "index_in_cat_token", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.__repr__": [[15, 17], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"(Left_match:{self.left_em}, Right_match:{self.right_em}, Span List:{self.span_list}, \"", "f\"ratio:{self.overlap_ratio})\"", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio": [[19, 27], ["len"], "methods", ["None"], ["", "def", "computer_ratio", "(", "self", ")", ":", "\n", "# print(self.span_list)", "\n", "        ", "span_start", "=", "self", ".", "span_list", "[", "0", "]", "[", "1", "]", "\n", "span_end", "=", "self", ".", "span_list", "[", "-", "1", "]", "[", "2", "]", "\n", "matched_length", "=", "span_end", "-", "span_start", "\n", "overlap_ratio", "=", "len", "(", "self", ".", "original_answer", ")", "/", "matched_length", "\n", "self", ".", "overlap_ratio", "=", "overlap_ratio", "\n", "return", "overlap_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.__init__": [[30, 57], ["object.__init__", "enumerate", "span_match_and_label.ContextAnswerMatcher.non_space_text.lower", "span_match_and_label.ContextAnswerMatcher.non_space_span.append", "len", "len", "len", "len", "len", "len", "len", "lower_token_list.append", "lower_token_list.append", "token.lower", "token.lower"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "token_list", ":", "List", "[", "str", "]", ",", "uncase", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "token_list", "=", "token_list", "\n", "self", ".", "uncase", "=", "uncase", "\n", "self", ".", "non_space_text", "=", "''", ".", "join", "(", "token_list", ")", "\n", "if", "self", ".", "uncase", ":", "\n", "            ", "buffered_non_space_text", "=", "self", ".", "non_space_text", ".", "lower", "(", ")", "\n", "if", "len", "(", "buffered_non_space_text", ")", "!=", "len", "(", "self", ".", "non_space_text", ")", ":", "\n", "                ", "lower_token_list", "=", "[", "]", "\n", "for", "token", "in", "token_list", ":", "\n", "                    ", "if", "len", "(", "token", ")", "==", "len", "(", "token", ".", "lower", "(", ")", ")", ":", "\n", "                        ", "lower_token_list", ".", "append", "(", "token", ".", "lower", "(", ")", ")", "\n", "", "else", ":", "\n", "                        ", "lower_token_list", ".", "append", "(", "token", ")", "\n", "", "", "buffered_non_space_text", "=", "''", ".", "join", "(", "lower_token_list", ")", "\n", "", "self", ".", "non_space_text", "=", "buffered_non_space_text", "\n", "assert", "len", "(", "buffered_non_space_text", ")", "==", "len", "(", "self", ".", "non_space_text", ")", "\n", "\n", "", "self", ".", "non_space_span", "=", "[", "]", "\n", "# The ith element of this list indicate the start and end position of the original token in the non_space_text.", "\n", "self", ".", "cat_token", "=", "' '", ".", "join", "(", "self", ".", "token_list", ")", "\n", "start", "=", "0", "\n", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "token_list", ")", ":", "\n", "            ", "end", "=", "start", "+", "len", "(", "token", ")", "\n", "self", ".", "non_space_span", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "start", "=", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.span_match": [[58, 80], ["None"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "span_match", "(", "cls", ",", "match_value", ":", "List", "[", "bool", "]", ",", "start", ":", "int", "=", "None", ",", "end", ":", "int", "=", "None", ")", ":", "\n", "        ", "all_match", "=", "True", "\n", "any_match", "=", "False", "\n", "\n", "for", "m", "in", "match_value", "[", "start", ":", "end", "]", ":", "\n", "            ", "any_match", "=", "any_match", "or", "m", "\n", "all_match", "=", "all_match", "and", "m", "\n", "\n", "", "if", "all_match", ":", "\n", "            ", "return", "'all'", "\n", "", "elif", "any_match", ":", "\n", "            ", "if", "match_value", "[", "start", ":", "end", "]", "[", "0", "]", "and", "match_value", "[", "start", ":", "end", "]", "[", "-", "1", "]", ":", "\n", "                ", "return", "'any_both'", "\n", "", "elif", "match_value", "[", "start", ":", "end", "]", "[", "0", "]", ":", "\n", "                ", "return", "'any_left'", "\n", "", "elif", "match_value", "[", "start", ":", "end", "]", "[", "-", "1", "]", ":", "\n", "                ", "return", "'any_right'", "\n", "", "else", ":", "\n", "                ", "return", "'any_mid'", "\n", "", "", "else", ":", "\n", "            ", "return", "'none'", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.find_all_answer": [[81, 212], ["answer_text.lower.lower.replace", "enumerate", "answer_text.lower.lower.lower", "m.start", "range", "span_match_and_label.ContextAnswerMatcher.span_match", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "re.finditer", "range", "len", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "re.escape", "len", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "len", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "MatchObject.span_list.append", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "span_match_and_label.MatchObject", "MatchObject.span_list.append", "span_match_and_label.MatchObject.computer_ratio", "span_match_and_label.MatchObject.computer_ratio", "matched_object_list.append", "ValueError"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.span_match", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.MatchObject.computer_ratio"], ["", "", "def", "find_all_answer", "(", "self", ",", "answer_text", ")", ":", "\n", "        ", "if", "self", ".", "uncase", ":", "\n", "            ", "answer_text", "=", "answer_text", ".", "lower", "(", ")", "\n", "", "non_space_answer_text", "=", "answer_text", ".", "replace", "(", "' '", ",", "''", ")", "# remove all space", "\n", "matched_starts", "=", "[", "m", ".", "start", "(", ")", "for", "m", "in", "re", ".", "finditer", "(", "re", ".", "escape", "(", "non_space_answer_text", ")", ",", "self", ".", "non_space_text", ")", "]", "\n", "\n", "match_checking_array", "=", "[", "False", "for", "_", "in", "range", "(", "len", "(", "self", ".", "non_space_text", ")", ")", "]", "\n", "for", "matched_start", "in", "matched_starts", ":", "\n", "            ", "matched_end", "=", "matched_start", "+", "len", "(", "non_space_answer_text", ")", "\n", "for", "i", "in", "range", "(", "matched_start", ",", "matched_end", ")", ":", "\n", "                ", "match_checking_array", "[", "i", "]", "=", "True", "# mark as match", "\n", "\n", "", "", "matched_object_list", "=", "[", "]", "\n", "in_overlap", "=", "False", "\n", "match_object", "=", "None", "# initial is none", "\n", "\n", "for", "i", ",", "(", "start", ",", "end", ")", "in", "enumerate", "(", "self", ".", "non_space_span", ")", ":", "\n", "            ", "match_code", "=", "self", ".", "span_match", "(", "match_checking_array", "[", "start", ":", "end", "]", ")", "\n", "if", "not", "in_overlap", "and", "match_code", "==", "'all'", ":", "\n", "# The start of a new token that matches with the answer.", "\n", "                ", "match_object", "=", "MatchObject", "(", "left_em", "=", "True", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "in_overlap", "=", "True", "\n", "if", "(", "end", "-", "start", ")", "%", "len", "(", "non_space_answer_text", ")", "==", "0", ":", "\n", "                    ", "match_object", ".", "right_em", "=", "True", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "\n", "", "", "elif", "not", "in_overlap", "and", "match_code", "==", "'any_both'", ":", "\n", "# start and end", "\n", "                ", "match_object", "=", "MatchObject", "(", "left_em", "=", "True", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "\n", "# start a new matching one.", "\n", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "in_overlap", "=", "True", "\n", "", "elif", "not", "in_overlap", "and", "match_code", "==", "'any_left'", ":", "\n", "# start and finish the match", "\n", "                ", "match_object", "=", "MatchObject", "(", "left_em", "=", "True", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "\n", "", "elif", "not", "in_overlap", "and", "match_code", "==", "'any_right'", ":", "\n", "# start a new one which doesn't match with the left boundary.", "\n", "                ", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "in_overlap", "=", "True", "\n", "\n", "", "elif", "not", "in_overlap", "and", "match_code", "==", "'any_mid'", ":", "\n", "                ", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "\n", "", "elif", "not", "in_overlap", "and", "match_code", "==", "'none'", ":", "\n", "                ", "pass", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'all'", ":", "\n", "                ", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'any_both'", ":", "\n", "# End the first", "\n", "                ", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "\n", "# Start a new one", "\n", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "in_overlap", "=", "True", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'any_left'", ":", "\n", "# start and end", "\n", "                ", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'any_right'", ":", "\n", "# end the last", "\n", "                ", "match_object", ".", "right_em", "=", "True", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "\n", "# start new", "\n", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "in_overlap", "=", "True", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'any_mid'", ":", "\n", "# end the last", "\n", "                ", "match_object", ".", "right_em", "=", "True", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "\n", "# start new", "\n", "match_object", "=", "MatchObject", "(", "left_em", "=", "False", ",", "right_em", "=", "False", ",", "span_list", "=", "[", "]", ",", "original_answer", "=", "answer_text", ")", "\n", "match_object", ".", "span_list", ".", "append", "(", "(", "i", ",", "start", ",", "end", ")", ")", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "\n", "", "elif", "in_overlap", "and", "match_code", "==", "'none'", ":", "\n", "# end the last", "\n", "                ", "match_object", ".", "right_em", "=", "True", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "in_overlap", "=", "False", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unexpected situation.\"", ")", "\n", "\n", "", "", "if", "match_object", "is", "not", "None", ":", "\n", "            ", "match_object", ".", "right_em", "=", "True", "\n", "match_object", ".", "computer_ratio", "(", ")", "\n", "matched_object_list", ".", "append", "(", "match_object", ")", "\n", "match_object", "=", "None", "\n", "\n", "", "return", "matched_object_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.get_most_plausible_answer": [[213, 236], ["span_match_and_label.ContextAnswerMatcher.find_all_answer", "sorted", "sorted", "sorted", "len", "sorted.append", "len", "sorted.append", "sorted.append"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.find_all_answer"], ["", "def", "get_most_plausible_answer", "(", "self", ",", "answer_text", ")", ":", "\n", "        ", "matched_list", "=", "self", ".", "find_all_answer", "(", "answer_text", ")", "\n", "best_both_matches", "=", "[", "]", "\n", "best_single_matches", "=", "[", "]", "\n", "best_raw_matches", "=", "[", "]", "\n", "for", "match_object", "in", "matched_list", ":", "\n", "            ", "if", "match_object", ".", "left_em", "and", "match_object", ".", "right_em", ":", "\n", "                ", "best_both_matches", ".", "append", "(", "(", "match_object", ",", "match_object", ".", "overlap_ratio", ")", ")", "\n", "", "elif", "match_object", ".", "left_em", "or", "match_object", ".", "right_em", ":", "\n", "                ", "best_single_matches", ".", "append", "(", "(", "match_object", ",", "match_object", ".", "overlap_ratio", ")", ")", "\n", "", "else", ":", "\n", "                ", "best_raw_matches", ".", "append", "(", "(", "match_object", ",", "match_object", ".", "overlap_ratio", ")", ")", "\n", "\n", "", "", "best_both_matches", "=", "sorted", "(", "best_both_matches", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "best_single_matches", "=", "sorted", "(", "best_single_matches", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "best_raw_matches", "=", "sorted", "(", "best_raw_matches", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "if", "len", "(", "best_both_matches", ")", ">", "0", ":", "\n", "            ", "return", "best_both_matches", "\n", "", "elif", "len", "(", "best_single_matches", ")", ">", "0", ":", "\n", "            ", "return", "best_single_matches", "\n", "", "else", ":", "\n", "            ", "return", "best_raw_matches", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.get_matches": [[237, 269], ["span_match_and_label.ContextAnswerMatcher.find_all_answer", "sorted", "len", "span_match_and_label.ContextAnswerMatcher.get_answer_start_position_in_cat_token", "sorted.append", "sorted.append"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.find_all_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.get_answer_start_position_in_cat_token"], ["", "", "def", "get_matches", "(", "self", ",", "answer_text", ",", "match_type", "=", "'both'", ",", "verify_content", "=", "False", ")", ":", "\n", "        ", "matched_list", "=", "self", ".", "find_all_answer", "(", "answer_text", ")", "\n", "left_matches", "=", "[", "]", "\n", "for", "match_object", "in", "matched_list", ":", "# We found only left match.", "\n", "            ", "valid_macth", "=", "False", "\n", "\n", "if", "match_type", "==", "'both'", ":", "\n", "                ", "valid_macth", "=", "match_object", ".", "left_em", "and", "match_object", ".", "right_em", "\n", "", "elif", "match_type", "==", "'left'", ":", "\n", "                ", "valid_macth", "=", "match_object", ".", "left_em", "\n", "", "elif", "match_type", "==", "'right'", ":", "\n", "                ", "valid_macth", "=", "match_object", ".", "right_em", "\n", "", "elif", "match_type", "==", "'any'", ":", "\n", "                ", "valid_macth", "=", "match_object", ".", "left_em", "or", "match_object", ".", "right_em", "\n", "\n", "", "if", "valid_macth", ":", "\n", "                ", "start_index_in_cat_token", "=", "self", ".", "get_answer_start_position_in_cat_token", "(", "match_object", ",", "answer_text", ")", "\n", "if", "verify_content", "and", "start_index_in_cat_token", "!=", "-", "1", ":", "\n", "                    ", "match_object", ".", "index_in_cat_token", "=", "start_index_in_cat_token", "\n", "left_matches", ".", "append", "(", "(", "match_object", ",", "match_object", ".", "overlap_ratio", ")", ")", "\n", "", "else", ":", "\n", "                    ", "match_object", ".", "index_in_cat_token", "=", "match_object", ".", "span_list", "[", "0", "]", "[", "1", "]", "+", "match_object", ".", "span_list", "[", "0", "]", "[", "0", "]", "\n", "left_matches", ".", "append", "(", "(", "match_object", ",", "match_object", ".", "overlap_ratio", ")", ")", "\n", "\n", "", "", "", "left_matches", "=", "sorted", "(", "left_matches", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "if", "len", "(", "left_matches", ")", "==", "0", ":", "# If no left matches we give nothing.", "\n", "            ", "results_match", "=", "[", "]", "\n", "", "else", ":", "# If we have one matches, we give all the matches that have the same highest score.", "\n", "            ", "best_score", "=", "left_matches", "[", "0", "]", "[", "1", "]", "\n", "results_match", "=", "[", "cur_match", "for", "cur_match", ",", "cur_score", "in", "left_matches", "if", "cur_score", "==", "best_score", "]", "\n", "", "return", "results_match", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.get_answer_start_position_in_cat_token": [[270, 277], ["len"], "methods", ["None"], ["", "def", "get_answer_start_position_in_cat_token", "(", "self", ",", "match_object", ",", "answer_text", ")", ":", "\n", "        ", "start_index", "=", "match_object", ".", "span_list", "[", "0", "]", "[", "1", "]", "+", "match_object", ".", "span_list", "[", "0", "]", "[", "0", "]", "\n", "original_span_text", "=", "self", ".", "cat_token", "[", "start_index", ":", "start_index", "+", "len", "(", "answer_text", ")", "]", "\n", "if", "original_span_text", "!=", "answer_text", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "start_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index": [[278, 285], ["span_match_and_label.ContextAnswerMatcher.get_matches", "answer_start_index.append"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.get_matches"], ["", "", "def", "concate_and_return_answer_index", "(", "self", ",", "answer_text", ",", "match_type", "=", "'both'", ",", "verify_content", "=", "False", ")", ":", "\n", "        ", "left_matches", "=", "self", ".", "get_matches", "(", "answer_text", ",", "match_type", ",", "verify_content", ")", "\n", "answer_start_index", "=", "[", "]", "\n", "for", "l_match_object", "in", "left_matches", ":", "\n", "            ", "cur_start_index", "=", "l_match_object", ".", "index_in_cat_token", "\n", "answer_start_index", ".", "append", "(", "cur_start_index", ")", "\n", "", "return", "self", ".", "cat_token", ",", "answer_start_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.regex_match_and_get_span": [[287, 313], ["list", "re.compile", "re.compile.finditer", "re.escape", "list", "NotImplemented", "re.compile", "re.compile.finditer"], "function", ["None"], ["", "", "def", "regex_match_and_get_span", "(", "text", ",", "pattern", ",", "type", "=", "'regex'", ")", ":", "\n", "    ", "\"\"\"Test if a regex pattern is contained within a text.\"\"\"", "\n", "if", "type", "==", "'regex'", ":", "\n", "        ", "try", ":", "\n", "            ", "pattern", "=", "re", ".", "compile", "(", "\n", "pattern", ",", "\n", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ",", "\n", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "return", "[", "]", "\n", "", "search_r", "=", "list", "(", "pattern", ".", "finditer", "(", "text", ")", ")", "\n", "return", "search_r", "\n", "", "elif", "type", "==", "'string'", ":", "\n", "        ", "pattern", "=", "re", ".", "escape", "(", "pattern", ")", "# if we only match string, we match the string literals, not regex.", "\n", "# print(pattern)", "\n", "try", ":", "\n", "            ", "pattern", "=", "re", ".", "compile", "(", "\n", "pattern", ",", "\n", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ",", "\n", ")", "\n", "", "except", "BaseException", ":", "\n", "            ", "return", "[", "]", "\n", "", "search_r", "=", "list", "(", "pattern", ".", "finditer", "(", "text", ")", ")", "\n", "return", "search_r", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils._check_is_max_context": [[22, 57], ["enumerate", "min"], "function", ["None"], ["def", "_check_is_max_context", "(", "doc_spans", ",", "cur_span_index", ",", "position", ")", ":", "\n", "    ", "\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"", "\n", "\n", "# Because of the sliding window approach taken to scoring documents, a single", "\n", "# token can appear in multiple documents. E.g.", "\n", "#  Doc: the man went to the store and bought a gallon of milk", "\n", "#  Span A: the man went to the", "\n", "#  Span B: to the store and bought", "\n", "#  Span C: and bought a gallon of", "\n", "#  ...", "\n", "#", "\n", "# Now the word 'bought' will have two scores from spans B and C. We only", "\n", "# want to consider the score with \"maximum context\", which we define as", "\n", "# the *minimum* of its left and right context (the *sum* of left and", "\n", "# right context will always be the same, of course).", "\n", "#", "\n", "# In the example the maximum context for 'bought' would be span C since", "\n", "# it has 1 left context and 3 right context, while span B has 4 left context", "\n", "# and 0 right context.", "\n", "best_score", "=", "None", "\n", "best_span_index", "=", "None", "\n", "for", "(", "span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "if", "position", "<", "doc_span", ".", "start", ":", "\n", "            ", "continue", "\n", "", "if", "position", ">", "end", ":", "\n", "            ", "continue", "\n", "", "num_left_context", "=", "position", "-", "doc_span", ".", "start", "\n", "num_right_context", "=", "end", "-", "position", "\n", "score", "=", "min", "(", "num_left_context", ",", "num_right_context", ")", "+", "0.01", "*", "doc_span", ".", "length", "\n", "if", "best_score", "is", "None", "or", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_span_index", "=", "span_index", "\n", "\n", "", "", "return", "cur_span_index", "==", "best_span_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocssing_span_prediction_item": [[60, 156], ["span_prediction_task_utils.w_tokens2c_tokens", "collections.namedtuple", "enumerate", "len", "doc_spans.append", "min", "range", "str", "range", "dict", "ready_training_list.append", "len", "collections.namedtuple.", "len", "uuid.uuid4", "squad_utils._check_is_max_context"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_tokens2c_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils._check_is_max_context"], ["", "def", "preprocssing_span_prediction_item", "(", "item", ",", "tokenizer", ",", "is_training", ",", "\n", "max_tokens_for_doc", ",", "doc_stride", ")", ":", "\n", "    ", "ready_training_list", "=", "[", "]", "\n", "\n", "c_tokens", ",", "c2w_index", ",", "w2c_index", ",", "c_start_position", ",", "c_end_position", "=", "span_utils", ".", "w_tokens2c_tokens", "(", "\n", "item", "[", "'w_token_context'", "]", ",", "item", "[", "'answer_start'", "]", ",", "item", "[", "'answer_end'", "]", ",", "\n", "item", "[", "'gt_answer_text'", "]", ",", "tokenizer", ",", "item", "[", "'no_answer'", "]", ",", "is_training", "\n", ")", "\n", "\n", "no_answer", "=", "item", "[", "'no_answer'", "]", "\n", "\n", "_DocSpan", "=", "collections", ".", "namedtuple", "(", "\n", "\"DocSpan\"", ",", "[", "\"start\"", ",", "\"length\"", "]", ")", "\n", "doc_spans", "=", "[", "]", "\n", "start_offset", "=", "0", "\n", "while", "start_offset", "<", "len", "(", "c_tokens", ")", ":", "\n", "        ", "length", "=", "len", "(", "c_tokens", ")", "-", "start_offset", "\n", "if", "length", ">", "max_tokens_for_doc", ":", "\n", "            ", "length", "=", "max_tokens_for_doc", "\n", "", "doc_spans", ".", "append", "(", "_DocSpan", "(", "start", "=", "start_offset", ",", "length", "=", "length", ")", ")", "\n", "if", "start_offset", "+", "length", "==", "len", "(", "c_tokens", ")", ":", "\n", "            ", "break", "\n", "", "start_offset", "+=", "min", "(", "length", ",", "doc_stride", ")", "\n", "\n", "", "for", "(", "doc_span_index", ",", "doc_span", ")", "in", "enumerate", "(", "doc_spans", ")", ":", "\n", "        ", "start_position", "=", "None", "\n", "end_position", "=", "None", "\n", "\n", "fctoken_to_wtoken_map", "=", "{", "}", "# we need a map to the original w-tokens", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "# The index offset is 0.", "\n", "# the_index = doc_span.start + i", "\n", "            ", "fctoken_to_wtoken_map", "[", "i", "]", "=", "c2w_index", "[", "doc_span", ".", "start", "+", "i", "]", "\n", "\n", "", "if", "is_training", "and", "not", "item", "[", "'no_answer'", "]", ":", "\n", "# For training, if our document chunk does not contain an annotation", "\n", "# we throw it out, since there is nothing to predict.", "\n", "            ", "doc_start", "=", "doc_span", ".", "start", "\n", "doc_end", "=", "doc_span", ".", "start", "+", "doc_span", ".", "length", "-", "1", "\n", "out_of_span", "=", "False", "\n", "if", "not", "(", "c_start_position", ">=", "doc_start", "and", "c_end_position", "<=", "doc_end", ")", ":", "\n", "                ", "out_of_span", "=", "True", "\n", "", "if", "out_of_span", ":", "\n", "                ", "no_answer", "=", "True", "\n", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "", "else", ":", "\n", "# doc_offset = len(query_tokens) + 2", "\n", "                ", "doc_offset", "=", "0", "\n", "start_position", "=", "c_start_position", "-", "doc_start", "+", "doc_offset", "\n", "end_position", "=", "c_end_position", "-", "doc_start", "+", "doc_offset", "\n", "# We change this from original code base, here the doc_offset will only w.r.t. the context.", "\n", "\n", "", "", "if", "is_training", "and", "item", "[", "'no_answer'", "]", ":", "\n", "            ", "no_answer", "=", "True", "\n", "start_position", "=", "-", "1", "\n", "end_position", "=", "-", "1", "\n", "\n", "", "if", "not", "is_training", ":", "\n", "# no_answer = True", "\n", "            ", "start_position", "=", "-", "2", "# -2 indicates that the start and end position is hidden.", "\n", "end_position", "=", "-", "2", "\n", "\n", "", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "# This is unique forward id for each of the doc that will be feeded into net.", "\n", "uid", "=", "item", "[", "'uid'", "]", "\n", "\n", "# This the the code from Google BERT where we keep track of the score of current token.", "\n", "# This is only for sliding window approach.", "\n", "token_is_max_context", "=", "{", "}", "\n", "\n", "# We do this for all the document. Bc this is not that time-consuming. 2019-03-14", "\n", "# if not is_training:     # We only need to do this during inference  # But maybe not. Check laters", "\n", "for", "i", "in", "range", "(", "doc_span", ".", "length", ")", ":", "\n", "            ", "split_token_index", "=", "doc_span", ".", "start", "+", "i", "\n", "is_max_context", "=", "_check_is_max_context", "(", "doc_spans", ",", "doc_span_index", ",", "\n", "split_token_index", ")", "\n", "token_is_max_context", "[", "i", "]", "=", "is_max_context", "\n", "\n", "", "fc_token", "=", "c_tokens", "[", "doc_span", ".", "start", ":", "doc_span", ".", "start", "+", "doc_span", ".", "length", "]", "# actual forward c-tokens", "\n", "f_item", "=", "dict", "(", ")", "\n", "f_item", "[", "'fid'", "]", "=", "fid", "\n", "f_item", "[", "'uid'", "]", "=", "uid", "\n", "f_item", "[", "'doc_span_index'", "]", "=", "doc_span_index", "\n", "f_item", "[", "'context_w_tokens'", "]", "=", "item", "[", "'w_token_context'", "]", "\n", "f_item", "[", "'context_c_tokens'", "]", "=", "fc_token", "\n", "f_item", "[", "'no_answer'", "]", "=", "no_answer", "\n", "f_item", "[", "'start_position'", "]", "=", "start_position", "\n", "f_item", "[", "'end_position'", "]", "=", "end_position", "\n", "f_item", "[", "'fctoken_to_wtoken_map'", "]", "=", "fctoken_to_wtoken_map", "\n", "f_item", "[", "'token_is_max_context'", "]", "=", "token_is_max_context", "\n", "\n", "# The item will not include query.", "\n", "ready_training_list", ".", "append", "(", "f_item", ")", "\n", "# each item in the list don't have query, we append the query latter outside the function.", "\n", "\n", "", "return", "ready_training_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems": [[159, 174], ["tqdm.tqdm", "squad_utils.preprocssing_span_prediction_item", "fitem_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocssing_span_prediction_item"], ["", "def", "eitems_to_fitems", "(", "eitem_list", ",", "tokenizer", ",", "is_training", ",", "max_tokens_for_doc", "=", "384", ",", "doc_stride", "=", "128", ")", ":", "\n", "    ", "fitem_dict", "=", "{", "}", "\n", "fitem_list", "=", "[", "]", "# The output of all fitems", "\n", "for", "item", "in", "tqdm", "(", "eitem_list", ")", ":", "\n", "        ", "f_items", "=", "preprocssing_span_prediction_item", "(", "item", ",", "tokenizer", ",", "is_training", ",", "\n", "max_tokens_for_doc", ",", "\n", "doc_stride", ")", "\n", "for", "fitem", "in", "f_items", ":", "\n", "            ", "query", "=", "item", "[", "'query'", "]", "\n", "fitem", "[", "'o_query'", "]", "=", "query", "\n", "fitem", "[", "'gt_answer_text'", "]", "=", "item", "[", "'gt_answer_text'", "]", "\n", "fitem_dict", "[", "fitem", "[", "'fid'", "]", "]", "=", "fitem", "\n", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "", "return", "fitem_dict", ",", "fitem_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocessing_squad": [[176, 241], ["span_prediction_task_utils.w_processing", "span_prediction_task_utils.pair_w_tokens_with_ground_truth_span", "dict", "d_list.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_processing", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.pair_w_tokens_with_ground_truth_span"], ["", "def", "preprocessing_squad", "(", "data_set", ",", "is_training", "=", "True", ")", ":", "\n", "    ", "article_list", "=", "data_set", "[", "'data'", "]", "\n", "d_list", "=", "[", "]", "\n", "for", "article", "in", "article_list", ":", "\n", "# print(article.keys()) # title, paragraphs", "\n", "        ", "title", "=", "article", "[", "'title'", "]", "\n", "for", "paragraph", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "# print(paragraph.keys())", "\n", "            ", "context", "=", "paragraph", "[", "'context'", "]", "# context, qas", "\n", "# Firstly, we do whitespace tokenization", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", "=", "span_utils", ".", "w_processing", "(", "context", ")", "\n", "for", "question", "in", "paragraph", "[", "'qas'", "]", ":", "\n", "                ", "uid", "=", "question", "[", "'id'", "]", "# id, question, answers", "\n", "query", "=", "question", "[", "'question'", "]", "\n", "answers", "=", "question", "[", "'answers'", "]", "\n", "no_answer", "=", "question", "[", "'is_impossible'", "]", "if", "'is_impossible'", "in", "question", "else", "False", "\n", "is_yes_no_question", "=", "False", "\n", "# Randomly, selection one answer", "\n", "if", "len", "(", "answers", ")", "<", "1", ":", "\n", "                    ", "assert", "no_answer", "\n", "# continue    # if there is no answer, we just ignore.", "\n", "\n", "", "if", "not", "no_answer", ":", "\n", "                    ", "answer", "=", "answers", "[", "0", "]", "# answer_start, text", "\n", "answer_start", "=", "answer", "[", "'answer_start'", "]", "\n", "answer_len", "=", "len", "(", "answer", "[", "'text'", "]", ")", "\n", "answer_text", "=", "answer", "[", "'text'", "]", "\n", "answer_end", "=", "answer_start", "+", "answer_len", "\n", "", "else", ":", "\n", "                    ", "answer", "=", "None", "\n", "answer_start", "=", "-", "1", "\n", "answer_end", "=", "-", "1", "\n", "answer_len", "=", "0", "\n", "answer_text", "=", "\"\"", "\n", "# no_answer = is_impossible", "\n", "\n", "", "_", ",", "adjusted_answer_start", ",", "adjusted_answer_end", "=", "span_utils", ".", "pair_w_tokens_with_ground_truth_span", "(", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", ",", "\n", "answer_text", ",", "answer_start", ",", "no_answer", ",", "is_yes_no_question", ",", "is_training", "\n", ")", "# The start and end are both inclusive.", "\n", "\n", "if", "adjusted_answer_start", "is", "None", "and", "not", "no_answer", ":", "\n", "                    ", "continue", "\n", "# The question should be answerable but", "\n", "# we can not find answer span even from whitespace tokenized sequence, there might be a problem", "\n", "\n", "", "if", "adjusted_answer_start", "==", "-", "1", ":", "\n", "                    ", "no_answer", "=", "True", "\n", "# print(adjusted_answer_start, adjusted_answer_end)", "\n", "\n", "# Then we have what we need.", "\n", "", "item", "=", "dict", "(", ")", "\n", "item", "[", "'article_title'", "]", "=", "title", "\n", "item", "[", "'uid'", "]", "=", "uid", "\n", "item", "[", "'w_token_context'", "]", "=", "context_w_tokens", "\n", "item", "[", "'gt_answer_text'", "]", "=", "answer_text", "\n", "item", "[", "'query'", "]", "=", "query", "\n", "item", "[", "'answer_start'", "]", "=", "adjusted_answer_start", "\n", "item", "[", "'answer_end'", "]", "=", "adjusted_answer_end", "\n", "item", "[", "'no_answer'", "]", "=", "no_answer", "\n", "item", "[", "'is_yes_no_question'", "]", "=", "is_yes_no_question", "\n", "\n", "d_list", ".", "append", "(", "item", ")", "\n", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.get_squad_question_selection_forward_list": [[243, 273], ["dict", "str", "str", "d_list.append", "uuid.uuid4"], "function", ["None"], ["", "def", "get_squad_question_selection_forward_list", "(", "data_set", ")", ":", "\n", "    ", "article_list", "=", "data_set", "[", "'data'", "]", "\n", "d_list", "=", "[", "]", "\n", "for", "article", "in", "article_list", ":", "\n", "# print(article.keys()) # title, paragraphs", "\n", "        ", "title", "=", "article", "[", "'title'", "]", "\n", "for", "paragraph", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "# print(paragraph.keys())", "\n", "            ", "context", "=", "paragraph", "[", "'context'", "]", "# context, qas", "\n", "# Firstly, we do whitespace tokenization", "\n", "for", "question", "in", "paragraph", "[", "'qas'", "]", ":", "\n", "                ", "uid", "=", "question", "[", "'id'", "]", "# id, question, answers", "\n", "query", "=", "question", "[", "'question'", "]", "\n", "answers", "=", "question", "[", "'answers'", "]", "\n", "no_answer", "=", "question", "[", "'is_impossible'", "]", "if", "'is_impossible'", "in", "question", "else", "False", "\n", "is_yes_no_question", "=", "False", "\n", "# Randomly, selection one answer", "\n", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "str", "(", "uid", ")", "# query id", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "# forward id", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "\n", "fitem", "[", "'context'", "]", "=", "context", "\n", "fitem", "[", "'element'", "]", "=", "\"Not used\"", "\n", "fitem", "[", "'s_labels'", "]", "=", "'true'", "if", "not", "no_answer", "else", "'false'", "\n", "\n", "d_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.get_squad_question_answer_list": [[275, 304], ["dict", "dict", "str", "str", "d_list.append", "uuid.uuid4", "str"], "function", ["None"], ["", "def", "get_squad_question_answer_list", "(", "data_set", ")", ":", "\n", "    ", "article_list", "=", "data_set", "[", "'data'", "]", "\n", "d_list", "=", "[", "]", "\n", "random_dict", "=", "dict", "(", ")", "\n", "for", "article", "in", "article_list", ":", "\n", "# print(article.keys()) # title, paragraphs", "\n", "        ", "title", "=", "article", "[", "'title'", "]", "\n", "for", "paragraph", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "# print(paragraph.keys())", "\n", "            ", "context", "=", "paragraph", "[", "'context'", "]", "# context, qas", "\n", "# Firstly, we do whitespace tokenization", "\n", "for", "question", "in", "paragraph", "[", "'qas'", "]", ":", "\n", "                ", "uid", "=", "question", "[", "'id'", "]", "# id, question, answers", "\n", "query", "=", "question", "[", "'question'", "]", "\n", "answers", "=", "question", "[", "'answers'", "]", "\n", "no_answer", "=", "question", "[", "'is_impossible'", "]", "if", "'is_impossible'", "in", "question", "else", "False", "\n", "is_yes_no_question", "=", "False", "\n", "# Randomly, selection one answer", "\n", "answer_text", "=", "answers", "[", "0", "]", "[", "'text'", "]", "\n", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "str", "(", "uid", ")", "# query id", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'answers'", "]", "=", "answer_text", "\n", "\n", "random_dict", "[", "str", "(", "uid", ")", "]", "=", "answer_text", "\n", "\n", "d_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "", "", "return", "d_list", ",", "random_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.format_convert": [[22, 57], ["dict", "span_prediction_task_utils.w_processing", "span_prediction_task_utils.pair_w_tokens_with_ground_truth_span", "ready_items.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_processing", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.pair_w_tokens_with_ground_truth_span"], ["def", "format_convert", "(", "items", ",", "is_training", ")", ":", "\n", "    ", "ready_items", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "ready_item", "=", "dict", "(", ")", "\n", "answer_text", "=", "item", "[", "'answer'", "]", "\n", "answer_start", "=", "item", "[", "'answer_start_index'", "]", "\n", "no_answer", "=", "item", "[", "'no_answer'", "]", "\n", "context", "=", "item", "[", "'context'", "]", "\n", "if", "context", "is", "None", "or", "len", "(", "context", ")", "==", "0", ":", "\n", "            ", "context", "=", "'empty'", "\n", "", "is_yes_no_question", "=", "item", "[", "'is_yes_no_question'", "]", "\n", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", "=", "span_utils", ".", "w_processing", "(", "context", ")", "\n", "_", ",", "adjusted_answer_start", ",", "adjusted_answer_end", "=", "span_utils", ".", "pair_w_tokens_with_ground_truth_span", "(", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", ",", "\n", "answer_text", ",", "answer_start", ",", "no_answer", ",", "is_yes_no_question", ",", "is_training", ",", "do_checking", "=", "False", "\n", ")", "\n", "\n", "ready_item", "[", "'uid'", "]", "=", "item", "[", "'qid'", "]", "\n", "ready_item", "[", "'w_token_context'", "]", "=", "context_w_tokens", "\n", "ready_item", "[", "'gt_answer_text'", "]", "=", "answer_text", "\n", "ready_item", "[", "'query'", "]", "=", "item", "[", "'query'", "]", "\n", "ready_item", "[", "'answer_start'", "]", "=", "adjusted_answer_start", "\n", "ready_item", "[", "'answer_end'", "]", "=", "adjusted_answer_end", "\n", "ready_item", "[", "'no_answer'", "]", "=", "no_answer", "\n", "ready_item", "[", "'is_yes_no_question'", "]", "=", "is_yes_no_question", "\n", "\n", "if", "'additional_fields'", "in", "item", ":", "\n", "            ", "ready_item", "[", "'additional_fields'", "]", "=", "item", "[", "'additional_fields'", "]", "\n", "\n", "", "ready_items", ".", "append", "(", "ready_item", ")", "\n", "\n", "", "assert", "len", "(", "ready_items", ")", "==", "len", "(", "items", ")", "\n", "\n", "return", "ready_items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.build_open_qa_forword_item": [[59, 142], ["dict", "build_rindex.raw_text_db.query_raw_text", "len", "json.loads", "span_prediction_task_utils.span_match_and_label.regex_match_and_get_span", "dict", "dict", "o_forward_items.append", "len", "answer.lower", "random.choice", "answer.lower", "len", "len", "random.choice", "random.choice.span", "random.choice.span", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.query_raw_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.regex_match_and_get_span"], ["", "def", "build_open_qa_forword_item", "(", "p_level_results", ",", "d_list", ",", "is_training", ",", "db_cursor", ",", "match_type", ")", ":", "\n", "    ", "o_forward_items", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "qid", "=", "item", "[", "'question'", "]", "\n", "query", "=", "item", "[", "'question'", "]", "\n", "# o_contexts = item['context']", "\n", "answers", "=", "item", "[", "'answers'", "]", "if", "'answers'", "in", "item", "else", "\"##hidden##\"", "\n", "# q_type = item['']", "\n", "\n", "retrieved_paragraph_indices", "=", "p_level_results", "[", "'pred_p_list'", "]", "[", "qid", "]", "# remember the key here.", "\n", "selected_scored_paragraph", "=", "p_level_results", "[", "'selected_scored_results'", "]", "[", "qid", "]", "\n", "\n", "scored_p_dict", "=", "dict", "(", ")", "\n", "for", "cur_score", ",", "(", "title", ",", "p_num", ")", "in", "selected_scored_paragraph", ":", "\n", "            ", "scored_p_dict", "[", "(", "title", ",", "p_num", ")", "]", "=", "cur_score", "\n", "\n", "", "for", "p_title", ",", "p_num", "in", "retrieved_paragraph_indices", ":", "\n", "# Loop for paragraph", "\n", "            ", "p_list", "=", "raw_text_db", ".", "query_raw_text", "(", "db_cursor", ",", "p_title", ",", "p_num", "=", "p_num", ")", "\n", "assert", "len", "(", "p_list", ")", "==", "1", "\n", "std_title", ",", "p_num", ",", "p_sentences", "=", "p_list", "[", "0", "]", "\n", "paragraph_text", "=", "' '", ".", "join", "(", "json", ".", "loads", "(", "p_sentences", ")", ")", "\n", "\n", "context", "=", "paragraph_text", "\n", "\n", "for", "answer", "in", "answers", ":", "\n", "                ", "if", "len", "(", "answer", ")", "<=", "2", ":", "\n", "# print(answer)", "\n", "# print(len(answer))", "\n", "                    ", "continue", "\n", "", "answer_start_list", "=", "regex_match_and_get_span", "(", "paragraph_text", ",", "answer", ",", "type", "=", "match_type", ")", "\n", "\n", "example_item", "=", "dict", "(", ")", "\n", "example_item", "[", "'query'", "]", "=", "query", "\n", "# if query == 'If Roman numerals were used, what would Super Bowl 50 have been called?':", "\n", "#     print(\"Found\")", "\n", "\n", "example_item", "[", "'qid'", "]", "=", "qid", "\n", "example_item", "[", "'context'", "]", "=", "context", "\n", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "\n", "example_item", "[", "'answer'", "]", "=", "answer", "\n", "if", "is_training", ":", "\n", "# Yes or no question:", "\n", "                    ", "if", "answer", ".", "lower", "(", ")", "==", "'yes'", ":", "\n", "                        ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "2", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "True", "\n", "", "elif", "answer", ".", "lower", "(", ")", "==", "'no'", ":", "\n", "                        ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "3", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "True", "\n", "", "elif", "len", "(", "answer_start_list", ")", ">", "0", ":", "\n", "                        ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "random_selected_index", "=", "random", ".", "choice", "(", "answer_start_list", ")", "\n", "start", "=", "random_selected_index", ".", "span", "(", ")", "[", "0", "]", "\n", "end", "=", "random_selected_index", ".", "span", "(", ")", "[", "1", "]", "\n", "new_answer", "=", "context", "[", "start", ":", "end", "]", "\n", "# print(context[start:end], answers)", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "start", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "example_item", "[", "'answer'", "]", "=", "new_answer", "\n", "", "elif", "len", "(", "answer_start_list", ")", "==", "0", ":", "\n", "                        ", "example_item", "[", "'no_answer'", "]", "=", "True", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "1", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "", "", "else", ":", "\n", "                    ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "random", ".", "choice", "(", "answer_start_list", ")", "if", "len", "(", "\n", "answer_start_list", ")", ">", "0", "else", "-", "4", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "\n", "", "additional_fields", "=", "dict", "(", ")", "\n", "# Important: Remember the field value!", "\n", "# (score, (p_title, p_num))", "\n", "additional_fields", "[", "'p_level_scored_results'", "]", "=", "[", "scored_p_dict", "[", "(", "p_title", ",", "p_num", ")", "]", ",", "[", "p_title", ",", "p_num", "]", "]", "\n", "\n", "example_item", "[", "'additional_fields'", "]", "=", "additional_fields", "\n", "\n", "o_forward_items", ".", "append", "(", "example_item", ")", "\n", "\n", "", "", "", "return", "o_forward_items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.get_open_qa_item_with_upstream_paragraphs": [[144, 173], ["wiki_util.wiki_db_tool.get_cursor", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "qa_sampler.build_open_qa_forword_item", "qa_sampler.format_convert", "span_prediction_task_utils.span_preprocess_tool.eitems_to_fitems", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.build_open_qa_forword_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.format_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems"], ["", "def", "get_open_qa_item_with_upstream_paragraphs", "(", "d_list", ",", "cur_eval_results_list", ",", "is_training", ",", "\n", "tokenizer", ":", "BertTokenizer", ",", "max_context_length", ",", "max_query_length", ",", "\n", "doc_stride", "=", "128", ",", "\n", "debug_mode", "=", "False", ",", "top_k", "=", "10", ",", "filter_value", "=", "0.1", ",", "match_type", "=", "'string'", ")", ":", "\n", "    ", "t_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_RAW_TEXT", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'question'", "]", "for", "item", "in", "d_list", "]", ")", "\n", "cur_eval_results_list", "=", "[", "item", "for", "item", "in", "cur_eval_results_list", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "d_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'question'", ")", "\n", "copied_d_o_dict", "=", "copy", ".", "deepcopy", "(", "d_o_dict", ")", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_d_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top10", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_d_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "top_k", ",", "filter_value", "=", "filter_value", ")", "\n", "\n", "forward_example_items", "=", "build_open_qa_forword_item", "(", "cur_results_dict_top10", ",", "d_list", ",", "is_training", ",", "t_cursor", ",", "\n", "match_type", ")", "\n", "forward_example_items", "=", "format_convert", "(", "forward_example_items", ",", "is_training", ")", "\n", "fitems_dict", ",", "read_fitems_list", "=", "span_preprocess_tool", ".", "eitems_to_fitems", "(", "forward_example_items", ",", "tokenizer", ",", "is_training", ",", "\n", "max_context_length", ",", "\n", "max_query_length", ",", "doc_stride", ",", "False", ")", "\n", "\n", "return", "fitems_dict", ",", "read_fitems_list", ",", "cur_results_dict_top10", "[", "'pred_p_list'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.inspect_upstream_eval": [[175, 205], ["utils.common.load_jsonl", "utils.common.load_jsonl", "wiki_util.wiki_db_tool.get_cursor", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "qa_sampler.build_open_qa_forword_item", "print", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.build_open_qa_forword_item"], ["", "def", "inspect_upstream_eval", "(", ")", ":", "\n", "    ", "is_training", "=", "True", "\n", "debug_mode", "=", "True", "\n", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_SQUAD_DEV_GT", ")", "\n", "in_file_name", "=", "config", ".", "PRO_ROOT", "/", "'saved_models/05-12-08:44:38_mtr_open_qa_p_level_(num_train_epochs:3)/i(2000)|e(2)|squad|top10(0.6909176915799432)|top20(0.7103122043519394)|seed(12)_eval_results.jsonl'", "\n", "cur_eval_results_list", "=", "common", ".", "load_jsonl", "(", "in_file_name", ")", "\n", "top_k", "=", "10", "\n", "filter_value", "=", "0.1", "\n", "t_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_RAW_TEXT", ")", "\n", "match_type", "=", "'string'", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'question'", "]", "for", "item", "in", "d_list", "]", ")", "\n", "cur_eval_results_list", "=", "[", "item", "for", "item", "in", "cur_eval_results_list", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "d_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'question'", ")", "\n", "copied_d_o_dict", "=", "copy", ".", "deepcopy", "(", "d_o_dict", ")", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_d_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top10", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_d_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "top_k", ",", "filter_value", "=", "filter_value", ")", "\n", "\n", "forward_example_items", "=", "build_open_qa_forword_item", "(", "cur_results_dict_top10", ",", "d_list", ",", "is_training", ",", "t_cursor", ",", "\n", "match_type", ")", "\n", "\n", "print", "(", "forward_example_items", ")", "\n", "# print(len(cur_results_dict_top10))", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.inspect_upstream_eval_v1": [[215, 241], ["utils.common.load_jsonl", "utils.common.load_jsonl", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "qa_sampler.get_open_qa_item_with_upstream_paragraphs", "print", "print", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.get_open_qa_item_with_upstream_paragraphs"], ["", "def", "inspect_upstream_eval_v1", "(", ")", ":", "\n", "    ", "bert_model_name", "=", "\"bert-base-uncased\"", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "do_lower_case", "=", "True", "\n", "\n", "max_pre_context_length", "=", "315", "\n", "max_query_length", "=", "64", "\n", "doc_stride", "=", "128", "\n", "\n", "is_training", "=", "True", "\n", "debug_mode", "=", "True", "\n", "\n", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_SQUAD_DEV_GT", ")", "\n", "in_file_name", "=", "config", ".", "PRO_ROOT", "/", "'saved_models/05-12-08:44:38_mtr_open_qa_p_level_(num_train_epochs:3)/i(2000)|e(2)|squad|top10(0.6909176915799432)|top20(0.7103122043519394)|seed(12)_eval_results.jsonl'", "\n", "cur_eval_results_list", "=", "common", ".", "load_jsonl", "(", "in_file_name", ")", "\n", "top_k", "=", "10", "\n", "filter_value", "=", "0.1", "\n", "match_type", "=", "'string'", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "\n", "fitems_dict", ",", "read_fitems_list", ",", "_", "=", "get_open_qa_item_with_upstream_paragraphs", "(", "d_list", ",", "cur_eval_results_list", ",", "is_training", ",", "\n", "tokenizer", ",", "max_pre_context_length", ",", "max_query_length", ",", "doc_stride", ",", "\n", "debug_mode", ",", "top_k", ",", "filter_value", ",", "match_type", ")", "\n", "print", "(", "len", "(", "read_fitems_list", ")", ")", "\n", "print", "(", "len", "(", "fitems_dict", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.qa_sampler.inspect_sampler_squad_examples": [[243, 263], ["pytorch_pretrained_bert.BertTokenizer.from_pretrained", "utils.common.load_json", "span_prediction_task_utils.squad_utils.preprocessing_squad", "span_prediction_task_utils.span_preprocess_tool.eitems_to_fitems", "print", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.preprocessing_squad", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems"], ["", "def", "inspect_sampler_squad_examples", "(", ")", ":", "\n", "    ", "bert_model_name", "=", "\"bert-base-uncased\"", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "do_lower_case", "=", "True", "\n", "max_pre_context_length", "=", "315", "\n", "max_query_length", "=", "64", "\n", "doc_stride", "=", "128", "\n", "debug", "=", "True", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "\n", "squad_train_v2", "=", "common", ".", "load_json", "(", "config", ".", "SQUAD_TRAIN_2_0", ")", "\n", "\n", "train_eitem_list", "=", "preprocessing_squad", "(", "squad_train_v2", ")", "\n", "train_fitem_dict", ",", "train_fitem_list", "=", "eitems_to_fitems", "(", "train_eitem_list", ",", "tokenizer", ",", "is_training", "=", "False", ",", "\n", "max_tokens_for_doc", "=", "max_pre_context_length", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "debug", "=", "debug", ")", "\n", "print", "(", "len", "(", "train_fitem_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict": [[3, 35], ["scored_dict.items", "dict", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append", "[].append", "[].append", "len"], "function", ["None"], ["def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "None", ",", "result_field", "=", "'pred_p_list'", ",", "\n", "result_scored_field", "=", "'scored_results'", ",", "\n", "selected_scored_field", "=", "'selected_scored_results'", ")", ":", "\n", "\n", "    ", "results_dict", "=", "{", "result_field", ":", "dict", "(", ")", ",", "result_scored_field", ":", "dict", "(", ")", ",", "selected_scored_field", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "result_scored_field", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "]", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", "=", "[", "]", "\n", "if", "filter_value", "is", "None", ":", "\n", "            ", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", "=", "[", "(", "s", ",", "e", ")", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "", "else", ":", "\n", "            ", "for", "s", ",", "e", "in", "sorted_e_list", ":", "\n", "                ", "if", "s", ">=", "filter_value", ":", "\n", "                    ", "results_dict", "[", "result_field", "]", "[", "key", "]", ".", "append", "(", "e", ")", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", ".", "append", "(", "(", "s", ",", "e", ")", ")", "\n", "", "if", "len", "(", "results_dict", "[", "result_field", "]", "[", "key", "]", ")", "==", "top_k", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "", "return", "results_dict", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.top_k_filter_score_list": [[18, 29], ["dict", "new_d_list.append"], "function", ["None"], ["def", "top_k_filter_score_list", "(", "d_list", ",", "top_k", ")", ":", "\n", "    ", "new_d_list", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "new_item", "=", "dict", "(", ")", "\n", "new_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "new_item", "[", "'qid'", "]", "=", "item", "[", "'question'", "]", "\n", "new_item", "[", "'answer'", "]", "=", "item", "[", "'answer'", "]", "\n", "new_item", "[", "'score_list'", "]", "=", "item", "[", "'score_list'", "]", "[", ":", "top_k", "]", "\n", "new_d_list", ".", "append", "(", "new_item", ")", "\n", "\n", "", "return", "new_d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.build_p_level_forward_item": [[31, 89], ["print", "tqdm.tqdm", "forward_item_list.extend", "set", "set", "dict", "str", "str", "build_rindex.raw_text_db.query_raw_text", "fitem_list.append", "uuid.uuid4", "len", "json.loads", "set.add", "total_paragraph.append", "set.add", "total_paragraph.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.raw_text_db.query_raw_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "build_p_level_forward_item", "(", "p_level_results_dict", ",", "distant_gt_dict", ",", "data_list", ",", "is_training", ",", "db_cursor", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "cur_id", "=", "item", "[", "'question'", "]", "\n", "query", "=", "item", "[", "'question'", "]", "\n", "selected_paragraph", "=", "p_level_results_dict", "[", "cur_id", "]", "[", "'score_list'", "]", "\n", "\n", "# gt_paragraph = gt_dict[cur_id]['score_list']", "\n", "distant_gt_paragraph", "=", "distant_gt_dict", "[", "cur_id", "]", "[", "'distant_gt_list'", "]", "\n", "distant_gt_paragraph_set", "=", "{", "(", "title", ",", "p_num", ")", "for", "(", "title", ",", "p_num", ")", ",", "score", "in", "distant_gt_paragraph", "}", "\n", "\n", "if", "is_training", ":", "# If is training, we give distant_gt_list", "\n", "            ", "total_paragraph", "=", "[", "]", "\n", "added_set", "=", "set", "(", ")", "\n", "for", "(", "title", ",", "p_num", ")", ",", "score", "in", "selected_paragraph", "+", "distant_gt_paragraph", ":", "\n", "                ", "if", "(", "title", ",", "p_num", ")", "not", "in", "added_set", ":", "\n", "                    ", "added_set", ".", "add", "(", "(", "title", ",", "p_num", ")", ")", "\n", "total_paragraph", ".", "append", "(", "(", "title", ",", "p_num", ")", ")", "\n", "\n", "", "", "", "else", ":", "# Else we only have selected_paragraph", "\n", "            ", "total_paragraph", "=", "[", "]", "\n", "added_set", "=", "set", "(", ")", "\n", "for", "(", "title", ",", "p_num", ")", ",", "score", "in", "selected_paragraph", ":", "\n", "                ", "if", "(", "title", ",", "p_num", ")", "not", "in", "added_set", ":", "\n", "                    ", "added_set", ".", "add", "(", "(", "title", ",", "p_num", ")", ")", "\n", "total_paragraph", ".", "append", "(", "(", "title", ",", "p_num", ")", ")", "\n", "\n", "", "", "", "fitem_list", "=", "[", "]", "\n", "\n", "for", "title", ",", "p_num", "in", "total_paragraph", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "str", "(", "cur_id", ")", "# query id", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "# forward id", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "\n", "p_list", "=", "raw_text_db", ".", "query_raw_text", "(", "db_cursor", ",", "title", ",", "p_num", "=", "p_num", ")", "\n", "assert", "len", "(", "p_list", ")", "==", "1", "\n", "std_title", ",", "p_num", ",", "p_sentences", "=", "p_list", "[", "0", "]", "\n", "paragraph_text", "=", "' '", ".", "join", "(", "json", ".", "loads", "(", "p_sentences", ")", ")", "\n", "\n", "fitem", "[", "'context'", "]", "=", "paragraph_text", "\n", "fitem", "[", "'element'", "]", "=", "(", "title", ",", "p_num", ")", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "(", "title", ",", "p_num", ")", "in", "distant_gt_paragraph_set", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.down_sample_neg": [[96, 136], ["print", "random.shuffle", "random.shuffle", "int", "print", "random.shuffle", "print", "pos_items.append", "neg_items.append", "other_items.append", "len"], "function", ["None"], ["", "def", "down_sample_neg", "(", "fitems", ",", "ratio", "=", "None", ")", ":", "\n", "    ", "pos_count", "=", "0", "\n", "neg_count", "=", "0", "\n", "other_count", "=", "0", "\n", "\n", "pos_items", "=", "[", "]", "\n", "neg_items", "=", "[", "]", "\n", "other_items", "=", "[", "]", "\n", "\n", "for", "item", "in", "fitems", ":", "\n", "        ", "if", "item", "[", "'s_labels'", "]", "==", "'true'", ":", "\n", "            ", "pos_count", "+=", "1", "\n", "pos_items", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "'s_labels'", "]", "==", "'false'", ":", "\n", "            ", "neg_count", "+=", "1", "\n", "neg_items", ".", "append", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "other_count", "+=", "1", "\n", "other_items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "other_count", "!=", "0", ":", "\n", "        ", "print", "(", "\"Potential Error! We have labels that are not true or false:\"", ",", "other_count", ")", "\n", "\n", "", "print", "(", "f\"Before Sampling, we have {pos_count}/{neg_count} (pos/neg).\"", ")", "\n", "\n", "if", "ratio", "is", "None", ":", "\n", "        ", "return", "fitems", "\n", "\n", "", "random", ".", "shuffle", "(", "pos_items", ")", "\n", "random", ".", "shuffle", "(", "neg_items", ")", "\n", "neg_sample_count", "=", "int", "(", "pos_count", "/", "ratio", ")", "\n", "\n", "sampled_neg", "=", "neg_items", "[", ":", "neg_sample_count", "]", "\n", "\n", "print", "(", "f\"After Sampling, we have {pos_count}/{len(sampled_neg)} (pos/neg).\"", ")", "\n", "\n", "sampled_list", "=", "sampled_neg", "+", "pos_items", "\n", "random", ".", "shuffle", "(", "sampled_list", ")", "\n", "\n", "return", "sampled_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.get_distant_top_k_ground_truth": [[138, 160], ["dict", "distant_gt_item_list.append", "distant_gt_list.append", "len"], "function", ["None"], ["", "def", "get_distant_top_k_ground_truth", "(", "gt_dict", ",", "d_list", ",", "top_k", ")", ":", "\n", "    ", "distant_gt_item_list", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "qid", "=", "item", "[", "'question'", "]", "\n", "score_list", "=", "item", "[", "'score_list'", "]", "\n", "gt_list", "=", "gt_dict", "[", "qid", "]", "[", "'gt_p_list'", "]", "\n", "distant_gt_list", "=", "[", "]", "\n", "for", "para_item", ",", "score", "in", "score_list", ":", "\n", "            ", "if", "para_item", "in", "gt_list", ":", "\n", "                ", "distant_gt_list", ".", "append", "(", "[", "para_item", ",", "score", "]", ")", "\n", "\n", "", "if", "len", "(", "distant_gt_list", ")", ">=", "top_k", ":", "\n", "                ", "break", "\n", "\n", "", "", "new_item", "=", "dict", "(", ")", "\n", "new_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "new_item", "[", "'qid'", "]", "=", "new_item", "[", "'question'", "]", "\n", "new_item", "[", "'answer'", "]", "=", "item", "[", "'answer'", "]", "\n", "new_item", "[", "'distant_gt_list'", "]", "=", "distant_gt_list", "\n", "distant_gt_item_list", ".", "append", "(", "new_item", ")", "\n", "\n", "", "return", "distant_gt_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data": [[162, 214], ["wiki_util.wiki_db_tool.get_cursor", "p_sampler.top_k_filter_score_list", "utils.list_dict_data_tool.list_to_dict", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "p_sampler.get_distant_top_k_ground_truth", "utils.list_dict_data_tool.list_to_dict", "p_sampler.build_p_level_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "p_sampler.down_sample_neg", "p_sampler.down_sample_neg", "NotImplemented"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.top_k_filter_score_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.get_distant_top_k_ground_truth", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.build_p_level_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg"], ["", "def", "prepare_forward_data", "(", "dataset_name", ",", "tag", ",", "is_training", ",", "upstream_top_k", "=", "20", ",", "distant_gt_top_k", "=", "2", ",", "down_sample_ratio", "=", "None", ",", "\n", "debug", "=", "False", ")", ":", "\n", "    ", "if", "dataset_name", "==", "'webq'", "and", "tag", "==", "'test'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_WEBQ_TEST_GT", "\n", "", "elif", "dataset_name", "==", "'webq'", "and", "tag", "==", "'train'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_WEBQ_TRAIN_GT", "\n", "", "elif", "dataset_name", "==", "'curatedtrec'", "and", "tag", "==", "'test'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_CURATEDTERC_TEST_GT", "\n", "", "elif", "dataset_name", "==", "'curatedtrec'", "and", "tag", "==", "'train'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_CURATEDTERC_TRAIN_GT", "\n", "", "elif", "dataset_name", "==", "'squad'", "and", "tag", "==", "'dev'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_SQUAD_DEV_GT", "\n", "", "elif", "dataset_name", "==", "'squad'", "and", "tag", "==", "'train'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_SQUAD_TRAIN_GT", "\n", "", "elif", "dataset_name", "==", "'wikimovie'", "and", "tag", "==", "'test'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_WIKIM_TEST_GT", "\n", "", "elif", "dataset_name", "==", "'wikimovie'", "and", "tag", "==", "'train'", ":", "\n", "        ", "gt_d_list_path", "=", "config", ".", "OPEN_WIKIM_TRAIN_GT", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n", "", "t_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_WIKI_RAW_TEXT", ")", "\n", "# debug = False", "\n", "# upstream_top_k = 20", "\n", "# distant_gt_top_k = 2", "\n", "# down_sample_ratio = None", "\n", "\n", "if", "dataset_name", "!=", "'wikimovie'", ":", "\n", "        ", "upstream_d_list_before_filter", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "f\"data/p_{dataset_name}/tf_idf_p_level/{dataset_name}_{tag}_para_tfidf.jsonl\"", ")", "\n", "", "else", ":", "\n", "        ", "upstream_d_list_before_filter", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "f\"data/p_{dataset_name}/kwm_p_level/{dataset_name}_{tag}_kwm_tfidf.jsonl\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "upstream_d_list_before_filter", "=", "upstream_d_list_before_filter", "[", ":", "50", "]", "\n", "", "upstream_d_list", "=", "top_k_filter_score_list", "(", "upstream_d_list_before_filter", ",", "top_k", "=", "upstream_top_k", ")", "\n", "\n", "upstream_d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "upstream_d_list", ",", "'question'", ")", "\n", "\n", "gt_d_list", "=", "common", ".", "load_jsonl", "(", "gt_d_list_path", ")", "\n", "gt_d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "gt_d_list", ",", "'question'", ")", "\n", "distant_gt_item_list", "=", "get_distant_top_k_ground_truth", "(", "gt_d_dict", ",", "upstream_d_list_before_filter", ",", "\n", "top_k", "=", "distant_gt_top_k", ")", "\n", "distant_gt_item_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "distant_gt_item_list", ",", "'qid'", ")", "\n", "\n", "fitems_list", "=", "build_p_level_forward_item", "(", "upstream_d_dict", ",", "distant_gt_item_dict", ",", "upstream_d_list", ",", "is_training", ",", "\n", "t_cursor", ")", "\n", "if", "is_training", ":", "\n", "        ", "return", "down_sample_neg", "(", "fitems_list", ",", "down_sample_ratio", ")", "\n", "", "else", ":", "\n", "        ", "return", "down_sample_neg", "(", "fitems_list", ",", "None", ")", "\n", "# return fitems_list", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_eval.nli_eval.eval_nli": [[11, 27], ["utils.common.load_jsonl", "utils.common.load_jsonl", "evaluation.fever_scorer.fever_score", "print", "evaluation.fever_scorer.fever_confusion_matrix"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_confusion_matrix"], ["def", "eval_nli", "(", ")", ":", "\n", "    ", "dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "# prediction_file = config.PRO_ROOT / \"data/p_fever/fever_nli/04-25-22:02:53_fever_v2_nli_th0.2/ema_i(20000)|e(3)|ss(0.7002700270027002)|ac(0.746024602460246)|pr(0.6141389138913633)|rec(0.8627362736273627)|f1(0.7175148212089147)|seed(12)/nli_dev_cp_results_th0.2.jsonl\"", "\n", "# prediction_file = config.PRO_ROOT / \"saved_models/04-15-00:15:59_fever_v1_nli/i(18000)|e(2)|ss(0.6154615461546155)|ac(0.6701170117011701)|pr(0.26657540754071885)|rec(0.8852385238523852)|f1(0.40975857963668794)|seed(12)_dev_nli_results.json\"", "\n", "prediction_file", "=", "config", ".", "PRO_ROOT", "/", "\"data/p_fever/non_sent_level/ema_i(32000)|e(4)|ss(0.5592059205920592)|ac(0.6104110411041104)|pr(0.2638851385138135)|rec(0.8928142814281428)|f1(0.4073667130110584)|seed(12)_dev_nli_results.json\"", "\n", "pred_list", "=", "common", ".", "load_jsonl", "(", "prediction_file", ")", "\n", "mode", "=", "{", "'standard'", ":", "True", "}", "\n", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_score", "(", "pred_list", ",", "dev_list", ",", "\n", "mode", "=", "mode", ",", "max_evidence", "=", "5", ")", "\n", "logging_item", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "'ac'", ":", "acc_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "fever_scorer", ".", "fever_confusion_matrix", "(", "pred_list", ",", "dev_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_eval.eval_p_level.p_eval": [[8, 38], ["utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only"], ["\n", "def", "eval_p_level", "(", ")", ":", "\n", "    ", "cur_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", "\n", ")", "\n", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "print", "(", "metrics_top5", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "eval_p_level", "(", ")", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_eval.eval_p_level.p_eval_term": [[40, 77], ["utils.common.load_jsonl", "utils.common.load_jsonl", "zip", "evaluation.fever_scorer.fever_doc_only", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only"], []], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique": [[10, 53], ["utils.common.load_jsonl", "dict", "print", "tqdm.tqdm", "len", "int", "sentence_selection_sampler.convert_evidence2scoring_format", "selection_id.split", "int", "augmented_dict[].values", "sorted", "selection_id.split", "print", "sorted.append", "int"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format"], ["def", "threshold_sampler_insure_unique", "(", "org_data_file", ",", "full_sent_list", ",", "prob_threshold", "=", "0.5", ",", "logist_threshold", "=", "None", ",", "top_n", "=", "5", ")", ":", "\n", "    ", "\"\"\"\n    Providing samples to the Training set by a probability threshold on the upstream selected sentences.\n    \"\"\"", "\n", "d_list", "=", "common", ".", "load_jsonl", "(", "org_data_file", ")", "\n", "augmented_dict", ":", "Dict", "[", "int", ",", "Dict", "[", "str", ",", "Dict", "]", "]", "=", "dict", "(", ")", "\n", "print", "(", "\"Build selected sentences file:\"", ",", "len", "(", "full_sent_list", ")", ")", "\n", "for", "sent_item", "in", "tqdm", "(", "full_sent_list", ")", ":", "\n", "        ", "selection_id", "=", "sent_item", "[", "'selection_id'", "]", "# The id for the current one selection.", "\n", "org_id", "=", "int", "(", "selection_id", ".", "split", "(", "'<##>'", ")", "[", "0", "]", ")", "\n", "remain_str", "=", "selection_id", ".", "split", "(", "'<##>'", ")", "[", "1", "]", "\n", "# doc_id = remain_str.split(c_scorer.SENT_LINE)[0]", "\n", "# ln = int(remain_str.split(c_scorer.SENT_LINE)[1])", "\n", "if", "org_id", "in", "augmented_dict", ":", "\n", "            ", "if", "remain_str", "not", "in", "augmented_dict", "[", "org_id", "]", ":", "\n", "                ", "augmented_dict", "[", "org_id", "]", "[", "remain_str", "]", "=", "sent_item", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"Exist\"", ")", "\n", "", "", "else", ":", "\n", "            ", "augmented_dict", "[", "org_id", "]", "=", "{", "remain_str", ":", "sent_item", "}", "\n", "\n", "", "", "for", "item", "in", "d_list", ":", "\n", "        ", "if", "int", "(", "item", "[", "'id'", "]", ")", "not", "in", "augmented_dict", ":", "\n", "# print(\"Potential error?\")", "\n", "            ", "cur_predicted_sentids", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "cur_predicted_sentids", "=", "[", "]", "# formating doc_id + c_score.SENTLINT + line_number", "\n", "sents", "=", "augmented_dict", "[", "int", "(", "item", "[", "'id'", "]", ")", "]", ".", "values", "(", ")", "\n", "# Modify some mechaism here to selection sentence whether by some score or label", "\n", "for", "sent_i", "in", "sents", ":", "\n", "                ", "if", "sent_i", "[", "'prob'", "]", ">=", "prob_threshold", ":", "\n", "                    ", "cur_predicted_sentids", ".", "append", "(", "(", "sent_i", "[", "'sid'", "]", ",", "sent_i", "[", "'score'", "]", ",", "\n", "sent_i", "[", "'prob'", "]", ")", ")", "# Important sentences for scaling training. Jul 21.", "\n", "# del sent_i['prob']", "\n", "\n", "", "", "cur_predicted_sentids", "=", "sorted", "(", "cur_predicted_sentids", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "\n", "", "item", "[", "'scored_sentids'", "]", "=", "cur_predicted_sentids", "[", ":", "top_n", "]", "# Important sentences for scaling training. Jul 21.", "\n", "item", "[", "'predicted_sentids'", "]", "=", "[", "sid", "for", "sid", ",", "_", ",", "_", "in", "item", "[", "'scored_sentids'", "]", "]", "[", ":", "top_n", "]", "\n", "item", "[", "'predicted_evidence'", "]", "=", "convert_evidence2scoring_format", "(", "item", "[", "'predicted_sentids'", "]", ")", "\n", "# item['predicted_label'] = item['label']  # give ground truth label", "\n", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.threshold_sampler_insure_unique_new_format": [[55, 103], ["utils.common.load_jsonl", "dict", "print", "tqdm.tqdm", "len", "sent_item[].replace", "int", "sentence_selection_sampler.convert_evidence2scoring_format", "int", "augmented_dict[].values", "sorted", "print", "sorted.append", "int"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format"], ["", "def", "threshold_sampler_insure_unique_new_format", "(", "org_data_file", ",", "full_sent_list", ",", "prob_threshold", "=", "0.5", ",", "\n", "logist_threshold", "=", "None", ",", "top_n", "=", "5", ")", ":", "\n", "    ", "\"\"\"\n    Providing samples to the Training set by a probability threshold on the upstream selected sentences.\n    \"\"\"", "\n", "d_list", "=", "common", ".", "load_jsonl", "(", "org_data_file", ")", "\n", "augmented_dict", ":", "Dict", "[", "int", ",", "Dict", "[", "str", ",", "Dict", "]", "]", "=", "dict", "(", ")", "\n", "print", "(", "\"Build selected sentences file:\"", ",", "len", "(", "full_sent_list", ")", ")", "\n", "for", "sent_item", "in", "tqdm", "(", "full_sent_list", ")", ":", "\n", "        ", "sent_item", "[", "'element'", "]", "=", "sent_item", "[", "'element'", "]", ".", "replace", "(", "SMILE_SEP", ",", "fever_scorer", ".", "SENT_LINE", ")", "\n", "\n", "sid", "=", "sent_item", "[", "'element'", "]", "# The id for the current one selection.", "\n", "fid", "=", "sent_item", "[", "'fid'", "]", "\n", "org_id", "=", "int", "(", "sent_item", "[", "'oid'", "]", ")", "\n", "# remain_str = selection_id.split('<##>')[1]", "\n", "# doc_id = remain_str.split(c_scorer.SENT_LINE)[0]", "\n", "# ln = int(remain_str.split(c_scorer.SENT_LINE)[1])", "\n", "if", "org_id", "in", "augmented_dict", ":", "\n", "            ", "if", "sid", "not", "in", "augmented_dict", "[", "org_id", "]", ":", "\n", "                ", "augmented_dict", "[", "org_id", "]", "[", "sid", "]", "=", "sent_item", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"Exist\"", ")", "\n", "", "", "else", ":", "\n", "            ", "augmented_dict", "[", "org_id", "]", "=", "{", "sid", ":", "sent_item", "}", "\n", "\n", "", "", "for", "item", "in", "d_list", ":", "\n", "        ", "if", "int", "(", "item", "[", "'id'", "]", ")", "not", "in", "augmented_dict", ":", "\n", "# print(\"Potential error?\")", "\n", "            ", "cur_predicted_sentids", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "cur_predicted_sentids", "=", "[", "]", "# formating doc_id + c_score.SENTLINT + line_number", "\n", "sents", "=", "augmented_dict", "[", "int", "(", "item", "[", "'id'", "]", ")", "]", ".", "values", "(", ")", "\n", "# Modify some mechaism here to selection sentence whether by some score or label", "\n", "for", "sent_i", "in", "sents", ":", "\n", "                ", "if", "sent_i", "[", "'prob'", "]", ">=", "prob_threshold", ":", "\n", "# cur_predicted_sentids.append((sent_i['sid'], sent_i['score'],", "\n", "#                               sent_i['prob']))  # Important sentences for scaling training. Jul 21.", "\n", "                    ", "cur_predicted_sentids", ".", "append", "(", "(", "sent_i", "[", "'element'", "]", ",", "sent_i", "[", "'prob'", "]", ")", ")", "# Important sentences for scaling training. Jul 21.", "\n", "# del sent_i['prob']", "\n", "\n", "", "", "cur_predicted_sentids", "=", "sorted", "(", "cur_predicted_sentids", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "\n", "", "item", "[", "'scored_sentids'", "]", "=", "cur_predicted_sentids", "[", ":", "top_n", "]", "# Important sentences for scaling training. Jul 21.", "\n", "item", "[", "'predicted_sentids'", "]", "=", "[", "sid", "for", "sid", ",", "_", "in", "item", "[", "'scored_sentids'", "]", "]", "[", ":", "top_n", "]", "\n", "item", "[", "'predicted_evidence'", "]", "=", "convert_evidence2scoring_format", "(", "item", "[", "'predicted_sentids'", "]", ")", "\n", "# item['predicted_label'] = item['label']  # give ground truth label", "\n", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format": [[105, 113], ["enumerate", "pred_evidence_list.append", "cur_e.split", "cur_e.split", "int"], "function", ["None"], ["", "def", "convert_evidence2scoring_format", "(", "predicted_sentids", ")", ":", "\n", "    ", "e_list", "=", "predicted_sentids", "\n", "pred_evidence_list", "=", "[", "]", "\n", "for", "i", ",", "cur_e", "in", "enumerate", "(", "e_list", ")", ":", "\n", "        ", "doc_id", "=", "cur_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "0", "]", "\n", "ln", "=", "cur_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "1", "]", "\n", "pred_evidence_list", ".", "append", "(", "[", "doc_id", ",", "int", "(", "ln", ")", "]", ")", "\n", "", "return", "pred_evidence_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format_smile": [[115, 123], ["enumerate", "pred_evidence_list.append", "cur_e.split", "cur_e.split", "int"], "function", ["None"], ["", "def", "convert_evidence2scoring_format_smile", "(", "predicted_sentids", ",", "SMILE_SEP", "=", "'(-.-)'", ")", ":", "\n", "    ", "e_list", "=", "predicted_sentids", "\n", "pred_evidence_list", "=", "[", "]", "\n", "for", "i", ",", "cur_e", "in", "enumerate", "(", "e_list", ")", ":", "\n", "        ", "doc_id", "=", "cur_e", ".", "split", "(", "SMILE_SEP", ")", "[", "0", "]", "\n", "ln", "=", "cur_e", ".", "split", "(", "SMILE_SEP", ")", "[", "1", "]", "\n", "pred_evidence_list", ".", "append", "(", "[", "doc_id", ",", "int", "(", "ln", ")", "]", ")", "\n", "", "return", "pred_evidence_list", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict": [[1, 33], ["scored_dict.items", "dict", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append", "[].append", "[].append", "len"], "function", ["None"], ["def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "None", ",", "result_field", "=", "'predicted_docids'", ",", "\n", "result_scored_field", "=", "'scored_results'", ",", "\n", "selected_scored_field", "=", "'selected_scored_results'", ")", ":", "\n", "\n", "    ", "results_dict", "=", "{", "result_field", ":", "dict", "(", ")", ",", "result_scored_field", ":", "dict", "(", ")", ",", "selected_scored_field", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "result_scored_field", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "]", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", "=", "[", "]", "\n", "if", "filter_value", "is", "None", ":", "\n", "            ", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", "=", "[", "(", "s", ",", "e", ")", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "", "else", ":", "\n", "            ", "for", "s", ",", "e", "in", "sorted_e_list", ":", "\n", "                ", "if", "s", ">=", "filter_value", ":", "\n", "                    ", "results_dict", "[", "result_field", "]", "[", "key", "]", ".", "append", "(", "e", ")", "\n", "results_dict", "[", "selected_scored_field", "]", "[", "key", "]", ".", "append", "(", "(", "s", ",", "e", ")", ")", "\n", "", "if", "len", "(", "results_dict", "[", "result_field", "]", "[", "key", "]", ")", "==", "top_k", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "", "return", "results_dict", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.ss_sampler.build_full_wiki_document_forward_item": [[18, 94], ["print", "tqdm.tqdm", "int", "zip", "forward_item_list.extend", "fever_utils.check_sentences.check_and_clean_evidence", "set", "fever_utils.fever_db.get_all_sent_by_doc_id", "range", "len", "len", "dict", "str", "str", "str", "ss_sampler.convert_to_formatted_sent", "fitem_list.append", "itertools.chain.from_iterable", "fever_utils.fever_db.get_evidence", "gt_evidence_texts.append", "all_texts_list.append", "gt_evidence_id.append", "all_id_list.append", "len", "uuid.uuid4", "all_texts_list.append", "all_id_list.append", "str", "str"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_all_sent_by_doc_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.convert_to_formatted_sent", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_evidence"], ["def", "build_full_wiki_document_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "# Forward item:", "\n", "# qid, fid, query, context, doc_t, s_labels.", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "cur_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "query", "=", "item", "[", "'claim'", "]", "\n", "selected_doc", "=", "doc_results", "[", "cur_id", "]", "[", "'predicted_docids'", "]", "\n", "\n", "# selected_evidence_list = []", "\n", "#", "\n", "# for doc_id in selected_doc:", "\n", "#     cur_r_list, cur_id_list = fever_db.get_all_sent_by_doc_id(db_cursor, doc_id, with_h_links=False)", "\n", "all_id_list", "=", "[", "]", "\n", "all_texts_list", "=", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "e_list", "=", "fever_utils", ".", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "all_evidence_set", "=", "set", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "[", "evids", ".", "evidences_list", "for", "evids", "in", "e_list", "]", ")", ")", "\n", "\n", "# Here, we retrieval all the evidence sentence", "\n", "gt_evidence_texts", "=", "[", "]", "\n", "gt_evidence_id", "=", "[", "]", "\n", "for", "doc_id", ",", "ln", "in", "all_evidence_set", ":", "\n", "                ", "_", ",", "text", ",", "_", "=", "fever_db", ".", "get_evidence", "(", "db_cursor", ",", "doc_id", ",", "ln", ")", "\n", "\n", "gt_evidence_texts", ".", "append", "(", "text", ")", "\n", "all_texts_list", ".", "append", "(", "text", ")", "\n", "\n", "gt_evidence_id", ".", "append", "(", "doc_id", "+", "'(-.-)'", "+", "str", "(", "ln", ")", ")", "\n", "all_id_list", ".", "append", "(", "doc_id", "+", "'(-.-)'", "+", "str", "(", "ln", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "gt_evidence_texts", "=", "[", "]", "\n", "gt_evidence_id", "=", "[", "]", "\n", "\n", "", "for", "doc_id", "in", "selected_doc", ":", "\n", "            ", "cur_text_list", ",", "cur_id_list", "=", "fever_db", ".", "get_all_sent_by_doc_id", "(", "db_cursor", ",", "doc_id", ",", "with_h_links", "=", "False", ")", "\n", "# Merging to data list and removing duplicate", "\n", "for", "i", "in", "range", "(", "len", "(", "cur_text_list", ")", ")", ":", "\n", "                ", "if", "cur_id_list", "[", "i", "]", "in", "all_id_list", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "all_texts_list", ".", "append", "(", "cur_text_list", "[", "i", "]", ")", "\n", "all_id_list", ".", "append", "(", "cur_id_list", "[", "i", "]", ")", "\n", "\n", "", "", "", "assert", "len", "(", "all_texts_list", ")", "==", "len", "(", "all_id_list", ")", "\n", "fitem_list", "=", "[", "]", "\n", "\n", "for", "text", ",", "sid", "in", "zip", "(", "all_texts_list", ",", "all_id_list", ")", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'cid'", "]", "=", "str", "(", "cur_id", ")", "\n", "fitem", "[", "'sid'", "]", "=", "str", "(", "sid", ")", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "\n", "cur_text", "=", "convert_to_formatted_sent", "(", "text", ",", "sid", ",", "contain_head", "=", "False", ")", "\n", "fitem", "[", "'context'", "]", "=", "cur_text", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "sid", "in", "gt_evidence_id", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.ss_sampler.convert_to_formatted_sent": [[96, 107], ["fever_utils.fever_db.convert_brc().replace", "fever_utils.fever_db.convert_brc", "int", "sid.split", "fever_utils.fever_db.convert_brc", "fever_db.convert_brc().replace.lower", "sent.lower", "sid.split"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc"], ["", "def", "convert_to_formatted_sent", "(", "text", ",", "sid", ",", "contain_head", "=", "False", ")", ":", "\n", "    ", "sent", "=", "text", "\n", "doc_id", ",", "ln", "=", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "0", "]", ",", "int", "(", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "1", "]", ")", "\n", "doc_id_natural_format", "=", "fever_db", ".", "convert_brc", "(", "doc_id", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "if", "contain_head", "and", "ln", "!=", "0", "and", "doc_id_natural_format", ".", "lower", "(", ")", "not", "in", "sent", ".", "lower", "(", ")", ":", "\n", "        ", "cur_sent", "=", "f\"{doc_id_natural_format} TTT \"", "+", "sent", "\n", "", "else", ":", "\n", "        ", "cur_sent", "=", "sent", "\n", "", "cur_sent", "=", "fever_db", ".", "convert_brc", "(", "cur_sent", ")", "\n", "\n", "return", "cur_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.ss_sampler.down_sample_neg": [[109, 149], ["print", "random.shuffle", "random.shuffle", "int", "print", "random.shuffle", "print", "pos_items.append", "neg_items.append", "other_items.append", "len"], "function", ["None"], ["", "def", "down_sample_neg", "(", "fitems", ",", "ratio", "=", "None", ")", ":", "\n", "    ", "if", "ratio", "is", "None", ":", "\n", "        ", "return", "fitems", "\n", "\n", "", "pos_count", "=", "0", "\n", "neg_count", "=", "0", "\n", "other_count", "=", "0", "\n", "\n", "pos_items", "=", "[", "]", "\n", "neg_items", "=", "[", "]", "\n", "other_items", "=", "[", "]", "\n", "\n", "for", "item", "in", "fitems", ":", "\n", "        ", "if", "item", "[", "'s_labels'", "]", "==", "'true'", ":", "\n", "            ", "pos_count", "+=", "1", "\n", "pos_items", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "'s_labels'", "]", "==", "'false'", ":", "\n", "            ", "neg_count", "+=", "1", "\n", "neg_items", ".", "append", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "other_count", "+=", "1", "\n", "other_items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "other_count", "!=", "0", ":", "\n", "        ", "print", "(", "\"Potential Error! We have labels that are not true or false:\"", ",", "other_count", ")", "\n", "\n", "", "print", "(", "f\"Before Sampling, we have {pos_count}/{neg_count} (pos/neg).\"", ")", "\n", "\n", "random", ".", "shuffle", "(", "pos_items", ")", "\n", "random", ".", "shuffle", "(", "neg_items", ")", "\n", "neg_sample_count", "=", "int", "(", "pos_count", "/", "ratio", ")", "\n", "\n", "sampled_neg", "=", "neg_items", "[", ":", "neg_sample_count", "]", "\n", "\n", "print", "(", "f\"After Sampling, we have {pos_count}/{len(sampled_neg)} (pos/neg).\"", ")", "\n", "\n", "sampled_list", "=", "sampled_neg", "+", "pos_items", "\n", "random", ".", "shuffle", "(", "sampled_list", ")", "\n", "\n", "return", "sampled_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.build_full_wiki_sentence_forward_item": [[16, 93], ["print", "tqdm.tqdm", "int", "zip", "forward_item_list.extend", "item.keys", "fever_utils.check_sentences.check_and_clean_evidence", "set", "fever_utils.fever_db.get_all_sent_by_doc_id", "range", "len", "len", "dict", "str", "str", "fever_s_level_sampler.convert_to_formatted_sent", "fitem_list.append", "itertools.chain.from_iterable", "fever_utils.fever_db.get_evidence", "all_texts_list.append", "gt_evidence_id.append", "all_id_list.append", "len", "uuid.uuid4", "int", "all_texts_list.append", "all_id_list.append", "sid.split", "sid.split"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_all_sent_by_doc_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.convert_to_formatted_sent", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_evidence"], ["def", "build_full_wiki_sentence_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ",", "ignore_non_verifiable", "=", "False", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "cur_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "query", "=", "item", "[", "'claim'", "]", "\n", "selected_doc", "=", "doc_results", "[", "'predicted_docids'", "]", "[", "cur_id", "]", "\n", "if", "'verifiable'", "in", "item", ".", "keys", "(", ")", ":", "\n", "            ", "verifiable", "=", "item", "[", "'verifiable'", "]", "==", "\"VERIFIABLE\"", "\n", "", "else", ":", "\n", "            ", "verifiable", "=", "None", "\n", "\n", "", "if", "not", "verifiable", "and", "is_training", "and", "ignore_non_verifiable", ":", "\n", "            ", "continue", "\n", "\n", "", "all_id_list", ":", "List", "[", "List", "[", "str", ",", "int", "]", "]", "=", "[", "]", "\n", "all_texts_list", "=", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "e_list", "=", "fever_utils", ".", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "all_evidence_set", "=", "set", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "[", "evids", ".", "evidences_list", "for", "evids", "in", "e_list", "]", ")", ")", "\n", "\n", "# Here, we retrieval all the evidence sentence", "\n", "# gt_evidence_texts = []", "\n", "gt_evidence_id", "=", "[", "]", "\n", "for", "doc_id", ",", "ln", "in", "all_evidence_set", ":", "\n", "                ", "_", ",", "text", ",", "_", "=", "fever_db", ".", "get_evidence", "(", "db_cursor", ",", "doc_id", ",", "ln", ")", "\n", "\n", "# gt_evidence_texts.append(text)", "\n", "all_texts_list", ".", "append", "(", "text", ")", "\n", "\n", "gt_evidence_id", ".", "append", "(", "[", "doc_id", ",", "ln", "]", ")", "\n", "all_id_list", ".", "append", "(", "[", "doc_id", ",", "ln", "]", ")", "\n", "", "", "else", ":", "\n", "# gt_evidence_texts = []", "\n", "            ", "gt_evidence_id", "=", "[", "]", "\n", "\n", "", "for", "doc_id", "in", "selected_doc", ":", "\n", "            ", "cur_text_list", ",", "cur_id_list", "=", "fever_db", ".", "get_all_sent_by_doc_id", "(", "db_cursor", ",", "doc_id", ",", "with_h_links", "=", "False", ")", "\n", "# Merging to data list and removing duplicate", "\n", "for", "i", "in", "range", "(", "len", "(", "cur_text_list", ")", ")", ":", "\n", "                ", "sid", "=", "cur_id_list", "[", "i", "]", "\n", "doc_id", ",", "ln", "=", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "0", "]", ",", "int", "(", "sid", ".", "split", "(", "'(-.-)'", ")", "[", "1", "]", ")", "\n", "if", "[", "doc_id", ",", "ln", "]", "not", "in", "all_id_list", ":", "\n", "                    ", "all_texts_list", ".", "append", "(", "cur_text_list", "[", "i", "]", ")", "\n", "all_id_list", ".", "append", "(", "[", "doc_id", ",", "ln", "]", ")", "\n", "\n", "", "", "", "assert", "len", "(", "all_texts_list", ")", "==", "len", "(", "all_id_list", ")", "\n", "fitem_list", "=", "[", "]", "\n", "\n", "for", "text", ",", "sid", "in", "zip", "(", "all_texts_list", ",", "all_id_list", ")", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "str", "(", "cur_id", ")", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "fitem", "[", "'element'", "]", "=", "sid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "\n", "cur_text", "=", "convert_to_formatted_sent", "(", "text", ",", "sid", ",", "contain_head", "=", "True", ")", "\n", "fitem", "[", "'context'", "]", "=", "cur_text", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "sid", "in", "gt_evidence_id", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.convert_to_formatted_sent": [[95, 106], ["fever_utils.fever_db.convert_brc().replace", "fever_utils.fever_db.convert_brc", "fever_utils.fever_db.convert_brc", "fever_db.convert_brc().replace.lower", "sent.lower"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc"], ["", "def", "convert_to_formatted_sent", "(", "text", ",", "sid", ",", "contain_head", "=", "False", ")", ":", "\n", "    ", "sent", "=", "text", "\n", "doc_id", ",", "ln", "=", "sid", "\n", "doc_id_natural_format", "=", "fever_db", ".", "convert_brc", "(", "doc_id", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "if", "contain_head", "and", "ln", "!=", "0", "and", "doc_id_natural_format", ".", "lower", "(", ")", "not", "in", "sent", ".", "lower", "(", ")", ":", "\n", "        ", "cur_sent", "=", "f\"{doc_id_natural_format} . \"", "+", "sent", "\n", "", "else", ":", "\n", "        ", "cur_sent", "=", "sent", "\n", "", "cur_sent", "=", "fever_db", ".", "convert_brc", "(", "cur_sent", ")", "\n", "\n", "return", "cur_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.get_sentence_forward_pair": [[108, 140], ["utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "fever_utils.fever_db.get_cursor", "fever_s_level_sampler.build_full_wiki_sentence_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_s_level_sampler.build_full_wiki_sentence_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_sentence_forward_pair", "(", "tag", ",", "ruleterm_doc_results", ",", "is_training", ",", "\n", "debug", "=", "False", ",", "ignore_non_verifiable", "=", "False", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.005", ")", ":", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "ruleterm_doc_results", "=", "ruleterm_doc_results", "[", ":", "100", "]", "\n", "\n", "# ruleterm_doc_results_dict = list_dict_data_tool.list_to_dict(ruleterm_doc_results, 'id')", "\n", "", "d_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'id'", ")", "\n", "copied_d_o_dict", "=", "copy", ".", "deepcopy", "(", "d_o_dict", ")", "\n", "# copied_d_list = copy.deepcopy(d_list)", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "ruleterm_doc_results", ",", "copied_d_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_filtered", "=", "select_top_k_and_to_results_dict", "(", "copied_d_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "top_k", ",", "filter_value", "=", "filter_value", ")", "\n", "\n", "db_cursor", "=", "fever_db", ".", "get_cursor", "(", ")", "\n", "fitems", "=", "build_full_wiki_sentence_forward_item", "(", "cur_results_dict_filtered", ",", "d_list", ",", "is_training", ",", "db_cursor", ",", "\n", "ignore_non_verifiable", ")", "\n", "\n", "return", "fitems", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.qa_aug_sampler.get_sample_data": [[6, 27], ["utils.common.load_json", "utils.common.load_json", "random.shuffle", "print", "random.shuffle", "random.random", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json"], ["def", "get_sample_data", "(", "size", "=", "-", "1", ")", ":", "\n", "    ", "qa_gt_s", "=", "common", ".", "load_json", "(", "config", ".", "FEVER_DATA_ROOT", "/", "\"qa_aug\"", "/", "\"squad_train_turker_groundtruth.json\"", ")", "\n", "# print(len(qa_gt_s))", "\n", "qa_aug_rnei", "=", "common", ".", "load_json", "(", "\n", "config", ".", "FEVER_DATA_ROOT", "/", "\"qa_aug\"", "/", "\"squad_train_refutes_bytype_3x_claim_stoch_answspan_stoch.json\"", ")", "\n", "# print(len(qa_aug_rnei))", "\n", "random", ".", "shuffle", "(", "qa_aug_rnei", ")", "\n", "for", "item", "in", "qa_aug_rnei", ":", "\n", "        ", "sv", "=", "random", ".", "random", "(", ")", "\n", "if", "sv", ">", "0.5", ":", "\n", "            ", "item", "[", "'label'", "]", "=", "\"REFUTES\"", "\n", "", "else", ":", "\n", "            ", "item", "[", "'label'", "]", "=", "\"NOT ENOUGH INFO\"", "\n", "\n", "", "", "balanced_aug_data", "=", "qa_gt_s", "+", "qa_aug_rnei", "[", ":", "len", "(", "qa_gt_s", ")", "*", "2", "]", "\n", "print", "(", "\"Total balanced size:\"", ",", "len", "(", "balanced_aug_data", ")", ")", "\n", "random", ".", "shuffle", "(", "balanced_aug_data", ")", "\n", "if", "size", "!=", "-", "1", ":", "\n", "        ", "return", "balanced_aug_data", "[", ":", "size", "]", "\n", "", "else", ":", "\n", "        ", "return", "balanced_aug_data", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.build_full_wiki_document_forward_item": [[16, 87], ["print", "tqdm.tqdm", "int", "forward_item_list.extend", "item.keys", "fever_utils.check_sentences.check_and_clean_evidence", "set", "set", "dict", "str", "str", "fever_p_level_sampler.get_paragraph_text", "fitem_list.append", "itertools.chain.from_iterable", "set.add", "gt_doc_id.append", "all_id_list.append", "all_id_list.append", "uuid.uuid4"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_paragraph_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["def", "build_full_wiki_document_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ",", "ignore_non_verifiable", "=", "False", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "# Forward item:", "\n", "# qid, fid, query, context, doc_t, s_labels.", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "cur_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "query", "=", "item", "[", "'claim'", "]", "\n", "selected_doc", "=", "doc_results", "[", "cur_id", "]", "[", "'predicted_docids'", "]", "\n", "if", "'verifiable'", "in", "item", ".", "keys", "(", ")", ":", "\n", "            ", "verifiable", "=", "item", "[", "'verifiable'", "]", "==", "\"VERIFIABLE\"", "\n", "", "else", ":", "\n", "            ", "verifiable", "=", "None", "\n", "\n", "", "if", "not", "verifiable", "and", "is_training", "and", "ignore_non_verifiable", ":", "\n", "            ", "continue", "\n", "\n", "", "all_id_list", "=", "[", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "e_list", "=", "fever_utils", ".", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "all_evidence_set", "=", "set", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "[", "evids", ".", "evidences_list", "for", "evids", "in", "e_list", "]", ")", ")", "\n", "\n", "# Here, we retrieval all the evidence sentence", "\n", "gt_doc_id", "=", "[", "]", "\n", "gt_doc_set", "=", "set", "(", ")", "\n", "\n", "for", "doc_id", ",", "ln", "in", "all_evidence_set", ":", "\n", "                ", "gt_doc_set", ".", "add", "(", "doc_id", ")", "\n", "\n", "", "for", "doc_id", "in", "gt_doc_set", ":", "\n", "                ", "gt_doc_id", ".", "append", "(", "doc_id", ")", "\n", "all_id_list", ".", "append", "(", "doc_id", ")", "\n", "", "", "else", ":", "\n", "            ", "gt_doc_id", "=", "[", "]", "\n", "\n", "", "for", "doc_id", "in", "selected_doc", ":", "\n", "            ", "if", "doc_id", "not", "in", "all_id_list", ":", "\n", "                ", "all_id_list", ".", "append", "(", "doc_id", ")", "\n", "\n", "# assert len(all_texts_list) == len(all_id_list)", "\n", "", "", "fitem_list", "=", "[", "]", "\n", "\n", "for", "doc_id", "in", "all_id_list", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "str", "(", "cur_id", ")", "# query id", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "# forward id", "\n", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "\n", "cur_text", "=", "get_paragraph_text", "(", "doc_id", ",", "db_cursor", ")", "\n", "\n", "fitem", "[", "'context'", "]", "=", "' '", ".", "join", "(", "cur_text", ")", "\n", "fitem", "[", "'element'", "]", "=", "doc_id", "# the element is just the doc_id", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "doc_id", "in", "gt_doc_id", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_text": [[89, 97], ["fever_utils.fever_db.get_all_sent_by_doc_id", "fever_utils.fever_db.convert_brc", "all_text.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_all_sent_by_doc_id", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc"], ["", "def", "get_paragraph_text", "(", "doc_id", ",", "db_cursor", ")", ":", "\n", "    ", "text_list", ",", "_", "=", "fever_db", ".", "get_all_sent_by_doc_id", "(", "db_cursor", ",", "doc_id", ")", "\n", "all_text", "=", "[", "]", "\n", "for", "sent_text", "in", "text_list", ":", "\n", "        ", "natural_formatted_text", "=", "fever_db", ".", "convert_brc", "(", "sent_text", ")", "\n", "all_text", ".", "append", "(", "natural_formatted_text", ")", "\n", "\n", "", "return", "all_text", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair": [[99, 119], ["utils.list_dict_data_tool.list_to_dict", "fever_utils.fever_db.get_cursor", "fever_p_level_sampler.build_full_wiki_document_forward_item", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.common.load_jsonl", "ValueError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.build_full_wiki_document_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_paragraph_forward_pair", "(", "tag", ",", "ruleterm_doc_results", ",", "is_training", ",", "debug", "=", "False", ",", "ignore_non_verifiable", "=", "False", ")", ":", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "ruleterm_doc_results", "=", "ruleterm_doc_results", "[", ":", "100", "]", "\n", "\n", "", "ruleterm_doc_results_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "ruleterm_doc_results", ",", "'id'", ")", "\n", "db_cursor", "=", "fever_db", ".", "get_cursor", "(", ")", "\n", "fitems", "=", "build_full_wiki_document_forward_item", "(", "ruleterm_doc_results_dict", ",", "d_list", ",", "is_training", ",", "db_cursor", ",", "\n", "ignore_non_verifiable", ")", "\n", "\n", "return", "fitems", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.down_sample_neg": [[121, 161], ["print", "random.shuffle", "random.shuffle", "int", "print", "random.shuffle", "print", "pos_items.append", "neg_items.append", "other_items.append", "len"], "function", ["None"], ["", "def", "down_sample_neg", "(", "fitems", ",", "ratio", "=", "None", ")", ":", "\n", "    ", "pos_count", "=", "0", "\n", "neg_count", "=", "0", "\n", "other_count", "=", "0", "\n", "\n", "pos_items", "=", "[", "]", "\n", "neg_items", "=", "[", "]", "\n", "other_items", "=", "[", "]", "\n", "\n", "for", "item", "in", "fitems", ":", "\n", "        ", "if", "item", "[", "'s_labels'", "]", "==", "'true'", ":", "\n", "            ", "pos_count", "+=", "1", "\n", "pos_items", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "'s_labels'", "]", "==", "'false'", ":", "\n", "            ", "neg_count", "+=", "1", "\n", "neg_items", ".", "append", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "other_count", "+=", "1", "\n", "other_items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "other_count", "!=", "0", ":", "\n", "        ", "print", "(", "\"Potential Error! We have labels that are not true or false:\"", ",", "other_count", ")", "\n", "\n", "", "print", "(", "f\"Before Sampling, we have {pos_count}/{neg_count} (pos/neg).\"", ")", "\n", "\n", "if", "ratio", "is", "None", ":", "\n", "        ", "return", "fitems", "\n", "\n", "", "random", ".", "shuffle", "(", "pos_items", ")", "\n", "random", ".", "shuffle", "(", "neg_items", ")", "\n", "neg_sample_count", "=", "int", "(", "pos_count", "/", "ratio", ")", "\n", "\n", "sampled_neg", "=", "neg_items", "[", ":", "neg_sample_count", "]", "\n", "\n", "print", "(", "f\"After Sampling, we have {pos_count}/{len(sampled_neg)} (pos/neg).\"", ")", "\n", "\n", "sampled_list", "=", "sampled_neg", "+", "pos_items", "\n", "random", ".", "shuffle", "(", "sampled_list", ")", "\n", "\n", "return", "sampled_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.evidences_to_text": [[17, 38], ["sorted", "fever_utils.fever_db.get_evidence", "fever_utils.fever_db.convert_brc", "sentences.append", "fever_utils.fever_db.convert_brc().replace", "fever_utils.fever_db.convert_brc"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc"], ["def", "evidences_to_text", "(", "evidences", ",", "db_cursor", ",", "contain_head", "=", "True", ")", ":", "\n", "    ", "evidences", "=", "sorted", "(", "evidences", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "cur_head", "=", "'DO NOT INCLUDE THIS FLAG'", "\n", "sentences", "=", "[", "]", "\n", "for", "doc_id", ",", "line_num", "in", "evidences", ":", "\n", "        ", "_", ",", "e_text", ",", "_", "=", "fever_db", ".", "get_evidence", "(", "db_cursor", ",", "doc_id", ",", "line_num", ")", "\n", "\n", "cur_text", "=", "\"\"", "\n", "e_text", "=", "fever_db", ".", "convert_brc", "(", "e_text", ")", "\n", "\n", "if", "contain_head", "and", "cur_head", "!=", "doc_id", ":", "\n", "            ", "cur_head", "=", "doc_id", "\n", "doc_id_natural_format", "=", "fever_db", ".", "convert_brc", "(", "doc_id", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "\n", "if", "line_num", "!=", "0", ":", "\n", "                ", "cur_text", "=", "f\"{doc_id_natural_format} {TITLE_SEP} \"", "\n", "\n", "", "", "cur_text", "=", "cur_text", "+", "e_text", "\n", "sentences", ".", "append", "(", "cur_text", ")", "\n", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.build_nli_forward_item": [[40, 159], ["print", "tqdm.tqdm", "str", "fever_utils.check_sentences.Evidences", "nli_new_sampler.evidences_to_text", "dict", "str", "str", "forward_item_list.append", "fever_utils.check_sentences.check_and_clean_evidence", "nli_new_sampler.evidences_to_text", "dict", "str", "str", "forward_item_list.append", "pred_evidence_list.append", "uuid.uuid4", "copy.deepcopy", "len", "forward_evidences_list.append", "len", "len", "fever_utils.check_sentences.check_and_clean_evidence", "sorted", "random.shuffle", "random.randint", "fever_utils.check_sentences.Evidences", "all", "forward_evidences_list.append", "uuid.uuid4", "random.randint", "random.shuffle", "flags.append", "flags.append", "raw_evidences_list.append", "len", "flags.append", "flags.append", "len", "check_sentences.Evidences.add_sent", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.evidences_to_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.evidences_to_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.add_sent"], ["", "def", "build_nli_forward_item", "(", "data_list", ",", "is_training", ",", "certain_k", "=", "2", ",", "db_cursor", "=", "None", ",", "contain_head", "=", "True", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "flags", "=", "[", "]", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "cur_id", "=", "str", "(", "item", "[", "'id'", "]", ")", "\n", "query", "=", "item", "[", "'claim'", "]", "\n", "scored_sent", "=", "item", "[", "'selected_scored_results'", "]", "\n", "\n", "item_verifiable", "=", "item", "[", "'verifiable'", "]", "if", "'verifiable'", "in", "item", "else", "None", "\n", "item_label", "=", "item", "[", "'label'", "]", "if", "'label'", "in", "item", "else", "None", "\n", "\n", "# print(cur_id)", "\n", "# print(query)", "\n", "# print(scored_sent)", "\n", "\n", "if", "is_training", ":", "# training mode", "\n", "            ", "forward_evidences_list", "=", "[", "]", "\n", "\n", "if", "item", "[", "'verifiable'", "]", "==", "\"VERIFIABLE\"", ":", "\n", "                ", "assert", "item", "[", "'label'", "]", "==", "'SUPPORTS'", "or", "item", "[", "'label'", "]", "==", "'REFUTES'", "\n", "e_list", "=", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "\n", "for", "evidences", "in", "e_list", ":", "\n", "# print(evidences)", "\n", "                    ", "new_evidences", "=", "copy", ".", "deepcopy", "(", "evidences", ")", "\n", "n_e", "=", "len", "(", "evidences", ")", "\n", "if", "n_e", "<", "5", ":", "\n", "                        ", "current_sample_num", "=", "random", ".", "randint", "(", "0", ",", "5", "-", "n_e", ")", "\n", "random", ".", "shuffle", "(", "scored_sent", ")", "\n", "for", "score", ",", "(", "doc_id", ",", "ln", ")", "in", "scored_sent", "[", ":", "current_sample_num", "]", ":", "\n", "# doc_ids = sampled_e.split(fever_scorer.SENT_LINE)[0]", "\n", "# ln = int(sampled_e.split(fever_scorer.SENT_LINE)[1])", "\n", "                            ", "new_evidences", ".", "add_sent", "(", "doc_id", ",", "ln", ")", "\n", "\n", "", "", "if", "new_evidences", "!=", "evidences", ":", "\n", "                        ", "flag", "=", "f\"verifiable.non_eq.{len(new_evidences) - len(evidences)}\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "", "else", ":", "\n", "                        ", "flag", "=", "\"verifiable.eq.0\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "", "forward_evidences_list", ".", "append", "(", "new_evidences", ")", "\n", "", "assert", "len", "(", "forward_evidences_list", ")", "==", "len", "(", "e_list", ")", "\n", "\n", "", "elif", "item", "[", "'verifiable'", "]", "==", "\"NOT VERIFIABLE\"", ":", "\n", "                ", "assert", "item", "[", "'label'", "]", "==", "'NOT ENOUGH INFO'", "\n", "\n", "e_list", "=", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "\n", "prioritized_additional_evidence_list", "=", "sorted", "(", "scored_sent", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "0", "]", ")", "\n", "# print(\"Pro:\", prioritized_additional_evidence_list)", "\n", "top_two_sent", "=", "prioritized_additional_evidence_list", "[", ":", "certain_k", "]", "\n", "\n", "random", ".", "shuffle", "(", "scored_sent", ")", "\n", "current_sample_num", "=", "random", ".", "randint", "(", "0", ",", "2", ")", "\n", "raw_evidences_list", "=", "[", "]", "\n", "\n", "for", "score", ",", "(", "doc_id", ",", "ln", ")", "in", "top_two_sent", "+", "scored_sent", "[", ":", "current_sample_num", "]", ":", "\n", "                    ", "raw_evidences_list", ".", "append", "(", "(", "doc_id", ",", "ln", ")", ")", "\n", "", "new_evidences", "=", "check_sentences", ".", "Evidences", "(", "raw_evidences_list", ")", "\n", "\n", "if", "len", "(", "new_evidences", ")", "==", "0", ":", "\n", "                    ", "flag", "=", "f\"verifiable.eq.0\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "", "else", ":", "\n", "                    ", "flag", "=", "f\"not_verifiable.non_eq.{len(new_evidences)}\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "\n", "", "assert", "all", "(", "len", "(", "e", ")", "==", "0", "for", "e", "in", "e_list", ")", "\n", "forward_evidences_list", ".", "append", "(", "new_evidences", ")", "\n", "assert", "len", "(", "forward_evidences_list", ")", "==", "1", "\n", "\n", "# handle result_sentids_list", "\n", "", "for", "forward_evidences", "in", "forward_evidences_list", ":", "\n", "                ", "forward_evidences_text", "=", "evidences_to_text", "(", "forward_evidences", ",", "db_cursor", ",", "contain_head", "=", "contain_head", ")", "\n", "# print(forward_evidences)", "\n", "# print(forward_evidences_text)", "\n", "\n", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'cid'", "]", "=", "str", "(", "cur_id", ")", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "cur_text", "=", "forward_evidences_text", "\n", "fitem", "[", "'context'", "]", "=", "' '", ".", "join", "(", "cur_text", ")", "\n", "\n", "fitem", "[", "'label'", "]", "=", "item_label", "\n", "fitem", "[", "'verifiable'", "]", "=", "item_verifiable", "\n", "\n", "forward_item_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "", "else", ":", "# non-training mode", "\n", "            ", "pred_evidence_list", "=", "[", "]", "\n", "for", "score", ",", "(", "doc_id", ",", "ln", ")", "in", "scored_sent", ":", "\n", "                ", "pred_evidence_list", ".", "append", "(", "(", "doc_id", ",", "ln", ")", ")", "\n", "\n", "", "forward_evidences", "=", "check_sentences", ".", "Evidences", "(", "pred_evidence_list", ")", "\n", "\n", "forward_evidences_text", "=", "evidences_to_text", "(", "forward_evidences", ",", "db_cursor", ",", "contain_head", "=", "contain_head", ")", "\n", "# print(forward_evidences)", "\n", "# print(forward_evidences_text)", "\n", "\n", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'cid'", "]", "=", "str", "(", "cur_id", ")", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "query", "\n", "cur_text", "=", "forward_evidences_text", "\n", "fitem", "[", "'context'", "]", "=", "' '", ".", "join", "(", "cur_text", ")", "\n", "\n", "fitem", "[", "'label'", "]", "=", "'hidden'", "\n", "fitem", "[", "'verifiable'", "]", "=", "'hidden'", "\n", "\n", "forward_item_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.get_nli_pair": [[161, 202], ["utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "fever_utils.fever_db.get_cursor", "nli_new_sampler.build_nli_forward_item", "utils.common.load_jsonl", "set", "utils.common.load_jsonl", "utils.common.load_jsonl", "ValueError", "new_sent_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_new_sampler.build_nli_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_nli_pair", "(", "tag", ",", "is_training", ",", "sent_level_results_list", ",", "\n", "debug", "=", "None", ",", "sent_top_k", "=", "5", ",", "sent_filter_value", "=", "0.05", ")", ":", "\n", "    ", "if", "tag", "==", "'dev'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "", "elif", "tag", "==", "'train'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TRAIN", ")", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "        ", "d_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_TEST", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Tag:{tag} not supported.\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "# sent_dict = list_dict_data_tool.list_to_dict(sent_level_results_list):", "\n", "\n", "", "d_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'id'", ")", "\n", "\n", "if", "debug", ":", "\n", "        ", "id_set", "=", "set", "(", "[", "item", "[", "'id'", "]", "for", "item", "in", "d_list", "]", ")", "\n", "new_sent_list", "=", "[", "]", "\n", "for", "item", "in", "sent_level_results_list", ":", "\n", "            ", "if", "item", "[", "\"qid\"", "]", "in", "id_set", ":", "\n", "                ", "new_sent_list", ".", "append", "(", "item", ")", "\n", "", "", "sent_level_results_list", "=", "new_sent_list", "\n", "\n", "", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "sent_level_results_list", ",", "d_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "filltered_sent_dict", "=", "select_top_k_and_to_results_dict", "(", "d_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "sent_top_k", ",", "filter_value", "=", "sent_filter_value", ",", "\n", "result_field", "=", "'predicted_evidence'", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "d_list", ",", "filltered_sent_dict", ",", "\n", "'id'", ",", "\n", "[", "'predicted_evidence'", ",", "'selected_scored_results'", "]", ")", "\n", "\n", "fever_db_cursor", "=", "fever_db", ".", "get_cursor", "(", "config", ".", "FEVER_DB", ")", "\n", "forward_items", "=", "build_nli_forward_item", "(", "d_list", ",", "is_training", "=", "is_training", ",", "db_cursor", "=", "fever_db_cursor", ")", "\n", "\n", "return", "forward_items", ",", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.select_sent_with_prob_for_eval": [[14, 83], ["fever_utils.fever_db.get_cursor", "isinstance", "dict", "utils.common.load_jsonl", "tqdm.tqdm", "dict", "utils.common.load_jsonl", "enumerate", "fever_utils.check_sentences.Evidences", "nli_sampler.evidence_list_to_text_list", "sorted", "int", "zip", "fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format", "int", "pred_evidence_list.append", "evidence_text_list_with_prob.append", "NotImplemented", "cur_e.split", "int", "cur_e.split"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.evidence_list_to_text_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.sentence_selection_sampler.convert_evidence2scoring_format"], ["def", "select_sent_with_prob_for_eval", "(", "input_file", ",", "additional_file", ",", "prob_dict_file", ",", "tokenized", "=", "False", ",", "pipeline", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    This method select sentences with upstream sentence retrieval.\n\n    :param input_file: This should be the file with 5 sentences selected.\n    :return:\n    \"\"\"", "\n", "cursor", "=", "fever_db", ".", "get_cursor", "(", ")", "\n", "\n", "if", "prob_dict_file", "is", "None", ":", "\n", "        ", "prob_dict_file", "=", "dict", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "additional_file", ",", "list", ")", ":", "\n", "        ", "additional_d_list", "=", "additional_file", "\n", "", "else", ":", "\n", "        ", "additional_d_list", "=", "common", ".", "load_jsonl", "(", "additional_file", ")", "\n", "", "additional_data_dict", "=", "dict", "(", ")", "\n", "\n", "for", "add_item", "in", "additional_d_list", ":", "\n", "        ", "additional_data_dict", "[", "add_item", "[", "'id'", "]", "]", "=", "add_item", "\n", "\n", "", "d_list", "=", "common", ".", "load_jsonl", "(", "input_file", ")", "\n", "\n", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "        ", "e_list", "=", "additional_data_dict", "[", "item", "[", "'id'", "]", "]", "[", "'predicted_sentids'", "]", "\n", "if", "not", "pipeline", ":", "\n", "            ", "assert", "additional_data_dict", "[", "item", "[", "'id'", "]", "]", "[", "'label'", "]", "==", "item", "[", "'label'", "]", "\n", "assert", "additional_data_dict", "[", "item", "[", "'id'", "]", "]", "[", "'verifiable'", "]", "==", "item", "[", "'verifiable'", "]", "\n", "", "assert", "additional_data_dict", "[", "item", "[", "'id'", "]", "]", "[", "'id'", "]", "==", "item", "[", "'id'", "]", "\n", "\n", "pred_evidence_list", "=", "[", "]", "\n", "for", "i", ",", "cur_e", "in", "enumerate", "(", "e_list", ")", ":", "\n", "            ", "doc_id", "=", "cur_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "0", "]", "\n", "ln", "=", "int", "(", "cur_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "1", "]", ")", "# Important changes Bugs: July 21", "\n", "pred_evidence_list", ".", "append", "(", "(", "doc_id", ",", "ln", ")", ")", "\n", "\n", "", "pred_evidence", "=", "check_sentences", ".", "Evidences", "(", "pred_evidence_list", ")", "\n", "\n", "evidence_text_list", "=", "evidence_list_to_text_list", "(", "cursor", ",", "pred_evidence", ",", "\n", "contain_head", "=", "True", ",", "id_tokenized", "=", "tokenized", ")", "\n", "\n", "evidences", "=", "sorted", "(", "pred_evidence", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "item_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "\n", "evidence_text_list_with_prob", "=", "[", "]", "\n", "for", "text", ",", "(", "doc_id", ",", "ln", ")", "in", "zip", "(", "evidence_text_list", ",", "evidences", ")", ":", "\n", "            ", "ssid", "=", "(", "item_id", ",", "doc_id", ",", "int", "(", "ln", ")", ")", "\n", "if", "ssid", "not", "in", "prob_dict_file", ":", "\n", "# print(\"Some sentence pair don't have 'prob'.\")", "\n", "                ", "prob", "=", "0.5", "\n", "", "else", ":", "\n", "                ", "prob", "=", "prob_dict_file", "[", "ssid", "]", "[", "'prob'", "]", "\n", "assert", "item", "[", "'claim'", "]", "==", "prob_dict_file", "[", "ssid", "]", "[", "'claim'", "]", "\n", "\n", "", "evidence_text_list_with_prob", ".", "append", "(", "(", "text", ",", "prob", ")", ")", "\n", "\n", "", "if", "tokenized", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplemented", "(", "\"Non tokenized is not implemented.\"", ")", "\n", "# item['claim'] = ' '.join(easy_tokenize(item['claim']))", "\n", "\n", "", "item", "[", "'evid'", "]", "=", "evidence_text_list_with_prob", "\n", "item", "[", "'predicted_evidence'", "]", "=", "convert_evidence2scoring_format", "(", "e_list", ")", "\n", "item", "[", "'predicted_sentids'", "]", "=", "e_list", "\n", "# This change need to be saved.", "\n", "# item['predicted_label'] = additional_data_dict[item['id']]['label']", "\n", "\n", "", "return", "d_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.adv_simi_sample_with_prob_v1_1": [[85, 159], ["fever_utils.fever_db.get_cursor", "utils.common.load_jsonl", "isinstance", "dict", "tqdm.tqdm", "fever_db.get_cursor.close", "print", "dict", "utils.common.load_jsonl", "nli_sampler.sample_additional_data_for_item_v1_1", "enumerate", "zip", "dict", "nli_sampler.evidence_list_to_text_list", "sorted", "int", "zip", "sampled_data_list.append", "evidence_text_list_with_prob.append", "str", "NotImplemented", "int", "int", "str"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.sample_additional_data_for_item_v1_1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.evidence_list_to_text_list"], ["", "def", "adv_simi_sample_with_prob_v1_1", "(", "input_file", ",", "additional_file", ",", "prob_dict_file", ",", "tokenized", "=", "False", ")", ":", "\n", "    ", "cursor", "=", "fever_db", ".", "get_cursor", "(", ")", "\n", "d_list", "=", "common", ".", "load_jsonl", "(", "input_file", ")", "\n", "\n", "if", "prob_dict_file", "is", "None", ":", "\n", "        ", "prob_dict_file", "=", "dict", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "additional_file", ",", "list", ")", ":", "\n", "        ", "additional_d_list", "=", "additional_file", "\n", "", "else", ":", "\n", "        ", "additional_d_list", "=", "common", ".", "load_jsonl", "(", "additional_file", ")", "\n", "", "additional_data_dict", "=", "dict", "(", ")", "\n", "\n", "for", "add_item", "in", "additional_d_list", ":", "\n", "        ", "additional_data_dict", "[", "add_item", "[", "'id'", "]", "]", "=", "add_item", "\n", "\n", "", "sampled_data_list", "=", "[", "]", "\n", "count", "=", "0", "\n", "\n", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "# e_list = check_sentences.check_and_clean_evidence(item)", "\n", "        ", "sampled_e_list", ",", "flags", "=", "sample_additional_data_for_item_v1_1", "(", "item", ",", "additional_data_dict", ")", "\n", "# print(flags)", "\n", "for", "i", ",", "(", "sampled_evidence", ",", "flag", ")", "in", "enumerate", "(", "zip", "(", "sampled_e_list", ",", "flags", ")", ")", ":", "\n", "# Do not copy, might change in the future for error analysis", "\n", "# new_item = copy.deepcopy(item)", "\n", "            ", "new_item", "=", "dict", "(", ")", "\n", "# print(new_item['claim'])", "\n", "# print(e_list)", "\n", "# print(sampled_evidence)", "\n", "# print(flag)", "\n", "evidence_text_list", "=", "evidence_list_to_text_list", "(", "\n", "cursor", ",", "sampled_evidence", ",", "\n", "contain_head", "=", "True", ",", "id_tokenized", "=", "tokenized", ")", "\n", "\n", "evidences", "=", "sorted", "(", "sampled_evidence", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "item_id", "=", "int", "(", "item", "[", "'id'", "]", ")", "\n", "\n", "evidence_text_list_with_prob", "=", "[", "]", "\n", "for", "text", ",", "(", "doc_id", ",", "ln", ")", "in", "zip", "(", "evidence_text_list", ",", "evidences", ")", ":", "\n", "                ", "ssid", "=", "(", "int", "(", "item_id", ")", ",", "doc_id", ",", "int", "(", "ln", ")", ")", "\n", "if", "ssid", "not", "in", "prob_dict_file", ":", "\n", "                    ", "count", "+=", "1", "\n", "# print(\"Some sentence pair don't have 'prob'.\")", "\n", "prob", "=", "0.5", "\n", "", "else", ":", "\n", "                    ", "prob", "=", "prob_dict_file", "[", "ssid", "]", "[", "'prob'", "]", "\n", "assert", "item", "[", "'claim'", "]", "==", "prob_dict_file", "[", "ssid", "]", "[", "'claim'", "]", "\n", "\n", "", "evidence_text_list_with_prob", ".", "append", "(", "(", "text", ",", "prob", ")", ")", "\n", "\n", "", "new_item", "[", "'id'", "]", "=", "str", "(", "item", "[", "'id'", "]", ")", "+", "'#'", "+", "str", "(", "i", ")", "\n", "\n", "if", "tokenized", ":", "\n", "                ", "new_item", "[", "'claim'", "]", "=", "item", "[", "'claim'", "]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplemented", "(", "\"Non tokenized is not implemented.\"", ")", "\n", "# new_item['claim'] = ' '.join(easy_tokenize(item['claim']))", "\n", "\n", "", "new_item", "[", "'evid'", "]", "=", "evidence_text_list_with_prob", "\n", "\n", "new_item", "[", "'verifiable'", "]", "=", "item", "[", "'verifiable'", "]", "\n", "new_item", "[", "'label'", "]", "=", "item", "[", "'label'", "]", "\n", "\n", "# print(\"C:\", new_item['claim'])", "\n", "# print(\"E:\", new_item['evid'])", "\n", "# print(\"L:\", new_item['label'])", "\n", "# print()", "\n", "sampled_data_list", ".", "append", "(", "new_item", ")", "\n", "\n", "", "", "cursor", ".", "close", "(", ")", "\n", "\n", "print", "(", "count", ")", "\n", "return", "sampled_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.evidence_list_to_text_list": [[161, 198], ["sorted", "fever_utils.fever_db.get_evidence", "current_evidence_text_list.append", "len", "len", "fever_utils.fever_db.convert_brc().replace", "fever_utils.fever_db.convert_brc"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.get_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.fever_db.convert_brc"], ["", "def", "evidence_list_to_text_list", "(", "cursor", ",", "evidences", ",", "contain_head", "=", "True", ",", "id_tokenized", "=", "False", ")", ":", "\n", "# id_tokenized is a deprecated argument.", "\n", "# One evidence one text and len(evidences) == len(text_list)", "\n", "    ", "current_evidence_text_list", "=", "[", "]", "\n", "evidences", "=", "sorted", "(", "evidences", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "\n", "cur_head", "=", "'DO NOT INCLUDE THIS FLAG'", "\n", "\n", "for", "doc_id", ",", "line_num", "in", "evidences", ":", "\n", "\n", "        ", "_", ",", "e_text", ",", "_", "=", "fever_db", ".", "get_evidence", "(", "cursor", ",", "doc_id", ",", "line_num", ")", "\n", "\n", "cur_text", "=", "\"\"", "\n", "\n", "if", "contain_head", "and", "cur_head", "!=", "doc_id", ":", "\n", "            ", "cur_head", "=", "doc_id", "\n", "\n", "doc_id_natural_format", "=", "fever_db", ".", "convert_brc", "(", "doc_id", ")", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "\n", "# if not id_tokenized:", "\n", "#     raise NotImplemented(\"Non tokenized is not implemented.\")", "\n", "#     # doc_id_natural_format = fever_db.convert_brc(doc_id).replace('_', ' ')", "\n", "#     # t_doc_id_natural_format = ' '.join(easy_tokenize(doc_id_natural_format))", "\n", "# else:", "\n", "#     t_doc_id_natural_format = common.doc_id_to_tokenized_text(doc_id)", "\n", "\n", "if", "line_num", "!=", "0", ":", "\n", "                ", "cur_text", "=", "f\"{doc_id_natural_format} {TITLE_SEP} \"", "\n", "\n", "# Important change move one line below: July 16", "\n", "# current_evidence_text.append(e_text)", "\n", "", "", "cur_text", "=", "cur_text", "+", "e_text", "\n", "\n", "current_evidence_text_list", ".", "append", "(", "cur_text", ")", "\n", "\n", "", "assert", "len", "(", "evidences", ")", "==", "len", "(", "current_evidence_text_list", ")", "\n", "return", "current_evidence_text_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.nli_sampler.sample_additional_data_for_item_v1_1": [[200, 284], ["fever_utils.check_sentences.check_and_clean_evidence", "len", "len", "copy.deepcopy", "len", "res_sentids_list.append", "len", "len", "fever_utils.check_sentences.check_and_clean_evidence", "sorted", "random.shuffle", "random.randint", "fever_utils.check_sentences.Evidences", "all", "res_sentids_list.append", "random.randint", "random.shuffle", "flags.append", "flags.append", "int", "raw_evidences_list.append", "len", "flags.append", "flags.append", "len", "int", "check_sentences.Evidences.add_sent", "sampled_e.split", "sampled_e.split", "sampled_e.split", "len", "len", "sampled_e.split", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.check_and_clean_evidence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_utils.check_sentences.Evidences.add_sent"], ["", "def", "sample_additional_data_for_item_v1_1", "(", "item", ",", "additional_data_dictionary", ")", ":", "\n", "    ", "res_sentids_list", "=", "[", "]", "\n", "flags", "=", "[", "]", "\n", "\n", "if", "item", "[", "'verifiable'", "]", "==", "\"VERIFIABLE\"", ":", "\n", "        ", "assert", "item", "[", "'label'", "]", "==", "'SUPPORTS'", "or", "item", "[", "'label'", "]", "==", "'REFUTES'", "\n", "e_list", "=", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "current_id", "=", "item", "[", "'id'", "]", "\n", "assert", "current_id", "in", "additional_data_dictionary", "\n", "additional_data", "=", "additional_data_dictionary", "[", "current_id", "]", "[", "'predicted_sentids'", "]", "\n", "# additional_data_with_score = additional_data_dictionary[current_id]['scored_sentids']", "\n", "\n", "# print(len(additional_data))", "\n", "\n", "for", "evidences", "in", "e_list", ":", "\n", "# print(evidences)", "\n", "            ", "new_evidences", "=", "copy", ".", "deepcopy", "(", "evidences", ")", "\n", "n_e", "=", "len", "(", "evidences", ")", "\n", "if", "n_e", "<", "5", ":", "\n", "                ", "current_sample_num", "=", "random", ".", "randint", "(", "0", ",", "5", "-", "n_e", ")", "\n", "random", ".", "shuffle", "(", "additional_data", ")", "\n", "for", "sampled_e", "in", "additional_data", "[", ":", "current_sample_num", "]", ":", "\n", "                    ", "doc_ids", "=", "sampled_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "0", "]", "\n", "ln", "=", "int", "(", "sampled_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "1", "]", ")", "\n", "new_evidences", ".", "add_sent", "(", "doc_ids", ",", "ln", ")", "\n", "\n", "", "", "if", "new_evidences", "!=", "evidences", ":", "\n", "                ", "flag", "=", "f\"verifiable.non_eq.{len(new_evidences) - len(evidences)}\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "pass", "\n", "", "else", ":", "\n", "                ", "flag", "=", "\"verifiable.eq.0\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "pass", "\n", "", "res_sentids_list", ".", "append", "(", "new_evidences", ")", "\n", "\n", "", "assert", "len", "(", "res_sentids_list", ")", "==", "len", "(", "e_list", ")", "\n", "\n", "", "elif", "item", "[", "'verifiable'", "]", "==", "\"NOT VERIFIABLE\"", ":", "\n", "        ", "assert", "item", "[", "'label'", "]", "==", "'NOT ENOUGH INFO'", "\n", "\n", "e_list", "=", "check_sentences", ".", "check_and_clean_evidence", "(", "item", ")", "\n", "current_id", "=", "item", "[", "'id'", "]", "\n", "\n", "additional_data", "=", "additional_data_dictionary", "[", "current_id", "]", "[", "'predicted_sentids'", "]", "\n", "prioritized_additional_evidence_list", "=", "additional_data_dictionary", "[", "current_id", "]", "[", "'scored_sentids'", "]", "\n", "\n", "#  cur_predicted_sentids.append((sent_i['sid'], sent_i['score'], sent_i['prob']))", "\n", "certain_k", "=", "2", "\n", "prioritized_additional_evidence_list", "=", "sorted", "(", "prioritized_additional_evidence_list", ",", "key", "=", "lambda", "x", ":", "-", "x", "[", "1", "]", ")", "\n", "top_two_sent", "=", "[", "sid", "for", "sid", ",", "_", ",", "_", "in", "prioritized_additional_evidence_list", "[", ":", "certain_k", "]", "]", "\n", "\n", "random", ".", "shuffle", "(", "additional_data", ")", "\n", "current_sample_num", "=", "random", ".", "randint", "(", "0", ",", "2", ")", "\n", "raw_evidences_list", "=", "[", "]", "\n", "\n", "# Debug", "\n", "# print(prioritized_additional_evidence_list)", "\n", "# print(top_two_sent)", "\n", "\n", "for", "sampled_e", "in", "top_two_sent", "+", "additional_data", "[", ":", "current_sample_num", "]", ":", "\n", "            ", "doc_ids", "=", "sampled_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "0", "]", "\n", "ln", "=", "int", "(", "sampled_e", ".", "split", "(", "fever_scorer", ".", "SENT_LINE", ")", "[", "1", "]", ")", "\n", "raw_evidences_list", ".", "append", "(", "(", "doc_ids", ",", "ln", ")", ")", "\n", "", "new_evidences", "=", "check_sentences", ".", "Evidences", "(", "raw_evidences_list", ")", "\n", "\n", "if", "len", "(", "new_evidences", ")", "==", "0", ":", "\n", "            ", "flag", "=", "f\"verifiable.eq.0\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "pass", "\n", "", "else", ":", "\n", "            ", "flag", "=", "f\"not_verifiable.non_eq.{len(new_evidences)}\"", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "\n", "", "assert", "all", "(", "len", "(", "e", ")", "==", "0", "for", "e", "in", "e_list", ")", "\n", "res_sentids_list", ".", "append", "(", "new_evidences", ")", "\n", "assert", "len", "(", "res_sentids_list", ")", "==", "1", "\n", "\n", "# Debug", "\n", "# print(res_sentids_list)", "\n", "\n", "", "assert", "len", "(", "res_sentids_list", ")", "==", "len", "(", "flags", ")", "\n", "\n", "return", "res_sentids_list", ",", "flags", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.__init__": [[12, 28], ["collections.OrderedDict", "model_EMA.EMA.set_back_up_model", "torch.device", "torch.device", "_parameter.detach().clone().to", "_parameter.detach().clone", "_parameter.detach"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.set_back_up_model"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "parameters", ",", "decay", "=", "0.9999", ",", "device_num", "=", "-", "1", ",", "warmup", "=", "True", ")", ":", "\n", "        ", "self", ".", "decay", "=", "decay", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "shadow", "=", "OrderedDict", "(", ")", "\n", "if", "device_num", "==", "-", "1", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "device", "=", "torch", ".", "device", "(", "f\"cuda:{device_num}\"", ")", "\n", "", "self", ".", "warmup", "=", "warmup", "\n", "\n", "for", "_name", ",", "_parameter", "in", "parameters", ":", "\n", "            ", "if", "_parameter", ".", "requires_grad", ":", "\n", "                ", "self", ".", "shadow", "[", "_name", "]", "=", "_parameter", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "", "self", ".", "back_up_model", "=", "None", "\n", "self", ".", "set_back_up_model", "(", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.set_back_up_model": [[29, 32], ["copy.deepcopy", "model_EMA.EMA.back_up_model.to"], "methods", ["None"], ["", "def", "set_back_up_model", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "back_up_model", "=", "copy", ".", "deepcopy", "(", "model", ")", "\n", "self", ".", "back_up_model", "=", "self", ".", "back_up_model", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.__call__": [[33, 47], ["min", "torch.no_grad", "model_EMA.EMA.shadow[].add_", "_parameter.data.to"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "parameters", ")", ":", "\n", "        ", "self", ".", "steps", "+=", "1", "\n", "if", "self", ".", "warmup", ":", "\n", "            ", "decay", "=", "min", "(", "(", "self", ".", "steps", "+", "1", ")", "/", "(", "10", "+", "self", ".", "steps", ")", ",", "self", ".", "decay", ")", "\n", "", "else", ":", "\n", "            ", "decay", "=", "self", ".", "decay", "\n", "\n", "", "for", "_name", ",", "_parameter", "in", "parameters", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "_parameter", ".", "requires_grad", ":", "\n", "                    ", "self", ".", "shadow", "[", "_name", "]", ".", "add_", "(", "(", "1.", "-", "decay", ")", "*", "(", "_parameter", ".", "data", ".", "to", "(", "self", ".", "device", ")", "-", "self", ".", "shadow", "[", "_name", "]", ")", ")", "\n", "# new_average = (1.0 - decay) * _parameter.detach().clone() + decay * self.shadow[_name]", "\n", "# self.shadow[_name] = new_average", "\n", "", "", "", "return", "self", ".", "shadow", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model": [[48, 56], ["model_instance.state_dict", "model_instance.state_dict.update", "model_instance.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["", "def", "get_inference_model", "(", "self", ",", "model_instance", "=", "None", ")", ":", "\n", "        ", "if", "model_instance", "is", "None", ":", "\n", "            ", "model_instance", "=", "self", ".", "back_up_model", "\n", "\n", "", "state_dict", "=", "model_instance", ".", "state_dict", "(", ")", "\n", "state_dict", ".", "update", "(", "self", ".", "shadow", ")", "\n", "model_instance", ".", "load_state_dict", "(", "state_dict", ")", "\n", "return", "model_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_state_dict": [[57, 61], ["model_EMA.EMA.set_back_up_model"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.set_back_up_model"], ["", "def", "load_ema_state_dict", "(", "self", ",", "state_dict", ",", "back_up_model_instance", "=", "None", ")", ":", "\n", "        ", "self", ".", "shadow", "=", "state_dict", "\n", "if", "back_up_model_instance", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_back_up_model", "(", "back_up_model_instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.save_ema_to_file": [[62, 65], ["torch.save"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "save_ema_to_file", "(", "cls", ",", "ema_model", ",", "filename", ")", ":", "\n", "        ", "torch", ".", "save", "(", "ema_model", ".", "shadow", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.load_ema_to_model": [[66, 77], ["model.state_dict", "model.state_dict.update", "model.load_state_dict", "isinstance", "torch.load"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update"], ["", "@", "classmethod", "\n", "def", "load_ema_to_model", "(", "cls", ",", "model", ",", "ema_tracker_instance", ")", ":", "# instance is a file or a EMA object", "\n", "        ", "if", "not", "isinstance", "(", "ema_tracker_instance", ",", "EMA", ")", ":", "\n", "# If ema_model is a filename", "\n", "            ", "ema_shadow", "=", "torch", ".", "load", "(", "ema_tracker_instance", ")", "\n", "", "else", ":", "\n", "            ", "ema_shadow", "=", "ema_tracker_instance", ".", "shadow", "\n", "\n", "", "state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "state_dict", ".", "update", "(", "ema_shadow", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.__init__": [[88, 100], ["list", "enumerate", "print", "model_EMA.EMAGPU.devices.append", "type", "torch.device"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "decays", ")", ":", "\n", "        ", "self", ".", "decays", "=", "list", "(", "decays", ")", "\n", "self", ".", "devices", "=", "[", "]", "\n", "for", "i", ",", "decay", "in", "enumerate", "(", "self", ".", "decays", ")", ":", "\n", "            ", "if", "type", "(", "decay", ")", "is", "int", ":", "\n", "                ", "self", ".", "decays", "[", "i", "]", "=", "1.", "-", "(", "1.", "/", "decay", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "decays", "[", "i", "]", "=", "decay", "\n", "", "self", ".", "devices", ".", "append", "(", "torch", ".", "device", "(", "\"cuda:%d\"", "%", "(", "i", "+", "1", ")", ")", ")", "\n", "", "print", "(", "\"Decays in EMA, \"", ",", "self", ".", "decays", ")", "\n", "self", ".", "shadow", "=", "{", "}", "\n", "self", ".", "backup", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.register": [[101, 106], ["val.detach().clone().to", "zip", "val.detach().clone().to", "val.detach().clone", "val.detach().clone", "val.detach", "val.detach"], "methods", ["None"], ["", "def", "register", "(", "self", ",", "name", ",", "val", ")", ":", "\n", "        ", "self", ".", "shadow", "[", "name", "]", "=", "{", "}", "\n", "self", ".", "backup", "[", "name", "]", "=", "val", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "to", "(", "self", ".", "devices", "[", "0", "]", ")", "\n", "for", "decay", ",", "device", "in", "zip", "(", "self", ".", "decays", ",", "self", ".", "devices", ")", ":", "\n", "            ", "self", ".", "shadow", "[", "name", "]", "[", "decay", "]", "=", "val", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update": [[107, 112], ["zip", "x.data.to", "[].add_"], "methods", ["None"], ["", "", "def", "update", "(", "self", ",", "name", ",", "x", ")", ":", "\n", "        ", "for", "decay", ",", "device", "in", "zip", "(", "self", ".", "decays", ",", "self", ".", "devices", ")", ":", "\n", "            ", "x_gpu", "=", "x", ".", "data", ".", "to", "(", "device", ")", "\n", "assert", "name", "in", "self", ".", "shadow", "\n", "self", ".", "shadow", "[", "name", "]", "[", "decay", "]", ".", "add_", "(", "(", "1.", "-", "decay", ")", "*", "(", "x_gpu", "-", "self", ".", "shadow", "[", "name", "]", "[", "decay", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.apply": [[113, 117], ["x.data.copy_"], "methods", ["None"], ["", "", "def", "apply", "(", "self", ",", "name", ",", "x", ",", "decay", "=", "None", ")", ":", "\n", "        ", "if", "decay", "is", "None", ":", "\n", "            ", "decay", "=", "self", ".", "decays", "[", "0", "]", "\n", "", "x", ".", "data", ".", "copy_", "(", "self", ".", "shadow", "[", "name", "]", "[", "decay", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.make_backup": [[118, 120], ["model_EMA.EMAGPU.backup[].data.copy_"], "methods", ["None"], ["", "def", "make_backup", "(", "self", ",", "name", ",", "x", ")", ":", "\n", "        ", "self", ".", "backup", "[", "name", "]", ".", "data", ".", "copy_", "(", "x", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.recover": [[121, 123], ["x.data.copy_"], "methods", ["None"], ["", "def", "recover", "(", "self", ",", "name", ",", "x", ")", ":", "\n", "        ", "x", ".", "data", ".", "copy_", "(", "self", ".", "backup", "[", "name", "]", ".", "data", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list": [[79, 85], ["range", "torch.cuda.device_count", "parallel_id_list.append"], "function", ["None"], ["", "", "def", "get_ema_gpu_id_list", "(", "master_device_num", "=", "1", ")", ":", "\n", "    ", "parallel_id_list", "=", "[", "master_device_num", "]", "\n", "for", "i", "in", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ":", "\n", "        ", "if", "i", "not", "in", "parallel_id_list", ":", "\n", "            ", "parallel_id_list", ".", "append", "(", "i", ")", "\n", "", "", "return", "parallel_id_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict": [[1, 28], ["scored_dict.items", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append", "[].append", "len"], "function", ["None"], ["def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "None", ",", "result_field", "=", "'sp_doc'", ",", "result_scored_field", "=", "'scored_results'", ")", ":", "\n", "\n", "    ", "results_dict", "=", "{", "result_field", ":", "dict", "(", ")", ",", "result_scored_field", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "result_scored_field", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "]", "\n", "if", "filter_value", "is", "None", ":", "\n", "            ", "results_dict", "[", "result_field", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "", "else", ":", "\n", "            ", "for", "s", ",", "e", "in", "sorted_e_list", ":", "\n", "                ", "if", "s", ">=", "filter_value", ":", "\n", "                    ", "results_dict", "[", "result_field", "]", "[", "key", "]", ".", "append", "(", "e", ")", "\n", "", "if", "len", "(", "results_dict", "[", "result_field", "]", "[", "key", "]", ")", "==", "top_k", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.field_name_convert": [[30, 36], ["None"], "function", ["None"], ["", "def", "field_name_convert", "(", "d_list", ",", "old_field_name", ",", "new_field_name", ")", ":", "\n", "    ", "for", "item", "in", "d_list", ":", "\n", "        ", "assert", "old_field_name", "in", "item", "\n", "item", "[", "new_field_name", "]", "=", "item", "[", "old_field_name", "]", "\n", "del", "item", "[", "old_field_name", "]", "\n", "", "return", "d_list", "\n", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_item": [[10, 62], ["set", "range", "dict", "sent_list.append", "len", "all_facts.append", "str", "str", "str"], "function", ["None"], ["def", "build_sent_match_data_from_distractor_item", "(", "item", ",", "is_training", "=", "False", ",", "title_head", "=", "True", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", "\n", "# print(supporting_doc)", "\n", "\n", "all_facts", "=", "[", "]", "\n", "for", "title", ",", "sentences", "in", "item", "[", "'context'", "]", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "all_facts", ".", "append", "(", "[", "title", ",", "i", ",", "sentences", "[", "i", "]", "]", ")", "\n", "\n", "", "", "for", "fact", "in", "all_facts", ":", "\n", "        ", "d_item", "=", "dict", "(", ")", "\n", "d_item", "[", "'in_sp_doc'", "]", "=", "False", "\n", "title", ",", "sent_num", ",", "sentence", "=", "fact", "\n", "pid", "=", "str", "(", "item", "[", "'_id'", "]", ")", "+", "ID_SEPARATOR", "+", "str", "(", "title", ")", "+", "ID_SEPARATOR", "+", "str", "(", "sent_num", ")", "\n", "if", "is_training", ":", "\n", "            ", "if", "[", "title", ",", "sent_num", "]", "in", "supporting_facts", ":", "\n", "# Positive example", "\n", "                ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'true'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [t] '", "+", "sentence", "\n", "", "else", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "", "d_item", "[", "'in_sp_doc'", "]", "=", "True", "\n", "", "else", ":", "\n", "# Negative example", "\n", "                ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'false'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [t] '", "+", "sentence", "\n", "", "else", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "\n", "", "if", "title", "in", "supporting_doc", ":", "\n", "                    ", "d_item", "[", "'in_sp_doc'", "]", "=", "True", "\n", "", "", "", "else", ":", "\n", "            ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'hidden'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [t] '", "+", "sentence", "\n", "", "else", ":", "\n", "                ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "\n", "", "", "sent_list", ".", "append", "(", "d_item", ")", "\n", "\n", "", "return", "sent_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.build_sent_match_data_from_distractor_list": [[64, 70], ["print", "tqdm.tqdm", "whole_data_list.extend", "sampler_from_distractor.build_sent_match_data_from_distractor_item"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.standardized_sampler_v0.build_sent_match_data_from_distractor_item"], ["", "def", "build_sent_match_data_from_distractor_list", "(", "d_list", ",", "is_training", "=", "False", ",", "title_head", "=", "True", ")", ":", "\n", "    ", "whole_data_list", "=", "[", "]", "\n", "print", "(", "\"Sample sentence match data.\"", ")", "\n", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "        ", "whole_data_list", ".", "extend", "(", "build_sent_match_data_from_distractor_item", "(", "item", ",", "is_training", ",", "title_head", ")", ")", "\n", "", "return", "whole_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.stimulate_prediction_file": [[72, 86], ["dict", "sid.split", "int", "[].append"], "function", ["None"], ["", "def", "stimulate_prediction_file", "(", "d_list", ",", "sent_list", ")", ":", "\n", "    ", "pred_dict", "=", "{", "'sp'", ":", "dict", "(", ")", "}", "\n", "\n", "for", "sent_item", "in", "sent_list", ":", "\n", "        ", "sid", "=", "sent_item", "[", "'selection_id'", "]", "\n", "oid", ",", "title", ",", "sent_num", "=", "sid", ".", "split", "(", "ID_SEPARATOR", ")", "\n", "sent_num", "=", "int", "(", "sent_num", ")", "\n", "# Change this to other later", "\n", "if", "oid", "not", "in", "pred_dict", "[", "'sp'", "]", ":", "\n", "            ", "pred_dict", "[", "'sp'", "]", "[", "oid", "]", "=", "[", "]", "\n", "", "if", "sent_item", "[", "'label'", "]", "==", "'true'", ":", "\n", "            ", "pred_dict", "[", "'sp'", "]", "[", "oid", "]", ".", "append", "(", "[", "title", ",", "sent_num", "]", ")", "\n", "\n", "", "", "return", "pred_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_from_distractor.downsample_negative_examples": [[88, 104], ["numpy.random.rand", "r_list.append", "numpy.random.rand", "r_list.append", "r_list.append"], "function", ["None"], ["", "def", "downsample_negative_examples", "(", "sent_p_list", ",", "selection_prob", ",", "same_doc_prob", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "p_item", "in", "sent_p_list", ":", "\n", "# Down sample neg examples", "\n", "        ", "if", "p_item", "[", "'label'", "]", "==", "'false'", "and", "not", "p_item", "[", "'in_sp_doc'", "]", ":", "\n", "            ", "p_v", "=", "np", ".", "random", ".", "rand", "(", ")", "\n", "if", "p_v", "<", "selection_prob", ":", "\n", "                ", "r_list", ".", "append", "(", "p_item", ")", "\n", "", "", "elif", "p_item", "[", "'label'", "]", "==", "'false'", "and", "p_item", "[", "'in_sp_doc'", "]", ":", "\n", "            ", "p_v", "=", "np", ".", "random", ".", "rand", "(", ")", "\n", "if", "p_v", "<", "same_doc_prob", ":", "\n", "                ", "r_list", ".", "append", "(", "p_item", ")", "\n", "", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "p_item", ")", "\n", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.standardized_sampler_v0.build_paragraph_match_data_from_list": [[10, 17], ["print", "tqdm.tqdm", "whole_data_list.extend", "standardized_sampler_v0.build_paragraph_match_data_from_item"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.standardized_sampler_v0.build_paragraph_match_data_from_item"], ["def", "build_paragraph_match_data_from_list", "(", "d_list", ",", "is_training", "=", "False", ",", "title_head", "=", "True", ")", ":", "\n", "# This method should be combined with the method below (build_paragraph_match_data_from_item) to provide paragraph_match data to be forwarded through neural nets.", "\n", "    ", "whole_data_list", "=", "[", "]", "\n", "print", "(", "\"Sample sentence match data.\"", ")", "\n", "for", "item", "in", "tqdm", "(", "d_list", ")", ":", "\n", "        ", "whole_data_list", ".", "extend", "(", "build_paragraph_match_data_from_item", "(", "item", ",", "is_training", ",", "title_head", ")", ")", "\n", "", "return", "whole_data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.standardized_sampler_v0.build_paragraph_match_data_from_item": [[19, 69], ["dict", "dict.items", "set", "set", "enumerate", "dict", "str", "forward_list.append", "dict", "str", "str", "context_sents_dict.items"], "function", ["None"], ["", "def", "build_paragraph_match_data_from_item", "(", "item", ",", "is_training", "=", "False", ",", "title_head", "=", "True", ")", ":", "\n", "# This method should be combined with the method above (build_paragraph_match_data_from_list)", "\n", "    ", "forward_list", "=", "[", "]", "\n", "\n", "qid", "=", "item", "[", "'_id'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "question", "=", "item", "[", "'question'", "]", "\n", "\n", "if", "is_training", ":", "\n", "        ", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "supporting_facts", "]", ")", "\n", "", "else", ":", "\n", "        ", "supporting_facts", "=", "[", "]", "\n", "supporting_doc", "=", "set", "(", ")", "\n", "\n", "", "retrieved_context_dict", "=", "dict", "(", ")", "\n", "# This is the retrieved upstream context.   The key is document title and the nested dict is (line_num, sentence).", "\n", "\n", "for", "doc_title", ",", "context_sents", "in", "contexts", ":", "\n", "        ", "if", "doc_title", "not", "in", "retrieved_context_dict", ":", "\n", "            ", "retrieved_context_dict", "[", "doc_title", "]", "=", "dict", "(", ")", "\n", "\n", "", "for", "i", ",", "sent", "in", "enumerate", "(", "context_sents", ")", ":", "\n", "            ", "retrieved_context_dict", "[", "doc_title", "]", "[", "i", "]", "=", "sent", "\n", "\n", "", "", "for", "doc_title", ",", "context_sents_dict", "in", "retrieved_context_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitem", "=", "dict", "(", ")", "\n", "paragraph", "=", "''", ".", "join", "(", "[", "s", "for", "k", ",", "s", "in", "context_sents_dict", ".", "items", "(", ")", "]", ")", "\n", "if", "title_head", ":", "\n", "            ", "paragraph", "=", "doc_title", "+", "' [T] '", "+", "paragraph", "\n", "", "fitem", "[", "'context'", "]", "=", "paragraph", "\n", "fitem", "[", "'query'", "]", "=", "question", "\n", "fitem", "[", "'qid'", "]", "=", "qid", "\n", "fid", "=", "str", "(", "qid", ")", "+", "ID_SEPARATOR", "+", "str", "(", "doc_title", ")", "# This id number is deprecated, but we keep it here.", "\n", "fitem", "[", "'doc_title'", "]", "=", "str", "(", "doc_title", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "if", "is_training", ":", "\n", "            ", "if", "doc_title", "in", "supporting_doc", ":", "\n", "# Positive example", "\n", "                ", "fitem", "[", "'selection_label'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "# Negative example", "\n", "                ", "fitem", "[", "'selection_label'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "            ", "fitem", "[", "'selection_label'", "]", "=", "'hidden'", "\n", "\n", "", "forward_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "return", "forward_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.standardized_sampler_v0.build_sent_match_data_from_distractor_item": [[71, 123], ["set", "range", "dict", "sent_list.append", "len", "all_facts.append", "str", "str", "str"], "function", ["None"], ["", "def", "build_sent_match_data_from_distractor_item", "(", "item", ",", "is_training", "=", "False", ",", "title_head", "=", "True", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", "\n", "# print(supporting_doc)", "\n", "\n", "all_facts", "=", "[", "]", "\n", "for", "title", ",", "sentences", "in", "item", "[", "'context'", "]", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "all_facts", ".", "append", "(", "[", "title", ",", "i", ",", "sentences", "[", "i", "]", "]", ")", "\n", "\n", "", "", "for", "fact", "in", "all_facts", ":", "\n", "        ", "d_item", "=", "dict", "(", ")", "\n", "d_item", "[", "'in_sp_doc'", "]", "=", "False", "\n", "title", ",", "sent_num", ",", "sentence", "=", "fact", "\n", "pid", "=", "str", "(", "item", "[", "'_id'", "]", ")", "+", "ID_SEPARATOR", "+", "str", "(", "title", ")", "+", "ID_SEPARATOR", "+", "str", "(", "sent_num", ")", "\n", "if", "is_training", ":", "\n", "            ", "if", "[", "title", ",", "sent_num", "]", "in", "supporting_facts", ":", "\n", "# Positive example", "\n", "                ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'true'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [T] '", "+", "sentence", "\n", "", "else", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "", "d_item", "[", "'in_sp_doc'", "]", "=", "True", "\n", "", "else", ":", "\n", "# Negative example", "\n", "                ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'false'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [T] '", "+", "sentence", "\n", "", "else", ":", "\n", "                    ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "\n", "", "if", "title", "in", "supporting_doc", ":", "\n", "                    ", "d_item", "[", "'in_sp_doc'", "]", "=", "True", "\n", "", "", "", "else", ":", "\n", "            ", "d_item", "[", "'selection_id'", "]", "=", "pid", "\n", "d_item", "[", "'label'", "]", "=", "'hidden'", "\n", "d_item", "[", "'question'", "]", "=", "item", "[", "'question'", "]", "\n", "if", "sent_num", "!=", "0", ":", "\n", "                ", "d_item", "[", "'sentence'", "]", "=", "title", "+", "' [T] '", "+", "sentence", "\n", "", "else", ":", "\n", "                ", "d_item", "[", "'sentence'", "]", "=", "sentence", "\n", "\n", "", "", "sent_list", ".", "append", "(", "d_item", ")", "\n", "\n", "", "return", "sent_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.append_baseline_context": [[15, 27], ["list", "provided_title.append", "set.union", "set", "set"], "function", ["None"], ["def", "append_baseline_context", "(", "doc_results", ",", "baseline_data_list", ")", ":", "\n", "    ", "data_list", "=", "baseline_data_list", "\n", "for", "item", "in", "data_list", ":", "\n", "        ", "key", "=", "item", "[", "'_id'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "provided_title", "=", "[", "]", "\n", "for", "title", ",", "paragraph", "in", "contexts", ":", "\n", "            ", "provided_title", ".", "append", "(", "title", ")", "\n", "\n", "", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", "=", "list", "(", "set", ".", "union", "(", "set", "(", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", ")", ",", "set", "(", "provided_title", ")", ")", ")", "\n", "\n", "", "return", "doc_results", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.append_additional_scored_title": [[29, 48], ["dict", "list", "provided_title.append", "sorted", "set.union", "set", "set"], "function", ["None"], ["", "def", "append_additional_scored_title", "(", "doc_results", ",", "top_k", ",", "terms_based_results_list", ")", ":", "\n", "# terms_based_results_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\")", "\n", "    ", "terms_based_results_dict", "=", "dict", "(", ")", "\n", "for", "item", "in", "terms_based_results_list", ":", "\n", "        ", "terms_based_results_dict", "[", "item", "[", "'qid'", "]", "]", "=", "item", "\n", "\n", "", "for", "item", "in", "data_list", ":", "\n", "        ", "key", "=", "item", "[", "'_id'", "]", "\n", "contexts", "=", "item", "[", "'context'", "]", "\n", "provided_title", "=", "[", "]", "\n", "for", "title", ",", "paragraph", "in", "contexts", ":", "\n", "            ", "provided_title", ".", "append", "(", "title", ")", "\n", "\n", "", "top_term_base", "=", "sorted", "(", "terms_based_results_dict", "[", "key", "]", "[", "'doc_list'", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "[", ":", "top_k", "]", "\n", "top_term_base", "=", "[", "e", "[", "1", "]", "for", "e", "in", "top_term_base", "]", "\n", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", "=", "list", "(", "set", ".", "union", "(", "set", "(", "doc_results", "[", "'sp_doc'", "]", "[", "key", "]", ")", ",", "set", "(", "top_term_base", ")", ")", ")", "\n", "\n", "", "return", "doc_results", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.results_analysis": [[50, 78], ["utils.common.load_json", "hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "utils.common.load_json", "sampler_full_wiki.append_baseline_context", "doc_results[].values", "print", "print", "print", "print", "print", "print", "print", "evaluation.ext_hotpot_eval.eval", "len_list.append", "collections.Counter().most_common", "len", "numpy.mean", "numpy.std", "numpy.max", "numpy.min", "len", "collections.Counter"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.append_baseline_context", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "results_analysis", "(", ")", ":", "\n", "    ", "doc_results", "=", "common", ".", "load_json", "(", "\n", "# config.PRO_ROOT / \"results/doc_retri_results/doc_retrieval_final_v8/hotpot_train_doc_retrieval_v8_before_multihop_filtering.json\")", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/doc_retrieval_final_v8/hotpot_dev_doc_retrieval_v8_before_multihop_filtering.json\"", ")", "\n", "doc_results", "=", "results_multihop_filtering", "(", "doc_results", ",", "multihop_retrieval_top_k", "=", "3", ",", "strict_mode", "=", "True", ")", "\n", "\n", "# terms_based_results_list = common.load_jsonl(", "\n", "#     config.RESULT_PATH / \"doc_retri_results/term_based_methods_results/hotpot_tf_idf_dev.jsonl\")", "\n", "\n", "data_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "# data_list = common.load_json(config.TRAIN_FILE)", "\n", "\n", "append_baseline_context", "(", "doc_results", ",", "data_list", ")", "\n", "\n", "len_list", "=", "[", "]", "\n", "for", "rset", "in", "doc_results", "[", "'sp_doc'", "]", ".", "values", "(", ")", ":", "\n", "        ", "len_list", ".", "append", "(", "len", "(", "rset", ")", ")", "\n", "\n", "", "print", "(", "\"Results with filtering:\"", ")", "\n", "\n", "print", "(", "collections", ".", "Counter", "(", "len_list", ")", ".", "most_common", "(", "10000", ")", ")", "\n", "print", "(", "len", "(", "len_list", ")", ")", "\n", "print", "(", "\"Mean:\\t\"", ",", "np", ".", "mean", "(", "len_list", ")", ")", "\n", "print", "(", "\"Std:\\t\"", ",", "np", ".", "std", "(", "len_list", ")", ")", "\n", "print", "(", "\"Max:\\t\"", ",", "np", ".", "max", "(", "len_list", ")", ")", "\n", "print", "(", "\"Min:\\t\"", ",", "np", ".", "min", "(", "len_list", ")", ")", "\n", "\n", "ext_hotpot_eval", ".", "eval", "(", "doc_results", ",", "data_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.build_full_wiki_document_forward_item": [[80, 133], ["print", "tqdm.tqdm", "list", "forward_item_list.extend", "list", "set.union", "dict", "str", "fitem_list.append", "set", "set", "set", "uuid.uuid4", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item"], ["", "def", "build_full_wiki_document_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ",", "to_text", "=", "True", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "# Forward item:", "\n", "# qid, fid, query, context, doc_t, s_labels.", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "question", "=", "item", "[", "'question'", "]", "\n", "selected_doc", "=", "doc_results", "[", "'sp_doc'", "]", "[", "qid", "]", "\n", "\n", "if", "is_training", ":", "\n", "            ", "gt_doc", "=", "list", "(", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "gt_doc", "=", "[", "]", "\n", "\n", "", "all_doc", "=", "list", "(", "set", ".", "union", "(", "set", "(", "selected_doc", ")", ",", "set", "(", "gt_doc", ")", ")", ")", "\n", "\n", "fitem_list", "=", "[", "]", "\n", "for", "doc", "in", "all_doc", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "qid", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "question", "\n", "fitem", "[", "'doc_t'", "]", "=", "doc", "\n", "# print(doc)", "\n", "# print(doc_results['raw_retrieval_set'][qid])", "\n", "\n", "if", "to_text", ":", "\n", "                ", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ")", "\n", "", "else", ":", "\n", "                ", "context", "=", "\"\"", "\n", "\n", "", "fitem", "[", "'context'", "]", "=", "\" \"", ".", "join", "(", "context", ")", "# Join the context", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "doc", "in", "gt_doc", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "\n", "# print(fitem)", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg": [[135, 175], ["print", "random.shuffle", "random.shuffle", "int", "print", "random.shuffle", "print", "pos_items.append", "neg_items.append", "other_items.append", "len"], "function", ["None"], ["", "def", "down_sample_neg", "(", "fitems", ",", "ratio", "=", "None", ")", ":", "\n", "    ", "if", "ratio", "is", "None", ":", "\n", "        ", "return", "fitems", "\n", "\n", "", "pos_count", "=", "0", "\n", "neg_count", "=", "0", "\n", "other_count", "=", "0", "\n", "\n", "pos_items", "=", "[", "]", "\n", "neg_items", "=", "[", "]", "\n", "other_items", "=", "[", "]", "\n", "\n", "for", "item", "in", "fitems", ":", "\n", "        ", "if", "item", "[", "'s_labels'", "]", "==", "'true'", ":", "\n", "            ", "pos_count", "+=", "1", "\n", "pos_items", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "'s_labels'", "]", "==", "'false'", ":", "\n", "            ", "neg_count", "+=", "1", "\n", "neg_items", ".", "append", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "other_count", "+=", "1", "\n", "other_items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "other_count", "!=", "0", ":", "\n", "        ", "print", "(", "\"Potential Error! We have labels that are not true or false:\"", ",", "other_count", ")", "\n", "\n", "", "print", "(", "f\"Before Sampling, we have {pos_count}/{neg_count} (pos/neg).\"", ")", "\n", "\n", "random", ".", "shuffle", "(", "pos_items", ")", "\n", "random", ".", "shuffle", "(", "neg_items", ")", "\n", "neg_sample_count", "=", "int", "(", "pos_count", "/", "ratio", ")", "\n", "\n", "sampled_neg", "=", "neg_items", "[", ":", "neg_sample_count", "]", "\n", "\n", "print", "(", "f\"After Sampling, we have {pos_count}/{len(sampled_neg)} (pos/neg).\"", ")", "\n", "\n", "sampled_list", "=", "sampled_neg", "+", "pos_items", "\n", "random", ".", "shuffle", "(", "sampled_list", ")", "\n", "\n", "return", "sampled_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.precompute_forward_items_and_cache": [[177, 200], ["utils.common.load_json", "hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "wiki_util.wiki_db_tool.get_cursor", "utils.common.load_json", "sampler_full_wiki.append_baseline_context", "sampler_full_wiki.build_full_wiki_document_forward_item", "print", "utils.common.save_jsonl", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_doc_retri.hotpot_doc_retri_v0.results_multihop_filtering", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.append_baseline_context", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_full_wiki.build_full_wiki_document_forward_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "def", "precompute_forward_items_and_cache", "(", ")", ":", "\n", "# 3 places need to switch from dev to train !!!", "\n", "\n", "    ", "is_training", "=", "False", "\n", "doc_results", "=", "common", ".", "load_json", "(", "\n", "# config.PRO_ROOT / \"results/doc_retri_results/doc_retrieval_final_v8/hotpot_train_doc_retrieval_v8_before_multihop_filtering.json\")", "\n", "# config.PRO_ROOT / \"results/doc_retri_results/doc_retrieval_final_v8/hotpot_dev_doc_retrieval_v8_before_multihop_filtering.json\")", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/doc_retrieval_final_v8/hotpot_test_doc_retrieval_v8_before_multihop_filtering.json\"", ")", "\n", "doc_results", "=", "results_multihop_filtering", "(", "doc_results", ",", "multihop_retrieval_top_k", "=", "3", ",", "strict_mode", "=", "True", ")", "\n", "\n", "# db_cursor = wiki_db_tool.get_cursor(config.WHOLE_WIKI_DB)", "\n", "\n", "t_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "\n", "# data_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "data_list", "=", "common", ".", "load_json", "(", "config", ".", "TEST_FULLWIKI_FILE", ")", "\n", "# data_list = common.load_json(config.TRAIN_FILE)", "\n", "append_baseline_context", "(", "doc_results", ",", "data_list", ")", "\n", "\n", "fitem_list", "=", "build_full_wiki_document_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "t_db_cursor", ",", "True", ")", "\n", "\n", "print", "(", "len", "(", "fitem_list", ")", ")", "\n", "common", ".", "save_jsonl", "(", "fitem_list", ",", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_test_p_level_unlabeled.jsonl\"", ")", "\n", "# common.save_jsonl(fitem_list, config.PDATA_ROOT / \"content_selection_forward\" / \"hotpot_dev_p_level_unlabeled.jsonl\")", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.format_convert": [[16, 47], ["dict", "span_prediction_task_utils.w_processing", "span_prediction_task_utils.pair_w_tokens_with_ground_truth_span", "ready_items.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.w_processing", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.pair_w_tokens_with_ground_truth_span"], ["def", "format_convert", "(", "items", ",", "is_training", ")", ":", "\n", "    ", "ready_items", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "        ", "ready_item", "=", "dict", "(", ")", "\n", "answer_text", "=", "item", "[", "'answer'", "]", "\n", "answer_start", "=", "item", "[", "'answer_start_index'", "]", "\n", "no_answer", "=", "item", "[", "'no_answer'", "]", "\n", "context", "=", "item", "[", "'context'", "]", "\n", "if", "context", "is", "None", "or", "len", "(", "context", ")", "==", "0", ":", "\n", "            ", "context", "=", "'empty'", "\n", "", "is_yes_no_question", "=", "item", "[", "'is_yes_no_question'", "]", "\n", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", "=", "span_utils", ".", "w_processing", "(", "context", ")", "\n", "_", ",", "adjusted_answer_start", ",", "adjusted_answer_end", "=", "span_utils", ".", "pair_w_tokens_with_ground_truth_span", "(", "\n", "context_w_tokens", ",", "w_tokens_char_to_word_offset", ",", "\n", "answer_text", ",", "answer_start", ",", "no_answer", ",", "is_yes_no_question", ",", "is_training", ",", "do_checking", "=", "False", "\n", ")", "\n", "\n", "ready_item", "[", "'uid'", "]", "=", "item", "[", "'qid'", "]", "\n", "ready_item", "[", "'w_token_context'", "]", "=", "context_w_tokens", "\n", "ready_item", "[", "'gt_answer_text'", "]", "=", "answer_text", "\n", "ready_item", "[", "'query'", "]", "=", "item", "[", "'query'", "]", "\n", "ready_item", "[", "'answer_start'", "]", "=", "adjusted_answer_start", "\n", "ready_item", "[", "'answer_end'", "]", "=", "adjusted_answer_end", "\n", "ready_item", "[", "'no_answer'", "]", "=", "no_answer", "\n", "ready_item", "[", "'is_yes_no_question'", "]", "=", "is_yes_no_question", "\n", "ready_items", ".", "append", "(", "ready_item", ")", "\n", "\n", "", "assert", "len", "(", "ready_items", ")", "==", "len", "(", "items", ")", "\n", "\n", "return", "ready_items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.build_qa_forword_item": [[49, 159], ["set", "dict", "list", "random.shuffle", "sorted", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index", "dict", "o_forward_items.append", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "len", "len", "additional_none_overlap_facts.append", "print", "answer.lower", "random.choice", "len", "shuffled_selected_facts.append", "utils.text_process_tool.normalize", "utils.text_process_tool.normalize", "answer.lower", "len", "doc.split", "context_token_list.append", "context_token_list.append", "len", "random.choice", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "def", "build_qa_forword_item", "(", "sentence_level_results", ",", "d_list", ",", "is_training", ",", "db_cursor", ",", "\n", "append_head", "=", "True", ",", "forward_type", "=", "'random_one'", ")", ":", "\n", "    ", "o_forward_items", "=", "[", "]", "\n", "for", "item", "in", "d_list", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "query", "=", "item", "[", "'question'", "]", "\n", "# o_contexts = item['context']", "\n", "answer", "=", "item", "[", "'answer'", "]", "if", "'answer'", "in", "item", "else", "\"##hidden##\"", "\n", "# q_type = item['']", "\n", "\n", "retrieved_facts", "=", "sentence_level_results", "[", "'sp'", "]", "[", "qid", "]", "# remember the key here.", "\n", "supporting_facts", "=", "[", "]", "\n", "if", "is_training", ":", "\n", "            ", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "\n", "", "additional_none_overlap_facts", "=", "[", "]", "\n", "for", "fact", "in", "retrieved_facts", ":", "\n", "            ", "if", "fact", "not", "in", "supporting_facts", ":", "\n", "                ", "additional_none_overlap_facts", ".", "append", "(", "fact", ")", "\n", "\n", "", "", "selected_facts", "=", "additional_none_overlap_facts", "+", "supporting_facts", "\n", "\n", "# Retrieve all the documents:", "\n", "all_document", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "selected_facts", "]", ")", "\n", "sentid2sent_token_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc", "in", "all_document", ":", "\n", "            ", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "# sentence_text = sentence_token", "\n", "                ", "if", "len", "(", "sentence_token", ")", "!=", "0", ":", "\n", "                    ", "sentid2sent_token_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_token", "\n", "\n", "", "", "", "all_document", "=", "list", "(", "all_document", ")", "\n", "random", ".", "shuffle", "(", "all_document", ")", "\n", "# end", "\n", "\n", "shuffled_selected_facts", "=", "[", "]", "\n", "selected_facts", "=", "sorted", "(", "selected_facts", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "for", "doc", "in", "all_document", ":", "\n", "            ", "for", "fact", "in", "selected_facts", ":", "\n", "                ", "if", "fact", "[", "0", "]", "==", "doc", ":", "\n", "                    ", "shuffled_selected_facts", ".", "append", "(", "fact", ")", "\n", "\n", "", "", "", "assert", "len", "(", "shuffled_selected_facts", ")", "==", "len", "(", "selected_facts", ")", "\n", "\n", "cur_doc", "=", "None", "\n", "context_token_list", "=", "[", "]", "\n", "for", "doc", ",", "i", "in", "shuffled_selected_facts", ":", "\n", "            ", "if", "(", "doc", ",", "i", ")", "not", "in", "sentid2sent_token_dict", ":", "\n", "                ", "print", "(", "f\"Potential Error: {(doc, i)} not exists in DB.\"", ")", "\n", "continue", "\n", "# print((doc, i), sentid2sent_token_dict[(doc, i)])", "\n", "", "paragraph_token_list", "=", "sentid2sent_token_dict", "[", "(", "doc", ",", "i", ")", "]", "\n", "\n", "if", "cur_doc", "!=", "doc", "and", "append_head", "and", "i", "!=", "0", ":", "\n", "                ", "for", "cur_token", "in", "doc", ".", "split", "(", "' '", ")", "+", "[", "'.'", "]", "+", "paragraph_token_list", ":", "\n", "                    ", "n_token", "=", "normalize", "(", "cur_token", ")", "\n", "if", "n_token", "is", "not", "None", "and", "len", "(", "n_token", ")", "!=", "0", ":", "\n", "                        ", "context_token_list", ".", "append", "(", "n_token", ")", "\n", "# context_token_list = context_token_list + doc.split(' ') + ['.'] + paragraph_token_list", "\n", "", "", "", "else", ":", "\n", "                ", "for", "cur_token", "in", "paragraph_token_list", ":", "\n", "                    ", "n_token", "=", "normalize", "(", "cur_token", ")", "\n", "if", "n_token", "is", "not", "None", "and", "len", "(", "n_token", ")", "!=", "0", ":", "\n", "                        ", "context_token_list", ".", "append", "(", "n_token", ")", "\n", "# context_token_list += paragraph_token_list", "\n", "\n", "", "", "", "", "context_matcher", "=", "ContextAnswerMatcher", "(", "context_token_list", ",", "uncase", "=", "True", ")", "\n", "context", ",", "answer_start_list", "=", "context_matcher", ".", "concate_and_return_answer_index", "(", "answer", ",", "match_type", "=", "'left'", ")", "\n", "\n", "example_item", "=", "dict", "(", ")", "\n", "example_item", "[", "'query'", "]", "=", "query", "\n", "example_item", "[", "'qid'", "]", "=", "qid", "\n", "example_item", "[", "'context'", "]", "=", "context", "\n", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "\n", "example_item", "[", "'answer'", "]", "=", "answer", "\n", "if", "is_training", ":", "\n", "# Yes or no question:", "\n", "            ", "if", "answer", ".", "lower", "(", ")", "==", "'yes'", ":", "\n", "                ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "2", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "True", "\n", "", "elif", "answer", ".", "lower", "(", ")", "==", "'no'", ":", "\n", "                ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "3", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "True", "\n", "", "elif", "len", "(", "answer_start_list", ")", ">", "0", ":", "\n", "                ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "random", ".", "choice", "(", "answer_start_list", ")", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "", "elif", "len", "(", "answer_start_list", ")", "==", "0", ":", "\n", "                ", "example_item", "[", "'no_answer'", "]", "=", "True", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "-", "1", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "", "", "else", ":", "\n", "            ", "example_item", "[", "'no_answer'", "]", "=", "False", "\n", "example_item", "[", "'answer_start_index'", "]", "=", "random", ".", "choice", "(", "answer_start_list", ")", "if", "len", "(", "answer_start_list", ")", ">", "0", "else", "-", "4", "\n", "example_item", "[", "'is_yes_no_question'", "]", "=", "False", "\n", "\n", "", "o_forward_items", ".", "append", "(", "example_item", ")", "\n", "\n", "# read_fitems = span_preprocess_tool.eitems_to_fitems(o_forward_items, bert_tokenizer, is_training, max_token_context,", "\n", "#                                                     max_token_query, doc_stride, False)", "\n", "\n", "", "return", "o_forward_items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence": [[161, 189], ["wiki_util.wiki_db_tool.get_cursor", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "sampler_s_level_to_qa.build_qa_forword_item", "sampler_s_level_to_qa.format_convert", "span_prediction_task_utils.span_preprocess_tool.eitems_to_fitems", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.build_qa_forword_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.format_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.eitems_to_fitems"], ["", "def", "get_qa_item_with_upstream_sentence", "(", "d_list", ",", "sentence_level_results", ",", "is_training", ",", "\n", "tokenizer", ":", "BertTokenizer", ",", "max_context_length", ",", "max_query_length", ",", "\n", "doc_stride", "=", "128", ",", "\n", "debug_mode", "=", "False", ",", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", ":", "\n", "    ", "t_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "d_list", "=", "d_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'_id'", "]", "for", "item", "in", "d_list", "]", ")", "\n", "sentence_level_results", "=", "[", "item", "for", "item", "in", "sentence_level_results", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "d_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "d_list", ",", "'_id'", ")", "\n", "copied_d_o_dict", "=", "copy", ".", "deepcopy", "(", "d_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "sentence_level_results", ",", "copied_d_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict", "=", "select_top_k_and_to_results_dict", "(", "copied_d_o_dict", ",", "top_k", "=", "top_k", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "filter_value", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "forward_example_items", "=", "build_qa_forword_item", "(", "cur_results_dict", ",", "d_list", ",", "is_training", ",", "t_db_cursor", ")", "\n", "forward_example_items", "=", "format_convert", "(", "forward_example_items", ",", "is_training", ")", "\n", "fitems_dict", ",", "read_fitems_list", "=", "span_preprocess_tool", ".", "eitems_to_fitems", "(", "forward_example_items", ",", "tokenizer", ",", "is_training", ",", "\n", "max_context_length", ",", "\n", "max_query_length", ",", "doc_stride", ",", "False", ")", "\n", "\n", "return", "fitems_dict", ",", "read_fitems_list", ",", "cur_results_dict", "[", "'sp'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.inspect_upstream_eval": [[191, 230], ["utils.common.load_json", "utils.list_dict_data_tool.list_to_dict", "utils.common.load_jsonl", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "evaluation.ext_hotpot_eval.eval", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "inspect_upstream_eval", "(", ")", ":", "\n", "    ", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "dev_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/dev_s_level_bert_v1_results.jsonl\"", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "dev_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "# 0.5", "\n", "# cur_results_dict_v05 = select_top_k_and_to_results_dict(copied_dev_o_dict, top_k=5,", "\n", "#                                                         score_field_name='prob',", "\n", "#                                                         filter_value=0.5,", "\n", "#                                                         result_field='sp')", "\n", "\n", "cur_results_dict_v02", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "filter_value", "=", "0.2", ",", "\n", "result_field", "=", "'sp'", ")", "\n", "\n", "# _, metrics_v5 = ext_hotpot_eval.eval(cur_results_dict_v05, dev_list, verbose=False)", "\n", "\n", "_", ",", "metrics_v2", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_v02", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "v02_sp_f1", "=", "metrics_v2", "[", "'sp_f1'", "]", "\n", "v02_sp_recall", "=", "metrics_v2", "[", "'sp_recall'", "]", "\n", "v02_sp_prec", "=", "metrics_v2", "[", "'sp_prec'", "]", "\n", "\n", "v05_sp_f1", "=", "metrics_v5", "[", "'sp_f1'", "]", "\n", "v05_sp_recall", "=", "metrics_v5", "[", "'sp_recall'", "]", "\n", "v05_sp_prec", "=", "metrics_v5", "[", "'sp_prec'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'v02'", ":", "metrics_v2", ",", "\n", "# 'v05': metrics_v5,", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.inspect_oracle_answer_text": [[232, 320], ["utils.common.load_json", "wiki_util.wiki_db_tool.get_cursor", "print", "set", "dict", "list", "random.shuffle", "sorted", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "len", "len", "len", "print", "print", "print", "print", "print", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher", "span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index", "print", "len", "len", "selected_fact.append", "shuffled_supporting_fact_list.append", "doc.split"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.span_match_and_label.ContextAnswerMatcher.concate_and_return_answer_index"], ["", "def", "inspect_oracle_answer_text", "(", "append_head", "=", "True", ")", ":", "\n", "    ", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "\n", "total", ",", "error", "=", "0", ",", "0", "\n", "\n", "for", "item", "in", "dev_list", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "query", "=", "item", "[", "'question'", "]", "\n", "answer", "=", "item", "[", "'answer'", "]", "\n", "o_contexts", "=", "item", "[", "'context'", "]", "\n", "supporting_facts", "=", "item", "[", "'supporting_facts'", "]", "\n", "\n", "# print(query)", "\n", "# print(answer)", "\n", "\n", "supporting_doc", "=", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", "\n", "selected_fact", "=", "[", "]", "\n", "sentid2sent_token_dict", "=", "dict", "(", ")", "\n", "\n", "for", "doc", "in", "supporting_doc", ":", "\n", "# if doc in gt_doc:", "\n", "#     continue", "\n", "            ", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "# sentence_text = sentence_token", "\n", "                ", "if", "len", "(", "sentence_token", ")", "!=", "0", ":", "\n", "                    ", "selected_fact", ".", "append", "(", "[", "doc", ",", "i", "]", ")", "\n", "sentid2sent_token_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_token", "\n", "\n", "# shuffle doc ordering.", "\n", "", "", "", "supporting_doc", "=", "list", "(", "supporting_doc", ")", "\n", "random", ".", "shuffle", "(", "supporting_doc", ")", "\n", "# end", "\n", "\n", "shuffled_supporting_fact_list", "=", "[", "]", "\n", "supporting_facts", "=", "sorted", "(", "supporting_facts", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "for", "doc", "in", "supporting_doc", ":", "\n", "            ", "for", "fact", "in", "supporting_facts", ":", "\n", "                ", "if", "fact", "[", "0", "]", "==", "doc", ":", "\n", "                    ", "shuffled_supporting_fact_list", ".", "append", "(", "fact", ")", "\n", "\n", "", "", "", "assert", "len", "(", "shuffled_supporting_fact_list", ")", "==", "len", "(", "supporting_facts", ")", "\n", "\n", "# print(supporting_facts)", "\n", "# print(shuffled_supporting_fact_list)", "\n", "#", "\n", "# print(\"Sup Fact.\")", "\n", "cur_doc", "=", "None", "\n", "context_token_list", "=", "[", "]", "\n", "for", "doc", ",", "i", "in", "shuffled_supporting_fact_list", ":", "\n", "            ", "if", "(", "doc", ",", "i", ")", "not", "in", "sentid2sent_token_dict", ":", "\n", "                ", "print", "(", "f\"Potential Error: {(doc, i)} not exists in DB.\"", ")", "\n", "continue", "\n", "# print((doc, i), sentid2sent_token_dict[(doc, i)])", "\n", "", "paragraph_token_list", "=", "sentid2sent_token_dict", "[", "(", "doc", ",", "i", ")", "]", "\n", "\n", "if", "cur_doc", "!=", "doc", "and", "append_head", "and", "i", "!=", "0", ":", "\n", "                ", "context_token_list", "=", "context_token_list", "+", "doc", ".", "split", "(", "' '", ")", "+", "[", "'.'", "]", "+", "paragraph_token_list", "\n", "", "else", ":", "\n", "                ", "context_token_list", "+=", "paragraph_token_list", "\n", "\n", "# print(context_token_list)", "\n", "", "", "context_matcher", "=", "ContextAnswerMatcher", "(", "context_token_list", ",", "uncase", "=", "True", ")", "\n", "context", ",", "answer_start_list", "=", "context_matcher", ".", "concate_and_return_answer_index", "(", "answer", ",", "match_type", "=", "'left'", ")", "\n", "\n", "if", "len", "(", "answer_start_list", ")", ">", "1", ":", "\n", "            ", "error", "+=", "1", "\n", "\n", "", "if", "len", "(", "answer_start_list", ")", "==", "0", "and", "answer", "!=", "'yes'", "and", "answer", "!=", "'no'", ":", "\n", "            ", "print", "(", "\"Error\"", ")", "\n", "print", "(", "\"Query:\"", ",", "query", ")", "\n", "print", "(", "\"Answer:\"", ",", "answer", ")", "\n", "print", "(", "\"Sp fact:\"", ",", "shuffled_supporting_fact_list", ")", "\n", "print", "(", "\"Context:\"", ",", "context", ")", "\n", "\n", "context_matcher", "=", "ContextAnswerMatcher", "(", "context_token_list", ",", "uncase", "=", "True", ")", "\n", "context", ",", "answer_start_list", "=", "context_matcher", ".", "concate_and_return_answer_index", "(", "answer", ")", "\n", "\n", "# print(sentid2sent_token_dict)", "\n", "\n", "# for title, number in supporting_facts:", "\n", "#     print(title, number)", "\n", "", "total", "+=", "1", "\n", "\n", "", "print", "(", "error", ",", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.sanity_check": [[13, 137], ["print", "tqdm.tqdm", "print", "dict", "dict", "enumerate", "list", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "set", "all_fact.append", "len", "selected_fact.append", "len", "selected_fact.append", "b_sent.replace.replace", "db_sent.replace.replace"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item"], ["def", "sanity_check", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ",", "append_head", "=", "True", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "e_count", "=", "0", "\n", "t_count", "=", "0", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "question", "=", "item", "[", "'question'", "]", "\n", "selected_doc", "=", "doc_results", "[", "'sp_doc'", "]", "[", "qid", "]", "\n", "\n", "# sentid2text_dict = dict()", "\n", "sentid2text_dict", ":", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "str", "]", "=", "dict", "(", ")", "\n", "context_dict", "=", "dict", "(", ")", "\n", "for", "doc", ",", "sent_list", "in", "item", "[", "'context'", "]", ":", "\n", "            ", "for", "i", ",", "sent", "in", "enumerate", "(", "sent_list", ")", ":", "\n", "                ", "context_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sent", "\n", "\n", "", "", "if", "is_training", ":", "\n", "            ", "gt_fact", "=", "item", "[", "'supporting_facts'", "]", "\n", "gt_doc", "=", "list", "(", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "gt_fact", "=", "[", "]", "\n", "gt_doc", "=", "[", "]", "\n", "\n", "", "selected_fact", "=", "[", "]", "\n", "for", "doc", "in", "gt_doc", ":", "\n", "            ", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "                ", "sentence_text", "=", "' '", ".", "join", "(", "sentence_token", ")", "\n", "if", "len", "(", "sentence_text", ")", "!=", "0", ":", "\n", "                    ", "selected_fact", ".", "append", "(", "[", "doc", ",", "i", "]", ")", "\n", "sentid2text_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_text", "\n", "\n", "", "", "", "for", "doc", "in", "selected_doc", ":", "\n", "            ", "if", "doc", "in", "gt_doc", ":", "\n", "                ", "continue", "\n", "", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "                ", "sentence_text", "=", "' '", ".", "join", "(", "sentence_token", ")", "\n", "if", "len", "(", "sentence_text", ")", "!=", "0", ":", "\n", "                    ", "selected_fact", ".", "append", "(", "[", "doc", ",", "i", "]", ")", "\n", "sentid2text_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_text", "\n", "\n", "", "", "", "all_fact", "=", "[", "]", "\n", "for", "fact", "in", "selected_fact", "+", "gt_fact", ":", "\n", "            ", "if", "fact", "not", "in", "all_fact", ":", "\n", "                ", "all_fact", ".", "append", "(", "fact", ")", "\n", "\n", "", "", "for", "(", "doc", ",", "i", ")", "in", "context_dict", ":", "\n", "            ", "if", "[", "doc", ",", "i", "]", "in", "gt_fact", ":", "\n", "                ", "if", "(", "doc", ",", "i", ")", "not", "in", "sentid2text_dict", ":", "\n", "                    ", "e_count", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "b_sent", "=", "context_dict", "[", "(", "doc", ",", "i", ")", "]", "\n", "b_sent", "=", "b_sent", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "db_sent", "=", "sentid2text_dict", "[", "(", "doc", ",", "i", ")", "]", "\n", "db_sent", "=", "db_sent", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "\n", "# print(b_sent)", "\n", "# print(db_sent)", "\n", "\n", "if", "b_sent", "!=", "db_sent", ":", "\n", "                        ", "e_count", "+=", "1", "\n", "\n", "", "", "t_count", "+=", "1", "\n", "\n", "# for (doc, i), sent in sentid2text_dict.items():", "\n", "#     if (doc, i) in context_dict:", "\n", "#         b_sent = context_dict[(doc, i)]", "\n", "#         b_sent = b_sent.replace(\" \", \"\")", "\n", "#         db_sent = sentid2text_dict[(doc, i)]", "\n", "#         db_sent = db_sent.replace(\" \", \"\")", "\n", "\n", "# print(b_sent)", "\n", "# print(db_sent)", "\n", "\n", "# if b_sent != db_sent:", "\n", "#     e_count += 1", "\n", "# t_count += 1", "\n", "\n", "", "", "", "print", "(", "e_count", ",", "t_count", ",", "e_count", "/", "t_count", ")", "\n", "\n", "# print(sentid2text_dict[(doc, i)])", "\n", "\n", "# fitem_list = []", "\n", "#", "\n", "# for fact in all_fact:", "\n", "#     fitem = dict()", "\n", "#     fitem['qid'] = qid", "\n", "#     fid = str(uuid.uuid4())", "\n", "#     fitem['fid'] = fid", "\n", "#", "\n", "#     fitem['query'] = question", "\n", "#     fitem['element'] = fact", "\n", "#     # print(doc)", "\n", "#     # print(doc_results['raw_retrieval_set'][qid])", "\n", "#     if (fact[0], fact[1]) not in sentid2text_dict:", "\n", "#         print()  # 'Immanuel Lutheran School (Perryville, Missouri)'", "\n", "#     context = sentid2text_dict[(fact[0], fact[1])]", "\n", "#", "\n", "#     if append_head and fact[1] != 0:", "\n", "#         context = fact[0] + ' . ' + context", "\n", "#", "\n", "#     fitem['context'] = context", "\n", "#", "\n", "#     if is_training:", "\n", "#         if fact in gt_fact:", "\n", "#             fitem['s_labels'] = 'true'", "\n", "#         else:", "\n", "#             fitem['s_labels'] = 'false'", "\n", "#     else:", "\n", "#         fitem['s_labels'] = 'hidden'", "\n", "#", "\n", "#     fitem_list.append(fitem)", "\n", "#", "\n", "# forward_item_list.extend(fitem_list)", "\n", "\n", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.build_sentence_forward_item": [[139, 223], ["print", "tqdm.tqdm", "dict", "forward_item_list.extend", "list", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "wiki_util.wiki_db_tool.get_item_by_key", "wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "enumerate", "dict", "str", "fitem_list.append", "set", "all_fact.append", "uuid.uuid4", "print", "len", "selected_fact.append", "len", "selected_fact.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item"], ["", "def", "build_sentence_forward_item", "(", "doc_results", ",", "data_list", ",", "is_training", ",", "\n", "db_cursor", "=", "None", ",", "append_head", "=", "True", ")", ":", "\n", "    ", "forward_item_list", "=", "[", "]", "\n", "\n", "print", "(", "\"Build forward items\"", ")", "\n", "for", "item", "in", "tqdm", "(", "data_list", ")", ":", "\n", "        ", "qid", "=", "item", "[", "'_id'", "]", "\n", "question", "=", "item", "[", "'question'", "]", "\n", "selected_doc", "=", "doc_results", "[", "'sp_doc'", "]", "[", "qid", "]", "\n", "\n", "# sentid2text_dict = dict()", "\n", "sentid2text_dict", ":", "Dict", "[", "Tuple", "[", "str", ",", "int", "]", ",", "str", "]", "=", "dict", "(", ")", "\n", "\n", "if", "is_training", ":", "\n", "            ", "gt_fact", "=", "item", "[", "'supporting_facts'", "]", "\n", "gt_doc", "=", "list", "(", "set", "(", "[", "fact", "[", "0", "]", "for", "fact", "in", "item", "[", "'supporting_facts'", "]", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "gt_fact", "=", "[", "]", "\n", "gt_doc", "=", "[", "]", "\n", "\n", "", "selected_fact", "=", "[", "]", "\n", "for", "doc", "in", "gt_doc", ":", "\n", "            ", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "                ", "sentence_text", "=", "' '", ".", "join", "(", "sentence_token", ")", "\n", "if", "len", "(", "sentence_text", ")", "!=", "0", ":", "\n", "                    ", "selected_fact", ".", "append", "(", "[", "doc", ",", "i", "]", ")", "\n", "sentid2text_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_text", "\n", "\n", "", "", "", "for", "doc", "in", "selected_doc", ":", "\n", "            ", "if", "doc", "in", "gt_doc", ":", "\n", "                ", "continue", "\n", "", "text_item", "=", "wiki_db_tool", ".", "get_item_by_key", "(", "db_cursor", ",", "key", "=", "doc", ")", "\n", "context", "=", "wiki_db_tool", ".", "get_first_paragraph_from_clean_text_item", "(", "text_item", ",", "flatten_to_paragraph", "=", "False", ",", "\n", "skip_first", "=", "True", ")", "\n", "for", "i", ",", "sentence_token", "in", "enumerate", "(", "context", ")", ":", "\n", "                ", "sentence_text", "=", "' '", ".", "join", "(", "sentence_token", ")", "\n", "if", "len", "(", "sentence_text", ")", "!=", "0", ":", "\n", "                    ", "selected_fact", ".", "append", "(", "[", "doc", ",", "i", "]", ")", "\n", "sentid2text_dict", "[", "(", "doc", ",", "i", ")", "]", "=", "sentence_text", "\n", "\n", "", "", "", "all_fact", "=", "[", "]", "\n", "for", "fact", "in", "selected_fact", "+", "gt_fact", ":", "\n", "            ", "if", "fact", "not", "in", "all_fact", ":", "\n", "                ", "all_fact", ".", "append", "(", "fact", ")", "\n", "\n", "", "", "fitem_list", "=", "[", "]", "\n", "\n", "for", "fact", "in", "all_fact", ":", "\n", "            ", "fitem", "=", "dict", "(", ")", "\n", "fitem", "[", "'qid'", "]", "=", "qid", "\n", "fid", "=", "str", "(", "uuid", ".", "uuid4", "(", ")", ")", "\n", "fitem", "[", "'fid'", "]", "=", "fid", "\n", "\n", "fitem", "[", "'query'", "]", "=", "question", "\n", "fitem", "[", "'element'", "]", "=", "fact", "\n", "# print(doc)", "\n", "# print(doc_results['raw_retrieval_set'][qid])", "\n", "if", "(", "fact", "[", "0", "]", ",", "fact", "[", "1", "]", ")", "not", "in", "sentid2text_dict", ":", "\n", "                ", "print", "(", "f\"Potential Error: {(fact[0], fact[1])} not exists in DB.\"", ")", "# 'Immanuel Lutheran School (Perryville, Missouri)'", "\n", "continue", "\n", "\n", "", "context", "=", "sentid2text_dict", "[", "(", "fact", "[", "0", "]", ",", "fact", "[", "1", "]", ")", "]", "\n", "\n", "if", "append_head", "and", "fact", "[", "1", "]", "!=", "0", ":", "\n", "                ", "context", "=", "fact", "[", "0", "]", "+", "' . '", "+", "context", "\n", "\n", "", "fitem", "[", "'context'", "]", "=", "context", "\n", "\n", "if", "is_training", ":", "\n", "                ", "if", "fact", "in", "gt_fact", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'true'", "\n", "", "else", ":", "\n", "                    ", "fitem", "[", "'s_labels'", "]", "=", "'false'", "\n", "", "", "else", ":", "\n", "                ", "fitem", "[", "'s_labels'", "]", "=", "'hidden'", "\n", "\n", "", "fitem_list", ".", "append", "(", "fitem", ")", "\n", "\n", "", "forward_item_list", ".", "extend", "(", "fitem_list", ")", "\n", "\n", "", "return", "forward_item_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg": [[225, 265], ["print", "random.shuffle", "random.shuffle", "int", "print", "random.shuffle", "print", "pos_items.append", "neg_items.append", "other_items.append", "len"], "function", ["None"], ["", "def", "down_sample_neg", "(", "fitems", ",", "ratio", "=", "None", ")", ":", "\n", "    ", "pos_count", "=", "0", "\n", "neg_count", "=", "0", "\n", "other_count", "=", "0", "\n", "\n", "pos_items", "=", "[", "]", "\n", "neg_items", "=", "[", "]", "\n", "other_items", "=", "[", "]", "\n", "\n", "for", "item", "in", "fitems", ":", "\n", "        ", "if", "item", "[", "'s_labels'", "]", "==", "'true'", ":", "\n", "            ", "pos_count", "+=", "1", "\n", "pos_items", ".", "append", "(", "item", ")", "\n", "", "elif", "item", "[", "'s_labels'", "]", "==", "'false'", ":", "\n", "            ", "neg_count", "+=", "1", "\n", "neg_items", ".", "append", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "other_count", "+=", "1", "\n", "other_items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "other_count", "!=", "0", ":", "\n", "        ", "print", "(", "\"Potential Error! We have labels that are not true or false:\"", ",", "other_count", ")", "\n", "\n", "", "print", "(", "f\"Before Sampling, we have {pos_count}/{neg_count} (pos/neg).\"", ")", "\n", "\n", "if", "ratio", "is", "None", ":", "\n", "        ", "return", "fitems", "\n", "\n", "", "random", ".", "shuffle", "(", "pos_items", ")", "\n", "random", ".", "shuffle", "(", "neg_items", ")", "\n", "neg_sample_count", "=", "int", "(", "pos_count", "/", "ratio", ")", "\n", "\n", "sampled_neg", "=", "neg_items", "[", ":", "neg_sample_count", "]", "\n", "\n", "print", "(", "f\"After Sampling, we have {pos_count}/{len(sampled_neg)} (pos/neg).\"", ")", "\n", "\n", "sampled_list", "=", "sampled_neg", "+", "pos_items", "\n", "random", ".", "shuffle", "(", "sampled_list", ")", "\n", "\n", "return", "sampled_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair": [[267, 295], ["wiki_util.wiki_db_tool.get_cursor", "utils.list_dict_data_tool.list_to_dict", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "hotpot_fact_selection_sampler.sampler_utils.select_top_k_and_to_results_dict", "sentence_level_sampler.build_sentence_forward_item", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.build_sentence_forward_item"], ["", "def", "get_sentence_pair", "(", "top_k", ",", "d_list", ",", "p_level_results_list", ",", "is_training", ",", "debug_mode", "=", "False", ")", ":", "\n", "#", "\n", "    ", "t_db_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "WHOLE_PROCESS_FOR_RINDEX_DB", ")", "\n", "#", "\n", "# dev_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "# dev_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "dev_list", "=", "d_list", "\n", "\n", "# cur_dev_eval_results_list = common.load_jsonl(", "\n", "#     config.PRO_ROOT / \"data/p_hotpotqa/hotpotqa_document_level/2019_4_17/dev_p_level_bert_v1_results.jsonl\")", "\n", "cur_dev_eval_results_list", "=", "p_level_results_list", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'_id'", "]", "for", "item", "in", "dev_list", "]", ")", "\n", "cur_dev_eval_results_list", "=", "[", "item", "for", "item", "in", "p_level_results_list", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_dev_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "cur_results_dict_top2", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "top_k", ",", "filter_value", "=", "None", ")", "\n", "# print(cur_results_dict_top2)", "\n", "fitems", "=", "build_sentence_forward_item", "(", "cur_results_dict_top2", ",", "dev_list", ",", "is_training", "=", "is_training", ",", "\n", "db_cursor", "=", "t_db_cursor", ")", "\n", "\n", "return", "fitems", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_dev_sentence_pair": [[297, 311], ["utils.common.load_json", "sentence_level_sampler.get_sentence_pair", "utils.common.load_jsonl", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_dev_sentence_pair", "(", "top_k", ",", "is_training", ",", "debug", "=", "False", ",", "cur_dev_eval_results_list", "=", "None", ")", ":", "\n", "    ", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "\n", "if", "cur_dev_eval_results_list", "is", "None", ":", "\n", "        ", "cur_dev_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/dev_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'_id'", "]", "for", "item", "in", "dev_list", "]", ")", "\n", "cur_dev_eval_results_list", "=", "[", "item", "for", "item", "in", "cur_dev_eval_results_list", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "return", "get_sentence_pair", "(", "top_k", ",", "dev_list", ",", "cur_dev_eval_results_list", ",", "is_training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_train_sentence_pair": [[313, 327], ["utils.common.load_json", "sentence_level_sampler.get_sentence_pair", "utils.common.load_jsonl", "set"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.get_sentence_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl"], ["", "def", "get_train_sentence_pair", "(", "top_k", ",", "is_training", ",", "debug", "=", "False", ",", "cur_train_eval_results_list", "=", "None", ")", ":", "\n", "    ", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "\n", "if", "cur_train_eval_results_list", "is", "None", ":", "\n", "        ", "cur_train_eval_results_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_paragraph_level/04-10-17:44:54_hotpot_v0_cs/\"", "\n", "\"i(40000)|e(4)|t5_doc_recall(0.8793382849426064)|t5_sp_recall(0.879496479212887)|t10_doc_recall(0.888656313301823)|t5_sp_recall(0.8888325134240054)|seed(12)/train_p_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "", "if", "debug", ":", "\n", "        ", "train_list", "=", "train_list", "[", ":", "100", "]", "\n", "id_set", "=", "set", "(", "[", "item", "[", "'_id'", "]", "for", "item", "in", "train_list", "]", ")", "\n", "cur_train_eval_results_list", "=", "[", "item", "for", "item", "in", "cur_train_eval_results_list", "if", "item", "[", "'qid'", "]", "in", "id_set", "]", "\n", "\n", "", "return", "get_sentence_pair", "(", "top_k", ",", "train_list", ",", "cur_train_eval_results_list", ",", "is_training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_abs_file.iterative_checking": [[11, 28], ["print", "open", "tqdm.tqdm", "json.loads", "print", "print", "print", "print", "json.loads.keys"], "function", ["None"], ["def", "iterative_checking", "(", "check_func_dict", ",", "debug_num", "=", "None", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "with", "open", "(", "config", ".", "ABS_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "print", "(", "item", ")", "\n", "print", "(", "item", ".", "keys", "(", ")", ")", "\n", "print", "(", "item", "[", "'text_with_links'", "]", ")", "\n", "print", "(", "item", "[", "'charoffset_with_links'", "]", ")", "\n", "cur_count", "+=", "1", "\n", "\n", "", "", "print", "(", "\"Total Count:\"", ",", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_abs_file.iterative_cross_checking_abs_whole": [[32, 71], ["print", "print", "print", "open", "open", "next", "next", "json.loads", "json.loads", "print", "print", "print", "print", "print", "print", "Exception", "len"], "function", ["None"], ["", "def", "iterative_cross_checking_abs_whole", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "error_count", "=", "0", "\n", "\n", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_whole_f", ",", "open", "(", "config", ".", "ABS_WIKI_FILE", ",", "'rb'", ")", "as", "in_abs_f", ":", "\n", "\n", "        ", "while", "True", ":", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "", "whl_line", "=", "next", "(", "in_whole_f", ")", "\n", "abs_line", "=", "next", "(", "in_abs_f", ")", "\n", "\n", "if", "whl_line", "and", "abs_line", ":", "\n", "                ", "whl_item", "=", "json", ".", "loads", "(", "whl_line", ")", "\n", "abs_item", "=", "json", ".", "loads", "(", "abs_line", ")", "\n", "\n", "cur_count", "+=", "1", "\n", "\n", "the_para", "=", "None", "\n", "for", "whl_para", "in", "whl_item", "[", "'text'", "]", "[", "1", ":", "]", ":", "\n", "                    ", "print", "(", "whl_para", ")", "\n", "if", "len", "(", "''", ".", "join", "(", "whl_para", ")", ")", ">", "50", ":", "\n", "                        ", "the_para", "=", "whl_para", "\n", "break", "\n", "\n", "", "", "if", "the_para", "!=", "abs_item", "[", "'text_with_links'", "]", ":", "\n", "                    ", "print", "(", "abs_item", "[", "'title'", "]", ")", "\n", "print", "(", "whl_item", "[", "'title'", "]", ")", "\n", "print", "(", "the_para", ")", "\n", "print", "(", "whl_item", "[", "'text'", "]", ")", "\n", "print", "(", "abs_item", "[", "'text_with_links'", "]", ")", "\n", "# print(whl_item['text'][1])", "\n", "error_count", "+=", "1", "\n", "raise", "Exception", "(", ")", "\n", "\n", "", "", "", "", "print", "(", "error_count", ")", "\n", "print", "(", "cur_count", ")", "\n", "print", "(", "error_count", "/", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_abs_file.iterative_cross_checking_abs_whole_from_db": [[73, 116], ["print", "print", "print", "sqlitedict.SqliteDict", "sqlitedict.SqliteDict", "tqdm.tqdm", "tqdm.tqdm", "str", "str", "abs_db.iterkeys", "titles.append", "len", "print", "print", "print", "print", "print", "print", "print", "Exception", "len"], "function", ["None"], ["", "def", "iterative_cross_checking_abs_whole_from_db", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "error_count", "=", "0", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "ABS_WIKI_DB", ")", ",", "flag", "=", "'r'", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "abs_db", ",", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ",", "flag", "=", "'r'", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_db", ":", "\n", "\n", "        ", "titles", "=", "[", "]", "\n", "for", "title", "in", "tqdm", "(", "abs_db", ".", "iterkeys", "(", ")", ",", "total", "=", "len", "(", "abs_db", ")", ")", ":", "\n", "            ", "titles", ".", "append", "(", "title", ")", "\n", "\n", "", "for", "title", "in", "tqdm", "(", "titles", ")", ":", "\n", "            ", "abs_item", "=", "abs_db", "[", "title", "]", "\n", "if", "title", "not", "in", "whole_db", ":", "\n", "                ", "print", "(", "f\"Title: {title} not in whole_db\"", ")", "\n", "return", "\n", "", "else", ":", "\n", "                ", "whl_item", "=", "whole_db", "[", "title", "]", "\n", "\n", "", "cur_count", "+=", "1", "\n", "\n", "the_para", "=", "None", "\n", "for", "whl_para", "in", "whl_item", "[", "'text'", "]", "[", "0", ":", "]", ":", "\n", "# print(whl_para)", "\n", "                ", "if", "len", "(", "''", ".", "join", "(", "whl_para", ")", ")", ">=", "50", ":", "\n", "                    ", "the_para", "=", "whl_para", "\n", "break", "\n", "\n", "", "", "if", "the_para", "!=", "abs_item", "[", "'text_with_links'", "]", ":", "\n", "                ", "print", "(", "abs_item", "[", "'title'", "]", ")", "\n", "print", "(", "whl_item", "[", "'title'", "]", ")", "\n", "print", "(", "the_para", ")", "\n", "print", "(", "whl_item", "[", "'text'", "]", ")", "\n", "print", "(", "the_para", ")", "\n", "print", "(", "abs_item", "[", "'text_with_links'", "]", ")", "\n", "# print(whl_item['text'][1])", "\n", "error_count", "+=", "1", "\n", "raise", "Exception", "(", ")", "\n", "\n", "", "", "", "print", "(", "error_count", ")", "\n", "print", "(", "cur_count", ")", "\n", "print", "(", "error_count", "/", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.stats_info.flatten_counter_info": [[8, 23], ["print", "list", "max", "min", "range", "counter_dict.keys", "counter_dict.keys", "y.append", "y.append"], "function", ["None"], ["def", "flatten_counter_info", "(", "counter_dict", ",", "range_min", "=", "None", ",", "range_max", "=", "None", ")", ":", "\n", "    ", "max_key", "=", "max", "(", "counter_dict", ".", "keys", "(", ")", ")", "if", "range_max", "is", "None", "else", "range_max", "\n", "min_key", "=", "min", "(", "counter_dict", ".", "keys", "(", ")", ")", "if", "range_min", "is", "None", "else", "range_min", "\n", "\n", "print", "(", "f\"Range from {min_key} to {max_key}.\"", ")", "\n", "\n", "x", "=", "list", "(", "range", "(", "min_key", ",", "max_key", "+", "1", ")", ")", "\n", "y", "=", "[", "]", "\n", "for", "i", "in", "x", ":", "\n", "        ", "if", "i", "in", "counter_dict", ":", "\n", "            ", "y", ".", "append", "(", "counter_dict", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "            ", "y", ".", "append", "(", "0", ")", "\n", "\n", "", "", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_identifier": [[17, 22], ["None"], "function", ["None"], ["def", "check_identifier", "(", "item", ")", ":", "\n", "    ", "if", "item", "[", "'title'", "]", "==", "''", ".", "join", "(", "item", "[", "'text'", "]", "[", "0", "]", ")", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_sent_para_stats": [[24, 40], ["sentence_nums_in_paragraph.append"], "function", ["None"], ["", "", "def", "check_sent_para_stats", "(", "item", ")", ":", "\n", "    ", "total_sentence_num", "=", "0", "\n", "total_paragraph_num", "=", "0", "\n", "sentence_nums_in_paragraph", "=", "[", "]", "\n", "\n", "for", "paragraph", "in", "item", "[", "'text'", "]", ":", "# Paragraph level", "\n", "        ", "total_paragraph_num", "+=", "1", "\n", "cur_paragraph_sent_num", "=", "0", "\n", "\n", "for", "_", "in", "paragraph", ":", "\n", "            ", "total_sentence_num", "+=", "1", "\n", "cur_paragraph_sent_num", "+=", "1", "\n", "\n", "", "sentence_nums_in_paragraph", ".", "append", "(", "cur_paragraph_sent_num", ")", "\n", "\n", "", "return", "total_paragraph_num", ",", "total_sentence_num", ",", "sentence_nums_in_paragraph", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_token_stats": [[42, 62], ["zip", "token_per_a.append", "zip", "token_per_p.append", "token_per_s.append"], "function", ["None"], ["", "def", "check_token_stats", "(", "item", ")", ":", "\n", "    ", "token_per_a", "=", "[", "]", "\n", "token_per_p", "=", "[", "]", "\n", "token_per_s", "=", "[", "]", "\n", "\n", "token_in_a", "=", "0", "\n", "for", "paragraphs", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "        ", "token_in_p", "=", "0", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraphs", ",", "paragraph_offsets", ")", ":", "# sentence", "\n", "            ", "token_in_s", "=", "0", "\n", "for", "start", ",", "end", "in", "sentence_offsets", ":", "# each token", "\n", "                ", "token_in_s", "+=", "1", "\n", "token_in_p", "+=", "1", "\n", "token_in_a", "+=", "1", "\n", "\n", "", "token_per_s", ".", "append", "(", "token_in_s", ")", "\n", "", "token_per_p", ".", "append", "(", "token_in_p", ")", "\n", "", "token_per_a", ".", "append", "(", "token_in_a", ")", "\n", "\n", "return", "token_per_a", ",", "token_per_p", ",", "token_per_s", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.iterative_counting_token": [[64, 101], ["collections.Counter", "collections.Counter", "collections.Counter", "print", "print", "open", "tqdm.tqdm", "open", "json.dump", "open", "json.dump", "open", "json.dump", "len", "json.loads", "inspect_whole_file.check_token_stats", "collections.Counter.update", "collections.Counter.update", "collections.Counter.update", "inspect_whole_file.check_boundary"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_token_stats", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_boundary"], ["", "def", "iterative_counting_token", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "a_counter", "=", "Counter", "(", ")", "\n", "p_counter", "=", "Counter", "(", ")", "\n", "s_counter", "=", "Counter", "(", ")", "\n", "\n", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "if", "not", "check_boundary", "(", "item", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "cur_count", "+=", "1", "\n", "\n", "token_per_a", ",", "token_per_p", ",", "token_per_s", "=", "check_token_stats", "(", "item", ")", "\n", "\n", "a_counter", ".", "update", "(", "token_per_a", ")", "\n", "p_counter", ".", "update", "(", "token_per_p", ")", "\n", "s_counter", ".", "update", "(", "token_per_s", ")", "\n", "\n", "", "", "with", "open", "(", "\"t_a_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "a_counter", ",", "out_f", ")", "\n", "\n", "", "with", "open", "(", "\"t_p_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "p_counter", ",", "out_f", ")", "\n", "\n", "", "with", "open", "(", "\"t_s_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "s_counter", ",", "out_f", ")", "\n", "\n", "", "print", "(", "cur_count", ")", "\n", "print", "(", "len", "(", "a_counter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.iterative_counting_sent_para": [[103, 139], ["collections.Counter", "collections.Counter", "collections.Counter", "print", "open", "tqdm.tqdm", "open", "json.dump", "open", "json.dump", "open", "json.dump", "json.loads", "inspect_whole_file.check_sent_para_stats", "collections.Counter.update", "collections.Counter.update", "collections.Counter.update", "inspect_whole_file.check_boundary"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_sent_para_stats", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_boundary"], ["", "def", "iterative_counting_sent_para", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "total_sent_counter", "=", "Counter", "(", ")", "\n", "total_para_counter", "=", "Counter", "(", ")", "\n", "sent_per_para_counter", "=", "Counter", "(", ")", "\n", "\n", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "if", "not", "check_boundary", "(", "item", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "cur_count", "+=", "1", "\n", "\n", "total_paragraph_num", ",", "total_sentence_num", ",", "sentence_nums_in_paragraph", "=", "check_sent_para_stats", "(", "item", ")", "\n", "\n", "total_para_counter", ".", "update", "(", "[", "total_paragraph_num", "]", ")", "\n", "total_sent_counter", ".", "update", "(", "[", "total_sentence_num", "]", ")", "\n", "sent_per_para_counter", ".", "update", "(", "sentence_nums_in_paragraph", ")", "\n", "\n", "", "", "with", "open", "(", "\"total_para_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "total_para_counter", ",", "out_f", ")", "\n", "\n", "", "with", "open", "(", "\"total_sent_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "total_sent_counter", ",", "out_f", ")", "\n", "\n", "", "with", "open", "(", "\"sent_per_para_counter.json\"", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'w'", ")", "as", "out_f", ":", "\n", "        ", "json", ".", "dump", "(", "sent_per_para_counter", ",", "out_f", ")", "\n", "\n", "", "print", "(", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.paragraph_text_fixing": [[141, 144], ["text.replace.replace"], "function", ["None"], ["", "def", "paragraph_text_fixing", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "    ", "text", "=", "text", ".", "replace", "(", "\"</a\u00bb\"", ",", "\"</a>\"", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_hyperlink_match_text": [[146, 221], ["zip", "len", "len", "zip", "len", "len", "cur_sent_tokens.append", "cur_token.startswith", "lxml.etree.fromstring", "etree.fromstring.get", "print", "print", "results_hyperlinks.append", "lxml.etree.fromstring", "lxml.etree.fromstring"], "function", ["None"], ["", "def", "check_hyperlink_match_text", "(", "item", ")", ":", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "# Article level", "\n", "for", "paragraphs", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "        ", "assert", "len", "(", "paragraphs", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraphs", ")", "# Text of the whole paragraph", "\n", "# paragraph_text_fixing(paragraph_text)", "\n", "\n", "in_hyperlink", "=", "False", "\n", "start_hyperlink_index", "=", "0", "\n", "start_inner_hyperlink_index", "=", "0", "\n", "end_hypoerlink_index", "=", "0", "\n", "end_inner_hyperlink_index", "=", "0", "\n", "\n", "# if item['title'] == 'Franz G\u00fcrtner':", "\n", "#     what = 0", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraphs", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list", "\n", "            ", "cur_sent_tokens", "=", "[", "]", "\n", "results_hyperlinks", "=", "[", "]", "\n", "\n", "for", "start", ",", "end", "in", "sentence_offsets", ":", "\n", "                ", "cur_token", "=", "paragraph_text", "[", "start", ":", "end", "]", "\n", "cur_sent_tokens", ".", "append", "(", "cur_token", ")", "\n", "\n", "if", "cur_token", ".", "startswith", "(", "\"<a href=\\\"\"", ")", ":", "\n", "                    ", "if", "\"<a href=\\\"http%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"https%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"//\"", "in", "cur_token", ":", "# Ignore external links.", "\n", "                        ", "continue", "\n", "\n", "", "if", "in_hyperlink", ":", "\n", "# We didn't find a correct \"</a>\" to close", "\n", "                        ", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "hl_head", "=", "etree", ".", "fromstring", "(", "cur_hyperlink_start_token", "+", "\"</a>\"", ")", "\n", "hl_href", "=", "hl_head", ".", "get", "(", "'href'", ")", "\n", "# Remember here to append another indices", "\n", "\n", "# in_hyperlink = True", "\n", "\n", "# print(item)", "\n", "# print(item['title'])", "\n", "# print(sentence)", "\n", "# print(cur_sent_tokens)", "\n", "# print(cur_token)", "\n", "print", "(", "hl_href", ")", "\n", "print", "(", "\"Potential Error. Check This.\"", ")", "\n", "# raise ValueError(\"Hyperlink Parsing Error!\")", "\n", "\n", "", "in_hyperlink", "=", "True", "\n", "start_hyperlink_index", "=", "start", "\n", "start_inner_hyperlink_index", "=", "end", "\n", "\n", "", "elif", "cur_token", "==", "\"</a>\"", "or", "(", "cur_token", "==", "\"\u00bb\"", "and", "paragraph_text", "[", "end", "-", "4", ":", "end", "]", "==", "\"</a\u00bb\"", ")", ":", "\n", "                    ", "if", "not", "in_hyperlink", ":", "\n", "                        ", "continue", "# Fail to reveal the start.", "\n", "\n", "", "in_hyperlink", "=", "False", "\n", "end_hypoerlink_index", "=", "end", "\n", "end_inner_hyperlink_index", "=", "start", "\n", "\n", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "cur_hyperlink", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "end_hypoerlink_index", "]", "\n", "cur_hyperlink_inner_text", "=", "paragraph_text", "[", "start_inner_hyperlink_index", ":", "end_inner_hyperlink_index", "]", "\n", "\n", "results_hyperlinks", ".", "append", "(", "cur_hyperlink", ")", "\n", "# print(cur_hyperlink)", "\n", "try", ":", "\n", "                        ", "hl", "=", "etree", ".", "fromstring", "(", "cur_hyperlink", ")", "\n", "assert", "hl", ".", "text", "==", "cur_hyperlink_inner_text", "\n", "\n", "", "except", ":", "\n", "                        ", "hl_head", "=", "etree", ".", "fromstring", "(", "cur_hyperlink_start_token", "+", "\"</a>\"", ")", "\n", "\n", "", "", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_boundary": [[223, 241], ["zip", "len", "len", "zip", "len", "len", "sentence.strip"], "function", ["None"], ["", "def", "check_boundary", "(", "item", ")", ":", "# Let's just delete all the boundary errors.", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "# Article level", "\n", "for", "paragraph", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "        ", "assert", "len", "(", "paragraph", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraph", ")", "# Text of the whole paragraph", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraph", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list", "\n", "            ", "sentence_start", "=", "sentence_offsets", "[", "0", "]", "[", "0", "]", "\n", "sentence_end", "=", "sentence_offsets", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "# Remember to strip the sentence to exclude any space in the original text.", "\n", "if", "sentence", ".", "strip", "(", ")", "!=", "paragraph_text", "[", "sentence_start", ":", "sentence_end", "]", ":", "\n", "                ", "return", "False", "\n", "\n", "# Important 2019/1/11 we just ignore the boundary error containing pages.", "\n", "# Total 660 boundary errors.", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index": [[244, 253], ["enumerate", "len"], "function", ["None"], ["", "def", "get_first_paragraph_index", "(", "item", ",", "text_field_name", "=", "None", ")", ":", "\n", "    ", "first_para_index", "=", "-", "1", "\n", "text_field_name", "=", "'text'", "if", "text_field_name", "is", "None", "else", "text_field_name", "\n", "for", "i", ",", "para", "in", "enumerate", "(", "item", "[", "text_field_name", "]", ")", ":", "\n", "        ", "cur_para", "=", "''", ".", "join", "(", "para", ")", "\n", "if", "len", "(", "cur_para", ")", ">=", "50", ":", "\n", "            ", "first_para_index", "=", "i", "\n", "break", "\n", "", "", "return", "first_para_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.iterative_save_all_title": [[256, 271], ["open", "open", "tqdm.tqdm", "json.loads", "out_f.write"], "function", ["None"], ["", "def", "iterative_save_all_title", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "# title_set = set()", "\n", "\n", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ",", "open", "(", "\"title_set.txt\"", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "cur_count", "+=", "1", "\n", "\n", "out_f", ".", "write", "(", "item", "[", "'title'", "]", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.iterative_checking": [[273, 306], ["check_func_dict.items", "error_count_dict.items", "print", "open", "tqdm.tqdm", "print", "json.loads", "check_func_dict.items", "inspect_whole_file.check_boundary", "vfunc", "print", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_boundary"], ["", "", "", "def", "iterative_checking", "(", "check_func_dict", ",", "debug_num", "=", "None", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "error_count_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "check_func_dict", ".", "items", "(", ")", ":", "\n", "        ", "error_count_dict", "[", "f\"{key}_count\"", "]", "=", "0", "\n", "\n", "", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "cur_count", "+=", "1", "\n", "\n", "# print(item)", "\n", "\n", "if", "not", "check_boundary", "(", "item", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "key", ",", "vfunc", "in", "check_func_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "not", "vfunc", "(", "item", ")", ":", "\n", "                    ", "error_count_dict", "[", "f\"{key}_count\"", "]", "=", "error_count_dict", "[", "f\"{key}_count\"", "]", "+", "1", "\n", "if", "verbose", "and", "key", "==", "'title_match_first_para'", ":", "\n", "                        ", "print", "(", "'title:'", ",", "item", "[", "'title'", "]", ")", "\n", "print", "(", "'text_0:'", ",", "' '", ".", "join", "(", "item", "[", "'text'", "]", "[", "0", "]", ")", ")", "\n", "\n", "", "", "", "", "", "for", "key", ",", "value", "in", "error_count_dict", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "key", ",", "value", ")", "\n", "\n", "", "print", "(", "\"Total Count:\"", ",", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.init_inspect.get_all_wiki_doc": [[17, 33], ["init_inspect.get_all_wiki_doc.append_wiki_file_list"], "function", ["None"], ["def", "get_all_wiki_doc", "(", "wiki_path", ")", ":", "\n", "    ", "wfile_list", "=", "[", "]", "\n", "\n", "def", "append_wiki_file_list", "(", "current_path", ",", "file_list", ")", ":", "\n", "        ", "if", "current_path", ".", "is_file", "(", ")", ":", "\n", "            ", "file_list", ".", "append", "(", "current_path", ")", "\n", "", "else", ":", "\n", "            ", "for", "subdir", "in", "current_path", ".", "iterdir", "(", ")", ":", "\n", "                ", "append_wiki_file_list", "(", "subdir", ",", "file_list", ")", "\n", "\n", "", "", "", "append_wiki_file_list", "(", "wiki_path", ",", "wfile_list", ")", "\n", "for", "wiki_file", "in", "wfile_list", ":", "\n", "        ", "if", "wiki_file", ".", "suffix", "!=", "'.bz2'", ":", "\n", "            ", "print", "(", "\"Potential wrong file type!\"", ")", "\n", "\n", "", "", "return", "wfile_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.init_inspect.file2json_items": [[35, 43], ["bz2.open", "json.loads", "json.loads", "items.append"], "function", ["None"], ["", "def", "file2json_items", "(", "file_path_name", ")", ":", "\n", "    ", "items", "=", "[", "]", "\n", "with", "bz2", ".", "open", "(", "file_path_name", ",", "mode", "=", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "in_f", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "items", ".", "append", "(", "item", ")", "\n", "\n", "", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.init_inspect.write_to_file": [[45, 52], ["open", "tqdm.tqdm", "enumerate", "bz2.open", "out_f.write"], "function", ["None"], ["", "def", "write_to_file", "(", "file_path_list", ",", "out_file_path_name", ")", ":", "\n", "    ", "with", "open", "(", "out_file_path_name", ",", "mode", "=", "'wb'", ")", "as", "out_f", ":", "\n", "        ", "for", "i", ",", "file_path_name", "in", "tqdm", "(", "enumerate", "(", "file_path_list", ")", ")", ":", "\n", "            ", "with", "bz2", ".", "open", "(", "file_path_name", ",", "mode", "=", "'rb'", ")", "as", "in_f", ":", "\n", "# print(f\"Writing {i} file from\", file_path_name, \"to\", out_file_path_name)", "\n", "                ", "for", "line", "in", "in_f", ":", "\n", "                    ", "out_f", ".", "write", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.init_inspect.write_to_db": [[54, 66], ["sqlitedict.SqliteDict", "tqdm.tqdm", "db_dict.commit", "db_dict.close", "str", "enumerate", "bz2.open", "json.loads", "json.loads"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close"], ["", "", "", "", "", "def", "write_to_db", "(", "file_path_list", ",", "out_file_path_name", ")", ":", "\n", "    ", "with", "SqliteDict", "(", "str", "(", "out_file_path_name", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "db_dict", ":", "\n", "        ", "for", "i", ",", "file_path_name", "in", "tqdm", "(", "enumerate", "(", "file_path_list", ")", ")", ":", "\n", "            ", "with", "bz2", ".", "open", "(", "file_path_name", ",", "mode", "=", "'rb'", ")", "as", "in_f", ":", "\n", "# print(f\"Writing {i} file from\", file_path_name, \"to\", out_file_path_name)", "\n", "                ", "for", "line", "in", "in_f", ":", "\n", "                    ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "title", "=", "item", "[", "'title'", "]", "\n", "db_dict", "[", "title", "]", "=", "item", "\n", "\n", "", "", "", "db_dict", ".", "commit", "(", ")", "\n", "db_dict", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.init_inspect.file_to_db": [[68, 78], ["open", "sqlitedict.SqliteDict", "tqdm.tqdm", "db_dict.commit", "db_dict.close", "str", "json.loads", "json.loads"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close"], ["", "", "def", "file_to_db", "(", "file_pathname", ",", "db_path_name", ")", ":", "\n", "    ", "with", "open", "(", "file_pathname", ",", "encoding", "=", "'utf-8'", ",", "mode", "=", "'r'", ")", "as", "in_f", ",", "SqliteDict", "(", "str", "(", "db_path_name", ")", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "db_dict", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "line", ")", "\n", "title", "=", "item", "[", "'title'", "]", "\n", "db_dict", "[", "title", "]", "=", "item", "\n", "\n", "", "db_dict", ".", "commit", "(", ")", "\n", "db_dict", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.extract_href": [[22, 31], ["list", "extract_pattern.finditer", "len", "list", "matches[].group", "matches[].group", "backup_extract_pattern.finditer", "len", "matches[].group"], "function", ["None"], ["def", "extract_href", "(", "h_text", ")", ":", "\n", "    ", "matches", "=", "list", "(", "extract_pattern", ".", "finditer", "(", "h_text", ")", ")", "\n", "if", "len", "(", "matches", ")", "!=", "0", ":", "\n", "        ", "return", "matches", "[", "0", "]", ".", "group", "(", "1", ")", ",", "matches", "[", "0", "]", ".", "group", "(", "2", ")", "\n", "", "else", ":", "\n", "        ", "matches", "=", "list", "(", "backup_extract_pattern", ".", "finditer", "(", "h_text", ")", ")", "\n", "if", "len", "(", "matches", ")", "==", "0", ":", "\n", "            ", "return", "None", ",", "None", "\n", "", "return", "matches", "[", "0", "]", ".", "group", "(", "1", ")", ",", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.spacy_get_pos": [[33, 42], ["spacy.tokens.doc.Doc", "spacy.tokens.doc.Doc", "proc"], "function", ["None"], ["", "", "def", "spacy_get_pos", "(", "tokens", ")", ":", "\n", "    ", "doc", "=", "spacy", ".", "tokens", ".", "doc", ".", "Doc", "(", "\n", "nlp", ".", "vocab", ",", "words", "=", "tokens", ")", "\n", "\n", "for", "name", ",", "proc", "in", "nlp", ".", "pipeline", ":", "\n", "        ", "if", "name", "==", "'tagger'", ":", "\n", "            ", "proc", "(", "doc", ")", "\n", "\n", "", "", "return", "[", "token", ".", "pos_", "for", "token", "in", "doc", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.convert_current_item": [[44, 184], ["zip", "len", "len", "zip", "len", "len", "enumerate", "aricle_valid_sentences.append", "aricle_valid_hyperlinks.append", "aricle_valid_paragraph_tags.append", "aricle_valid_raw_tokens.append", "valid_raw_tokens.append", "cur_sent_tokens.append", "cur_token.startswith", "valid_tokens.append", "lxml.etree.fromstring", "etree.fromstring.get", "print", "print", "results_hyperlinks.append", "db_convert_and_enhance.extract_href", "urllib.parse.unquote", "len", "aricle_valid_hyperlinks[].append", "valid_hyperlinks.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.extract_href"], ["", "def", "convert_current_item", "(", "item", ")", ":", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "# Article level", "\n", "\n", "processed_item", "=", "{", "\n", "'sentences'", ":", "[", "]", ",", "\n", "'paragraph_tags'", ":", "[", "]", ",", "\n", "'hyperlinks'", ":", "[", "]", ",", "\n", "'poss'", ":", "[", "]", "\n", "}", "\n", "\n", "paragraph_num", "=", "0", "\n", "\n", "aricle_valid_paragraph_tags", "=", "[", "]", "\n", "aricle_valid_poss", "=", "[", "]", "\n", "aricle_valid_sentences", "=", "[", "]", "\n", "aricle_valid_hyperlinks", "=", "[", "]", "\n", "aricle_valid_raw_tokens", "=", "[", "]", "\n", "\n", "for", "paragraphs", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "        ", "assert", "len", "(", "paragraphs", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraphs", ")", "# Text of the whole paragraph", "\n", "# paragraph_text_fixing(paragraph_text)", "\n", "\n", "in_hyperlink", "=", "False", "\n", "start_hyperlink_index", "=", "0", "\n", "start_inner_hyperlink_index", "=", "0", "\n", "end_hypoerlink_index", "=", "0", "\n", "end_inner_hyperlink_index", "=", "0", "\n", "\n", "# if item['title'] == 'Franz G\u00fcrtner':", "\n", "#     what = 0", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraphs", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list", "\n", "            ", "valid_tokens", "=", "[", "]", "\n", "valid_hyperlinks", "=", "[", "]", "\n", "valid_raw_tokens", "=", "[", "]", "\n", "\n", "cur_sent_tokens", "=", "[", "]", "\n", "results_hyperlinks", "=", "[", "]", "\n", "\n", "hyper_link_start_token_num", "=", "-", "1", "\n", "hyper_link_end_token_num", "=", "-", "1", "\n", "\n", "for", "token_i", ",", "(", "start", ",", "end", ")", "in", "enumerate", "(", "sentence_offsets", ")", ":", "\n", "\n", "                ", "cur_token", "=", "paragraph_text", "[", "start", ":", "end", "]", "\n", "valid_raw_tokens", ".", "append", "(", "cur_token", ")", "\n", "cur_sent_tokens", ".", "append", "(", "cur_token", ")", "\n", "\n", "if", "cur_token", ".", "startswith", "(", "\"<a href=\\\"\"", ")", ":", "\n", "                    ", "if", "\"<a href=\\\"http%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"https%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"//\"", "in", "cur_token", ":", "# Ignore external links.", "\n", "                        ", "continue", "\n", "\n", "", "if", "in_hyperlink", ":", "\n", "# We didn't find a correct \"</a>\" to close", "\n", "                        ", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "hl_head", "=", "etree", ".", "fromstring", "(", "cur_hyperlink_start_token", "+", "\"</a>\"", ")", "\n", "hl_href", "=", "hl_head", ".", "get", "(", "'href'", ")", "\n", "# Remember here to append another indices", "\n", "\n", "# in_hyperlink = True", "\n", "\n", "# print(item)", "\n", "# print(item['title'])", "\n", "# print(sentence)", "\n", "# print(cur_sent_tokens)", "\n", "# print(cur_token)", "\n", "print", "(", "hl_href", ")", "\n", "print", "(", "\"Potential Error. Check This.\"", ")", "\n", "# raise ValueError(\"Hyperlink Parsing Error!\")", "\n", "\n", "", "in_hyperlink", "=", "True", "\n", "hyper_link_start_token_num", "=", "token_i", "\n", "start_hyperlink_index", "=", "start", "\n", "start_inner_hyperlink_index", "=", "end", "\n", "continue", "\n", "\n", "", "elif", "cur_token", "==", "\"</a>\"", "or", "(", "cur_token", "==", "\"\u00bb\"", "and", "paragraph_text", "[", "end", "-", "4", ":", "end", "]", "==", "\"</a\u00bb\"", ")", ":", "\n", "                    ", "if", "not", "in_hyperlink", ":", "\n", "                        ", "continue", "# Fail to reveal the start.", "\n", "\n", "", "in_hyperlink", "=", "False", "\n", "end_hypoerlink_index", "=", "end", "\n", "end_inner_hyperlink_index", "=", "start", "\n", "hyper_link_end_token_num", "=", "token_i", "\n", "\n", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "cur_hyperlink", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "end_hypoerlink_index", "]", "\n", "cur_hyperlink_inner_text", "=", "paragraph_text", "[", "start_inner_hyperlink_index", ":", "end_inner_hyperlink_index", "]", "\n", "\n", "results_hyperlinks", ".", "append", "(", "cur_hyperlink", ")", "\n", "# print(cur_hyperlink)", "\n", "\n", "raw_title", ",", "h_inner_text", "=", "extract_href", "(", "cur_hyperlink", ")", "\n", "\n", "if", "raw_title", "is", "not", "None", ":", "\n", "                        ", "if", "len", "(", "h_inner_text", ")", ">", "0", ":", "\n", "                            ", "assert", "h_inner_text", "==", "cur_hyperlink_inner_text", "\n", "", "link_title", "=", "urllib", ".", "parse", ".", "unquote", "(", "raw_title", ")", "\n", "if", "hyper_link_start_token_num", "==", "-", "1", "and", "len", "(", "aricle_valid_hyperlinks", ")", ">", "0", ":", "\n", "# append to last sentence", "\n", "                            ", "aricle_valid_hyperlinks", "[", "-", "1", "]", ".", "append", "(", "(", "len", "(", "aricle_valid_sentences", "[", "-", "1", "]", ")", "-", "1", ",", "\n", "len", "(", "aricle_valid_sentences", "[", "-", "1", "]", ")", "-", "1", ",", "\n", "link_title", ")", ")", "\n", "", "else", ":", "\n", "                            ", "valid_hyperlinks", ".", "append", "(", "(", "hyper_link_start_token_num", ",", "hyper_link_end_token_num", ",", "link_title", ")", ")", "\n", "# we reach the end of a hyperlink", "\n", "", "", "continue", "\n", "\n", "# try:", "\n", "#     hl = etree.fromstring(cur_hyperlink)", "\n", "#     assert hl.text == cur_hyperlink_inner_text", "\n", "#     raw_title = hl.get('href')", "\n", "#     link_title = urllib.parse.unquote(raw_title)", "\n", "# except:", "\n", "#     hl_head = etree.fromstring(cur_hyperlink_start_token + \"</a>\")", "\n", "#     raw_title = hl_head.get('href')", "\n", "#     link_title = urllib.parse.unquote(raw_title)", "\n", "", "valid_tokens", ".", "append", "(", "cur_token", ")", "\n", "# token end", "\n", "\n", "", "aricle_valid_sentences", ".", "append", "(", "valid_tokens", ")", "\n", "# aricle_valid_poss.append(spacy_get_pos(valid_tokens))", "\n", "aricle_valid_hyperlinks", ".", "append", "(", "valid_hyperlinks", ")", "\n", "aricle_valid_paragraph_tags", ".", "append", "(", "paragraph_num", ")", "\n", "aricle_valid_raw_tokens", ".", "append", "(", "valid_raw_tokens", ")", "\n", "# sentence end", "\n", "\n", "", "paragraph_num", "+=", "1", "\n", "# paragraph end", "\n", "\n", "", "processed_item", "[", "'sentences'", "]", "=", "aricle_valid_sentences", "\n", "processed_item", "[", "'poss'", "]", "=", "aricle_valid_poss", "\n", "processed_item", "[", "'hyperlinks'", "]", "=", "aricle_valid_hyperlinks", "\n", "processed_item", "[", "'paragraph_tags'", "]", "=", "aricle_valid_paragraph_tags", "\n", "\n", "return", "processed_item", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.iterative_checking_from_db": [[186, 222], ["print", "sqlitedict.SqliteDict", "tqdm.tqdm", "tqdm.tqdm", "str", "whole_db.iterkeys", "titles.append", "db_convert_and_enhance.convert_current_item", "len", "whole_db.close"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.db_convert_and_enhance.convert_current_item", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.persistent_index_db.IndexingDB.close"], ["", "def", "iterative_checking_from_db", "(", "debug_num", "=", "None", ")", ":", "\n", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "# error_count_dict = {}", "\n", "# for key, value in check_func_dict.items():", "\n", "#     error_count_dict[f\"{key}_count\"] = 0", "\n", "\n", "with", "SqliteDict", "(", "str", "(", "config", ".", "WHOLE_WIKI_DB", ")", ",", "flag", "=", "'r'", ",", "encode", "=", "json", ".", "dumps", ",", "decode", "=", "json", ".", "loads", ")", "as", "whole_db", ":", "\n", "\n", "        ", "titles", "=", "[", "]", "\n", "for", "title", "in", "tqdm", "(", "whole_db", ".", "iterkeys", "(", ")", ",", "total", "=", "len", "(", "whole_db", ")", ")", ":", "\n", "            ", "titles", ".", "append", "(", "title", ")", "\n", "\n", "", "for", "title", "in", "tqdm", "(", "titles", ")", ":", "\n", "            ", "item", "=", "whole_db", "[", "title", "]", "\n", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "whole_db", ".", "close", "(", ")", "\n", "break", "\n", "\n", "# item = json.loads(line, encoding='utf-8')", "\n", "", "cur_count", "+=", "1", "\n", "\n", "# print(item)", "\n", "\n", "# Important, we don't check boundary error here to accelerate but might give problem in the future.", "\n", "# if not check_boundary(item):", "\n", "#     continue", "\n", "\n", "processed_item", "=", "convert_current_item", "(", "item", ")", "\n", "\n", "# for key, value in error_count_dict.items():", "\n", "#     print(key, value)", "\n", "#", "\n", "", "", "print", "(", "\"Total Count:\"", ",", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.whole_abs_cross_checking.get_cursor": [[10, 14], ["sqlite3.connect", "sqlite3.connect.cursor"], "function", ["None"], ["def", "get_cursor", "(", "save_path", ")", ":", "\n", "    ", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "return", "cursor", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only": [[15, 58], ["enumerate", "collections.Counter", "numpy.mean", "len", "print", "print", "print", "print", "fever_scorer.doc_macro_precision", "fever_scorer.doc_macro_recall", "fever_scorer.check_doc_id_correct", "evidence_number_list.append", "collections.Counter.most_common", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.doc_macro_precision", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.doc_macro_recall", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_doc_id_correct"], ["def", "fever_doc_only", "(", "predictions", ",", "actual", "=", "None", ",", "max_evidence", "=", "5", ")", ":", "\n", "    ", "'''\n    This method is used to only evaluate document retrieval\n    '''", "\n", "macro_precision", "=", "0", "\n", "macro_precision_hits", "=", "0", "\n", "\n", "macro_recall", "=", "0", "\n", "macro_recall_hits", "=", "0", "\n", "doc_id_hits", "=", "0", "\n", "\n", "evidence_number_list", "=", "[", "]", "\n", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "        ", "macro_prec", "=", "doc_macro_precision", "(", "instance", ",", "max_evidence", ")", "\n", "macro_precision", "+=", "macro_prec", "[", "0", "]", "\n", "macro_precision_hits", "+=", "macro_prec", "[", "1", "]", "\n", "\n", "macro_rec", "=", "doc_macro_recall", "(", "instance", ",", "max_evidence", ")", "\n", "macro_recall", "+=", "macro_rec", "[", "0", "]", "\n", "macro_recall_hits", "+=", "macro_rec", "[", "1", "]", "\n", "\n", "if", "check_doc_id_correct", "(", "instance", ",", "actual", "[", "idx", "]", ",", "max_evidence", ")", ":", "\n", "            ", "doc_id_hits", "+=", "1", "\n", "", "evidence_number_list", ".", "append", "(", "len", "(", "instance", "[", "'predicted_docids'", "]", "[", ":", "max_evidence", "]", ")", ")", "\n", "\n", "", "evidence_number_counter", "=", "Counter", "(", "evidence_number_list", ")", "\n", "avg_length", "=", "np", ".", "mean", "(", "evidence_number_list", ")", "\n", "total", "=", "len", "(", "predictions", ")", "\n", "\n", "oracle_score", "=", "doc_id_hits", "/", "total", "\n", "\n", "pr", "=", "(", "macro_precision", "/", "macro_precision_hits", ")", "if", "macro_precision_hits", ">", "0", "else", "1.0", "\n", "rec", "=", "(", "macro_recall", "/", "macro_recall_hits", ")", "if", "macro_recall_hits", ">", "0", "else", "0.0", "\n", "\n", "f1", "=", "2.0", "*", "pr", "*", "rec", "/", "(", "pr", "+", "rec", ")", "\n", "\n", "print", "(", "\"Hit:\"", ",", "doc_id_hits", ")", "\n", "print", "(", "\"Total:\"", ",", "total", ")", "\n", "print", "(", "evidence_number_counter", ".", "most_common", "(", ")", ")", "\n", "print", "(", "\"Avg. Len.:\"", ",", "avg_length", ")", "\n", "\n", "return", "oracle_score", ",", "pr", ",", "rec", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_sent_only": [[60, 103], ["enumerate", "collections.Counter", "numpy.mean", "len", "print", "print", "print", "print", "fever_scorer.evidence_macro_precision", "fever_scorer.evidence_macro_recall", "fever_scorer.check_sent_correct", "evidence_number_list.append", "collections.Counter.most_common", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_precision", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_recall", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_sent_correct"], ["", "def", "fever_sent_only", "(", "predictions", ",", "actual", "=", "None", ",", "max_evidence", "=", "5", ")", ":", "\n", "    ", "'''\n    This method is used to only evaluate document retrieval\n    '''", "\n", "macro_precision", "=", "0", "\n", "macro_precision_hits", "=", "0", "\n", "\n", "macro_recall", "=", "0", "\n", "macro_recall_hits", "=", "0", "\n", "sent_id_hits", "=", "0", "\n", "\n", "evidence_number_list", "=", "[", "]", "\n", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "        ", "macro_prec", "=", "evidence_macro_precision", "(", "instance", ",", "max_evidence", ")", "\n", "macro_precision", "+=", "macro_prec", "[", "0", "]", "\n", "macro_precision_hits", "+=", "macro_prec", "[", "1", "]", "\n", "\n", "macro_rec", "=", "evidence_macro_recall", "(", "instance", ",", "max_evidence", ")", "\n", "macro_recall", "+=", "macro_rec", "[", "0", "]", "\n", "macro_recall_hits", "+=", "macro_rec", "[", "1", "]", "\n", "\n", "if", "check_sent_correct", "(", "instance", ",", "actual", "[", "idx", "]", ",", "max_evidence", ")", ":", "\n", "            ", "sent_id_hits", "+=", "1", "\n", "", "evidence_number_list", ".", "append", "(", "len", "(", "instance", "[", "'predicted_evidence'", "]", "[", ":", "max_evidence", "]", ")", ")", "\n", "\n", "", "evidence_number_counter", "=", "Counter", "(", "evidence_number_list", ")", "\n", "avg_length", "=", "np", ".", "mean", "(", "evidence_number_list", ")", "\n", "total", "=", "len", "(", "predictions", ")", "\n", "\n", "oracle_score", "=", "sent_id_hits", "/", "total", "\n", "\n", "pr", "=", "(", "macro_precision", "/", "macro_precision_hits", ")", "if", "macro_precision_hits", ">", "0", "else", "1.0", "\n", "rec", "=", "(", "macro_recall", "/", "macro_recall_hits", ")", "if", "macro_recall_hits", ">", "0", "else", "0.0", "\n", "\n", "f1", "=", "2.0", "*", "pr", "*", "rec", "/", "(", "pr", "+", "rec", ")", "\n", "\n", "print", "(", "\"Hit:\"", ",", "sent_id_hits", ")", "\n", "print", "(", "\"Total:\"", ",", "total", ")", "\n", "print", "(", "evidence_number_counter", ".", "most_common", "(", ")", ")", "\n", "print", "(", "\"Avg. Len.:\"", ",", "avg_length", ")", "\n", "\n", "return", "oracle_score", ",", "pr", ",", "rec", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.doc_macro_precision": [[105, 128], ["instance[].upper"], "function", ["None"], ["", "def", "doc_macro_precision", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "    ", "this_precision", "=", "0.0", "\n", "this_precision_hits", "=", "0.0", "\n", "\n", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "# all_evi = [[e[2], e[3]] for eg in instance[\"evidence\"] for e in eg if e[3] is not None]", "\n", "\n", "# predicted_evidence = instance[\"predicted_evidence\"] if max_evidence is None else \\", "\n", "#     instance[\"predicted_evidence\"][:max_evidence]", "\n", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "        ", "doc_evi", "=", "[", "e", "[", "2", "]", "for", "eg", "in", "instance", "[", "\"evidence\"", "]", "for", "e", "in", "eg", "if", "e", "[", "3", "]", "is", "not", "None", "]", "\n", "pred_ids", "=", "instance", "[", "\"predicted_docids\"", "]", "if", "max_evidence", "is", "None", "else", "instance", "[", "\"predicted_docids\"", "]", "[", ":", "max_evidence", "]", "\n", "\n", "for", "prediction", "in", "pred_ids", ":", "\n", "            ", "if", "prediction", "in", "doc_evi", ":", "\n", "                ", "this_precision", "+=", "1.0", "\n", "", "this_precision_hits", "+=", "1.0", "\n", "\n", "", "return", "(", "this_precision", "/", "this_precision_hits", ")", "if", "this_precision_hits", ">", "0", "else", "1.0", ",", "1.0", "\n", "\n", "", "return", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.doc_macro_recall": [[130, 158], ["instance[].upper", "all", "all", "len", "len"], "function", ["None"], ["", "def", "doc_macro_recall", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "# We only want to score F1/Precision/Recall of recalled evidence for NEI claims", "\n", "    ", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "# If there's no evidence to predict, return 1", "\n", "        ", "if", "len", "(", "instance", "[", "\"evidence\"", "]", ")", "==", "0", "or", "all", "(", "[", "len", "(", "eg", ")", "==", "0", "for", "eg", "in", "instance", "]", ")", ":", "\n", "            ", "return", "1.0", ",", "1.0", "\n", "\n", "", "for", "evience_group", "in", "instance", "[", "\"evidence\"", "]", ":", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "            ", "docids", "=", "[", "e", "[", "2", "]", "for", "e", "in", "evience_group", "]", "\n", "# Only return true if an entire group of actual sentences is in the predicted sentences", "\n", "pred_ids", "=", "instance", "[", "\"predicted_docids\"", "]", "if", "max_evidence", "is", "None", "else", "instance", "[", "\"predicted_docids\"", "]", "[", ":", "max_evidence", "]", "\n", "\n", "if", "all", "(", "[", "docid", "in", "pred_ids", "for", "docid", "in", "docids", "]", ")", ":", "\n", "                ", "return", "1.0", ",", "1.0", "\n", "\n", "", "", "return", "0.0", ",", "1.0", "\n", "# predicted_evidence = instance[\"predicted_evidence\"] if max_evidence is None else \\", "\n", "#                                                                 instance[\"predicted_evidence\"][:max_evidence]", "\n", "#", "\n", "# for evidence_group in instance[\"evidence\"]:", "\n", "#     evidence = [[e[2], e[3]] for e in evidence_group]", "\n", "#     if all([item in predicted_evidence for item in evidence]):", "\n", "#         We only want to score complete groups of evidence. Incomplete groups are worthless.", "\n", "# return 1.0, 1.0", "\n", "# return 0.0, 1.0", "\n", "", "return", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_predicted_evidence_format": [[160, 177], ["len", "all", "all", "all", "all", "instance.keys", "isinstance", "isinstance", "isinstance", "len"], "function", ["None"], ["", "def", "check_predicted_evidence_format", "(", "instance", ")", ":", "\n", "    ", "if", "'predicted_evidence'", "in", "instance", ".", "keys", "(", ")", "and", "len", "(", "instance", "[", "'predicted_evidence'", "]", ")", ":", "\n", "        ", "assert", "all", "(", "isinstance", "(", "prediction", ",", "list", ")", "\n", "for", "prediction", "in", "instance", "[", "\"predicted_evidence\"", "]", ")", ",", "\"Predicted evidence must be a list of (page,line) lists\"", "\n", "\n", "assert", "all", "(", "len", "(", "prediction", ")", "==", "2", "\n", "for", "prediction", "in", "instance", "[", "\"predicted_evidence\"", "]", ")", ",", "\"Predicted evidence must be a list of (page,line) lists\"", "\n", "\n", "assert", "all", "(", "isinstance", "(", "prediction", "[", "0", "]", ",", "six", ".", "string_types", ")", "\n", "for", "prediction", "in", "instance", "[", "\"predicted_evidence\"", "]", ")", ",", "\"Predicted evidence must be a list of (page<string>,line<int>) lists\"", "\n", "\n", "assert", "all", "(", "isinstance", "(", "prediction", "[", "1", "]", ",", "int", ")", "\n", "for", "prediction", "in", "instance", "[", "\"predicted_evidence\"", "]", ")", ",", "\"Predicted evidence must be a list of (page<string>,line<int>) lists\"", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label": [[179, 181], ["instance[].upper", "instance[].upper"], "function", ["None"], ["", "", "def", "is_correct_label", "(", "instance", ")", ":", "\n", "    ", "return", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "==", "instance", "[", "\"predicted_label\"", "]", ".", "upper", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_strictly_correct": [[183, 205], ["fever_scorer.check_predicted_evidence_format", "fever_scorer.is_correct_label", "instance[].upper", "len", "all", "fever_scorer.is_correct_label", "instance[].upper"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_predicted_evidence_format", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label"], ["", "def", "is_strictly_correct", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "# Strict evidence matching is only for NEI class", "\n", "    ", "check_predicted_evidence_format", "(", "instance", ")", "\n", "\n", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", "and", "is_correct_label", "(", "instance", ")", ":", "\n", "        ", "assert", "'predicted_evidence'", "in", "instance", ",", "\"Predicted evidence must be provided for strict scoring\"", "\n", "\n", "if", "max_evidence", "is", "None", ":", "\n", "            ", "max_evidence", "=", "len", "(", "instance", "[", "\"predicted_evidence\"", "]", ")", "\n", "\n", "", "for", "evience_group", "in", "instance", "[", "\"evidence\"", "]", ":", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "            ", "actual_sentences", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "e", "in", "evience_group", "]", "\n", "# Only return true if an entire group of actual sentences is in the predicted sentences", "\n", "if", "all", "(", "[", "actual_sent", "in", "instance", "[", "\"predicted_evidence\"", "]", "[", ":", "max_evidence", "]", "for", "actual_sent", "in", "actual_sentences", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "# If the class is NEI, we don't score the evidence retrieval component", "\n", "", "", "", "elif", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "==", "\"NOT ENOUGH INFO\"", "and", "is_correct_label", "(", "instance", ")", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_evidence_correct": [[207, 228], ["fever_scorer.check_predicted_evidence_format", "fever_scorer.is_correct_label", "instance[].upper", "len", "all", "instance[].upper"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_predicted_evidence_format", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label"], ["", "def", "is_evidence_correct", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "    ", "check_predicted_evidence_format", "(", "instance", ")", "\n", "\n", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", "and", "is_correct_label", "(", "instance", ")", ":", "\n", "        ", "assert", "'predicted_evidence'", "in", "instance", ",", "\"Predicted evidence must be provided for strict scoring\"", "\n", "\n", "if", "max_evidence", "is", "None", ":", "\n", "            ", "max_evidence", "=", "len", "(", "instance", "[", "\"predicted_evidence\"", "]", ")", "\n", "\n", "", "for", "evience_group", "in", "instance", "[", "\"evidence\"", "]", ":", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "            ", "actual_sentences", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "e", "in", "evience_group", "]", "\n", "# Only return true if an entire group of actual sentences is in the predicted sentences", "\n", "if", "all", "(", "[", "actual_sent", "in", "instance", "[", "\"predicted_evidence\"", "]", "[", ":", "max_evidence", "]", "for", "actual_sent", "in", "actual_sentences", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "# If the class is NEI, we don't score the evidence retrieval component", "\n", "", "", "", "elif", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "==", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_precision": [[230, 248], ["instance[].upper"], "function", ["None"], ["", "def", "evidence_macro_precision", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "    ", "this_precision", "=", "0.0", "\n", "this_precision_hits", "=", "0.0", "\n", "\n", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "all_evi", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "eg", "in", "instance", "[", "\"evidence\"", "]", "for", "e", "in", "eg", "if", "e", "[", "3", "]", "is", "not", "None", "]", "\n", "\n", "predicted_evidence", "=", "instance", "[", "\"predicted_evidence\"", "]", "if", "max_evidence", "is", "None", "else", "instance", "[", "\"predicted_evidence\"", "]", "[", ":", "max_evidence", "]", "\n", "\n", "for", "prediction", "in", "predicted_evidence", ":", "\n", "            ", "if", "prediction", "in", "all_evi", ":", "\n", "                ", "this_precision", "+=", "1.0", "\n", "", "this_precision_hits", "+=", "1.0", "\n", "\n", "", "return", "(", "this_precision", "/", "this_precision_hits", ")", "if", "this_precision_hits", ">", "0", "else", "1.0", ",", "1.0", "\n", "\n", "", "return", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_recall": [[250, 267], ["instance[].upper", "all", "all", "len", "len"], "function", ["None"], ["", "def", "evidence_macro_recall", "(", "instance", ",", "max_evidence", "=", "None", ")", ":", "\n", "# We only want to score F1/Precision/Recall of recalled evidence for NEI claims", "\n", "    ", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "# If there's no evidence to predict, return 1", "\n", "        ", "if", "len", "(", "instance", "[", "\"evidence\"", "]", ")", "==", "0", "or", "all", "(", "[", "len", "(", "eg", ")", "==", "0", "for", "eg", "in", "instance", "]", ")", ":", "\n", "            ", "return", "1.0", ",", "1.0", "\n", "\n", "", "predicted_evidence", "=", "instance", "[", "\"predicted_evidence\"", "]", "if", "max_evidence", "is", "None", "else", "instance", "[", "\"predicted_evidence\"", "]", "[", ":", "max_evidence", "]", "\n", "\n", "for", "evidence_group", "in", "instance", "[", "\"evidence\"", "]", ":", "\n", "            ", "evidence", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "e", "in", "evidence_group", "]", "\n", "if", "all", "(", "[", "item", "in", "predicted_evidence", "for", "item", "in", "evidence", "]", ")", ":", "\n", "# We only want to score complete groups of evidence. Incomplete groups are worthless.", "\n", "                ", "return", "1.0", ",", "1.0", "\n", "", "", "return", "0.0", ",", "1.0", "\n", "", "return", "0.0", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_micro_precision": [[270, 284], ["instance[].upper"], "function", ["None"], ["", "def", "evidence_micro_precision", "(", "instance", ")", ":", "\n", "    ", "this_precision", "=", "0", "\n", "this_precision_hits", "=", "0", "\n", "\n", "# We only want to score Macro F1/Precision/Recall of recalled evidence for NEI claims", "\n", "if", "instance", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "all_evi", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "eg", "in", "instance", "[", "\"evidence\"", "]", "for", "e", "in", "eg", "if", "e", "[", "3", "]", "is", "not", "None", "]", "\n", "\n", "for", "prediction", "in", "instance", "[", "\"predicted_evidence\"", "]", ":", "\n", "            ", "if", "prediction", "in", "all_evi", ":", "\n", "                ", "this_precision", "+=", "1.0", "\n", "", "this_precision_hits", "+=", "1.0", "\n", "\n", "", "", "return", "this_precision", ",", "this_precision_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_doc_id_correct": [[286, 307], ["fever_scorer.check_predicted_evidence_format", "actual[].upper", "all", "actual[].upper"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_predicted_evidence_format"], ["", "def", "check_doc_id_correct", "(", "instance", ",", "actual", ",", "max_length", "=", "None", ")", ":", "\n", "    ", "check_predicted_evidence_format", "(", "instance", ")", "\n", "predicted_docids", "=", "instance", "[", "\"predicted_docids\"", "]", "[", ":", "max_length", "]", "\n", "\n", "if", "actual", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "for", "evience_group", "in", "actual", "[", "\"evidence\"", "]", ":", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "            ", "docids", "=", "[", "e", "[", "2", "]", "for", "e", "in", "evience_group", "]", "\n", "# Only return true if an entire group of actual sentences is in the predicted sentences", "\n", "# if max_length is None:", "\n", "#     max_length = 5", "\n", "# pred_ids = sorted(instance[\"predicted_docids\"], reverse=True)[:max_length]", "\n", "# instance[\"predicted_docids\"] = instance[\"predicted_docids\"][:max_length]", "\n", "pred_ids", "=", "predicted_docids", "[", ":", "max_length", "]", "\n", "if", "all", "(", "[", "docid", "in", "pred_ids", "for", "docid", "in", "docids", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "", "elif", "actual", "[", "\"label\"", "]", ".", "upper", "(", ")", "==", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_sent_correct": [[309, 328], ["fever_scorer.check_predicted_evidence_format", "actual[].upper", "all", "actual[].upper", "sorted"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_predicted_evidence_format"], ["", "def", "check_sent_correct", "(", "instance", ",", "actual", ",", "max_length", "=", "5", ")", ":", "\n", "    ", "check_predicted_evidence_format", "(", "instance", ")", "\n", "\n", "# if actual is not None:", "\n", "\n", "# if actual", "\n", "if", "actual", "[", "\"label\"", "]", ".", "upper", "(", ")", "!=", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "for", "evience_group", "in", "actual", "[", "\"evidence\"", "]", ":", "\n", "# Filter out the annotation ids. We just want the evidence page and line number", "\n", "            ", "sentids", "=", "[", "[", "e", "[", "2", "]", ",", "e", "[", "3", "]", "]", "for", "e", "in", "evience_group", "]", "\n", "# Only return true if an entire group of actual sentences is in the predicted sentences", "\n", "pred_ids", "=", "sorted", "(", "instance", "[", "\"predicted_evidence\"", "]", ",", "reverse", "=", "True", ")", "[", ":", "max_length", "]", "\n", "if", "all", "(", "[", "sentid", "in", "pred_ids", "for", "sentid", "in", "sentids", "]", ")", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "", "elif", "actual", "[", "\"label\"", "]", ".", "upper", "(", ")", "==", "\"NOT ENOUGH INFO\"", ":", "\n", "        ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score": [[330, 450], ["enumerate", "len", "print", "print", "mode.items", "mode.keys", "k.endswith", "key_list.append", "fever_scorer.is_correct_label", "fever_scorer.evidence_macro_precision", "fever_scorer.evidence_macro_recall", "print", "instance.keys", "instance.keys", "fever_scorer.is_strictly_correct", "fever_scorer.check_doc_id_correct", "fever_scorer.check_sent_correct", "len", "len", "actual[].keys"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_precision", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_recall", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_strictly_correct", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_doc_id_correct", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_sent_correct"], ["", "def", "fever_score", "(", "predictions", ",", "actual", "=", "None", ",", "max_evidence", "=", "5", ",", "mode", "=", "None", ",", "\n", "error_analysis_file", "=", "None", ",", "\n", "verbose", "=", "False", ",", "label_it", "=", "False", ")", ":", "\n", "    ", "'''\n    This is a important function for different scoring.\n    Pass in different parameter in mode for specific score.\n\n    :param verbose:\n    :param predictions:\n    :param actual:\n    :param max_evidence:\n    :param mode:\n    :return:\n    '''", "\n", "\n", "# log_print = utils.get_adv_print_func(error_analysis_file, verbose=verbose)", "\n", "\n", "correct", "=", "0", "\n", "strict", "=", "0", "\n", "error_count", "=", "0", "\n", "\n", "macro_precision", "=", "0", "\n", "macro_precision_hits", "=", "0", "\n", "\n", "macro_recall", "=", "0", "\n", "macro_recall_hits", "=", "0", "\n", "\n", "# ana_f = None", "\n", "# if error_analysis_file is not None:", "\n", "#     ana_f = open(error_analysis_file, mode='w')", "\n", "\n", "if", "mode", "is", "not", "None", ":", "\n", "        ", "key_list", "=", "[", "]", "\n", "for", "key", "in", "mode", ".", "keys", "(", ")", ":", "\n", "            ", "key_list", ".", "append", "(", "key", ")", "\n", "\n", "", "for", "key", "in", "key_list", ":", "\n", "            ", "mode", "[", "key", "+", "'_hits'", "]", "=", "0", "\n", "\n", "", "", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "\n", "        ", "if", "label_it", ":", "\n", "            ", "instance", "[", "'correct'", "]", "=", "False", "\n", "\n", "", "if", "mode", "[", "'standard'", "]", ":", "\n", "            ", "assert", "'predicted_evidence'", "in", "instance", ".", "keys", "(", ")", ",", "'evidence must be provided for the prediction'", "\n", "\n", "# If it's a blind test set, we need to copy in the values from the actual data", "\n", "if", "'evidence'", "not", "in", "instance", "or", "'label'", "not", "in", "instance", ":", "\n", "                ", "assert", "actual", "is", "not", "None", ",", "'in blind evaluation mode, actual data must be provided'", "\n", "assert", "len", "(", "actual", ")", "==", "len", "(", "predictions", ")", ",", "'actual data and predicted data length must match'", "\n", "assert", "'evidence'", "in", "actual", "[", "idx", "]", ".", "keys", "(", ")", ",", "'evidence must be provided for the actual evidence'", "\n", "instance", "[", "'evidence'", "]", "=", "actual", "[", "idx", "]", "[", "'evidence'", "]", "\n", "instance", "[", "'label'", "]", "=", "actual", "[", "idx", "]", "[", "'label'", "]", "\n", "\n", "", "assert", "'evidence'", "in", "instance", ".", "keys", "(", ")", ",", "'gold evidence must be provided'", "\n", "\n", "if", "is_correct_label", "(", "instance", ")", ":", "\n", "                ", "correct", "+=", "1.0", "\n", "\n", "if", "is_strictly_correct", "(", "instance", ",", "max_evidence", ")", ":", "\n", "                    ", "strict", "+=", "1.0", "\n", "\n", "if", "label_it", ":", "\n", "                        ", "instance", "[", "'correct'", "]", "=", "True", "\n", "\n", "# if not is_strictly_correct(instance, max_evidence):", "\n", "#     is_strictly_correct(instance, max_evidence)", "\n", "# print(instance)", "\n", "\n", "", "", "", "macro_prec", "=", "evidence_macro_precision", "(", "instance", ",", "max_evidence", ")", "\n", "macro_precision", "+=", "macro_prec", "[", "0", "]", "\n", "macro_precision_hits", "+=", "macro_prec", "[", "1", "]", "\n", "\n", "macro_rec", "=", "evidence_macro_recall", "(", "instance", ",", "max_evidence", ")", "\n", "macro_recall", "+=", "macro_rec", "[", "0", "]", "\n", "macro_recall_hits", "+=", "macro_rec", "[", "1", "]", "\n", "\n", "", "if", "mode", "is", "not", "None", ":", "\n", "            ", "if", "'check_doc_id_correct'", "in", "mode", "and", "mode", "[", "'check_doc_id_correct'", "]", ":", "\n", "                ", "if", "check_doc_id_correct", "(", "instance", ",", "actual", "[", "idx", "]", ",", "max_length", "=", "max_evidence", ")", ":", "\n", "                    ", "mode", "[", "'check_doc_id_correct_hits'", "]", "+=", "1", "\n", "", "else", ":", "\n", "# error_count += 1", "\n", "# log_print(instance)", "\n", "                    ", "pass", "\n", "\n", "", "", "if", "'check_sent_id_correct'", "in", "mode", "and", "mode", "[", "'check_sent_id_correct'", "]", ":", "\n", "                ", "if", "check_sent_correct", "(", "instance", ",", "actual", "[", "idx", "]", ")", ":", "\n", "                    ", "mode", "[", "'check_sent_id_correct_hits'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "pass", "\n", "# error_count += 1", "\n", "# log_print(instance)", "\n", "\n", "# log_print(\"Error count:\", error_count)", "\n", "", "", "", "", "total", "=", "len", "(", "predictions", ")", "\n", "\n", "# log_print(\"Total:\", total)", "\n", "print", "(", "\"Total:\"", ",", "total", ")", "\n", "# log_print(\"Strict:\", strict)", "\n", "print", "(", "\"Strict:\"", ",", "strict", ")", "\n", "\n", "for", "k", ",", "v", "in", "mode", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", ".", "endswith", "(", "'_hits'", ")", ":", "\n", "# log_print(k, v, v / total)", "\n", "            ", "print", "(", "k", ",", "v", ",", "v", "/", "total", ")", "\n", "\n", "", "", "strict_score", "=", "strict", "/", "total", "\n", "acc_score", "=", "correct", "/", "total", "\n", "\n", "pr", "=", "(", "macro_precision", "/", "macro_precision_hits", ")", "if", "macro_precision_hits", ">", "0", "else", "1.0", "\n", "rec", "=", "(", "macro_recall", "/", "macro_recall_hits", ")", "if", "macro_recall_hits", ">", "0", "else", "0.0", "\n", "\n", "if", "(", "pr", "+", "rec", ")", "==", "0", ":", "\n", "        ", "f1", "=", "0", "\n", "", "else", ":", "\n", "        ", "f1", "=", "2.0", "*", "pr", "*", "rec", "/", "(", "pr", "+", "rec", ")", "\n", "\n", "", "return", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_score_analysis": [[452, 569], ["enumerate", "print", "len", "print", "print", "mode.items", "mode.keys", "k.endswith", "key_list.append", "fever_scorer.is_correct_label", "fever_scorer.evidence_macro_precision", "fever_scorer.evidence_macro_recall", "print", "instance.keys", "instance.keys", "fever_scorer.is_strictly_correct", "fever_scorer.check_doc_id_correct", "fever_scorer.check_sent_correct", "len", "len", "actual[].keys", "error_list.append", "print", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_correct_label", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_precision", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.evidence_macro_recall", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.is_strictly_correct", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_doc_id_correct", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.check_sent_correct"], ["", "def", "fever_score_analysis", "(", "predictions", ",", "actual", "=", "None", ",", "max_evidence", "=", "5", ",", "mode", "=", "None", ",", "\n", "error_analysis_file", "=", "None", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "    ", "'''\n    This is a important function for different scoring.\n    Pass in different parameter in mode for specific score.\n\n    :param verbose:\n    :param predictions:\n    :param actual:\n    :param max_evidence:\n    :param mode:\n    :return:\n    '''", "\n", "error_list", "=", "[", "]", "\n", "\n", "# log_print = utils.get_adv_print_func(error_analysis_file, verbose=verbose)", "\n", "\n", "correct", "=", "0", "\n", "strict", "=", "0", "\n", "error_count", "=", "0", "\n", "\n", "macro_precision", "=", "0", "\n", "macro_precision_hits", "=", "0", "\n", "\n", "macro_recall", "=", "0", "\n", "macro_recall_hits", "=", "0", "\n", "\n", "# ana_f = None", "\n", "# if error_analysis_file is not None:", "\n", "#     ana_f = open(error_analysis_file, mode='w')", "\n", "\n", "if", "mode", "is", "not", "None", ":", "\n", "        ", "key_list", "=", "[", "]", "\n", "for", "key", "in", "mode", ".", "keys", "(", ")", ":", "\n", "            ", "key_list", ".", "append", "(", "key", ")", "\n", "\n", "", "for", "key", "in", "key_list", ":", "\n", "            ", "mode", "[", "key", "+", "'_hits'", "]", "=", "0", "\n", "\n", "", "", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "        ", "if", "mode", "[", "'standard'", "]", ":", "\n", "            ", "assert", "'predicted_evidence'", "in", "instance", ".", "keys", "(", ")", ",", "'evidence must be provided for the prediction'", "\n", "\n", "# If it's a blind test set, we need to copy in the values from the actual data", "\n", "if", "'evidence'", "not", "in", "instance", "or", "'label'", "not", "in", "instance", ":", "\n", "                ", "assert", "actual", "is", "not", "None", ",", "'in blind evaluation mode, actual data must be provided'", "\n", "assert", "len", "(", "actual", ")", "==", "len", "(", "predictions", ")", ",", "'actual data and predicted data length must match'", "\n", "assert", "'evidence'", "in", "actual", "[", "idx", "]", ".", "keys", "(", ")", ",", "'evidence must be provided for the actual evidence'", "\n", "instance", "[", "'evidence'", "]", "=", "actual", "[", "idx", "]", "[", "'evidence'", "]", "\n", "instance", "[", "'label'", "]", "=", "actual", "[", "idx", "]", "[", "'label'", "]", "\n", "\n", "", "assert", "'evidence'", "in", "instance", ".", "keys", "(", ")", ",", "'gold evidence must be provided'", "\n", "\n", "if", "is_correct_label", "(", "instance", ")", ":", "\n", "                ", "correct", "+=", "1.0", "\n", "\n", "if", "is_strictly_correct", "(", "instance", ",", "max_evidence", ")", ":", "\n", "                    ", "strict", "+=", "1.0", "\n", "", "else", ":", "\n", "                    ", "error_list", ".", "append", "(", "instance", ")", "\n", "\n", "# if not is_strictly_correct(instance, max_evidence):", "\n", "#     is_strictly_correct(instance, max_evidence)", "\n", "# print(instance)", "\n", "\n", "", "", "macro_prec", "=", "evidence_macro_precision", "(", "instance", ",", "max_evidence", ")", "\n", "macro_precision", "+=", "macro_prec", "[", "0", "]", "\n", "macro_precision_hits", "+=", "macro_prec", "[", "1", "]", "\n", "\n", "macro_rec", "=", "evidence_macro_recall", "(", "instance", ",", "max_evidence", ")", "\n", "macro_recall", "+=", "macro_rec", "[", "0", "]", "\n", "macro_recall_hits", "+=", "macro_rec", "[", "1", "]", "\n", "\n", "", "if", "mode", "is", "not", "None", ":", "\n", "            ", "if", "'check_doc_id_correct'", "in", "mode", "and", "mode", "[", "'check_doc_id_correct'", "]", ":", "\n", "                ", "if", "check_doc_id_correct", "(", "instance", ",", "actual", "[", "idx", "]", ")", ":", "\n", "                    ", "mode", "[", "'check_doc_id_correct_hits'", "]", "+=", "1", "\n", "", "else", ":", "\n", "# error_count += 1", "\n", "# log_print(instance)", "\n", "                    ", "print", "(", "instance", ",", "actual", "[", "idx", "]", ")", "\n", "\n", "", "", "if", "'check_sent_id_correct'", "in", "mode", "and", "mode", "[", "'check_sent_id_correct'", "]", ":", "\n", "                ", "if", "check_sent_correct", "(", "instance", ",", "actual", "[", "idx", "]", ")", ":", "\n", "                    ", "mode", "[", "'check_sent_id_correct_hits'", "]", "+=", "1", "\n", "", "else", ":", "\n", "# error_count += 1", "\n", "# log_print(instance)", "\n", "                    ", "print", "(", "instance", ",", "actual", "[", "idx", "]", ")", "\n", "\n", "# log_print(\"Error count:\", error_count)", "\n", "", "", "", "", "print", "(", "\"Error count:\"", ",", "error_count", ")", "\n", "total", "=", "len", "(", "predictions", ")", "\n", "\n", "# log_print(\"Total:\", total)", "\n", "# log_print(\"Total:\", total)", "\n", "print", "(", "\"Total:\"", ",", "total", ")", "\n", "# log_print(\"Strict:\", strict)", "\n", "# log_print(\"Strict:\", strict)", "\n", "print", "(", "\"Strict:\"", ",", "strict", ")", "\n", "\n", "for", "k", ",", "v", "in", "mode", ".", "items", "(", ")", ":", "\n", "        ", "if", "k", ".", "endswith", "(", "'_hits'", ")", ":", "\n", "# log_print(k, v, v / total)", "\n", "# log_print(k, v, v / total)", "\n", "            ", "print", "(", "k", ",", "v", ",", "v", "/", "total", ")", "\n", "\n", "", "", "strict_score", "=", "strict", "/", "total", "\n", "acc_score", "=", "correct", "/", "total", "\n", "\n", "pr", "=", "(", "macro_precision", "/", "macro_precision_hits", ")", "if", "macro_precision_hits", ">", "0", "else", "1.0", "\n", "rec", "=", "(", "macro_recall", "/", "macro_recall_hits", ")", "if", "macro_recall_hits", ">", "0", "else", "0.0", "\n", "\n", "f1", "=", "2.0", "*", "pr", "*", "rec", "/", "(", "pr", "+", "rec", ")", "\n", "\n", "return", "strict_score", ",", "acc_score", ",", "pr", ",", "rec", ",", "f1", ",", "error_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.delete_label": [[571, 577], ["None"], "function", ["None"], ["", "def", "delete_label", "(", "d_list", ")", ":", "\n", "    ", "for", "item", "in", "d_list", ":", "\n", "        ", "if", "'label'", "in", "item", ":", "\n", "            ", "del", "item", "[", "'label'", "]", "\n", "", "if", "'evidence'", "in", "item", ":", "\n", "            ", "del", "item", "[", "'evidence'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.nei_stats": [[579, 630], ["enumerate", "print", "print", "print", "print", "len", "len", "len"], "function", ["None"], ["", "", "", "def", "nei_stats", "(", "predictions", ",", "actual", "=", "None", ")", ":", "\n", "    ", "'''\n    This is a important function for different scoring.\n    Pass in different parameter in mode for specific score.\n\n    :param verbose:\n    :param predictions:\n    :param actual:\n    :param max_evidence:\n    :param mode:\n    :return:\n    '''", "\n", "total", "=", "0", "\n", "empty", "=", "0", "\n", "\n", "empty_support", "=", "0", "\n", "empty_refutes", "=", "0", "\n", "total_s", "=", "0", "\n", "total_r", "=", "0", "\n", "\n", "p_nei", ",", "p_s", ",", "p_r", "=", "0", ",", "0", ",", "0", "\n", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "\n", "        ", "if", "actual", "[", "idx", "]", "[", "'label'", "]", "==", "\"NOT ENOUGH INFO\"", ":", "\n", "            ", "if", "len", "(", "instance", "[", "'predicted_evidence'", "]", ")", "==", "0", ":", "\n", "                ", "empty", "+=", "1", "\n", "", "total", "+=", "1", "\n", "", "elif", "actual", "[", "idx", "]", "[", "'label'", "]", "==", "'SUPPORTS'", ":", "\n", "            ", "if", "len", "(", "instance", "[", "'predicted_evidence'", "]", ")", "==", "0", ":", "\n", "                ", "empty_support", "+=", "1", "\n", "", "total_s", "+=", "1", "\n", "", "elif", "actual", "[", "idx", "]", "[", "'label'", "]", "==", "'REFUTES'", ":", "\n", "            ", "if", "len", "(", "instance", "[", "'predicted_evidence'", "]", ")", "==", "0", ":", "\n", "                ", "empty_refutes", "+=", "1", "\n", "", "total_r", "+=", "1", "\n", "\n", "", "if", "instance", "[", "'predicted_label'", "]", "==", "\"NOT ENOUGH INFO\"", ":", "\n", "            ", "p_nei", "+=", "1", "\n", "", "elif", "instance", "[", "'predicted_label'", "]", "==", "'SUPPORTS'", ":", "\n", "            ", "p_s", "+=", "1", "\n", "", "elif", "instance", "[", "'predicted_label'", "]", "==", "'REFUTES'", ":", "\n", "            ", "p_r", "+=", "1", "\n", "\n", "", "", "print", "(", "f\"prediction: NEI/S/R:{p_nei}/{p_s}/{p_r}\"", ")", "\n", "\n", "print", "(", "\"NEI\"", ",", "empty", ",", "total", ",", "empty", "/", "total", ")", "\n", "print", "(", "\"SUP\"", ",", "empty_support", ",", "total_s", ",", "empty_support", "/", "total_s", ")", "\n", "print", "(", "\"REF\"", ",", "empty_refutes", ",", "total_r", ",", "empty_refutes", "/", "total_r", ")", "\n", "\n", "return", "empty", ",", "total", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_confusion_matrix": [[632, 650], ["enumerate", "evaluation.confusion_matrix_analysis.report", "y_true.append", "y_pred.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.confusion_matrix_analysis.report"], ["", "def", "fever_confusion_matrix", "(", "predictions", ",", "actual", "=", "None", ")", ":", "\n", "    ", "label2id", "=", "{", "\n", "'SUPPORTS'", ":", "0", ",", "\n", "'REFUTES'", ":", "1", ",", "\n", "\"NOT ENOUGH INFO\"", ":", "2", ",", "\n", "}", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "\n", "labels", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "label_names", "=", "[", "'S'", ",", "'R'", ",", "'NEI'", "]", "\n", "report_name", "=", "\"FEVER CM Results:\"", "\n", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "predictions", ")", ":", "\n", "        ", "y_true", ".", "append", "(", "label2id", "[", "actual", "[", "idx", "]", "[", "'label'", "]", "]", ")", "\n", "y_pred", ".", "append", "(", "label2id", "[", "instance", "[", "'predicted_label'", "]", "]", ")", "\n", "\n", "", "report", "(", "y_true", ",", "y_pred", ",", "labels", ",", "label_names", ",", "report_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.normalize_answer": [[16, 31], ["ext_hotpot_eval.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.f1_score": [[33, 54], ["ext_hotpot_eval.normalize_answer", "ext_hotpot_eval.normalize_answer", "normalize_answer.split", "normalize_answer.split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "normalized_prediction", "=", "normalize_answer", "(", "prediction", ")", "\n", "normalized_ground_truth", "=", "normalize_answer", "(", "ground_truth", ")", "\n", "\n", "ZERO_METRIC", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "\n", "if", "normalized_prediction", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "if", "normalized_ground_truth", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "\n", "", "prediction_tokens", "=", "normalized_prediction", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalized_ground_truth", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.exact_match_score": [[56, 58], ["ext_hotpot_eval.normalize_answer", "ext_hotpot_eval.normalize_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.update_answer": [[60, 68], ["ext_hotpot_eval.exact_match_score", "ext_hotpot_eval.f1_score", "float"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.exact_match_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.f1_score"], ["", "def", "update_answer", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "em", "=", "exact_match_score", "(", "prediction", ",", "gold", ")", "\n", "f1", ",", "prec", ",", "recall", "=", "f1_score", "(", "prediction", ",", "gold", ")", "\n", "metrics", "[", "'em'", "]", "+=", "float", "(", "em", ")", "\n", "metrics", "[", "'f1'", "]", "+=", "f1", "\n", "metrics", "[", "'prec'", "]", "+=", "prec", "\n", "metrics", "[", "'recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.update_document": [[70, 95], ["set", "set", "map", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "update_document", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "prediction", ")", "\n", "\n", "gold_sp_pred", "=", "set", "(", ")", "\n", "for", "doc", ",", "ln", "in", "map", "(", "tuple", ",", "gold", ")", ":", "\n", "        ", "gold_sp_pred", ".", "add", "(", "doc", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'doc_em'", "]", "+=", "em", "\n", "metrics", "[", "'doc_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'doc_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'doc_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.update_sp": [[97, 118], ["set", "set", "map", "map"], "function", ["None"], ["", "def", "update_sp", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "prediction", ")", ")", "\n", "gold_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "gold", ")", ")", "\n", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'sp_em'", "]", "+=", "em", "\n", "metrics", "[", "'sp_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'sp_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'sp_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.ext_hotpot_eval.eval": [[128, 248], ["len", "metrics.keys", "scorer_all.update", "global_score_tracker.items", "print", "print", "print", "global_score_tracker[].update", "global_score_tracker[].update", "print", "print", "scorer_all.keys", "len", "len", "print", "global_score_tracker[].update", "ext_hotpot_eval.update_document", "print", "global_score_tracker[].update", "ext_hotpot_eval.update_answer", "print", "global_score_tracker[].update", "ext_hotpot_eval.update_sp"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.update_document", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_sp"], ["", "def", "eval", "(", "prediction", ",", "gold", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "metrics", "=", "{", "'em'", ":", "0", ",", "'f1'", ":", "0", ",", "'prec'", ":", "0", ",", "'recall'", ":", "0", ",", "\n", "'doc_em'", ":", "0", ",", "'doc_f1'", ":", "0", ",", "'doc_prec'", ":", "0", ",", "'doc_recall'", ":", "0", ",", "\n", "'sp_em'", ":", "0", ",", "'sp_f1'", ":", "0", ",", "'sp_prec'", ":", "0", ",", "'sp_recall'", ":", "0", ",", "\n", "'joint_em'", ":", "0", ",", "'joint_f1'", ":", "0", ",", "'joint_prec'", ":", "0", ",", "'joint_recall'", ":", "0", "}", "\n", "\n", "global_score_tracker", "=", "{", "}", "\n", "\n", "eval_doc", "=", "True", "\n", "eval_answer", "=", "True", "\n", "eval_sp", "=", "True", "\n", "\n", "if", "'sp_doc'", "not", "in", "prediction", ":", "\n", "        ", "print", "(", "'Key \\'sp_doc\\' not in prediction, do not evaluation document retrieval performance.'", ")", "\n", "eval_doc", "=", "False", "\n", "\n", "", "if", "'p_answer'", "not", "in", "prediction", ":", "\n", "        ", "print", "(", "'Key \\'p_answer\\' not in prediction, do not evaluation answer performance.'", ")", "\n", "eval_answer", "=", "False", "\n", "\n", "", "if", "'sp'", "not", "in", "prediction", ":", "\n", "        ", "print", "(", "'Key \\'sp\\' not in prediction, do not evaluation sp retrieval performance.'", ")", "\n", "eval_sp", "=", "False", "\n", "\n", "", "for", "dp", "in", "gold", ":", "\n", "        ", "cur_id", "=", "dp", "[", "'_id'", "]", "\n", "can_eval_joint", "=", "True", "\n", "global_score_tracker", "[", "cur_id", "]", "=", "{", "}", "\n", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'type'", "]", "=", "dp", "[", "'type'", "]", "\n", "# More subtype can go here", "\n", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "empty_metrics", ")", "\n", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "dp", ")", "\n", "\n", "# First build gold document", "\n", "if", "eval_doc", ":", "\n", "            ", "if", "cur_id", "not", "in", "prediction", "[", "'sp_doc'", "]", ":", "\n", "                ", "print", "(", "'missing sp doc {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "# can_eval_joint = False", "\n", "", "else", ":", "\n", "                ", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "{", "'sp_doc'", ":", "prediction", "[", "'sp_doc'", "]", "[", "cur_id", "]", "}", ")", "\n", "\n", "doc_em", ",", "doc_prec", ",", "doc_recall", ",", "doc_f1", "=", "update_document", "(", "\n", "metrics", ",", "prediction", "[", "'sp_doc'", "]", "[", "cur_id", "]", ",", "dp", "[", "'supporting_facts'", "]", ")", "\n", "# Supporting facts will not be used later.", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'doc_em'", "]", "=", "doc_em", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'doc_prec'", "]", "=", "doc_prec", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'doc_recall'", "]", "=", "doc_recall", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'doc_f1'", "]", "=", "doc_f1", "\n", "", "", "if", "eval_answer", ":", "\n", "            ", "if", "cur_id", "not", "in", "prediction", "[", "'p_answer'", "]", ":", "\n", "                ", "print", "(", "'missing answer {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "can_eval_joint", "=", "False", "\n", "", "else", ":", "\n", "                ", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "{", "'p_answer'", ":", "prediction", "[", "'p_answer'", "]", "[", "cur_id", "]", "}", ")", "\n", "\n", "em", ",", "prec", ",", "recall", ",", "f1", "=", "update_answer", "(", "\n", "metrics", ",", "prediction", "[", "'p_answer'", "]", "[", "cur_id", "]", ",", "dp", "[", "'answer'", "]", ")", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'em'", "]", "=", "em", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'prec'", "]", "=", "prec", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'recall'", "]", "=", "recall", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'f1'", "]", "=", "f1", "\n", "\n", "", "", "if", "eval_sp", ":", "\n", "            ", "if", "cur_id", "not", "in", "prediction", "[", "'sp'", "]", ":", "\n", "                ", "print", "(", "'missing sp fact {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "can_eval_joint", "=", "False", "\n", "", "else", ":", "\n", "                ", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "{", "'sp'", ":", "prediction", "[", "'sp'", "]", "[", "cur_id", "]", "}", ")", "\n", "\n", "sp_em", ",", "sp_prec", ",", "sp_recall", ",", "sp_f1", "=", "update_sp", "(", "\n", "metrics", ",", "prediction", "[", "'sp'", "]", "[", "cur_id", "]", ",", "dp", "[", "'supporting_facts'", "]", ")", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'sp_em'", "]", "=", "sp_em", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'sp_prec'", "]", "=", "sp_prec", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'sp_recall'", "]", "=", "sp_recall", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'sp_f1'", "]", "=", "sp_f1", "\n", "\n", "", "", "if", "can_eval_joint", "and", "eval_sp", "and", "eval_answer", ":", "\n", "            ", "joint_prec", "=", "prec", "*", "sp_prec", "\n", "joint_recall", "=", "recall", "*", "sp_recall", "\n", "if", "joint_prec", "+", "joint_recall", ">", "0", ":", "\n", "                ", "joint_f1", "=", "2", "*", "joint_prec", "*", "joint_recall", "/", "(", "joint_prec", "+", "joint_recall", ")", "\n", "", "else", ":", "\n", "                ", "joint_f1", "=", "0.", "\n", "", "joint_em", "=", "em", "*", "sp_em", "\n", "\n", "metrics", "[", "'joint_em'", "]", "+=", "joint_em", "\n", "metrics", "[", "'joint_f1'", "]", "+=", "joint_f1", "\n", "metrics", "[", "'joint_prec'", "]", "+=", "joint_prec", "\n", "metrics", "[", "'joint_recall'", "]", "+=", "joint_recall", "\n", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'joint_em'", "]", "=", "joint_em", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'joint_f1'", "]", "=", "joint_f1", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'joint_prec'", "]", "=", "joint_prec", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'joint_recall'", "]", "=", "joint_recall", "\n", "\n", "", "", "N", "=", "len", "(", "gold", ")", "\n", "for", "k", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "        ", "metrics", "[", "k", "]", "/=", "N", "\n", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Total examples:\"", ",", "N", ")", "\n", "print", "(", "metrics", ")", "\n", "\n", "", "scorer_all", "=", "{", "}", "\n", "scorer_all", ".", "update", "(", "empty_metrics", ")", "\n", "total_count", "=", "0", "\n", "for", "k", ",", "value", "in", "global_score_tracker", ".", "items", "(", ")", ":", "\n", "        ", "for", "metrices_name", "in", "scorer_all", ".", "keys", "(", ")", ":", "\n", "            ", "scorer_all", "[", "metrices_name", "]", "+=", "global_score_tracker", "[", "k", "]", "[", "metrices_name", "]", "\n", "", "total_count", "+=", "1", "\n", "\n", "# for k in scorer_all.keys():", "\n", "#     print(k, scorer_all[k] / total_count)", "\n", "#", "\n", "", "assert", "total_count", "==", "N", "\n", "\n", "assert", "len", "(", "global_score_tracker", ")", "==", "len", "(", "gold", ")", "\n", "\n", "return", "global_score_tracker", ",", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.parse_args": [[20, 37], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "argparse.ArgumentParser.print_help", "sys.exit"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'Official evaluation script for SQuAD version 2.0.'", ")", "\n", "parser", ".", "add_argument", "(", "'data_file'", ",", "metavar", "=", "'data.json'", ",", "help", "=", "'Input data JSON file.'", ")", "\n", "parser", ".", "add_argument", "(", "'pred_file'", ",", "metavar", "=", "'pred.json'", ",", "help", "=", "'Model predictions.'", ")", "\n", "parser", ".", "add_argument", "(", "'--out-file'", ",", "'-o'", ",", "metavar", "=", "'eval.json'", ",", "\n", "help", "=", "'Write accuracy metrics to file (default is stdout).'", ")", "\n", "parser", ".", "add_argument", "(", "'--na-prob-file'", ",", "'-n'", ",", "metavar", "=", "'na_prob.json'", ",", "\n", "help", "=", "'Model estimates of probability of no answer.'", ")", "\n", "parser", ".", "add_argument", "(", "'--na-prob-thresh'", ",", "'-t'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Predict \"\" if no-answer probability exceeds this (default = 1.0).'", ")", "\n", "parser", ".", "add_argument", "(", "'--out-image-dir'", ",", "'-p'", ",", "metavar", "=", "'out_images'", ",", "default", "=", "None", ",", "\n", "help", "=", "'Save precision-recall curves to directory.'", ")", "\n", "parser", ".", "add_argument", "(", "'--verbose'", ",", "'-v'", ",", "action", "=", "'store_true'", ")", "\n", "if", "len", "(", "sys", ".", "argv", ")", "==", "1", ":", "\n", "        ", "parser", ".", "print_help", "(", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_qid_to_has_ans": [[39, 46], ["bool"], "function", ["None"], ["", "def", "make_qid_to_has_ans", "(", "dataset", ")", ":", "\n", "    ", "qid_to_has_ans", "=", "{", "}", "\n", "for", "article", "in", "dataset", ":", "\n", "        ", "for", "p", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "            ", "for", "qa", "in", "p", "[", "'qas'", "]", ":", "\n", "                ", "qid_to_has_ans", "[", "qa", "[", "'id'", "]", "]", "=", "bool", "(", "qa", "[", "'answers'", "]", ")", "\n", "", "", "", "return", "qid_to_has_ans", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.normalize_answer": [[48, 66], ["squad_eval_v1.normalize_answer.white_space_fix"], "function", ["None"], ["", "def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "\"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"", "\n", "\n", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "regex", "=", "re", ".", "compile", "(", "r'\\b(a|an|the)\\b'", ",", "re", ".", "UNICODE", ")", "\n", "return", "re", ".", "sub", "(", "regex", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_tokens": [[68, 71], ["normalize_answer().split", "squad_eval_v1.normalize_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "get_tokens", "(", "s", ")", ":", "\n", "    ", "if", "not", "s", ":", "return", "[", "]", "\n", "return", "normalize_answer", "(", "s", ")", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.compute_exact": [[73, 75], ["int", "squad_eval_v1.normalize_answer", "squad_eval_v1.normalize_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "compute_exact", "(", "a_gold", ",", "a_pred", ")", ":", "\n", "    ", "return", "int", "(", "normalize_answer", "(", "a_gold", ")", "==", "normalize_answer", "(", "a_pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.compute_f1": [[77, 91], ["squad_eval_v1.get_tokens", "squad_eval_v1.get_tokens", "sum", "collections.Counter", "collections.Counter", "common.values", "int", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_tokens", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_tokens"], ["", "def", "compute_f1", "(", "a_gold", ",", "a_pred", ")", ":", "\n", "    ", "gold_toks", "=", "get_tokens", "(", "a_gold", ")", "\n", "pred_toks", "=", "get_tokens", "(", "a_pred", ")", "\n", "common", "=", "collections", ".", "Counter", "(", "gold_toks", ")", "&", "collections", ".", "Counter", "(", "pred_toks", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "len", "(", "gold_toks", ")", "==", "0", "or", "len", "(", "pred_toks", ")", "==", "0", ":", "\n", "# If either is no-answer, then F1 is 1 if they agree, 0 otherwise", "\n", "        ", "return", "int", "(", "gold_toks", "==", "pred_toks", ")", "\n", "", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "pred_toks", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "gold_toks", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_raw_scores": [[93, 131], ["max", "max", "global_score_tracker[].update", "print", "squad_eval_v1.normalize_answer", "squad_eval_v1.compute_exact", "squad_eval_v1.compute_f1"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.compute_exact", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.compute_f1"], ["", "def", "get_raw_scores", "(", "dataset", ",", "preds", ",", "global_score_tracker", "=", "None", ")", ":", "\n", "    ", "exact_scores", "=", "{", "}", "\n", "f1_scores", "=", "{", "}", "\n", "\n", "for", "article", "in", "dataset", ":", "\n", "        ", "for", "p", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "            ", "for", "qa", "in", "p", "[", "'qas'", "]", ":", "\n", "                ", "qid", "=", "qa", "[", "'id'", "]", "\n", "gold_answers", "=", "[", "a", "[", "'text'", "]", "for", "a", "in", "qa", "[", "'answers'", "]", "\n", "if", "normalize_answer", "(", "a", "[", "'text'", "]", ")", "]", "\n", "if", "not", "gold_answers", ":", "\n", "# For unanswerable questions, only correct answer is empty string", "\n", "                    ", "gold_answers", "=", "[", "''", "]", "\n", "\n", "", "if", "global_score_tracker", "is", "not", "None", ":", "\n", "                    ", "if", "qid", "not", "in", "global_score_tracker", ":", "\n", "                        ", "global_score_tracker", "[", "qid", "]", "=", "{", "}", "\n", "", "item", "=", "{", "}", "\n", "item", "[", "'context'", "]", "=", "p", "[", "'context'", "]", "\n", "item", "[", "'answers'", "]", "=", "gold_answers", "\n", "item", "[", "'id'", "]", "=", "qid", "\n", "item", "[", "'qa'", "]", "=", "qa", "[", "'question'", "]", "\n", "global_score_tracker", "[", "qid", "]", ".", "update", "(", "item", ")", "\n", "\n", "", "if", "qid", "not", "in", "preds", ":", "\n", "                    ", "print", "(", "'Missing prediction for %s'", "%", "qid", ")", "\n", "continue", "\n", "", "a_pred", "=", "preds", "[", "qid", "]", "\n", "# Take max over all gold answers", "\n", "exact_scores", "[", "qid", "]", "=", "max", "(", "compute_exact", "(", "a", ",", "a_pred", ")", "for", "a", "in", "gold_answers", ")", "\n", "f1_scores", "[", "qid", "]", "=", "max", "(", "compute_f1", "(", "a", ",", "a_pred", ")", "for", "a", "in", "gold_answers", ")", "\n", "\n", "if", "global_score_tracker", "is", "not", "None", ":", "\n", "                    ", "global_score_tracker", "[", "qid", "]", "[", "'p_answer'", "]", "=", "a_pred", "\n", "global_score_tracker", "[", "qid", "]", "[", "'em'", "]", "=", "exact_scores", "[", "qid", "]", "\n", "global_score_tracker", "[", "qid", "]", "[", "'f1'", "]", "=", "f1_scores", "[", "qid", "]", "\n", "\n", "", "", "", "", "return", "exact_scores", ",", "f1_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.apply_no_ans_threshold": [[133, 142], ["scores.items", "float"], "function", ["None"], ["", "def", "apply_no_ans_threshold", "(", "scores", ",", "na_probs", ",", "qid_to_has_ans", ",", "na_prob_thresh", ")", ":", "\n", "    ", "new_scores", "=", "{", "}", "\n", "for", "qid", ",", "s", "in", "scores", ".", "items", "(", ")", ":", "\n", "        ", "pred_na", "=", "na_probs", "[", "qid", "]", ">", "na_prob_thresh", "\n", "if", "pred_na", ":", "\n", "            ", "new_scores", "[", "qid", "]", "=", "float", "(", "not", "qid_to_has_ans", "[", "qid", "]", ")", "\n", "", "else", ":", "\n", "            ", "new_scores", "[", "qid", "]", "=", "s", "\n", "", "", "return", "new_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict": [[144, 158], ["len", "collections.OrderedDict", "len", "collections.OrderedDict", "sum", "sum", "sum", "sum", "exact_scores.values", "f1_scores.values"], "function", ["None"], ["", "def", "make_eval_dict", "(", "exact_scores", ",", "f1_scores", ",", "qid_list", "=", "None", ")", ":", "\n", "    ", "if", "not", "qid_list", ":", "\n", "        ", "total", "=", "len", "(", "exact_scores", ")", "\n", "return", "collections", ".", "OrderedDict", "(", "[", "\n", "(", "'exact'", ",", "100.0", "*", "sum", "(", "exact_scores", ".", "values", "(", ")", ")", "/", "total", ")", ",", "\n", "(", "'f1'", ",", "100.0", "*", "sum", "(", "f1_scores", ".", "values", "(", ")", ")", "/", "total", ")", ",", "\n", "(", "'total'", ",", "total", ")", ",", "\n", "]", ")", "\n", "", "else", ":", "\n", "        ", "total", "=", "len", "(", "qid_list", ")", "\n", "return", "collections", ".", "OrderedDict", "(", "[", "\n", "(", "'exact'", ",", "100.0", "*", "sum", "(", "exact_scores", "[", "k", "]", "for", "k", "in", "qid_list", ")", "/", "total", ")", ",", "\n", "(", "'f1'", ",", "100.0", "*", "sum", "(", "f1_scores", "[", "k", "]", "for", "k", "in", "qid_list", ")", "/", "total", ")", ",", "\n", "(", "'total'", ",", "total", ")", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval": [[161, 164], ["None"], "function", ["None"], ["", "", "def", "merge_eval", "(", "main_eval", ",", "new_eval", ",", "prefix", ")", ":", "\n", "    ", "for", "k", "in", "new_eval", ":", "\n", "        ", "main_eval", "[", "'%s_%s'", "%", "(", "prefix", ",", "k", ")", "]", "=", "new_eval", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.plot_pr_curve": [[166, 176], ["plt.step", "plt.fill_between", "plt.xlabel", "plt.ylabel", "plt.xlim", "plt.ylim", "plt.title", "plt.savefig", "plt.clf"], "function", ["None"], ["", "", "def", "plot_pr_curve", "(", "precisions", ",", "recalls", ",", "out_image", ",", "title", ")", ":", "\n", "    ", "plt", ".", "step", "(", "recalls", ",", "precisions", ",", "color", "=", "'b'", ",", "alpha", "=", "0.2", ",", "where", "=", "'post'", ")", "\n", "plt", ".", "fill_between", "(", "recalls", ",", "precisions", ",", "step", "=", "'post'", ",", "alpha", "=", "0.2", ",", "color", "=", "'b'", ")", "\n", "plt", ".", "xlabel", "(", "'Recall'", ")", "\n", "plt", ".", "ylabel", "(", "'Precision'", ")", "\n", "plt", ".", "xlim", "(", "[", "0.0", ",", "1.05", "]", ")", "\n", "plt", ".", "ylim", "(", "[", "0.0", ",", "1.05", "]", ")", "\n", "plt", ".", "title", "(", "title", ")", "\n", "plt", ".", "savefig", "(", "out_image", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_precision_recall_eval": [[178, 200], ["sorted", "enumerate", "squad_eval_v1.plot_pr_curve", "float", "float", "precisions.append", "recalls.append", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.plot_pr_curve"], ["", "def", "make_precision_recall_eval", "(", "scores", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "None", ",", "title", "=", "None", ")", ":", "\n", "    ", "qid_list", "=", "sorted", "(", "na_probs", ",", "key", "=", "lambda", "k", ":", "na_probs", "[", "k", "]", ")", "\n", "true_pos", "=", "0.0", "\n", "cur_p", "=", "1.0", "\n", "cur_r", "=", "0.0", "\n", "precisions", "=", "[", "1.0", "]", "\n", "recalls", "=", "[", "0.0", "]", "\n", "avg_prec", "=", "0.0", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qid_list", ")", ":", "\n", "        ", "if", "qid_to_has_ans", "[", "qid", "]", ":", "\n", "            ", "true_pos", "+=", "scores", "[", "qid", "]", "\n", "", "cur_p", "=", "true_pos", "/", "float", "(", "i", "+", "1", ")", "\n", "cur_r", "=", "true_pos", "/", "float", "(", "num_true_pos", ")", "\n", "if", "i", "==", "len", "(", "qid_list", ")", "-", "1", "or", "na_probs", "[", "qid", "]", "!=", "na_probs", "[", "qid_list", "[", "i", "+", "1", "]", "]", ":", "\n", "# i.e., if we can put a threshold after this point", "\n", "            ", "avg_prec", "+=", "cur_p", "*", "(", "cur_r", "-", "recalls", "[", "-", "1", "]", ")", "\n", "precisions", ".", "append", "(", "cur_p", ")", "\n", "recalls", ".", "append", "(", "cur_r", ")", "\n", "", "", "if", "out_image", ":", "\n", "        ", "plot_pr_curve", "(", "precisions", ",", "recalls", ",", "out_image", ",", "title", ")", "\n", "", "return", "{", "'ap'", ":", "100.0", "*", "avg_prec", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.run_precision_recall_analysis": [[202, 225], ["sum", "squad_eval_v1.make_precision_recall_eval", "squad_eval_v1.make_precision_recall_eval", "squad_eval_v1.make_precision_recall_eval", "squad_eval_v1.merge_eval", "squad_eval_v1.merge_eval", "squad_eval_v1.merge_eval", "os.makedirs", "float", "os.path.exists", "os.path.join", "os.path.join", "qid_to_has_ans.items", "os.path.join", "qid_to_has_ans.values"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_precision_recall_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_precision_recall_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_precision_recall_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval"], ["", "def", "run_precision_recall_analysis", "(", "main_eval", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "\n", "qid_to_has_ans", ",", "out_image_dir", ")", ":", "\n", "    ", "if", "out_image_dir", "and", "not", "os", ".", "path", ".", "exists", "(", "out_image_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "out_image_dir", ")", "\n", "", "num_true_pos", "=", "sum", "(", "1", "for", "v", "in", "qid_to_has_ans", ".", "values", "(", ")", "if", "v", ")", "\n", "if", "num_true_pos", "==", "0", ":", "\n", "        ", "return", "\n", "", "pr_exact", "=", "make_precision_recall_eval", "(", "\n", "exact_raw", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_exact.png'", ")", ",", "\n", "title", "=", "'Precision-Recall curve for Exact Match score'", ")", "\n", "pr_f1", "=", "make_precision_recall_eval", "(", "\n", "f1_raw", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_f1.png'", ")", ",", "\n", "title", "=", "'Precision-Recall curve for F1 score'", ")", "\n", "oracle_scores", "=", "{", "k", ":", "float", "(", "v", ")", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "}", "\n", "pr_oracle", "=", "make_precision_recall_eval", "(", "\n", "oracle_scores", ",", "na_probs", ",", "num_true_pos", ",", "qid_to_has_ans", ",", "\n", "out_image", "=", "os", ".", "path", ".", "join", "(", "out_image_dir", ",", "'pr_oracle.png'", ")", ",", "\n", "title", "=", "'Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_exact", ",", "'pr_exact'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_f1", ",", "'pr_f1'", ")", "\n", "merge_eval", "(", "main_eval", ",", "pr_oracle", ",", "'pr_oracle'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.histogram_na_prob": [[227, 238], ["plt.hist", "plt.xlabel", "plt.ylabel", "plt.title", "plt.savefig", "plt.clf", "numpy.ones_like", "float", "os.path.join", "len"], "function", ["None"], ["", "def", "histogram_na_prob", "(", "na_probs", ",", "qid_list", ",", "image_dir", ",", "name", ")", ":", "\n", "    ", "if", "not", "qid_list", ":", "\n", "        ", "return", "\n", "", "x", "=", "[", "na_probs", "[", "k", "]", "for", "k", "in", "qid_list", "]", "\n", "weights", "=", "np", ".", "ones_like", "(", "x", ")", "/", "float", "(", "len", "(", "x", ")", ")", "\n", "plt", ".", "hist", "(", "x", ",", "weights", "=", "weights", ",", "bins", "=", "20", ",", "range", "=", "(", "0.0", ",", "1.0", ")", ")", "\n", "plt", ".", "xlabel", "(", "'Model probability of no-answer'", ")", "\n", "plt", ".", "ylabel", "(", "'Proportion of dataset'", ")", "\n", "plt", ".", "title", "(", "'Histogram of no-answer probability: %s'", "%", "name", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "image_dir", ",", "'na_prob_hist_%s.png'", "%", "name", ")", ")", "\n", "plt", ".", "clf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.find_best_thresh": [[240, 260], ["sum", "sorted", "enumerate", "len"], "function", ["None"], ["", "def", "find_best_thresh", "(", "preds", ",", "scores", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "    ", "num_no_ans", "=", "sum", "(", "1", "for", "k", "in", "qid_to_has_ans", "if", "not", "qid_to_has_ans", "[", "k", "]", ")", "\n", "cur_score", "=", "num_no_ans", "\n", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "0.0", "\n", "qid_list", "=", "sorted", "(", "na_probs", ",", "key", "=", "lambda", "k", ":", "na_probs", "[", "k", "]", ")", "\n", "for", "i", ",", "qid", "in", "enumerate", "(", "qid_list", ")", ":", "\n", "        ", "if", "qid", "not", "in", "scores", ":", "continue", "\n", "if", "qid_to_has_ans", "[", "qid", "]", ":", "\n", "            ", "diff", "=", "scores", "[", "qid", "]", "\n", "", "else", ":", "\n", "            ", "if", "preds", "[", "qid", "]", ":", "\n", "                ", "diff", "=", "-", "1", "\n", "", "else", ":", "\n", "                ", "diff", "=", "0", "\n", "", "", "cur_score", "+=", "diff", "\n", "if", "cur_score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "cur_score", "\n", "best_thresh", "=", "na_probs", "[", "qid", "]", "\n", "", "", "return", "100.0", "*", "best_score", "/", "len", "(", "scores", ")", ",", "best_thresh", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.find_all_best_thresh": [[262, 269], ["squad_eval_v1.find_best_thresh", "squad_eval_v1.find_best_thresh"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.find_best_thresh", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.find_best_thresh"], ["", "def", "find_all_best_thresh", "(", "main_eval", ",", "preds", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", ":", "\n", "    ", "best_exact", ",", "exact_thresh", "=", "find_best_thresh", "(", "preds", ",", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "best_f1", ",", "f1_thresh", "=", "find_best_thresh", "(", "preds", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "main_eval", "[", "'best_exact'", "]", "=", "best_exact", "\n", "main_eval", "[", "'best_exact_thresh'", "]", "=", "exact_thresh", "\n", "main_eval", "[", "'best_f1'", "]", "=", "best_f1", "\n", "main_eval", "[", "'best_f1_thresh'", "]", "=", "f1_thresh", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_score": [[271, 308], ["squad_eval_v1.make_qid_to_has_ans", "squad_eval_v1.get_raw_scores", "squad_eval_v1.apply_no_ans_threshold", "squad_eval_v1.apply_no_ans_threshold", "squad_eval_v1.make_eval_dict", "print", "json.dumps", "make_qid_to_has_ans.items", "make_qid_to_has_ans.items", "squad_eval_v1.make_eval_dict", "squad_eval_v1.merge_eval", "squad_eval_v1.make_eval_dict", "squad_eval_v1.merge_eval"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_qid_to_has_ans", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_raw_scores", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.apply_no_ans_threshold", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.apply_no_ans_threshold", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval"], ["", "def", "get_score", "(", "pred_dict", ",", "actual_dict", ",", "show_ans_vs_no_results", "=", "False", ")", ":", "\n", "# dataset = dataset_json['data']", "\n", "    ", "na_prob_thresh", "=", "1.0", "\n", "\n", "dataset", "=", "actual_dict", "\n", "preds", "=", "pred_dict", "\n", "\n", "global_score_tracker", "=", "{", "}", "\n", "\n", "qid_to_has_ans", "=", "make_qid_to_has_ans", "(", "dataset", ")", "\n", "# This is a dictionary of id to True/False indicating whether the given uid has an answer.", "\n", "has_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "v", "]", "\n", "# All the id that has an answer.", "\n", "no_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "not", "v", "]", "\n", "# All the id that don't have an answer.", "\n", "exact_raw", ",", "f1_raw", "=", "get_raw_scores", "(", "dataset", ",", "preds", ",", "global_score_tracker", ")", "\n", "\n", "na_probs", "=", "{", "k", ":", "0.0", "for", "k", "in", "preds", "}", "\n", "\n", "exact_thresh", "=", "apply_no_ans_threshold", "(", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "na_prob_thresh", ")", "\n", "f1_thresh", "=", "apply_no_ans_threshold", "(", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "na_prob_thresh", ")", "\n", "\n", "out_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ")", "\n", "\n", "if", "show_ans_vs_no_results", ":", "\n", "        ", "if", "has_ans_qids", ":", "\n", "            ", "has_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "has_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "has_ans_eval", ",", "'HasAns'", ")", "\n", "", "if", "no_ans_qids", ":", "\n", "            ", "no_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "no_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "no_ans_eval", ",", "'NoAns'", ")", "\n", "\n", "", "", "print", "(", "json", ".", "dumps", "(", "out_eval", ",", "indent", "=", "2", ")", ")", "\n", "\n", "return", "global_score_tracker", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.main": [[310, 348], ["squad_eval_v1.make_qid_to_has_ans", "squad_eval_v1.get_raw_scores", "squad_eval_v1.apply_no_ans_threshold", "squad_eval_v1.apply_no_ans_threshold", "squad_eval_v1.make_eval_dict", "open", "json.load", "open", "json.load", "squad_eval_v1.make_eval_dict", "squad_eval_v1.merge_eval", "squad_eval_v1.make_eval_dict", "squad_eval_v1.merge_eval", "squad_eval_v1.find_all_best_thresh", "squad_eval_v1.run_precision_recall_analysis", "squad_eval_v1.histogram_na_prob", "squad_eval_v1.histogram_na_prob", "print", "open", "json.load", "make_qid_to_has_ans.items", "make_qid_to_has_ans.items", "open", "json.dump", "json.dumps"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_qid_to_has_ans", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.get_raw_scores", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.apply_no_ans_threshold", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.apply_no_ans_threshold", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.make_eval_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.merge_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.find_all_best_thresh", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.run_precision_recall_analysis", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.histogram_na_prob", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.squad_eval_v1.histogram_na_prob"], ["", "def", "main", "(", ")", ":", "\n", "    ", "with", "open", "(", "OPTS", ".", "data_file", ")", "as", "f", ":", "\n", "        ", "dataset_json", "=", "json", ".", "load", "(", "f", ")", "\n", "dataset", "=", "dataset_json", "[", "'data'", "]", "\n", "", "with", "open", "(", "OPTS", ".", "pred_file", ")", "as", "f", ":", "\n", "        ", "preds", "=", "json", ".", "load", "(", "f", ")", "\n", "", "if", "OPTS", ".", "na_prob_file", ":", "\n", "        ", "with", "open", "(", "OPTS", ".", "na_prob_file", ")", "as", "f", ":", "\n", "            ", "na_probs", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "else", ":", "\n", "        ", "na_probs", "=", "{", "k", ":", "0.0", "for", "k", "in", "preds", "}", "\n", "", "qid_to_has_ans", "=", "make_qid_to_has_ans", "(", "dataset", ")", "# maps qid to True/False", "\n", "has_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "v", "]", "\n", "no_ans_qids", "=", "[", "k", "for", "k", ",", "v", "in", "qid_to_has_ans", ".", "items", "(", ")", "if", "not", "v", "]", "\n", "exact_raw", ",", "f1_raw", "=", "get_raw_scores", "(", "dataset", ",", "preds", ")", "\n", "exact_thresh", "=", "apply_no_ans_threshold", "(", "exact_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "OPTS", ".", "na_prob_thresh", ")", "\n", "f1_thresh", "=", "apply_no_ans_threshold", "(", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ",", "\n", "OPTS", ".", "na_prob_thresh", ")", "\n", "out_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ")", "\n", "if", "has_ans_qids", ":", "\n", "        ", "has_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "has_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "has_ans_eval", ",", "'HasAns'", ")", "\n", "", "if", "no_ans_qids", ":", "\n", "        ", "no_ans_eval", "=", "make_eval_dict", "(", "exact_thresh", ",", "f1_thresh", ",", "qid_list", "=", "no_ans_qids", ")", "\n", "merge_eval", "(", "out_eval", ",", "no_ans_eval", ",", "'NoAns'", ")", "\n", "", "if", "OPTS", ".", "na_prob_file", ":", "\n", "        ", "find_all_best_thresh", "(", "out_eval", ",", "preds", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "qid_to_has_ans", ")", "\n", "", "if", "OPTS", ".", "na_prob_file", "and", "OPTS", ".", "out_image_dir", ":", "\n", "        ", "run_precision_recall_analysis", "(", "out_eval", ",", "exact_raw", ",", "f1_raw", ",", "na_probs", ",", "\n", "qid_to_has_ans", ",", "OPTS", ".", "out_image_dir", ")", "\n", "histogram_na_prob", "(", "na_probs", ",", "has_ans_qids", ",", "OPTS", ".", "out_image_dir", ",", "'hasAns'", ")", "\n", "histogram_na_prob", "(", "na_probs", ",", "no_ans_qids", ",", "OPTS", ".", "out_image_dir", ",", "'noAns'", ")", "\n", "", "if", "OPTS", ".", "out_file", ":", "\n", "        ", "with", "open", "(", "OPTS", ".", "out_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "out_eval", ",", "f", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "json", ".", "dumps", "(", "out_eval", ",", "indent", "=", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.normalize_answer": [[18, 33], ["open_domain_qa_eval.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.f1_score": [[35, 56], ["open_domain_qa_eval.normalize_answer", "open_domain_qa_eval.normalize_answer", "normalize_answer.split", "normalize_answer.split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "normalized_prediction", "=", "normalize_answer", "(", "prediction", ")", "\n", "normalized_ground_truth", "=", "normalize_answer", "(", "ground_truth", ")", "\n", "\n", "ZERO_METRIC", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "\n", "if", "normalized_prediction", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "if", "normalized_ground_truth", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "\n", "", "prediction_tokens", "=", "normalized_prediction", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalized_ground_truth", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.exact_match_score": [[58, 60], ["open_domain_qa_eval.normalize_answer", "open_domain_qa_eval.normalize_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.regex_match_score": [[62, 73], ["re.compile", "re.compile.match", "print"], "function", ["None"], ["", "def", "regex_match_score", "(", "prediction", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Check if the prediction matches the given regular expression.\"\"\"", "\n", "try", ":", "\n", "        ", "compiled", "=", "re", ".", "compile", "(", "\n", "pattern", ",", "\n", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", "\n", ")", "\n", "", "except", "BaseException", ":", "\n", "        ", "print", "(", "'Regular expression failed to compile: %s'", "%", "pattern", ")", "\n", "return", "False", "\n", "", "return", "compiled", ".", "match", "(", "prediction", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.update_answer": [[75, 110], ["isinstance", "float", "open_domain_qa_eval.exact_match_score", "open_domain_qa_eval.f1_score", "max", "max", "max", "max", "float", "ValueError", "float", "open_domain_qa_eval.regex_match_score", "max", "max", "max", "max", "float"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.exact_match_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.f1_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.regex_match_score"], ["", "def", "update_answer", "(", "metrics", ",", "prediction", ",", "gold", ",", "match_type", "=", "'string'", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "gold", ",", "list", ")", ":", "\n", "        ", "gold", "=", "[", "gold", "]", "\n", "", "if", "match_type", "==", "'string'", ":", "\n", "        ", "max_em", ",", "max_f1", ",", "max_prec", ",", "max_recall", "=", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", "\n", "for", "g_anw", "in", "gold", ":", "\n", "            ", "em", "=", "exact_match_score", "(", "prediction", ",", "g_anw", ")", "\n", "f1", ",", "prec", ",", "recall", "=", "f1_score", "(", "prediction", ",", "g_anw", ")", "\n", "max_em", "=", "max", "(", "float", "(", "em", ")", ",", "max_em", ")", "\n", "max_f1", "=", "max", "(", "f1", ",", "max_f1", ")", "\n", "max_prec", "=", "max", "(", "prec", ",", "max_prec", ")", "\n", "max_recall", "=", "max", "(", "recall", ",", "max_recall", ")", "\n", "", "metrics", "[", "'em'", "]", "+=", "float", "(", "max_em", ")", "\n", "metrics", "[", "'f1'", "]", "+=", "max_f1", "\n", "metrics", "[", "'prec'", "]", "+=", "max_prec", "\n", "metrics", "[", "'recall'", "]", "+=", "max_recall", "\n", "", "elif", "match_type", "==", "'regex'", ":", "\n", "        ", "max_em", ",", "max_f1", ",", "max_prec", ",", "max_recall", "=", "-", "1", ",", "0", ",", "0", ",", "0", "\n", "for", "g_anw", "in", "gold", ":", "\n", "            ", "em", "=", "regex_match_score", "(", "prediction", ",", "g_anw", ")", "\n", "# f1, prec, recall = f1_score(prediction, g_anw)", "\n", "f1", ",", "prec", ",", "recall", "=", "0", ",", "0", ",", "0", "\n", "max_em", "=", "max", "(", "float", "(", "em", ")", ",", "max_em", ")", "\n", "max_f1", "=", "max", "(", "f1", ",", "max_f1", ")", "\n", "max_prec", "=", "max", "(", "prec", ",", "max_prec", ")", "\n", "max_recall", "=", "max", "(", "recall", ",", "max_recall", ")", "\n", "\n", "", "metrics", "[", "'em'", "]", "+=", "float", "(", "max_em", ")", "\n", "metrics", "[", "'f1'", "]", "+=", "max_f1", "\n", "metrics", "[", "'prec'", "]", "+=", "max_prec", "\n", "metrics", "[", "'recall'", "]", "+=", "max_recall", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Invalid Match Type: {match_type}\"", ")", "\n", "\n", "", "return", "max_em", ",", "max_f1", ",", "max_prec", ",", "max_recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.update_document": [[112, 137], ["set", "set", "map", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "update_document", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "prediction", ")", "\n", "\n", "gold_sp_pred", "=", "set", "(", ")", "\n", "for", "doc", ",", "ln", "in", "map", "(", "tuple", ",", "gold", ")", ":", "\n", "        ", "gold_sp_pred", ".", "add", "(", "doc", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'doc_em'", "]", "+=", "em", "\n", "metrics", "[", "'doc_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'doc_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'doc_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.update_sp": [[139, 160], ["set", "set", "map", "map"], "function", ["None"], ["", "def", "update_sp", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "prediction", ")", ")", "\n", "gold_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "gold", ")", ")", "\n", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'sp_em'", "]", "+=", "em", "\n", "metrics", "[", "'sp_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'sp_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'sp_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval": [[172, 188], ["len", "zip", "print", "len", "len", "set", "set.add"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "qa_paragraph_eval", "(", "pred_list", ",", "gt_list", ",", "top_k", "=", "100", ")", ":", "\n", "    ", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "gt_list", ")", "\n", "total", "=", "len", "(", "pred_list", ")", "\n", "hit", "=", "0", "\n", "for", "p_item", ",", "gt_item", "in", "zip", "(", "pred_list", ",", "gt_list", ")", ":", "\n", "        ", "gt_set", "=", "set", "(", ")", "\n", "for", "item", "in", "gt_item", "[", "\"gt_p_list\"", "]", ":", "\n", "            ", "gt_set", ".", "add", "(", "(", "item", "[", "0", "]", ",", "item", "[", "1", "]", ")", ")", "\n", "\n", "", "for", "item", ",", "score", "in", "p_item", "[", "'score_list'", "]", "[", ":", "top_k", "]", ":", "\n", "            ", "cur_item", "=", "(", "item", "[", "0", "]", ",", "item", "[", "1", "]", ")", "\n", "if", "cur_item", "in", "gt_set", ":", "\n", "                ", "hit", "+=", "1", "\n", "break", "\n", "\n", "", "", "", "print", "(", "total", ",", "hit", ",", "hit", "/", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval_v1": [[190, 213], ["len", "zip", "collections.Counter", "print", "print", "print", "len", "len", "set", "pred_len_list.append", "collections.Counter.most_common", "numpy.mean", "numpy.std", "set.add", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add"], ["", "def", "qa_paragraph_eval_v1", "(", "pred_list", ",", "gt_list", ")", ":", "\n", "    ", "assert", "len", "(", "pred_list", ")", "==", "len", "(", "gt_list", ")", "\n", "total", "=", "len", "(", "pred_list", ")", "\n", "hit", "=", "0", "\n", "pred_len_list", "=", "[", "]", "\n", "for", "p_item", ",", "gt_item", "in", "zip", "(", "pred_list", ",", "gt_list", ")", ":", "\n", "        ", "gt_set", "=", "set", "(", ")", "\n", "for", "item", "in", "gt_item", "[", "\"gt_p_list\"", "]", ":", "\n", "            ", "gt_set", ".", "add", "(", "(", "item", "[", "0", "]", ",", "item", "[", "1", "]", ")", ")", "\n", "\n", "", "pred_len_list", ".", "append", "(", "len", "(", "p_item", "[", "'pred_p_list'", "]", ")", ")", "\n", "for", "item", "in", "p_item", "[", "'pred_p_list'", "]", ":", "\n", "            ", "cur_item", "=", "(", "item", "[", "0", "]", ",", "item", "[", "1", "]", ")", "\n", "if", "cur_item", "in", "gt_set", ":", "\n", "                ", "hit", "+=", "1", "\n", "break", "\n", "\n", "", "", "", "len_counter", "=", "Counter", "(", "pred_len_list", ")", "\n", "print", "(", "len_counter", ".", "most_common", "(", ")", ")", "\n", "print", "(", "np", ".", "mean", "(", "pred_len_list", ")", ")", "\n", "print", "(", "np", ".", "std", "(", "pred_len_list", ")", ")", "\n", "recall", "=", "hit", "/", "total", "\n", "return", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_eval": [[215, 276], ["len", "metrics.keys", "scorer_all.update", "global_score_tracker.items", "print", "global_score_tracker[].update", "global_score_tracker[].update", "print", "print", "scorer_all.keys", "print", "len", "len", "print", "global_score_tracker[].update", "open_domain_qa_eval.update_answer", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMAGPU.update", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_answer"], ["", "def", "qa_eval", "(", "prediction", ",", "gold", ",", "verbose", "=", "True", ",", "type", "=", "'string'", ",", "missing_ignore", "=", "True", ")", ":", "\n", "    ", "metrics", "=", "{", "'em'", ":", "0", ",", "'f1'", ":", "0", ",", "'prec'", ":", "0", ",", "'recall'", ":", "0", "}", "\n", "\n", "global_score_tracker", "=", "{", "}", "\n", "\n", "eval_answer", "=", "True", "\n", "\n", "if", "'p_answer'", "not", "in", "prediction", ":", "\n", "        ", "print", "(", "'Key \\'p_answer\\' not in prediction, do not evaluation answer performance.'", ")", "\n", "eval_answer", "=", "False", "\n", "\n", "", "for", "dp", "in", "gold", ":", "\n", "        ", "cur_id", "=", "dp", "[", "'qid'", "]", "\n", "global_score_tracker", "[", "cur_id", "]", "=", "{", "}", "\n", "\n", "# More subtype can go here", "\n", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "empty_metrics", ")", "\n", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "dp", ")", "\n", "\n", "if", "eval_answer", ":", "\n", "            ", "if", "cur_id", "not", "in", "prediction", "[", "'p_answer'", "]", ":", "\n", "                ", "if", "not", "missing_ignore", ":", "\n", "                    ", "print", "(", "'missing answer {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "# global_score_tracker[cur_id].update({'p_answer': \"\"})", "\n", "", "", "else", ":", "\n", "                ", "global_score_tracker", "[", "cur_id", "]", ".", "update", "(", "{", "'p_answer'", ":", "prediction", "[", "'p_answer'", "]", "[", "cur_id", "]", "}", ")", "\n", "\n", "em", ",", "prec", ",", "recall", ",", "f1", "=", "update_answer", "(", "\n", "metrics", ",", "prediction", "[", "'p_answer'", "]", "[", "cur_id", "]", ",", "dp", "[", "'answers'", "]", ",", "match_type", "=", "type", ")", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'em'", "]", "=", "em", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'prec'", "]", "=", "prec", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'recall'", "]", "=", "recall", "\n", "global_score_tracker", "[", "cur_id", "]", "[", "'f1'", "]", "=", "f1", "\n", "\n", "", "", "", "N", "=", "len", "(", "gold", ")", "\n", "for", "k", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "        ", "metrics", "[", "k", "]", "/=", "N", "\n", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Total examples:\"", ",", "N", ")", "\n", "print", "(", "metrics", ")", "\n", "\n", "", "scorer_all", "=", "{", "}", "\n", "scorer_all", ".", "update", "(", "empty_metrics", ")", "\n", "total_count", "=", "0", "\n", "for", "k", ",", "value", "in", "global_score_tracker", ".", "items", "(", ")", ":", "\n", "        ", "for", "metrices_name", "in", "scorer_all", ".", "keys", "(", ")", ":", "\n", "            ", "scorer_all", "[", "metrices_name", "]", "+=", "global_score_tracker", "[", "k", "]", "[", "metrices_name", "]", "\n", "", "total_count", "+=", "1", "\n", "\n", "# for k in scorer_all.keys():", "\n", "#     print(k, scorer_all[k] / total_count)", "\n", "#", "\n", "\n", "", "if", "total_count", "!=", "N", ":", "\n", "        ", "print", "(", "f\"Potential Duplicate Question, {total_count}, {N}\"", ")", "\n", "\n", "", "if", "len", "(", "global_score_tracker", ")", "!=", "len", "(", "gold", ")", ":", "\n", "        ", "print", "(", "\"Same issue above.\"", ")", "\n", "\n", "", "return", "global_score_tracker", ",", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.regex_match": [[13, 23], ["regex.compile", "re.compile.search"], "function", ["None"], ["def", "regex_match", "(", "text", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Test if a regex pattern is contained within a text.\"\"\"", "\n", "try", ":", "\n", "        ", "pattern", "=", "re", ".", "compile", "(", "\n", "pattern", ",", "\n", "flags", "=", "re", ".", "IGNORECASE", "+", "re", ".", "UNICODE", "+", "re", ".", "MULTILINE", ",", "\n", ")", "\n", "", "except", "BaseException", ":", "\n", "        ", "return", "False", "\n", "", "return", "pattern", ".", "search", "(", "text", ")", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.has_answer": [[25, 64], ["isinstance", "md.detokenize", "utils.text_process_tool.spacy_tokenize", "utils.text_process_tool.spacy_tokenize", "range", "utils.text_process_tool.normalize", "retrieval_drqa_eval.regex_match", "md.detokenize", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.text_process_tool.spacy_tokenize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.text_process_tool.spacy_tokenize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.regex_match"], ["", "def", "has_answer", "(", "answers", ",", "retrieved_text", ",", "match", "=", "'string'", ",", "tokenized", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"Check if retrieved_text contains an answer string.\n\n    If `match` is string, token matching is done between the text and answer.\n    If `match` is regex, we search the whole text with the regex.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "answers", ",", "list", ")", ":", "\n", "        ", "answers", "=", "[", "answers", "]", "\n", "\n", "", "if", "match", "==", "'string'", ":", "\n", "        ", "if", "tokenized", ":", "\n", "            ", "text", "=", "md", ".", "detokenize", "(", "retrieved_text", ")", "\n", "t_text", "=", "retrieved_text", "\n", "", "else", ":", "\n", "            ", "text", "=", "retrieved_text", "\n", "t_text", "=", "spacy_tokenize", "(", "retrieved_text", ",", "uncase", "=", "True", ")", "\n", "\n", "", "for", "single_answer", "in", "answers", ":", "\n", "            ", "single_answer", "=", "spacy_tokenize", "(", "single_answer", ",", "uncase", "=", "True", ")", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "t_text", ")", "-", "len", "(", "single_answer", ")", "+", "1", ")", ":", "\n", "                ", "if", "single_answer", "==", "t_text", "[", "i", ":", "i", "+", "len", "(", "single_answer", ")", "]", ":", "\n", "                    ", "return", "True", "\n", "\n", "", "", "", "for", "single_answer", "in", "answers", ":", "# If raw covered.", "\n", "            ", "if", "single_answer", "in", "text", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "", "elif", "match", "==", "'regex'", ":", "\n", "        ", "if", "tokenized", ":", "\n", "            ", "text", "=", "md", ".", "detokenize", "(", "retrieved_text", ")", "\n", "", "else", ":", "\n", "            ", "text", "=", "retrieved_text", "\n", "\n", "# Answer is a regex", "\n", "", "single_answer", "=", "normalize", "(", "answers", "[", "0", "]", ")", "\n", "if", "regex_match", "(", "text", ",", "single_answer", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.utest_normal": [[66, 70], ["print", "retrieval_drqa_eval.has_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.has_answer"], ["", "def", "utest_normal", "(", ")", ":", "\n", "    ", "paragraph", "=", "\"I'm name is eason\"", "\n", "answer", "=", "\"Eason\"", "\n", "print", "(", "has_answer", "(", "answer", ",", "paragraph", ",", "match", "=", "'string'", ",", "tokenized", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.utest_regex": [[72, 87], ["print", "retrieval_drqa_eval.has_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.retrieval_drqa_eval.has_answer"], ["", "def", "utest_regex", "(", ")", ":", "\n", "# {\"question\": \"How deep is Crater Lake?\",", "\n", "#  \"answer\": [\"1\\\\s?,\\\\s?932 feet|1,?949\\\\s*f(ee|oo)?t|594\\\\s*m|593 m|593\\\\.0|594\\\\.0552\"]}", "\n", "# paragraph = \"When is Fashion week in NYC?\"", "\n", "# paragraph = \"1 , 932 feet\"", "\n", "# paragraph = \"120  km/h\"", "\n", "    ", "paragraph", "=", "[", "'3'", ",", "','", ",", "'390'", ",", "'km'", "]", "\n", "\n", "# answer = \"Sept?(ember)?|Feb(ruary)?\"", "\n", "# answer = \"1\\\\s?,\\\\s?932 feet|1,?949\\\\s*f(ee|oo)?t|594\\\\s*m|593 m|593\\\\.0|594\\\\.0552\"", "\n", "# answer = \"120\\\\s*km/?h|75\\\\s*mph\"", "\n", "\n", "answer", "=", "\"diameter.*(4,?21[0-9]\\\\s*miles|6[78][0-9][0-9]\\\\s*(km|kilometers))|radius.*(2,?106\\\\s*miles|3,?390\\\\s*(km|kilometers))|3390km|3390 km|3\\\\,389\\\\.5 km\"", "\n", "\n", "print", "(", "has_answer", "(", "answer", ",", "paragraph", ",", "match", "=", "'regex'", ",", "tokenized", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer": [[9, 24], ["hotpot_evaluate_v1.normalize_answer.white_space_fix"], "function", ["None"], ["def", "normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.f1_score": [[26, 47], ["hotpot_evaluate_v1.normalize_answer", "hotpot_evaluate_v1.normalize_answer", "normalize_answer.split", "normalize_answer.split", "sum", "collections.Counter", "collections.Counter", "common.values", "len", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "f1_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "normalized_prediction", "=", "normalize_answer", "(", "prediction", ")", "\n", "normalized_ground_truth", "=", "normalize_answer", "(", "ground_truth", ")", "\n", "\n", "ZERO_METRIC", "=", "(", "0", ",", "0", ",", "0", ")", "\n", "\n", "if", "normalized_prediction", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "if", "normalized_ground_truth", "in", "[", "'yes'", ",", "'no'", ",", "'noanswer'", "]", "and", "normalized_prediction", "!=", "normalized_ground_truth", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "\n", "", "prediction_tokens", "=", "normalized_prediction", ".", "split", "(", ")", "\n", "ground_truth_tokens", "=", "normalized_ground_truth", ".", "split", "(", ")", "\n", "common", "=", "Counter", "(", "prediction_tokens", ")", "&", "Counter", "(", "ground_truth_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "        ", "return", "ZERO_METRIC", "\n", "", "precision", "=", "1.0", "*", "num_same", "/", "len", "(", "prediction_tokens", ")", "\n", "recall", "=", "1.0", "*", "num_same", "/", "len", "(", "ground_truth_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", ",", "precision", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.exact_match_score": [[49, 51], ["hotpot_evaluate_v1.normalize_answer", "hotpot_evaluate_v1.normalize_answer"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.normalize_answer"], ["", "def", "exact_match_score", "(", "prediction", ",", "ground_truth", ")", ":", "\n", "    ", "return", "(", "normalize_answer", "(", "prediction", ")", "==", "normalize_answer", "(", "ground_truth", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_answer": [[53, 61], ["hotpot_evaluate_v1.exact_match_score", "hotpot_evaluate_v1.f1_score", "float"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.exact_match_score", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.f1_score"], ["", "def", "update_answer", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "em", "=", "exact_match_score", "(", "prediction", ",", "gold", ")", "\n", "f1", ",", "prec", ",", "recall", "=", "f1_score", "(", "prediction", ",", "gold", ")", "\n", "metrics", "[", "'em'", "]", "+=", "float", "(", "em", ")", "\n", "metrics", "[", "'f1'", "]", "+=", "f1", "\n", "metrics", "[", "'prec'", "]", "+=", "prec", "\n", "metrics", "[", "'recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_sp": [[63, 84], ["set", "set", "map", "map"], "function", ["None"], ["", "def", "update_sp", "(", "metrics", ",", "prediction", ",", "gold", ")", ":", "\n", "    ", "cur_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "prediction", ")", ")", "\n", "gold_sp_pred", "=", "set", "(", "map", "(", "tuple", ",", "gold", ")", ")", "\n", "tp", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", "\n", "for", "e", "in", "cur_sp_pred", ":", "\n", "        ", "if", "e", "in", "gold_sp_pred", ":", "\n", "            ", "tp", "+=", "1", "\n", "", "else", ":", "\n", "            ", "fp", "+=", "1", "\n", "", "", "for", "e", "in", "gold_sp_pred", ":", "\n", "        ", "if", "e", "not", "in", "cur_sp_pred", ":", "\n", "            ", "fn", "+=", "1", "\n", "", "", "prec", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fp", ")", "if", "tp", "+", "fp", ">", "0", "else", "0.0", "\n", "recall", "=", "1.0", "*", "tp", "/", "(", "tp", "+", "fn", ")", "if", "tp", "+", "fn", ">", "0", "else", "0.0", "\n", "f1", "=", "2", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "if", "prec", "+", "recall", ">", "0", "else", "0.0", "\n", "em", "=", "1.0", "if", "fp", "+", "fn", "==", "0", "else", "0.0", "\n", "metrics", "[", "'sp_em'", "]", "+=", "em", "\n", "metrics", "[", "'sp_f1'", "]", "+=", "f1", "\n", "metrics", "[", "'sp_prec'", "]", "+=", "prec", "\n", "metrics", "[", "'sp_recall'", "]", "+=", "recall", "\n", "return", "em", ",", "prec", ",", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval": [[86, 130], ["len", "metrics.keys", "print", "open", "ujson.load", "open", "ujson.load", "print", "hotpot_evaluate_v1.update_answer", "print", "hotpot_evaluate_v1.update_sp"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_answer", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.update_sp"], ["", "def", "eval", "(", "prediction_file", ",", "gold_file", ")", ":", "\n", "    ", "with", "open", "(", "prediction_file", ")", "as", "f", ":", "\n", "        ", "prediction", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "gold_file", ")", "as", "f", ":", "\n", "        ", "gold", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "metrics", "=", "{", "'em'", ":", "0", ",", "'f1'", ":", "0", ",", "'prec'", ":", "0", ",", "'recall'", ":", "0", ",", "\n", "'sp_em'", ":", "0", ",", "'sp_f1'", ":", "0", ",", "'sp_prec'", ":", "0", ",", "'sp_recall'", ":", "0", ",", "\n", "'joint_em'", ":", "0", ",", "'joint_f1'", ":", "0", ",", "'joint_prec'", ":", "0", ",", "'joint_recall'", ":", "0", "}", "\n", "for", "dp", "in", "gold", ":", "\n", "        ", "cur_id", "=", "dp", "[", "'_id'", "]", "\n", "can_eval_joint", "=", "True", "\n", "if", "cur_id", "not", "in", "prediction", "[", "'answer'", "]", ":", "\n", "            ", "print", "(", "'missing answer {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "can_eval_joint", "=", "False", "\n", "", "else", ":", "\n", "            ", "em", ",", "prec", ",", "recall", "=", "update_answer", "(", "\n", "metrics", ",", "prediction", "[", "'answer'", "]", "[", "cur_id", "]", ",", "dp", "[", "'answer'", "]", ")", "\n", "", "if", "cur_id", "not", "in", "prediction", "[", "'sp'", "]", ":", "\n", "            ", "print", "(", "'missing sp fact {}'", ".", "format", "(", "cur_id", ")", ")", "\n", "can_eval_joint", "=", "False", "\n", "", "else", ":", "\n", "            ", "sp_em", ",", "sp_prec", ",", "sp_recall", "=", "update_sp", "(", "\n", "metrics", ",", "prediction", "[", "'sp'", "]", "[", "cur_id", "]", ",", "dp", "[", "'supporting_facts'", "]", ")", "\n", "\n", "", "if", "can_eval_joint", ":", "\n", "            ", "joint_prec", "=", "prec", "*", "sp_prec", "\n", "joint_recall", "=", "recall", "*", "sp_recall", "\n", "if", "joint_prec", "+", "joint_recall", ">", "0", ":", "\n", "                ", "joint_f1", "=", "2", "*", "joint_prec", "*", "joint_recall", "/", "(", "joint_prec", "+", "joint_recall", ")", "\n", "", "else", ":", "\n", "                ", "joint_f1", "=", "0.", "\n", "", "joint_em", "=", "em", "*", "sp_em", "\n", "\n", "metrics", "[", "'joint_em'", "]", "+=", "joint_em", "\n", "metrics", "[", "'joint_f1'", "]", "+=", "joint_f1", "\n", "metrics", "[", "'joint_prec'", "]", "+=", "joint_prec", "\n", "metrics", "[", "'joint_recall'", "]", "+=", "joint_recall", "\n", "\n", "", "", "N", "=", "len", "(", "gold", ")", "\n", "for", "k", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "        ", "metrics", "[", "k", "]", "/=", "N", "\n", "\n", "", "print", "(", "metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.confusion_matrix_analysis.compute_RkCC": [[35, 100], ["numpy.shape", "range", "range", "range", "range", "range", "range", "range", "range", "range", "numpy.sqrt", "numpy.sqrt", "range", "range"], "function", ["None"], ["def", "compute_RkCC", "(", "confusion_matrix", ")", ":", "\n", "    ", "'''\n    Function to compute the K-category correlation coefficient\n    http://www.sciencedirect.com/science/article/pii/S1476927104000799\n\n    http://rk.kvl.dk/suite/04022321447260711221/\n\n\n    Parameters\n    ----------\n    confusion_matrix : k X k confusion matrix of int\n\n    n_samples : int\n\n\n    Returns\n    -------\n    RkCC: float\n\n\n    '''", "\n", "rows", ",", "cols", "=", "np", ".", "shape", "(", "confusion_matrix", ")", "\n", "\n", "RkCC_numerator", "=", "0", "\n", "for", "k_", "in", "range", "(", "cols", ")", ":", "\n", "        ", "for", "l_", "in", "range", "(", "cols", ")", ":", "\n", "            ", "for", "m_", "in", "range", "(", "cols", ")", ":", "\n", "                ", "this_term", "=", "(", "confusion_matrix", "[", "k_", ",", "k_", "]", "*", "confusion_matrix", "[", "m_", ",", "l_", "]", ")", "-", "(", "confusion_matrix", "[", "l_", ",", "k_", "]", "*", "confusion_matrix", "[", "k_", ",", "m_", "]", ")", "\n", "\n", "RkCC_numerator", "=", "RkCC_numerator", "+", "this_term", "\n", "\n", "", "", "", "RkCC_denominator_1", "=", "0", "\n", "for", "k_", "in", "range", "(", "cols", ")", ":", "\n", "        ", "RkCC_den_1_part1", "=", "0", "\n", "for", "l_", "in", "range", "(", "cols", ")", ":", "\n", "            ", "RkCC_den_1_part1", "=", "RkCC_den_1_part1", "+", "confusion_matrix", "[", "l_", ",", "k_", "]", "\n", "\n", "", "RkCC_den_1_part2", "=", "0", "\n", "for", "f_", "in", "range", "(", "cols", ")", ":", "\n", "            ", "if", "f_", "!=", "k_", ":", "\n", "\n", "                ", "for", "g_", "in", "range", "(", "cols", ")", ":", "\n", "                    ", "RkCC_den_1_part2", "=", "RkCC_den_1_part2", "+", "confusion_matrix", "[", "g_", ",", "f_", "]", "\n", "\n", "", "", "", "RkCC_denominator_1", "=", "(", "RkCC_denominator_1", "+", "(", "RkCC_den_1_part1", "*", "RkCC_den_1_part2", ")", ")", "\n", "\n", "", "RkCC_denominator_2", "=", "0", "\n", "for", "k_", "in", "range", "(", "cols", ")", ":", "\n", "        ", "RkCC_den_2_part1", "=", "0", "\n", "for", "l_", "in", "range", "(", "cols", ")", ":", "\n", "            ", "RkCC_den_2_part1", "=", "RkCC_den_2_part1", "+", "confusion_matrix", "[", "k_", ",", "l_", "]", "\n", "\n", "", "RkCC_den_2_part2", "=", "0", "\n", "for", "f_", "in", "range", "(", "cols", ")", ":", "\n", "            ", "if", "f_", "!=", "k_", ":", "\n", "\n", "                ", "for", "g_", "in", "range", "(", "cols", ")", ":", "\n", "                    ", "RkCC_den_2_part2", "=", "RkCC_den_2_part2", "+", "confusion_matrix", "[", "f_", ",", "g_", "]", "\n", "\n", "", "", "", "RkCC_denominator_2", "=", "(", "RkCC_denominator_2", "+", "(", "RkCC_den_2_part1", "*", "RkCC_den_2_part2", ")", ")", "\n", "\n", "", "RkCC", "=", "(", "RkCC_numerator", ")", "/", "(", "np", ".", "sqrt", "(", "RkCC_denominator_1", ")", "*", "np", ".", "sqrt", "(", "RkCC_denominator_2", ")", ")", "\n", "\n", "return", "RkCC", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.confusion_matrix_analysis.print_cm": [[102, 125], ["print", "max", "print", "print", "enumerate", "print", "print", "range", "print", "len", "print", "len", "float"], "function", ["None"], ["", "def", "print_cm", "(", "cm", ",", "labels", ",", "hide_zeroes", "=", "False", ",", "hide_diagonal", "=", "False", ",", "hide_threshold", "=", "None", ")", ":", "\n", "    ", "\"\"\"pretty print for confusion matrixes\"\"\"", "\n", "print", "(", "cm", ")", "\n", "columnwidth", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "labels", "]", "+", "[", "6", "]", ")", "# 5 is value length", "\n", "empty_cell", "=", "\" \"", "*", "columnwidth", "\n", "# Print header", "\n", "print", "(", "\"    \"", "+", "empty_cell", ")", "\n", "for", "label", "in", "labels", ":", "\n", "        ", "print", "(", "\"%{0}s\"", ".", "format", "(", "columnwidth", ")", "%", "label", ")", "\n", "", "print", "(", ")", "\n", "# Print rows", "\n", "for", "i", ",", "label1", "in", "enumerate", "(", "labels", ")", ":", "\n", "        ", "print", "(", "\"    %{0}s\"", ".", "format", "(", "columnwidth", ")", "%", "label1", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "            ", "cell", "=", "\"%{0}.4f\"", ".", "format", "(", "columnwidth", ")", "%", "cm", "[", "i", ",", "j", "]", "\n", "if", "hide_zeroes", ":", "\n", "                ", "cell", "=", "cell", "if", "float", "(", "cm", "[", "i", ",", "j", "]", ")", "!=", "0", "else", "empty_cell", "\n", "", "if", "hide_diagonal", ":", "\n", "                ", "cell", "=", "cell", "if", "i", "!=", "j", "else", "empty_cell", "\n", "", "if", "hide_threshold", ":", "\n", "                ", "cell", "=", "cell", "if", "cm", "[", "i", ",", "j", "]", ">", "hide_threshold", "else", "empty_cell", "\n", "", "print", "(", "cell", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.confusion_matrix_analysis.report": [[127, 146], ["print", "sklearn.metrics.confusion_matrix", "print", "print", "print", "confusion_matrix_analysis.compute_RkCC", "print", "len", "len", "sklearn.metrics.classification_report"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.confusion_matrix_analysis.compute_RkCC"], ["", "", "def", "report", "(", "y_true", ":", "List", "[", "int", "]", ",", "y_pred", ":", "List", "[", "int", "]", ",", "labels", ":", "List", "[", "int", "]", ",", "\n", "class_name", ":", "List", "[", "str", "]", "=", "None", ",", "report_name", ":", "str", "=", "\"Default Name\"", ")", "->", "Tuple", ":", "\n", "    ", "print", "(", "\"{} Result Reporting:\"", ".", "format", "(", "report_name", ")", ")", "\n", "\n", "if", "class_name", "is", "None", ":", "\n", "        ", "class_name", "=", "[", "\"class {}\"", ".", "format", "(", "i", ")", "for", "i", "in", "labels", "]", "\n", "\n", "", "assert", "len", "(", "class_name", ")", "==", "len", "(", "labels", ")", "\n", "\n", "cmatrix", "=", "confusion_matrix", "(", "y_true", ",", "y_pred", ",", "labels", "=", "labels", ")", "\n", "\n", "print", "(", "\"Sklearn Confusion Matrix:\"", ")", "\n", "print", "(", "cmatrix", ")", "\n", "print", "(", "classification_report", "(", "y_true", ",", "y_pred", ",", "target_names", "=", "class_name", ",", "digits", "=", "5", ")", ")", "\n", "\n", "RkCC", "=", "compute_RkCC", "(", "cmatrix", ")", "\n", "print", "(", "'Rk correlation coefficient = %.4f'", "%", "RkCC", ")", "\n", "\n", "return", "cmatrix", ",", "RkCC", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.BertSpan.__init__": [[50, 69], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "hotpot_bert_v0.init_bert_weights", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "hotpot_bert_v0.init_bert_weights", "hotpot_bert_v0.init_bert_weights", "ValueError"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights"], ["", "def", "__init__", "(", "self", ",", "bert_encoder", ",", "num_of_layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "BertSpan", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_encoder", "=", "bert_encoder", "\n", "if", "num_of_layers", "==", "1", ":", "\n", "            ", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ",", "2", ")", "# Should we have dropout here? Later?", "\n", "init_bert_weights", "(", "self", ".", "qa_outputs", ",", "initializer_range", "=", "0.02", ")", "# Hard code this value", "\n", "\n", "", "elif", "num_of_layers", "==", "2", ":", "\n", "            ", "self", ".", "output_layer1", "=", "nn", ".", "Linear", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ",", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "output_layer2", "=", "nn", ".", "Linear", "(", "self", ".", "bert_encoder", ".", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Sequential", "(", "self", ".", "output_layer1", ",", "self", ".", "activation", ",", "self", ".", "dropout", ",", "self", ".", "output_layer2", ")", "\n", "\n", "init_bert_weights", "(", "self", ".", "output_layer1", ",", "initializer_range", "=", "0.02", ")", "\n", "init_bert_weights", "(", "self", ".", "output_layer2", ",", "initializer_range", "=", "0.02", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Number of layers not supported.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.BertSpan.forward": [[70, 98], ["hotpot_bert_v0.BertSpan.bert_encoder", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "hotpot_bert_v0.BertSpan.qa_outputs", "allennlp.nn.util.replace_masked_values", "allennlp.nn.util.replace_masked_values", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "torch.nn.functional.nll_loss", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "gt_span", "=", "None", ",", "mode", "=", "ForwardMode", ".", "TRAIN", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert_encoder", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "joint_length", "=", "allen_util", ".", "get_lengths_from_binary_sequence_mask", "(", "attention_mask", ")", "\n", "\n", "joint_seq_logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "\n", "# The following line is from AllenNLP bidaf.", "\n", "start_logits", "=", "allen_util", ".", "replace_masked_values", "(", "joint_seq_logits", "[", ":", ",", ":", ",", "0", "]", ",", "attention_mask", ",", "-", "1e18", ")", "\n", "# B, T, 2", "\n", "end_logits", "=", "allen_util", ".", "replace_masked_values", "(", "joint_seq_logits", "[", ":", ",", ":", ",", "1", "]", ",", "attention_mask", ",", "-", "1e18", ")", "\n", "\n", "if", "mode", "==", "BertSpan", ".", "ForwardMode", ".", "TRAIN", ":", "\n", "            ", "assert", "gt_span", "is", "not", "None", "\n", "gt_start", "=", "gt_span", "[", ":", ",", "0", "]", "# gt_span: [B, 2] -> [B]", "\n", "gt_end", "=", "gt_span", "[", ":", ",", "1", "]", "\n", "\n", "start_loss", "=", "nll_loss", "(", "allen_util", ".", "masked_log_softmax", "(", "start_logits", ",", "attention_mask", ")", ",", "gt_start", ")", "\n", "end_loss", "=", "nll_loss", "(", "allen_util", ".", "masked_log_softmax", "(", "end_logits", ",", "attention_mask", ")", ",", "gt_end", ")", "\n", "# We delete squeeze bc it will cause problem when the batch size is 1, and remember the gt_start and gt_end should have shape [B].", "\n", "# start_loss = nll_loss(allen_util.masked_log_softmax(start_logits, context_mask), gt_start.squeeze(-1))", "\n", "# end_loss = nll_loss(allen_util.masked_log_softmax(end_logits, context_mask), gt_end.squeeze(-1))", "\n", "\n", "loss", "=", "start_loss", "+", "end_loss", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", ",", "joint_length", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.init_bert_weights": [[31, 43], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.beta.data.normal_", "module.gamma.data.normal_"], "function", ["None"], ["def", "init_bert_weights", "(", "module", ",", "initializer_range", ")", ":", "\n", "    ", "\"\"\" Initialize the weights.\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "        ", "module", ".", "beta", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "module", ".", "gamma", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "initializer_range", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "        ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.span_eval": [[100, 127], ["span_prediction_task_utils.common_utils.merge_predicted_fitem_to_eitem", "torch.no_grad", "torch.no_grad", "model.eval", "tqdm.tqdm", "enumerate", "allennlp.nn.util.move_to_device", "flint.get_length_and_mask", "model", "span_prediction_task_utils.common_utils.write_to_predicted_fitem"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.merge_predicted_fitem_to_eitem", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.common_utils.write_to_predicted_fitem"], ["", "", "", "def", "span_eval", "(", "model", ",", "data_iter", ",", "do_lower_case", ",", "fitem_dict", ",", "device_num", ",", "show_progress", ",", "pred_no_answer", "=", "True", ")", ":", "\n", "# fitem_dict in the parameter is the original fitem_dict", "\n", "    ", "output_fitem_dict", "=", "{", "}", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "allen_util", ".", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "gt_span", "=", "batch", "[", "'gt_span'", "]", "\n", "\n", "start_logits", ",", "end_logits", ",", "context_length", "=", "model", "(", "mode", "=", "BertSpan", ".", "ForwardMode", ".", "EVAL", ",", "\n", "input_ids", "=", "paired_sequence", ",", "\n", "token_type_ids", "=", "paired_segments_ids", ",", "\n", "attention_mask", "=", "att_mask", ",", "\n", "gt_span", "=", "gt_span", ")", "\n", "b_fids", "=", "batch", "[", "'fid'", "]", "\n", "b_uids", "=", "batch", "[", "'uid'", "]", "\n", "\n", "write_to_predicted_fitem", "(", "start_logits", ",", "end_logits", ",", "context_length", ",", "b_fids", ",", "b_uids", ",", "gt_span", ",", "fitem_dict", ",", "\n", "output_fitem_dict", ",", "do_lower_case", ")", "\n", "\n", "", "", "eitem_list", ",", "eval_dict", "=", "merge_predicted_fitem_to_eitem", "(", "output_fitem_dict", ",", "None", ",", "pred_no_answer", "=", "pred_no_answer", ")", "\n", "return", "eitem_list", ",", "eval_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.eval_model": [[129, 284], ["torch.manual_seed", "torch.manual_seed", "print", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "utils.common.load_json", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "hotpot_bert_v0.BertSpan", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "torch.cuda.is_available", "torch.cuda.is_available", "utils.common.load_jsonl", "utils.common.load_jsonl", "torch.load", "torch.load", "torch.nn.DataParallel", "torch.nn.DataParallel", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader.read", "allennlp.data.iterators.BasicIterator.", "hotpot_bert_v0.span_eval", "dict", "evaluation.ext_hotpot_eval.eval", "print", "print", "torch.cuda.is_available", "torch.cuda.is_available", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader.read", "allennlp.data.iterators.BasicIterator.", "hotpot_bert_v0.span_eval", "dict", "utils.common.save_json", "evaluation.ext_hotpot_eval.eval", "print"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.span_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.span_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval"], ["", "def", "eval_model", "(", "model_path", ",", "data_file", "=", "None", ",", "filter_value", "=", "0.5", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "\"bert-base-uncased\"", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "16", "\n", "batch_size", "=", "32", "\n", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug", "=", "False", "\n", "\n", "max_pre_context_length", "=", "320", "\n", "max_query_length", "=", "64", "\n", "doc_stride", "=", "128", "\n", "qa_num_of_layer", "=", "2", "\n", "s_filter_value", "=", "filter_value", "\n", "s_top_k", "=", "5", "\n", "\n", "tag", "=", "'dev'", "\n", "\n", "print", "(", "\"Potential total length:\"", ",", "max_pre_context_length", "+", "max_query_length", "+", "3", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "\n", "# Load Dataset.", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "test_list", "=", "common", ".", "load_json", "(", "config", ".", "TEST_FULLWIKI_FILE", ")", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "\n", "if", "data_file", "is", "None", ":", "\n", "        ", "dev_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/dev_s_level_bert_v1_results.jsonl\"", ")", "\n", "", "else", ":", "\n", "        ", "dev_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "data_file", "\n", ")", "\n", "\n", "", "test_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/test_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "train_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/train_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "dev_fitem_dict", ",", "dev_fitem_list", ",", "dev_sp_results_dict", "=", "get_qa_item_with_upstream_sentence", "(", "dev_list", ",", "\n", "dev_sentence_level_results", ",", "\n", "is_training", "=", "False", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_context_length", "=", "max_pre_context_length", ",", "\n", "max_query_length", "=", "max_query_length", ",", "\n", "filter_value", "=", "s_filter_value", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "top_k", "=", "s_top_k", ",", "\n", "debug_mode", "=", "debug", ")", "\n", "\n", "test_fitem_dict", ",", "test_fitem_list", ",", "test_sp_results_dict", "=", "get_qa_item_with_upstream_sentence", "(", "test_list", ",", "\n", "test_sentence_level_results", ",", "\n", "is_training", "=", "False", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_context_length", "=", "max_pre_context_length", ",", "\n", "max_query_length", "=", "max_query_length", ",", "\n", "filter_value", "=", "s_filter_value", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "top_k", "=", "s_top_k", ",", "\n", "debug_mode", "=", "debug", ")", "\n", "\n", "# train_fitem_dict, train_fitem_list, _ = get_qa_item_with_upstream_sentence(train_list, train_sentence_level_results,", "\n", "#                                                                            is_training=True,", "\n", "#                                                                            tokenizer=tokenizer,", "\n", "#                                                                            max_context_length=max_pre_context_length,", "\n", "#                                                                            max_query_length=max_query_length,", "\n", "#                                                                            filter_value=s_filter_value,", "\n", "#                                                                            doc_stride=doc_stride,", "\n", "#                                                                            top_k=s_top_k,", "\n", "#                                                                            debug_mode=debug)", "\n", "\n", "if", "debug", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "\n", "", "span_pred_reader", "=", "BertPairedSpanPredReader", "(", "bert_tokenizer", "=", "tokenizer", ",", "lazy", "=", "lazy", ",", "\n", "example_filter", "=", "None", ")", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertSpan", "(", "bert_encoder", ",", "qa_num_of_layer", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "iterator", "=", "BasicIterator", "(", "batch_size", "=", "batch_size", ")", "\n", "\n", "if", "tag", "==", "'dev'", ":", "\n", "        ", "dev_instances", "=", "span_pred_reader", ".", "read", "(", "dev_fitem_list", ")", "\n", "# test_instances = span_pred_reader.read(test_fitem_list)", "\n", "eval_iter", "=", "iterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "# eval_iter = iterator(test_instances, num_epochs=1, shuffle=False)", "\n", "\n", "cur_eitem_list", ",", "cur_eval_dict", "=", "span_eval", "(", "model", ",", "eval_iter", ",", "do_lower_case", ",", "dev_fitem_dict", ",", "\n", "device_num", ",", "show_progress", "=", "True", ",", "pred_no_answer", "=", "True", ")", "\n", "# cur_eitem_list, cur_eval_dict = span_eval(model, eval_iter, do_lower_case, test_fitem_dict,", "\n", "#                                           device_num, show_progress=True)", "\n", "\n", "cur_results_dict", "=", "dict", "(", ")", "\n", "cur_results_dict", "[", "'answer'", "]", "=", "cur_eval_dict", "\n", "cur_results_dict", "[", "'sp'", "]", "=", "dev_sp_results_dict", "\n", "# cur_results_dict['sp'] = test_sp_results_dict", "\n", "\n", "# common.save_json(cur_results_dict, f\"{tag}_qa_sp_results_{filter_value}_doctopk_5.json\")", "\n", "\n", "cur_results_dict", "[", "'p_answer'", "]", "=", "cur_eval_dict", "\n", "_", ",", "metrics", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "# _, metrics = ext_hotpot_eval.eval(cur_results_dict, test_list, verbose=False)", "\n", "\n", "logging_item", "=", "{", "\n", "'score'", ":", "metrics", ",", "\n", "}", "\n", "\n", "print", "(", "data_file", ")", "\n", "print", "(", "logging_item", ")", "\n", "\n", "", "elif", "tag", "==", "'test'", ":", "\n", "# dev_instances = span_pred_reader.read(dev_fitem_list)", "\n", "        ", "test_instances", "=", "span_pred_reader", ".", "read", "(", "test_fitem_list", ")", "\n", "# eval_iter = iterator(dev_instances, num_epochs=1, shuffle=False)", "\n", "eval_iter", "=", "iterator", "(", "test_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "# cur_eitem_list, cur_eval_dict = span_eval(model, eval_iter, do_lower_case, dev_fitem_dict,", "\n", "#                                           device_num, show_progress=True)", "\n", "cur_eitem_list", ",", "cur_eval_dict", "=", "span_eval", "(", "model", ",", "eval_iter", ",", "do_lower_case", ",", "test_fitem_dict", ",", "\n", "device_num", ",", "show_progress", "=", "True", ")", "\n", "\n", "cur_results_dict", "=", "dict", "(", ")", "\n", "cur_results_dict", "[", "'answer'", "]", "=", "cur_eval_dict", "\n", "# cur_results_dict['sp'] = dev_sp_results_dict", "\n", "cur_results_dict", "[", "'sp'", "]", "=", "test_sp_results_dict", "\n", "\n", "common", ".", "save_json", "(", "cur_results_dict", ",", "f\"{tag}_qa_sp_results.json\"", ")", "\n", "\n", "cur_results_dict", "[", "'p_answer'", "]", "=", "cur_eval_dict", "\n", "# _, metrics = ext_hotpot_eval.eval(cur_results_dict, dev_list, verbose=False)", "\n", "_", ",", "metrics", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict", ",", "test_list", ",", "verbose", "=", "False", ")", "\n", "\n", "logging_item", "=", "{", "\n", "'score'", ":", "metrics", ",", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.model_go": [[286, 544], ["torch.manual_seed", "torch.manual_seed", "int", "print", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "utils.common.load_json", "utils.common.load_json", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "utils.list_dict_data_tool.list_to_dict", "len", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "hotpot_bert_v0.BertSpan", "torch.nn.DataParallel.to", "allennlp.data.iterators.BasicIterator", "list", "print", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader.read", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "torch.cuda.is_available", "neural_modules.model_EMA.EMA", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "len", "int", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "print", "hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "random.shuffle", "data_utils.readers.paired_span_pred_reader.BertPairedSpanPredReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "any", "neural_modules.model_EMA.EMA.", "any", "hasattr", "updated_model.named_parameters", "print", "neural_modules.model_EMA.EMA.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "torch.nn.DataParallel", "allennlp.data.iterators.BasicIterator.", "hotpot_bert_v0.span_eval", "dict", "evaluation.ext_hotpot_eval.eval", "print", "print", "print", "save_tool.ScoreLogger.incorporate_results", "save_tool.ScoreLogger.logging_to_file", "torch.save", "torch.save", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_s_level_to_qa.get_qa_item_with_upstream_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.qa_models.hotpot_bert_v0.span_eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "", "def", "model_go", "(", "sent_filter_value", ",", "sent_top_k", "=", "5", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "\"bert-base-uncased\"", "\n", "lazy", "=", "False", "\n", "forward_size", "=", "32", "\n", "batch_size", "=", "32", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_rate", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "5", "\n", "eval_frequency", "=", "1000", "\n", "\n", "do_lower_case", "=", "True", "\n", "\n", "debug", "=", "False", "\n", "\n", "max_pre_context_length", "=", "320", "\n", "max_query_length", "=", "64", "\n", "doc_stride", "=", "128", "\n", "qa_num_of_layer", "=", "2", "\n", "do_ema", "=", "True", "\n", "ema_device_num", "=", "1", "\n", "# s_filter_value = 0.5", "\n", "s_filter_value", "=", "sent_filter_value", "\n", "# s_top_k = 5", "\n", "s_top_k", "=", "sent_top_k", "\n", "\n", "experiment_name", "=", "f'hotpot_v0_qa_(s_top_k:{s_top_k},s_fv:{s_filter_value},qa_layer:{qa_num_of_layer})'", "\n", "\n", "print", "(", "\"Potential total length:\"", ",", "max_pre_context_length", "+", "max_query_length", "+", "3", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "\n", "# Load Dataset.", "\n", "dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "\n", "dev_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/dev_s_level_bert_v1_results.jsonl\"", ")", "\n", "train_sentence_level_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"data/p_hotpotqa/hotpotqa_sentence_level/04-19-02:17:11_hotpot_v0_slevel_retri_(doc_top_k:2)/i(12000)|e(2)|v02_f1(0.7153646038858843)|v02_recall(0.7114645831323757)|v05_f1(0.7153646038858843)|v05_recall(0.7114645831323757)|seed(12)/train_s_level_bert_v1_results.jsonl\"", ")", "\n", "\n", "dev_fitem_dict", ",", "dev_fitem_list", ",", "dev_sp_results_dict", "=", "get_qa_item_with_upstream_sentence", "(", "dev_list", ",", "\n", "dev_sentence_level_results", ",", "\n", "is_training", "=", "False", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_context_length", "=", "max_pre_context_length", ",", "\n", "max_query_length", "=", "max_query_length", ",", "\n", "filter_value", "=", "s_filter_value", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "top_k", "=", "s_top_k", ",", "\n", "debug_mode", "=", "debug", ")", "\n", "\n", "train_fitem_dict", ",", "train_fitem_list", ",", "_", "=", "get_qa_item_with_upstream_sentence", "(", "train_list", ",", "train_sentence_level_results", ",", "\n", "is_training", "=", "True", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_context_length", "=", "max_pre_context_length", ",", "\n", "max_query_length", "=", "max_query_length", ",", "\n", "filter_value", "=", "s_filter_value", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "top_k", "=", "s_top_k", ",", "\n", "debug_mode", "=", "debug", ")", "\n", "\n", "# print(len(dev_fitem_list))", "\n", "# print(len(dev_fitem_dict))", "\n", "\n", "dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "dev_list", ",", "'_id'", ")", "\n", "\n", "if", "debug", ":", "\n", "        ", "dev_list", "=", "dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "\n", "", "est_datasize", "=", "len", "(", "train_fitem_list", ")", "\n", "\n", "span_pred_reader", "=", "BertPairedSpanPredReader", "(", "bert_tokenizer", "=", "tokenizer", ",", "lazy", "=", "lazy", ",", "\n", "example_filter", "=", "None", ")", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertSpan", "(", "bert_encoder", ",", "qa_num_of_layer", ")", "\n", "\n", "ema", "=", "None", "\n", "if", "do_ema", ":", "\n", "        ", "ema", "=", "EMA", "(", "model", ",", "model", ".", "named_parameters", "(", ")", ",", "device_num", "=", "ema_device_num", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "", "iterator", "=", "BasicIterator", "(", "batch_size", "=", "batch_size", ")", "\n", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "print", "(", "\"Total train instances:\"", ",", "len", "(", "train_fitem_list", ")", ")", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_rate", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "dev_instances", "=", "span_pred_reader", ".", "read", "(", "dev_fitem_list", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "# # # Create Log File", "\n", "file_path_prefix", "=", "None", "\n", "if", "not", "debug", ":", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "\n", "print", "(", "\"Resampling:\"", ")", "\n", "train_fitem_dict", ",", "train_fitem_list", ",", "_", "=", "get_qa_item_with_upstream_sentence", "(", "train_list", ",", "\n", "train_sentence_level_results", ",", "\n", "is_training", "=", "True", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "max_context_length", "=", "max_pre_context_length", ",", "\n", "max_query_length", "=", "max_query_length", ",", "\n", "filter_value", "=", "s_filter_value", ",", "\n", "doc_stride", "=", "doc_stride", ",", "\n", "top_k", "=", "s_top_k", ",", "\n", "debug_mode", "=", "debug", ")", "\n", "\n", "random", ".", "shuffle", "(", "train_fitem_list", ")", "\n", "train_instances", "=", "span_pred_reader", ".", "read", "(", "train_fitem_list", ")", "\n", "train_iter", "=", "iterator", "(", "train_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ",", "desc", "=", "\"Batch Loop\"", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "allen_util", ".", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "gt_span", "=", "batch", "[", "'gt_span'", "]", "\n", "\n", "loss", "=", "model", "(", "mode", "=", "BertSpan", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "input_ids", "=", "paired_sequence", ",", "\n", "token_type_ids", "=", "paired_segments_ids", ",", "\n", "attention_mask", "=", "att_mask", ",", "\n", "gt_span", "=", "gt_span", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "if", "ema", "is", "not", "None", "and", "do_ema", ":", "\n", "                    ", "updated_model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "ema", "(", "updated_model", ".", "named_parameters", "(", ")", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "# print(\"Non-EMA EVAL:\")", "\n", "# eval_iter = iterator(dev_instances, num_epochs=1, shuffle=False)", "\n", "# cur_eitem_list, cur_eval_dict = span_eval(model, eval_iter, do_lower_case, dev_fitem_dict,", "\n", "#                                           device_num)", "\n", "# cur_results_dict = dict()", "\n", "# cur_results_dict['p_answer'] = cur_eval_dict", "\n", "# cur_results_dict['sp'] = dev_sp_results_dict", "\n", "#", "\n", "# _, metrics = ext_hotpot_eval.eval(cur_results_dict, dev_list, verbose=False)", "\n", "# # print(metrics)", "\n", "#", "\n", "# logging_item = {", "\n", "#     'score': metrics,", "\n", "# }", "\n", "#", "\n", "# joint_f1 = metrics['joint_f1']", "\n", "# joint_em = metrics['joint_em']", "\n", "#", "\n", "# print(logging_item)", "\n", "#", "\n", "# if not debug:", "\n", "#     save_file_name = f'i({update_step})|e({epoch_i})' \\", "\n", "#         f'|j_f1({joint_f1})|j_em({joint_em})|seed({seed})'", "\n", "#", "\n", "#     # print(save_file_name)", "\n", "#     logging_agent.incorporate_results({}, save_file_name, logging_item)", "\n", "#     logging_agent.logging_to_file(Path(file_path_prefix) / \"log.json\")", "\n", "#", "\n", "#     model_to_save = model.module if hasattr(model, 'module') else model", "\n", "#     output_model_file = Path(file_path_prefix) / save_file_name", "\n", "#     torch.save(model_to_save.state_dict(), str(output_model_file))", "\n", "\n", "                    ", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "                        ", "print", "(", "\"EMA EVAL\"", ")", "\n", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "ema_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "ema_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "dev_iter", "=", "iterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eitem_list", ",", "cur_eval_dict", "=", "span_eval", "(", "ema_model", ",", "dev_iter", ",", "do_lower_case", ",", "dev_fitem_dict", ",", "\n", "ema_device_num", ",", "show_progress", "=", "False", ")", "\n", "cur_results_dict", "=", "dict", "(", ")", "\n", "cur_results_dict", "[", "'p_answer'", "]", "=", "cur_eval_dict", "\n", "cur_results_dict", "[", "'sp'", "]", "=", "dev_sp_results_dict", "\n", "\n", "_", ",", "metrics", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "print", "(", "metrics", ")", "\n", "print", "(", "\"---------------\"", "*", "3", ")", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'score'", ":", "metrics", ",", "\n", "}", "\n", "\n", "joint_f1", "=", "metrics", "[", "'joint_f1'", "]", "\n", "joint_em", "=", "metrics", "[", "'joint_em'", "]", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "if", "not", "debug", ":", "\n", "                            ", "save_file_name", "=", "f'ema_i({update_step})|e({epoch_i})'", "f'|j_f1({joint_f1})|j_em({joint_em})|seed({seed})'", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__": [[57, 85], ["object.__init__", "span_util.ParagraphSpan._span_lengths.append"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__init__"], ["    ", "def", "__init__", "(", "self", ",", "span_list", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "total_length", ":", "int", "=", "None", ",", "\n", "is_consecutive", "=", "True", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        :param span_list:   unwrapped raw span list\n        :param total_length:    total length of the paragraph for data validation.\n        :param is_consecutive:  whether to check if the span if consecutive.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Check whether it is valid span list:", "\n", "pre_end_ind", "=", "0", "\n", "cur_length", "=", "0", "\n", "self", ".", "_span_lengths", "=", "[", "]", "\n", "for", "start_ind", ",", "end_ind", "in", "span_list", ":", "\n", "            ", "span_length", "=", "end_ind", "-", "start_ind", "\n", "assert", "span_length", ">=", "0", "\n", "\n", "if", "is_consecutive", ":", "\n", "                ", "assert", "start_ind", "==", "pre_end_ind", "\n", "\n", "", "self", ".", "_span_lengths", ".", "append", "(", "span_length", ")", "\n", "cur_length", "+=", "span_length", "\n", "pre_end_ind", "=", "end_ind", "\n", "\n", "", "if", "total_length", "is", "not", "None", ":", "\n", "            ", "assert", "cur_length", "==", "total_length", "\n", "\n", "", "self", ".", "span_list", "=", "span_list", "\n", "self", ".", "_total_length", "=", "cur_length", "\n", "# self._span_lengths =", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__iter__": [[87, 89], ["span_util.ParagraphSpan.span_list.__iter__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__iter__"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "span_list", ".", "__iter__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__repr__": [[90, 92], ["span_util.ParagraphSpan.span_list.__repr__"], "methods", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.__repr__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "span_list", ".", "__repr__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span": [[93, 95], ["len"], "methods", ["None"], ["", "def", "number_of_span", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "span_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.span_lengths": [[96, 99], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "span_lengths", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_span_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.total_length": [[100, 103], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "total_length", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_total_length", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.span_select": [[26, 54], ["input_p.size", "range", "flint.torch_util.pack_list_sequence", "input_p.new_tensor().long", "len", "output_seq_list.append", "output_seq_l.append", "cur_selected_span.size", "input_p.new_tensor"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence"], ["def", "span_select", "(", "input_p", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "max_span_length", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    :param input_p:             This is the input tensor [B, T, ...]\n    :param batch_span:          This is a list of span.     The size of the list should match the batch_size\n    :param max_span_length:     An optional argument that specific the max_padding value\n                                Start is inclusive, and end is exclusive.\n                                Remember this is just one span object indicating a start and end, not a list of span\n    :return:\n    ouput_seqs:         This is a span of text selected by the span from input_seq.\n    output_seqs_l:      This is the length of the selected span\n    \"\"\"", "\n", "batch_size", "=", "input_p", ".", "size", "(", "0", ")", "\n", "assert", "batch_size", "==", "len", "(", "batch_span", ")", "\n", "\n", "output_seq_list", "=", "[", "]", "\n", "output_seq_l", "=", "[", "]", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "cur_p", ":", "torch", ".", "Tensor", "=", "input_p", "[", "b_i", "]", "# [T, *]", "\n", "cur_span_index", "=", "batch_span", "[", "b_i", "]", "\n", "cur_selected_span", "=", "cur_p", "[", "cur_span_index", "[", "0", "]", ":", "cur_span_index", "[", "1", "]", "]", "\n", "output_seq_list", ".", "append", "(", "cur_selected_span", ")", "\n", "output_seq_l", ".", "append", "(", "cur_selected_span", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "output_sents", "=", "torch_util", ".", "pack_list_sequence", "(", "output_seq_list", ",", "output_seq_l", ",", "max_span_length", ")", "\n", "output_l", "=", "input_p", ".", "new_tensor", "(", "output_seq_l", ")", ".", "long", "(", ")", "\n", "\n", "return", "output_sents", ",", "output_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.cut_paragraph_to_sentence": [[105, 185], ["input_p.size", "input_p.size", "range", "flint.torch_util.pack_list_sequence", "p_l.data.new_tensor", "p_l.data.new_tensor", "len", "p_l.size", "batch_modified_span.append", "batch_modified_l.append", "torch_util.pack_list_sequence.size", "sum", "min", "retrieved_tensor.size", "sequence_tensors_list.append", "modified_span_list.append", "sequence_span_lengths.append", "span_util.ParagraphSpan", "cur_p.size", "min", "span.number_of_span"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span"], ["", "", "def", "cut_paragraph_to_sentence", "(", "input_p", ":", "torch", ".", "Tensor", ",", "p_l", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ",", "\n", "max_sentence_length", ":", "int", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "List", "[", "ParagraphSpan", "]", "]", ":", "\n", "    ", "\"\"\"\n    :param input_p: The input paragraph Tensor. [B (batch_size: number of paragraph), T, D]\n    :param p_l:     The length of paragraph Tensor. [B] This is for checking the length\n    :param batch_span:    A List of The span of the paragraph for each example in the batch.\n    :param max_sentence_length:     Use this value to truncate the max length of each sentence to avoid out-of-memory\n\n            Note: If you set max_sentence_length, then we will not be able to map the token back to the original\n                Paragraph. The method will be in-revertable. Maybe fix this in the future.\n    :return:\n        output_sents:        The new packed sentence vector.\n                            Shape: [B (new batch size: number of spans in the original batch), T, D]\n\n        output_l:            The lengths of the outputs.\n                            Shape: [B]\n\n        batched_l:           This is the new paragraph length after sentence truncate for original paragraph format.\n                            Shape: [B]\n\n        batch_modified_span: This is the span after sentence truncate.\n\n        if max_sentence_length is None: the batched_l will remain the same, and valid batch span will also be the same.\n    \"\"\"", "\n", "batch_size", "=", "input_p", ".", "size", "(", "0", ")", "\n", "dim", "=", "input_p", ".", "size", "(", "-", "1", ")", "\n", "assert", "batch_size", "==", "len", "(", "batch_span", ")", "\n", "assert", "batch_size", "==", "p_l", ".", "size", "(", "0", ")", "\n", "\n", "sequence_tensors_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "# The list to save all the output sequence tensors.", "\n", "sequence_span_lengths", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "batch_modified_span", ":", "List", "[", "ParagraphSpan", "]", "=", "[", "]", "\n", "batch_modified_l", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "cur_p", ":", "torch", ".", "Tensor", "=", "input_p", "[", "b_i", "]", "# [T, D]", "\n", "cur_span", ":", "ParagraphSpan", "=", "batch_span", "[", "b_i", "]", "\n", "# assert p_l[b_i] == cur_span.total_length    # Checking that current length is consistent with span total length", "\n", "# remove this because the input paragraph might already be truncated and will not be consistent with the span", "\n", "\n", "modified_span_list", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "m_start", ",", "m_end", "=", "0", ",", "0", "\n", "m_total_l", "=", "0", "\n", "for", "o_start", ",", "o_end", "in", "cur_span", ":", "\n", "            ", "if", "o_start", ">=", "cur_p", ".", "size", "(", "0", ")", ":", "\n", "# If the current span is already surpass the boundary, then just ignore this span", "\n", "# This line just ignore all the span that surpass the boundary due to paragraph truncate.", "\n", "                ", "continue", "\n", "\n", "", "cur_sent_length", "=", "o_end", "-", "o_start", "\n", "if", "max_sentence_length", "is", "not", "None", ":", "\n", "                ", "cur_sent_length", "=", "min", "(", "max_sentence_length", ",", "cur_sent_length", ")", "\n", "\n", "# retrieve the spanned tensor", "\n", "", "start_point", "=", "o_start", "\n", "end_point", "=", "min", "(", "p_l", "[", "b_i", "]", ",", "o_start", "+", "cur_sent_length", ")", "\n", "# input span might be greater because truncating paragraph", "\n", "retrieved_tensor", "=", "cur_p", "[", "start_point", ":", "end_point", "]", "\n", "cur_sent_length", "=", "retrieved_tensor", ".", "size", "(", "0", ")", "# there might be some cut in the end of the paragraph", "\n", "sequence_tensors_list", ".", "append", "(", "retrieved_tensor", ")", "\n", "\n", "# calculate the new start and end of the span", "\n", "m_end", "=", "m_start", "+", "cur_sent_length", "\n", "modified_span_list", ".", "append", "(", "(", "m_start", ",", "m_end", ")", ")", "\n", "m_total_l", "+=", "cur_sent_length", "\n", "sequence_span_lengths", ".", "append", "(", "cur_sent_length", ")", "\n", "m_start", "=", "m_end", "# reset m_start", "\n", "\n", "# sequence_tensors_list.append(cur_p[o_start:o_start + cur_sent_length])", "\n", "\n", "", "batch_modified_span", ".", "append", "(", "ParagraphSpan", "(", "modified_span_list", ")", ")", "\n", "batch_modified_l", ".", "append", "(", "m_total_l", ")", "\n", "\n", "", "output_sents", "=", "torch_util", ".", "pack_list_sequence", "(", "sequence_tensors_list", ",", "sequence_span_lengths", ")", "\n", "output_l", "=", "p_l", ".", "data", ".", "new_tensor", "(", "sequence_span_lengths", ")", "\n", "batched_l", "=", "p_l", ".", "data", ".", "new_tensor", "(", "batch_modified_l", ")", "\n", "assert", "output_sents", ".", "size", "(", "0", ")", "==", "sum", "(", "[", "span", ".", "number_of_span", "(", ")", "for", "span", "in", "batch_modified_span", "]", ")", "\n", "\n", "return", "output_sents", ",", "output_l", ",", "batched_l", ",", "batch_modified_span", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.merge_sentence_to_paragraph": [[187, 236], ["len", "range", "flint.torch_util.pack_list_sequence", "span_l.data.new_tensor", "torch.cat", "batch_p_l_list.append", "batch_ptensor_list.append", "torch.equal", "paragraph_tensor_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence"], ["", "def", "merge_sentence_to_paragraph", "(", "span_inputs", ":", "torch", ".", "Tensor", ",", "span_l", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ",", "\n", "expected_l", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    This is the reverse method of the `cut_paragraph_to_sentence` method.\n    :param span_inputs:     The input span batched sequence.\n                            Shape: [B (batch size: number of spans in the original batch), T, D]\n\n    :param span_l:          The length of span batched inputs.\n                            Shape: [B]\n    :param batch_span:      The span object.\n\n    :param expected_l:      The expected paragraph length for the batch. This should be the `batched_l` output of `cut_paragraph_to_sentence` method\n    :return:\n    \"\"\"", "\n", "batch_size", "=", "len", "(", "batch_span", ")", "\n", "# dim = span_inputs.size(-1)", "\n", "# assert batch_size == len(batch_span)", "\n", "# assert batch_size == span_l.size(0)", "\n", "\n", "batch_ptensor_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "batch_p_l_list", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "tensor_index", "=", "0", "# one span per tensor", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "paragraph_tensor_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "paragraph_total_length", "=", "0", "\n", "paragraph_span", "=", "batch_span", "[", "b_i", "]", "\n", "for", "m_start", ",", "m_end", "in", "paragraph_span", ":", "\n", "            ", "cur_length", "=", "m_end", "-", "m_start", "\n", "cur_tensor", "=", "span_inputs", "[", "tensor_index", "]", "# [T, D]", "\n", "assert", "cur_length", "==", "span_l", "[", "tensor_index", "]", "\n", "tensor_index", "+=", "1", "# increase the index", "\n", "valid_tensor", "=", "cur_tensor", "[", ":", "cur_length", "]", "\n", "paragraph_tensor_list", ".", "append", "(", "valid_tensor", ")", "\n", "paragraph_total_length", "+=", "cur_length", "\n", "\n", "", "paragraph_tensor", "=", "torch", ".", "cat", "(", "paragraph_tensor_list", ",", "dim", "=", "0", ")", "\n", "\n", "batch_p_l_list", ".", "append", "(", "paragraph_total_length", ")", "\n", "batch_ptensor_list", ".", "append", "(", "paragraph_tensor", ")", "\n", "\n", "", "output_batched_paragraph_tensor", "=", "torch_util", ".", "pack_list_sequence", "(", "batch_ptensor_list", ",", "batch_p_l_list", ")", "\n", "output_batched_l", "=", "span_l", ".", "data", ".", "new_tensor", "(", "batch_p_l_list", ")", "\n", "\n", "if", "expected_l", "is", "not", "None", ":", "\n", "        ", "assert", "torch", ".", "equal", "(", "expected_l", ",", "output_batched_l", ")", "\n", "\n", "", "return", "output_batched_paragraph_tensor", ",", "output_batched_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.replicate_query_for_span_align": [[238, 287], ["input_tensor.size", "input_tensor.size", "range", "len", "input_l.size", "batch_span[].number_of_span", "range", "span_util.ParagraphSpan", "query_span_list.append", "org_paragraph_lengths.append", "torch.stack", "torch.stack", "input_l.data.new_tensor", "rep_query_tensors_list.append", "rep_query_span_lengths.append", "batch_query_span_list.append", "int"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span"], ["", "def", "replicate_query_for_span_align", "(", "\n", "input_tensor", ":", "torch", ".", "Tensor", ",", "input_l", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", ",", "List", "[", "ParagraphSpan", "]", "]", ":", "\n", "    ", "\"\"\"\n    The method replicate the query to align to each span in the paragraph.\n\n    :param input_tensor:    The query input.\n                            Shape: [B, T, D]\n    :param input_l:         The length of the input.\n                            Shape: [B]\n    :param batch_span:      The span of the paragraph.\n\n    :return:\n    output_tensor:      The replicated query tensor according to the input span.\n                        Shape: [B (query replicated), T, D]\n    output_l:           The length of the output_tensor\n                        Shape: [B]\n\n    query_batch_span    This is the output list of query span for later compositional operation.\n    \"\"\"", "\n", "\n", "batch_size", "=", "input_tensor", ".", "size", "(", "0", ")", "\n", "dim", "=", "input_tensor", ".", "size", "(", "-", "1", ")", "\n", "assert", "batch_size", "==", "len", "(", "batch_span", ")", "\n", "assert", "batch_size", "==", "input_l", ".", "size", "(", "0", ")", "\n", "query_span_list", "=", "[", "]", "\n", "\n", "rep_query_tensors_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "# The list to save all the output sequence tensors.", "\n", "rep_query_span_lengths", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "org_paragraph_lengths", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "cur_number_of_span", "=", "batch_span", "[", "b_i", "]", ".", "number_of_span", "(", ")", "\n", "query_span_start", "=", "0", "\n", "batch_query_span_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "cur_number_of_span", ")", ":", "\n", "            ", "rep_query_tensors_list", ".", "append", "(", "input_tensor", "[", "b_i", "]", ")", "\n", "rep_query_span_lengths", ".", "append", "(", "input_l", "[", "b_i", "]", ")", "\n", "\n", "query_span_end", "=", "query_span_start", "+", "int", "(", "input_l", "[", "b_i", "]", ")", "\n", "batch_query_span_list", ".", "append", "(", "(", "query_span_start", ",", "query_span_end", ")", ")", "\n", "query_span_start", "=", "query_span_end", "\n", "\n", "", "cur_span_obj", "=", "ParagraphSpan", "(", "batch_query_span_list", ")", "\n", "query_span_list", ".", "append", "(", "cur_span_obj", ")", "\n", "org_paragraph_lengths", ".", "append", "(", "cur_span_obj", ".", "total_length", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "rep_query_tensors_list", ",", "dim", "=", "0", ")", ",", "torch", ".", "stack", "(", "rep_query_span_lengths", ")", ",", "input_l", ".", "data", ".", "new_tensor", "(", "org_paragraph_lengths", ")", ",", "query_span_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.quick_truncate": [[289, 316], ["span_util.cut_paragraph_to_sentence", "span_util.merge_sentence_to_paragraph", "span_util.replicate_query_for_span_align", "span_util.merge_sentence_to_paragraph", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.cut_paragraph_to_sentence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.merge_sentence_to_paragraph", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.replicate_query_for_span_align", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.merge_sentence_to_paragraph"], ["", "def", "quick_truncate", "(", "input_tensor", ":", "torch", ".", "Tensor", ",", "input_l", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ",", "\n", "max_sentence_length", ",", "mode", ":", "str", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Remember that the span here should be the original span input rather than the modified span.\n    :param input_tensor:\n    :param input_l:\n    :param batch_span:\n    :param max_sentence_length:\n    :param mode:\n    :return:\n    \"\"\"", "\n", "if", "mode", "==", "'paragraph'", ":", "\n", "        ", "s1_span_output", ",", "s1_span_output_l", ",", "s1_modified_l", ",", "s1_modified_span_obj", "=", "cut_paragraph_to_sentence", "(", "\n", "input_tensor", ",", "\n", "input_l", ",", "batch_span", ",", "max_sentence_length", "=", "max_sentence_length", ")", "\n", "return", "merge_sentence_to_paragraph", "(", "s1_span_output", ",", "s1_span_output_l", ",", "s1_modified_span_obj", ",", "s1_modified_l", ")", "\n", "\n", "", "elif", "mode", "==", "'query'", ":", "\n", "        ", "s2_span_output", ",", "s2_span_output_l", ",", "s2_modified_l", ",", "s2_modified_span_obj", "=", "replicate_query_for_span_align", "(", "\n", "input_tensor", ",", "\n", "input_l", ",", "\n", "batch_span", ")", "\n", "\n", "return", "merge_sentence_to_paragraph", "(", "s2_span_output", ",", "s2_span_output_l", ",", "s2_modified_span_obj", ",", "s2_modified_l", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.convert_input_weight_list_to_tensor": [[318, 333], ["enumerate", "flint.torch_util.pack_list_sequence", "torch.from_numpy().to", "len", "len", "weight_tensors_list.append", "weight_lengths_list.append", "torch.from_numpy().to", "cur_span.number_of_span", "torch.from_numpy", "cur_span.number_of_span", "numpy.asarray", "torch.from_numpy", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span"], ["", "", "def", "convert_input_weight_list_to_tensor", "(", "weight_list", ":", "List", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ",", "device", ":", "torch", ".", "device", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "assert", "len", "(", "weight_list", ")", "==", "len", "(", "batch_span", ")", "\n", "weight_tensors_list", "=", "[", "]", "\n", "weight_lengths_list", "=", "[", "]", "\n", "for", "i", ",", "weights", "in", "enumerate", "(", "weight_list", ")", ":", "\n", "        ", "cur_span", "=", "batch_span", "[", "i", "]", "\n", "cur_weights", "=", "weights", "[", ":", "cur_span", ".", "number_of_span", "(", ")", "]", "\n", "weight_tensors_list", ".", "append", "(", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "cur_weights", ",", "dtype", "=", "np", ".", "float32", ")", ")", ".", "to", "(", "device", ")", ")", "\n", "weight_lengths_list", ".", "append", "(", "cur_span", ".", "number_of_span", "(", ")", ")", "\n", "\n", "", "output_weight", "=", "torch_util", ".", "pack_list_sequence", "(", "weight_tensors_list", ",", "weight_lengths_list", ")", "\n", "output_l", "=", "torch", ".", "from_numpy", "(", "np", ".", "asarray", "(", "weight_lengths_list", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "return", "output_weight", ",", "output_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.concate_rep_query": [[335, 362], ["len", "range", "flint.torch_util.pack_list_sequence", "input_l.data.new_tensor", "torch.cat", "batch_p_l_list.append", "batch_ptensor_list.append", "paragraph_tensor_list.append", "valid_tensor.size"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence"], ["", "def", "concate_rep_query", "(", "input_tensor", ":", "torch", ".", "Tensor", ",", "input_l", ":", "torch", ".", "Tensor", ",", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "batch_size", "=", "len", "(", "batch_span", ")", "\n", "batch_ptensor_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "batch_p_l_list", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "tensor_index", "=", "0", "# one span per tensor", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "paragraph_tensor_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "paragraph_total_length", "=", "0", "\n", "paragraph_span", "=", "batch_span", "[", "b_i", "]", "\n", "for", "_", "in", "paragraph_span", ":", "\n", "            ", "cur_tensor", "=", "input_tensor", "[", "tensor_index", "]", "# [T, D]", "\n", "valid_tensor", "=", "cur_tensor", "[", ":", "input_l", "[", "tensor_index", "]", "]", "\n", "tensor_index", "+=", "1", "# increase the index", "\n", "paragraph_tensor_list", ".", "append", "(", "valid_tensor", ")", "\n", "paragraph_total_length", "+=", "valid_tensor", ".", "size", "(", "0", ")", "\n", "\n", "", "paragraph_tensor", "=", "torch", ".", "cat", "(", "paragraph_tensor_list", ",", "dim", "=", "0", ")", "\n", "\n", "batch_p_l_list", ".", "append", "(", "paragraph_total_length", ")", "\n", "batch_ptensor_list", ".", "append", "(", "paragraph_tensor", ")", "\n", "\n", "", "output_batched_paragraph_tensor", "=", "torch_util", ".", "pack_list_sequence", "(", "batch_ptensor_list", ",", "batch_p_l_list", ")", "\n", "output_batched_l", "=", "input_l", ".", "data", ".", "new_tensor", "(", "batch_p_l_list", ")", "\n", "\n", "return", "output_batched_paragraph_tensor", ",", "output_batched_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.weighted_max_pooling_over_span": [[364, 408], ["len", "input_tensor.size", "range", "flint.torch_util.pack_list_sequence", "input_l.data.new_tensor", "len", "input_l.size", "cur_span.number_of_span", "torch.stack", "batched_span_pooling_results.append", "batched_span_pooling_l.append", "retrieved_tensor.size", "retrieved_tensor.max", "cur_pooling_results_list.append"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.span_util.ParagraphSpan.number_of_span"], ["", "def", "weighted_max_pooling_over_span", "(", "input_tensor", ":", "torch", ".", "Tensor", ",", "input_l", ":", "torch", ".", "Tensor", ",", "\n", "batch_span", ":", "List", "[", "ParagraphSpan", "]", ",", "weights", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    :param input_tensor: The input tensor is with original shape and format. [B, T, D]\n    :param input_l:         [B]\n    :param batch_span:   The list of span object to represent the span of the paragraph.\n    :param weights:      The optional weight applied after pooling.\n    :return:\n    cspan_output:   The output of tensor after pooling over span.\n                    Shape: [B, T, D]    Batch size is the same as before but T is the number of span for each paragraph.\n    cspan_l:        The number of span for each paragraph.\n    \"\"\"", "\n", "batch_size", "=", "len", "(", "batch_span", ")", "\n", "dim", "=", "input_tensor", ".", "size", "(", "-", "1", ")", "\n", "assert", "batch_size", "==", "len", "(", "batch_span", ")", "\n", "assert", "batch_size", "==", "input_l", ".", "size", "(", "0", ")", "\n", "batched_span_pooling_results", "=", "[", "]", "\n", "batched_span_pooling_l", "=", "[", "]", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "cur_p", ":", "torch", ".", "Tensor", "=", "input_tensor", "[", "b_i", "]", "# [T, D]", "\n", "cur_span", ":", "ParagraphSpan", "=", "batch_span", "[", "b_i", "]", "\n", "cur_num_of_span", "=", "cur_span", ".", "number_of_span", "(", ")", "\n", "tracting_length", "=", "0", "\n", "cur_pooling_results_list", "=", "[", "]", "\n", "for", "m_start", ",", "m_end", "in", "cur_span", ":", "\n", "            ", "retrieved_tensor", "=", "cur_p", "[", "m_start", ":", "m_end", "]", "# [T, D]", "\n", "tracting_length", "+=", "retrieved_tensor", ".", "size", "(", "0", ")", "\n", "pooling_tensor", ",", "_", "=", "retrieved_tensor", ".", "max", "(", "dim", "=", "0", ")", "# [D]", "\n", "cur_pooling_results_list", ".", "append", "(", "pooling_tensor", ")", "\n", "\n", "", "pooling_result", "=", "torch", ".", "stack", "(", "cur_pooling_results_list", ",", "dim", "=", "0", ")", "# [T, D]", "\n", "batched_span_pooling_results", ".", "append", "(", "pooling_result", ")", "\n", "batched_span_pooling_l", ".", "append", "(", "cur_num_of_span", ")", "\n", "assert", "tracting_length", "==", "input_l", "[", "b_i", "]", "\n", "\n", "", "cspan_output", "=", "torch_util", ".", "pack_list_sequence", "(", "batched_span_pooling_results", ",", "batched_span_pooling_l", ")", "\n", "cspan_l", "=", "input_l", ".", "data", ".", "new_tensor", "(", "batched_span_pooling_l", ")", "\n", "\n", "if", "weights", "is", "not", "None", ":", "\n", "        ", "cspan_output", "=", "weights", "*", "cspan_output", "\n", "\n", "", "return", "cspan_output", ",", "cspan_l", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask": [[11, 15], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask"], "function", ["None"], ["def", "get_length_and_mask", "(", "seq", ")", ":", "\n", "    ", "len_mask", "=", "(", "seq", "!=", "0", ")", ".", "long", "(", ")", "\n", "len_t", "=", "get_lengths_from_binary_sequence_mask", "(", "len_mask", ")", "\n", "return", "len_mask", ",", "len_t", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.length_truncate": [[17, 33], ["torch_util.length_truncate._truncate"], "function", ["None"], ["", "def", "length_truncate", "(", "seq", ",", "max_l", ",", "is_elmo", "=", "False", ")", ":", "\n", "    ", "def", "_truncate", "(", "seq", ")", ":", "\n", "        ", "if", "seq", ".", "size", "(", "1", ")", ">", "max_l", ":", "\n", "            ", "return", "seq", "[", ":", ",", ":", "max_l", ",", "...", "]", "\n", "", "else", ":", "\n", "            ", "return", "seq", "\n", "\n", "", "", "if", "not", "is_elmo", ":", "\n", "        ", "return", "_truncate", "(", "seq", ")", "\n", "", "else", ":", "\n", "        ", "s1_elmo_embd", "=", "dict", "(", ")", "\n", "s1_elmo_embd", "[", "'mask'", "]", "=", "_truncate", "(", "seq", "[", "'mask'", "]", ")", "\n", "s1_elmo_embd", "[", "'elmo_representations'", "]", "=", "[", "]", "\n", "for", "e_rep", "in", "seq", "[", "'elmo_representations'", "]", ":", "\n", "            ", "s1_elmo_embd", "[", "'elmo_representations'", "]", ".", "append", "(", "_truncate", "(", "e_rep", ")", ")", "\n", "", "return", "s1_elmo_embd", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pad_1d": [[35, 49], ["seq.size", "seq.new_zeros", "torch.cat", "torch.cat", "torch.cat", "seq.size"], "function", ["None"], ["", "", "def", "pad_1d", "(", "seq", ",", "pad_l", ")", ":", "\n", "    ", "\"\"\"\n    The seq is a sequence having shape [T, ..]. Note: The seq contains only one instance. This is not batched.\n    \n    :param seq:  Input sequence with shape [T, ...]\n    :param pad_l: The required pad_length.\n    :return:  Output sequence will have shape [Pad_L, ...]\n    \"\"\"", "\n", "l", "=", "seq", ".", "size", "(", "0", ")", "\n", "if", "l", ">=", "pad_l", ":", "\n", "        ", "return", "seq", "[", ":", "pad_l", ",", "]", "# Truncate the length if the length is bigger than required padded_length.", "\n", "", "else", ":", "\n", "        ", "pad_seq", "=", "seq", ".", "new_zeros", "(", "pad_l", "-", "l", ",", "*", "seq", ".", "size", "(", ")", "[", "1", ":", "]", ")", "# Requires_grad is False", "\n", "return", "torch", ".", "cat", "(", "[", "seq", ",", "pad_seq", "]", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_state_shape": [[51, 67], ["None"], "function", ["None"], ["", "", "def", "get_state_shape", "(", "rnn", ":", "nn", ".", "RNN", ",", "batch_size", ",", "bidirectional", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Return the state shape of a given RNN. This is helpful when you want to create a init state for RNN.\n\n    Example:\n    c0 = h0 = Variable(src_seq_p.data.new(*get_state_shape([your rnn], 3, bidirectional)).zero_())\n    \n    :param rnn: nn.LSTM, nn.GRU or subclass of nn.RNN\n    :param batch_size:  \n    :param bidirectional:  \n    :return: \n    \"\"\"", "\n", "if", "bidirectional", ":", "\n", "        ", "return", "rnn", ".", "num_layers", "*", "2", ",", "batch_size", ",", "rnn", ".", "hidden_size", "\n", "", "else", ":", "\n", "        ", "return", "rnn", ".", "num_layers", ",", "batch_size", ",", "rnn", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_list_sequence": [[69, 87], ["len", "range", "max", "batch_list.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "list", "torch_util.pad_1d"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pad_1d"], ["", "", "def", "pack_list_sequence", "(", "inputs", ",", "l", ",", "max_l", "=", "None", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Pack a batch of Tensor into one Tensor with max_length.\n    :param inputs: \n    :param l: \n    :param max_l: The max_length of the packed sequence.\n    :param batch_first: \n    :return: \n    \"\"\"", "\n", "batch_list", "=", "[", "]", "\n", "max_l", "=", "max", "(", "list", "(", "l", ")", ")", "if", "not", "max_l", "else", "max_l", "\n", "batch_size", "=", "len", "(", "inputs", ")", "\n", "\n", "for", "b_i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "batch_list", ".", "append", "(", "pad_1d", "(", "inputs", "[", "b_i", "]", ",", "max_l", ")", ")", "\n", "", "pack_batch_list", "=", "torch", ".", "stack", "(", "batch_list", ",", "dim", "=", "1", ")", "if", "not", "batch_first", "else", "torch", ".", "stack", "(", "batch_list", ",", "dim", "=", "0", ")", "\n", "return", "pack_batch_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_for_rnn_seq": [[89, 156], ["lengths.sort", "reversed", "numpy.zeros", "enumerate", "list", "torch.cat", "torch.cat", "torch.cat", "torch.utils.rnn.pack_padded_sequence", "lengths.sort", "reversed", "numpy.zeros", "tuple", "enumerate", "list", "torch.stack", "torch.stack", "torch.stack", "torch.utils.rnn.pack_padded_sequence", "tuple", "list", "lengths.size", "s_inputs_list.append", "lengths_list.append", "list", "lengths.size", "s_inputs_list.append", "lengths_list.append", "zip", "len", "inputs[].unsqueeze", "isinstance", "state_list.append", "torch.cat", "torch.cat", "torch.cat", "state[].unsqueeze"], "function", ["None"], ["", "def", "pack_for_rnn_seq", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ",", "states", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param states: [rnn.num_layers, batch_size, rnn.hidden_size]\n    :param inputs: Shape of the input should be [B, T, D] if batch_first else [T, B, D].\n    :param lengths:  [B]\n    :param batch_first: \n    :return: \n    \"\"\"", "\n", "if", "not", "batch_first", ":", "\n", "        ", "_", ",", "sorted_indices", "=", "lengths", ".", "sort", "(", ")", "\n", "'''\n            Reverse to decreasing order\n        '''", "\n", "r_index", "=", "reversed", "(", "list", "(", "sorted_indices", ")", ")", "\n", "\n", "s_inputs_list", "=", "[", "]", "\n", "lengths_list", "=", "[", "]", "\n", "reverse_indices", "=", "np", ".", "zeros", "(", "lengths", ".", "size", "(", "0", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "r_index", ")", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "inputs", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "lengths_list", ".", "append", "(", "lengths", "[", "i", "]", ")", "\n", "reverse_indices", "[", "i", "]", "=", "j", "\n", "\n", "", "reverse_indices", "=", "list", "(", "reverse_indices", ")", "\n", "\n", "s_inputs", "=", "torch", ".", "cat", "(", "s_inputs_list", ",", "1", ")", "\n", "packed_seq", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "s_inputs", ",", "lengths_list", ")", "\n", "\n", "return", "packed_seq", ",", "reverse_indices", "\n", "\n", "", "else", ":", "\n", "        ", "_", ",", "sorted_indices", "=", "lengths", ".", "sort", "(", ")", "\n", "'''\n            Reverse to decreasing order\n        '''", "\n", "r_index", "=", "reversed", "(", "list", "(", "sorted_indices", ")", ")", "\n", "\n", "s_inputs_list", "=", "[", "]", "\n", "lengths_list", "=", "[", "]", "\n", "reverse_indices", "=", "np", ".", "zeros", "(", "lengths", ".", "size", "(", "0", ")", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "if", "states", "is", "None", ":", "\n", "            ", "states", "=", "(", ")", "\n", "", "elif", "not", "isinstance", "(", "states", ",", "tuple", ")", ":", "\n", "            ", "states", "=", "(", "states", ",", ")", "# rnn.num_layers, batch_size, rnn.hidden_size", "\n", "\n", "", "states_lists", "=", "tuple", "(", "[", "]", "for", "_", "in", "states", ")", "\n", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "r_index", ")", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "inputs", "[", "i", ",", ":", ",", ":", "]", ")", "\n", "lengths_list", ".", "append", "(", "lengths", "[", "i", "]", ")", "\n", "reverse_indices", "[", "i", "]", "=", "j", "\n", "\n", "for", "state_list", ",", "state", "in", "zip", "(", "states_lists", ",", "states", ")", ":", "\n", "                ", "state_list", ".", "append", "(", "state", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "", "reverse_indices", "=", "list", "(", "reverse_indices", ")", "\n", "\n", "s_inputs", "=", "torch", ".", "stack", "(", "s_inputs_list", ",", "dim", "=", "0", ")", "\n", "packed_seq", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "s_inputs", ",", "lengths_list", ",", "batch_first", "=", "batch_first", ")", "\n", "\n", "r_states", "=", "tuple", "(", "torch", ".", "cat", "(", "state_list", ",", "dim", "=", "1", ")", "for", "state_list", "in", "states_lists", ")", "\n", "if", "len", "(", "r_states", ")", "==", "1", ":", "\n", "            ", "r_states", "=", "r_states", "[", "0", "]", "\n", "\n", "", "return", "packed_seq", ",", "reverse_indices", ",", "r_states", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.unpack_from_rnn_seq": [[158, 170], ["torch.utils.rnn.pad_packed_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "s_inputs_list.append", "s_inputs_list.append", "unpacked_seq[].unsqueeze", "unpacked_seq[].unsqueeze"], "function", ["None"], ["", "", "def", "unpack_from_rnn_seq", "(", "packed_seq", ",", "reverse_indices", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "unpacked_seq", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_seq", ",", "batch_first", "=", "batch_first", ")", "\n", "s_inputs_list", "=", "[", "]", "\n", "\n", "if", "not", "batch_first", ":", "\n", "        ", "for", "i", "in", "reverse_indices", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "unpacked_seq", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "s_inputs_list", ",", "1", ")", "\n", "", "else", ":", "\n", "        ", "for", "i", "in", "reverse_indices", ":", "\n", "            ", "s_inputs_list", ".", "append", "(", "unpacked_seq", "[", "i", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "return", "torch", ".", "cat", "(", "s_inputs_list", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.reverse_indice_for_state": [[172, 192], ["tuple", "tuple", "zip", "len", "isinstance", "state_list.append", "torch.cat", "torch.cat", "torch.cat", "state[].unsqueeze"], "function", ["None"], ["", "", "def", "reverse_indice_for_state", "(", "states", ",", "reverse_indices", ")", ":", "\n", "    ", "\"\"\"\n    :param states: [rnn.num_layers, batch_size, rnn.hidden_size]\n    :param reverse_indices: [batch_size]\n    :return:\n    \"\"\"", "\n", "if", "states", "is", "None", ":", "\n", "        ", "states", "=", "(", ")", "\n", "", "elif", "not", "isinstance", "(", "states", ",", "tuple", ")", ":", "\n", "        ", "states", "=", "(", "states", ",", ")", "# rnn.num_layers, batch_size, rnn.hidden_size", "\n", "\n", "", "states_lists", "=", "tuple", "(", "[", "]", "for", "_", "in", "states", ")", "\n", "for", "i", "in", "reverse_indices", ":", "\n", "        ", "for", "state_list", ",", "state", "in", "zip", "(", "states_lists", ",", "states", ")", ":", "\n", "            ", "state_list", ".", "append", "(", "state", "[", ":", ",", "i", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "", "r_states", "=", "tuple", "(", "torch", ".", "cat", "(", "state_list", ",", "dim", "=", "1", ")", "for", "state_list", "in", "states_lists", ")", "\n", "if", "len", "(", "r_states", ")", "==", "1", ":", "\n", "        ", "r_states", "=", "r_states", "[", "0", "]", "\n", "", "return", "r_states", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.auto_rnn": [[194, 218], ["torch_util.get_state_shape", "torch_util.pack_for_rnn_seq", "rnn", "torch_util.unpack_from_rnn_seq", "seqs.size", "seqs.size", "len", "torch.autograd.Variable", "torch_util.reverse_indice_for_state", "seqs.data.new().zero_", "seqs.data.new"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_state_shape", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_for_rnn_seq", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.unpack_from_rnn_seq", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.reverse_indice_for_state"], ["", "def", "auto_rnn", "(", "rnn", ":", "nn", ".", "RNN", ",", "seqs", ",", "lengths", ",", "batch_first", "=", "True", ",", "init_state", "=", "None", ",", "output_last_states", "=", "False", ")", ":", "\n", "    ", "batch_size", "=", "seqs", ".", "size", "(", "0", ")", "if", "batch_first", "else", "seqs", ".", "size", "(", "1", ")", "\n", "state_shape", "=", "get_state_shape", "(", "rnn", ",", "batch_size", ",", "rnn", ".", "bidirectional", ")", "\n", "\n", "# if init_state is None:", "\n", "#     h0 = c0 = Variable(seqs.data.new(*state_shape).zero_())", "\n", "# else:", "\n", "#     h0 = init_state[0] # rnn.num_layers, batch_size, rnn.hidden_size", "\n", "#     c0 = init_state[1]", "\n", "\n", "packed_pinputs", ",", "r_index", ",", "init_state", "=", "pack_for_rnn_seq", "(", "seqs", ",", "lengths", ",", "batch_first", ",", "init_state", ")", "\n", "\n", "if", "len", "(", "init_state", ")", "==", "0", ":", "\n", "        ", "h0", "=", "c0", "=", "Variable", "(", "seqs", ".", "data", ".", "new", "(", "*", "state_shape", ")", ".", "zero_", "(", ")", ")", "\n", "init_state", "=", "(", "h0", ",", "c0", ")", "\n", "\n", "", "output", ",", "last_state", "=", "rnn", "(", "packed_pinputs", ",", "init_state", ")", "\n", "output", "=", "unpack_from_rnn_seq", "(", "output", ",", "r_index", ",", "batch_first", ")", "\n", "\n", "if", "not", "output_last_states", ":", "\n", "        ", "return", "output", "\n", "", "else", ":", "\n", "        ", "last_state", "=", "reverse_indice_for_state", "(", "last_state", ",", "r_index", ")", "\n", "return", "output", ",", "last_state", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_sequence_for_linear": [[220, 239], ["enumerate", "torch.cat", "torch.cat", "torch.cat", "NotImplemented", "batch_list.append"], "function", ["None"], ["", "", "def", "pack_sequence_for_linear", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D] if batch_first \n    :param lengths:  [B]\n    :param batch_first:  \n    :return: \n    \"\"\"", "\n", "batch_list", "=", "[", "]", "\n", "if", "batch_first", ":", "\n", "        ", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "# print(inputs[i, :l].size())", "\n", "            ", "batch_list", ".", "append", "(", "inputs", "[", "i", ",", ":", "l", "]", ")", "\n", "", "packed_sequence", "=", "torch", ".", "cat", "(", "batch_list", ",", "0", ")", "\n", "# if chuck:", "\n", "#     return list(torch.chunk(packed_sequence, chuck, dim=0))", "\n", "# else:", "\n", "return", "packed_sequence", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.chucked_forward": [[241, 247], ["net", "torch.cat", "torch.cat", "torch.cat", "net", "torch.chunk", "torch.chunk", "torch.chunk"], "function", ["None"], ["", "", "def", "chucked_forward", "(", "inputs", ",", "net", ",", "chuck", "=", "None", ")", ":", "\n", "    ", "if", "not", "chuck", ":", "\n", "        ", "return", "net", "(", "inputs", ")", "\n", "", "else", ":", "\n", "        ", "output_list", "=", "[", "net", "(", "chuck", ")", "for", "chuck", "in", "torch", ".", "chunk", "(", "inputs", ",", "chuck", ",", "dim", "=", "0", ")", "]", "\n", "return", "torch", ".", "cat", "(", "output_list", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.unpack_sequence_for_linear": [[249, 266], ["max", "torch.cat", "torch.cat", "torch.cat", "isinstance", "torch.stack", "torch.stack", "torch.stack", "NotImplemented", "batch_list.append", "torch_util.pad_1d"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pad_1d"], ["", "", "def", "unpack_sequence_for_linear", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "batch_list", "=", "[", "]", "\n", "max_l", "=", "max", "(", "lengths", ")", "\n", "\n", "if", "not", "isinstance", "(", "inputs", ",", "list", ")", ":", "\n", "        ", "inputs", "=", "[", "inputs", "]", "\n", "", "inputs", "=", "torch", ".", "cat", "(", "inputs", ")", "\n", "\n", "if", "batch_first", ":", "\n", "        ", "start", "=", "0", "\n", "for", "l", "in", "lengths", ":", "\n", "            ", "end", "=", "start", "+", "l", "\n", "batch_list", ".", "append", "(", "pad_1d", "(", "inputs", "[", "start", ":", "end", "]", ",", "max_l", ")", ")", "\n", "start", "=", "end", "\n", "", "return", "torch", ".", "stack", "(", "batch_list", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplemented", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.seq2seq_cross_entropy": [[268, 290], ["torch_util.pack_sequence_for_linear", "functools.partial", "sum", "zip", "logits.size", "pack_sequence_for_linear.size", "logits.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "functools.partial.", "functools.partial."], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.pack_sequence_for_linear"], ["", "", "def", "seq2seq_cross_entropy", "(", "logits", ",", "label", ",", "l", ",", "chuck", "=", "None", ",", "sos_truncate", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    :param logits: [exB, V] : exB = sum(l)\n    :param label: [B] : a batch of Label\n    :param l: [B] : a batch of LongTensor indicating the lengths of each inputs\n    :param chuck: Number of chuck to process\n    :return: A loss value\n    \"\"\"", "\n", "packed_label", "=", "pack_sequence_for_linear", "(", "label", ",", "l", ")", "\n", "cross_entropy_loss", "=", "functools", ".", "partial", "(", "F", ".", "cross_entropy", ",", "size_average", "=", "False", ")", "\n", "total", "=", "sum", "(", "l", ")", "\n", "\n", "assert", "total", "==", "logits", ".", "size", "(", "0", ")", "or", "packed_label", ".", "size", "(", "0", ")", "==", "logits", ".", "size", "(", "0", ")", ",", "\"logits length mismatch with label length.\"", "\n", "\n", "if", "chuck", ":", "\n", "        ", "logits_losses", "=", "0", "\n", "for", "x", ",", "y", "in", "zip", "(", "torch", ".", "chunk", "(", "logits", ",", "chuck", ",", "dim", "=", "0", ")", ",", "torch", ".", "chunk", "(", "packed_label", ",", "chuck", ",", "dim", "=", "0", ")", ")", ":", "\n", "            ", "logits_losses", "+=", "cross_entropy_loss", "(", "x", ",", "y", ")", "\n", "", "return", "logits_losses", "*", "(", "1", "/", "total", ")", "\n", "", "else", ":", "\n", "        ", "return", "cross_entropy_loss", "(", "logits", ",", "packed_label", ")", "*", "(", "1", "/", "total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.max_along_time": [[292, 319], ["list", "enumerate", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "seq_i.max", "seq_i_max.squeeze.squeeze", "b_seq_max_list.append", "seq_i.max", "seq_i_max.squeeze.squeeze", "b_seq_max_list.append"], "function", ["None"], ["", "", "def", "max_along_time", "(", "inputs", ",", "lengths", ",", "list_in", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D] \n    :param lengths:  [B]\n    :return: [B * D] max_along_time\n    :param list_in: \n    \"\"\"", "\n", "ls", "=", "list", "(", "lengths", ")", "\n", "\n", "if", "not", "list_in", ":", "\n", "        ", "b_seq_max_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", ",", ":", "l", ",", ":", "]", "\n", "seq_i_max", ",", "_", "=", "seq_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "seq_i_max", "=", "seq_i_max", ".", "squeeze", "(", ")", "\n", "b_seq_max_list", ".", "append", "(", "seq_i_max", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_max_list", ")", "\n", "", "else", ":", "\n", "        ", "b_seq_max_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", "]", "\n", "seq_i_max", ",", "_", "=", "seq_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "seq_i_max", "=", "seq_i_max", ".", "squeeze", "(", ")", "\n", "b_seq_max_list", ".", "append", "(", "seq_i_max", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_max_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.avg_along_time": [[321, 348], ["list", "enumerate", "torch.stack", "torch.stack", "torch.stack", "enumerate", "torch.stack", "torch.stack", "torch.stack", "seq_i.mean", "seq_i_avg.squeeze.squeeze", "b_seq_avg_list.append", "seq_i.mean", "seq_i_avg.squeeze.squeeze", "b_seq_avg_list.append"], "function", ["None"], ["", "", "def", "avg_along_time", "(", "inputs", ",", "lengths", ",", "list_in", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, D]\n    :param lengths:  [B]\n    :return: [B * D] max_along_time\n    :param list_in:\n    \"\"\"", "\n", "ls", "=", "list", "(", "lengths", ")", "\n", "\n", "if", "not", "list_in", ":", "\n", "        ", "b_seq_avg_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", ",", ":", "l", ",", ":", "]", "\n", "seq_i_avg", "=", "seq_i", ".", "mean", "(", "dim", "=", "0", ")", "\n", "seq_i_avg", "=", "seq_i_avg", ".", "squeeze", "(", ")", "\n", "b_seq_avg_list", ".", "append", "(", "seq_i_avg", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_avg_list", ")", "\n", "", "else", ":", "\n", "        ", "b_seq_avg_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "ls", ")", ":", "\n", "            ", "seq_i", "=", "inputs", "[", "i", "]", "\n", "seq_i_avg", ",", "_", "=", "seq_i", ".", "mean", "(", "dim", "=", "0", ")", "\n", "seq_i_avg", "=", "seq_i_avg", ".", "squeeze", "(", ")", "\n", "b_seq_avg_list", ".", "append", "(", "seq_i_avg", ")", "\n", "\n", "", "return", "torch", ".", "stack", "(", "b_seq_avg_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_reverse_indices": [[366, 377], ["indices.data.new().fill_", "indices.size", "range", "int", "enumerate", "indices.data.new", "indices.size", "int"], "function", ["None"], ["", "", "def", "get_reverse_indices", "(", "indices", ",", "lengths", ")", ":", "\n", "    ", "r_indices", "=", "indices", ".", "data", ".", "new", "(", "indices", ".", "size", "(", ")", ")", ".", "fill_", "(", "0", ")", "\n", "batch_size", "=", "indices", ".", "size", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "int", "(", "batch_size", ")", ")", ":", "\n", "        ", "b_ind", "=", "indices", "[", "i", "]", "\n", "b_l", "=", "lengths", "[", "i", "]", "\n", "for", "k", ",", "ind", "in", "enumerate", "(", "b_ind", ")", ":", "\n", "            ", "if", "k", ">=", "b_l", ":", "\n", "                ", "break", "\n", "", "r_indices", "[", "i", ",", "int", "(", "ind", ")", "]", "=", "k", "\n", "", "", "return", "r_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.index_ordering": [[379, 399], ["inputs.size", "range", "torch.stack", "torch.stack", "torch.stack", "int", "ordered_out_list.append", "b_out.size"], "function", ["None"], ["", "def", "index_ordering", "(", "inputs", ",", "lengths", ",", "indices", ",", "pad_value", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T, ~]\n    :param lengths: [B]\n    :param indices: [B, T]\n    :return:\n    \"\"\"", "\n", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "ordered_out_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "int", "(", "batch_size", ")", ")", ":", "\n", "        ", "b_input", "=", "inputs", "[", "i", "]", "\n", "b_l", "=", "lengths", "[", "i", "]", "\n", "b_ind", "=", "indices", "[", "i", "]", "\n", "b_out", "=", "b_input", "[", "b_ind", "]", "\n", "if", "b_out", ".", "size", "(", "0", ")", ">", "b_l", ":", "\n", "            ", "b_out", "[", "b_l", ":", "]", "=", "pad_value", "\n", "", "ordered_out_list", ".", "append", "(", "b_out", ")", "\n", "\n", "", "outs", "=", "torch", ".", "stack", "(", "ordered_out_list", ",", "dim", "=", "0", ")", "\n", "return", "outs", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.start_and_end_token_handling": [[401, 431], ["torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "range", "torch.autograd.Variable", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat.data.new().zero_", "torch.autograd.Variable", "torch.cat.data.new", "torch.cat.data.new().zero_", "torch.cat.data.new"], "function", ["None"], ["", "def", "start_and_end_token_handling", "(", "inputs", ",", "lengths", ",", "sos_index", "=", "1", ",", "eos_index", "=", "2", ",", "pad_index", "=", "0", ",", "\n", "op", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param inputs: [B, T]\n    :param lengths: [B]\n    :param sos_index: \n    :param eos_index: \n    :param pad_index: \n    :return: \n    \"\"\"", "\n", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", "\n", "\n", "if", "not", "op", ":", "\n", "        ", "return", "inputs", ",", "lengths", "\n", "", "elif", "op", "==", "'rm_start'", ":", "\n", "        ", "inputs", "=", "torch", ".", "cat", "(", "[", "inputs", "[", ":", ",", "1", ":", "]", ",", "Variable", "(", "inputs", ".", "data", ".", "new", "(", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", ")", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", ",", "lengths", "-", "1", "\n", "", "elif", "op", "==", "'rm_end'", ":", "\n", "        ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "pass", "\n", "# Potential problems!?", "\n", "# inputs[i, lengths[i] - 1] = pad_index", "\n", "", "return", "inputs", ",", "lengths", "-", "1", "\n", "", "elif", "op", "==", "'rm_both'", ":", "\n", "        ", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "pass", "\n", "# Potential problems!?", "\n", "# inputs[i, lengths[i] - 1] = pad_index", "\n", "", "inputs", "=", "torch", ".", "cat", "(", "[", "inputs", "[", ":", ",", "1", ":", "]", ",", "Variable", "(", "inputs", ".", "data", ".", "new", "(", "batch_size", ",", "1", ")", ".", "zero_", "(", ")", ")", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", ",", "lengths", "-", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.seq2seq_att": [[433, 497], ["state.size", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "att_net", "enumerate", "torch.stack", "torch.stack", "torch.stack", "batch_list_mems.append", "state[].expand", "batch_list_state.append", "torch.softmax().transpose", "torch.sum", "torch.sum", "torch.sum", "result_list.append", "b_mems.size", "torch.softmax", "b_score.transpose"], "function", ["None"], ["", "", "def", "seq2seq_att", "(", "mems", ",", "lengths", ",", "state", ",", "att_net", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param mems: [B, T, D_mem] This are the memories.\n                    I call memory for this variable because I think attention is just like read something and then\n                    make alignments with your memories.\n                    This memory here is usually the input hidden state of the encoder.\n\n    :param lengths: [B]\n    :param state: [B, D_state]\n                    I call state for this variable because it's the state I percepts at this time step.\n\n    :param att_net: This is the attention network that will be used to calculate the alignment score between\n                    state and memories.\n                    input of the att_net is mems and state with shape:\n                        mems: [exB, D_mem]\n                        state: [exB, D_state]\n                    return of the att_net is [exB, 1]\n\n                    So any function that map a vector to a scalar could work.\n\n    :return: [B, D_result] \n    \"\"\"", "\n", "\n", "d_state", "=", "state", ".", "size", "(", "1", ")", "\n", "\n", "if", "not", "att_net", ":", "\n", "        ", "return", "state", "\n", "", "else", ":", "\n", "        ", "batch_list_mems", "=", "[", "]", "\n", "batch_list_state", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "b_mems", "=", "mems", "[", "i", ",", ":", "l", "]", "# [T, D_mem]", "\n", "batch_list_mems", ".", "append", "(", "b_mems", ")", "\n", "\n", "b_state", "=", "state", "[", "i", "]", ".", "expand", "(", "b_mems", ".", "size", "(", "0", ")", ",", "d_state", ")", "# [T, D_state]", "\n", "batch_list_state", ".", "append", "(", "b_state", ")", "\n", "\n", "", "packed_sequence_mems", "=", "torch", ".", "cat", "(", "batch_list_mems", ",", "0", ")", "# [sum(l), D_mem]", "\n", "packed_sequence_state", "=", "torch", ".", "cat", "(", "batch_list_state", ",", "0", ")", "# [sum(l), D_state]", "\n", "\n", "align_score", "=", "att_net", "(", "packed_sequence_mems", ",", "packed_sequence_state", ")", "# [sum(l), 1]", "\n", "\n", "# The score grouped as [(a1, a2, a3), (a1, a2), (a1, a2, a3, a4)].", "\n", "# aligned_seq = packed_sequence_mems * align_score", "\n", "\n", "start", "=", "0", "\n", "result_list", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "lengths", ")", ":", "\n", "            ", "end", "=", "start", "+", "l", "\n", "\n", "b_mems", "=", "packed_sequence_mems", "[", "start", ":", "end", ",", ":", "]", "# [l, D_mems]", "\n", "b_score", "=", "align_score", "[", "start", ":", "end", ",", ":", "]", "# [l, 1]", "\n", "\n", "softed_b_score", "=", "F", ".", "softmax", "(", "b_score", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "# [l, 1]", "\n", "\n", "weighted_sum", "=", "torch", ".", "sum", "(", "b_mems", "*", "softed_b_score", ",", "dim", "=", "0", ",", "keepdim", "=", "False", ")", "# [D_mems]", "\n", "\n", "result_list", ".", "append", "(", "weighted_sum", ")", "\n", "\n", "start", "=", "end", "\n", "\n", "", "result", "=", "torch", ".", "stack", "(", "result_list", ",", "dim", "=", "0", ")", "\n", "\n", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.select_top_k_and_to_results_dict": [[35, 52], ["scored_dict.items", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append"], "function", ["None"], ["def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ")", ":", "\n", "    ", "results_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'scored_results'", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "'scored_results'", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "results_dict", "[", "'sp_doc'", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "\n", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_model": [[54, 118], ["print", "range", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "model.size", "torch.max", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure": [[120, 264], ["print", "ema.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "print", "utils.common.save_jsonl", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "datetime.datetime.now", "pathlib.Path", "pathlib.Path", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "eval_open_qa_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "ema_device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "dataset_name", ")", ":", "\n", "    ", "print", "(", "f\"Eval {dataset_name}!\"", ")", "\n", "# dev_iter = biterator(dev_instances, num_epochs=1, shuffle=False)", "\n", "#", "\n", "# cur_eval_results_list = eval_model(model, dev_iter, device_num, make_int=True, with_probs=True)", "\n", "# copied_dev_o_dict = copy.deepcopy(dev_o_dict)", "\n", "# copied_dev_d_list = copy.deepcopy(dev_list)", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_dev_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_5 = od_sample_utils.select_top_k_and_to_results_dict(copied_dev_o_dict,", "\n", "#                                                                               score_field_name='prob',", "\n", "#                                                                               top_k=5, filter_value=0.5)", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_dev_d_list,", "\n", "#                                                                cur_results_dict_th0_5,", "\n", "#                                                                'id', 'predicted_docids')", "\n", "# # mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "# strict_score, pr, rec, f1 = fever_scorer.fever_doc_only(copied_dev_d_list, dev_list,", "\n", "#                                                         max_evidence=5)", "\n", "# score_05 = {", "\n", "#     'ss': strict_score,", "\n", "#     'pr': pr, 'rec': rec, 'f1': f1,", "\n", "# }", "\n", "#", "\n", "# list_dict_data_tool.append_subfield_from_list_to_dict(cur_eval_results_list, copied_dev_o_dict,", "\n", "#                                                       'qid', 'fid', check=True)", "\n", "#", "\n", "# cur_results_dict_th0_2 = fever_sampler_utils.select_top_k_and_to_results_dict(copied_dev_o_dict,", "\n", "#                                                                               score_field_name='prob',", "\n", "#                                                                               top_k=5, filter_value=0.2)", "\n", "#", "\n", "# list_dict_data_tool.append_item_from_dict_to_list_hotpot_style(copied_dev_d_list,", "\n", "#                                                                cur_results_dict_th0_2,", "\n", "#                                                                'id', 'predicted_docids')", "\n", "# # mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "# strict_score, pr, rec, f1 = fever_scorer.fever_doc_only(copied_dev_d_list, dev_list,", "\n", "#                                                         max_evidence=5)", "\n", "# score_02 = {", "\n", "#     'ss': strict_score,", "\n", "#     'pr': pr, 'rec': rec, 'f1': f1,", "\n", "# }", "\n", "#", "\n", "# logging_item = {", "\n", "#     'step:': update_step,", "\n", "#     'epoch': epoch_i,", "\n", "#     'score_02': score_02,", "\n", "#     'score_05': score_05,", "\n", "#     'time': str(datetime.datetime.now())", "\n", "# }", "\n", "#", "\n", "# print(logging_item)", "\n", "#", "\n", "# s02_ss_score = score_02['ss']", "\n", "# s05_ss_score = score_05['ss']", "\n", "#", "\n", "# if not debug_mode:", "\n", "#     save_file_name = f'i({update_step})|e({epoch_i})' \\", "\n", "#         f'|v02_ofever({s02_ss_score})' \\", "\n", "#         f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "#", "\n", "#     # print(save_file_name)", "\n", "#     logging_agent.incorporate_results({}, save_file_name, logging_item)", "\n", "#     logging_agent.logging_to_file(Path(file_path_prefix) / \"log.json\")", "\n", "#", "\n", "#     model_to_save = model.module if hasattr(model, 'module') else model", "\n", "#     output_model_file = Path(file_path_prefix) / save_file_name", "\n", "#     torch.save(model_to_save.state_dict(), str(output_model_file))", "\n", "\n", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "ema_device_num", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "make_int", "=", "False", ",", "with_probs", "=", "True", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top10", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "10", ",", "filter_value", "=", "0.01", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_top10", ",", "\n", "'qid'", ",", "'pred_p_list'", ")", "\n", "\n", "t10_recall", "=", "open_domain_qa_eval", ".", "qa_paragraph_eval_v1", "(", "copied_dev_d_list", ",", "dev_list", ")", "\n", "\n", "top_10_recall", "=", "{", "\n", "'recall'", ":", "t10_recall", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top20", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "20", ",", "filter_value", "=", "0.01", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_top20", ",", "\n", "'qid'", ",", "'pred_p_list'", ")", "\n", "\n", "t20_recall", "=", "open_domain_qa_eval", ".", "qa_paragraph_eval_v1", "(", "copied_dev_d_list", ",", "dev_list", ")", "\n", "\n", "top_20_recall", "=", "{", "\n", "'top_20_recall'", ":", "t20_recall", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'dataset_name'", ":", "dataset_name", ",", "\n", "'top10'", ":", "top_10_recall", ",", "\n", "'top20'", ":", "top_20_recall", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "Path", "(", "file_path_prefix", ")", "/", "Path", "(", "\n", "f\"i({update_step})|e({epoch_i})|{dataset_name}|top10({t10_recall})|top20({t20_recall})|seed({seed})_eval_results.jsonl\"", ")", ")", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "            ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})|{dataset_name}'", "f'|top10({t10_recall})'", "f'|top20({t20_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.separate_eval_open_qa_procedure": [[266, 325], ["print", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_domain_sampler.od_sample_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "print", "str", "utils.common.save_jsonl", "str", "pathlib.Path", "pathlib.Path", "datetime.datetime.now", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.open_domain_qa_eval.qa_paragraph_eval_v1", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.save_jsonl"], ["", "", "", "def", "separate_eval_open_qa_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "dataset_name", ",", "save_path", "=", "None", ",", "tag", "=", "\"\"", ")", ":", "\n", "    ", "print", "(", "f\"Eval {dataset_name}!\"", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "False", ",", "with_probs", "=", "True", ",", "show_progress", "=", "True", ")", "\n", "\n", "if", "save_path", "is", "not", "None", ":", "\n", "        ", "model_file", "=", "str", "(", "Path", "(", "model_path", ")", ".", "stem", ")", "\n", "save_filename", "=", "Path", "(", "save_path", ")", "/", "f\"{model_file}_{dataset_name}_{tag}_p_level_eval.jsonl\"", "\n", "common", ".", "save_jsonl", "(", "cur_eval_results_list", ",", "Path", "(", "save_filename", ")", ")", "\n", "", "else", ":", "\n", "        ", "pass", "\n", "\n", "", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top10", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "10", ",", "filter_value", "=", "0.01", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_top10", ",", "\n", "'qid'", ",", "'pred_p_list'", ")", "\n", "\n", "t10_recall", "=", "open_domain_qa_eval", ".", "qa_paragraph_eval_v1", "(", "copied_dev_d_list", ",", "dev_list", ")", "\n", "\n", "top_10_recall", "=", "{", "\n", "'recall'", ":", "t10_recall", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_top20", "=", "od_sample_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "20", ",", "filter_value", "=", "0.01", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_top20", ",", "\n", "'qid'", ",", "'pred_p_list'", ")", "\n", "\n", "t20_recall", "=", "open_domain_qa_eval", ".", "qa_paragraph_eval_v1", "(", "copied_dev_d_list", ",", "dev_list", ")", "\n", "\n", "top_20_recall", "=", "{", "\n", "'top_20_recall'", ":", "t20_recall", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'dataset_name'", ":", "dataset_name", ",", "\n", "'top10'", ":", "top_10_recall", ",", "\n", "'top20'", ":", "top_20_recall", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_fever_procedure": [[327, 471], ["print", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "ema.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "pathlib.Path", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "eval_fever_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "ema_device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ")", ":", "\n", "    ", "print", "(", "\"Eval FEVER!\"", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "ema_device_num", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "            ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_hotpot_procedure": [[473, 589], ["print", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_qa_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "open_qa_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "ema.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "biterator", "open_qa_p_level.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "open_qa_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "open_qa_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "pathlib.Path", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "", "", "def", "eval_hotpot_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "ema_device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ")", ":", "\n", "    ", "print", "(", "\"Eval HOTPOT!\"", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "top5_doc_recall", "=", "metrics_top5", "[", "'doc_recall'", "]", "\n", "top5_UB_sp_recall", "=", "metrics_top5_UB", "[", "'sp_recall'", "]", "\n", "top10_doc_recall", "=", "metrics_top10", "[", "'doc_recall'", "]", "\n", "top10_Ub_sp_recall", "=", "metrics_top10_UB", "[", "'sp_recall'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|t5_doc_recall({top5_doc_recall})|t5_sp_recall({top5_UB_sp_recall})'", "f'|t10_doc_recall({top10_doc_recall})|t5_sp_recall({top10_Ub_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "ema_device_num", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "with_probs", "=", "True", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "top5_doc_recall", "=", "metrics_top5", "[", "'doc_recall'", "]", "\n", "top5_UB_sp_recall", "=", "metrics_top5_UB", "[", "'sp_recall'", "]", "\n", "top10_doc_recall", "=", "metrics_top10", "[", "'doc_recall'", "]", "\n", "top10_Ub_sp_recall", "=", "metrics_top10_UB", "[", "'sp_recall'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "if", "not", "debug_mode", ":", "\n", "            ", "save_file_name", "=", "f'ema_i({update_step})|e({epoch_i})'", "f'|t5_doc_recall({top5_doc_recall})|t5_sp_recall({top5_UB_sp_recall})'", "f'|t10_doc_recall({top10_doc_recall})|t5_sp_recall({top10_Ub_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.multitask_open_qa_model_go": [[591, 1020], ["torch.manual_seed", "int", "torch.device", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "span_prediction_task_utils.squad_utils.get_squad_question_selection_forward_list", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "len", "len", "len", "len", "print", "print", "print", "print", "print", "print", "print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "range", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "torch.cuda.is_available", "utils.common.load_json", "len", "len", "len", "neural_modules.model_EMA.EMA", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "random.shuffle", "random.shuffle", "random.shuffle", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "random.shuffle", "random.shuffle", "random.shuffle", "random.shuffle", "print", "random.shuffle", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "print", "torch.save", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "open", "open", "out_f.write", "out_f.flush", "len", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "print", "neural_modules.model_EMA.EMA.get_inference_model", "torch.save", "len", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "len", "any", "neural_modules.model_EMA.EMA.", "print", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "open_qa_p_level.eval_open_qa_procedure", "any", "hasattr", "updated_model.named_parameters"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.span_prediction_task_utils.squad_utils.get_squad_question_selection_forward_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.eval_open_qa_procedure"], ["", "", "", "def", "multitask_open_qa_model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "64", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "3e-5", "\n", "num_train_epochs", "=", "3", "\n", "eval_frequency", "=", "10000", "\n", "hotpot_pos_ratio", "=", "0.25", "\n", "do_lower_case", "=", "True", "\n", "max_l", "=", "254", "\n", "\n", "hotpot_train_size", "=", "None", "\n", "fever_train_size", "=", "None", "\n", "curatedtrec_train_size", "=", "0", "\n", "webq_train_size", "=", "0", "\n", "squad_train_size", "=", "0", "\n", "wikimovie_train_size", "=", "0", "\n", "\n", "squad_v11_pos_size", "=", "None", "\n", "\n", "# hotpot_train_size = 0", "\n", "# fever_train_size = 0", "\n", "# squad_train_size = 80_000", "\n", "# squad_v11_pos_size = 0", "\n", "\n", "experiment_name", "=", "f'mtr_open_qa_p_level_(num_train_epochs:{num_train_epochs})'", "\n", "\n", "debug_mode", "=", "False", "\n", "do_ema", "=", "True", "\n", "\n", "open_qa_paras", "=", "{", "\n", "'webq'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "0.25", "}", ",", "\n", "'curatedtrec'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "0.25", "}", ",", "\n", "'squad'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "1", ",", "'down_sample_ratio'", ":", "0.25", "}", ",", "\n", "'wikimovie'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "0.25", "}", ",", "\n", "}", "\n", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Hotpot Dataset", "\n", "# hotpot_train_list = common.load_json(config.TRAIN_FILE)", "\n", "# hotpot_dev_list = common.load_json(config.DEV_FULLWIKI_FILE)", "\n", "# hotpot_dev_o_dict = list_dict_data_tool.list_to_dict(hotpot_dev_list, '_id')", "\n", "\n", "# Load Hotpot upstream paragraph forward item", "\n", "hotpot_dev_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_dev_p_level_unlabeled.jsonl\"", ")", "\n", "hotpot_train_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_train_p_level_labeled.jsonl\"", ")", "\n", "\n", "hotpot_train_fitems_list", "=", "hotpot_sampler_utils", ".", "field_name_convert", "(", "hotpot_train_fitems_list", ",", "'doc_t'", ",", "'element'", ")", "\n", "hotpot_dev_fitems_list", "=", "hotpot_sampler_utils", ".", "field_name_convert", "(", "hotpot_dev_fitems_list", ",", "'doc_t'", ",", "'element'", ")", "\n", "\n", "# Load FEVER Dataset", "\n", "# fever_train_list = common.load_json(config.FEVER_TRAIN)", "\n", "# fever_dev_list = common.load_jsonl(config.FEVER_DEV)", "\n", "# fever_dev_o_dict = list_dict_data_tool.list_to_dict(fever_dev_list, 'id')", "\n", "\n", "train_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_train.jsonl\"", ")", "\n", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_dev.jsonl\"", ")", "\n", "\n", "fever_train_fitems_list", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'train'", ",", "train_ruleterm_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "True", ")", "\n", "fever_dev_fitems_list", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'dev'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "\n", "# Load Open QA Dataset.", "\n", "webq_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'webq'", ",", "'test'", ",", "False", ",", "debug", "=", "debug_mode", ",", "\n", "upstream_top_k", "=", "40", ")", "\n", "webq_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'webq'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "webq_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WEBQ_TEST_GT", ")", "\n", "\n", "curatedtrec_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'curatedtrec'", ",", "'test'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "curatedtrec_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'curatedtrec'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "curatedtrec_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_CURATEDTERC_TEST_GT", ")", "\n", "\n", "squad_dev_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'squad'", ",", "'dev'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "squad_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'squad'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "squad_dev_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_SQUAD_DEV_GT", ")", "\n", "\n", "wikimovie_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'wikimovie'", ",", "'test'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "wikimovie_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'wikimovie'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "wikimovie_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WIKIM_TEST_GT", ")", "\n", "\n", "# Load squadv11 forward:", "\n", "squad_v11_pos_fitems", "=", "get_squad_question_selection_forward_list", "(", "common", ".", "load_json", "(", "config", ".", "SQUAD_TRAIN_1_1", ")", ")", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "webq_test_gt_list", "=", "webq_test_gt_list", "[", ":", "50", "]", "\n", "curatedtrec_test_gt_list", "=", "curatedtrec_test_gt_list", "[", ":", "50", "]", "\n", "squad_dev_gt_list", "=", "squad_dev_gt_list", "[", ":", "50", "]", "\n", "wikimovie_test_gt_list", "=", "wikimovie_test_gt_list", "[", ":", "50", "]", "\n", "\n", "# hotpot_dev_list = hotpot_dev_list[:10]", "\n", "hotpot_dev_fitems_list", "=", "hotpot_dev_fitems_list", "[", ":", "296", "]", "\n", "hotpot_train_fitems_list", "=", "hotpot_train_fitems_list", "[", ":", "300", "]", "\n", "\n", "# fever_dev_list = fever_dev_list[:100]", "\n", "eval_frequency", "=", "2", "\n", "\n", "", "webq_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "webq_test_gt_list", ",", "'question'", ")", "\n", "curatedtrec_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "curatedtrec_test_gt_list", ",", "'question'", ")", "\n", "squad_dev_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "squad_dev_gt_list", ",", "'question'", ")", "\n", "wikimovie_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "wikimovie_test_gt_list", ",", "'question'", ")", "\n", "\n", "# Down_sample for hotpot.", "\n", "hotpot_sampled_train_list", "=", "down_sample_neg", "(", "hotpot_train_fitems_list", ",", "ratio", "=", "hotpot_pos_ratio", ")", "\n", "if", "hotpot_train_size", "is", "None", ":", "\n", "        ", "hotpot_est_datasize", "=", "len", "(", "hotpot_sampled_train_list", ")", "\n", "", "else", ":", "\n", "        ", "hotpot_est_datasize", "=", "hotpot_train_size", "\n", "\n", "", "if", "fever_train_size", "is", "None", ":", "\n", "        ", "fever_est_datasize", "=", "len", "(", "fever_train_fitems_list", ")", "\n", "", "else", ":", "\n", "        ", "fever_est_datasize", "=", "fever_train_size", "\n", "\n", "", "sampled_squad_v11_pos_fitems", "=", "squad_v11_pos_fitems", "[", ":", "squad_v11_pos_size", "]", "\n", "\n", "webq_est_datasize", "=", "len", "(", "webq_train_fitem_list", "[", ":", "webq_train_size", "]", ")", "\n", "curatedtrec_est_datasize", "=", "len", "(", "curatedtrec_train_fitem_list", "[", ":", "curatedtrec_train_size", "]", ")", "\n", "squad_est_datasize", "=", "len", "(", "squad_train_fitem_list", "[", ":", "squad_train_size", "]", ")", "\n", "wikimovie_est_datasize", "=", "len", "(", "wikimovie_train_fitem_list", "[", ":", "wikimovie_train_size", "]", ")", "\n", "\n", "print", "(", "\"Hotpot Train Size:\"", ",", "hotpot_est_datasize", ")", "\n", "print", "(", "\"Fever Train Size:\"", ",", "fever_est_datasize", ")", "\n", "print", "(", "\"WebQ Train Size:\"", ",", "webq_est_datasize", ")", "\n", "print", "(", "\"TREC Train Size:\"", ",", "curatedtrec_est_datasize", ")", "\n", "print", "(", "\"SQuAD Train Size:\"", ",", "squad_est_datasize", ")", "\n", "print", "(", "\"WikiMovie Train Size:\"", ",", "wikimovie_est_datasize", ")", "\n", "\n", "print", "(", "\"SQuADv11 pos size:\"", ",", "len", "(", "sampled_squad_v11_pos_fitems", ")", ")", "\n", "\n", "est_datasize", "=", "hotpot_est_datasize", "+", "fever_est_datasize", "+", "webq_est_datasize", "+", "curatedtrec_est_datasize", "+", "len", "(", "sampled_squad_v11_pos_fitems", ")", "+", "squad_est_datasize", "+", "wikimovie_est_datasize", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "ema", "=", "None", "\n", "if", "do_ema", ":", "\n", "        ", "ema", "=", "EMA", "(", "model", ",", "model", ".", "named_parameters", "(", ")", ",", "device_num", "=", "1", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "# hotpot_dev_instances = bert_cs_reader.read(hotpot_dev_fitems_list)", "\n", "# fever_dev_instances = bert_cs_reader.read(fever_dev_fitems_list)", "\n", "webq_test_instance", "=", "bert_cs_reader", ".", "read", "(", "webq_test_fitem_list", ")", "\n", "curatedtrec_test_instance", "=", "bert_cs_reader", ".", "read", "(", "curatedtrec_test_fitem_list", ")", "\n", "squad_dev_instance", "=", "bert_cs_reader", ".", "read", "(", "squad_dev_fitem_list", ")", "\n", "wikimovie_test_instance", "=", "bert_cs_reader", ".", "read", "(", "wikimovie_test_fitem_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "# # # Create Log File", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "hotpot_sampled_train_list", "=", "down_sample_neg", "(", "hotpot_train_fitems_list", ",", "ratio", "=", "hotpot_pos_ratio", ")", "\n", "random", ".", "shuffle", "(", "hotpot_sampled_train_list", ")", "\n", "hotpot_sampled_train_list", "=", "hotpot_sampled_train_list", "[", ":", "hotpot_train_size", "]", "\n", "\n", "random", ".", "shuffle", "(", "fever_train_fitems_list", ")", "\n", "fever_train_fitems_list", "=", "fever_train_fitems_list", "[", ":", "fever_train_size", "]", "\n", "\n", "random", ".", "shuffle", "(", "squad_v11_pos_fitems", ")", "\n", "sampled_squad_v11_pos_fitems", "=", "squad_v11_pos_fitems", "[", ":", "squad_v11_pos_size", "]", "\n", "\n", "all_train_data", "=", "hotpot_sampled_train_list", "+", "fever_train_fitems_list", "+", "sampled_squad_v11_pos_fitems", "\n", "# all_train_data = []", "\n", "\n", "webq_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'webq'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "curatedtrec_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'curatedtrec'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "squad_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'squad'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "wikimovie_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'wikimovie'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "\n", "random", ".", "shuffle", "(", "squad_train_fitem_list", ")", "\n", "squad_train_fitem_list", "=", "squad_train_fitem_list", "[", ":", "squad_train_size", "]", "\n", "\n", "random", ".", "shuffle", "(", "wikimovie_train_fitem_list", ")", "\n", "wikimovie_train_fitem_list", "=", "wikimovie_train_fitem_list", "[", ":", "wikimovie_train_size", "]", "\n", "\n", "random", ".", "shuffle", "(", "curatedtrec_train_fitem_list", ")", "\n", "curatedtrec_train_fitem_list", "=", "curatedtrec_train_fitem_list", "[", ":", "curatedtrec_train_size", "]", "\n", "\n", "random", ".", "shuffle", "(", "webq_train_fitem_list", ")", "\n", "webq_train_fitem_list", "=", "webq_train_fitem_list", "[", ":", "webq_train_size", "]", "\n", "\n", "all_train_data", "=", "all_train_data", "+", "webq_train_fitem_list", "+", "curatedtrec_train_fitem_list", "+", "squad_train_fitem_list", "+", "wikimovie_train_fitem_list", "\n", "\n", "print", "(", "\"Current all train size:\"", ",", "len", "(", "all_train_data", ")", ")", "\n", "\n", "random", ".", "shuffle", "(", "all_train_data", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "all_train_data", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "if", "ema", "is", "not", "None", "and", "do_ema", ":", "\n", "                    ", "updated_model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "ema", "(", "updated_model", ".", "named_parameters", "(", ")", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "webq_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "webq_test_gt_list", ",", "\n", "webq_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'webq'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "curatedtrec_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "curatedtrec_test_gt_list", ",", "\n", "curatedtrec_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'curatedtrec'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "squad_dev_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "squad_dev_gt_list", ",", "\n", "squad_dev_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'squad'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "wikimovie_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "wikimovie_test_gt_list", ",", "\n", "wikimovie_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'wikimovie'", ")", "\n", "# Eval FEVER", "\n", "# eval_fever_procedure(biterator, fever_dev_instances, model, device_num, 1, fever_dev_list,", "\n", "#                      fever_dev_o_dict, debug_mode, logging_agent, update_step, epoch_i,", "\n", "#                      file_path_prefix,", "\n", "#                      do_ema, ema, seed)", "\n", "# eval_hotpot_procedure(biterator, hotpot_dev_instances, model, device_num, 1, hotpot_dev_list,", "\n", "#                       hotpot_dev_o_dict, debug_mode, logging_agent, update_step, epoch_i,", "\n", "#                       file_path_prefix, do_ema, ema, seed)", "\n", "", "", "", "", "epoch_i", "=", "num_train_epochs", "-", "1", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "webq_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "webq_test_gt_list", ",", "\n", "webq_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'webq'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "curatedtrec_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "curatedtrec_test_gt_list", ",", "\n", "curatedtrec_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'curatedtrec'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "squad_dev_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "squad_dev_gt_list", ",", "\n", "squad_dev_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'squad'", ")", "\n", "\n", "eval_open_qa_procedure", "(", "biterator", ",", "wikimovie_test_instance", ",", "model", ",", "device_num", ",", "1", ",", "\n", "wikimovie_test_gt_list", ",", "\n", "wikimovie_test_gt_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ",", "'wikimovie'", ")", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "print", "(", "\"Final Saving.\"", ")", "\n", "save_file_name", "=", "f'i({update_step})|e({num_train_epochs})_final_model'", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"Final EMA Saving\"", ")", "\n", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "save_file_name", "=", "f'i({update_step})|e({num_train_epochs})_final_ema_model'", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.selective_eval": [[1022, 1182], ["torch.manual_seed", "torch.device", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "utils.common.load_jsonl", "open_domain_sampler.p_sampler.prepare_forward_data", "open_domain_sampler.p_sampler.prepare_forward_data", "utils.common.load_jsonl", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "utils.list_dict_data_tool.list_to_dict", "len", "len", "print", "print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.load_state_dict", "torch.nn.DataParallel.to", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "print", "print", "print", "print", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "open_qa_p_level.separate_eval_open_qa_procedure", "torch.cuda.is_available", "torch.load", "torch.nn.DataParallel", "len", "len", "len", "len", "torch.cuda.is_available", "len"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.open_domain_sampler.p_sampler.prepare_forward_data", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.open_qa_p_level.separate_eval_open_qa_procedure"], ["", "", "", "def", "selective_eval", "(", "model_path", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "True", "\n", "# lazy = True", "\n", "forward_size", "=", "128", "\n", "do_lower_case", "=", "True", "\n", "max_l", "=", "264", "\n", "\n", "debug_mode", "=", "False", "\n", "\n", "open_qa_paras", "=", "{", "\n", "'webq'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "None", "}", ",", "\n", "'curatedtrec'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "None", "}", ",", "\n", "'squad'", ":", "{", "'upstream_top_k'", ":", "30", ",", "'distant_gt_top_k'", ":", "1", ",", "'down_sample_ratio'", ":", "None", "}", ",", "\n", "'wikimovie'", ":", "{", "'upstream_top_k'", ":", "40", ",", "'distant_gt_top_k'", ":", "2", ",", "'down_sample_ratio'", ":", "None", "}", ",", "\n", "}", "\n", "\n", "num_class", "=", "1", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Open QA Dataset.", "\n", "webq_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'webq'", ",", "'test'", ",", "False", ",", "debug", "=", "debug_mode", ",", "\n", "upstream_top_k", "=", "40", ")", "\n", "webq_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'webq'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'webq'", "]", "[", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "webq_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WEBQ_TEST_GT", ")", "\n", "webq_train_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WEBQ_TRAIN_GT", ")", "\n", "\n", "curatedtrec_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'curatedtrec'", ",", "'test'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "curatedtrec_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'curatedtrec'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'curatedtrec'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "curatedtrec_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_CURATEDTERC_TEST_GT", ")", "\n", "curatedtrec_train_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_CURATEDTERC_TRAIN_GT", ")", "\n", "\n", "squad_dev_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'squad'", ",", "'dev'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "squad_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'squad'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'squad'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "squad_dev_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_SQUAD_DEV_GT", ")", "\n", "squad_train_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_SQUAD_TRAIN_GT", ")", "\n", "\n", "wikimovie_test_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'wikimovie'", ",", "'test'", ",", "False", ",", "\n", "upstream_top_k", "=", "40", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "wikimovie_train_fitem_list", "=", "open_domain_p_sampler", ".", "prepare_forward_data", "(", "'wikimovie'", ",", "'train'", ",", "True", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'upstream_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'distant_gt_top_k'", "]", ",", "\n", "open_qa_paras", "[", "'wikimovie'", "]", "[", "\n", "'down_sample_ratio'", "]", ",", "\n", "debug", "=", "debug_mode", ")", "\n", "wikimovie_test_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WIKIM_TEST_GT", ")", "\n", "wikimovie_train_gt_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "OPEN_WIKIM_TRAIN_GT", ")", "\n", "\n", "# Load squadv11 forward:", "\n", "\n", "webq_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "webq_test_gt_list", ",", "'question'", ")", "\n", "curatedtrec_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "curatedtrec_test_gt_list", ",", "'question'", ")", "\n", "squad_dev_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "squad_dev_gt_list", ",", "'question'", ")", "\n", "wikimovie_test_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "wikimovie_test_gt_list", ",", "'question'", ")", "\n", "\n", "webq_train_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "webq_train_gt_list", ",", "'question'", ")", "\n", "curatedtrec_train_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "curatedtrec_train_gt_list", ",", "'question'", ")", "\n", "squad_train_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "squad_train_gt_list", ",", "'question'", ")", "\n", "wikimovie_train_gt_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "wikimovie_train_gt_list", ",", "'question'", ")", "\n", "\n", "webq_est_datasize", "=", "len", "(", "webq_train_fitem_list", ")", "\n", "curatedtrec_est_datasize", "=", "len", "(", "curatedtrec_train_fitem_list", ")", "\n", "\n", "print", "(", "\"WebQ Train Size:\"", ",", "webq_est_datasize", ")", "\n", "print", "(", "\"TREC Train Size:\"", ",", "curatedtrec_est_datasize", ")", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "\n", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "\n", "", "webq_test_instance", "=", "bert_cs_reader", ".", "read", "(", "webq_test_fitem_list", ")", "\n", "curatedtrec_test_instance", "=", "bert_cs_reader", ".", "read", "(", "curatedtrec_test_fitem_list", ")", "\n", "squad_dev_instance", "=", "bert_cs_reader", ".", "read", "(", "squad_dev_fitem_list", ")", "\n", "wikimovie_test_instance", "=", "bert_cs_reader", ".", "read", "(", "wikimovie_test_fitem_list", ")", "\n", "\n", "webq_train_instance", "=", "bert_cs_reader", ".", "read", "(", "webq_train_fitem_list", ")", "\n", "curatedtrec_train_instance", "=", "bert_cs_reader", ".", "read", "(", "curatedtrec_train_fitem_list", ")", "\n", "squad_train_instance", "=", "bert_cs_reader", ".", "read", "(", "squad_train_fitem_list", ")", "\n", "wikimovie_train_instance", "=", "bert_cs_reader", ".", "read", "(", "wikimovie_train_fitem_list", ")", "\n", "\n", "print", "(", "'webq:'", ",", "len", "(", "webq_train_fitem_list", ")", ")", "\n", "print", "(", "'curatedtrec:'", ",", "len", "(", "curatedtrec_train_fitem_list", ")", ")", "\n", "print", "(", "'squad:'", ",", "len", "(", "squad_train_fitem_list", ")", ")", "\n", "print", "(", "'wikimovie:'", ",", "len", "(", "wikimovie_train_fitem_list", ")", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "# separate_eval_open_qa_procedure(biterator, curatedtrec_test_instance, model, 0,", "\n", "#                                 curatedtrec_test_gt_list, curatedtrec_test_gt_dict, 'curatedtrec',", "\n", "#                                 save_path=\".\", tag='test')", "\n", "#", "\n", "# separate_eval_open_qa_procedure(biterator, curatedtrec_train_instance, model, 0,", "\n", "#                                 curatedtrec_train_gt_list, curatedtrec_train_gt_dict, 'curatedtrec',", "\n", "#                                 save_path=\".\", tag='train')", "\n", "\n", "# separate_eval_open_qa_procedure(biterator, squad_train_instance, model, 0,", "\n", "#                                 squad_train_gt_list, squad_train_gt_dict, 'squad',", "\n", "#                                 save_path=\".\", tag='train')", "\n", "\n", "# separate_eval_open_qa_procedure(biterator, wikimovie_train_instance, model, 0,", "\n", "#                                 wikimovie_train_gt_list, wikimovie_train_gt_dict, 'wikimovie',", "\n", "#                                 save_path=\".\", tag='train')", "\n", "\n", "separate_eval_open_qa_procedure", "(", "biterator", ",", "webq_train_instance", ",", "model", ",", "0", ",", "\n", "webq_train_gt_list", ",", "webq_train_gt_dict", ",", "'webq'", ",", "\n", "save_path", "=", "\".\"", ",", "tag", "=", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict": [[31, 48], ["scored_dict.items", "dict", "dict", "fitems_dict.values", "sorted", "scored_element_list.append"], "function", ["None"], ["def", "select_top_k_and_to_results_dict", "(", "scored_dict", ",", "merged_field_name", "=", "'merged_field'", ",", "\n", "score_field_name", "=", "'score'", ",", "item_field_name", "=", "'element'", ",", "\n", "top_k", "=", "5", ")", ":", "\n", "    ", "results_dict", "=", "{", "'sp_doc'", ":", "dict", "(", ")", ",", "'scored_results'", ":", "dict", "(", ")", "}", "\n", "for", "key", ",", "value", "in", "scored_dict", ".", "items", "(", ")", ":", "\n", "        ", "fitems_dict", "=", "value", "[", "merged_field_name", "]", "\n", "scored_element_list", "=", "[", "]", "\n", "for", "item", "in", "fitems_dict", ".", "values", "(", ")", ":", "\n", "            ", "score", "=", "item", "[", "score_field_name", "]", "\n", "element", "=", "item", "[", "item_field_name", "]", "\n", "scored_element_list", ".", "append", "(", "(", "score", ",", "element", ")", ")", "# score is index 0.", "\n", "\n", "", "results_dict", "[", "'scored_results'", "]", "[", "key", "]", "=", "scored_element_list", "\n", "sorted_e_list", "=", "sorted", "(", "scored_element_list", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "results_dict", "[", "'sp_doc'", "]", "[", "key", "]", "=", "[", "e", "for", "s", ",", "e", "in", "sorted_e_list", "[", ":", "top_k", "]", "]", "\n", "\n", "", "return", "results_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model": [[50, 114], ["print", "range", "torch.no_grad", "model.eval", "tqdm.tqdm", "len", "len", "len", "len", "len", "len", "len", "len", "len", "dict", "result_items_list.append", "enumerate", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "model", "y_pid_list.extend", "y_fid_list.extend", "y_element_list.extend", "y_pred_list.extend", "y_logits_list.extend", "model.size", "len", "len", "int", "list", "list", "list", "[].view().tolist", "model.view().tolist", "y_probs_list.extend", "torch.sigmoid().view().tolist", "[].view", "model.view", "model.size", "model.size", "torch.sigmoid().view", "model.size", "torch.max", "torch.sigmoid"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask"], ["", "def", "eval_model", "(", "model", ",", "data_iter", ",", "device_num", ",", "with_probs", "=", "False", ",", "make_int", "=", "False", ",", "show_progress", "=", "False", ")", ":", "\n", "    ", "print", "(", "\"Evaluating ...\"", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "totoal_size", "=", "0", "\n", "\n", "y_pred_list", "=", "[", "]", "\n", "y_fid_list", "=", "[", "]", "\n", "y_pid_list", "=", "[", "]", "\n", "y_element_list", "=", "[", "]", "\n", "\n", "y_logits_list", "=", "[", "]", "\n", "y_probs_list", "=", "[", "]", "\n", "\n", "for", "batch_idx", ",", "batch", "in", "tqdm", "(", "enumerate", "(", "data_iter", ")", ",", "disable", "=", "(", "not", "show_progress", ")", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "eval_paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "eval_paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "eval_labels_ids", "=", "batch", "[", "'label'", "]", "\n", "eval_att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "eval_paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "out", "=", "model", "(", "eval_paired_sequence", ",", "token_type_ids", "=", "eval_paired_segments_ids", ",", "attention_mask", "=", "eval_att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "EVAL", ",", "\n", "labels", "=", "eval_labels_ids", ")", "\n", "\n", "y_pid_list", ".", "extend", "(", "list", "(", "batch", "[", "'qid'", "]", ")", ")", "\n", "y_fid_list", ".", "extend", "(", "list", "(", "batch", "[", "'fid'", "]", ")", ")", "\n", "y_element_list", ".", "extend", "(", "list", "(", "batch", "[", "'item'", "]", ")", ")", "\n", "\n", "y_pred_list", ".", "extend", "(", "torch", ".", "max", "(", "out", ",", "1", ")", "[", "1", "]", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "y_logits_list", ".", "extend", "(", "out", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "if", "with_probs", ":", "\n", "                ", "y_probs_list", ".", "extend", "(", "torch", ".", "sigmoid", "(", "out", ")", ".", "view", "(", "out", ".", "size", "(", "0", ")", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "", "totoal_size", "+=", "out", ".", "size", "(", "0", ")", "\n", "\n", "", "", "result_items_list", "=", "[", "]", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_fid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_pid_list", ")", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_element_list", ")", "\n", "\n", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_logits_list", ")", "\n", "\n", "if", "with_probs", ":", "\n", "        ", "assert", "len", "(", "y_pred_list", ")", "==", "len", "(", "y_probs_list", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "y_pred_list", ")", ")", ":", "\n", "        ", "r_item", "=", "dict", "(", ")", "\n", "r_item", "[", "'fid'", "]", "=", "y_fid_list", "[", "i", "]", "\n", "r_item", "[", "'qid'", "]", "=", "y_pid_list", "[", "i", "]", "if", "not", "make_int", "else", "int", "(", "y_pid_list", "[", "i", "]", ")", "\n", "r_item", "[", "'score'", "]", "=", "y_logits_list", "[", "i", "]", "\n", "r_item", "[", "'element'", "]", "=", "y_element_list", "[", "i", "]", "\n", "\n", "if", "with_probs", ":", "\n", "            ", "r_item", "[", "'prob'", "]", "=", "y_probs_list", "[", "i", "]", "\n", "\n", "", "result_items_list", ".", "append", "(", "r_item", ")", "\n", "\n", "", "return", "result_items_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_fever_procedure": [[116, 260], ["print", "biterator", "mtr_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "ema.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "biterator", "mtr_p_level.eval_model", "copy.deepcopy", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "fever_sampler.fever_sampler_utils.select_top_k_and_to_results_dict", "utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "evaluation.fever_scorer.fever_doc_only", "print", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "pathlib.Path", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_item_from_dict_to_list_hotpot_style", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.fever_scorer.fever_doc_only", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "def", "eval_fever_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "ema_device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ")", ":", "\n", "    ", "print", "(", "\"Eval FEVER!\"", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "ema_device_num", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "make_int", "=", "True", ",", "with_probs", "=", "True", ")", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "copied_dev_d_list", "=", "copy", ".", "deepcopy", "(", "dev_list", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_5", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.5", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_5", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "# mode = {'standard': False, 'check_doc_id_correct': True}", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_05", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "\n", "cur_results_dict_th0_2", "=", "fever_sampler_utils", ".", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "\n", "score_field_name", "=", "'prob'", ",", "\n", "top_k", "=", "5", ",", "filter_value", "=", "0.2", ")", "\n", "\n", "list_dict_data_tool", ".", "append_item_from_dict_to_list_hotpot_style", "(", "copied_dev_d_list", ",", "\n", "cur_results_dict_th0_2", ",", "\n", "'id'", ",", "'predicted_docids'", ")", "\n", "\n", "strict_score", ",", "pr", ",", "rec", ",", "f1", "=", "fever_scorer", ".", "fever_doc_only", "(", "copied_dev_d_list", ",", "dev_list", ",", "\n", "max_evidence", "=", "5", ")", "\n", "score_02", "=", "{", "\n", "'ss'", ":", "strict_score", ",", "\n", "'pr'", ":", "pr", ",", "'rec'", ":", "rec", ",", "'f1'", ":", "f1", ",", "\n", "}", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'score_02'", ":", "score_02", ",", "\n", "'score_05'", ":", "score_05", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "\n", "s02_ss_score", "=", "score_02", "[", "'ss'", "]", "\n", "s05_ss_score", "=", "score_05", "[", "'ss'", "]", "\n", "\n", "if", "not", "debug_mode", ":", "\n", "            ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|v02_ofever({s02_ss_score})'", "f'|v05_ofever({s05_ss_score})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_hotpot_procedure": [[262, 378], ["print", "biterator", "mtr_p_level.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "mtr_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "mtr_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "ema.get_inference_model", "neural_modules.model_EMA.get_ema_gpu_id_list", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "biterator", "mtr_p_level.eval_model", "copy.deepcopy", "utils.list_dict_data_tool.append_subfield_from_list_to_dict", "mtr_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "mtr_p_level.select_top_k_and_to_results_dict", "hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "evaluation.ext_hotpot_eval.eval", "print", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "str", "logging_agent.incorporate_results", "logging_agent.logging_to_file", "torch.save", "pathlib.Path", "datetime.datetime.now", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.get_ema_gpu_id_list", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.append_subfield_from_list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.select_top_k_and_to_results_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_data_analysis.fullwiki_provided_upperbound.append_gt_downstream_to_get_upperbound_from_doc_retri", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.evaluation.hotpot_evaluate_v1.eval", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.incorporate_results", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.ScoreLogger.logging_to_file"], ["", "", "", "def", "eval_hotpot_procedure", "(", "biterator", ",", "dev_instances", ",", "model", ",", "device_num", ",", "ema_device_num", ",", "\n", "dev_list", ",", "dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ")", ":", "\n", "    ", "print", "(", "\"Eval HOTPOT!\"", ")", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "cur_eval_results_list", "=", "eval_model", "(", "model", ",", "dev_iter", ",", "device_num", ",", "with_probs", "=", "True", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "top5_doc_recall", "=", "metrics_top5", "[", "'doc_recall'", "]", "\n", "top5_UB_sp_recall", "=", "metrics_top5_UB", "[", "'sp_recall'", "]", "\n", "top10_doc_recall", "=", "metrics_top10", "[", "'doc_recall'", "]", "\n", "top10_Ub_sp_recall", "=", "metrics_top10_UB", "[", "'sp_recall'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "if", "not", "debug_mode", ":", "\n", "        ", "save_file_name", "=", "f'i({update_step})|e({epoch_i})'", "f'|t5_doc_recall({top5_doc_recall})|t5_sp_recall({top5_UB_sp_recall})'", "f'|t10_doc_recall({top10_doc_recall})|t5_sp_recall({top10_Ub_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "        ", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "master_device_num", "=", "ema_device_num", "\n", "ema_inference_device_ids", "=", "get_ema_gpu_id_list", "(", "master_device_num", "=", "master_device_num", ")", "\n", "ema_model", "=", "ema_model", ".", "to", "(", "master_device_num", ")", "\n", "ema_model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "ema_model", ",", "device_ids", "=", "ema_inference_device_ids", ")", "\n", "\n", "dev_iter", "=", "biterator", "(", "dev_instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "\n", "cur_eval_results_list", "=", "eval_model", "(", "ema_model", ",", "dev_iter", ",", "master_device_num", ",", "with_probs", "=", "True", ")", "\n", "\n", "copied_dev_o_dict", "=", "copy", ".", "deepcopy", "(", "dev_o_dict", ")", "\n", "list_dict_data_tool", ".", "append_subfield_from_list_to_dict", "(", "cur_eval_results_list", ",", "copied_dev_o_dict", ",", "\n", "'qid'", ",", "'fid'", ",", "check", "=", "True", ")", "\n", "# Top_5", "\n", "cur_results_dict_top5", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "5", ")", "\n", "upperbound_results_dict_top5", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top5", ",", "\n", "dev_list", ")", "\n", "\n", "cur_results_dict_top10", "=", "select_top_k_and_to_results_dict", "(", "copied_dev_o_dict", ",", "top_k", "=", "10", ")", "\n", "upperbound_results_dict_top10", "=", "append_gt_downstream_to_get_upperbound_from_doc_retri", "(", "\n", "cur_results_dict_top10", ",", "\n", "dev_list", ")", "\n", "\n", "_", ",", "metrics_top5", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top5_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top5", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "_", ",", "metrics_top10", "=", "ext_hotpot_eval", ".", "eval", "(", "cur_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "_", ",", "metrics_top10_UB", "=", "ext_hotpot_eval", ".", "eval", "(", "upperbound_results_dict_top10", ",", "dev_list", ",", "verbose", "=", "False", ")", "\n", "\n", "top5_doc_recall", "=", "metrics_top5", "[", "'doc_recall'", "]", "\n", "top5_UB_sp_recall", "=", "metrics_top5_UB", "[", "'sp_recall'", "]", "\n", "top10_doc_recall", "=", "metrics_top10", "[", "'doc_recall'", "]", "\n", "top10_Ub_sp_recall", "=", "metrics_top10_UB", "[", "'sp_recall'", "]", "\n", "\n", "logging_item", "=", "{", "\n", "'label'", ":", "'ema'", ",", "\n", "'step:'", ":", "update_step", ",", "\n", "'epoch'", ":", "epoch_i", ",", "\n", "'top5'", ":", "metrics_top5", ",", "\n", "'top5_UB'", ":", "metrics_top5_UB", ",", "\n", "'top10'", ":", "metrics_top10", ",", "\n", "'top10_UB'", ":", "metrics_top10_UB", ",", "\n", "'time'", ":", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", "}", "\n", "\n", "print", "(", "logging_item", ")", "\n", "if", "not", "debug_mode", ":", "\n", "            ", "save_file_name", "=", "f'ema_i({update_step})|e({epoch_i})'", "f'|t5_doc_recall({top5_doc_recall})|t5_sp_recall({top5_UB_sp_recall})'", "f'|t10_doc_recall({top10_doc_recall})|t5_sp_recall({top10_Ub_sp_recall})|seed({seed})'", "\n", "\n", "# print(save_file_name)", "\n", "logging_agent", ".", "incorporate_results", "(", "{", "}", ",", "save_file_name", ",", "logging_item", ")", "\n", "logging_agent", ".", "logging_to_file", "(", "Path", "(", "file_path_prefix", ")", "/", "\"log.json\"", ")", "\n", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.multitask_model_go": [[380, 596], ["torch.manual_seed", "int", "torch.device", "torch.cuda.device_count", "data_utils.exvocab.ExVocabulary", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.add_token_to_namespace", "data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "utils.common.load_json", "utils.common.load_json", "utils.list_dict_data_tool.list_to_dict", "utils.common.load_jsonl", "utils.common.load_jsonl", "hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "utils.common.load_jsonl", "utils.list_dict_data_tool.list_to_dict", "utils.common.load_jsonl", "utils.common.load_jsonl", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "len", "len", "print", "print", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader", "pytorch_pretrained_bert.BertModel.from_pretrained", "bert_model_variances.bert_multilayer_output.BertMultiLayerSeqClassification", "torch.nn.DataParallel.to", "list", "print", "print", "pytorch_pretrained_bert.BertAdam", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "utils.save_tool.ScoreLogger", "range", "torch.cuda.is_available", "neural_modules.model_EMA.EMA", "torch.nn.DataParallel", "torch.nn.DataParallel.named_parameters", "int", "utils.save_tool.gen_file_prefix", "os.path.basename", "print", "hotpot_fact_selection_sampler.sampler_full_wiki.down_sample_neg", "random.shuffle", "data_utils.readers.bert_reader_content_selection.BertContentSelectionReader.read", "allennlp.data.iterators.BasicIterator.", "tqdm.tqdm", "print", "torch.save", "torch.cuda.is_available", "torch.nn.DataParallel.named_parameters", "open", "open", "out_f.write", "out_f.flush", "torch.nn.DataParallel.train", "allennlp.nn.util.move_to_device", "flint.torch_util.get_length_and_mask", "torch.nn.DataParallel.", "loss.mean.backward", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "print", "neural_modules.model_EMA.EMA.get_inference_model", "torch.save", "os.path.join", "it.read", "loss.mean.mean", "pytorch_pretrained_bert.BertAdam.step", "pytorch_pretrained_bert.BertAdam.zero_grad", "hasattr", "pathlib.Path", "model_to_save.state_dict", "str", "len", "any", "neural_modules.model_EMA.EMA.", "print", "mtr_p_level.eval_fever_procedure", "mtr_p_level.eval_hotpot_procedure", "any", "hasattr", "updated_model.named_parameters"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.data_utils.exvocab.ExVocabulary.change_token_with_index_to_namespace", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_json", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sampler_utils.field_name_convert", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.list_dict_data_tool.list_to_dict", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.common.load_jsonl", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.fever_sampler.fever_p_level_sampler.get_paragraph_forward_pair", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.utils.save_tool.gen_file_prefix", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.hotpot_fact_selection_sampler.sentence_level_sampler.down_sample_neg", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.flint.torch_util.get_length_and_mask", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.neural_modules.model_EMA.EMA.get_inference_model", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_fever_procedure", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.multi_task_retrieval.mtr_p_level.eval_hotpot_procedure"], ["", "", "", "def", "multitask_model_go", "(", ")", ":", "\n", "    ", "seed", "=", "12", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "# bert_model_name = 'bert-large-uncased'", "\n", "bert_pretrain_path", "=", "config", ".", "PRO_ROOT", "/", "'.pytorch_pretrained_bert'", "\n", "bert_model_name", "=", "'bert-base-uncased'", "\n", "lazy", "=", "False", "\n", "# lazy = True", "\n", "forward_size", "=", "64", "\n", "# batch_size = 64", "\n", "batch_size", "=", "128", "\n", "gradient_accumulate_step", "=", "int", "(", "batch_size", "/", "forward_size", ")", "\n", "warmup_proportion", "=", "0.1", "\n", "learning_rate", "=", "5e-5", "\n", "num_train_epochs", "=", "1", "\n", "eval_frequency", "=", "5000", "\n", "hotpot_pos_ratio", "=", "0.2", "\n", "do_lower_case", "=", "True", "\n", "max_l", "=", "264", "\n", "\n", "experiment_name", "=", "f'mtr_p_level_(num_train_epochs:{num_train_epochs})'", "\n", "\n", "debug_mode", "=", "False", "\n", "do_ema", "=", "True", "\n", "# est_datasize = 900_000", "\n", "\n", "num_class", "=", "1", "\n", "# num_train_optimization_steps", "\n", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "device_num", "=", "0", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "-", "1", "\n", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "unk_token_num", "=", "{", "'tokens'", ":", "1", "}", "# work around for initiating vocabulary.", "\n", "vocab", "=", "ExVocabulary", "(", "unk_token_num", "=", "unk_token_num", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"false\"", ",", "namespace", "=", "\"labels\"", ")", "# 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"true\"", ",", "namespace", "=", "\"labels\"", ")", "# 1", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"hidden\"", ",", "namespace", "=", "\"labels\"", ")", "\n", "vocab", ".", "change_token_with_index_to_namespace", "(", "\"hidden\"", ",", "-", "2", ",", "namespace", "=", "'labels'", ")", "\n", "\n", "# Load Hotpot Dataset", "\n", "hotpot_train_list", "=", "common", ".", "load_json", "(", "config", ".", "TRAIN_FILE", ")", "\n", "hotpot_dev_list", "=", "common", ".", "load_json", "(", "config", ".", "DEV_FULLWIKI_FILE", ")", "\n", "hotpot_dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "hotpot_dev_list", ",", "'_id'", ")", "\n", "\n", "# Load Hotpot upstream paragraph forward item", "\n", "hotpot_dev_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_dev_p_level_unlabeled.jsonl\"", ")", "\n", "hotpot_train_fitems_list", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PDATA_ROOT", "/", "\"content_selection_forward\"", "/", "\"hotpot_train_p_level_labeled.jsonl\"", ")", "\n", "\n", "hotpot_train_fitems_list", "=", "hotpot_sampler_utils", ".", "field_name_convert", "(", "hotpot_train_fitems_list", ",", "'doc_t'", ",", "'element'", ")", "\n", "hotpot_dev_fitems_list", "=", "hotpot_sampler_utils", ".", "field_name_convert", "(", "hotpot_dev_fitems_list", ",", "'doc_t'", ",", "'element'", ")", "\n", "\n", "# Load FEVER Dataset", "\n", "# fever_train_list = common.load_json(config.FEVER_TRAIN)", "\n", "fever_dev_list", "=", "common", ".", "load_jsonl", "(", "config", ".", "FEVER_DEV", ")", "\n", "fever_dev_o_dict", "=", "list_dict_data_tool", ".", "list_to_dict", "(", "fever_dev_list", ",", "'id'", ")", "\n", "\n", "train_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_train.jsonl\"", ")", "\n", "dev_ruleterm_doc_results", "=", "common", ".", "load_jsonl", "(", "\n", "config", ".", "PRO_ROOT", "/", "\"results/doc_retri_results/fever_results/merged_doc_results/m_doc_dev.jsonl\"", ")", "\n", "\n", "fever_train_fitems_list", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'train'", ",", "train_ruleterm_doc_results", ",", "\n", "is_training", "=", "True", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "True", ")", "\n", "fever_dev_fitems_list", "=", "fever_p_level_sampler", ".", "get_paragraph_forward_pair", "(", "'dev'", ",", "dev_ruleterm_doc_results", ",", "\n", "is_training", "=", "False", ",", "debug", "=", "debug_mode", ",", "\n", "ignore_non_verifiable", "=", "False", ")", "\n", "if", "debug_mode", ":", "\n", "        ", "hotpot_dev_list", "=", "hotpot_dev_list", "[", ":", "10", "]", "\n", "hotpot_dev_fitems_list", "=", "hotpot_dev_fitems_list", "[", ":", "296", "]", "\n", "hotpot_train_fitems_list", "=", "hotpot_train_fitems_list", "[", ":", "300", "]", "\n", "\n", "fever_dev_list", "=", "fever_dev_list", "[", ":", "100", "]", "\n", "eval_frequency", "=", "2", "\n", "\n", "# Down_sample for hotpot.", "\n", "", "hotpot_sampled_train_list", "=", "down_sample_neg", "(", "hotpot_train_fitems_list", ",", "ratio", "=", "hotpot_pos_ratio", ")", "\n", "hotpot_est_datasize", "=", "len", "(", "hotpot_sampled_train_list", ")", "\n", "fever_est_datasize", "=", "len", "(", "fever_train_fitems_list", ")", "\n", "\n", "print", "(", "\"Hotpot Train Size:\"", ",", "hotpot_est_datasize", ")", "\n", "print", "(", "\"Fever Train Size:\"", ",", "fever_est_datasize", ")", "\n", "\n", "est_datasize", "=", "hotpot_est_datasize", "+", "fever_est_datasize", "\n", "\n", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ",", "do_lower_case", "=", "do_lower_case", ",", "\n", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "bert_cs_reader", "=", "BertContentSelectionReader", "(", "bert_tokenizer", ",", "lazy", ",", "is_paired", "=", "True", ",", "\n", "example_filter", "=", "lambda", "x", ":", "len", "(", "x", "[", "'context'", "]", ")", "==", "0", ",", "max_l", "=", "max_l", ",", "\n", "element_fieldname", "=", "'element'", ")", "\n", "\n", "bert_encoder", "=", "BertModel", ".", "from_pretrained", "(", "bert_model_name", ",", "cache_dir", "=", "bert_pretrain_path", ")", "\n", "model", "=", "BertMultiLayerSeqClassification", "(", "bert_encoder", ",", "num_labels", "=", "num_class", ",", "num_of_pooling_layer", "=", "1", ",", "\n", "act_type", "=", "'tanh'", ",", "use_pretrained_pooler", "=", "True", ",", "use_sigmoid", "=", "True", ")", "\n", "\n", "ema", "=", "None", "\n", "if", "do_ema", ":", "\n", "        ", "ema", "=", "EMA", "(", "model", ",", "model", ".", "named_parameters", "(", ")", ",", "device_num", "=", "1", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "#", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "est_datasize", "/", "forward_size", "/", "gradient_accumulate_step", ")", "*", "num_train_epochs", "\n", "\n", "if", "debug_mode", ":", "\n", "        ", "num_train_optimization_steps", "=", "100", "\n", "\n", "", "print", "(", "\"Estimated training size\"", ",", "est_datasize", ")", "\n", "print", "(", "\"Number of optimization steps:\"", ",", "num_train_optimization_steps", ")", "\n", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "learning_rate", ",", "\n", "warmup", "=", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "hotpot_dev_instances", "=", "bert_cs_reader", ".", "read", "(", "hotpot_dev_fitems_list", ")", "\n", "fever_dev_instances", "=", "bert_cs_reader", ".", "read", "(", "fever_dev_fitems_list", ")", "\n", "\n", "biterator", "=", "BasicIterator", "(", "batch_size", "=", "forward_size", ")", "\n", "biterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "forbackward_step", "=", "0", "\n", "update_step", "=", "0", "\n", "\n", "logging_agent", "=", "save_tool", ".", "ScoreLogger", "(", "{", "}", ")", "\n", "\n", "file_path_prefix", "=", "'.'", "\n", "if", "not", "debug_mode", ":", "\n", "# # # Create Log File", "\n", "        ", "file_path_prefix", ",", "date", "=", "save_tool", ".", "gen_file_prefix", "(", "f\"{experiment_name}\"", ")", "\n", "# Save the source code.", "\n", "script_name", "=", "os", ".", "path", ".", "basename", "(", "__file__", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "file_path_prefix", ",", "script_name", ")", ",", "'w'", ")", "as", "out_f", ",", "open", "(", "__file__", ",", "'r'", ")", "as", "it", ":", "\n", "            ", "out_f", ".", "write", "(", "it", ".", "read", "(", ")", ")", "\n", "out_f", ".", "flush", "(", ")", "\n", "# # # Log File end", "\n", "\n", "", "", "for", "epoch_i", "in", "range", "(", "num_train_epochs", ")", ":", "\n", "        ", "print", "(", "\"Epoch:\"", ",", "epoch_i", ")", "\n", "# sampled_train_list = down_sample_neg(train_fitems_list, ratio=pos_ratio)", "\n", "hotpot_sampled_train_list", "=", "down_sample_neg", "(", "hotpot_train_fitems_list", ",", "ratio", "=", "hotpot_pos_ratio", ")", "\n", "all_train_data", "=", "hotpot_sampled_train_list", "+", "fever_train_fitems_list", "\n", "random", ".", "shuffle", "(", "all_train_data", ")", "\n", "train_instance", "=", "bert_cs_reader", ".", "read", "(", "all_train_data", ")", "\n", "train_iter", "=", "biterator", "(", "train_instance", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "batch", "in", "tqdm", "(", "train_iter", ")", ":", "\n", "            ", "model", ".", "train", "(", ")", "\n", "batch", "=", "move_to_device", "(", "batch", ",", "device_num", ")", "\n", "\n", "paired_sequence", "=", "batch", "[", "'paired_sequence'", "]", "\n", "paired_segments_ids", "=", "batch", "[", "'paired_segments_ids'", "]", "\n", "labels_ids", "=", "batch", "[", "'label'", "]", "\n", "att_mask", ",", "_", "=", "torch_util", ".", "get_length_and_mask", "(", "paired_sequence", ")", "\n", "s1_span", "=", "batch", "[", "'bert_s1_span'", "]", "\n", "s2_span", "=", "batch", "[", "'bert_s2_span'", "]", "\n", "\n", "loss", "=", "model", "(", "paired_sequence", ",", "token_type_ids", "=", "paired_segments_ids", ",", "attention_mask", "=", "att_mask", ",", "\n", "mode", "=", "BertMultiLayerSeqClassification", ".", "ForwardMode", ".", "TRAIN", ",", "\n", "labels", "=", "labels_ids", ")", "\n", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "\n", "", "if", "gradient_accumulate_step", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "gradient_accumulate_step", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "forbackward_step", "+=", "1", "\n", "\n", "if", "forbackward_step", "%", "gradient_accumulate_step", "==", "0", ":", "\n", "                ", "optimizer", ".", "step", "(", ")", "\n", "if", "ema", "is", "not", "None", "and", "do_ema", ":", "\n", "                    ", "updated_model", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "ema", "(", "updated_model", ".", "named_parameters", "(", ")", ")", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "update_step", "+=", "1", "\n", "\n", "if", "update_step", "%", "eval_frequency", "==", "0", ":", "\n", "                    ", "print", "(", "\"Update steps:\"", ",", "update_step", ")", "\n", "# Eval FEVER", "\n", "eval_fever_procedure", "(", "biterator", ",", "fever_dev_instances", ",", "model", ",", "device_num", ",", "1", ",", "fever_dev_list", ",", "\n", "fever_dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "\n", "do_ema", ",", "ema", ",", "seed", ")", "\n", "eval_hotpot_procedure", "(", "biterator", ",", "hotpot_dev_instances", ",", "model", ",", "device_num", ",", "1", ",", "hotpot_dev_list", ",", "\n", "hotpot_dev_o_dict", ",", "debug_mode", ",", "logging_agent", ",", "update_step", ",", "epoch_i", ",", "\n", "file_path_prefix", ",", "do_ema", ",", "ema", ",", "seed", ")", "\n", "\n", "", "", "", "", "if", "not", "debug_mode", ":", "\n", "        ", "print", "(", "\"Final Saving.\"", ")", "\n", "save_file_name", "=", "f'i({update_step})|e({num_train_epochs})_final_model'", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n", "if", "do_ema", "and", "ema", "is", "not", "None", ":", "\n", "            ", "print", "(", "\"Final EMA Saving\"", ")", "\n", "ema_model", "=", "ema", ".", "get_inference_model", "(", ")", "\n", "save_file_name", "=", "f'i({update_step})|e({num_train_epochs})_final_ema_model'", "\n", "model_to_save", "=", "ema_model", ".", "module", "if", "hasattr", "(", "ema_model", ",", "'module'", ")", "else", "ema_model", "\n", "output_model_file", "=", "Path", "(", "file_path_prefix", ")", "/", "save_file_name", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "str", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.iterative_save_all_title": [[19, 30], ["wiki_util.wiki_db_tool.get_cursor", "wiki_db_tool.get_cursor.execute", "open", "tqdm.tqdm", "json.loads", "out_f.write"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor"], ["def", "iterative_save_all_title", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "wiki_db_tool", ".", "TOTAL_ARTICLE_NUMBER_ABS", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "a_cursor", "=", "wiki_db_tool", ".", "get_cursor", "(", "config", ".", "ABS_WIKI_DB", ")", "\n", "a_cursor", ".", "execute", "(", "\"SELECT * from unnamed\"", ")", "\n", "\n", "with", "open", "(", "config", ".", "WIKI_TITLE_SET_FILE", ",", "mode", "=", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out_f", ":", "\n", "        ", "for", "key", ",", "value", "in", "tqdm", "(", "a_cursor", ",", "total", "=", "total_doc_num", ")", ":", "\n", "            ", "item", "=", "json", ".", "loads", "(", "value", ")", "\n", "cur_count", "+=", "1", "\n", "out_f", ".", "write", "(", "item", "[", "'title'", "]", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_disamb_match": [[32, 39], ["list", "disamb_pattern.finditer", "len", "match.group", "match.group"], "function", ["None"], ["", "", "", "def", "get_disamb_match", "(", "text_in", ")", ":", "\n", "    ", "matches", "=", "list", "(", "disamb_pattern", ".", "finditer", "(", "text_in", ")", ")", "\n", "if", "len", "(", "matches", ")", "==", "0", ":", "\n", "        ", "return", "None", ",", "None", "\n", "", "else", ":", "\n", "        ", "match", "=", "matches", "[", "0", "]", "\n", "return", "match", ".", "group", "(", "0", ")", ",", "match", ".", "group", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_title_entity_set": [[41, 75], ["dict", "set", "dict", "open", "print", "tqdm.tqdm", "line.strip", "set.add", "title_entities_set.get_disamb_match", "disambiguation_group[].add", "utils.text_process_tool.normalize", "disambiguation_group[].add", "set", "utils.text_process_tool.normalize"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.title_entities_set.get_disamb_match", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_rvindex.DocumentLengthTable.add", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.build_rindex.build_wiki_rindex.normalize"], ["", "", "def", "get_title_entity_set", "(", ")", ":", "\n", "    ", "global", "title_entity_set", "\n", "global", "disambiguation_group", "\n", "global", "normalization_mapping", "\n", "if", "disambiguation_group", "is", "None", ":", "\n", "        ", "disambiguation_group", "=", "dict", "(", ")", "\n", "", "if", "title_entity_set", "is", "None", ":", "\n", "        ", "title_entity_set", "=", "set", "(", ")", "\n", "with", "open", "(", "config", ".", "WIKI_TITLE_SET_FILE", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "in_f", ":", "\n", "            ", "print", "(", "\"Get title entity set from\"", ",", "config", ".", "WIKI_TITLE_SET_FILE", ")", "\n", "for", "line", "in", "tqdm", "(", "in_f", ")", ":", "\n", "                ", "cur_title", "=", "line", ".", "strip", "(", ")", "\n", "title_entity_set", ".", "add", "(", "cur_title", ")", "\n", "\n", "# Build disambiguation groups", "\n", "dis_whole", ",", "dis_org", "=", "get_disamb_match", "(", "cur_title", ")", "\n", "if", "dis_whole", "is", "not", "None", ":", "\n", "                    ", "if", "dis_org", "not", "in", "disambiguation_group", ":", "\n", "                        ", "disambiguation_group", "[", "dis_org", "]", "=", "set", "(", ")", "\n", "", "disambiguation_group", "[", "dis_org", "]", ".", "add", "(", "dis_whole", ")", "\n", "\n", "", "", "", "for", "title", "in", "title_entity_set", ":", "\n", "            ", "if", "title", "in", "disambiguation_group", ":", "\n", "                ", "disambiguation_group", "[", "title", "]", ".", "add", "(", "title", ")", "\n", "\n", "", "", "", "if", "normalization_mapping", "is", "None", ":", "\n", "        ", "normalization_mapping", "=", "dict", "(", ")", "\n", "for", "title", "in", "title_entity_set", ":", "\n", "            ", "if", "title", "!=", "text_process_tool", ".", "normalize", "(", "title", ")", ":", "\n", "                ", "normalization_mapping", "[", "text_process_tool", ".", "normalize", "(", "title", ")", "]", "=", "title", "\n", "\n", "", "", "return", "title_entity_set", "\n", "", "else", ":", "\n", "        ", "return", "title_entity_set", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.item_get_clean_text": [[15, 46], ["zip", "len", "len", "zip", "article_tokens.append", "len", "len", "paragraph_tokens.append", "cur_token.startswith", "len", "cur_sent_tokens.append"], "function", ["None"], ["def", "item_get_clean_text", "(", "item", ",", "hyperlinks", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "article_tokens", "=", "[", "]", "# article level hyperlinks", "\n", "\n", "# Article level", "\n", "for", "paragraph", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "# paragraph is a list of strings (each indicates one sentence), and paragraph_offsets is a list of tuples.", "\n", "        ", "assert", "len", "(", "paragraph", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraph", ")", "# Text of the whole paragraph", "\n", "\n", "paragraph_tokens", "=", "[", "]", "# paragraph level tokens", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraph", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list (indicating token range)", "\n", "            ", "cur_sent_tokens", "=", "[", "]", "# This is with all the hyperlink-tag deleted.", "\n", "\n", "for", "start", ",", "end", "in", "sentence_offsets", ":", "\n", "                ", "cur_token", "=", "paragraph_text", "[", "start", ":", "end", "]", "\n", "\n", "if", "cur_token", ".", "startswith", "(", "\"<a href=\\\"\"", ")", ":", "\n", "                    ", "continue", "\n", "", "elif", "cur_token", "==", "\"</a>\"", "or", "(", "cur_token", "==", "\"\u00bb\"", "and", "paragraph_text", "[", "end", "-", "4", ":", "end", "]", "==", "\"</a\u00bb\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "len", "(", "cur_token", ")", "!=", "0", ":", "\n", "                    ", "cur_sent_tokens", ".", "append", "(", "cur_token", ")", "# We keep all the tokens.", "\n", "\n", "", "", "paragraph_tokens", ".", "append", "(", "cur_sent_tokens", ")", "\n", "\n", "", "article_tokens", ".", "append", "(", "paragraph_tokens", ")", "# End article level", "\n", "", "return", "article_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.extract_hyperlink_and_append": [[48, 188], ["zip", "enumerate", "len", "len", "zip", "article_hyperlinks.append", "len", "len", "zip", "zip", "len", "len", "paragraph_hyperlinks.append", "all_paragraph_hyperlinks.extend", "len", "len", "fixed_paragraph_hyperlinks.append", "fixed_article_hyperlinks.append", "cur_sent_tokens.append", "cur_token.startswith", "fixed_sentence_hyperlinks.append", "lxml.etree.fromstring", "etree.fromstring.get", "sentence_hyperlinks.append", "print", "print", "results_hyperlinks_text.append", "sentence_hyperlinks.append", "Hyperlink", "lxml.etree.fromstring", "urllib.parse.unquote", "Hyperlink", "etree.fromstring.get", "lxml.etree.fromstring", "urllib.parse.unquote", "etree.fromstring.get"], "function", ["None"], ["", "def", "extract_hyperlink_and_append", "(", "item", ")", ":", "\n", "# This function will return one more field (append to item later) in which the extracted hyperlink lies.", "\n", "# The new field will be as structure as text, each sentence will have a list of hyperlinks.", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "article_hyperlinks", "=", "[", "]", "# article level hyperlinks", "\n", "\n", "# Article level", "\n", "for", "paragraph", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "# paragraph is a list of strings (each indicates one sentence), and paragraph_offsets is a list of tuples.", "\n", "        ", "assert", "len", "(", "paragraph", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraph", ")", "# Text of the whole paragraph", "\n", "\n", "paragraph_hyperlinks", "=", "[", "]", "# paragraph level hyperlinks", "\n", "\n", "in_hyperlink", "=", "False", "\n", "start_hyperlink_index", "=", "0", "\n", "start_inner_hyperlink_index", "=", "0", "\n", "end_hypoerlink_index", "=", "0", "\n", "end_inner_hyperlink_index", "=", "0", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraph", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list (indicating token range)", "\n", "            ", "cur_sent_tokens", "=", "[", "]", "# This is with all the hyperlink-tag none-deleted.", "\n", "results_hyperlinks_text", "=", "[", "]", "# This variable will be useless, not for probing.", "\n", "sentence_hyperlinks", "=", "[", "]", "\n", "# sentence-level hyperlinks, (according to the end position.) this will be fixed laster.", "\n", "\n", "for", "start", ",", "end", "in", "sentence_offsets", ":", "\n", "                ", "cur_token", "=", "paragraph_text", "[", "start", ":", "end", "]", "\n", "cur_sent_tokens", ".", "append", "(", "cur_token", ")", "# We keep all the tokens.", "\n", "\n", "if", "cur_token", ".", "startswith", "(", "\"<a href=\\\"\"", ")", ":", "\n", "                    ", "if", "\"<a href=\\\"http%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"https%3A//\"", "in", "cur_token", "or", "\"<a href=\\\"//\"", "in", "cur_token", ":", "# Ignore external links.", "\n", "                        ", "continue", "\n", "\n", "", "if", "in_hyperlink", ":", "\n", "# We didn't find a correct \"</a>\" to close the last encountered hyperlink", "\n", "                        ", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "hl_head", "=", "etree", ".", "fromstring", "(", "cur_hyperlink_start_token", "+", "\"</a>\"", ")", "\n", "hl_href", "=", "hl_head", ".", "get", "(", "'href'", ")", "\n", "\n", "# Here is an important addition of empty text hyperlink", "\n", "# We only append the token head here.", "\n", "sentence_hyperlinks", ".", "append", "(", "Hyperlink", "(", "inner_text", "=", "\"\"", ",", "href", "=", "hl_href", ",", "\n", "start_position", "=", "start_hyperlink_index", ",", "\n", "end_position", "=", "start_inner_hyperlink_index", ",", "\n", "start_inner_text_position", "=", "start_inner_hyperlink_index", ",", "\n", "end_inner_text_position", "=", "start_inner_hyperlink_index", ")", ")", "\n", "\n", "# in_hyperlink = True", "\n", "\n", "# print(item)", "\n", "# print(item['title'])", "\n", "# print(sentence)", "\n", "# print(cur_sent_tokens)", "\n", "# print(cur_token)", "\n", "print", "(", "hl_href", ")", "\n", "print", "(", "\"Potential Error. We didn't find a correct '</a>' to close the last hyperlink.\"", ")", "\n", "\n", "", "in_hyperlink", "=", "True", "\n", "start_hyperlink_index", "=", "start", "\n", "start_inner_hyperlink_index", "=", "end", "\n", "\n", "", "elif", "cur_token", "==", "\"</a>\"", "or", "(", "cur_token", "==", "\"\u00bb\"", "and", "paragraph_text", "[", "end", "-", "4", ":", "end", "]", "==", "\"</a\u00bb\"", ")", ":", "\n", "                    ", "if", "not", "in_hyperlink", ":", "\n", "                        ", "continue", "# Fail to reveal the start.   We just ignore.", "\n", "\n", "", "in_hyperlink", "=", "False", "\n", "end_inner_hyperlink_index", "=", "start", "\n", "end_hypoerlink_index", "=", "end", "\n", "\n", "cur_hyperlink_start_token", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "start_inner_hyperlink_index", "]", "\n", "cur_hyperlink", "=", "paragraph_text", "[", "start_hyperlink_index", ":", "end_hypoerlink_index", "]", "\n", "cur_hyperlink_inner_text", "=", "paragraph_text", "[", "start_inner_hyperlink_index", ":", "end_inner_hyperlink_index", "]", "\n", "\n", "results_hyperlinks_text", ".", "append", "(", "cur_hyperlink", ")", "\n", "# print(cur_hyperlink)", "\n", "try", ":", "\n", "                        ", "hl", "=", "etree", ".", "fromstring", "(", "cur_hyperlink", ")", "\n", "assert", "hl", ".", "text", "==", "cur_hyperlink_inner_text", "\n", "hl_href", "=", "urllib", ".", "parse", ".", "unquote", "(", "hl", ".", "get", "(", "'href'", ")", ")", "\n", "", "except", ":", "\n", "                        ", "hl_head", "=", "etree", ".", "fromstring", "(", "cur_hyperlink_start_token", "+", "\"</a>\"", ")", "\n", "hl_href", "=", "urllib", ".", "parse", ".", "unquote", "(", "hl_head", ".", "get", "(", "'href'", ")", ")", "\n", "\n", "# Append the (hyperlink + [start:end])  w.r.t the current paragraph.", "\n", "", "sentence_hyperlinks", ".", "append", "(", "Hyperlink", "(", "inner_text", "=", "cur_hyperlink_inner_text", ",", "href", "=", "hl_href", ",", "\n", "start_position", "=", "start_hyperlink_index", ",", "\n", "end_position", "=", "end_hypoerlink_index", ",", "\n", "start_inner_text_position", "=", "start_inner_hyperlink_index", ",", "\n", "end_inner_text_position", "=", "end_inner_hyperlink_index", ")", ")", "\n", "# sentence_hyperlinks.append((Hyperlink(cur_hyperlink_inner_text, hl_href),", "\n", "#                             [start_hyperlink_index, end_hypoerlink_index,", "\n", "#                              start_inner_hyperlink_index, end_inner_hyperlink_index]))", "\n", "\n", "", "", "paragraph_hyperlinks", ".", "append", "(", "sentence_hyperlinks", ")", "# End paragraph level", "\n", "\n", "", "article_hyperlinks", ".", "append", "(", "paragraph_hyperlinks", ")", "# End article level", "\n", "\n", "# Hyperlink offset fixing.", "\n", "", "assert", "len", "(", "article_hyperlinks", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "\"\"\"\n    The process above extract the hyperlinks and append them to the sentences.\n    However, which sentence the hyperlink appended to is positioned according to the end position of the hyperlink.\n    But, we want to append the hyperlink according to the start position of inner text. (Just like how human read webpage.)\n    \"\"\"", "\n", "\n", "fixed_article_hyperlinks", "=", "[", "]", "\n", "# fixed_paragraph_hyperlinks = None", "\n", "for", "i", ",", "(", "paragraph_hyperlinks", ",", "paragraph_offsets", ")", "in", "enumerate", "(", "zip", "(", "article_hyperlinks", ",", "item", "[", "\"charoffset\"", "]", ")", ")", ":", "\n", "        ", "fixed_paragraph_hyperlinks", "=", "[", "]", "\n", "all_paragraph_hyperlinks", "=", "[", "]", "\n", "paragraph_text", "=", "''", ".", "join", "(", "item", "[", "'text'", "]", "[", "i", "]", ")", "# Text of the whole paragraph", "\n", "\n", "for", "sentence_hyperlinks", "in", "paragraph_hyperlinks", ":", "\n", "            ", "all_paragraph_hyperlinks", ".", "extend", "(", "sentence_hyperlinks", ")", "\n", "#", "\n", "", "assert", "len", "(", "paragraph_hyperlinks", ")", "==", "len", "(", "paragraph_offsets", ")", "# Number of sentences", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraph_hyperlinks", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list (indicating token range)", "\n", "            ", "fixed_sentence_hyperlinks", "=", "[", "]", "\n", "\n", "sentence_start", "=", "sentence_offsets", "[", "0", "]", "[", "0", "]", "\n", "sentence_end", "=", "sentence_offsets", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "for", "hyperlink", "in", "all_paragraph_hyperlinks", ":", "\n", "                ", "cur_hlink_inner_text_start", "=", "hyperlink", ".", "start_inner_text_position", "\n", "cur_hlink_inner_text_end", "=", "hyperlink", ".", "end_inner_text_position", "\n", "\n", "if", "cur_hlink_inner_text_start", ">=", "sentence_start", "and", "cur_hlink_inner_text_end", "<=", "sentence_end", ":", "\n", "                    ", "fixed_sentence_hyperlinks", ".", "append", "(", "hyperlink", ")", "\n", "\n", "# print(paragraph_text[sentence_start:sentence_end])", "\n", "# print(fixed_sentence_hyperlinks)", "\n", "", "", "fixed_paragraph_hyperlinks", ".", "append", "(", "fixed_sentence_hyperlinks", ")", "\n", "\n", "", "fixed_article_hyperlinks", ".", "append", "(", "fixed_paragraph_hyperlinks", ")", "if", "fixed_paragraph_hyperlinks", "is", "not", "None", "else", "None", "\n", "\n", "", "return", "fixed_article_hyperlinks", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.iterative_processing": [[190, 211], ["print", "open", "tqdm.tqdm", "json.loads", "hyperlink.extract_hyperlink_and_append", "print", "print", "inspect_wikidump.inspect_whole_file.check_boundary"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.extract_hyperlink_and_append", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.check_boundary"], ["", "def", "iterative_processing", "(", "debug_num", "=", "None", ")", ":", "\n", "    ", "total_doc_num", "=", "init_inspect", ".", "TOTAL_NUM_DOC", "if", "debug_num", "is", "None", "else", "debug_num", "\n", "cur_count", "=", "0", "\n", "\n", "with", "open", "(", "config", ".", "WHOLE_WIKI_FILE", ",", "'rb'", ")", "as", "in_f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "in_f", ",", "total", "=", "total_doc_num", ")", ":", "\n", "\n", "            ", "if", "debug_num", "is", "not", "None", "and", "debug_num", "==", "cur_count", ":", "\n", "                ", "break", "\n", "\n", "", "item", "=", "json", ".", "loads", "(", "line", ",", "encoding", "=", "'utf-8'", ")", "\n", "cur_count", "+=", "1", "\n", "\n", "if", "not", "check_boundary", "(", "item", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "item_hyperlinks", "=", "extract_hyperlink_and_append", "(", "item", ")", "\n", "print", "(", "item", ")", "\n", "print", "(", "item_hyperlinks", ")", "\n", "\n", "", "", "print", "(", "\"Total Count:\"", ",", "cur_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.item_get_clean_text": [[17, 48], ["zip", "len", "len", "zip", "article_tokens.append", "len", "len", "paragraph_tokens.append", "cur_token.startswith", "len", "cur_sent_tokens.append"], "function", ["None"], ["def", "item_get_clean_text", "(", "item", ",", "hyperlinks", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "item", "[", "'text'", "]", ")", "==", "len", "(", "item", "[", "\"charoffset\"", "]", ")", "\n", "article_tokens", "=", "[", "]", "# article level hyperlinks", "\n", "\n", "# Article level", "\n", "for", "paragraph", ",", "paragraph_offsets", "in", "zip", "(", "item", "[", "'text'", "]", ",", "item", "[", "\"charoffset\"", "]", ")", ":", "# Paragraph level", "\n", "# paragraph is a list of strings (each indicates one sentence), and paragraph_offsets is a list of tuples.", "\n", "        ", "assert", "len", "(", "paragraph", ")", "==", "len", "(", "paragraph_offsets", ")", "\n", "paragraph_text", "=", "''", ".", "join", "(", "paragraph", ")", "# Text of the whole paragraph", "\n", "\n", "paragraph_tokens", "=", "[", "]", "# paragraph level tokens", "\n", "\n", "for", "sentence", ",", "sentence_offsets", "in", "zip", "(", "paragraph", ",", "paragraph_offsets", ")", ":", "\n", "# sentence is str, sentece_offset: list of tuple_list (indicating token range)", "\n", "            ", "cur_sent_tokens", "=", "[", "]", "# This is with all the hyperlink-tag deleted.", "\n", "\n", "for", "start", ",", "end", "in", "sentence_offsets", ":", "\n", "                ", "cur_token", "=", "paragraph_text", "[", "start", ":", "end", "]", "\n", "\n", "if", "cur_token", ".", "startswith", "(", "\"<a href=\\\"\"", ")", ":", "\n", "                    ", "continue", "\n", "", "elif", "cur_token", "==", "\"</a>\"", "or", "(", "cur_token", "==", "\"\u00bb\"", "and", "paragraph_text", "[", "end", "-", "4", ":", "end", "]", "==", "\"</a\u00bb\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "len", "(", "cur_token", ")", "!=", "0", ":", "\n", "                    ", "cur_sent_tokens", ".", "append", "(", "cur_token", ")", "# We keep all the tokens.", "\n", "\n", "", "", "paragraph_tokens", ".", "append", "(", "cur_sent_tokens", ")", "\n", "\n", "", "article_tokens", ".", "append", "(", "paragraph_tokens", ")", "# End article level", "\n", "", "return", "article_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_cursor": [[50, 56], ["sqlite3.connect", "sqlite3.connect.cursor", "isinstance", "str"], "function", ["None"], ["", "def", "get_cursor", "(", "save_path", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "save_path", ",", "str", ")", ":", "\n", "        ", "save_path", "=", "str", "(", "save_path", ")", "\n", "", "conn", "=", "sqlite3", ".", "connect", "(", "save_path", ")", "\n", "cursor", "=", "conn", ".", "cursor", "(", ")", "\n", "return", "cursor", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key": [[58, 66], ["cursor.execute", "cursor.fetchall", "json.loads", "len"], "function", ["None"], ["", "def", "get_item_by_key", "(", "cursor", ",", "key", ")", ":", "\n", "    ", "cursor", ".", "execute", "(", "\"SELECT * from unnamed WHERE key=(?)\"", ",", "(", "key", ",", ")", ")", "\n", "fetched_data", "=", "cursor", ".", "fetchall", "(", ")", "\n", "if", "len", "(", "fetched_data", ")", "==", "0", ":", "\n", "        ", "return", "None", "\n", "", "else", ":", "\n", "        ", "cur_key", ",", "cur_whole_value", "=", "fetched_data", "[", "0", "]", "\n", "", "return", "json", ".", "loads", "(", "cur_whole_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks": [[68, 74], ["wiki_db_tool.get_item_by_key", "wiki_util.hyperlink.extract_hyperlink_and_append", "inspect_wikidump.inspect_whole_file.get_first_paragraph_index"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.extract_hyperlink_and_append", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index"], ["", "def", "get_first_paragraph_hyperlinks", "(", "cursor", ",", "key", ")", ":", "\n", "    ", "hyperlinks", "=", "None", "\n", "item", "=", "get_item_by_key", "(", "cursor", ",", "key", ")", "\n", "item_hyperlinks", "=", "wiki_util", ".", "hyperlink", ".", "extract_hyperlink_and_append", "(", "item", ")", "\n", "frist_p_index", "=", "get_first_paragraph_index", "(", "item", ")", "\n", "return", "item_hyperlinks", "[", "frist_p_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_hyperlinks_from_item": [[76, 80], ["wiki_util.hyperlink.extract_hyperlink_and_append", "inspect_wikidump.inspect_whole_file.get_first_paragraph_index"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.hyperlink.extract_hyperlink_and_append", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index"], ["", "def", "get_first_paragraph_hyperlinks_from_item", "(", "item", ")", ":", "\n", "    ", "item_hyperlinks", "=", "wiki_util", ".", "hyperlink", ".", "extract_hyperlink_and_append", "(", "item", ")", "\n", "frist_p_index", "=", "get_first_paragraph_index", "(", "item", ")", "\n", "return", "item_hyperlinks", "[", "frist_p_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_paragraph_text": [[82, 98], ["wiki_db_tool.get_item_by_key", "wiki_db_tool.item_get_clean_text", "print", "inspect_wikidump.inspect_whole_file.get_first_paragraph_index", "all_para_text.extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_item_by_key", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.item_get_clean_text", "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.inspect_wikidump.inspect_whole_file.get_first_paragraph_index"], ["", "def", "get_paragraph_text", "(", "cursor", ",", "key", ",", "only_abs", "=", "False", ",", "flatten", "=", "False", ")", ":", "\n", "    ", "item", "=", "get_item_by_key", "(", "cursor", ",", "key", ")", "\n", "if", "item", "is", "None", ":", "\n", "        ", "print", "(", "key", ",", "\"No values!\"", ")", "\n", "", "clean_text", "=", "item_get_clean_text", "(", "item", ")", "\n", "if", "only_abs", ":", "\n", "        ", "index", "=", "get_first_paragraph_index", "(", "item", ")", "\n", "clean_text", "=", "clean_text", "[", "index", "]", "\n", "\n", "", "if", "flatten", ":", "\n", "        ", "all_para_text", "=", "[", "]", "\n", "for", "sentence", "in", "clean_text", ":", "\n", "            ", "all_para_text", ".", "extend", "(", "sentence", ")", "\n", "", "return", "all_para_text", "\n", "", "else", ":", "\n", "        ", "return", "clean_text", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.item_to_paragraph_list": [[100, 114], ["enumerate", "len", "paragraph_list.append", "paragraph_list.append", "flatten_paragraph_token_list.extend"], "function", ["None"], ["", "", "def", "item_to_paragraph_list", "(", "item", ",", "flatten_paragraph", "=", "False", ")", ":", "\n", "    ", "all_paragraphs", "=", "item", "[", "'clean_text'", "]", "\n", "paragraph_list", "=", "[", "]", "\n", "for", "i", ",", "paragraph", "in", "enumerate", "(", "all_paragraphs", ")", ":", "\n", "        ", "if", "len", "(", "paragraph", ")", ">", "0", ":", "\n", "            ", "if", "flatten_paragraph", ":", "\n", "                ", "flatten_paragraph_token_list", "=", "[", "]", "\n", "for", "sentence", "in", "paragraph", ":", "\n", "                    ", "flatten_paragraph_token_list", ".", "extend", "(", "sentence", ")", "\n", "", "paragraph_list", ".", "append", "(", "(", "i", ",", "flatten_paragraph_token_list", ")", ")", "\n", "", "else", ":", "\n", "                ", "paragraph_list", ".", "append", "(", "(", "i", ",", "paragraph", ")", ")", "\n", "\n", "", "", "", "return", "paragraph_list", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_index_from_clean_text_item": [[116, 132], ["enumerate", "flatten_para.extend", "len"], "function", ["None"], ["", "def", "get_first_paragraph_index_from_clean_text_item", "(", "item", ",", "text_field_name", "=", "'clean_text'", ",", "skip_first", "=", "False", ")", ":", "\n", "    ", "first_para_index", "=", "-", "1", "\n", "text_field_name", "=", "'text'", "if", "text_field_name", "is", "None", "else", "text_field_name", "\n", "for", "i", ",", "para", "in", "enumerate", "(", "item", "[", "text_field_name", "]", ")", ":", "\n", "        ", "if", "skip_first", "and", "i", "==", "0", ":", "\n", "            ", "continue", "\n", "", "flatten_para", "=", "[", "]", "\n", "for", "sent", "in", "para", ":", "\n", "            ", "flatten_para", ".", "extend", "(", "sent", ")", "\n", "# print(flatten_para)", "\n", "", "cur_para", "=", "' '", ".", "join", "(", "flatten_para", ")", "\n", "# if len(cur_para) >= 50:", "\n", "if", "len", "(", "cur_para", ")", ">=", "50", ":", "\n", "            ", "first_para_index", "=", "i", "\n", "break", "\n", "", "", "return", "first_para_index", "\n", "\n"]], "home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_from_clean_text_item": [[134, 151], ["wiki_db_tool.get_first_paragraph_index_from_clean_text_item", "flatten_para_list.extend"], "function", ["home.repos.pwc.inspect_result.easonnie_semanticRetrievalMRS.wiki_util.wiki_db_tool.get_first_paragraph_index_from_clean_text_item"], ["", "def", "get_first_paragraph_from_clean_text_item", "(", "t_item", ",", "text_field_name", "=", "'clean_text'", ",", "flatten_to_paragraph", "=", "True", ",", "skip_first", "=", "False", ")", ":", "\n", "# if fix_index is None:", "\n", "#     index = get_first_paragraph_index_from_clean_text_item(t_item, text_field_name)", "\n", "# else:", "\n", "#     index = fix_index", "\n", "    ", "index", "=", "get_first_paragraph_index_from_clean_text_item", "(", "t_item", ",", "text_field_name", ",", "skip_first", ")", "\n", "the_para", "=", "t_item", "[", "'clean_text'", "]", "[", "index", "]", "\n", "\n", "if", "flatten_to_paragraph", ":", "\n", "        ", "flatten_para_list", "=", "[", "]", "\n", "# print(the_para)", "\n", "for", "sent", "in", "the_para", ":", "\n", "            ", "flatten_para_list", ".", "extend", "(", "sent", ")", "\n", "\n", "", "return", "flatten_para_list", "\n", "", "else", ":", "\n", "        ", "return", "the_para", "\n", "\n"]]}