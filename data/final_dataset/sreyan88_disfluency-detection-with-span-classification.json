{"home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.calculate_score.calculate_score": [[3, 95], ["open", "gold.readlines.readlines", "open", "pred.readlines.readlines", "enumerate", "enumerate", "sklearn.metrics.classification_report", "print", "line.split", "sents.append", "tags.append", "spans.append", "len", "line.split", "tmp_words.append", "line.strip", "gold_tags.append", "gold_words.append", "j.split", "k[].split", "line.strip", "tmp_tags.append", "len", "int", "int", "range", "pred_tags.append", "pred_tags.append", "gold_tags_flat.append", "gold_tags_flat.append", "tmp_tags.append", "tmp_tags.append", "parts[].split", "range", "range", "range"], "function", ["None"], ["def", "calculate_score", "(", "gold_file", ",", "pred_file", ")", ":", "\n", "    ", "gold", "=", "open", "(", "gold_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "\n", "gold", "=", "gold", ".", "readlines", "(", ")", "\n", "\n", "pred", "=", "open", "(", "pred_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "\n", "pred", "=", "pred", ".", "readlines", "(", ")", "\n", "\n", "gold_tags", "=", "[", "]", "\n", "gold_words", "=", "[", "]", "\n", "tmp_tags", "=", "[", "]", "\n", "tmp_words", "=", "[", "]", "\n", "for", "line", "in", "gold", ":", "\n", "        ", "if", "len", "(", "line", ".", "strip", "(", ")", ")", "!=", "0", ":", "\n", "            ", "word", ",", "tag", "=", "line", ".", "split", "(", ")", "\n", "tmp_words", ".", "append", "(", "word", ")", "\n", "if", "tag", "==", "\"B-Target\"", ":", "\n", "                ", "tmp_tags", ".", "append", "(", "'B'", ")", "\n", "", "elif", "tag", "==", "\"I-Target\"", ":", "\n", "                ", "tmp_tags", ".", "append", "(", "'I'", ")", "\n", "", "else", ":", "\n", "                ", "tmp_tags", ".", "append", "(", "'O'", ")", "\n", "", "", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "            ", "gold_tags", ".", "append", "(", "tmp_tags", ")", "\n", "gold_words", ".", "append", "(", "tmp_words", ")", "\n", "tmp_tags", "=", "[", "]", "\n", "tmp_words", "=", "[", "]", "\n", "\n", "", "", "sents", "=", "[", "]", "\n", "spans", "=", "[", "]", "\n", "tags", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "pred", ")", ":", "\n", "        ", "parts", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "sents", ".", "append", "(", "(", "parts", "[", "0", "]", ",", "len", "(", "parts", "[", "0", "]", ".", "split", "(", "\" \"", ")", ")", ")", ")", "\n", "#     tags.append(['O']*len(texts[i]))", "\n", "#     if sents[-1][1] > len(texts[i]):", "\n", "#                 err += 1", "\n", "tags", ".", "append", "(", "[", "'O'", "]", "*", "sents", "[", "-", "1", "]", "[", "1", "]", ")", "\n", "spans", ".", "append", "(", "parts", "[", "1", ":", "-", "1", "]", ")", "\n", "\n", "", "count", "=", "0", "\n", "for", "ind", ",", "i", "in", "enumerate", "(", "spans", ")", ":", "\n", "        ", "for", "j", "in", "i", ":", "\n", "            ", "k", "=", "j", ".", "split", "(", "\":: \"", ")", "\n", "sp", "=", "k", "[", "1", "]", ".", "split", "(", "\",\"", ")", "\n", "start", ",", "end", "=", "int", "(", "sp", "[", "0", "]", ")", ",", "int", "(", "sp", "[", "1", "]", ")", "\n", "if", "k", "[", "2", "]", "==", "k", "[", "3", "]", ":", "\n", "                ", "tags", "[", "ind", "]", "[", "start", "]", "=", "\"B-\"", "+", "k", "[", "2", "]", "\n", "for", "t", "in", "range", "(", "start", "+", "1", ",", "end", ")", ":", "\n", "                    ", "tags", "[", "ind", "]", "[", "t", "]", "=", "\"I-\"", "+", "k", "[", "2", "]", "\n", "", "", "elif", "k", "[", "2", "]", "!=", "k", "[", "3", "]", ":", "\n", "                ", "count", "+=", "1", "\n", "if", "k", "[", "2", "]", "==", "\"O\"", ":", "\n", "                    ", "count", "+=", "1", "\n", "tags", "[", "ind", "]", "[", "start", "]", "=", "\"B-\"", "+", "k", "[", "3", "]", "\n", "for", "t", "in", "range", "(", "start", "+", "1", ",", "end", ")", ":", "\n", "                        ", "tags", "[", "ind", "]", "[", "t", "]", "=", "\"I-\"", "+", "k", "[", "3", "]", "\n", "", "", "elif", "k", "[", "3", "]", "==", "\"O\"", ":", "\n", "                    ", "tags", "[", "ind", "]", "[", "start", "]", "=", "\"B-\"", "+", "k", "[", "2", "]", "\n", "for", "t", "in", "range", "(", "start", "+", "1", ",", "end", ")", ":", "\n", "                        ", "tags", "[", "ind", "]", "[", "t", "]", "=", "\"I-\"", "+", "k", "[", "2", "]", "\n", "", "", "else", ":", "\n", "                    ", "tags", "[", "ind", "]", "[", "start", "]", "=", "\"B-\"", "+", "k", "[", "2", "]", "\n", "for", "t", "in", "range", "(", "start", "+", "1", ",", "end", ")", ":", "\n", "                        ", "tags", "[", "ind", "]", "[", "t", "]", "=", "\"I-\"", "+", "k", "[", "2", "]", "\n", "\n", "#Converting the BIO to IO", "\n", "", "", "", "", "", "pred_tags", "=", "[", "]", "\n", "for", "i", "in", "tags", ":", "\n", "#     if len(i) <= 5:", "\n", "            ", "for", "tag", "in", "i", ":", "\n", "                ", "if", "tag", "!=", "\"O\"", ":", "\n", "                    ", "pred_tags", ".", "append", "(", "'I'", ")", "\n", "", "else", ":", "\n", "                    ", "pred_tags", ".", "append", "(", "'O'", ")", "\n", "\n", "", "", "", "gold_tags_flat", "=", "[", "]", "\n", "for", "i", "in", "gold_tags", ":", "\n", "#     if len(i) <= 5:", "\n", "            ", "for", "j", "in", "i", ":", "\n", "                ", "if", "j", "!=", "'O'", ":", "\n", "                    ", "gold_tags_flat", ".", "append", "(", "'I'", ")", "\n", "", "else", ":", "\n", "                    ", "gold_tags_flat", ".", "append", "(", "'O'", ")", "\n", "\n", "", "", "", "classification_report_dict", "=", "classification_report", "(", "\n", "gold_tags_flat", ",", "\n", "pred_tags", ",", "\n", "zero_division", "=", "0", ",", "\n", "output_dict", "=", "True", ",", "\n", ")", "\n", "\n", "print", "(", "classification_report_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.evaluate.evaluate": [[12, 27], ["pytorch_lightning.Trainer", "trainer.BertNerTagger.load_from_checkpoint", "pytorch_lightning.Trainer.test"], "function", ["None"], ["def", "evaluate", "(", "ckpt", ",", "hparams_file", ")", ":", "\n", "\t", "\"\"\"main\"\"\"", "\n", "\n", "trainer", "=", "Trainer", "(", "gpus", "=", "[", "2", "]", ",", "distributed_backend", "=", "\"dp\"", ")", "\n", "# trainer = Trainer(distributed_backend=\"dp\")", "\n", "\n", "model", "=", "BertNerTagger", ".", "load_from_checkpoint", "(", "\n", "checkpoint_path", "=", "ckpt", ",", "\n", "hparams_file", "=", "hparams_file", ",", "\n", "map_location", "=", "None", ",", "\n", "batch_size", "=", "1", ",", "\n", "max_length", "=", "128", ",", "\n", "workers", "=", "0", "\n", ")", "\n", "trainer", ".", "test", "(", "model", "=", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.__init__": [[38, 75], ["pytorch_lightning.LightningModule.__init__", "isinstance", "models.config_spanner.BertNerConfig.from_pretrained", "models.bert_model_spanner.BertNER.from_pretrained", "logging.info", "torch.nn.CrossEntropyLoss", "torch.nn.Softmax", "open", "trainer.BertNerTagger.fwrite_epoch_res.write", "trainer.BertNerTagger.save_hyperparameters", "collections.namedtuple", "collections.namedtuple.", "str", "list", "isinstance", "collections.namedtuple.keys"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "args", ":", "argparse", ".", "Namespace", "\n", ")", ":", "\n", "        ", "\"\"\"Initialize a model, tokenizer and config.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "isinstance", "(", "args", ",", "argparse", ".", "Namespace", ")", ":", "\n", "            ", "self", ".", "save_hyperparameters", "(", "args", ")", "\n", "self", ".", "args", "=", "args", "\n", "", "else", ":", "\n", "# eval mode", "\n", "            ", "TmpArgs", "=", "namedtuple", "(", "\"tmp_args\"", ",", "field_names", "=", "list", "(", "args", ".", "keys", "(", ")", ")", ")", "\n", "self", ".", "args", "=", "args", "=", "TmpArgs", "(", "**", "args", ")", "\n", "\n", "", "self", ".", "bert_dir", "=", "args", ".", "bert_config_dir", "\n", "self", ".", "data_dir", "=", "self", ".", "args", ".", "data_dir", "\n", "\n", "bert_config", "=", "BertNerConfig", ".", "from_pretrained", "(", "args", ".", "bert_config_dir", ",", "\n", "hidden_dropout_prob", "=", "args", ".", "bert_dropout", ",", "\n", "attention_probs_dropout_prob", "=", "args", ".", "bert_dropout", ",", "\n", "model_dropout", "=", "args", ".", "model_dropout", ")", "\n", "\n", "self", ".", "model", "=", "BertNER", ".", "from_pretrained", "(", "args", ".", "bert_config_dir", ",", "\n", "config", "=", "bert_config", ",", "\n", "args", "=", "self", ".", "args", ")", "\n", "logging", ".", "info", "(", "str", "(", "args", ".", "__dict__", "if", "isinstance", "(", "args", ",", "argparse", ".", "ArgumentParser", ")", "else", "args", ")", ")", "\n", "\n", "self", ".", "optimizer", "=", "args", ".", "optimizer", "\n", "self", ".", "n_class", "=", "args", ".", "n_class", "\n", "\n", "self", ".", "max_spanLen", "=", "args", ".", "max_spanLen", "\n", "self", ".", "cross_entropy", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "classifier", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "\n", "self", ".", "fwrite_epoch_res", "=", "open", "(", "args", ".", "fp_epoch_result", ",", "'w'", ")", "\n", "self", ".", "fwrite_epoch_res", ".", "write", "(", "\"f1, recall, precision, correct_pred, total_pred, total_golden\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_parser": [[76, 165], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "print", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "random.randint", "v.lower", "v.lower", "argparse.ArgumentTypeError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parser", "(", ")", ":", "\n", "        ", "def", "str2bool", "(", "v", ")", ":", "\n", "            ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "                ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "                ", "return", "False", "\n", "", "else", ":", "\n", "                ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n", "", "", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Training\"", ")", "\n", "\n", "# basic argument&value", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"data dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_config_dir\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"bert config dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pretrained_checkpoint\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "help", "=", "\"pretrained checkpoint path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_max_length\"", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "\"max length of dataset\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "\"learning rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--workers\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"num workers for dataloader\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.01", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"warmup steps used for scheduler.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "\"--model_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"model dropout rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_dropout\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "\"bert dropout rate\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--final_div_factor\"", ",", "type", "=", "float", ",", "default", "=", "1e4", ",", "\n", "help", "=", "\"final div factor of linear decay scheduler\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "choices", "=", "[", "\"adamw\"", ",", "\"sgd\"", "]", ",", "default", "=", "\"adamw\"", ",", "\n", "help", "=", "\"loss type\"", ")", "\n", "#choices=[\"conll03\", \"ace04\",\"notebn\",\"notebc\",\"notewb\",\"notemz\",'notenw','notetc']", "\n", "parser", ".", "add_argument", "(", "\"--dataname\"", ",", "default", "=", "\"conll03\"", ",", "\n", "help", "=", "\"the name of a dataset\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_spanLen\"", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "\"max span length\"", ")", "\n", "# parser.add_argument(\"--margin\", type=float, default=0.03, help=\"margin of the ranking loss\")", "\n", "parser", ".", "add_argument", "(", "\"--n_class\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"the classes of a task\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--modelName\"", ",", "default", "=", "'test'", ",", "help", "=", "\"the classes of a task\"", ")", "\n", "\n", "# parser.add_argument('--use_allspan', type=str2bool, default=True, help='use all the spans with O-labels ', nargs='?',", "\n", "#                     choices=['yes (default)', True, 'no', False])", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_tokenLen'", ",", "type", "=", "str2bool", ",", "default", "=", "False", ",", "help", "=", "'use the token length (after the bert tokenizer process) as a feature'", ",", "\n", "nargs", "=", "'?'", ",", "\n", "choices", "=", "[", "'yes (default)'", ",", "True", ",", "'no'", ",", "False", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--tokenLen_emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "\"the embedding dim of a span\"", ")", "\n", "parser", ".", "add_argument", "(", "'--span_combination_mode'", ",", "default", "=", "'x,y'", ",", "\n", "help", "=", "'Train data in format defined by --data-io param.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_spanLen'", ",", "type", "=", "str2bool", ",", "default", "=", "False", ",", "help", "=", "'use the span length as a feature'", ",", "\n", "nargs", "=", "'?'", ",", "\n", "choices", "=", "[", "'yes (default)'", ",", "True", ",", "'no'", ",", "False", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--spanLen_emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"the embedding dim of a span length\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_morph'", ",", "type", "=", "str2bool", ",", "default", "=", "True", ",", "help", "=", "'use the span length as a feature'", ",", "\n", "nargs", "=", "'?'", ",", "\n", "choices", "=", "[", "'yes (default)'", ",", "True", ",", "'no'", ",", "False", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--morph_emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "\"the embedding dim of the morphology feature.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--morph2idx_list'", ",", "type", "=", "list", ",", "help", "=", "'a list to store a pair of (morph, index).'", ",", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--label2idx_list'", ",", "type", "=", "list", ",", "help", "=", "'a list to store a pair of (label, index).'", ",", ")", "\n", "\n", "\n", "random_int", "=", "'%08d'", "%", "(", "random", ".", "randint", "(", "0", ",", "100000000", ")", ")", "\n", "print", "(", "'random_int:'", ",", "random_int", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--random_int'", ",", "type", "=", "str", ",", "default", "=", "random_int", ",", "help", "=", "'a list to store a pair of (label, index).'", ",", ")", "\n", "parser", ".", "add_argument", "(", "'--param_name'", ",", "type", "=", "str", ",", "default", "=", "'param_name'", ",", "\n", "help", "=", "'a prexfix for a param file name'", ",", ")", "\n", "parser", ".", "add_argument", "(", "'--best_dev_f1'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'best_dev_f1 value'", ",", ")", "\n", "parser", ".", "add_argument", "(", "'--use_prune'", ",", "type", "=", "str2bool", ",", "default", "=", "True", ",", "\n", "help", "=", "'best_dev_f1 value'", ",", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--use_span_weight\"", ",", "type", "=", "str2bool", ",", "default", "=", "True", ",", "\n", "help", "=", "\"range: [0,1.0], the weight of negative span for the loss.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--neg_span_weight\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "\n", "help", "=", "\"range: [0,1.0], the weight of negative span for the loss.\"", ")", "\n", "\n", "\n", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.configure_optimizers": [[167, 195], ["len", "torch.optim.lr_scheduler.OneCycleLR", "transformers.AdamW", "torch.optim.SGD", "float", "str().split", "x.strip", "len", "trainer.BertNerTagger.model.named_parameters", "trainer.BertNerTagger.model.named_parameters", "any", "trainer.BertNerTagger.train_dataloader", "any", "str"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.train_dataloader"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "\"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "if", "self", ".", "optimizer", "==", "\"adamw\"", ":", "\n", "            ", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "\n", "betas", "=", "(", "0.9", ",", "0.98", ")", ",", "# according to RoBERTa paper", "\n", "lr", "=", "self", ".", "args", ".", "lr", ",", "\n", "eps", "=", "self", ".", "args", ".", "adam_epsilon", ",", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "SGD", "(", "optimizer_grouped_parameters", ",", "lr", "=", "self", ".", "args", ".", "lr", ",", "momentum", "=", "0.9", ")", "\n", "", "num_gpus", "=", "len", "(", "[", "x", "for", "x", "in", "str", "(", "self", ".", "args", ".", "gpus", ")", ".", "split", "(", "\",\"", ")", "if", "x", ".", "strip", "(", ")", "]", ")", "\n", "t_total", "=", "(", "len", "(", "self", ".", "train_dataloader", "(", ")", ")", "//", "(", "self", ".", "args", ".", "accumulate_grad_batches", "*", "num_gpus", ")", "+", "1", ")", "*", "self", ".", "args", ".", "max_epochs", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "OneCycleLR", "(", "\n", "optimizer", ",", "max_lr", "=", "self", ".", "args", ".", "lr", ",", "pct_start", "=", "float", "(", "self", ".", "args", ".", "warmup_steps", "/", "t_total", ")", ",", "\n", "final_div_factor", "=", "self", ".", "args", ".", "final_div_factor", ",", "\n", "total_steps", "=", "t_total", ",", "anneal_strategy", "=", "'linear'", "\n", ")", "\n", "return", "[", "optimizer", "]", ",", "[", "{", "\"scheduler\"", ":", "scheduler", ",", "\"interval\"", ":", "\"step\"", "}", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.forward": [[196, 199], ["trainer.BertNerTagger.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "loadall", ",", "all_span_lens", ",", "all_span_idxs_ltoken", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "adjs", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "return", "self", ".", "model", "(", "loadall", ",", "all_span_lens", ",", "all_span_idxs_ltoken", ",", "input_ids", ",", "adjs", ",", "attention_mask", "=", "attention_mask", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.compute_loss": [[201, 228], ["span_label_ltoken.size", "all_span_rep.view", "span_label_ltoken.view", "trainer.BertNerTagger.cross_entropy", "torch.mean.view", "torch.masked_select", "torch.mean", "trainer.BertNerTagger.classifier", "real_span_mask_ltoken.bool"], "methods", ["None"], ["", "def", "compute_loss", "(", "self", ",", "loadall", ",", "all_span_rep", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ",", "mode", ")", ":", "\n", "        ", "'''\n\n        :param all_span_rep: shape: (bs, n_span, n_class)\n        :param span_label_ltoken:\n        :param real_span_mask_ltoken:\n        :return:\n        '''", "\n", "batch_size", ",", "n_span", "=", "span_label_ltoken", ".", "size", "(", ")", "\n", "all_span_rep1", "=", "all_span_rep", ".", "view", "(", "-", "1", ",", "self", ".", "n_class", ")", "\n", "span_label_ltoken1", "=", "span_label_ltoken", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "self", ".", "cross_entropy", "(", "all_span_rep1", ",", "span_label_ltoken1", ")", "\n", "loss", "=", "loss", ".", "view", "(", "batch_size", ",", "n_span", ")", "\n", "# print('loss 1: ', loss)", "\n", "if", "mode", "==", "'train'", "and", "self", ".", "args", ".", "use_span_weight", ":", "# when training we should multiply the span-weight", "\n", "            ", "span_weight", "=", "loadall", "[", "6", "]", "\n", "loss", "=", "loss", "*", "span_weight", "\n", "# print('loss 2: ', loss)", "\n", "\n", "", "loss", "=", "torch", ".", "masked_select", "(", "loss", ",", "real_span_mask_ltoken", ".", "bool", "(", ")", ")", "\n", "\n", "# print(\"1 loss: \", loss)", "\n", "loss", "=", "torch", ".", "mean", "(", "loss", ")", "\n", "# print(\"loss: \", loss)", "\n", "predict", "=", "self", ".", "classifier", "(", "all_span_rep", ")", "# shape: (bs, n_span, n_class)", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.training_step": [[238, 268], ["trainer.BertNerTagger.forward", "trainer.BertNerTagger.classifier", "trainer.BertNerTagger.compute_loss", "eval_metric.span_f1_prune", "eval_metric.span_f1"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.MultiNonLinearClassifier.forward", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.compute_loss", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1_prune", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "tf_board_logs", "=", "{", "\n", "\"lr\"", ":", "self", ".", "trainer", ".", "optimizers", "[", "0", "]", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "}", "\n", "# tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, match_labels, sample_idx, label_idx = batch", "\n", "tokens", ",", "token_type_ids", ",", "all_span_idxs_ltoken", ",", "morph_idxs", ",", "span_label_ltoken", ",", "all_span_lens", ",", "all_span_weights", ",", "real_span_mask_ltoken", ",", "words", ",", "all_span_word", ",", "all_span_idxs", ",", "adjs", "=", "batch", "\n", "loadall", "=", "[", "tokens", ",", "token_type_ids", ",", "all_span_idxs_ltoken", ",", "morph_idxs", ",", "span_label_ltoken", ",", "all_span_lens", ",", "all_span_weights", ",", "\n", "real_span_mask_ltoken", ",", "words", ",", "all_span_word", ",", "all_span_idxs", ",", "adjs", "]", "\n", "\n", "attention_mask", "=", "(", "tokens", "!=", "0", ")", ".", "long", "(", ")", "\n", "all_span_rep", "=", "self", ".", "forward", "(", "loadall", ",", "all_span_lens", ",", "all_span_idxs_ltoken", ",", "tokens", ",", "attention_mask", ",", "token_type_ids", ",", "adjs", ")", "\n", "predicts", "=", "self", ".", "classifier", "(", "all_span_rep", ")", "\n", "# print('all_span_rep.shape: ', all_span_rep.shape)", "\n", "\n", "output", "=", "{", "}", "\n", "if", "self", ".", "args", ".", "use_prune", ":", "\n", "            ", "span_f1s", ",", "pred_label_idx", "=", "span_f1_prune", "(", "all_span_idxs", ",", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", "\n", "", "else", ":", "\n", "            ", "span_f1s", "=", "span_f1", "(", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", "\n", "", "output", "[", "\"span_f1s\"", "]", "=", "span_f1s", "\n", "loss", "=", "self", ".", "compute_loss", "(", "loadall", ",", "all_span_rep", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ",", "mode", "=", "'train'", ")", "\n", "output", "[", "f\"train_loss\"", "]", "=", "loss", "\n", "\n", "tf_board_logs", "[", "f\"loss\"", "]", "=", "loss", "\n", "\n", "output", "[", "'loss'", "]", "=", "loss", "\n", "output", "[", "'log'", "]", "=", "tf_board_logs", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.training_epoch_end": [[270, 293], ["print", "torch.stack().mean", "torch.stack().sum", "print", "print", "print", "print", "trainer.BertNerTagger.fwrite_epoch_res.write", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "training_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "print", "(", "\"use... training_epoch_end: \"", ",", ")", "\n", "avg_loss", "=", "torch", ".", "stack", "(", "[", "x", "[", "'train_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "tensorboard_logs", "=", "{", "'train_loss'", ":", "avg_loss", "}", "\n", "all_counts", "=", "torch", ".", "stack", "(", "[", "x", "[", "f'span_f1s'", "]", "for", "x", "in", "outputs", "]", ")", ".", "sum", "(", "0", ")", "\n", "correct_pred", ",", "total_pred", ",", "total_golden", "=", "all_counts", "\n", "print", "(", "'in train correct_pred, total_pred, total_golden: '", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", "\n", "precision", "=", "correct_pred", "/", "(", "total_pred", "+", "1e-10", ")", "\n", "recall", "=", "correct_pred", "/", "(", "total_golden", "+", "1e-10", ")", "\n", "f1", "=", "precision", "*", "recall", "*", "2", "/", "(", "precision", "+", "recall", "+", "1e-10", ")", "\n", "\n", "print", "(", "\"in train span_precision: \"", ",", "precision", ")", "\n", "print", "(", "\"in train span_recall: \"", ",", "recall", ")", "\n", "print", "(", "\"in train span_f1: \"", ",", "f1", ")", "\n", "tensorboard_logs", "[", "f\"span_precision\"", "]", "=", "precision", "\n", "tensorboard_logs", "[", "f\"span_recall\"", "]", "=", "recall", "\n", "tensorboard_logs", "[", "f\"span_f1\"", "]", "=", "f1", "\n", "\n", "self", ".", "fwrite_epoch_res", ".", "write", "(", "\n", "\"train: %f, %f, %f, %d, %d, %d\\n\"", "%", "(", "f1", ",", "recall", ",", "precision", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", ")", "\n", "\n", "return", "{", "'val_loss'", ":", "avg_loss", ",", "'log'", ":", "tensorboard_logs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.validation_step": [[294, 334], ["trainer.BertNerTagger.forward", "trainer.BertNerTagger.classifier", "trainer.BertNerTagger.compute_loss", "eval_metric.span_f1_prune", "eval_metric.get_predict_prune", "eval_metric.span_f1", "eval_metric.get_predict"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.MultiNonLinearClassifier.forward", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.compute_loss", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1_prune", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_predict_prune", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_predict"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "\n", "output", "=", "{", "}", "\n", "\n", "# tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, match_labels, sample_idx, label_idx = batch", "\n", "tokens", ",", "token_type_ids", ",", "all_span_idxs_ltoken", ",", "morph_idxs", ",", "span_label_ltoken", ",", "all_span_lens", ",", "all_span_weights", ",", "real_span_mask_ltoken", ",", "words", ",", "all_span_word", ",", "all_span_idxs", ",", "adjs", "=", "batch", "\n", "loadall", "=", "[", "tokens", ",", "token_type_ids", ",", "all_span_idxs_ltoken", ",", "morph_idxs", ",", "span_label_ltoken", ",", "all_span_lens", ",", "all_span_weights", ",", "real_span_mask_ltoken", ",", "words", ",", "all_span_word", ",", "all_span_idxs", ",", "adjs", "]", "\n", "\n", "attention_mask", "=", "(", "tokens", "!=", "0", ")", ".", "long", "(", ")", "\n", "all_span_rep", "=", "self", ".", "forward", "(", "loadall", ",", "all_span_lens", ",", "all_span_idxs_ltoken", ",", "tokens", ",", "attention_mask", ",", "token_type_ids", ",", "adjs", ")", "\n", "predicts", "=", "self", ".", "classifier", "(", "all_span_rep", ")", "\n", "\n", "# pred_label_idx_new = torch.zeros_like(real_span_mask_ltoken)", "\n", "if", "self", ".", "args", ".", "use_prune", ":", "\n", "            ", "span_f1s", ",", "pred_label_idx", "=", "span_f1_prune", "(", "all_span_idxs", ",", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", "\n", "# print('pred_label_idx_new: ',pred_label_idx_new.shape)", "\n", "# print('predicts: ', predicts.shape)", "\n", "# print('pred_label_idx_new: ',pred_label_idx_new)", "\n", "# print('predicts: ', predicts)", "\n", "\n", "batch_preds", "=", "get_predict_prune", "(", "self", ".", "args", ",", "all_span_word", ",", "words", ",", "pred_label_idx", ",", "span_label_ltoken", ",", "\n", "all_span_idxs", ")", "\n", "", "else", ":", "\n", "            ", "span_f1s", "=", "span_f1", "(", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", "\n", "batch_preds", "=", "get_predict", "(", "self", ".", "args", ",", "all_span_word", ",", "words", ",", "predicts", ",", "span_label_ltoken", ",", "\n", "all_span_idxs", ")", "\n", "\n", "", "output", "[", "\"span_f1s\"", "]", "=", "span_f1s", "\n", "loss", "=", "self", ".", "compute_loss", "(", "loadall", ",", "all_span_rep", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ",", "mode", "=", "'test/dev'", ")", "\n", "\n", "\n", "output", "[", "\"batch_preds\"", "]", "=", "batch_preds", "\n", "# output[\"batch_preds_prune\"] = pred_label_idx_new", "\n", "output", "[", "f\"val_loss\"", "]", "=", "loss", "\n", "\n", "output", "[", "\"predicts\"", "]", "=", "predicts", "\n", "output", "[", "'all_span_word'", "]", "=", "all_span_word", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.validation_epoch_end": [[335, 385], ["print", "torch.stack().mean", "torch.stack().sum", "print", "print", "print", "print", "trainer.BertNerTagger.fwrite_epoch_res.write", "open", "print", "open", "pickle.dump", "torch.stack", "torch.stack", "list", "list", "int", "open.write"], "methods", ["None"], ["", "def", "validation_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "print", "(", "\"use... validation_epoch_end: \"", ",", ")", "\n", "avg_loss", "=", "torch", ".", "stack", "(", "[", "x", "[", "'val_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "tensorboard_logs", "=", "{", "'val_loss'", ":", "avg_loss", "}", "\n", "all_counts", "=", "torch", ".", "stack", "(", "[", "x", "[", "f'span_f1s'", "]", "for", "x", "in", "outputs", "]", ")", ".", "sum", "(", "0", ")", "\n", "correct_pred", ",", "total_pred", ",", "total_golden", "=", "all_counts", "\n", "print", "(", "'correct_pred, total_pred, total_golden: '", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", "\n", "precision", "=", "correct_pred", "/", "(", "total_pred", "+", "1e-10", ")", "\n", "recall", "=", "correct_pred", "/", "(", "total_golden", "+", "1e-10", ")", "\n", "f1", "=", "precision", "*", "recall", "*", "2", "/", "(", "precision", "+", "recall", "+", "1e-10", ")", "\n", "\n", "print", "(", "\"span_precision: \"", ",", "precision", ")", "\n", "print", "(", "\"span_recall: \"", ",", "recall", ")", "\n", "print", "(", "\"span_f1: \"", ",", "f1", ")", "\n", "tensorboard_logs", "[", "f\"span_precision\"", "]", "=", "precision", "\n", "tensorboard_logs", "[", "f\"span_recall\"", "]", "=", "recall", "\n", "tensorboard_logs", "[", "f\"span_f1\"", "]", "=", "f1", "\n", "self", ".", "fwrite_epoch_res", ".", "write", "(", "\"dev: %f, %f, %f, %d, %d, %d\\n\"", "%", "(", "f1", ",", "recall", ",", "precision", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", ")", "\n", "\n", "if", "f1", ">", "self", ".", "args", ".", "best_dev_f1", ":", "\n", "            ", "pred_batch_results", "=", "[", "x", "[", "'batch_preds'", "]", "for", "x", "in", "outputs", "]", "\n", "fp_write", "=", "self", ".", "args", ".", "default_root_dir", "+", "'/'", "+", "self", ".", "args", ".", "modelName", "+", "'_dev.txt'", "\n", "fwrite", "=", "open", "(", "fp_write", ",", "'w'", ")", "\n", "for", "pred_batch_result", "in", "pred_batch_results", ":", "\n", "                ", "for", "pred_result", "in", "pred_batch_result", ":", "\n", "# print(\"pred_result: \", pred_result)", "\n", "                    ", "fwrite", ".", "write", "(", "pred_result", "+", "'\\n'", ")", "\n", "", "", "self", ".", "args", ".", "best_dev_f1", "=", "f1", "\n", "\n", "# begin{save the predict prob}", "\n", "all_predicts", "=", "[", "list", "(", "x", "[", "'predicts'", "]", ")", "for", "x", "in", "outputs", "]", "\n", "all_span_words", "=", "[", "list", "(", "x", "[", "'all_span_word'", "]", ")", "for", "x", "in", "outputs", "]", "\n", "\n", "# begin{get the label2idx dictionary}", "\n", "label2idx", "=", "{", "}", "\n", "label2idx_list", "=", "self", ".", "args", ".", "label2idx_list", "\n", "for", "labidx", "in", "label2idx_list", ":", "\n", "                ", "lab", ",", "idx", "=", "labidx", "\n", "label2idx", "[", "lab", "]", "=", "int", "(", "idx", ")", "\n", "# end{get the label2idx dictionary}", "\n", "\n", "", "file_prob1", "=", "self", ".", "args", ".", "default_root_dir", "+", "'/'", "+", "self", ".", "args", ".", "modelName", "+", "'_prob_dev.pkl'", "\n", "print", "(", "\"the file path of probs: \"", ",", "file_prob1", ")", "\n", "fwrite_prob", "=", "open", "(", "file_prob1", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "[", "label2idx", ",", "all_predicts", ",", "all_span_words", "]", ",", "fwrite_prob", ")", "\n", "# end{save the predict prob...}", "\n", "\n", "\n", "", "return", "{", "'val_loss'", ":", "avg_loss", ",", "'log'", ":", "tensorboard_logs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.test_step": [[386, 389], ["trainer.BertNerTagger.validation_step"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.validation_step"], ["", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "return", "self", ".", "validation_step", "(", "batch", ",", "batch_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.test_epoch_end": [[390, 446], ["print", "torch.stack().mean", "torch.stack().sum", "print", "print", "print", "print", "open", "trainer.BertNerTagger.fwrite_epoch_res.write", "print", "open", "pickle.dump", "list", "list", "int", "torch.stack", "torch.stack", "open.write", "x[].cpu"], "methods", ["None"], ["", "def", "test_epoch_end", "(", "\n", "self", ",", "\n", "outputs", "\n", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\"\"\"", "\n", "print", "(", "\"use... test_epoch_end: \"", ",", ")", "\n", "avg_loss", "=", "torch", ".", "stack", "(", "[", "x", "[", "'val_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "tensorboard_logs", "=", "{", "'val_loss'", ":", "avg_loss", "}", "\n", "all_counts", "=", "torch", ".", "stack", "(", "[", "x", "[", "f'span_f1s'", "]", "for", "x", "in", "outputs", "]", ")", ".", "sum", "(", "0", ")", "\n", "correct_pred", ",", "total_pred", ",", "total_golden", "=", "all_counts", "\n", "print", "(", "'correct_pred, total_pred, total_golden: '", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", "\n", "precision", "=", "correct_pred", "/", "(", "total_pred", "+", "1e-10", ")", "\n", "recall", "=", "correct_pred", "/", "(", "total_golden", "+", "1e-10", ")", "\n", "f1", "=", "precision", "*", "recall", "*", "2", "/", "(", "precision", "+", "recall", "+", "1e-10", ")", "\n", "\n", "print", "(", "\"span_precision: \"", ",", "precision", ")", "\n", "print", "(", "\"span_recall: \"", ",", "recall", ")", "\n", "print", "(", "\"span_f1: \"", ",", "f1", ")", "\n", "tensorboard_logs", "[", "f\"span_precision\"", "]", "=", "precision", "\n", "tensorboard_logs", "[", "f\"span_recall\"", "]", "=", "recall", "\n", "tensorboard_logs", "[", "f\"span_f1\"", "]", "=", "f1", "\n", "\n", "\n", "# begin{save the predict results}", "\n", "pred_batch_results", "=", "[", "x", "[", "'batch_preds'", "]", "for", "x", "in", "outputs", "]", "\n", "fp_write", "=", "self", ".", "args", ".", "default_root_dir", "+", "'/'", "+", "self", ".", "args", ".", "modelName", "+", "'_test.txt'", "\n", "fwrite", "=", "open", "(", "fp_write", ",", "'w'", ")", "\n", "for", "pred_batch_result", "in", "pred_batch_results", ":", "\n", "            ", "for", "pred_result", "in", "pred_batch_result", ":", "\n", "# print(\"pred_result: \", pred_result)", "\n", "                ", "fwrite", ".", "write", "(", "pred_result", "+", "'\\n'", ")", "\n", "\n", "", "", "self", ".", "fwrite_epoch_res", ".", "write", "(", "\n", "\"test: %f, %f, %f, %d, %d, %d\\n\"", "%", "(", "f1", ",", "recall", ",", "precision", ",", "correct_pred", ",", "total_pred", ",", "total_golden", ")", ")", "\n", "# end{save the predict results}", "\n", "\n", "\n", "# begin{save the predict prob}", "\n", "all_predicts", "=", "[", "list", "(", "x", "[", "'predicts'", "]", ".", "cpu", "(", ")", ")", "for", "x", "in", "outputs", "]", "\n", "all_span_words", "=", "[", "list", "(", "x", "[", "'all_span_word'", "]", ")", "for", "x", "in", "outputs", "]", "\n", "\n", "# begin{get the label2idx dictionary}", "\n", "label2idx", "=", "{", "}", "\n", "label2idx_list", "=", "self", ".", "args", ".", "label2idx_list", "\n", "for", "labidx", "in", "label2idx_list", ":", "\n", "            ", "lab", ",", "idx", "=", "labidx", "\n", "label2idx", "[", "lab", "]", "=", "int", "(", "idx", ")", "\n", "# end{get the label2idx dictionary}", "\n", "\n", "", "file_prob1", "=", "self", ".", "args", ".", "default_root_dir", "+", "'/'", "+", "self", ".", "args", ".", "modelName", "+", "'_prob_test.pkl'", "\n", "print", "(", "\"the file path of probs: \"", ",", "file_prob1", ")", "\n", "fwrite_prob", "=", "open", "(", "file_prob1", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "[", "label2idx", ",", "all_predicts", ",", "all_span_words", "]", ",", "fwrite_prob", ")", "\n", "# end{save the predict prob...}", "\n", "\n", "return", "{", "'val_loss'", ":", "avg_loss", ",", "'log'", ":", "tensorboard_logs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.train_dataloader": [[447, 449], ["trainer.BertNerTagger.get_dataloader"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_dataloader"], ["", "def", "train_dataloader", "(", "self", ")", "->", "DataLoader", ":", "\n", "        ", "return", "self", ".", "get_dataloader", "(", "\"train\"", ")", "\n", "# return self.get_dataloader(\"dev\", 100)", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.val_dataloader": [[451, 454], ["trainer.BertNerTagger.get_dataloader"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_dataloader"], ["", "def", "val_dataloader", "(", "self", ")", ":", "\n", "        ", "val_data", "=", "self", ".", "get_dataloader", "(", "\"dev\"", ")", "\n", "return", "val_data", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.test_dataloader": [[455, 457], ["trainer.BertNerTagger.get_dataloader"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_dataloader"], ["", "def", "test_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_dataloader", "(", "\"test\"", ")", "\n", "# return self.get_dataloader(\"dev\")", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_dataloader": [[459, 499], ["os.path.join", "print", "os.path.join", "print", "dataloaders.dataload.BERTNERDataset", "torch.utils.data.DataLoader", "dataloaders.truncate_dataset.TruncateDataset", "tokenizers.BertWordPieceTokenizer"], "methods", ["None"], ["", "def", "get_dataloader", "(", "self", ",", "prefix", "=", "\"train\"", ",", "limit", ":", "int", "=", "None", ")", "->", "DataLoader", ":", "\n", "        ", "\"\"\"get training dataloader\"\"\"", "\n", "\"\"\"\n        load_mmap_dataset\n        \"\"\"", "\n", "json_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "f\"spanner.{prefix}\"", ")", "\n", "print", "(", "\"json_path: \"", ",", "json_path", ")", "\n", "# vocab_path = os.path.join(self.bert_dir, \"vocab.txt\")", "\n", "# dataset = BERTNERDataset(self.args,json_path=json_path,", "\n", "#                         tokenizer=BertWordPieceTokenizer(vocab_path),", "\n", "#                         # tokenizer=BertWordPieceTokenizer(vocab_file=vocab_path),", "\n", "#                         max_length=self.args.bert_max_length,", "\n", "#                         pad_to_maxlen=False", "\n", "#                         )", "\n", "\n", "vocab_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "bert_dir", ",", "\"vocab.txt\"", ")", "\n", "print", "(", "\"use BertWordPieceTokenizer as the tokenizer \"", ")", "\n", "dataset", "=", "BERTNERDataset", "(", "self", ".", "args", ",", "json_path", "=", "json_path", ",", "\n", "tokenizer", "=", "BertWordPieceTokenizer", "(", "vocab_path", ")", ",", "\n", "# tokenizer=BertWordPieceTokenizer(vocab_file=vocab_path),", "\n", "max_length", "=", "self", ".", "args", ".", "bert_max_length", ",", "\n", "pad_to_maxlen", "=", "False", "\n", ")", "\n", "\n", "\n", "\n", "\n", "if", "limit", "is", "not", "None", ":", "\n", "            ", "dataset", "=", "TruncateDataset", "(", "dataset", ",", "limit", ")", "\n", "\n", "", "dataloader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "self", ".", "args", ".", "batch_size", ",", "\n", "# num_workers=self.args.workers,", "\n", "shuffle", "=", "True", "if", "prefix", "==", "\"train\"", "else", "False", ",", "\n", "# shuffle=False,", "\n", "drop_last", "=", "False", ",", "\n", "collate_fn", "=", "collate_to_max_length", "\n", ")", "\n", "return", "dataloader", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.main": [[501, 590], ["trainer.BertNerTagger.get_parser", "pytorch_lightning.Trainer.add_argparse_args", "Trainer.add_argparse_args.parse_args", "print", "label2idx.items", "morph2idx.items", "print", "trainer.BertNerTagger", "pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint", "pytorch_lightning.Trainer.from_argparse_args", "Trainer.from_argparse_args.fit", "Trainer.from_argparse_args.test", "label2idx_list.append", "morph2idx_list.append", "os.path.exists", "os.makedirs", "BertNerTagger.load_state_dict", "open", "text_file.write", "str().replace().replace().split", "str().replace().replace().split", "torch.load", "str().replace().replace", "str().replace().replace", "torch.device", "str().replace", "str().replace", "str", "str"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.trainer.BertNerTagger.get_parser"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "\"\"\"main\"\"\"", "\n", "# parser = get_parser()", "\n", "\n", "# add model specific args", "\n", "parser", "=", "BertNerTagger", ".", "get_parser", "(", ")", "\n", "\n", "# add all the available trainer options to argparse", "\n", "# ie: now --gpus --num_nodes ... --fast_dev_run all work in the cli", "\n", "parser", "=", "Trainer", ".", "add_argparse_args", "(", "parser", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# begin{add label2indx augument into the args.}", "\n", "label2idx", "=", "{", "}", "\n", "if", "'conll'", "in", "args", ".", "dataname", ":", "\n", "        ", "label2idx", "=", "{", "\"O\"", ":", "0", ",", "\"ORG\"", ":", "1", ",", "\"PER\"", ":", "2", ",", "\"LOC\"", ":", "3", ",", "\"MISC\"", ":", "4", "}", "\n", "", "elif", "'note'", "in", "args", ".", "dataname", ":", "\n", "        ", "label2idx", "=", "{", "'O'", ":", "0", ",", "'PERSON'", ":", "1", ",", "'ORG'", ":", "2", ",", "'GPE'", ":", "3", ",", "'DATE'", ":", "4", ",", "'NORP'", ":", "5", ",", "'CARDINAL'", ":", "6", ",", "'TIME'", ":", "7", ",", "\n", "'LOC'", ":", "8", ",", "\n", "'FAC'", ":", "9", ",", "'PRODUCT'", ":", "10", ",", "'WORK_OF_ART'", ":", "11", ",", "'MONEY'", ":", "12", ",", "'ORDINAL'", ":", "13", ",", "'QUANTITY'", ":", "14", ",", "\n", "'EVENT'", ":", "15", ",", "\n", "'PERCENT'", ":", "16", ",", "'LAW'", ":", "17", ",", "'LANGUAGE'", ":", "18", "}", "\n", "", "elif", "args", ".", "dataname", "==", "'wnut16'", ":", "\n", "        ", "label2idx", "=", "{", "'O'", ":", "0", ",", "'loc'", ":", "1", ",", "'facility'", ":", "2", ",", "'movie'", ":", "3", ",", "'company'", ":", "4", ",", "'product'", ":", "5", ",", "'person'", ":", "6", ",", "'other'", ":", "7", ",", "\n", "'tvshow'", ":", "8", ",", "'musicartist'", ":", "9", ",", "'sportsteam'", ":", "10", "}", "\n", "", "elif", "args", ".", "dataname", "==", "'wnut17'", ":", "\n", "        ", "label2idx", "=", "{", "'O'", ":", "0", ",", "'location'", ":", "1", ",", "'group'", ":", "2", ",", "'corporation'", ":", "3", ",", "'person'", ":", "4", ",", "'creative-work'", ":", "5", ",", "'product'", ":", "6", "}", "\n", "\n", "# label2idx = {'O': 0, 'PER': 1, 'GRP': 2, 'CW': 3, 'LOC': 4, 'CORP': 5, 'PROD': 6}", "\n", "", "label2idx", "=", "{", "'O'", ":", "0", ",", "'Target'", ":", "1", "}", "\n", "print", "(", "label2idx", ")", "\n", "label2idx_list", "=", "[", "]", "\n", "for", "lab", ",", "idx", "in", "label2idx", ".", "items", "(", ")", ":", "\n", "        ", "pair", "=", "(", "lab", ",", "idx", ")", "\n", "label2idx_list", ".", "append", "(", "pair", ")", "\n", "", "args", ".", "label2idx_list", "=", "label2idx_list", "\n", "# end{add label2indx augument into the args.}", "\n", "\n", "# begin{add case2idx augument into the args.}", "\n", "morph2idx_list", "=", "[", "]", "\n", "morph2idx", "=", "{", "'isupper'", ":", "1", ",", "'islower'", ":", "2", ",", "'istitle'", ":", "3", ",", "'isdigit'", ":", "4", ",", "'other'", ":", "5", "}", "\n", "for", "morph", ",", "idx", "in", "morph2idx", ".", "items", "(", ")", ":", "\n", "        ", "pair", "=", "(", "morph", ",", "idx", ")", "\n", "morph2idx_list", ".", "append", "(", "pair", ")", "\n", "", "args", ".", "morph2idx_list", "=", "morph2idx_list", "\n", "# end{add case2idx augument into the args.}", "\n", "\n", "args", ".", "default_root_dir", "=", "args", ".", "default_root_dir", "+", "'_'", "+", "args", ".", "random_int", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "default_root_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "default_root_dir", ")", "\n", "\n", "", "fp_epoch_result", "=", "args", ".", "default_root_dir", "+", "'/epoch_results.txt'", "\n", "args", ".", "fp_epoch_result", "=", "fp_epoch_result", "\n", "\n", "\n", "\n", "\n", "text", "=", "'\\n'", ".", "join", "(", "[", "hp", "for", "hp", "in", "str", "(", "args", ")", ".", "replace", "(", "'Namespace('", ",", "''", ")", ".", "replace", "(", "')'", ",", "''", ")", ".", "split", "(", "', '", ")", "]", ")", "\n", "print", "(", "text", ")", "\n", "\n", "text", "=", "'\\n'", ".", "join", "(", "[", "hp", "for", "hp", "in", "str", "(", "args", ")", ".", "replace", "(", "'Namespace('", ",", "''", ")", ".", "replace", "(", "')'", ",", "''", ")", ".", "split", "(", "', '", ")", "]", ")", "\n", "fn_path", "=", "args", ".", "default_root_dir", "+", "'/'", "+", "args", ".", "param_name", "+", "'.txt'", "\n", "if", "fn_path", "is", "not", "None", ":", "\n", "        ", "with", "open", "(", "fn_path", ",", "mode", "=", "'w'", ")", "as", "text_file", ":", "\n", "            ", "text_file", ".", "write", "(", "text", ")", "\n", "\n", "", "", "model", "=", "BertNerTagger", "(", "args", ")", "\n", "if", "args", ".", "pretrained_checkpoint", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "pretrained_checkpoint", ",", "\n", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", "[", "\"state_dict\"", "]", ")", "\n", "\n", "# save the best model", "\n", "", "checkpoint_callback", "=", "ModelCheckpoint", "(", "\n", "filepath", "=", "args", ".", "default_root_dir", ",", "\n", "save_top_k", "=", "1", ",", "\n", "verbose", "=", "True", ",", "\n", "monitor", "=", "\"span_f1\"", ",", "\n", "period", "=", "1", ",", "\n", "mode", "=", "\"max\"", ",", "\n", ")", "\n", "trainer", "=", "Trainer", ".", "from_argparse_args", "(", "\n", "args", ",", "\n", "checkpoint_callback", "=", "checkpoint_callback", "\n", ")", "\n", "\n", "trainer", ".", "fit", "(", "model", ")", "\n", "trainer", ".", "test", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1": [[7, 22], ["torch.sum", "torch.sum", "torch.sum", "torch.stack", "torch.max", "real_span_mask_ltoken.bool"], "function", ["None"], ["def", "span_f1", "(", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", ":", "\n", "    ", "'''\n    :param predicts: the prediction of model\n    :param span_label_ltoken: the label of span\n    :param real_span_mask_ltoken: 1 for real span, and 0 for padding span.\n    '''", "\n", "pred_label_idx", "=", "torch", ".", "max", "(", "predicts", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "# (bs, n_span)", "\n", "pred_label_mask", "=", "(", "pred_label_idx", "!=", "0", ")", "# (bs, n_span)", "\n", "all_correct", "=", "pred_label_idx", "==", "span_label_ltoken", "\n", "all_correct", "=", "all_correct", "*", "pred_label_mask", "*", "real_span_mask_ltoken", ".", "bool", "(", ")", "\n", "correct_pred", "=", "torch", ".", "sum", "(", "all_correct", ")", "\n", "total_pred", "=", "torch", ".", "sum", "(", "pred_label_idx", "!=", "0", ")", "\n", "total_golden", "=", "torch", ".", "sum", "(", "span_label_ltoken", "!=", "0", ")", "\n", "\n", "return", "torch", ".", "stack", "(", "[", "correct_pred", ",", "total_pred", ",", "total_golden", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.span_f1_prune": [[25, 44], ["predicts.tolist", "eval_metric.get_pruning_predIdxs", "pred_label_idx_new.cuda", "torch.sum", "torch.sum", "torch.sum", "torch.max", "real_span_mask_ltoken.bool", "torch.stack"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_pruning_predIdxs"], ["", "def", "span_f1_prune", "(", "all_span_idxs", ",", "predicts", ",", "span_label_ltoken", ",", "real_span_mask_ltoken", ")", ":", "\n", "    ", "'''\n    :param all_span_idxs: the positon of span;\n    :param predicts: the prediction of model;\n    :param span_label_ltoken: the label of the span.  SHAPE: (batch_size,n_span)\n    :param real_span_mask_ltoken: 1 for real span, and 0 for padding span.\n    '''", "\n", "pred_label_idx", "=", "torch", ".", "max", "(", "predicts", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "# (bs, n_span)", "\n", "span_probs", "=", "predicts", ".", "tolist", "(", ")", "\n", "nonO_idxs2labs", ",", "nonO_kidxs_all", ",", "pred_label_idx_new", "=", "get_pruning_predIdxs", "(", "pred_label_idx", ",", "all_span_idxs", ",", "span_probs", ")", "\n", "pred_label_idx", "=", "pred_label_idx_new", ".", "cuda", "(", ")", "\n", "pred_label_mask", "=", "(", "pred_label_idx", "!=", "0", ")", "# (bs, n_span)", "\n", "all_correct", "=", "pred_label_idx", "==", "span_label_ltoken", "\n", "all_correct", "=", "all_correct", "*", "pred_label_mask", "*", "real_span_mask_ltoken", ".", "bool", "(", ")", "\n", "correct_pred", "=", "torch", ".", "sum", "(", "all_correct", ")", "\n", "total_pred", "=", "torch", ".", "sum", "(", "pred_label_idx", "!=", "0", ")", "\n", "total_golden", "=", "torch", ".", "sum", "(", "span_label_ltoken", "!=", "0", ")", "\n", "\n", "return", "torch", ".", "stack", "(", "[", "correct_pred", ",", "total_pred", ",", "total_golden", "]", ")", ",", "pred_label_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_predict": [[45, 73], ["zip", "torch.max", "zip", "batch_preds.append", "int", "int", "lp.item", "lt.item", "str", "int", "str", "int"], "function", ["None"], ["", "def", "get_predict", "(", "args", ",", "all_span_word", ",", "words", ",", "predicts", ",", "span_label_ltoken", ",", "all_span_idxs", ")", ":", "\n", "    ", "'''\n    :param all_span_word: tokens for a span;\n    :param words: token in setence-level;\n    :param predicts: the prediction of model;\n    :param span_label_ltoken: the label for span;\n    :param all_span_idxs: the position for span;\n    '''", "\n", "pred_label_idx", "=", "torch", ".", "max", "(", "predicts", ",", "dim", "=", "-", "1", ")", "[", "1", "]", "# (bs, n_span)", "\n", "# for context", "\n", "idx2label", "=", "{", "0", ":", "\"O\"", ",", "1", ":", "\"Target\"", "}", "\n", "# label2idx_list = args.label2idx_list", "\n", "# for labidx in label2idx_list:", "\n", "#     lab, idx = labidx", "\n", "#     idx2label[int(idx)] = lab", "\n", "\n", "batch_preds", "=", "[", "]", "\n", "for", "span_idxs", ",", "word", ",", "ws", ",", "lps", ",", "lts", "in", "zip", "(", "all_span_idxs", ",", "words", ",", "all_span_word", ",", "pred_label_idx", ",", "span_label_ltoken", ")", ":", "\n", "        ", "text", "=", "' '", ".", "join", "(", "word", ")", "+", "\"\\t\"", "\n", "for", "sid", ",", "w", ",", "lp", ",", "lt", "in", "zip", "(", "span_idxs", ",", "ws", ",", "lps", ",", "lts", ")", ":", "\n", "            ", "if", "lp", "!=", "0", "or", "lt", "!=", "0", ":", "\n", "                ", "plabel", "=", "idx2label", "[", "int", "(", "lp", ".", "item", "(", ")", ")", "]", "\n", "tlabel", "=", "idx2label", "[", "int", "(", "lt", ".", "item", "(", ")", ")", "]", "\n", "sidx", ",", "eidx", "=", "sid", "\n", "ctext", "=", "' '", ".", "join", "(", "w", ")", "+", "':: '", "+", "str", "(", "int", "(", "sidx", ")", ")", "+", "','", "+", "str", "(", "int", "(", "eidx", "+", "1", ")", ")", "+", "':: '", "+", "tlabel", "+", "':: '", "+", "plabel", "+", "'\\t'", "\n", "text", "+=", "ctext", "\n", "", "", "batch_preds", ".", "append", "(", "text", ")", "\n", "", "return", "batch_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_predict_prune": [[75, 104], ["zip", "zip", "batch_preds.append", "int", "int", "lp.item", "lt.item", "str", "int", "str", "int"], "function", ["None"], ["", "def", "get_predict_prune", "(", "args", ",", "all_span_word", ",", "words", ",", "predicts_new", ",", "span_label_ltoken", ",", "all_span_idxs", ")", ":", "\n", "    ", "'''\n    :param all_span_word: tokens for a span;\n    :param words: token in setence-level;\n    :param predicts_new: the prediction of model;\n    :param span_label_ltoken: the label for span;\n    :param all_span_idxs: the position for span;\n    '''", "\n", "# for context", "\n", "idx2label", "=", "{", "}", "\n", "label2idx_list", "=", "args", ".", "label2idx_list", "\n", "# for labidx in label2idx_list:", "\n", "#     lab, idx = labidx", "\n", "#     idx2label[int(idx)] = lab", "\n", "# idx2label = {0:'O', 1:'PER', 2:'GRP', 3:'CW', 4:'LOC', 5:'CORP', 6:'PROD'}", "\n", "idx2label", "=", "{", "0", ":", "'O'", ",", "1", ":", "'Target'", "}", "\n", "\n", "batch_preds", "=", "[", "]", "\n", "for", "span_idxs", ",", "word", ",", "ws", ",", "lps", ",", "lts", "in", "zip", "(", "all_span_idxs", ",", "words", ",", "all_span_word", ",", "predicts_new", ",", "span_label_ltoken", ")", ":", "\n", "        ", "text", "=", "' '", ".", "join", "(", "word", ")", "+", "\"\\t\"", "\n", "for", "sid", ",", "w", ",", "lp", ",", "lt", "in", "zip", "(", "span_idxs", ",", "ws", ",", "lps", ",", "lts", ")", ":", "\n", "            ", "if", "lp", "!=", "0", "or", "lt", "!=", "0", ":", "\n", "                ", "plabel", "=", "idx2label", "[", "int", "(", "lp", ".", "item", "(", ")", ")", "]", "\n", "tlabel", "=", "idx2label", "[", "int", "(", "lt", ".", "item", "(", ")", ")", "]", "\n", "sidx", ",", "eidx", "=", "sid", "\n", "ctext", "=", "' '", ".", "join", "(", "w", ")", "+", "':: '", "+", "str", "(", "int", "(", "sidx", ")", ")", "+", "','", "+", "str", "(", "int", "(", "eidx", "+", "1", ")", ")", "+", "':: '", "+", "tlabel", "+", "':: '", "+", "plabel", "+", "'\\t'", "\n", "text", "+=", "ctext", "\n", "", "", "batch_preds", ".", "append", "(", "text", ")", "\n", "", "return", "batch_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.has_overlapping": [[106, 111], ["None"], "function", ["None"], ["", "def", "has_overlapping", "(", "idx1", ",", "idx2", ")", ":", "\n", "    ", "overlapping", "=", "True", "\n", "if", "(", "idx1", "[", "0", "]", ">", "idx2", "[", "1", "]", "or", "idx2", "[", "0", "]", ">", "idx1", "[", "1", "]", ")", ":", "\n", "        ", "overlapping", "=", "False", "\n", "", "return", "overlapping", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.clean_overlapping_span": [[112, 163], ["range", "range", "len", "kidxs.append", "len", "len", "eval_metric.has_overlapping", "kidxs.append", "eval_metric.has_overlapping", "kidxs.append", "didxs.append", "kidxs.remove", "kidxs.append", "didxs.append"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.has_overlapping", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.has_overlapping"], ["", "def", "clean_overlapping_span", "(", "idxs_list", ",", "nonO_idxs2prob", ")", ":", "\n", "    ", "kidxs", "=", "[", "]", "\n", "didxs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "idxs_list", ")", "-", "1", ")", ":", "\n", "        ", "idx1", "=", "idxs_list", "[", "i", "]", "\n", "\n", "kidx", "=", "idx1", "\n", "kidx1", "=", "True", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "len", "(", "idxs_list", ")", ")", ":", "\n", "            ", "idx2", "=", "idxs_list", "[", "j", "]", "\n", "isoverlapp", "=", "has_overlapping", "(", "idx1", ",", "idx2", ")", "\n", "\n", "\n", "\n", "\n", "if", "isoverlapp", ":", "\n", "                ", "prob1", "=", "nonO_idxs2prob", "[", "idx1", "]", "\n", "prob2", "=", "nonO_idxs2prob", "[", "idx2", "]", "\n", "\n", "if", "prob1", "<", "prob2", ":", "\n", "                    ", "kidx1", "=", "False", "\n", "didxs", ".", "append", "(", "kidx1", ")", "\n", "", "elif", "prob1", "==", "prob2", ":", "\n", "                    ", "len1", "=", "idx1", "[", "1", "]", "-", "idx1", "[", "0", "]", "+", "1", "\n", "len2", "=", "idx1", "[", "1", "]", "-", "idx1", "[", "0", "]", "+", "1", "\n", "if", "len1", "<", "len2", ":", "\n", "                        ", "kidx1", "=", "False", "\n", "didxs", ".", "append", "(", "kidx1", ")", "\n", "\n", "", "", "", "", "if", "kidx1", ":", "\n", "            ", "flag", "=", "True", "\n", "for", "idx", "in", "kidxs", ":", "\n", "                ", "isoverlap", "=", "has_overlapping", "(", "idx1", ",", "idx", ")", "\n", "if", "isoverlap", ":", "\n", "                    ", "flag", "=", "False", "\n", "prob1", "=", "nonO_idxs2prob", "[", "idx1", "]", "\n", "prob2", "=", "nonO_idxs2prob", "[", "idx", "]", "\n", "if", "prob1", ">", "prob2", ":", "# del the keept idex", "\n", "                        ", "kidxs", ".", "remove", "(", "idx", ")", "\n", "kidxs", ".", "append", "(", "idx1", ")", "\n", "", "break", "\n", "", "", "if", "flag", "==", "True", ":", "\n", "                ", "kidxs", ".", "append", "(", "idx1", ")", "\n", "\n", "", "", "", "if", "len", "(", "didxs", ")", "==", "0", ":", "\n", "        ", "kidxs", ".", "append", "(", "idxs_list", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "if", "idxs_list", "[", "-", "1", "]", "not", "in", "didxs", ":", "\n", "            ", "kidxs", ".", "append", "(", "idxs_list", "[", "-", "1", "]", ")", "\n", "\n", "", "", "return", "kidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.get_pruning_predIdxs": [[164, 202], ["enumerate", "pred_label_idx.size", "enumerate", "torch.LongTensor", "zip", "enumerate", "nonO_idxs2labs.append", "nonO_kidxs_all.append", "zip", "enumerate", "torch.LongTensor.append", "zip", "int", "len", "eval_metric.clean_overlapping_span", "zip", "pred_label_idx_new1.append", "len", "pred_label_idx_new1.append", "int.item", "nonO_idxs.append"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.eval_metric.clean_overlapping_span"], ["", "def", "get_pruning_predIdxs", "(", "pred_label_idx", ",", "all_span_idxs", ",", "span_probs", ")", ":", "\n", "    ", "nonO_kidxs_all", "=", "[", "]", "\n", "nonO_idxs2labs", "=", "[", "]", "\n", "# begin{Constraint the span that was predicted can not be overlapping.}", "\n", "for", "i", ",", "(", "bs", ",", "idxs", ")", "in", "enumerate", "(", "zip", "(", "pred_label_idx", ",", "all_span_idxs", ")", ")", ":", "\n", "# collect the span indexs that non-O", "\n", "        ", "nonO_idxs2lab", "=", "{", "}", "\n", "nonO_idxs2prob", "=", "{", "}", "\n", "nonO_idxs", "=", "[", "]", "\n", "for", "j", ",", "(", "plb", ",", "idx", ")", "in", "enumerate", "(", "zip", "(", "bs", ",", "idxs", ")", ")", ":", "\n", "            ", "plb", "=", "int", "(", "plb", ".", "item", "(", ")", ")", "\n", "if", "plb", "!=", "0", ":", "# only consider the non-O label span...", "\n", "                ", "nonO_idxs2lab", "[", "idx", "]", "=", "plb", "\n", "nonO_idxs2prob", "[", "idx", "]", "=", "span_probs", "[", "i", "]", "[", "j", "]", "[", "plb", "]", "\n", "nonO_idxs", ".", "append", "(", "idx", ")", "\n", "\n", "", "", "nonO_idxs2labs", ".", "append", "(", "nonO_idxs2lab", ")", "\n", "if", "len", "(", "nonO_idxs", ")", "!=", "0", ":", "\n", "            ", "nonO_kidxs", "=", "clean_overlapping_span", "(", "nonO_idxs", ",", "nonO_idxs2prob", ")", "\n", "", "else", ":", "\n", "            ", "nonO_kidxs", "=", "[", "]", "\n", "", "nonO_kidxs_all", ".", "append", "(", "nonO_kidxs", ")", "\n", "\n", "", "pred_label_idx_new", "=", "[", "]", "\n", "n_span", "=", "pred_label_idx", ".", "size", "(", "1", ")", "\n", "for", "i", ",", "(", "bs", ",", "idxs", ")", "in", "enumerate", "(", "zip", "(", "pred_label_idx", ",", "all_span_idxs", ")", ")", ":", "\n", "        ", "pred_label_idx_new1", "=", "[", "]", "\n", "for", "j", ",", "(", "plb", ",", "idx", ")", "in", "enumerate", "(", "zip", "(", "bs", ",", "idxs", ")", ")", ":", "\n", "            ", "nlb_id", "=", "0", "\n", "if", "idx", "in", "nonO_kidxs_all", "[", "i", "]", ":", "\n", "                ", "nlb_id", "=", "plb", "\n", "", "pred_label_idx_new1", ".", "append", "(", "nlb_id", ")", "\n", "", "while", "len", "(", "pred_label_idx_new1", ")", "<", "n_span", ":", "\n", "            ", "pred_label_idx_new1", ".", "append", "(", "0", ")", "\n", "\n", "", "pred_label_idx_new", ".", "append", "(", "pred_label_idx_new1", ")", "\n", "", "pred_label_idx_new", "=", "torch", ".", "LongTensor", "(", "pred_label_idx_new", ")", "\n", "return", "nonO_idxs2labs", ",", "nonO_kidxs_all", ",", "pred_label_idx_new", "\n", "", ""]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.None.radom_seed.set_random_seed": [[8, 14], ["numpy.random.seed", "torch.manual_seed"], "function", ["None"], ["def", "set_random_seed", "(", "seed", ":", "int", ")", ":", "\n", "    ", "\"\"\"set seeds for reproducibility\"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.get_chunk_type": [[8, 20], ["tok.split", "tok.split"], "function", ["None"], ["def", "get_chunk_type", "(", "tok", ")", ":", "\n", "\t", "\"\"\"\n\tArgs:\n\t\ttok: id of token, ex 4\n\t\tidx_to_tag: dictionary {4: \"B-PER\", ...}\n\tReturns:\n\t\ttuple: \"B\", \"PER\"\n\t\"\"\"", "\n", "tag_class", "=", "tok", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "# tag_type = tok.split('-')[-1]", "\n", "tag_type", "=", "'-'", ".", "join", "(", "tok", ".", "split", "(", "'-'", ")", "[", "1", ":", "]", ")", "\n", "return", "tag_class", ",", "tag_type", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.get_chunks": [[21, 63], ["enumerate", "chunks.append", "chunks.append", "len", "bio2spannerformat.get_chunk_type", "chunks.append"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.get_chunk_type"], ["", "def", "get_chunks", "(", "seq", ")", ":", "\n", "\t", "\"\"\"\n\ttags:dic{'per':1,....}\n\tArgs:\n\t\tseq: [4, 4, 0, 0, ...] sequence of labels\n\t\ttags: dict[\"O\"] = 4\n\tReturns:\n\t\tlist of (chunk_type, chunk_start, chunk_end)\n\n\tExample:\n\t\tseq = [4, 5, 0, 3]\n\t\ttags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n\t\tresult = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n\t\"\"\"", "\n", "default", "=", "'O'", "\n", "chunks", "=", "[", "]", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "seq", ")", ":", "\n", "#End of a chunk 1", "\n", "\t\t", "if", "tok", "==", "default", "and", "chunk_type", "is", "not", "None", ":", "\n", "# Add a chunk.", "\n", "\t\t\t", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "None", ",", "None", "\n", "\n", "# End of a chunk + start of a chunk!", "\n", "", "elif", "tok", "!=", "default", ":", "\n", "\t\t\t", "tok_chunk_class", ",", "tok_chunk_type", "=", "get_chunk_type", "(", "tok", ")", "\n", "if", "chunk_type", "is", "None", ":", "\n", "\t\t\t\t", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "elif", "tok_chunk_type", "!=", "chunk_type", "or", "tok_chunk_class", "==", "\"B\"", ":", "\n", "\t\t\t\t", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "i", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "chunk_type", ",", "chunk_start", "=", "tok_chunk_type", ",", "i", "\n", "", "", "else", ":", "\n", "\t\t\t", "pass", "\n", "# end condition", "\n", "", "", "if", "chunk_type", "is", "not", "None", ":", "\n", "\t\t", "chunk", "=", "(", "chunk_type", ",", "chunk_start", ",", "len", "(", "seq", ")", ")", "\n", "chunks", ".", "append", "(", "chunk", ")", "\n", "\n", "", "return", "chunks", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.keep_spanPred_data": [[65, 113], ["bio2spannerformat.read_data", "collections.Counter().most_common", "enumerate", "print", "enumerate", "zip", "print", "bio2spannerformat.get_chunks", "all_datas.append", "tag.split", "all_labs.append", "collections.Counter", "print", "print", "len", "tag.split", "str", "str"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.read_data", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.get_chunks"], ["", "def", "keep_spanPred_data", "(", "dataname", ",", "fpath_bio", ",", "column_no", ",", "delimiter", ")", ":", "\n", "\t", "word_seqs", ",", "trueTag_seqs", ",", "word_seqs_sent", ",", "trueTag_seqs_sent", ",", "head_sequences", "=", "read_data", "(", "\n", "dataname", ",", "fpath_bio", ",", "column_no", "=", "column_no", ",", "delimiter", "=", "delimiter", ")", "# column_no=3 for ontonotes5.0", "\n", "\n", "all_labs", "=", "[", "]", "\n", "for", "tag", "in", "trueTag_seqs", ":", "\n", "\t\t", "if", "tag", "!=", "'O'", ":", "\n", "\t\t\t", "tags", "=", "tag", ".", "split", "(", "'-'", ")", "\n", "if", "len", "(", "tags", ")", ">", "2", ":", "\n", "\t\t\t\t", "lab", "=", "'-'", ".", "join", "(", "tags", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "pre", ",", "lab", "=", "tag", ".", "split", "(", "'-'", ")", "\n", "", "all_labs", ".", "append", "(", "lab", ")", "\n", "\n", "", "", "counter", "=", "Counter", "(", "all_labs", ")", ".", "most_common", "(", ")", "\n", "\n", "tag_dic", "=", "{", "\"O\"", ":", "0", "}", "\n", "for", "i", ",", "elem", "in", "enumerate", "(", "counter", ")", ":", "\n", "\t\t", "tag", ",", "c", "=", "elem", "\n", "tag_dic", "[", "tag", "]", "=", "i", "+", "1", "\n", "", "print", "(", "tag_dic", ")", "\n", "\n", "\n", "all_datas", "=", "[", "]", "\n", "for", "i", ",", "(", "tokens", ",", "labs", ",", "head", ")", "in", "enumerate", "(", "zip", "(", "word_seqs_sent", ",", "trueTag_seqs_sent", ",", "head_sequences", ")", ")", ":", "\n", "\t\t", "print", "(", "head", ")", "\n", "chunks", "=", "get_chunks", "(", "labs", ")", "\n", "context", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "if", "\"[emoji]  \"", "in", "context", ":", "\n", "\t\t\t", "print", "(", "'context: '", ",", "context", ")", "\n", "print", "(", "'tokens: '", ",", "tokens", ")", "\n", "\n", "", "pos", "=", "{", "}", "\n", "for", "chunk", "in", "chunks", ":", "\n", "\t\t\t", "lab", ",", "sidx", ",", "eidx", "=", "chunk", "\n", "key1", "=", "str", "(", "sidx", ")", "+", "';'", "+", "str", "(", "eidx", "-", "1", ")", "\n", "pos", "[", "key1", "]", "=", "lab", "\n", "\n", "", "one_samp", "=", "{", "\n", "\"context\"", ":", "context", ",", "\n", "\"span_posLabel\"", ":", "pos", ",", "\n", "\"heads\"", ":", "head_sequences", "[", "i", "]", "\n", "}", "\n", "\n", "all_datas", ".", "append", "(", "one_samp", ")", "\n", "\n", "\n", "", "return", "all_datas", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataprocess.bio2spannerformat.read_data": [[115, 165], ["print", "list", "list", "list", "list", "list", "list", "list", "list", "list", "range", "codecs.open", "f.readlines", "len", "lines[].strip", "line.replace.split", "strings[].strip", "strings[].strip", "strings[].strip", "word.strip.strip", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append", "line.replace.replace", "len", "list.append", "list.append", "list.append", "len", "list.append", "list.append", "list.append", "list", "list", "list", "len"], "function", ["None"], ["", "def", "read_data", "(", "corpus_type", ",", "fn", ",", "column_no", "=", "-", "1", ",", "delimiter", "=", "\"\\t\"", ")", ":", "\n", "\t", "print", "(", "'corpus_type'", ",", "corpus_type", ")", "\n", "word_sequences", "=", "list", "(", ")", "\n", "tag_sequences", "=", "list", "(", ")", "\n", "head_sequences", "=", "list", "(", ")", "\n", "total_word_sequences", "=", "list", "(", ")", "\n", "total_tag_sequences", "=", "list", "(", ")", "\n", "total_head_sequences", "=", "list", "(", ")", "\n", "with", "codecs", ".", "open", "(", "fn", ",", "'r'", ",", "'utf-8'", ")", "as", "f", ":", "\n", "\t\t", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "curr_words", "=", "list", "(", ")", "\n", "curr_tags", "=", "list", "(", ")", "\n", "curr_heads", "=", "list", "(", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "lines", ")", ")", ":", "\n", "\t\t", "line", "=", "lines", "[", "k", "]", ".", "strip", "(", ")", "\n", "if", "\"\u2764 \ufe0f\"", "in", "line", ":", "\n", "\t\t\t", "line", "=", "line", ".", "replace", "(", "\"\u2764 \ufe0f\ufe0f\"", ",", "\"[emoji]\"", ")", "\n", "\n", "", "if", "len", "(", "line", ")", "==", "0", ":", "# new sentence or new document", "\n", "\t\t\t", "if", "len", "(", "curr_words", ")", ">", "0", ":", "\n", "\t\t\t\t", "word_sequences", ".", "append", "(", "curr_words", ")", "\n", "tag_sequences", ".", "append", "(", "curr_tags", ")", "\n", "head_sequences", ".", "append", "(", "curr_heads", ")", "\n", "curr_words", "=", "list", "(", ")", "\n", "curr_tags", "=", "list", "(", ")", "\n", "curr_heads", "=", "list", "(", ")", "\n", "", "continue", "\n", "\n", "", "strings", "=", "line", ".", "split", "(", ")", "\n", "# print(strings)", "\n", "word", "=", "strings", "[", "1", "]", ".", "strip", "(", ")", "\n", "tag", "=", "strings", "[", "column_no", "]", ".", "strip", "(", ")", "# be default, we take the last tag", "\n", "head", "=", "strings", "[", "-", "5", "]", ".", "strip", "(", ")", "\n", "if", "corpus_type", "==", "'ptb2'", ":", "\n", "\t\t\t", "tag", "=", "'B-'", "+", "tag", "\n", "", "if", "word", "==", "'\u2764 '", ":", "\n", "\t\t\t", "word", "=", "\"[emoji]\"", "\n", "", "word", "=", "word", ".", "strip", "(", ")", "\n", "curr_words", ".", "append", "(", "word", ")", "\n", "curr_tags", ".", "append", "(", "tag", ")", "\n", "curr_heads", ".", "append", "(", "head", ")", "\n", "total_word_sequences", ".", "append", "(", "word", ")", "\n", "total_tag_sequences", ".", "append", "(", "tag", ")", "\n", "total_head_sequences", ".", "append", "(", "head", ")", "\n", "if", "k", "==", "len", "(", "lines", ")", "-", "1", ":", "\n", "\t\t\t", "word_sequences", ".", "append", "(", "curr_words", ")", "\n", "tag_sequences", ".", "append", "(", "curr_tags", ")", "\n", "head_sequences", ".", "append", "(", "curr_heads", ")", "\n", "\n", "", "", "return", "total_word_sequences", ",", "total_tag_sequences", ",", "word_sequences", ",", "tag_sequences", ",", "head_sequences", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.config_spanner.BertNerConfig.__init__": [[8, 11], ["transformers.BertConfig.__init__", "kwargs.get"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertNerConfig", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "model_dropout", "=", "kwargs", ".", "get", "(", "\"model_dropout\"", ",", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.BertNER.__init__": [[15, 85], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "print", "print", "allennlp.modules.span_extractors.EndpointSpanExtractor", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "models.classifier.MultiNonLinearClassifier", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "range", "torch.Linear", "torch.Linear", "transformers.RobertaModel", "print", "bert_model_spanner.BertNER.W.append().to", "len", "bert_model_spanner.BertNER.W.append", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "args", ")", ":", "\n", "        ", "super", "(", "BertNER", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "config", ".", "hidden_size", "=", "config", ".", "hidden_size", "*", "2", "\n", "self", ".", "args", "=", "args", "\n", "if", "'roberta'", "in", "self", ".", "args", ".", "bert_config_dir", ":", "\n", "            ", "self", ".", "bert", "=", "RobertaModel", "(", "config", ")", "\n", "print", "(", "'use the roberta pre-trained model...'", ")", "\n", "\n", "\n", "# self.start_outputs = nn.Linear(config.hidden_size, 2)", "\n", "# self.end_outputs = nn.Linear(config.hidden_size, 2)", "\n", "", "self", ".", "start_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "end_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "# self.span_embedding = SingleLinearClassifier(config.hidden_size * 2, 1)", "\n", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "self", ".", "span_combination_mode", "=", "self", ".", "args", ".", "span_combination_mode", "\n", "self", ".", "max_span_width", "=", "args", ".", "max_spanLen", "\n", "self", ".", "n_class", "=", "args", ".", "n_class", "\n", "self", ".", "tokenLen_emb_dim", "=", "self", ".", "args", ".", "tokenLen_emb_dim", "# must set, when set a value to the max_span_width.", "\n", "\n", "# if self.args.use_tokenLen:", "\n", "#     self.tokenLen_emb_dim = self.args.tokenLen_emb_dim", "\n", "# else:", "\n", "#     self.tokenLen_emb_dim = None", "\n", "\n", "\n", "\n", "\n", "print", "(", "\"self.max_span_width: \"", ",", "self", ".", "max_span_width", ")", "\n", "print", "(", "\"self.tokenLen_emb_dim: \"", ",", "self", ".", "tokenLen_emb_dim", ")", "\n", "\n", "#  bucket_widths: Whether to bucket the span widths into log-space buckets. If `False`, the raw span widths are used.", "\n", "\n", "self", ".", "_endpoint_span_extractor", "=", "EndpointSpanExtractor", "(", "config", ".", "hidden_size", ",", "\n", "combination", "=", "self", ".", "span_combination_mode", ",", "\n", "num_width_embeddings", "=", "self", ".", "max_span_width", ",", "\n", "span_width_embedding_dim", "=", "self", ".", "tokenLen_emb_dim", ",", "\n", "bucket_widths", "=", "True", ")", "\n", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "10", ",", "1", ")", "\n", "self", ".", "score_func", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# import span-length embedding", "\n", "self", ".", "spanLen_emb_dim", "=", "args", ".", "spanLen_emb_dim", "\n", "self", ".", "morph_emb_dim", "=", "args", ".", "morph_emb_dim", "\n", "input_dim", "=", "config", ".", "hidden_size", "*", "2", "+", "self", ".", "tokenLen_emb_dim", "\n", "if", "self", ".", "args", ".", "use_spanLen", "and", "not", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "input_dim", "=", "config", ".", "hidden_size", "*", "2", "+", "self", ".", "tokenLen_emb_dim", "+", "self", ".", "spanLen_emb_dim", "\n", "", "elif", "not", "self", ".", "args", ".", "use_spanLen", "and", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "input_dim", "=", "config", ".", "hidden_size", "*", "2", "+", "self", ".", "tokenLen_emb_dim", "+", "self", ".", "morph_emb_dim", "\n", "", "elif", "self", ".", "args", ".", "use_spanLen", "and", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "input_dim", "=", "config", ".", "hidden_size", "*", "2", "+", "self", ".", "tokenLen_emb_dim", "+", "self", ".", "spanLen_emb_dim", "+", "self", ".", "morph_emb_dim", "\n", "\n", "\n", "", "self", ".", "span_embedding", "=", "MultiNonLinearClassifier", "(", "input_dim", ",", "self", ".", "n_class", ",", "\n", "config", ".", "model_dropout", ")", "\n", "\n", "self", ".", "spanLen_embedding", "=", "nn", ".", "Embedding", "(", "args", ".", "max_spanLen", "+", "1", ",", "self", ".", "spanLen_emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "\n", "self", ".", "morph_embedding", "=", "nn", ".", "Embedding", "(", "len", "(", "args", ".", "morph2idx_list", ")", "+", "1", ",", "self", ".", "morph_emb_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "W", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "layer", "in", "range", "(", "2", ")", ":", "\n", "            ", "self", ".", "W", ".", "append", "(", "nn", ".", "Linear", "(", "768", ",", "768", ")", ")", ".", "to", "(", "'cuda'", ")", "\n", "\n", "", "self", ".", "gate", "=", "nn", ".", "Linear", "(", "1536", ",", "768", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.BertNER.forward": [[89, 176], ["bert_model_spanner.BertNER.bert", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.neg().add", "torch.neg().add", "torch.neg().add", "torch.neg().add", "torch.add", "torch.add", "torch.add", "torch.add", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_model_spanner.BertNER._endpoint_span_extractor", "torch.tensor.sum().unsqueeze", "torch.tensor.sum().unsqueeze", "torch.tensor.bmm", "torch.tensor.bmm", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "bert_model_spanner.BertNER.gate", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "all_span_idxs_ltoken.long", "bert_model_spanner.BertNER.span_embedding", "torch.neg", "torch.neg", "torch.neg", "torch.neg", "bert_model_spanner.BertNER.spanLen_embedding", "torch.nn.functional.relu", "torch.nn.functional.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_model_spanner.BertNER.span_embedding", "torch.tensor.sum", "torch.tensor.sum", "bert_model_spanner.BertNER.morph_embedding", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_model_spanner.BertNER.span_embedding", "bert_model_spanner.BertNER.morph_embedding", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "bert_model_spanner.BertNER.spanLen_embedding", "torch.nn.functional.relu", "torch.nn.functional.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_model_spanner.BertNER.span_embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "loadall", ",", "all_span_lens", ",", "all_span_idxs_ltoken", ",", "input_ids", ",", "adjs", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input_ids: bert input tokens, tensor of shape [seq_len]\n            token_type_ids: 0 for query, 1 for context, tensor of shape [seq_len]\n            attention_mask: attention mask, tensor of shape [seq_len]\n            all_span_idxs: the span-idxs on token-level. (bs, n_span)\n            pos_span_mask: 0 for negative span, 1 for the positive span. SHAPE: (bs, n_span)\n            pad_span_mask: 1 for real span, 0 for padding SHAPE: (bs, n_span)\n        Returns:\n            start_logits: start/non-start probs of shape [seq_len]\n            end_logits: end/non-end probs of shape [seq_len]\n            match_logits: start-end-match probs of shape [seq_len, 1]\n        \"\"\"", "\n", "bert_outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "# bert_outputs = torch.cat((bert_outputs, adjs), 2)", "\n", "\n", "sequence_heatmap", "=", "bert_outputs", "[", "0", "]", "# [batch, seq_len, hidden]", "\n", "# print(sequence_heatmap.shape)", "\n", "adjs", "=", "torch", ".", "tensor", "(", "adjs", ",", "device", "=", "'cuda'", ")", "\n", "denom", "=", "adjs", ".", "sum", "(", "2", ")", ".", "unsqueeze", "(", "2", ")", "+", "1", "\n", "graph_output", "=", "None", "\n", "for", "l", "in", "range", "(", "2", ")", ":", "\n", "            ", "Ax", "=", "adjs", ".", "bmm", "(", "sequence_heatmap", ")", "## N x N  times N x h  = Nxh", "\n", "AxW", "=", "self", ".", "W", "[", "l", "]", "(", "Ax", ")", "## N x m", "\n", "AxW", "=", "AxW", "+", "self", ".", "W", "[", "l", "]", "(", "sequence_heatmap", ")", "## self loop  N x h", "\n", "AxW", "=", "AxW", "/", "denom", "\n", "graph_output", "=", "torch", ".", "relu", "(", "AxW", ")", "\n", "\n", "# visual gate", "\n", "", "merge_bert_graph", "=", "torch", ".", "cat", "(", "(", "sequence_heatmap", ",", "graph_output", ")", ",", "dim", "=", "-", "1", ")", "\n", "gate_value", "=", "torch", ".", "sigmoid", "(", "self", ".", "gate", "(", "merge_bert_graph", ")", ")", "\n", "gated_converted", "=", "torch", ".", "mul", "(", "gate_value", ",", "graph_output", ")", "\n", "reverse_gate_value", "=", "torch", ".", "neg", "(", "gate_value", ")", ".", "add", "(", "1", ")", "\n", "gated_converted", "=", "torch", ".", "add", "(", "torch", ".", "mul", "(", "reverse_gate_value", ",", "sequence_heatmap", ")", ",", "\n", "torch", ".", "mul", "(", "gate_value", ",", "graph_output", ")", ")", "\n", "sequence_heatmap", "=", "torch", ".", "cat", "(", "(", "sequence_heatmap", ",", "gated_converted", ")", ",", "2", ")", "\n", "\n", "# print(sequence_heatmap.shape)", "\n", "\n", "# lstm_out = self.lstm_f(sequence_heatmap, graph_output)", "\n", "# # backward LSTM", "\n", "# word_rep_b = masked_flip(sequence_heatmap, word_seq_len.tolist())", "\n", "# c_b = masked_flip(graph_input, word_seq_len.tolist())", "\n", "# lstm_out_b = self.lstm_b(word_rep_b, c_b)", "\n", "# lstm_out_b = masked_flip(lstm_out_b, word_seq_len.tolist())", "\n", "\n", "# feature_out = torch.cat((lstm_out, lstm_out_b), dim=2)", "\n", "# feature_out = self.drop_lstm(feature_out)", "\n", "\n", "all_span_rep", "=", "self", ".", "_endpoint_span_extractor", "(", "sequence_heatmap", ",", "all_span_idxs_ltoken", ".", "long", "(", ")", ")", "# [batch, n_span, hidden]", "\n", "if", "not", "self", ".", "args", ".", "use_spanLen", "and", "not", "self", ".", "args", ".", "use_morph", ":", "\n", "# roberta_outputs = self.roberta(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)", "\n", "# sequence_heatmap = roberta_outputs[0]  # [batch, seq_len, hidden]", "\n", "#", "\n", "# # get span_representation with different labels.", "\n", "# # put the positive span in the first and use the span_mask to keep the positive span.", "\n", "# # then, for the negative span, we can random sample n_pos_span *2", "\n", "# all_span_rep = self._endpoint_span_extractor(sequence_heatmap, all_span_idxs_ltoken.long())", "\n", "            ", "all_span_rep", "=", "self", ".", "span_embedding", "(", "all_span_rep", ")", "# (batch,n_span,n_class)", "\n", "\n", "", "elif", "self", ".", "args", ".", "use_spanLen", "and", "not", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "spanlen_rep", "=", "self", ".", "spanLen_embedding", "(", "all_span_lens", ")", "# (bs, n_span, len_dim)", "\n", "spanlen_rep", "=", "F", ".", "relu", "(", "spanlen_rep", ")", "\n", "all_span_rep", "=", "torch", ".", "cat", "(", "(", "all_span_rep", ",", "spanlen_rep", ")", ",", "dim", "=", "-", "1", ")", "\n", "all_span_rep", "=", "self", ".", "span_embedding", "(", "all_span_rep", ")", "# (batch,n_span,n_class)", "\n", "", "elif", "not", "self", ".", "args", ".", "use_spanLen", "and", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "morph_idxs", "=", "loadall", "[", "3", "]", "\n", "span_morph_rep", "=", "self", ".", "morph_embedding", "(", "morph_idxs", ")", "#(bs, n_span, max_spanLen, dim)", "\n", "span_morph_rep", "=", "torch", ".", "sum", "(", "span_morph_rep", ",", "dim", "=", "2", ")", "#(bs, n_span, dim)", "\n", "\n", "all_span_rep", "=", "torch", ".", "cat", "(", "(", "all_span_rep", ",", "span_morph_rep", ")", ",", "dim", "=", "-", "1", ")", "\n", "all_span_rep", "=", "self", ".", "span_embedding", "(", "all_span_rep", ")", "# (batch,n_span,n_class)", "\n", "\n", "", "elif", "self", ".", "args", ".", "use_spanLen", "and", "self", ".", "args", ".", "use_morph", ":", "\n", "            ", "morph_idxs", "=", "loadall", "[", "3", "]", "\n", "span_morph_rep", "=", "self", ".", "morph_embedding", "(", "morph_idxs", ")", "#(bs, n_span, max_spanLen, dim)", "\n", "span_morph_rep", "=", "torch", ".", "sum", "(", "span_morph_rep", ",", "dim", "=", "2", ")", "#(bs, n_span, dim)", "\n", "\n", "spanlen_rep", "=", "self", ".", "spanLen_embedding", "(", "all_span_lens", ")", "# (bs, n_span, len_dim)", "\n", "spanlen_rep", "=", "F", ".", "relu", "(", "spanlen_rep", ")", "\n", "\n", "all_span_rep", "=", "torch", ".", "cat", "(", "(", "all_span_rep", ",", "spanlen_rep", ",", "span_morph_rep", ")", ",", "dim", "=", "-", "1", ")", "\n", "all_span_rep", "=", "self", ".", "span_embedding", "(", "all_span_rep", ")", "# (batch,n_span,n_class)", "\n", "\n", "\n", "", "return", "all_span_rep", "\n", "", "", "class", "MyLSTM", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.__init__": [[177, 192], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "bert_model_spanner.MyLSTM.init_weights", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "input_sz", ",", "hidden_sz", ",", "g_sz", ")", ":", "\n", "        ", "super", "(", "MyLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_sz", "=", "input_sz", "\n", "self", ".", "hidden_sz", "=", "hidden_sz", "\n", "self", ".", "g_sz", "=", "g_sz", "\n", "self", ".", "all1", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "input_sz", "*", "1", ")", ",", "self", ".", "hidden_sz", ")", "\n", "self", ".", "all2", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "input_sz", "+", "self", ".", "g_sz", ")", ",", "self", ".", "hidden_sz", ")", "\n", "self", ".", "all3", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "input_sz", "+", "self", ".", "g_sz", ")", ",", "self", ".", "hidden_sz", ")", "\n", "self", ".", "all4", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "input_sz", "*", "1", ")", ",", "self", ".", "hidden_sz", ")", "\n", "\n", "self", ".", "all11", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "g_sz", ")", ",", "self", ".", "hidden_sz", ")", "\n", "self", ".", "all44", "=", "nn", ".", "Linear", "(", "(", "self", ".", "hidden_sz", "*", "1", "+", "self", ".", "g_sz", ")", ",", "self", ".", "hidden_sz", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "0.5", ")", "\n", "", "def", "init_weights", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.init_weights": [[192, 196], ["bert_model_spanner.MyLSTM.parameters", "math.sqrt", "torch.init.uniform_", "torch.init.uniform_"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "stdv", "=", "1.0", "/", "math", ".", "sqrt", "(", "self", ".", "hidden_sz", ")", "\n", "for", "weight", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "nn", ".", "init", ".", "uniform_", "(", "weight", ",", "-", "stdv", ",", "stdv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.node_forward": [[197, 218], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "bert_model_spanner.MyLSTM.all1", "bert_model_spanner.MyLSTM.all2", "bert_model_spanner.MyLSTM.all3", "bert_model_spanner.MyLSTM.all4", "bert_model_spanner.MyLSTM.all11", "bert_model_spanner.MyLSTM.all44", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["None"], ["", "", "def", "node_forward", "(", "self", ",", "xt", ",", "ht", ",", "Ct_x", ",", "mt", ",", "Ct_m", ")", ":", "\n", "\n", "# # # new standard lstm", "\n", "        ", "hx_concat", "=", "torch", ".", "cat", "(", "(", "ht", ",", "xt", ")", ",", "dim", "=", "1", ")", "\n", "hm_concat", "=", "torch", ".", "cat", "(", "(", "ht", ",", "mt", ")", ",", "dim", "=", "1", ")", "\n", "hxm_concat", "=", "torch", ".", "cat", "(", "(", "ht", ",", "xt", ",", "mt", ")", ",", "dim", "=", "1", ")", "\n", "\n", "\n", "i", "=", "self", ".", "all1", "(", "hx_concat", ")", "\n", "o", "=", "self", ".", "all2", "(", "hxm_concat", ")", "\n", "f", "=", "self", ".", "all3", "(", "hxm_concat", ")", "\n", "u", "=", "self", ".", "all4", "(", "hx_concat", ")", "\n", "ii", "=", "self", ".", "all11", "(", "hm_concat", ")", "\n", "uu", "=", "self", ".", "all44", "(", "hm_concat", ")", "\n", "\n", "i", ",", "f", ",", "o", ",", "u", "=", "torch", ".", "sigmoid", "(", "i", ")", ",", "torch", ".", "sigmoid", "(", "f", ")", ",", "torch", ".", "sigmoid", "(", "o", ")", ",", "torch", ".", "tanh", "(", "u", ")", "\n", "ii", ",", "uu", "=", "torch", ".", "sigmoid", "(", "ii", ")", ",", "torch", ".", "tanh", "(", "uu", ")", "\n", "Ct_x", "=", "i", "*", "u", "+", "ii", "*", "uu", "+", "f", "*", "Ct_x", "\n", "ht", "=", "o", "*", "torch", ".", "tanh", "(", "Ct_x", ")", "\n", "\n", "return", "ht", ",", "Ct_x", ",", "Ct_m", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.forward": [[219, 243], ["x.size", "range", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "bert_model_spanner.MyLSTM.node_forward", "torch.stack().permute.append", "torch.stack().permute.append", "cell_seq.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.MyLSTM.node_forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "m", ",", "init_stat", "=", "None", ")", ":", "\n", "        ", "batch_sz", ",", "seq_sz", ",", "_", "=", "x", ".", "size", "(", ")", "\n", "hidden_seq", "=", "[", "]", "\n", "cell_seq", "=", "[", "]", "\n", "if", "init_stat", "is", "None", ":", "\n", "            ", "ht", "=", "torch", ".", "zeros", "(", "(", "batch_sz", ",", "self", ".", "hidden_sz", ")", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "Ct_x", "=", "torch", ".", "zeros", "(", "(", "batch_sz", ",", "self", ".", "hidden_sz", ")", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "Ct_m", "=", "torch", ".", "zeros", "(", "(", "batch_sz", ",", "self", ".", "hidden_sz", ")", ")", ".", "to", "(", "x", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "ht", ",", "Ct", "=", "init_stat", "\n", "", "for", "t", "in", "range", "(", "seq_sz", ")", ":", "# iterate over the time steps", "\n", "            ", "xt", "=", "x", "[", ":", ",", "t", ",", ":", "]", "\n", "mt", "=", "m", "[", ":", ",", "t", ",", ":", "]", "\n", "ht", ",", "Ct_x", ",", "Ct_m", "=", "self", ".", "node_forward", "(", "xt", ",", "ht", ",", "Ct_x", ",", "mt", ",", "Ct_m", ")", "\n", "hidden_seq", ".", "append", "(", "ht", ")", "\n", "cell_seq", ".", "append", "(", "Ct_x", ")", "\n", "if", "t", "==", "0", ":", "\n", "                ", "mht", "=", "ht", "\n", "mct", "=", "Ct_x", "\n", "", "else", ":", "\n", "                ", "mht", "=", "torch", ".", "max", "(", "torch", ".", "stack", "(", "hidden_seq", ")", ",", "dim", "=", "0", ")", "[", "0", "]", "\n", "mct", "=", "torch", ".", "max", "(", "torch", ".", "stack", "(", "cell_seq", ")", ",", "dim", "=", "0", ")", "[", "0", "]", "\n", "", "", "hidden_seq", "=", "torch", ".", "stack", "(", "hidden_seq", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "##batch_size x max_len x hidden", "\n", "return", "hidden_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.bert_model_spanner.masked_flip": [[244, 266], ["padded_sequence.size", "torch.flip", "torch.flip", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "padded_sequence.size", "len", "len", "padded_sequence.size", "enumerate"], "function", ["None"], ["", "", "def", "masked_flip", "(", "padded_sequence", ":", "torch", ".", "Tensor", ",", "sequence_lengths", ")", ":", "\n", "    ", "\"\"\"\n        Flips a padded tensor along the time dimension without affecting masked entries.\n        # Parameters\n        padded_sequence : `torch.Tensor`\n            The tensor to flip along the time dimension.\n            Assumed to be of dimensions (batch size, num timesteps, ...)\n        sequence_lengths : `torch.Tensor`\n            A list containing the lengths of each unpadded sequence in the batch.\n        # Returns\n        A `torch.Tensor` of the same shape as padded_sequence.\n        \"\"\"", "\n", "assert", "padded_sequence", ".", "size", "(", "0", ")", "==", "len", "(", "\n", "sequence_lengths", "\n", ")", ",", "f\"sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}\"", "\n", "num_timesteps", "=", "padded_sequence", ".", "size", "(", "1", ")", "\n", "flipped_padded_sequence", "=", "torch", ".", "flip", "(", "padded_sequence", ",", "[", "1", "]", ")", "\n", "sequences", "=", "[", "\n", "flipped_padded_sequence", "[", "i", ",", "num_timesteps", "-", "length", ":", "]", "\n", "for", "i", ",", "length", "in", "enumerate", "(", "sequence_lengths", ")", "\n", "]", "\n", "return", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_sequence", "(", "sequences", ",", "batch_first", "=", "True", ")", "", "", ""]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.SingleLinearClassifier.__init__": [[9, 13], ["torch.Module.__init__", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "num_label", ")", ":", "\n", "        ", "super", "(", "SingleLinearClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_label", "=", "num_label", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.SingleLinearClassifier.forward": [[14, 17], ["classifier.SingleLinearClassifier.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_features", ")", ":", "\n", "        ", "features_output", "=", "self", ".", "classifier", "(", "input_features", ")", "\n", "return", "features_output", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.MultiNonLinearClassifier.__init__": [[20, 26], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "num_label", ",", "dropout_rate", ")", ":", "\n", "        ", "super", "(", "MultiNonLinearClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_label", "=", "num_label", "\n", "self", ".", "classifier1", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "classifier2", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "num_label", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.models.classifier.MultiNonLinearClassifier.forward": [[27, 35], ["classifier.MultiNonLinearClassifier.classifier1", "torch.nn.functional.gelu", "classifier.MultiNonLinearClassifier.dropout", "classifier.MultiNonLinearClassifier.classifier2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_features", ")", ":", "\n", "        ", "features_output1", "=", "self", ".", "classifier1", "(", "input_features", ")", "\n", "# features_output1 = F.relu(features_output1)", "\n", "features_output1", "=", "F", ".", "gelu", "(", "features_output1", ")", "\n", "features_output1", "=", "self", ".", "dropout", "(", "features_output1", ")", "\n", "\n", "features_output2", "=", "self", ".", "classifier2", "(", "features_output1", ")", "\n", "return", "features_output2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.__init__": [[28, 47], ["json.load", "int", "open"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "args", ",", "json_path", ",", "tokenizer", ":", "BertWordPieceTokenizer", ",", "max_length", ":", "int", "=", "128", ",", "possible_only", "=", "False", ",", "\n", "pad_to_maxlen", "=", "False", ")", ":", "\n", "        ", "self", ".", "all_data", "=", "json", ".", "load", "(", "open", "(", "json_path", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "self", ".", "tokenzier", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "max_length", "\n", "self", ".", "possible_only", "=", "possible_only", "\n", "if", "self", ".", "possible_only", ":", "\n", "            ", "self", ".", "all_data", "=", "[", "\n", "x", "for", "x", "in", "self", ".", "all_data", "if", "x", "[", "\"start_position\"", "]", "\n", "]", "\n", "", "self", ".", "pad_to_maxlen", "=", "pad_to_maxlen", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "max_spanLen", "=", "self", ".", "args", ".", "max_spanLen", "\n", "minus", "=", "int", "(", "(", "self", ".", "max_spanLen", "+", "1", ")", "*", "self", ".", "max_spanLen", "/", "2", ")", "\n", "self", ".", "max_num_span", "=", "self", ".", "max_length", "*", "self", ".", "max_spanLen", "-", "minus", "\n", "self", ".", "dataname", "=", "self", ".", "args", ".", "dataname", "\n", "self", ".", "spancase2idx_dic", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.__len__": [[48, 50], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "all_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.__getitem__": [[51, 239], ["data[].strip", "span_idxLab.items", "context.replace.replace.split", "zip", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "dataload.BERTNERDataset.case_feature_tokenLevel", "enumerate", "tokenizer_.encode", "tokenizer.encode", "dataload.BERTNERDataset.convert2tokenIdx", "all_span_idxs_new_label.items", "tokenizer.token_to_id", "numpy.ones_like", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "numpy.max", "int", "int", "context.replace.replace.replace", "seidx.split", "sidxs.append", "eidxs.append", "pos_span_idxs.append", "context.replace.replace.split", "dataload.BERTNERDataset.append", "dataload.BERTNERDataset.append", "orig_to_tok_index.append", "tokenizer_.tokenize", "dataload.BERTNERDataset.append", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "dataload.BERTNERDataset.pad", "numpy.array", "context.replace.replace.replace", "int", "int", "len", "tokens_.append", "len", "context.replace.replace.replace"], "methods", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.case_feature_tokenLevel", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.convert2tokenIdx", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad", "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            item: int, idx\n        Returns:\n            tokens: tokens of query + context, [seq_len]\n            token_type_ids: token type ids, 0 for query, 1 for context, [seq_len]\n            start_labels: start labels of NER in tokens, [seq_len]\n            end_labels: end labels of NER in tokens, [seq_len]\n            label_mask: label mask, 1 for counting into loss, 0 for ignoring. [seq_len]\n            match_labels: match labels, [seq_len, seq_len]\n            sample_idx: sample id\n            label_idx: label id\n\n        \"\"\"", "\n", "cls_tok", "=", "\"[CLS]\"", "\n", "sep_tok", "=", "\"[SEP]\"", "\n", "\n", "# begin{get the label2idx dictionary}", "\n", "label2idx", "=", "{", "}", "\n", "label2idx_list", "=", "self", ".", "args", ".", "label2idx_list", "\n", "for", "labidx", "in", "label2idx_list", ":", "\n", "            ", "lab", ",", "idx", "=", "labidx", "\n", "label2idx", "[", "lab", "]", "=", "int", "(", "idx", ")", "\n", "# end{get the label2idx dictionary}", "\n", "\n", "# begin{get the morph2idx dictionary}", "\n", "", "morph2idx", "=", "{", "}", "\n", "morph2idx_list", "=", "self", ".", "args", ".", "morph2idx_list", "\n", "for", "morphidx", "in", "morph2idx_list", ":", "\n", "            ", "morph", ",", "idx", "=", "morphidx", "\n", "morph2idx", "[", "morph", "]", "=", "int", "(", "idx", ")", "\n", "# end{get the morph2idx dictionary}", "\n", "\n", "", "data", "=", "self", ".", "all_data", "[", "item", "]", "\n", "tokenizer", "=", "self", ".", "tokenzier", "\n", "\n", "context", "=", "data", "[", "\"context\"", "]", ".", "strip", "(", ")", "\n", "# print(context)", "\n", "heads", "=", "data", "[", "'heads'", "]", "\n", "\n", "if", "'\\u200b'", "in", "context", ":", "\n", "            ", "context", "=", "context", ".", "replace", "(", "'\\u200b'", ",", "''", ")", "\n", "", "elif", "'\\ufeff'", "in", "context", ":", "\n", "            ", "context", "=", "context", ".", "replace", "(", "'\\ufeff'", ",", "''", ")", "\n", "", "elif", "'  '", "in", "context", ":", "\n", "            ", "context", "=", "context", ".", "replace", "(", "'  '", ",", "' '", ")", "\n", "\n", "", "span_idxLab", "=", "data", "[", "\"span_posLabel\"", "]", "\n", "\n", "sidxs", "=", "[", "]", "\n", "eidxs", "=", "[", "]", "\n", "for", "seidx", ",", "label", "in", "span_idxLab", ".", "items", "(", ")", ":", "\n", "            ", "sidx", ",", "eidx", "=", "seidx", ".", "split", "(", "';'", ")", "\n", "sidxs", ".", "append", "(", "int", "(", "sidx", ")", ")", "\n", "eidxs", ".", "append", "(", "int", "(", "eidx", ")", ")", "\n", "\n", "# add space offsets", "\n", "", "words", "=", "context", ".", "split", "(", ")", "\n", "\n", "# convert the span position into the character index, space is also a position.", "\n", "pos_span_idxs", "=", "[", "]", "\n", "for", "sidx", ",", "eidx", "in", "zip", "(", "sidxs", ",", "eidxs", ")", ":", "\n", "            ", "pos_span_idxs", ".", "append", "(", "(", "sidx", ",", "eidx", ")", ")", "\n", "\n", "# all span (sidx, eidx)", "\n", "", "all_span_idxs", "=", "enumerate_spans", "(", "context", ".", "split", "(", ")", ",", "offset", "=", "0", ",", "max_span_width", "=", "self", ".", "args", ".", "max_spanLen", ")", "\n", "\n", "# begin{compute the span weight}", "\n", "all_span_weights", "=", "[", "]", "\n", "for", "span_idx", "in", "all_span_idxs", ":", "\n", "            ", "weight", "=", "self", ".", "args", ".", "neg_span_weight", "\n", "if", "span_idx", "in", "pos_span_idxs", ":", "\n", "                ", "weight", "=", "1.0", "\n", "", "all_span_weights", ".", "append", "(", "weight", ")", "\n", "# end{compute the span weight}", "\n", "\n", "", "all_span_lens", "=", "[", "]", "\n", "for", "idxs", "in", "all_span_idxs", ":", "\n", "            ", "sid", ",", "eid", "=", "idxs", "\n", "slen", "=", "eid", "-", "sid", "+", "1", "\n", "all_span_lens", ".", "append", "(", "slen", ")", "\n", "\n", "\n", "", "morph_idxs", "=", "self", ".", "case_feature_tokenLevel", "(", "morph2idx", ",", "all_span_idxs", ",", "words", ",", "self", ".", "args", ".", "max_spanLen", ")", "\n", "\n", "# print(words)", "\n", "tokens_", "=", "[", "]", "\n", "orig_to_tok_index", "=", "[", "]", "# 0 - >0, 1-> len(all word_piece)", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "orig_to_tok_index", ".", "append", "(", "len", "(", "tokens_", ")", ")", "\n", "# word_tokens = tokenizer.tokenize(word.lower())", "\n", "word_tokens", "=", "tokenizer_", ".", "tokenize", "(", "word", ")", "\n", "for", "sub_token", "in", "word_tokens", ":", "\n", "# orig_to_tok_index.append(i)", "\n", "                ", "tokens_", ".", "append", "(", "sub_token", ")", "\n", "", "", "vec", "=", "tokenizer_", ".", "encode", "(", "tokens_", ",", "show_tokens", "=", "True", ",", "is_tokenized", "=", "True", ")", "\n", "# vec = vec[0][:, 1:, :][:, orig_to_tok_index, :]", "\n", "# print(\"vec:\", vec)", "\n", "# print(\"Orig2Token:\", orig_to_tok_index)", "\n", "# print(\"Tokens:\", tokens_)", "\n", "\n", "context_tokens", "=", "tokenizer", ".", "encode", "(", "context", ",", "add_special_tokens", "=", "True", ")", "\n", "# print(\"context_tokens\",context_tokens.ids)", "\n", "# sys.exit()", "\n", "tokens", "=", "context_tokens", ".", "ids", "# subword index", "\n", "type_ids", "=", "context_tokens", ".", "type_ids", "# the split of two sentence on the subword-level, 0 for first sent, 1 for the second sent", "\n", "offsets", "=", "context_tokens", ".", "offsets", "# the subword's start-index and end-idx of the character-level.", "\n", "\n", "all_span_idxs_ltoken", ",", "all_span_word", ",", "all_span_idxs_new_label", "=", "self", ".", "convert2tokenIdx", "(", "words", ",", "tokens", ",", "type_ids", ",", "\n", "offsets", ",", "all_span_idxs", ",", "\n", "span_idxLab", ")", "\n", "span_label_ltoken", "=", "[", "]", "\n", "for", "seidx_str", ",", "label", "in", "all_span_idxs_new_label", ".", "items", "(", ")", ":", "\n", "            ", "span_label_ltoken", ".", "append", "(", "label2idx", "[", "label", "]", ")", "\n", "\n", "\n", "", "'''\n        an example of tokens, type_ids, and offsets value.\n        inputs: \n            query = \"you are beautiful .\"\n            context = 'i love you .'\n\n        outputs:\n            tokens:  [101, 2017, 2024, 3376, 1012, 102, 1045, 2293, 2017, 1012, 102]\n            type_ids:  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n            offsets:  [(0, 0), (0, 3), (4, 7), (8, 17), (18, 19), (0, 0), (0, 1), (2, 6), (7, 10), (11, 12), (0, 0)]\n            query_context_tokens.tokens: ['[CLS]', 'you', 'are', 'beautiful', '.', '[SEP]', 'i', 'love', 'you', '.', '[SEP]']\n            query_context_tokens.words:  [None, 0, 1, 2, 3, None, 0, 1, 2, 3, None]\n        '''", "\n", "\n", "# # the max-end-index should not exceed the max-length.", "\n", "# all_span_idxs_ltoken", "\n", "\n", "# return  tokens, type_ids, all_span_idxs_ltoken, pos_span_mask_ltoken", "\n", "tokens", "=", "tokens", "[", ":", "self", ".", "max_length", "]", "\n", "type_ids", "=", "type_ids", "[", ":", "self", ".", "max_length", "]", "\n", "all_span_idxs_ltoken", "=", "all_span_idxs_ltoken", "[", ":", "self", ".", "max_num_span", "]", "\n", "span_label_ltoken", "=", "span_label_ltoken", "[", ":", "self", ".", "max_num_span", "]", "\n", "all_span_lens", "=", "all_span_lens", "[", ":", "self", ".", "max_num_span", "]", "\n", "morph_idxs", "=", "morph_idxs", "[", ":", "self", ".", "max_num_span", "]", "\n", "all_span_weights", "=", "all_span_weights", "[", ":", "self", ".", "max_num_span", "]", "\n", "\n", "# make sure last token is [SEP]", "\n", "sep_token", "=", "tokenizer", ".", "token_to_id", "(", "sep_tok", ")", "\n", "if", "tokens", "[", "-", "1", "]", "!=", "sep_token", ":", "\n", "            ", "assert", "len", "(", "tokens", ")", "==", "self", ".", "max_length", "\n", "tokens", "=", "tokens", "[", ":", "-", "1", "]", "+", "[", "sep_token", "]", "\n", "\n", "# padding to the max length.", "\n", "", "import", "numpy", "as", "np", "\n", "real_span_mask_ltoken", "=", "np", ".", "ones_like", "(", "span_label_ltoken", ")", "\n", "if", "self", ".", "pad_to_maxlen", ":", "\n", "            ", "tokens", "=", "self", ".", "pad", "(", "tokens", ",", "0", ")", "\n", "type_ids", "=", "self", ".", "pad", "(", "type_ids", ",", "1", ")", "\n", "all_span_idxs_ltoken", "=", "self", ".", "pad", "(", "all_span_idxs_ltoken", ",", "value", "=", "(", "0", ",", "0", ")", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "real_span_mask_ltoken", "=", "self", ".", "pad", "(", "real_span_mask_ltoken", ",", "value", "=", "0", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "span_label_ltoken", "=", "self", ".", "pad", "(", "span_label_ltoken", ",", "value", "=", "0", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "all_span_lens", "=", "self", ".", "pad", "(", "all_span_lens", ",", "value", "=", "0", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "morph_idxs", "=", "self", ".", "pad", "(", "morph_idxs", ",", "value", "=", "0", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "all_span_weights", "=", "self", ".", "pad", "(", "all_span_weights", ",", "value", "=", "0", ",", "max_length", "=", "self", ".", "max_num_span", ")", "\n", "\n", "", "tokens", "=", "torch", ".", "LongTensor", "(", "tokens", ")", "\n", "type_ids", "=", "torch", ".", "LongTensor", "(", "type_ids", ")", "# use to split the first and second sentence.", "\n", "all_span_idxs_ltoken", "=", "torch", ".", "LongTensor", "(", "all_span_idxs_ltoken", ")", "\n", "real_span_mask_ltoken", "=", "torch", ".", "LongTensor", "(", "real_span_mask_ltoken", ")", "\n", "span_label_ltoken", "=", "torch", ".", "LongTensor", "(", "span_label_ltoken", ")", "\n", "all_span_lens", "=", "torch", ".", "LongTensor", "(", "all_span_lens", ")", "\n", "morph_idxs", "=", "torch", ".", "LongTensor", "(", "morph_idxs", ")", "\n", "all_span_weights", "=", "torch", ".", "Tensor", "(", "all_span_weights", ")", "\n", "\n", "min_idx", "=", "np", ".", "max", "(", "np", ".", "array", "(", "all_span_idxs_ltoken", ")", ")", "\n", "\n", "\n", "return", "[", "\n", "tokens", ",", "\n", "type_ids", ",", "# use to split the first and second sentence.", "\n", "all_span_idxs_ltoken", ",", "\n", "morph_idxs", ",", "\n", "span_label_ltoken", ",", "\n", "all_span_lens", ",", "\n", "all_span_weights", ",", "\n", "real_span_mask_ltoken", ",", "\n", "words", ",", "\n", "all_span_word", ",", "\n", "all_span_idxs", ",", "\n", "heads", ",", "\n", "orig_to_tok_index", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.case_feature_tokenLevel": [[242, 269], ["enumerate", "caseidxs.append", "token.isupper", "range", "token.islower", "token.istitle", "token.isdigit"], "methods", ["None"], ["", "def", "case_feature_tokenLevel", "(", "self", ",", "morph2idx", ",", "span_idxs", ",", "words", ",", "max_spanlen", ")", ":", "\n", "        ", "'''\n        this function use to characterize the capitalization feature.\n        :return:\n        '''", "\n", "caseidxs", "=", "[", "]", "\n", "\n", "for", "idxs", "in", "span_idxs", ":", "\n", "            ", "sid", ",", "eid", "=", "idxs", "\n", "span_word", "=", "words", "[", "sid", ":", "eid", "+", "1", "]", "\n", "caseidx1", "=", "[", "0", "for", "_", "in", "range", "(", "max_spanlen", ")", "]", "\n", "for", "j", ",", "token", "in", "enumerate", "(", "span_word", ")", ":", "\n", "                ", "tfeat", "=", "''", "\n", "if", "token", ".", "isupper", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'isupper'", "\n", "", "elif", "token", ".", "islower", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'islower'", "\n", "", "elif", "token", ".", "istitle", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'istitle'", "\n", "", "elif", "token", ".", "isdigit", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'isdigit'", "\n", "", "else", ":", "\n", "                    ", "tfeat", "=", "'other'", "\n", "", "caseidx1", "[", "j", "]", "=", "morph2idx", "[", "tfeat", "]", "\n", "", "caseidxs", ".", "append", "(", "caseidx1", ")", "\n", "\n", "", "return", "caseidxs", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.case_feature_spanLevel": [[271, 304], ["caseidx.append", "token.isupper", "caseidx1.append", "token.islower", "len", "token.istitle", "token.isdigit"], "methods", ["None"], ["", "def", "case_feature_spanLevel", "(", "self", ",", "spancase2idx_dic", ",", "span_idxs", ",", "words", ")", ":", "\n", "        ", "'''\n        this function use to characterize the capitalization feature.\n        :return:\n        '''", "\n", "\n", "case2idx", "=", "{", "'isupper'", ":", "0", ",", "'islower'", ":", "1", ",", "'istitle'", ":", "2", ",", "'isdigit'", ":", "3", ",", "'other'", ":", "4", "}", "\n", "\n", "caseidx", "=", "[", "]", "\n", "for", "idxs", "in", "span_idxs", ":", "\n", "            ", "sid", ",", "eid", "=", "idxs", "\n", "span_word", "=", "words", "[", "sid", ":", "eid", "+", "1", "]", "\n", "caseidx1", "=", "[", "]", "\n", "for", "token", "in", "span_word", ":", "\n", "                ", "tfeat", "=", "''", "\n", "if", "token", ".", "isupper", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'isupper'", "\n", "", "elif", "token", ".", "islower", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'islower'", "\n", "", "elif", "token", ".", "istitle", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'istitle'", "\n", "", "elif", "token", ".", "isdigit", "(", ")", ":", "\n", "                    ", "tfeat", "=", "'isdigit'", "\n", "", "else", ":", "\n", "                    ", "tfeat", "=", "'other'", "\n", "", "caseidx1", ".", "append", "(", "tfeat", ")", "\n", "\n", "", "caseidx1_str", "=", "' '", ".", "join", "(", "caseidx1", ")", "\n", "if", "caseidx1_str", "not", "in", "spancase2idx_dic", ":", "\n", "                ", "spancase2idx_dic", "[", "caseidx1_str", "]", "=", "len", "(", "spancase2idx_dic", ")", "+", "1", "\n", "", "caseidx", ".", "append", "(", "spancase2idx_dic", "[", "caseidx1_str", "]", ")", "\n", "\n", "", "return", "caseidx", ",", "spancase2idx_dic", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.pad": [[307, 312], ["len", "lst.append"], "methods", ["None"], ["", "def", "pad", "(", "self", ",", "lst", ",", "value", "=", "None", ",", "max_length", "=", "None", ")", ":", "\n", "        ", "max_length", "=", "max_length", "or", "self", ".", "max_length", "\n", "while", "len", "(", "lst", ")", "<", "max_length", ":", "\n", "            ", "lst", ".", "append", "(", "value", ")", "\n", "", "return", "lst", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.convert2tokenIdx": [[313, 366], ["zip", "range", "zip", "zip", "len", "span_new_sidxs.append", "span_new_eidxs.append", "all_span_word.append", "span_idxs_ltoken.append", "sum", "sum", "len", "len"], "methods", ["None"], ["", "def", "convert2tokenIdx", "(", "self", ",", "words", ",", "tokens", ",", "type_ids", ",", "offsets", ",", "span_idxs", ",", "span_idxLab", ")", ":", "\n", "# convert the all the span_idxs from word-level to token-level", "\n", "        ", "max_length", "=", "self", ".", "max_length", "\n", "sidxs", "=", "[", "x1", "+", "sum", "(", "[", "len", "(", "w", ")", "for", "w", "in", "words", "[", ":", "x1", "]", "]", ")", "for", "(", "x1", ",", "x2", ")", "in", "span_idxs", "]", "\n", "eidxs", "=", "[", "x2", "+", "sum", "(", "[", "len", "(", "w", ")", "for", "w", "in", "words", "[", ":", "x2", "+", "1", "]", "]", ")", "for", "(", "x1", ",", "x2", ")", "in", "span_idxs", "]", "\n", "\n", "span_idxs_new_label", "=", "{", "}", "\n", "for", "ns", ",", "ne", ",", "ose", "in", "zip", "(", "sidxs", ",", "eidxs", ",", "span_idxs", ")", ":", "\n", "            ", "os", ",", "oe", "=", "ose", "\n", "oes_str", "=", "\"{};{}\"", ".", "format", "(", "os", ",", "oe", ")", "\n", "nes_str", "=", "\"{};{}\"", ".", "format", "(", "ns", ",", "ne", ")", "\n", "if", "oes_str", "in", "span_idxLab", ":", "\n", "                ", "label", "=", "span_idxLab", "[", "oes_str", "]", "\n", "span_idxs_new_label", "[", "nes_str", "]", "=", "label", "\n", "", "else", ":", "\n", "                ", "span_idxs_new_label", "[", "nes_str", "]", "=", "'O'", "\n", "\n", "", "", "origin_offset2token_sidx", "=", "{", "}", "\n", "origin_offset2token_eidx", "=", "{", "}", "\n", "for", "token_idx", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "# skip query tokens", "\n", "            ", "token_start", ",", "token_end", "=", "offsets", "[", "token_idx", "]", "\n", "\n", "# skip [CLS] or [SEP]", "\n", "if", "token_start", "==", "token_end", "==", "0", ":", "\n", "                ", "continue", "\n", "", "origin_offset2token_sidx", "[", "token_start", "]", "=", "token_idx", "\n", "origin_offset2token_eidx", "[", "token_end", "]", "=", "token_idx", "\n", "\n", "# convert the position from character-level to token-level.", "\n", "", "span_new_sidxs", "=", "[", "]", "\n", "span_new_eidxs", "=", "[", "]", "\n", "n_span_keep", "=", "0", "\n", "\n", "for", "start", ",", "end", "in", "zip", "(", "sidxs", ",", "eidxs", ")", ":", "\n", "            ", "if", "origin_offset2token_eidx", "[", "end", "]", ">", "max_length", "-", "1", "or", "origin_offset2token_sidx", "[", "\n", "start", "]", ">", "max_length", "-", "1", ":", "\n", "                ", "continue", "\n", "", "span_new_sidxs", ".", "append", "(", "origin_offset2token_sidx", "[", "start", "]", ")", "\n", "span_new_eidxs", ".", "append", "(", "origin_offset2token_eidx", "[", "end", "]", ")", "\n", "n_span_keep", "+=", "1", "\n", "\n", "", "all_span_word", "=", "[", "]", "\n", "for", "(", "sidx", ",", "eidx", ")", "in", "span_idxs", ":", "\n", "            ", "all_span_word", ".", "append", "(", "words", "[", "sidx", ":", "eidx", "+", "1", "]", ")", "\n", "", "all_span_word", "=", "all_span_word", "[", ":", "n_span_keep", "+", "1", "]", "\n", "\n", "span_idxs_ltoken", "=", "[", "]", "\n", "for", "sidx", ",", "eidx", "in", "zip", "(", "span_new_sidxs", ",", "span_new_eidxs", ")", ":", "\n", "            ", "span_idxs_ltoken", ".", "append", "(", "(", "sidx", ",", "eidx", ")", ")", "\n", "\n", "\n", "", "return", "span_idxs_ltoken", ",", "all_span_word", ",", "span_idxs_new_label", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.head_to_adj": [[367, 388], ["numpy.zeros", "enumerate", "range", "len"], "methods", ["None"], ["", "def", "head_to_adj", "(", "max_len", ",", "inst", ",", "config", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Convert a tree object to an (numpy) adjacency matrix.\n        \"\"\"", "\n", "# directed = config.adj_directed", "\n", "self_loop", "=", "False", "#config.adj_self_loop", "\n", "ret", "=", "np", ".", "zeros", "(", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "for", "i", ",", "head", "in", "enumerate", "(", "inst", ".", "input", ".", "heads", ")", ":", "\n", "            ", "if", "head", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "", "ret", "[", "head", ",", "i", "]", "=", "1", "\n", "\n", "", "if", "not", "directed", ":", "\n", "            ", "ret", "=", "ret", "+", "ret", ".", "T", "\n", "\n", "", "if", "self_loop", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "inst", ".", "input", ".", "words", ")", ")", ":", "\n", "                ", "ret", "[", "i", ",", "i", "]", "=", "1", "\n", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.dataload.BERTNERDataset.head_to_adj_label": [[389, 411], ["numpy.zeros", "enumerate", "range", "len"], "methods", ["None"], ["", "def", "head_to_adj_label", "(", "max_len", ",", "inst", ",", "config", ")", ":", "\n", "        ", "\"\"\"\n        Convert a tree object to an (numpy) adjacency matrix.\n        \"\"\"", "\n", "directed", "=", "config", ".", "adj_directed", "\n", "self_loop", "=", "config", ".", "adj_self_loop", "\n", "\n", "dep_label_ret", "=", "np", ".", "zeros", "(", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "np", ".", "long", ")", "\n", "\n", "for", "i", ",", "head", "in", "enumerate", "(", "inst", ".", "input", ".", "heads", ")", ":", "\n", "            ", "if", "head", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "", "dep_label_ret", "[", "head", ",", "i", "]", "=", "inst", ".", "dep_label_ids", "[", "i", "]", "\n", "\n", "", "if", "not", "directed", ":", "\n", "            ", "dep_label_ret", "=", "dep_label_ret", "+", "dep_label_ret", ".", "T", "\n", "\n", "", "if", "self_loop", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "inst", ".", "input", ".", "words", ")", ")", ":", "\n", "                ", "dep_label_ret", "[", "i", ",", "i", "]", "=", "config", ".", "root_dep_label_id", "\n", "\n", "", "", "return", "dep_label_ret", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__init__": [[8, 11], ["min", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ":", "Dataset", ",", "max_num", ":", "int", "=", "100", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "max_num", "=", "min", "(", "max_num", ",", "len", "(", "self", ".", "dataset", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__len__": [[12, 14], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "max_num", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__getitem__": [[15, 17], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "item", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.truncate_dataset.TruncateDataset.__getattr__": [[18, 21], ["getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "item", ")", ":", "\n", "        ", "\"\"\"other dataset func\"\"\"", "\n", "return", "getattr", "(", "self", ".", "dataset", ",", "item", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.collate_functions.collate_to_max_length": [[9, 96], ["len", "max", "max", "range", "range", "torch.Tensor", "range", "output.append", "len", "range", "torch.LongTensor", "range", "output.append", "range", "output.append", "range", "output.append", "range", "output.append", "output.append", "torch.full", "range", "output.append", "range", "torch.Tensor.append", "range", "torch.LongTensor.append", "torch.full", "range", "output.append", "words.append", "all_span_word.append", "all_span_idxs.append", "collate_functions.head_to_adj", "sma.append", "range", "sma.append", "range"], "function", ["home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.collate_functions.head_to_adj"], ["def", "collate_to_max_length", "(", "batch", ":", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", ")", "->", "List", "[", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    pad to maximum length of this batch\n    Args:\n        batch: a batch of samples, each contains a list of field data(Tensor):\n             tokens,type_ids,all_span_idxs_ltoken,morph_idxs, ...\n    Returns:\n        output: list of field batched data, which shape is [batch, max_length]\n    \"\"\"", "\n", "\n", "batch_size", "=", "len", "(", "batch", ")", "\n", "max_length", "=", "max", "(", "x", "[", "0", "]", ".", "shape", "[", "0", "]", "for", "x", "in", "batch", ")", "\n", "sent_max_len", "=", "[", "x", "[", "0", "]", ".", "shape", "[", "0", "]", "for", "x", "in", "batch", "]", "\n", "max_num_span", "=", "max", "(", "x", "[", "3", "]", ".", "shape", "[", "0", "]", "for", "x", "in", "batch", ")", "\n", "output", "=", "[", "]", "\n", "\n", "for", "field_idx", "in", "range", "(", "2", ")", ":", "\n", "        ", "pad_output", "=", "torch", ".", "full", "(", "[", "batch_size", ",", "max_length", "]", ",", "0", ",", "dtype", "=", "batch", "[", "0", "]", "[", "field_idx", "]", ".", "dtype", ")", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "data", "=", "batch", "[", "sample_idx", "]", "[", "field_idx", "]", "\n", "pad_output", "[", "sample_idx", "]", "[", ":", "data", ".", "shape", "[", "0", "]", "]", "=", "data", "\n", "", "output", ".", "append", "(", "pad_output", ")", "\n", "\n", "# begin{for the pad_all_span_idxs_ltoken... }", "\n", "", "pad_all_span_idxs_ltoken", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "sma", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "max_num_span", ")", ":", "\n", "            ", "sma", ".", "append", "(", "(", "0", ",", "0", ")", ")", "\n", "", "pad_all_span_idxs_ltoken", ".", "append", "(", "sma", ")", "\n", "", "pad_all_span_idxs_ltoken", "=", "torch", ".", "Tensor", "(", "pad_all_span_idxs_ltoken", ")", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "data", "=", "batch", "[", "sample_idx", "]", "[", "2", "]", "\n", "pad_all_span_idxs_ltoken", "[", "sample_idx", ",", ":", "data", ".", "shape", "[", "0", "]", ",", ":", "]", "=", "data", "\n", "", "output", ".", "append", "(", "pad_all_span_idxs_ltoken", ")", "\n", "# end{for the pad_all_span_idxs_ltoken... }", "\n", "\n", "\n", "# begin{for the morph feature... morph_idxs}", "\n", "pad_morph_len", "=", "len", "(", "batch", "[", "0", "]", "[", "3", "]", "[", "0", "]", ")", "\n", "pad_morph", "=", "[", "0", "for", "i", "in", "range", "(", "pad_morph_len", ")", "]", "\n", "pad_morph_idxs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "sma", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "max_num_span", ")", ":", "\n", "            ", "sma", ".", "append", "(", "pad_morph", ")", "\n", "", "pad_morph_idxs", ".", "append", "(", "sma", ")", "\n", "", "pad_morph_idxs", "=", "torch", ".", "LongTensor", "(", "pad_morph_idxs", ")", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "data", "=", "batch", "[", "sample_idx", "]", "[", "3", "]", "\n", "pad_morph_idxs", "[", "sample_idx", ",", ":", "data", ".", "shape", "[", "0", "]", ",", ":", "]", "=", "data", "\n", "", "output", ".", "append", "(", "pad_morph_idxs", ")", "\n", "# end{for the morph feature... morph_idxs}", "\n", "\n", "\n", "for", "field_idx", "in", "[", "4", ",", "5", ",", "6", ",", "7", "]", ":", "\n", "        ", "pad_output", "=", "torch", ".", "full", "(", "[", "batch_size", ",", "max_num_span", "]", ",", "0", ",", "dtype", "=", "batch", "[", "0", "]", "[", "field_idx", "]", ".", "dtype", ")", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "data", "=", "batch", "[", "sample_idx", "]", "[", "field_idx", "]", "\n", "pad_output", "[", "sample_idx", "]", "[", ":", "data", ".", "shape", "[", "0", "]", "]", "=", "data", "\n", "", "output", ".", "append", "(", "pad_output", ")", "\n", "\n", "", "words", "=", "[", "]", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "words", ".", "append", "(", "batch", "[", "sample_idx", "]", "[", "8", "]", ")", "\n", "", "output", ".", "append", "(", "words", ")", "\n", "\n", "\n", "all_span_word", "=", "[", "]", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "all_span_word", ".", "append", "(", "batch", "[", "sample_idx", "]", "[", "9", "]", ")", "\n", "", "output", ".", "append", "(", "all_span_word", ")", "\n", "\n", "all_span_idxs", "=", "[", "]", "\n", "for", "sample_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "all_span_idxs", ".", "append", "(", "batch", "[", "sample_idx", "]", "[", "10", "]", ")", "\n", "", "output", ".", "append", "(", "all_span_idxs", ")", "\n", "\n", "# all_span_heads = []", "\n", "# for sample_idx in range(batch_size):", "\n", "#     all_span_heads.append(batch[sample_idx][11])", "\n", "\n", "adjs", "=", "[", "head_to_adj", "(", "max_length", ",", "batch", "[", "sample_idx", "]", "[", "11", "]", ",", "batch", "[", "sample_idx", "]", "[", "12", "]", ",", "sent_max_len", "[", "sample_idx", "]", ")", "for", "sample_idx", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "output", ".", "append", "(", "adjs", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.sreyan88_disfluency-detection-with-span-classification.dataloaders.collate_functions.head_to_adj": [[99, 132], ["numpy.zeros", "int", "int"], "function", ["None"], ["", "def", "head_to_adj", "(", "max_len", ",", "heads", ",", "orig_to_tok_index", ",", "sent_max_len", ")", ":", "\n", "        ", "\"\"\"\n        Convert a tree object to an (numpy) adjacency matrix.\n        \"\"\"", "\n", "directed", "=", "0", "\n", "self_loop", "=", "False", "#config.adj_self_loop", "\n", "ret", "=", "np", ".", "zeros", "(", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "\n", "# for i, head in enumerate(heads):", "\n", "#     if head == -1:", "\n", "#         continue", "\n", "\n", "#     ret[head, orig_to_tok_index[i]] = 1", "\n", "\n", "i", "=", "0", "\n", "head", "=", "-", "1", "\n", "while", "i", "<", "sent_max_len", ":", "\n", "            ", "if", "i", "in", "orig_to_tok_index", ":", "\n", "                ", "head", "+=", "1", "\n", "ret", "[", "int", "(", "heads", "[", "head", "]", ")", ",", "i", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "ret", "[", "int", "(", "heads", "[", "head", "]", ")", ",", "i", "]", "=", "1", "\n", "", "i", "+=", "1", "\n", "\n", "", "if", "not", "directed", ":", "\n", "            ", "ret", "=", "ret", "+", "ret", ".", "T", "\n", "\n", "# if self_loop:", "\n", "#     for i in range(len(inst.input.words)):", "\n", "#         ret[i, i] = 1", "\n", "\n", "", "return", "ret", "\n", "\n"]]}