{"home.repos.pwc.inspect_result.apexrl_AORPO.None.main.run_eval": [[38, 65], ["controller.prep_rollouts", "range", "numpy.mean", "utils.make_env.make_env", "utils.make_env.make_env.seed", "utils.make_env.make_env.reset", "range", "rets.append", "utils.make_env.make_env.close", "controller.step", "utils.make_env.make_env.step", "numpy.mean", "utils.misc.n_2_t().reshape", "utils.misc.t_2_n().reshape", "range", "utils.misc.n_2_t", "utils.misc.t_2_n"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.__init__.make_env", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.n_2_t", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.t_2_n"], ["def", "run_eval", "(", "\n", "env_id", ":", "str", ",", "\n", "controller", ":", "MA_Controller", ",", "\n", "episode_length", ":", "int", "=", "25", ",", "\n", "n_episode", ":", "int", "=", "10", ",", "\n", ")", ":", "\n", "    ", "controller", ".", "prep_rollouts", "(", "\"cpu\"", ")", "\n", "rets", "=", "[", "]", "\n", "for", "ep_i", "in", "range", "(", "n_episode", ")", ":", "\n", "        ", "env", "=", "make_env", "(", "env_id", ")", "\n", "env", ".", "seed", "(", "ep_i", ")", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "ret", "=", "0", "\n", "for", "_", "in", "range", "(", "episode_length", ")", ":", "\n", "            ", "torch_obs", "=", "[", "\n", "n_2_t", "(", "obs", "[", "a_i", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "for", "a_i", "in", "range", "(", "controller", ".", "n_agent", ")", "\n", "]", "\n", "torch_act", "=", "controller", ".", "step", "(", "torch_obs", ",", "explore", "=", "False", ")", "\n", "act", "=", "[", "t_2_n", "(", "a_act", ")", ".", "reshape", "(", "-", "1", ")", "for", "a_act", "in", "torch_act", "]", "\n", "n_obs", ",", "rew", ",", "_", ",", "_", "=", "env", ".", "step", "(", "act", ")", "\n", "obs", "=", "n_obs", "\n", "\n", "ret", "+=", "np", ".", "mean", "(", "rew", ")", "\n", "", "rets", ".", "append", "(", "ret", ")", "\n", "env", ".", "close", "(", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "rets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.None.main.run": [[67, 518], ["os.makedirs", "print", "torch.utils.tensorboard.SummaryWriter", "utils.misc.setup_seed", "utils.make_env.make_env", "torch.set_num_threads", "utils.make_env.make_parallel_env", "algorithms.maframework.MA_Controller.init_from_env", "utils.buffer.ReplayBuffer", "any", "int", "any", "range", "utils.make_env.make_env.close", "utils.make_env.make_parallel_env.close", "torch.utils.tensorboard.SummaryWriter.close", "os.makedirs", "MA_Controller.init_from_env.save", "model_dir.exists", "open", "json.dump", "str", "int", "utils.misc.optimizer_lr_multistep_scheduler", "print", "print", "tqdm.trange", "main.run_eval", "tqdm.trange.set_description", "torch.utils.tensorboard.SummaryWriter.add_scalars", "torch.utils.tensorboard.SummaryWriter.add_scalars", "int", "len", "os.path.join", "int", "isinstance", "isinstance", "MA_Controller.init_from_env.compute_cooperate_interaction", "utils.make_env.make_parallel_env.reset", "MA_Controller.init_from_env.prep_rollouts", "range", "utils.buffer.ReplayBuffer.get_average_rewards", "enumerate", "tqdm.trange.set_description", "torch.utils.tensorboard.SummaryWriter.add_scalars", "os.makedirs", "MA_Controller.init_from_env.save", "MA_Controller.init_from_env.save", "MA_Controller.init_from_env.compute_cooperate_interaction", "pathlib.Path", "model_dir.iterdir", "str().startswith", "isinstance", "MA_Controller.init_from_env.step", "utils.make_env.make_parallel_env.step", "numpy.stack", "utils.buffer.ReplayBuffer.push_sample_first", "torch.utils.tensorboard.SummaryWriter.add_scalars", "torch.utils.tensorboard.SummaryWriter.add_scalars", "str().split", "max", "isinstance", "sum", "utils.misc.n_2_t", "utils.misc.t_2_n", "any", "any", "range", "MA_Controller.init_from_env.update_all_targets", "MA_Controller.init_from_env.prep_rollouts", "numpy.mean", "str", "numpy.vstack", "range", "range", "len", "utils.buffer.ReplayBuffer.sample", "range", "MA_Controller.init_from_env.evaluate_dynamics_model", "MA_Controller.init_from_env.prep_rollouts", "utils.misc.optimizer_lr_multistep_scheduler.step", "len", "any", "any", "min", "utils.buffer.ReplayBuffer.latest_sample", "range", "MA_Controller.init_from_env.evaluate_opp_model", "MA_Controller.init_from_env.prep_rollouts", "any", "MA_Controller.init_from_env.prep_rollouts", "int", "MA_Controller.init_from_env.prep_rollouts", "MA_Controller.init_from_env.prep_training", "MA_Controller.init_from_env.prep_training", "numpy.mean", "str", "isinstance", "MA_Controller.init_from_env.prep_training", "MA_Controller.init_from_env.prep_training", "len", "float", "MA_Controller.init_from_env.get_dynamics_model", "MA_Controller.init_from_env.update_dynamics_models", "MA_Controller.init_from_env.prep_rollouts", "MA_Controller.init_from_env.evaluate_dynamics_model", "range", "print", "MA_Controller.init_from_env.set_dynamics_model", "len", "isinstance", "MA_Controller.init_from_env.get_opponent_model", "MA_Controller.init_from_env.update_opponent_models", "MA_Controller.init_from_env.prep_rollouts", "MA_Controller.init_from_env.evaluate_opp_model", "range", "print", "MA_Controller.init_from_env.set_opponent_model", "min", "MA_Controller.init_from_env.rollout_models", "isinstance", "int", "range", "utils.buffer.ReplayBuffer.sample", "MA_Controller.init_from_env.update", "range", "range", "a_obj[].cpu", "MA_Controller.init_from_env.prep_training", "MA_Controller.init_from_env.prep_training", "isinstance", "isinstance", "a_obj[].cpu", "float", "range", "range", "MA_Controller.init_from_env.prep_training", "MA_Controller.init_from_env.prep_training", "isinstance", "max", "MA_Controller.init_from_env.prep_rollouts", "MA_Controller.init_from_env.prep_rollouts", "utils.buffer.ReplayBuffer.sample", "min", "utils.buffer.ReplayBuffer.sample", "MA_Controller.init_from_env.agents[].replay_buffer.sample", "zip", "MA_Controller.init_from_env.update", "MA_Controller.init_from_env.get_dynamics_model", "MA_Controller.init_from_env.get_opponent_model", "min", "enumerate", "len", "zip", "torch.cat"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.setup_seed", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.__init__.make_env", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.make_env.make_parallel_env", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.init_from_env", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save", "home.repos.pwc.inspect_result.apexrl_AORPO.None.eval.run_eval", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.compute_cooperate_interaction", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.get_average_rewards", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.compute_cooperate_interaction", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.push_sample_first", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.n_2_t", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.t_2_n", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.update_all_targets", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_dynamics_model", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.latest_sample", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_opp_model", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_dynamics_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update_dynamics_models", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_dynamics_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.set_dynamics_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_opponent_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update_opponent_models", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_opp_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.set_opponent_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.rollout_models", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_dynamics_model", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_opponent_model"], ["", "def", "run", "(", "config", ":", "argparse", ".", "Namespace", ")", "->", "Dict", ":", "\n", "    ", "model_dir", "=", "(", "\n", "Path", "(", "config", ".", "model_dir", ")", "/", "config", ".", "algorithm", "/", "config", ".", "env_id", "/", "config", ".", "model_name", "\n", ")", "\n", "if", "not", "model_dir", ".", "exists", "(", ")", ":", "\n", "        ", "curr_run", "=", "\"run1\"", "\n", "", "else", ":", "\n", "        ", "exst_run_nums", "=", "[", "\n", "int", "(", "str", "(", "folder", ".", "name", ")", ".", "split", "(", "\"run\"", ")", "[", "1", "]", ")", "\n", "for", "folder", "in", "model_dir", ".", "iterdir", "(", ")", "\n", "if", "str", "(", "folder", ".", "name", ")", ".", "startswith", "(", "\"run\"", ")", "\n", "]", "\n", "if", "len", "(", "exst_run_nums", ")", "==", "0", ":", "\n", "            ", "curr_run", "=", "\"run1\"", "\n", "", "else", ":", "\n", "            ", "curr_run", "=", "\"run%i\"", "%", "(", "max", "(", "exst_run_nums", ")", "+", "1", ")", "\n", "", "", "run_dir", "=", "model_dir", "/", "curr_run", "\n", "log_dir", "=", "run_dir", "/", "\"logs\"", "\n", "os", ".", "makedirs", "(", "log_dir", ")", "\n", "print", "(", "\"run dir\"", ",", "run_dir", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "run_dir", ",", "\"config\"", ")", ",", "\"w\"", ")", "as", "cf", ":", "\n", "        ", "json", ".", "dump", "(", "config", ".", "__dict__", ",", "cf", ")", "\n", "", "logger", "=", "SummaryWriter", "(", "str", "(", "log_dir", ")", ")", "\n", "\n", "setup_seed", "(", "config", ".", "seed", ")", "\n", "\n", "eval_env", "=", "make_env", "(", "config", ".", "env_id", ")", "\n", "n_agent", "=", "eval_env", ".", "n", "\n", "\n", "torch", ".", "set_num_threads", "(", "config", ".", "n_training_thread", ")", "\n", "\n", "env", "=", "make_parallel_env", "(", "\n", "config", ".", "env_id", ",", "config", ".", "n_sample_thread", ",", "config", ".", "seed", ",", "config", ".", "discrete_action", "\n", ")", "\n", "controller", "=", "MA_Controller", ".", "init_from_env", "(", "\n", "env", ",", "\n", "config", ".", "env_id", ",", "\n", "config", ",", "\n", "alg", "=", "config", ".", "algorithm", ",", "\n", "rew_scale", "=", "config", ".", "rew_scale", ",", "\n", "gamma", "=", "config", ".", "gamma", ",", "\n", "tau", "=", "config", ".", "tau", ",", "\n", "lr", "=", "config", ".", "lr", ",", "\n", "hidden_dim", "=", "config", ".", "hidden_dim", ",", "\n", "model_lr", "=", "config", ".", "model_lr", ",", "\n", "model_hidden_dim", "=", "config", ".", "model_hidden_dim", ",", "\n", "ensemble_size", "=", "config", ".", "ensemble_size", ",", "\n", "env_model_buffer_size", "=", "int", "(", "config", ".", "env_model_buffer_size", ")", ",", "\n", "opp_lr", "=", "config", ".", "opp_lr", ",", "\n", ")", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "\n", "int", "(", "config", ".", "buffer_size", ")", ",", "\n", "n_agent", ",", "\n", "[", "obsp", ".", "shape", "[", "0", "]", "for", "obsp", "in", "env", ".", "observation_space", "]", ",", "\n", "[", "\n", "acsp", ".", "shape", "[", "0", "]", "\n", "if", "isinstance", "(", "acsp", ",", "Box", ")", "\n", "else", "(", "\n", "acsp", ".", "n", "if", "isinstance", "(", "acsp", ",", "Discrete", ")", "else", "sum", "(", "acsp", ".", "high", "-", "acsp", ".", "low", "+", "1", ")", "\n", ")", "\n", "for", "acsp", "in", "env", ".", "action_space", "\n", "]", ",", "\n", ")", "\n", "\n", "if", "any", "(", "[", "isinstance", "(", "a", ",", "AgentMB", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", ":", "\n", "        ", "models_lr_scheduler", "=", "optimizer_lr_multistep_scheduler", "(", "\n", "config", ".", "model_lr_schedule_steps", ",", "\n", "[", "a", ".", "model_optimizer", "for", "a", "in", "controller", ".", "agents", "]", ",", "\n", "logger", "=", "logger", ",", "\n", ")", "\n", "\n", "", "n_step", "=", "0", "\n", "K", "=", "1", "\n", "env_rate", "=", "1", "\n", "model_trained", "=", "False", "\n", "eval_ret", "=", "0", "\n", "\n", "eval_ret_dict", "=", "{", "}", "\n", "\n", "# train the policy and opponent models but not the dynamics model", "\n", "warmup_episode", "=", "int", "(", "1e6", ")", "\n", "if", "any", "(", "[", "isinstance", "(", "a", ",", "AgentMB", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", ":", "\n", "        ", "warmup_episode", "=", "config", ".", "n_model_warmup_episode", "\n", "print", "(", "\"initialize episodes\"", ",", "warmup_episode", ")", "\n", "\n", "", "for", "epoch", "in", "range", "(", "0", ",", "config", ".", "n_epoch", ")", ":", "\n", "        ", "print", "(", "\n", "f\"tag {config.model_name}, seed {config.seed}, {config.algorithm}, epoch {epoch+1}, {config.episode_per_epoch} episodes, {config.n_sample_thread} threads\"", "\n", ")", "\n", "\n", "# the noise is changed each epoch, but is the noise is not used at all", "\n", "# eplr_remain_frac = max(0, config.n_eplr_epoch - epoch) / config.n_eplr_epoch", "\n", "# noise_scale = (", "\n", "#     config.noise_scale_final", "\n", "#     + (config.noise_scale_start - config.noise_scale_final) * eplr_remain_frac", "\n", "# )", "\n", "# controller.scale_noise(noise_scale)", "\n", "# logger.add_scalars(", "\n", "#     \"hypers/\", {\"noise_scale\": noise_scale}, logger_dynamics_iter", "\n", "# )", "\n", "\n", "episode_2_epoch_iter", "=", "tqdm", ".", "trange", "(", "\n", "0", ",", "\n", "config", ".", "episode_per_epoch", ",", "\n", "config", ".", "n_sample_thread", ",", "\n", ")", "\n", "for", "episode_in_epoch", "in", "episode_2_epoch_iter", ":", "\n", "            ", "episode_i", "=", "epoch", "*", "config", ".", "episode_per_epoch", "+", "(", "\n", "episode_in_epoch", "+", "config", ".", "n_sample_thread", "\n", ")", "\n", "logger_dynamics_iter", "=", "episode_i", "\n", "logger_oppo_iter", "=", "controller", ".", "compute_cooperate_interaction", "(", "\n", "episode_i", ",", "config", ".", "episode_length", "\n", ")", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "# thread, agent, feature", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "episode_length", ")", ":", "\n", "# rearrange observations to be per agent, and convert to torch Variable", "\n", "                ", "torch_obs", "=", "[", "\n", "n_2_t", "(", "np", ".", "vstack", "(", "obs", "[", ":", ",", "a_i", "]", ")", ",", "requires_grad", "=", "False", ")", "\n", "for", "a_i", "in", "range", "(", "n_agent", ")", "\n", "]", "\n", "# [agent, tensor: thread, feature]", "\n", "\n", "# get actions as torch Variables", "\n", "torch_action", "=", "controller", ".", "step", "(", "torch_obs", ",", "explore", "=", "True", ")", "\n", "# agent, thread, action_dim", "\n", "\n", "# convert actions to numpy arrays", "\n", "agent_first_action", "=", "[", "\n", "t_2_n", "(", "agent_action", ")", "for", "agent_action", "in", "torch_action", "\n", "]", "\n", "# rearrange actions to be per environment", "\n", "thread_first_action", "=", "[", "\n", "[", "agent_action", "[", "i", "]", "for", "agent_action", "in", "agent_first_action", "]", "\n", "for", "i", "in", "range", "(", "config", ".", "n_sample_thread", ")", "\n", "]", "\n", "# thread, agent, action_dim", "\n", "next_obs", ",", "rewards", ",", "dones", ",", "infos", "=", "env", ".", "step", "(", "thread_first_action", ")", "\n", "thread_first_action", "=", "np", ".", "stack", "(", "thread_first_action", ",", "axis", "=", "0", ")", "\n", "\n", "replay_buffer", ".", "push_sample_first", "(", "\n", "obs", ",", "thread_first_action", ",", "rewards", ",", "next_obs", ",", "dones", "\n", ")", "\n", "\n", "obs", "=", "next_obs", "\n", "n_step", "+=", "config", ".", "n_sample_thread", "\n", "\n", "if", "(", "\n", "len", "(", "replay_buffer", ")", ">", "config", ".", "batch_size", "*", "5", "\n", "and", "n_step", "%", "config", ".", "dynamics_model_update_step_interval", "\n", "<", "config", ".", "n_sample_thread", "\n", ")", ":", "\n", "# update model", "\n", "                    ", "if", "any", "(", "[", "isinstance", "(", "a", ",", "AgentMB", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", ":", "\n", "                        ", "if", "config", ".", "cuda", ":", "\n", "                            ", "controller", ".", "prep_training", "(", "device", "=", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                            ", "controller", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "", "sample", "=", "replay_buffer", ".", "sample", "(", "\n", "len", "(", "replay_buffer", ")", ",", "\n", "to_gpu", "=", "config", ".", "cuda", ",", "\n", ")", "\n", "i", "=", "0", "\n", "min_i", "=", "i", "\n", "min_evl_loss", "=", "[", "float", "(", "\"inf\"", ")", "for", "_", "in", "range", "(", "n_agent", ")", "]", "\n", "best_models", "=", "[", "\n", "controller", ".", "get_dynamics_model", "(", "a_i", ")", "for", "a_i", "in", "range", "(", "n_agent", ")", "\n", "]", "\n", "sample_train", "=", "[", "\n", "[", "a_obj", "[", "a_obj", ".", "shape", "[", "0", "]", "//", "5", ":", "]", "for", "a_obj", "in", "obj", "]", "\n", "for", "obj", "in", "sample", "\n", "]", "\n", "sample_eval", "=", "[", "\n", "[", "a_obj", "[", ":", "a_obj", ".", "shape", "[", "0", "]", "//", "5", "]", ".", "cpu", "(", ")", "for", "a_obj", "in", "obj", "]", "\n", "for", "obj", "in", "sample", "\n", "]", "\n", "while", "True", ":", "\n", "                            ", "if", "config", ".", "cuda", ":", "\n", "                                ", "controller", ".", "prep_training", "(", "device", "=", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                                ", "controller", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "\n", "", "controller", ".", "update_dynamics_models", "(", "\n", "sample_train", ",", "\n", "logger_iter", "=", "logger_dynamics_iter", ",", "\n", "logger", "=", "logger", ",", "\n", "epochs", "=", "1", ",", "\n", ")", "\n", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "evl_loss", "=", "controller", ".", "evaluate_dynamics_model", "(", "sample_eval", ")", "\n", "\n", "i", "+=", "1", "\n", "for", "a_i", "in", "range", "(", "n_agent", ")", ":", "\n", "                                ", "if", "min_evl_loss", "[", "a_i", "]", ">", "evl_loss", "[", "a_i", "]", ":", "\n", "                                    ", "min_evl_loss", "[", "a_i", "]", "=", "evl_loss", "[", "a_i", "]", "\n", "min_i", "=", "i", "\n", "best_models", "[", "a_i", "]", "=", "controller", ".", "get_dynamics_model", "(", "\n", "a_i", "\n", ")", "\n", "", "", "if", "min_i", "<=", "i", "-", "5", "or", "i", ">", "50", ":", "\n", "                                ", "break", "\n", "", "", "if", "config", ".", "DEBUG", ":", "\n", "                            ", "print", "(", "f\"min_i in dynamics model update {min_i}\"", ")", "\n", "\n", "", "for", "a_i", "in", "range", "(", "n_agent", ")", ":", "\n", "                            ", "controller", ".", "set_dynamics_model", "(", "a_i", ",", "best_models", "[", "a_i", "]", ")", "\n", "", "controller", ".", "evaluate_dynamics_model", "(", "\n", "sample_eval", ",", "logger", ",", "logger_dynamics_iter", "\n", ")", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "models_lr_scheduler", ".", "step", "(", "epoch", "*", "config", ".", "episode_per_epoch", ")", "\n", "model_trained", "=", "True", "\n", "\n", "", "", "if", "(", "\n", "len", "(", "replay_buffer", ")", ">=", "config", ".", "batch_size", "*", "5", "\n", "and", "(", "n_step", "%", "config", ".", "update_step_interval", ")", "<", "config", ".", "n_sample_thread", "\n", ")", ":", "\n", "                    ", "if", "any", "(", "\n", "[", "isinstance", "(", "a", ",", "AgentOppMd", ")", "for", "a", "in", "controller", ".", "agents", "]", "\n", ")", "or", "any", "(", "[", "isinstance", "(", "a", ",", "AgentMB", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", ":", "\n", "                        ", "batch_size", "=", "min", "(", "config", ".", "batch_size", "*", "3", ",", "len", "(", "replay_buffer", ")", ")", "\n", "latest_sample", "=", "replay_buffer", ".", "latest_sample", "(", "\n", "batch_size", ",", "to_gpu", "=", "config", ".", "cuda", "\n", ")", "\n", "latest_sample_train", "=", "[", "\n", "[", "a_obj", "[", "a_obj", ".", "shape", "[", "0", "]", "//", "5", ":", "]", "for", "a_obj", "in", "obj", "]", "\n", "for", "obj", "in", "latest_sample", "\n", "]", "\n", "latest_sample_eval", "=", "[", "\n", "[", "a_obj", "[", ":", "a_obj", ".", "shape", "[", "0", "]", "//", "5", "]", ".", "cpu", "(", ")", "for", "a_obj", "in", "obj", "]", "\n", "for", "obj", "in", "latest_sample", "\n", "]", "\n", "# update opponents", "\n", "", "if", "any", "(", "[", "isinstance", "(", "a", ",", "AgentOppMd", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", ":", "\n", "                        ", "i", "=", "0", "\n", "max_i", "=", "0", "\n", "max_match_rates", "=", "[", "-", "float", "(", "\"inf\"", ")", "for", "_", "in", "range", "(", "n_agent", ")", "]", "\n", "best_opponents", "=", "[", "\n", "controller", ".", "get_opponent_model", "(", "a_i", ")", "for", "a_i", "in", "range", "(", "n_agent", ")", "\n", "]", "\n", "while", "True", ":", "\n", "                            ", "if", "config", ".", "cuda", ":", "\n", "                                ", "controller", ".", "prep_training", "(", "device", "=", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                                ", "controller", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "", "controller", ".", "update_opponent_models", "(", "\n", "latest_sample_train", ",", "\n", "logger", "=", "logger", ",", "\n", "epochs", "=", "1", ",", "\n", "logger_iter", "=", "logger_dynamics_iter", ",", "\n", ")", "\n", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "(", "\n", "_", ",", "\n", "match_rate_list", ",", "\n", ")", "=", "controller", ".", "evaluate_opp_model", "(", "latest_sample_eval", ")", "\n", "# matching rate for each agent", "\n", "i", "+=", "1", "\n", "\n", "for", "a_i", "in", "range", "(", "n_agent", ")", ":", "\n", "                                ", "if", "max_match_rates", "[", "a_i", "]", "<", "match_rate_list", "[", "a_i", "]", ":", "\n", "                                    ", "max_match_rates", "[", "a_i", "]", "=", "match_rate_list", "[", "a_i", "]", "\n", "max_i", "=", "i", "\n", "best_opponents", "[", "a_i", "]", "=", "controller", ".", "get_opponent_model", "(", "\n", "a_i", "\n", ")", "\n", "", "", "if", "max_i", "<=", "i", "-", "5", "or", "i", ">", "50", ":", "\n", "                                ", "break", "\n", "", "", "if", "config", ".", "DEBUG", ":", "\n", "                            ", "print", "(", "f\"max_i in opponent model update {max_i}\"", ")", "\n", "", "for", "a_i", "in", "range", "(", "n_agent", ")", ":", "\n", "                            ", "controller", ".", "set_opponent_model", "(", "a_i", ",", "best_opponents", "[", "a_i", "]", ")", "\n", "", "controller", ".", "evaluate_opp_model", "(", "\n", "latest_sample_eval", ",", "\n", "logger", "=", "logger", ",", "\n", "ep_i", "=", "n_step", "//", "config", ".", "episode_length", ",", "\n", ")", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "# use model", "\n", "", "if", "(", "\n", "episode_i", ">=", "warmup_episode", "\n", "and", "any", "(", "[", "isinstance", "(", "a", ",", "AgentMB", ")", "for", "a", "in", "controller", ".", "agents", "]", ")", "\n", "and", "model_trained", "\n", ")", ":", "\n", "                        ", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "# model rollout", "\n", "K", "=", "int", "(", "\n", "min", "(", "\n", "config", ".", "K", ",", "\n", "max", "(", "\n", "K", ",", "\n", "K", "\n", "+", "(", "epoch", "-", "config", ".", "A", ")", "\n", "/", "(", "(", "config", ".", "B", "-", "config", ".", "A", ")", ")", "\n", "*", "(", "config", ".", "K", "-", "K", ")", ",", "\n", ")", ",", "\n", ")", "\n", ")", "\n", "if", "K", ">", "0", ":", "\n", "                            ", "if", "config", ".", "gpu_rollout_model", ":", "\n", "# gpu model rollout", "\n", "                                ", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "# cpu model rollout", "\n", "                                ", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "", "init_states", "=", "replay_buffer", ".", "sample", "(", "\n", "config", ".", "M", ",", "to_gpu", "=", "config", ".", "gpu_rollout_model", ",", "replace", "=", "True", "\n", ")", "[", "0", "]", "\n", "controller", ".", "rollout_models", "(", "\n", "K", ",", "init_states", ",", "logger", ",", "n_step", "//", "config", ".", "episode_length", "\n", ")", "\n", "", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "# update policy", "\n", "", "if", "config", ".", "cuda", ":", "\n", "                        ", "controller", ".", "prep_training", "(", "device", "=", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                        ", "controller", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "\n", "", "for", "a_i", "in", "range", "(", "controller", ".", "n_agent", ")", ":", "\n", "                        ", "if", "(", "\n", "episode_i", ">=", "warmup_episode", "\n", "and", "(", "isinstance", "(", "controller", ".", "agents", "[", "a_i", "]", ",", "AgentMB", ")", ")", "\n", "and", "K", ">", "0", "\n", "and", "model_trained", "\n", ")", ":", "\n", "                            ", "env_rate", "=", "config", ".", "Env_rate_start", "+", "min", "(", "\n", "1.0", ",", "epoch", "/", "(", "config", ".", "Env_rate_n_epoch", ")", "\n", ")", "*", "(", "config", ".", "Env_rate_finish", "-", "config", ".", "Env_rate_start", ")", "\n", "\n", "env_batch_size", "=", "int", "(", "config", ".", "batch_size", "*", "env_rate", ")", "\n", "model_batch_size", "=", "config", ".", "batch_size", "-", "env_batch_size", "\n", "# TODO: add warning!", "\n", "if", "(", "\n", "config", ".", "env_model_buffer_size", "//", "config", ".", "batch_size", "\n", "<", "config", ".", "G", "\n", ")", ":", "\n", "                                ", "G", "=", "config", ".", "G", "\n", "", "else", ":", "\n", "                                ", "G", "=", "min", "(", "\n", "len", "(", "controller", ".", "agents", "[", "a_i", "]", ".", "replay_buffer", ")", "\n", "//", "(", "config", ".", "batch_size", ")", ",", "\n", "config", ".", "G", ",", "\n", ")", "\n", "", "for", "_", "in", "range", "(", "G", ")", ":", "\n", "                                ", "env_sample", "=", "replay_buffer", ".", "sample", "(", "\n", "env_batch_size", ",", "to_gpu", "=", "config", ".", "cuda", "\n", ")", "\n", "model_sample", "=", "controller", ".", "agents", "[", "\n", "a_i", "\n", "]", ".", "replay_buffer", ".", "sample", "(", "\n", "model_batch_size", ",", "to_gpu", "=", "config", ".", "cuda", "\n", ")", "\n", "sample", "=", "env_sample", "\n", "# concat for each component", "\n", "for", "s", ",", "m_s", "in", "zip", "(", "sample", ",", "model_sample", ")", ":", "\n", "                                    ", "for", "i", ",", "(", "a_s", ",", "a_m_s", ")", "in", "enumerate", "(", "zip", "(", "s", ",", "m_s", ")", ")", ":", "\n", "                                        ", "s", "[", "i", "]", "=", "torch", ".", "cat", "(", "[", "a_s", ",", "a_m_s", "]", ",", "dim", "=", "0", ")", "\n", "", "", "controller", ".", "update", "(", "\n", "sample", ",", "\n", "a_i", ",", "\n", "logger", "=", "logger", ",", "\n", "logger_iter", "=", "logger_dynamics_iter", ",", "\n", ")", "\n", "", "", "else", ":", "\n", "                            ", "sample", "=", "replay_buffer", ".", "sample", "(", "\n", "config", ".", "batch_size", ",", "to_gpu", "=", "config", ".", "cuda", "\n", ")", "\n", "controller", ".", "update", "(", "\n", "sample", ",", "\n", "a_i", ",", "\n", "logger", "=", "logger", ",", "\n", "logger_iter", "=", "logger_dynamics_iter", ",", "\n", ")", "\n", "\n", "", "", "controller", ".", "update_all_targets", "(", ")", "\n", "\n", "controller", ".", "prep_rollouts", "(", "device", "=", "\"cpu\"", ")", "\n", "\n", "", "", "ep_rews", "=", "replay_buffer", ".", "get_average_rewards", "(", "\n", "config", ".", "episode_length", ",", "config", ".", "n_sample_thread", "\n", ")", "\n", "for", "a_i", ",", "a_ep_rew", "in", "enumerate", "(", "ep_rews", ")", ":", "\n", "                ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/mean_episode_return\"", "%", "a_i", ",", "\n", "{", "\"dynamics_interaction\"", ":", "a_ep_rew", "}", ",", "\n", "logger_dynamics_iter", ",", "\n", ")", "\n", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/mean_episode_return\"", "%", "a_i", ",", "\n", "{", "\"opponent_interaction\"", ":", "a_ep_rew", "}", ",", "\n", "logger_oppo_iter", ",", "\n", ")", "\n", "\n", "", "episode_2_epoch_iter", ".", "set_description", "(", "\n", "f\"train return {np.mean(ep_rews):.4f}  eval ret {eval_ret:.4f}\"", "\n", ")", "\n", "\n", "logger", ".", "add_scalars", "(", "\n", "\"hypers/\"", ",", "\n", "{", "\"K\"", ":", "K", ",", "\"Env_rate\"", ":", "env_rate", "}", ",", "\n", "logger_dynamics_iter", ",", "\n", ")", "\n", "", "n_episode", "=", "(", "epoch", "+", "1", ")", "*", "config", ".", "episode_per_epoch", "\n", "if", "(", "n_episode", "%", "config", ".", "save_episode_interval", ")", "==", "0", ":", "\n", "            ", "os", ".", "makedirs", "(", "run_dir", "/", "\"incremental\"", ",", "exist_ok", "=", "True", ")", "\n", "controller", ".", "save", "(", "\n", "run_dir", "\n", "/", "\"incremental\"", "\n", "/", "(", "\"model_dynamics%i.pt\"", "%", "(", "logger_dynamics_iter", ")", ")", "\n", ")", "\n", "controller", ".", "save", "(", "\n", "run_dir", "/", "\"incremental\"", "/", "(", "\"model_opponent%i.pt\"", "%", "(", "logger_oppo_iter", ")", ")", "\n", ")", "\n", "\n", "", "eval_ret", "=", "run_eval", "(", "config", ".", "env_id", ",", "controller", ")", "\n", "episode_2_epoch_iter", ".", "set_description", "(", "\n", "f\"train ret {np.mean(ep_rews):.4f} eval ret {eval_ret:.4f}\"", "\n", ")", "\n", "\n", "eval_ret_dict", "[", "(", "epoch", "+", "1", ")", "*", "config", ".", "episode_per_epoch", "]", "=", "eval_ret", "\n", "logger", ".", "add_scalars", "(", "\n", "\"eval\"", ",", "\n", "{", "\"mean_episode_return_dynamics_interaction\"", ":", "eval_ret", "}", ",", "\n", "(", "epoch", "+", "1", ")", "*", "config", ".", "episode_per_epoch", ",", "\n", ")", "\n", "logger", ".", "add_scalars", "(", "\n", "\"eval\"", ",", "\n", "{", "\"mean_episode_return_opponent_interaction\"", ":", "eval_ret", "}", ",", "\n", "controller", ".", "compute_cooperate_interaction", "(", "\n", "(", "epoch", "+", "1", ")", "*", "config", ".", "episode_per_epoch", ",", "config", ".", "episode_length", "\n", ")", ",", "\n", ")", "\n", "\n", "", "eval_env", ".", "close", "(", ")", "\n", "env", ".", "close", "(", ")", "\n", "logger", ".", "close", "(", ")", "\n", "\n", "os", ".", "makedirs", "(", "run_dir", "/", "\"incremental\"", ",", "exist_ok", "=", "True", ")", "\n", "controller", ".", "save", "(", "run_dir", "/", "\"incremental\"", "/", "(", "\"model.pt\"", ")", ")", "\n", "\n", "return", "{", "curr_run", ":", "eval_ret_dict", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.None.eval.run_eval": [[15, 45], ["controller.prep_rollouts", "tqdm.trange", "numpy.mean", "utils.make_env.make_env", "utils.make_env.make_env.seed", "utils.make_env.make_env.reset", "range", "rets.append", "utils.make_env.make_env.close", "controller.step", "utils.make_env.make_env.step", "numpy.mean", "utils.make_env.make_env.render", "utils.misc.n_2_t().reshape", "utils.misc.t_2_n().reshape", "range", "utils.misc.n_2_t", "utils.misc.t_2_n"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.__init__.make_env", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.n_2_t", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.t_2_n"], ["def", "run_eval", "(", "\n", "env_id", ":", "str", ",", "\n", "controller", ":", "MA_Controller", ",", "\n", "episode_length", ":", "int", "=", "25", ",", "\n", "n_episode", ":", "int", "=", "10", ",", "\n", "render", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "controller", ".", "prep_rollouts", "(", "\"cpu\"", ")", "\n", "rets", "=", "[", "]", "\n", "for", "ep_i", "in", "tqdm", ".", "trange", "(", "n_episode", ")", ":", "\n", "        ", "env", "=", "make_env", "(", "env_id", ")", "\n", "env", ".", "seed", "(", "ep_i", ")", "\n", "obs", "=", "env", ".", "reset", "(", ")", "\n", "ret", "=", "0", "\n", "for", "_", "in", "range", "(", "episode_length", ")", ":", "\n", "            ", "if", "render", ":", "\n", "                ", "env", ".", "render", "(", ")", "\n", "", "torch_obs", "=", "[", "\n", "n_2_t", "(", "obs", "[", "a_i", "]", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", "for", "a_i", "in", "range", "(", "controller", ".", "n_agent", ")", "\n", "]", "\n", "torch_act", "=", "controller", ".", "step", "(", "torch_obs", ",", "explore", "=", "False", ")", "\n", "act", "=", "[", "t_2_n", "(", "a_act", ")", ".", "reshape", "(", "-", "1", ")", "for", "a_act", "in", "torch_act", "]", "\n", "n_obs", ",", "rew", ",", "_", ",", "_", "=", "env", ".", "step", "(", "act", ")", "\n", "obs", "=", "n_obs", "\n", "\n", "ret", "+=", "np", ".", "mean", "(", "rew", ")", "\n", "", "rets", ".", "append", "(", "ret", ")", "\n", "env", ".", "close", "(", ")", "\n", "\n", "", "return", "np", ".", "mean", "(", "rets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.__init__": [[12, 39], ["zip", "buffer.ReplayBuffer.obs_buffs.append", "buffer.ReplayBuffer.ac_buffs.append", "buffer.ReplayBuffer.rew_buffs.append", "buffer.ReplayBuffer.next_obs_buffs.append", "buffer.ReplayBuffer.done_buffs.append", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "max_steps", ":", "int", ",", "num_agents", ":", "int", ",", "obs_dims", ":", "List", ",", "ac_dims", ":", "List", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            max_steps (int): Maximum number of timepoints to store in buffer\n            num_agents (int): Number of agents in environment\n            obs_dims (list of ints): number of obervation dimensions for each\n                                     agent\n            ac_dims (list of ints): number of action dimensions for each agent\n        \"\"\"", "\n", "self", ".", "max_steps", "=", "max_steps", "\n", "self", ".", "num_agents", "=", "num_agents", "\n", "self", ".", "obs_buffs", "=", "[", "]", "\n", "self", ".", "ac_buffs", "=", "[", "]", "\n", "self", ".", "rew_buffs", "=", "[", "]", "\n", "self", ".", "next_obs_buffs", "=", "[", "]", "\n", "self", ".", "done_buffs", "=", "[", "]", "\n", "for", "odim", ",", "adim", "in", "zip", "(", "obs_dims", ",", "ac_dims", ")", ":", "\n", "            ", "self", ".", "obs_buffs", ".", "append", "(", "np", ".", "zeros", "(", "(", "max_steps", ",", "odim", ")", ")", ")", "\n", "self", ".", "ac_buffs", ".", "append", "(", "np", ".", "zeros", "(", "(", "max_steps", ",", "adim", ")", ")", ")", "\n", "self", ".", "rew_buffs", ".", "append", "(", "np", ".", "zeros", "(", "max_steps", ")", ")", "\n", "self", ".", "next_obs_buffs", ".", "append", "(", "np", ".", "zeros", "(", "(", "max_steps", ",", "odim", ")", ")", ")", "\n", "self", ".", "done_buffs", ".", "append", "(", "np", ".", "zeros", "(", "max_steps", ")", ")", "\n", "\n", "", "self", ".", "filled_i", "=", "(", "\n", "0", "# index of first empty location in buffer (last index when full)", "\n", ")", "\n", "self", ".", "curr_i", "=", "0", "# current index to write to (ovewrite oldest data)", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.__len__": [[40, 42], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "filled_i", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.clear": [[43, 46], ["None"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "filled_i", "=", "0", "\n", "self", ".", "curr_i", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.push_sample_first": [[47, 88], ["range", "range", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll"], "methods", ["None"], ["", "def", "push_sample_first", "(", "\n", "self", ",", "observations", ",", "actions", ",", "rewards", ",", "next_observations", ",", "dones", "\n", ")", ":", "\n", "        ", "nentries", "=", "observations", ".", "shape", "[", "0", "]", "# handle multiple parallel environments", "\n", "if", "self", ".", "curr_i", "+", "nentries", ">", "self", ".", "max_steps", ":", "\n", "            ", "rollover", "=", "self", ".", "max_steps", "-", "self", ".", "curr_i", "# num of indices to roll over", "\n", "for", "agent_i", "in", "range", "(", "self", ".", "num_agents", ")", ":", "\n", "                ", "self", ".", "obs_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "obs_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "rew_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "self", ".", "rew_buffs", "[", "agent_i", "]", ",", "rollover", ")", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "done_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "self", ".", "done_buffs", "[", "agent_i", "]", ",", "rollover", ")", "\n", "", "self", ".", "curr_i", "=", "0", "\n", "self", ".", "filled_i", "=", "self", ".", "max_steps", "\n", "", "for", "agent_i", "in", "range", "(", "self", ".", "num_agents", ")", ":", "\n", "            ", "self", ".", "obs_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "np", ".", "vstack", "(", "\n", "observations", "[", ":", ",", "agent_i", "]", "\n", ")", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "np", ".", "vstack", "(", "\n", "actions", "[", ":", ",", "agent_i", "]", "\n", ")", "\n", "self", ".", "rew_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "rewards", "[", "\n", ":", ",", "agent_i", "\n", "]", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", "[", "\n", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "\n", "]", "=", "np", ".", "vstack", "(", "next_observations", "[", ":", ",", "agent_i", "]", ")", "\n", "self", ".", "done_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "dones", "[", "\n", ":", ",", "agent_i", "\n", "]", "\n", "", "self", ".", "curr_i", "+=", "nentries", "\n", "if", "self", ".", "filled_i", "<", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "filled_i", "+=", "nentries", "\n", "", "if", "self", ".", "curr_i", "==", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "curr_i", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.push_agent_first": [[89, 130], ["range", "range", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll", "numpy.roll"], "methods", ["None"], ["", "", "def", "push_agent_first", "(", "\n", "self", ",", "observations", ",", "actions", ",", "rewards", ",", "next_observations", ",", "dones", ",", "nentries", "\n", ")", ":", "\n", "# all inputs are in \"per agent\" arrangement", "\n", "        ", "if", "self", ".", "curr_i", "+", "nentries", ">", "self", ".", "max_steps", ":", "\n", "            ", "rollover", "=", "self", ".", "max_steps", "-", "self", ".", "curr_i", "# num of indices to roll over", "\n", "for", "agent_i", "in", "range", "(", "self", ".", "num_agents", ")", ":", "\n", "                ", "self", ".", "obs_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "obs_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "rew_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "self", ".", "rew_buffs", "[", "agent_i", "]", ",", "rollover", ")", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", ",", "rollover", ",", "axis", "=", "0", "\n", ")", "\n", "self", ".", "done_buffs", "[", "agent_i", "]", "=", "np", ".", "roll", "(", "self", ".", "done_buffs", "[", "agent_i", "]", ",", "rollover", ")", "\n", "", "self", ".", "curr_i", "=", "0", "\n", "self", ".", "filled_i", "=", "self", ".", "max_steps", "\n", "", "for", "agent_i", "in", "range", "(", "self", ".", "num_agents", ")", ":", "\n", "            ", "self", ".", "obs_buffs", "[", "agent_i", "]", "[", "\n", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "\n", "]", "=", "observations", "[", "agent_i", "]", "\n", "self", ".", "ac_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "actions", "[", "\n", "agent_i", "\n", "]", "\n", "self", ".", "rew_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "rewards", "[", "\n", "agent_i", "\n", "]", "\n", "self", ".", "next_obs_buffs", "[", "agent_i", "]", "[", "\n", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "\n", "]", "=", "next_observations", "[", "agent_i", "]", "\n", "self", ".", "done_buffs", "[", "agent_i", "]", "[", "self", ".", "curr_i", ":", "self", ".", "curr_i", "+", "nentries", "]", "=", "dones", "[", "\n", "agent_i", "\n", "]", "\n", "", "self", ".", "curr_i", "+=", "nentries", "\n", "if", "self", ".", "filled_i", "<", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "filled_i", "+=", "nentries", "\n", "", "if", "self", ".", "curr_i", "==", "self", ".", "max_steps", ":", "\n", "            ", "self", ".", "curr_i", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.sample_from_inds": [[131, 157], ["torch.tensor().cuda", "torch.tensor", "cast", "cast", "cast", "cast", "cast", "cast", "range", "range", "range", "range", "range", "range", "torch.tensor", "[].std", "[].mean"], "methods", ["None"], ["", "", "def", "sample_from_inds", "(", "self", ",", "inds", ",", "to_gpu", "=", "False", ",", "norm_rews", "=", "True", ")", ":", "\n", "        ", "if", "to_gpu", ":", "\n", "            ", "cast", "=", "lambda", "x", ":", "torch", ".", "tensor", "(", "\n", "x", ",", "requires_grad", "=", "False", ",", "dtype", "=", "torch", ".", "float", "\n", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "            ", "cast", "=", "lambda", "x", ":", "torch", ".", "tensor", "(", "x", ",", "requires_grad", "=", "False", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "", "if", "norm_rews", ":", "\n", "            ", "ret_rews", "=", "[", "\n", "cast", "(", "\n", "(", "\n", "self", ".", "rew_buffs", "[", "i", "]", "[", "inds", "]", "\n", "-", "self", ".", "rew_buffs", "[", "i", "]", "[", ":", "self", ".", "filled_i", "]", ".", "mean", "(", ")", "\n", ")", "\n", "/", "self", ".", "rew_buffs", "[", "i", "]", "[", ":", "self", ".", "filled_i", "]", ".", "std", "(", ")", "\n", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "ret_rews", "=", "[", "cast", "(", "self", ".", "rew_buffs", "[", "i", "]", "[", "inds", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", "\n", "", "return", "(", "\n", "[", "cast", "(", "self", ".", "obs_buffs", "[", "i", "]", "[", "inds", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", ",", "\n", "[", "cast", "(", "self", ".", "ac_buffs", "[", "i", "]", "[", "inds", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", ",", "\n", "ret_rews", ",", "\n", "[", "cast", "(", "self", ".", "next_obs_buffs", "[", "i", "]", "[", "inds", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", ",", "\n", "[", "cast", "(", "self", ".", "done_buffs", "[", "i", "]", "[", "inds", "]", ")", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.latest_sample": [[159, 166], ["numpy.random.shuffle", "buffer.ReplayBuffer.sample_from_inds", "numpy.arange", "numpy.arange", "max"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.sample_from_inds"], ["", "def", "latest_sample", "(", "self", ",", "N", ",", "to_gpu", "=", "False", ",", "norm_rews", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "filled_i", "==", "self", ".", "max_steps", ":", "\n", "            ", "inds", "=", "np", ".", "arange", "(", "self", ".", "curr_i", "-", "N", ",", "self", ".", "curr_i", ")", "\n", "", "else", ":", "\n", "            ", "inds", "=", "np", ".", "arange", "(", "max", "(", "0", ",", "self", ".", "curr_i", "-", "N", ")", ",", "self", ".", "curr_i", ")", "\n", "", "np", ".", "random", ".", "shuffle", "(", "inds", ")", "\n", "return", "self", ".", "sample_from_inds", "(", "inds", ",", "to_gpu", ",", "norm_rews", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.episode_sample": [[167, 173], ["numpy.arange", "numpy.random.choice", "numpy.random.shuffle", "numpy.concatenate", "buffer.ReplayBuffer.sample_from_inds", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.sample_from_inds"], ["", "def", "episode_sample", "(", "self", ",", "N", ",", "to_gpu", "=", "False", ",", "norm_rews", "=", "True", ")", ":", "\n", "        ", "all_inds", "=", "np", ".", "arange", "(", "0", ",", "self", ".", "filled_i", ",", "50", ")", "\n", "inds", "=", "np", ".", "random", ".", "choice", "(", "all_inds", ",", "N", "//", "50", ",", "replace", "=", "False", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "inds", ")", "\n", "inds", "=", "np", ".", "concatenate", "(", "[", "np", ".", "arange", "(", "i", ",", "i", "+", "50", ")", "for", "i", "in", "inds", "]", ",", "axis", "=", "0", ")", "\n", "return", "self", ".", "sample_from_inds", "(", "inds", ",", "to_gpu", ",", "norm_rews", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.sample": [[174, 177], ["numpy.random.choice", "buffer.ReplayBuffer.sample_from_inds", "numpy.arange"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.sample_from_inds"], ["", "def", "sample", "(", "self", ",", "N", ",", "to_gpu", "=", "False", ",", "norm_rews", "=", "True", ",", "replace", ":", "bool", "=", "False", ")", ":", "\n", "        ", "inds", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "self", ".", "filled_i", ")", ",", "size", "=", "N", ",", "replace", "=", "replace", ")", "\n", "return", "self", ".", "sample_from_inds", "(", "inds", ",", "to_gpu", ",", "norm_rews", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.get_average_rewards": [[178, 186], ["numpy.arange", "numpy.arange", "max", "[].sum", "range"], "methods", ["None"], ["", "def", "get_average_rewards", "(", "self", ",", "ep_length", ",", "n_roll", ")", ":", "\n", "        ", "if", "self", ".", "filled_i", "==", "self", ".", "max_steps", ":", "\n", "            ", "inds", "=", "np", ".", "arange", "(", "\n", "self", ".", "curr_i", "-", "ep_length", "*", "n_roll", ",", "self", ".", "curr_i", "\n", ")", "# allow for negative indexing", "\n", "", "else", ":", "\n", "            ", "inds", "=", "np", ".", "arange", "(", "max", "(", "0", ",", "self", ".", "curr_i", "-", "ep_length", "*", "n_roll", ")", ",", "self", ".", "curr_i", ")", "\n", "", "return", "[", "self", ".", "rew_buffs", "[", "i", "]", "[", "inds", "]", ".", "sum", "(", ")", "/", "n_roll", "for", "i", "in", "range", "(", "self", ".", "num_agents", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.MLPNetwork.__init__": [[21, 49], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "networks.MLPNetwork.in_fn.weight.data.fill_", "networks.MLPNetwork.in_fn.bias.data.fill_"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "out_dim", ":", "int", ",", "\n", "hidden_dim", ":", "int", "=", "64", ",", "\n", "nonlin", ":", "Callable", "[", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", "=", "F", ".", "relu", ",", "\n", "norm_in", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            input_dim (int): Number of dimensions in input\n            out_dim (int): Number of dimensions in output\n            hidden_dim (int): Number of hidden dimensions\n            nonlin (PyTorch function): Nonlinearity to apply to hidden layers\n        \"\"\"", "\n", "super", "(", "MLPNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "norm_in", ":", "# normalize inputs", "\n", "            ", "self", ".", "in_fn", "=", "nn", ".", "BatchNorm1d", "(", "input_dim", ")", "\n", "self", ".", "in_fn", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "self", ".", "in_fn", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "in_fn", "=", "lambda", "x", ":", "x", "\n", "", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "hidden_dim", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", "\n", "self", ".", "fc_log_std", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "out_dim", ")", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "out_fn", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.MLPNetwork.forward": [[50, 64], ["networks.MLPNetwork.nonlin", "networks.MLPNetwork.nonlin", "networks.MLPNetwork.fc3", "networks.MLPNetwork.out_fn", "networks.MLPNetwork.fc1", "networks.MLPNetwork.fc2", "networks.MLPNetwork.in_fn"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "X", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            X (PyTorch Matrix): Batch of observations\n        Outputs:\n            out (PyTorch Matrix): Output of network (actions, values, etc)\n        \"\"\"", "\n", "h1", "=", "self", ".", "nonlin", "(", "self", ".", "fc1", "(", "self", ".", "in_fn", "(", "X", ")", ")", ")", "\n", "h2", "=", "self", ".", "nonlin", "(", "self", ".", "fc2", "(", "h1", ")", ")", "\n", "h3", "=", "self", ".", "fc3", "(", "h2", ")", "\n", "return", "self", ".", "out_fn", "(", "h3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.__init__": [[68, 96], ["torch.Module.__init__", "utils.misc.get_affine_params", "utils.misc.get_affine_params", "utils.misc.get_affine_params", "utils.misc.get_affine_params", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_affine_params", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_affine_params", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_affine_params", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_affine_params"], ["    ", "def", "__init__", "(", "self", ",", "ensemble_size", ":", "int", ",", "dim_in", ":", "int", ",", "dim_out", ":", "int", ",", "hidden_dim", ":", "int", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_net", "=", "ensemble_size", "\n", "\n", "self", ".", "dim_in", "=", "dim_in", "\n", "self", ".", "dim_out", "=", "dim_out", "\n", "\n", "self", ".", "lin0_w", ",", "self", ".", "lin0_b", "=", "get_affine_params", "(", "ensemble_size", ",", "dim_in", ",", "hidden_dim", ")", "\n", "\n", "self", ".", "lin1_w", ",", "self", ".", "lin1_b", "=", "get_affine_params", "(", "\n", "ensemble_size", ",", "hidden_dim", ",", "hidden_dim", "\n", ")", "\n", "\n", "self", ".", "lin2_w", ",", "self", ".", "lin2_b", "=", "get_affine_params", "(", "\n", "ensemble_size", ",", "hidden_dim", ",", "hidden_dim", "\n", ")", "\n", "\n", "self", ".", "lin3_w", ",", "self", ".", "lin3_b", "=", "get_affine_params", "(", "ensemble_size", ",", "hidden_dim", ",", "dim_out", ")", "\n", "\n", "self", ".", "inputs_mu", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "dim_in", ")", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "inputs_sigma", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "dim_in", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n", "self", ".", "max_logvar", "=", "nn", ".", "Parameter", "(", "\n", "torch", ".", "ones", "(", "1", ",", "dim_out", "//", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "/", "2.0", "\n", ")", "\n", "self", ".", "min_logvar", "=", "nn", ".", "Parameter", "(", "\n", "-", "torch", ".", "ones", "(", "1", ",", "dim_out", "//", "2", ",", "dtype", "=", "torch", ".", "float32", ")", "*", "10.0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.compute_decays": [[98, 106], ["None"], "methods", ["None"], ["", "def", "compute_decays", "(", "self", ")", ":", "\n", "\n", "        ", "lin0_decays", "=", "0.0001", "*", "(", "self", ".", "lin0_w", "**", "2", ")", ".", "sum", "(", ")", "/", "2.0", "\n", "lin1_decays", "=", "0.00025", "*", "(", "self", ".", "lin1_w", "**", "2", ")", ".", "sum", "(", ")", "/", "2.0", "\n", "lin2_decays", "=", "0.00025", "*", "(", "self", ".", "lin2_w", "**", "2", ")", ".", "sum", "(", ")", "/", "2.0", "\n", "lin3_decays", "=", "0.0005", "*", "(", "self", ".", "lin3_w", "**", "2", ")", ".", "sum", "(", ")", "/", "2.0", "\n", "\n", "return", "lin0_decays", "+", "lin1_decays", "+", "lin2_decays", "+", "lin3_decays", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.fit_input_stats": [[107, 115], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.std", "torch.std", "torch.std", "torch.std", "torch.std", "torch.std", "torch.std", "torch.std", "torch.std", "torch.mean.to().float", "torch.mean.to().float", "torch.mean.to().float", "torch.std.to().float", "torch.std.to().float", "torch.std.to().float", "torch.mean.to", "torch.mean.to", "torch.mean.to", "torch.std.to", "torch.std.to", "torch.std.to", "next", "next", "networks.PtModel.parameters", "networks.PtModel.parameters"], "methods", ["None"], ["", "def", "fit_input_stats", "(", "self", ",", "data", ":", "torch", ".", "Tensor", ")", ":", "\n", "\n", "        ", "mu", "=", "torch", ".", "mean", "(", "data", ",", "0", ",", "keepdims", "=", "True", ")", "\n", "sigma", "=", "torch", ".", "std", "(", "data", ",", "0", ",", "keepdims", "=", "True", ")", "\n", "sigma", "[", "sigma", "<", "1e-12", "]", "=", "1.0", "\n", "\n", "self", ".", "inputs_mu", ".", "data", "=", "mu", ".", "to", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", ".", "float", "(", ")", "\n", "self", ".", "inputs_sigma", ".", "data", "=", "sigma", ".", "to", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.forward": [[116, 144], ["utils.misc.swish", "utils.misc.swish", "utils.misc.swish", "utils.misc.swish.matmul", "utils.misc.swish.matmul", "utils.misc.swish.matmul", "utils.misc.swish.matmul", "torch.softplus", "torch.softplus", "torch.softplus", "torch.softplus", "torch.softplus", "torch.softplus", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.swish", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.swish", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.swish"], ["", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "ret_logvar", ":", "bool", "=", "False", ")", ":", "\n", "\n", "# Transform inputs", "\n", "        ", "inputs", "=", "(", "inputs", "-", "self", ".", "inputs_mu", ")", "/", "self", ".", "inputs_sigma", "\n", "\n", "inputs", "=", "inputs", ".", "matmul", "(", "self", ".", "lin0_w", ")", "+", "self", ".", "lin0_b", "\n", "\n", "inputs", "=", "swish", "(", "inputs", ")", "\n", "\n", "inputs", "=", "inputs", ".", "matmul", "(", "self", ".", "lin1_w", ")", "+", "self", ".", "lin1_b", "\n", "inputs", "=", "swish", "(", "inputs", ")", "\n", "\n", "inputs", "=", "inputs", ".", "matmul", "(", "self", ".", "lin2_w", ")", "+", "self", ".", "lin2_b", "\n", "inputs", "=", "swish", "(", "inputs", ")", "\n", "\n", "inputs", "=", "inputs", ".", "matmul", "(", "self", ".", "lin3_w", ")", "+", "self", ".", "lin3_b", "\n", "\n", "mean", "=", "inputs", "[", "...", ",", ":", "self", ".", "dim_out", "//", "2", "]", "\n", "\n", "logvar", "=", "inputs", "[", "...", ",", "self", ".", "dim_out", "//", "2", ":", "]", "\n", "\n", "logvar", "=", "self", ".", "max_logvar", "-", "F", ".", "softplus", "(", "self", ".", "max_logvar", "-", "logvar", ")", "\n", "logvar", "=", "self", ".", "min_logvar", "+", "F", ".", "softplus", "(", "logvar", "-", "self", ".", "min_logvar", ")", "\n", "\n", "if", "ret_logvar", ":", "\n", "            ", "return", "mean", ",", "logvar", "\n", "\n", "", "return", "mean", ",", "torch", ".", "exp", "(", "logvar", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.make_env.make_env": [[19, 66], ["scenarios.load().Scenario", "scenarios.load().Scenario.make_world", "gym.make", "MultiAgentEnv", "MultiAgentEnv", "scenarios.load"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.make_world", "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load"], ["def", "make_env", "(", "scenario_name", ",", "benchmark", ":", "bool", "=", "False", ",", "discrete_action", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Creates a MultiAgentEnv object as env. This can be used similar to a gym\n    environment by calling env.reset() and env.step().\n    Use env.render() to view the environment on the screen.\n\n    Input:\n        scenario_name   :   name of the scenario from ./scenarios/ to be Returns\n                            (without the .py extension)\n        benchmark       :   whether you want to produce benchmarking data\n                            (usually only done during evaluation)\n\n    Some useful env properties (see environment.py):\n        .observation_space  :   Returns the observation space for each agent\n        .action_space       :   Returns the action space for each agent\n        .n                  :   Returns the number of Agents\n    \"\"\"", "\n", "from", "multiagent", ".", "environment", "import", "MultiAgentEnv", "\n", "import", "multiagent", ".", "scenarios", "as", "scenarios", "\n", "\n", "if", "\"v0\"", "in", "scenario_name", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "scenario_name", ")", "\n", "return", "env", "\n", "# load scenario from script", "\n", "", "scenario", "=", "scenarios", ".", "load", "(", "scenario_name", "+", "\".py\"", ")", ".", "Scenario", "(", ")", "\n", "# create world", "\n", "world", "=", "scenario", ".", "make_world", "(", ")", "\n", "# create multiagent environment", "\n", "\n", "if", "benchmark", ":", "\n", "        ", "env", "=", "MultiAgentEnv", "(", "\n", "world", ",", "\n", "scenario", ".", "reset_world", ",", "\n", "scenario", ".", "reward", ",", "\n", "scenario", ".", "observation", ",", "\n", "scenario", ".", "benchmark_data", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "env", "=", "MultiAgentEnv", "(", "\n", "world", ",", "\n", "scenario", ".", "reset_world", ",", "\n", "scenario", ".", "reward", ",", "\n", "scenario", ".", "observation", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.make_env.make_parallel_env": [[68, 84], ["utils.env_wrappers.DummyVecEnv", "utils.env_wrappers.SubprocVecEnv", "numpy.random.seed", "make_env.make_env", "make_env.seed", "make_env.make_parallel_env.get_env_fn"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.__init__.make_env", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed"], ["", "def", "make_parallel_env", "(", "\n", "env_id", ":", "str", ",", "n_rollout_threads", ":", "int", ",", "seed", ":", "int", ",", "discrete_action", "=", "True", "\n", ")", ":", "\n", "    ", "def", "get_env_fn", "(", "rank", ")", ":", "\n", "        ", "def", "init_env", "(", ")", ":", "\n", "            ", "np", ".", "random", ".", "seed", "(", "seed", "+", "rank", "*", "1000", ")", "\n", "env", "=", "make_env", "(", "env_id", ",", "discrete_action", "=", "discrete_action", ")", "\n", "env", ".", "seed", "(", "seed", "+", "rank", "*", "1000", ")", "\n", "return", "env", "\n", "\n", "", "return", "init_env", "\n", "\n", "", "if", "n_rollout_threads", "==", "1", ":", "\n", "        ", "return", "DummyVecEnv", "(", "[", "get_env_fn", "(", "0", ")", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "SubprocVecEnv", "(", "[", "get_env_fn", "(", "i", ")", "for", "i", "in", "range", "(", "n_rollout_threads", ")", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.optimizer_lr_multistep_scheduler.__init__": [[240, 252], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "steps", ":", "List", "[", "int", "]", ",", "\n", "optimizers", ":", "List", "[", "torch", ".", "optim", ".", "Optimizer", "]", ",", "\n", "gamma", ":", "float", "=", "0.5", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "step_i", "=", "0", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "steps", "=", "steps", "\n", "self", ".", "optimizers", "=", "optimizers", "\n", "self", ".", "logger", "=", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.optimizer_lr_multistep_scheduler.step": [[253, 263], ["print", "len", "misc.optimizer_lr_multistep_scheduler.logger.add_scalars"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "episode", ":", "int", ")", ":", "\n", "        ", "if", "self", ".", "step_i", "<", "len", "(", "self", ".", "steps", ")", "and", "episode", ">=", "self", ".", "steps", "[", "self", ".", "step_i", "]", ":", "\n", "            ", "for", "optimizer", "in", "self", ".", "optimizers", ":", "\n", "                ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "param_group", "[", "\"lr\"", "]", "*=", "self", ".", "gamma", "\n", "", "", "self", ".", "step_i", "+=", "1", "\n", "lr", "=", "self", ".", "optimizers", "[", "0", "]", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", "\n", "if", "self", ".", "logger", ":", "\n", "                ", "self", ".", "logger", ".", "add_scalars", "(", "\"hypers/\"", ",", "{", "\"model_lr\"", ":", "lr", "}", ",", "episode", ")", "\n", "", "print", "(", "f\"decrease dynamics lr to {lr}\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.t_2_n": [[15, 17], ["tensor.cpu().detach().numpy", "tensor.cpu().detach", "tensor.cpu"], "function", ["None"], ["def", "t_2_n", "(", "tensor", ":", "torch", ".", "Tensor", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "return", "tensor", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.n_2_t": [[19, 34], ["torch.tensor", "torch.tensor", "torch.from_numpy", "torch.from_numpy", "torch.device", "torch.device"], "function", ["None"], ["", "def", "n_2_t", "(", "\n", "array", ":", "np", ".", "ndarray", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "copy", "=", "True", ",", "\n", "dtype", ":", "torch", ".", "dtype", "=", "torch", ".", "float", ",", "\n", "device", ":", "torch", ".", "device", "=", "\"cpu\"", ",", "\n", ")", ":", "\n", "    ", "if", "copy", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "\n", "array", ",", "requires_grad", "=", "requires_grad", ",", "dtype", "=", "dtype", ",", "device", "=", "device", "\n", ")", "\n", "", "else", ":", "\n", "        ", "assert", "requires_grad", "is", "False", ",", "\"copy from numpy\"", "\n", "assert", "torch", ".", "device", "(", "device", ")", ".", "type", "==", "\"cpu\"", ",", "\"copy from numpy\"", "\n", "return", "torch", ".", "from_numpy", "(", "array", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.soft_update": [[36, 48], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["", "", "def", "soft_update", "(", "target", ",", "source", ",", "tau", ")", ":", "\n", "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L11", "\n", "    ", "\"\"\"\n    Perform DDPG soft update (move target params toward source based on weight\n    factor tau)\n    Inputs:\n        target (torch.nn.Module): Net to copy parameters to\n        source (torch.nn.Module): Net whose parameters to copy\n        tau (float, 0 < x < 1): Weight factor for update\n    \"\"\"", "\n", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "target_param", ".", "data", "*", "(", "1.0", "-", "tau", ")", "+", "param", ".", "data", "*", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.hard_update": [[50, 60], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["", "", "def", "hard_update", "(", "target", ",", "source", ")", ":", "\n", "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L15", "\n", "    ", "\"\"\"\n    Copy network parameters from source to target\n    Inputs:\n        target (torch.nn.Module): Net to copy parameters to\n        source (torch.nn.Module): Net whose parameters to copy\n    \"\"\"", "\n", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.onehot_from_logits": [[62, 98], ["torch.eye", "torch.eye", "torch.log", "torch.log", "torch.stack", "torch.stack", "torch.argmax", "torch.argmax", "torch.softmax", "torch.log", "torch.log", "numpy.random.choice", "torch.stack", "torch.stack", "range", "F.softmax.max", "F.softmax.gather", "torch.sum", "torch.sum", "enumerate", "enumerate", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "function", ["None"], ["", "", "def", "onehot_from_logits", "(", "logits", ",", "eps", "=", "0.0", ",", "return_log_prob", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Given batch of logits, return one-hot sample using epsilon greedy strategy\n    (based on given epsilon)\n    \"\"\"", "\n", "# get best (according to current policy) actions in one-hot form", "\n", "argmax_acs", "=", "(", "logits", "==", "logits", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", ")", ".", "float", "(", ")", "\n", "if", "eps", "==", "0.0", ":", "\n", "        ", "if", "return_log_prob", ":", "\n", "            ", "inds", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "logits", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "log_prob", "=", "torch", ".", "log", "(", "logits", ".", "gather", "(", "1", ",", "inds", ")", "+", "1e-10", ")", "\n", "return", "argmax_acs", ",", "log_prob", "\n", "", "return", "argmax_acs", "\n", "# get random actions in one-hot form", "\n", "", "rand_acs", "=", "torch", ".", "eye", "(", "logits", ".", "shape", "[", "1", "]", ",", "requires_grad", "=", "False", ")", "[", "\n", "np", ".", "random", ".", "choice", "(", "range", "(", "logits", ".", "shape", "[", "1", "]", ")", ",", "size", "=", "logits", ".", "shape", "[", "0", "]", ")", "\n", "]", "\n", "# chooses between best and random actions using epsilon greedy", "\n", "\n", "if", "return_log_prob", ":", "\n", "        ", "log_prob", "=", "torch", ".", "log", "(", "logits", "/", "torch", ".", "sum", "(", "logits", ")", "+", "1e-10", ")", "\n", "return", "(", "\n", "torch", ".", "stack", "(", "\n", "[", "\n", "argmax_acs", "[", "i", "]", "if", "r", ">", "eps", "else", "rand_acs", "[", "i", "]", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "torch", ".", "rand", "(", "logits", ".", "shape", "[", "0", "]", ")", ")", "\n", "]", "\n", ")", ",", "\n", "log_prob", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "\n", "[", "\n", "argmax_acs", "[", "i", "]", "if", "r", ">", "eps", "else", "rand_acs", "[", "i", "]", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "torch", ".", "rand", "(", "logits", ".", "shape", "[", "0", "]", ")", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.sample_gumbel": [[102, 107], ["tens_type().clone().detach().requires_grad_().uniform_().to", "torch.log", "torch.log", "tens_type().clone().detach().requires_grad_().uniform_", "tens_type().clone().detach().requires_grad_", "torch.log", "torch.log", "tens_type().clone().detach", "tens_type().clone", "tens_type"], "function", ["None"], ["", "", "def", "sample_gumbel", "(", "shape", ",", "eps", "=", "1e-20", ",", "tens_type", "=", "torch", ".", "FloatTensor", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb", "\n", "    ", "\"\"\"Sample from Gumbel(0, 1)\"\"\"", "\n", "U", "=", "tens_type", "(", "*", "shape", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "requires_grad_", "(", "False", ")", ".", "uniform_", "(", ")", ".", "to", "(", "device", ")", "\n", "return", "-", "torch", ".", "log", "(", "-", "torch", ".", "log", "(", "U", "+", "eps", ")", "+", "eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax_sample": [[109, 117], ["torch.softmax", "misc.sample_gumbel", "type"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.sample_gumbel"], ["", "def", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", ":", "\n", "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb", "\n", "    ", "\"\"\"Draw a sample from the Gumbel-Softmax distribution\"\"\"", "\n", "# print(logits)", "\n", "y", "=", "logits", "+", "sample_gumbel", "(", "\n", "logits", ".", "shape", ",", "tens_type", "=", "type", "(", "logits", ".", "data", ")", ",", "device", "=", "logits", ".", "device", "\n", ")", "\n", "return", "F", ".", "softmax", "(", "y", "/", "temperature", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax": [[119, 141], ["misc.gumbel_softmax_sample", "misc.onehot_from_logits", "torch.argmax", "torch.argmax", "torch.log", "torch.log", "softmax_output.gather"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax_sample", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.onehot_from_logits"], ["", "def", "gumbel_softmax", "(", "logits", ",", "temperature", "=", "1.0", ",", "hard", "=", "False", ",", "return_log_prob", "=", "False", ")", ":", "\n", "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb", "\n", "    ", "\"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n    Args:\n      logits: [batch_size, n_class] unnormalized log-probs\n      temperature: non-negative scalar\n      hard: if True, take argmax, but differentiate w.r.t. soft sample y\n    Returns:\n      [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n      If hard=True, then the returned sample will be one-hot, otherwise it will\n      be a probabilitiy distribution that sums to 1 across classes\n    \"\"\"", "\n", "y", "=", "gumbel_softmax_sample", "(", "logits", ",", "temperature", ")", "\n", "softmax_output", "=", "y", "\n", "if", "hard", ":", "\n", "        ", "y_hard", "=", "onehot_from_logits", "(", "y", ")", "\n", "y", "=", "(", "y_hard", "-", "y", ")", ".", "detach", "(", ")", "+", "y", "# for the correct gradients", "\n", "", "if", "return_log_prob", ":", "\n", "        ", "inds", "=", "torch", ".", "argmax", "(", "y", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "log_prob", "=", "torch", ".", "log", "(", "softmax_output", ".", "gather", "(", "1", ",", "inds", ")", "+", "1e-10", ")", "\n", "return", "y", ",", "log_prob", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.swish": [[143, 146], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "# from https://github.com/quanvuong/handful-of-trials-pytorch/blob/master/config/utils.py", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.truncated_normal": [[148, 165], ["tensorflow.ConfigProto", "tensorflow.Session", "tf.Session.run", "tf.Session.close", "torch.tensor", "torch.tensor", "tensorflow.random.truncated_normal"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.None.main.run", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.truncated_normal"], ["", "def", "truncated_normal", "(", "size", ",", "std", ")", ":", "\n", "# from https://github.com/quanvuong/handful-of-trials-pytorch/blob/master/config/utils.py", "\n", "# We use TF to implement initialization function for neural network weight because:", "\n", "# 1. Pytorch doesn't support truncated normal", "\n", "# 2. This specific type of initialization is important for rapid progress early in training in cartpole", "\n", "\n", "# Do not allow tf to use gpu memory unnecessarily", "\n", "    ", "cfg", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "cfg", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "cfg", ")", "\n", "val", "=", "sess", ".", "run", "(", "tf", ".", "random", ".", "truncated_normal", "(", "shape", "=", "size", ",", "stddev", "=", "std", ")", ")", "\n", "\n", "# Close the session and free resources", "\n", "sess", ".", "close", "(", ")", "\n", "\n", "return", "torch", ".", "tensor", "(", "val", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_affine_params": [[167, 178], ["misc.truncated_normal", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.zeros", "numpy.sqrt"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.truncated_normal"], ["", "def", "get_affine_params", "(", "ensemble_size", ",", "in_features", ",", "out_features", ")", ":", "\n", "# from https://github.com/quanvuong/handful-of-trials-pytorch/blob/master/config/utils.py", "\n", "    ", "w", "=", "truncated_normal", "(", "\n", "size", "=", "(", "ensemble_size", ",", "in_features", ",", "out_features", ")", ",", "\n", "std", "=", "1.0", "/", "(", "2.0", "*", "np", ".", "sqrt", "(", "in_features", ")", ")", ",", "\n", ")", "\n", "w", "=", "nn", ".", "Parameter", "(", "w", ")", "\n", "\n", "b", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "ensemble_size", ",", "1", ",", "out_features", ",", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "\n", "return", "w", ",", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.apply_with_grad": [[180, 188], ["fn", "fn"], "function", ["None"], ["", "def", "apply_with_grad", "(", "tensor", ",", "fn", ")", ":", "\n", "    ", "\"\"\"\n    move the grad data and the tensor data to device with fn in-place\n    \"\"\"", "\n", "tensor", ".", "data", "=", "fn", "(", "tensor", ".", "data", ")", "\n", "if", "tensor", ".", "grad", "is", "not", "None", ":", "\n", "        ", "tensor", ".", "grad", ".", "data", "=", "fn", "(", "tensor", ".", "grad", ".", "data", ")", "\n", "", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.setup_seed": [[190, 196], ["torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed"], ["", "def", "setup_seed", "(", "seed", ":", "int", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action": [[198, 237], ["len", "fn", "torch.tensor", "torch.tensor", "enumerate", "enumerate", "fn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fn", "sum", "sum", "sum", "sum"], "function", ["None"], ["", "def", "get_multi_discrete_action", "(", "\n", "action", ":", "torch", ".", "Tensor", ",", "\n", "action_shape_list", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "fn", ":", "Callable", ",", "\n", "return_log_prob", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "if", "len", "(", "action_shape_list", ")", "==", "1", ":", "\n", "        ", "return", "fn", "(", "action", ",", "return_log_prob", "=", "return_log_prob", ")", "\n", "", "else", ":", "\n", "        ", "actions", "=", "torch", ".", "tensor", "(", "[", "]", ",", "device", "=", "action", ".", "device", ")", "\n", "if", "return_log_prob", ":", "\n", "            ", "log_prob", "=", "1", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "action_shape_list", ")", ":", "\n", "                ", "act", ",", "log_prob_", "=", "fn", "(", "\n", "action", "[", "\n", ":", ",", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "+", "dim", "\n", "]", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "actions", "=", "torch", ".", "cat", "(", "[", "actions", ",", "act", "]", ",", "dim", "=", "1", ")", "\n", "log_prob", "*=", "log_prob_", "\n", "", "return", "actions", ",", "log_prob", "\n", "", "else", ":", "\n", "            ", "for", "i", ",", "dim", "in", "enumerate", "(", "action_shape_list", ")", ":", "\n", "                ", "actions", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "actions", ",", "\n", "fn", "(", "\n", "action", "[", "\n", ":", ",", "\n", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "\n", "+", "dim", ",", "\n", "]", ",", "\n", "return_log_prob", "=", "False", ",", "\n", ")", ",", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.__init__": [[43, 73], ["len", "zip", "env_wrappers.SubprocVecEnv.remotes[].send", "env_wrappers.SubprocVecEnv.remotes[].recv", "env_wrappers.SubprocVecEnv.remotes[].send", "env_wrappers.SubprocVecEnv.remotes[].recv", "len", "stable_baselines3.common.vec_env.base_vec_env.VecEnv.__init__", "multiprocessing.Process", "p.start", "remote.close", "len", "zip", "multiprocessing.Pipe", "range", "stable_baselines3.common.vec_env.base_vec_env.CloudpickleWrapper"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["    ", "def", "__init__", "(", "self", ",", "env_fns", ",", "spaces", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        envs: list of gym environments to run in subprocesses\n        \"\"\"", "\n", "self", ".", "waiting", "=", "False", "\n", "self", ".", "closed", "=", "False", "\n", "nenvs", "=", "len", "(", "env_fns", ")", "\n", "self", ".", "remotes", ",", "self", ".", "work_remotes", "=", "zip", "(", "*", "[", "Pipe", "(", ")", "for", "_", "in", "range", "(", "nenvs", ")", "]", ")", "\n", "self", ".", "ps", "=", "[", "\n", "Process", "(", "\n", "target", "=", "worker", ",", "args", "=", "(", "work_remote", ",", "remote", ",", "CloudpickleWrapper", "(", "env_fn", ")", ")", "\n", ")", "\n", "for", "(", "work_remote", ",", "remote", ",", "env_fn", ")", "in", "zip", "(", "\n", "self", ".", "work_remotes", ",", "self", ".", "remotes", ",", "env_fns", "\n", ")", "\n", "]", "\n", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "daemon", "=", "(", "\n", "True", "# if the main process crashes, we should not cause things to hang", "\n", ")", "\n", "p", ".", "start", "(", ")", "\n", "", "for", "remote", "in", "self", ".", "work_remotes", ":", "\n", "            ", "remote", ".", "close", "(", ")", "\n", "\n", "", "self", ".", "remotes", "[", "0", "]", ".", "send", "(", "(", "\"get_spaces\"", ",", "None", ")", ")", "\n", "observation_space", ",", "action_space", "=", "self", ".", "remotes", "[", "0", "]", ".", "recv", "(", ")", "\n", "self", ".", "remotes", "[", "0", "]", ".", "send", "(", "(", "\"get_agent_types\"", ",", "None", ")", ")", "\n", "self", ".", "agent_types", "=", "self", ".", "remotes", "[", "0", "]", ".", "recv", "(", ")", "\n", "self", ".", "n", "=", "len", "(", "self", ".", "agent_types", ")", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "observation_space", ",", "action_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.step_async": [[74, 78], ["zip", "remote.send"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "for", "remote", ",", "action", "in", "zip", "(", "self", ".", "remotes", ",", "actions", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "\"step\"", ",", "action", ")", ")", "\n", "", "self", ".", "waiting", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.step_wait": [[79, 84], ["zip", "remote.recv", "numpy.stack", "numpy.stack", "numpy.stack"], "methods", ["None"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "results", "=", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "self", ".", "waiting", "=", "False", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "results", ")", "\n", "return", "np", ".", "stack", "(", "obs", ")", ",", "np", ".", "stack", "(", "rews", ")", ",", "np", ".", "stack", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.reset": [[85, 89], ["numpy.stack", "remote.send", "remote.recv"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "\"reset\"", ",", "None", ")", ")", "\n", "", "return", "np", ".", "stack", "(", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.reset_task": [[90, 94], ["numpy.stack", "remote.send", "remote.recv"], "methods", ["None"], ["", "def", "reset_task", "(", "self", ")", ":", "\n", "        ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "\"reset_task\"", ",", "None", ")", ")", "\n", "", "return", "np", ".", "stack", "(", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.close": [[95, 106], ["remote.send", "p.join", "remote.recv"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "closed", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "waiting", ":", "\n", "            ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "                ", "remote", ".", "recv", "(", ")", "\n", "", "", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "\"close\"", ",", "None", ")", ")", "\n", "", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "join", "(", ")", "\n", "", "self", ".", "closed", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.env_is_wrapped": [[107, 109], ["None"], "methods", ["None"], ["", "def", "env_is_wrapped", "(", "self", ",", "wrapper_class", ",", "indices", ")", "->", "List", "[", "bool", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.env_method": [[110, 112], ["None"], "methods", ["None"], ["", "def", "env_method", "(", "self", ",", "method_name", ":", "str", ",", "*", "method_args", ",", "indices", ",", "**", "method_kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.get_attr": [[113, 115], ["None"], "methods", ["None"], ["", "def", "get_attr", "(", "self", ",", "attr_name", ":", "str", ",", "indice", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.set_attr": [[116, 118], ["None"], "methods", ["None"], ["", "def", "set_attr", "(", "self", ",", "attr_name", ":", "str", ",", "value", ",", "indices", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.seed": [[119, 121], ["None"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "seed", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.DummyVecEnv.__init__": [[124, 136], ["stable_baselines3.common.vec_env.base_vec_env.VecEnv.__init__", "all", "numpy.zeros", "fn", "len", "len", "hasattr"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env_fns", ")", ":", "\n", "        ", "self", ".", "envs", "=", "[", "fn", "(", ")", "for", "fn", "in", "env_fns", "]", "\n", "env", "=", "self", ".", "envs", "[", "0", "]", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", "\n", "if", "all", "(", "[", "hasattr", "(", "a", ",", "\"adversary\"", ")", "for", "a", "in", "env", ".", "agents", "]", ")", ":", "\n", "            ", "self", ".", "agent_types", "=", "[", "\n", "\"adversary\"", "if", "a", ".", "adversary", "else", "\"agent\"", "for", "a", "in", "env", ".", "agents", "\n", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "agent_types", "=", "[", "\"agent\"", "for", "_", "in", "env", ".", "agents", "]", "\n", "", "self", ".", "ts", "=", "np", ".", "zeros", "(", "len", "(", "self", ".", "envs", ")", ",", "dtype", "=", "\"int\"", ")", "\n", "self", ".", "actions", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.DummyVecEnv.step_async": [[137, 139], ["None"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "actions", "=", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.DummyVecEnv.step_wait": [[140, 150], ["map", "enumerate", "env.step", "zip", "all", "numpy.array", "numpy.array", "numpy.array", "zip", "env_wrappers.DummyVecEnv.envs[].reset"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "results", "=", "[", "env", ".", "step", "(", "a", ")", "for", "(", "a", ",", "env", ")", "in", "zip", "(", "self", ".", "actions", ",", "self", ".", "envs", ")", "]", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "map", "(", "np", ".", "array", ",", "zip", "(", "*", "results", ")", ")", "\n", "self", ".", "ts", "+=", "1", "\n", "for", "(", "i", ",", "done", ")", "in", "enumerate", "(", "dones", ")", ":", "\n", "            ", "if", "all", "(", "done", ")", ":", "\n", "                ", "obs", "[", "i", "]", "=", "self", ".", "envs", "[", "i", "]", ".", "reset", "(", ")", "\n", "self", ".", "ts", "[", "i", "]", "=", "0", "\n", "", "", "self", ".", "actions", "=", "None", "\n", "return", "np", ".", "array", "(", "obs", ")", ",", "np", ".", "array", "(", "rews", ")", ",", "np", ".", "array", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.DummyVecEnv.reset": [[151, 154], ["numpy.array", "env.reset"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "results", "=", "[", "env", ".", "reset", "(", ")", "for", "env", "in", "self", ".", "envs", "]", "\n", "return", "np", ".", "array", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.DummyVecEnv.close": [[155, 157], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.worker": [[10, 40], ["parent_remote.close", "env_fn_wrapper.var", "remote.recv", "env_fn_wrapper.var.step", "all", "remote.send", "env_fn_wrapper.var.reset", "env_fn_wrapper.var.reset", "remote.send", "env_fn_wrapper.var.reset_task", "remote.send", "remote.close", "remote.send", "all", "remote.send", "remote.send", "hasattr"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.env_wrappers.SubprocVecEnv.reset_task", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["def", "worker", "(", "remote", ",", "parent_remote", ",", "env_fn_wrapper", ":", "CloudpickleWrapper", ")", ":", "\n", "    ", "parent_remote", ".", "close", "(", ")", "\n", "env", "=", "env_fn_wrapper", ".", "var", "(", ")", "\n", "while", "True", ":", "\n", "        ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "\"step\"", ":", "\n", "            ", "ob", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "", "remote", ".", "send", "(", "(", "ob", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "\"reset\"", ":", "\n", "            ", "ob", "=", "env", ".", "reset", "(", ")", "\n", "remote", ".", "send", "(", "ob", ")", "\n", "", "elif", "cmd", "==", "\"reset_task\"", ":", "\n", "            ", "ob", "=", "env", ".", "reset_task", "(", ")", "\n", "remote", ".", "send", "(", "ob", ")", "\n", "", "elif", "cmd", "==", "\"close\"", ":", "\n", "            ", "remote", ".", "close", "(", ")", "\n", "break", "\n", "", "elif", "cmd", "==", "\"get_spaces\"", ":", "\n", "            ", "remote", ".", "send", "(", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", ")", "\n", "", "elif", "cmd", "==", "\"get_agent_types\"", ":", "\n", "            ", "if", "all", "(", "[", "hasattr", "(", "a", ",", "\"adversary\"", ")", "for", "a", "in", "env", ".", "agents", "]", ")", ":", "\n", "                ", "remote", ".", "send", "(", "\n", "[", "\"adversary\"", "if", "a", ".", "adversary", "else", "\"agent\"", "for", "a", "in", "env", ".", "agents", "]", "\n", ")", "\n", "", "else", ":", "\n", "                ", "remote", ".", "send", "(", "[", "\"agent\"", "for", "_", "in", "env", ".", "agents", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.__init__": [[19, 107], ["baseframework.BaseFramework.__init__", "dict", "enumerate", "enumerate", "zip", "print", "maframework.MA_Controller.agents.append", "print", "maframework.MA_Controller.agent_difficulties.append"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "agent_init_params", ":", "List", "[", "Dict", "]", ",", "\n", "alg_types", ":", "List", "[", "str", "]", ",", "\n", "rew_scale", ":", "float", "=", "1.0", ",", "\n", "gamma", ":", "float", "=", "0.95", ",", "\n", "tau", ":", "float", "=", "0.01", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "hidden_dim", ":", "int", "=", "64", ",", "\n", "discrete_action", ":", "int", "=", "True", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "alg_types", ",", "\n", "rew_scale", "=", "rew_scale", ",", "\n", "gamma", "=", "gamma", ",", "\n", "tau", "=", "tau", ",", "\n", "lr", "=", "lr", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", ")", "\n", "\"\"\"\n        Inputs:\n            agent_init_params (list of dict): List of dicts with parameters to\n                                              initialize each agent\n                (possible argments)\n                dim_in_pol (int): Input dimensions to policy\n                dim_out_pol (int): Output dimensions to policy\n                dim_in_critic (int): Input dimensions to critic\n                dim_in_opp_pols (int): Input dimensions to opponent policies\n                dim_out_opp_pols (int): Output dimensions to opponent policies\n                n_agent (int): number of agents\n                agent_index (int): index of the agent\n                alg_type (str): name of the algorithm the agent uses\n                auto_target_entropy (bool): learn the temperature automatically\n                target_entropy (numeral or function)\n                reparameterize (bool): seems useless\n                action_shape_list (bool): in some environment the agents can do many actions at a time\n\n            alg_types (list of str): Learning algorithm for each agent (DDPG\n                                       or MADDPG)\n            gamma (float): Discount factor\n            tau (float): Target update rate\n            lr (float): Learning rate for policy and critic\n            hidden_dim (int): Number of hidden dimensions for networks\n            discrete_action (bool): Whether or not to use discrete action space\n        \"\"\"", "\n", "self", ".", "agent_init_params", "=", "agent_init_params", "\n", "self", ".", "alg_types", "=", "alg_types", "\n", "self", ".", "agents", ":", "List", "[", "BaseAgent", "]", "=", "[", "]", "\n", "self", ".", "alg_names", "=", "[", "\n", "\"DDPG\"", ",", "\n", "\"MADDPGOppMd\"", ",", "\n", "\"AORDPG\"", ",", "\n", "\"SAC\"", ",", "\n", "\"MASACOppMd\"", ",", "\n", "\"AORPO\"", ",", "\n", "]", "\n", "self", ".", "name_to_alg", "=", "dict", "(", "\n", "zip", "(", "\n", "self", ".", "alg_names", ",", "\n", "[", "\n", "DDPGAgent", ",", "\n", "DDPGAgentOppMd", ",", "\n", "DDPGAgentOppMdCondMB", ",", "\n", "SACAgent", ",", "\n", "SACAgentOppMd", ",", "\n", "SACAgentOppMdCondMB", ",", "\n", "]", ",", "\n", ")", "\n", ")", "\n", "for", "i", ",", "params", "in", "enumerate", "(", "agent_init_params", ")", ":", "\n", "            ", "print", "(", "params", ")", "\n", "self", ".", "agents", ".", "append", "(", "\n", "self", ".", "name_to_alg", "[", "alg_types", "[", "i", "]", "]", "(", "\n", "lr", "=", "lr", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", "rew_scale", "=", "rew_scale", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", "**", "params", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "total_difficulty", "=", "0", "\n", "self", ".", "agent_difficulties", "=", "[", "]", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "self", ".", "agents", ")", ":", "\n", "            ", "print", "(", "\"agent\"", ",", "i", ",", "a", ".", "alg_type", ")", "\n", "self", ".", "total_difficulty", "+=", "a", ".", "dim_in_pol", "+", "a", ".", "dim_out_pol", "\n", "self", ".", "agent_difficulties", ".", "append", "(", "a", ".", "dim_in_pol", "+", "a", ".", "dim_out_pol", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.random_step": [[108, 110], ["a.random_step", "zip"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.random_step"], ["", "", "def", "random_step", "(", "self", ",", "observations", ":", "List", ")", ":", "\n", "        ", "return", "[", "a", ".", "random_step", "(", "obs", ")", "for", "a", ",", "obs", "in", "zip", "(", "self", ".", "agents", ",", "observations", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.step": [[111, 138], ["torch.stack", "torch.softmax", "torch.softmax.detach", "isinstance", "a.step", "a.step", "zip", "isinstance", "a.step", "a.step", "zip"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step"], ["", "def", "step", "(", "self", ",", "observations", ":", "List", ",", "explore", ":", "bool", "=", "False", ",", "return_raw", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment with all agents\n        Inputs:\n            observations: List of observations for each agent\n            explore (boolean): Whether or not to add exploration noise\n            return_raw(boolean): Whether return the raw action\n        Outputs:\n            actions: List of actions for each agent\n        \"\"\"", "\n", "if", "return_raw", ":", "\n", "# for discrete actions, return probabilities", "\n", "            ", "actions", "=", "[", "\n", "a", ".", "step", "(", "observations", ",", "explore", "=", "explore", ",", "return_raw", "=", "True", ")", "\n", "if", "(", "isinstance", "(", "a", ",", "AgentCond", ")", ")", "\n", "else", "a", ".", "step", "(", "obs", ",", "explore", "=", "explore", ",", "return_raw", "=", "True", ")", "\n", "for", "a", ",", "obs", "in", "zip", "(", "self", ".", "agents", ",", "observations", ")", "\n", "]", "\n", "actions", "=", "torch", ".", "stack", "(", "actions", ")", "\n", "actions", "=", "torch", ".", "softmax", "(", "actions", ",", "dim", "=", "2", ")", "\n", "return", "actions", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "[", "\n", "a", ".", "step", "(", "observations", ",", "explore", "=", "explore", ")", "\n", "if", "isinstance", "(", "a", ",", "AgentCond", ")", "\n", "else", "a", ".", "step", "(", "obs", ",", "explore", "=", "explore", ")", "\n", "for", "a", ",", "obs", "in", "zip", "(", "self", ".", "agents", ",", "observations", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update": [[140, 161], ["maframework.MA_Controller.agents[].update", "logger.add_scalars"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update"], ["", "", "def", "update", "(", "\n", "self", ",", "\n", "sample", ":", "Tuple", ",", "\n", "agent_i", ":", "int", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", "logger_iter", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Update parameters of a agent model based on sample from replay buffer\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            agent_i (int): index of agent to update\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        \"\"\"", "\n", "loss_dict", "=", "self", ".", "agents", "[", "agent_i", "]", ".", "update", "(", "sample", ")", "\n", "if", "logger", "and", "loss_dict", ":", "\n", "            ", "logger", ".", "add_scalars", "(", "\"agent%i/losses\"", "%", "(", "agent_i", ")", ",", "loss_dict", ",", "logger_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_dynamics_model": [[162, 165], ["isinstance", "copy.deepcopy", "maframework.MA_Controller.agents[].dynamics_model.state_dict"], "methods", ["None"], ["", "", "def", "get_dynamics_model", "(", "self", ",", "agent_i", ":", "int", ")", "->", "Dict", ":", "\n", "        ", "assert", "isinstance", "(", "self", ".", "agents", "[", "agent_i", "]", ",", "AgentMB", ")", ",", "\"get dynamics model\"", "\n", "return", "copy", ".", "deepcopy", "(", "self", ".", "agents", "[", "agent_i", "]", ".", "dynamics_model", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.set_dynamics_model": [[166, 169], ["isinstance", "maframework.MA_Controller.agents[].dynamics_model.load_state_dict"], "methods", ["None"], ["", "def", "set_dynamics_model", "(", "self", ",", "agent_i", ":", "int", ",", "model", ":", "Dict", ")", ":", "\n", "        ", "assert", "isinstance", "(", "self", ".", "agents", "[", "agent_i", "]", ",", "AgentMB", ")", ",", "\"get dynamics model\"", "\n", "self", ".", "agents", "[", "agent_i", "]", ".", "dynamics_model", ".", "load_state_dict", "(", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.get_opponent_model": [[170, 175], ["isinstance", "copy.deepcopy", "opp_pl.state_dict"], "methods", ["None"], ["", "def", "get_opponent_model", "(", "self", ",", "agent_i", ":", "int", ")", "->", "List", "[", "Dict", "]", ":", "\n", "        ", "assert", "isinstance", "(", "self", ".", "agents", "[", "agent_i", "]", ",", "AgentOppMd", ")", ",", "\"get opponent model\"", "\n", "return", "[", "\n", "copy", ".", "deepcopy", "(", "opp_pl", ".", "state_dict", "(", ")", ")", "\n", "for", "opp_pl", "in", "self", ".", "agents", "[", "agent_i", "]", ".", "opp_policies", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.set_opponent_model": [[177, 181], ["isinstance", "enumerate", "opp_pl.load_state_dict"], "methods", ["None"], ["", "def", "set_opponent_model", "(", "self", ",", "agent_i", ":", "int", ",", "models", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "assert", "isinstance", "(", "self", ".", "agents", "[", "agent_i", "]", ",", "AgentOppMd", ")", ",", "\"get opponent model\"", "\n", "for", "k", ",", "opp_pl", "in", "enumerate", "(", "self", ".", "agents", "[", "agent_i", "]", ".", "opp_policies", ")", ":", "\n", "            ", "opp_pl", ".", "load_state_dict", "(", "models", "[", "k", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update_opponent_models": [[182, 204], ["enumerate", "range", "isinstance", "a.update_opponent", "logger.add_scalars"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.update_opponent"], ["", "", "def", "update_opponent_models", "(", "\n", "self", ",", "\n", "sample", ":", "List", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", "logger_iter", ":", "int", "=", "0", ",", "\n", "epochs", ":", "int", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled the\n                    latest data from the replay buffer.\n                    Each is a list with entries corresponding to each agent.\n        \"\"\"", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "self", ".", "agents", ")", ":", "\n", "            ", "for", "e", "in", "range", "(", "epochs", ")", ":", "\n", "                ", "if", "isinstance", "(", "a", ",", "AgentOppMd", ")", ":", "\n", "                    ", "opp_loss", "=", "a", ".", "update_opponent", "(", "sample", ")", "\n", "\n", "if", "e", "==", "epochs", "-", "1", "and", "logger", "and", "opp_loss", ":", "\n", "                        ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/model_losses\"", "%", "(", "i", ")", ",", "opp_loss", ",", "logger_iter", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.rollout_models": [[206, 216], ["isinstance", "a.rollout_model"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.rollout_model"], ["", "", "", "", "", "def", "rollout_models", "(", "\n", "self", ",", "\n", "K", ":", "int", ",", "\n", "init_states", ":", "List", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", "logger_iter", ":", "int", "=", "0", ",", "\n", ")", ":", "\n", "        ", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "if", "isinstance", "(", "a", ",", "AgentMB", ")", ":", "\n", "                ", "a", ".", "rollout_model", "(", "K", ",", "init_states", ",", "self", ".", "agents", ",", "logger", ",", "logger_iter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update_dynamics_models": [[217, 230], ["enumerate", "isinstance", "a.update_model", "logger.add_scalars"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.update_model"], ["", "", "", "def", "update_dynamics_models", "(", "\n", "self", ",", "\n", "sample", ":", "Tuple", "[", "List", "]", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", "logger_iter", ":", "int", "=", "0", ",", "\n", "epochs", ":", "int", "=", "1", ",", "\n", ")", ":", "\n", "        ", "for", "i", ",", "a", "in", "enumerate", "(", "self", ".", "agents", ")", ":", "\n", "            ", "if", "isinstance", "(", "a", ",", "AgentMB", ")", ":", "\n", "                ", "m_loss_dict", "=", "a", ".", "update_model", "(", "sample", ",", "epochs", ")", "\n", "if", "logger", "and", "m_loss_dict", ":", "\n", "                    ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/model_losses\"", "%", "(", "i", ")", ",", "m_loss_dict", ",", "logger_iter", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.save": [[232, 242], ["maframework.MA_Controller.prep_training", "torch.save", "a.get_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params"], ["", "", "", "", "def", "save", "(", "self", ",", "model_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Save trained parameters of all agents into one file\n        \"\"\"", "\n", "self", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "# move parameters to CPU before saving", "\n", "save_dict", "=", "{", "\n", "\"init_dict\"", ":", "self", ".", "init_dict", ",", "\n", "\"agent_params\"", ":", "[", "a", ".", "get_params", "(", ")", "for", "a", "in", "self", ".", "agents", "]", ",", "\n", "}", "\n", "torch", ".", "save", "(", "save_dict", ",", "model_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.init_from_save": [[243, 254], ["torch.load", "cls", "zip", "a.load_params", "torch.device"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "@", "classmethod", "\n", "def", "init_from_save", "(", "cls", ",", "model_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate instance of this class from file created by 'save' method\n        \"\"\"", "\n", "save_dict", "=", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "\n", "instance", "=", "cls", "(", "**", "save_dict", "[", "\"init_dict\"", "]", ")", "\n", "instance", ".", "init_dict", "=", "save_dict", "[", "\"init_dict\"", "]", "\n", "for", "a", ",", "params", "in", "zip", "(", "instance", ".", "agents", ",", "save_dict", "[", "\"agent_params\"", "]", ")", ":", "\n", "            ", "a", ".", "load_params", "(", "params", ")", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.update_all_targets": [[255, 259], ["type", "a.update_targets"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.update_targets"], ["", "def", "update_all_targets", "(", "self", ")", ":", "\n", "        ", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "if", "type", "(", "a", ")", "!=", "BaseAgent", ":", "\n", "                ", "a", ".", "update_targets", "(", "self", ".", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.prep_rollouts": [[260, 263], ["a.prep_rollouts"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "", "", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "a", ".", "prep_rollouts", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.prep_training": [[264, 267], ["a.prep_training"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training"], ["", "", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "a", ".", "prep_training", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.compute_cooperate_interaction": [[268, 285], ["int", "enumerate"], "methods", ["None"], ["", "", "def", "compute_cooperate_interaction", "(", "self", ",", "n_episode", ":", "int", ",", "episode_length", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        return the normalized opponent sample complexity\n        \"\"\"", "\n", "num_inter", "=", "n_episode", "# count the interactions when sampling data", "\n", "\n", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "for", "i", ",", "opp_n", "in", "enumerate", "(", "a", ".", "opp_sample_num", ")", ":", "\n", "                ", "opp_i", "=", "i", "if", "i", "<", "a", ".", "index", "else", "i", "+", "1", "\n", "num_inter", "+=", "(", "\n", "opp_n", "\n", "*", "self", ".", "agent_difficulties", "[", "opp_i", "]", "\n", "/", "self", ".", "total_difficulty", "\n", "/", "episode_length", "\n", ")", "\n", "\n", "", "", "return", "int", "(", "num_inter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.init_from_env": [[286, 429], ["print", "print", "print", "print", "enumerate", "range", "cls", "zip", "dim_obs_list.append", "isinstance", "maframework.MA_Controller.init_from_env.get_shape"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "init_from_env", "(", "\n", "cls", ",", "\n", "env", ",", "\n", "env_id", ":", "str", ",", "\n", "config", ":", "argparse", ".", "Namespace", ",", "\n", "alg", ":", "str", "=", "\"MADDPG\"", ",", "\n", "rew_scale", ":", "float", "=", "1.0", ",", "\n", "gamma", ":", "float", "=", "0.95", ",", "\n", "tau", ":", "float", "=", "0.01", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "hidden_dim", ":", "int", "=", "64", ",", "\n", "model_lr", ":", "float", "=", "0.001", ",", "\n", "model_hidden_dim", ":", "int", "=", "512", ",", "\n", "ensemble_size", ":", "int", "=", "7", ",", "\n", "env_model_buffer_size", ":", "int", "=", "1e6", ",", "\n", "opp_lr", ":", "float", "=", "0.001", ",", "\n", ")", ":", "\n", "\n", "        ", "print", "(", "\"Environment settings\"", ")", "\n", "print", "(", "\"Action\"", ",", "env", ".", "action_space", ")", "\n", "print", "(", "\"Observation\"", ",", "[", "obsp", ".", "shape", "for", "obsp", "in", "env", ".", "observation_space", "]", ")", "\n", "print", "(", "env", ".", "agent_types", ")", "\n", "\n", "agent_init_params", "=", "[", "]", "\n", "alg_types", "=", "[", "alg", "]", "*", "env", ".", "n", "\n", "\n", "n_agent", "=", "env", ".", "n", "\n", "\n", "dim_in_pol_list", "=", "[", "]", "# the input dimension of all agents", "\n", "dim_out_pol_list", "=", "[", "]", "# the ouput dimension of all agents", "\n", "dim_obs_list", "=", "[", "]", "# the observation dimentsion of all agents", "\n", "action_shape_list", "=", "[", "]", "# the action dims of all agents", "\n", "for", "i", ",", "(", "acsp", ",", "obsp", ",", "algtype", ")", "in", "enumerate", "(", "\n", "zip", "(", "env", ".", "action_space", ",", "env", ".", "observation_space", ",", "alg_types", ")", "\n", ")", ":", "\n", "            ", "dim_in_pol", "=", "obsp", ".", "shape", "[", "0", "]", "\n", "dim_obs_list", ".", "append", "(", "dim_in_pol", ")", "\n", "\n", "discrete_action", "=", "True", "\n", "\n", "def", "get_shape", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "n", "if", "isinstance", "(", "x", ",", "Discrete", ")", "else", "sum", "(", "x", ".", "high", "-", "x", ".", "low", "+", "1", ")", "\n", "\n", "# for action sample reshaping", "\n", "", "if", "isinstance", "(", "acsp", ",", "Discrete", ")", ":", "\n", "                ", "action_shape", "=", "[", "get_shape", "(", "acsp", ")", "]", "\n", "", "else", ":", "# multidiscrete", "\n", "                ", "action_shape", "=", "(", "acsp", ".", "high", "-", "acsp", ".", "low", "+", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "", "dim_out_pol", "=", "get_shape", "(", "acsp", ")", "\n", "\n", "dim_in_critic", "=", "dim_in_pol", "+", "dim_out_pol", "\n", "# DDPG/SAC", "\n", "agent_init_params", ".", "append", "(", "\n", "{", "\n", "\"dim_in_pol\"", ":", "dim_in_pol", ",", "\n", "\"dim_out_pol\"", ":", "dim_out_pol", ",", "\n", "\"dim_in_critic\"", ":", "dim_in_critic", ",", "\n", "\"agent_index\"", ":", "i", ",", "\n", "\"n_agent\"", ":", "n_agent", ",", "\n", "\"alg_type\"", ":", "algtype", ",", "\n", "\"gamma\"", ":", "gamma", ",", "\n", "\"grad_bound\"", ":", "config", ".", "grad_bound", ",", "\n", "}", "\n", ")", "\n", "if", "\"OppMd\"", "in", "algtype", "or", "\"AOR\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "agent_init_params", "[", "i", "]", ".", "update", "(", "{", "\"opp_lr\"", ":", "opp_lr", "}", ")", "\n", "", "dim_in_pol_list", ".", "append", "(", "dim_in_pol", ")", "\n", "dim_out_pol_list", ".", "append", "(", "dim_out_pol", ")", "\n", "action_shape_list", ".", "append", "(", "action_shape", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "n_agent", ")", ":", "\n", "            ", "agent_init_params", "[", "i", "]", ".", "update", "(", "{", "\"action_shape_list\"", ":", "action_shape_list", "}", ")", "\n", "if", "\"Cond\"", "in", "alg_types", "[", "i", "]", "or", "\"AOR\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "agent_init_params", "[", "i", "]", ".", "update", "(", "\n", "{", "\n", "\"dim_in_pol\"", ":", "agent_init_params", "[", "i", "]", "[", "\"dim_in_pol\"", "]", "\n", "+", "sum", "(", "dim_out_pol_list", ")", "\n", "-", "agent_init_params", "[", "i", "]", "[", "\"dim_out_pol\"", "]", "\n", "}", "\n", ")", "\n", "", "if", "\"MA\"", "in", "alg_types", "[", "i", "]", "or", "\"AOR\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "dim_in_critic", "=", "sum", "(", "dim_obs_list", ")", "+", "sum", "(", "dim_out_pol_list", ")", "\n", "agent_init_params", "[", "i", "]", ".", "update", "(", "{", "\"dim_in_critic\"", ":", "dim_in_critic", "}", ")", "\n", "", "if", "\"SAC\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "agent_init_params", "[", "i", "]", ".", "update", "(", "\n", "{", "\n", "\"auto_target_entropy\"", ":", "True", ",", "\n", "}", "\n", ")", "\n", "", "if", "\"AOR\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "agent_init_params", "[", "i", "]", ".", "update", "(", "\n", "{", "\n", "\"dim_obs_list\"", ":", "dim_obs_list", ",", "\n", "\"dim_actions\"", ":", "dim_out_pol_list", ",", "\n", "\"dim_rewards\"", ":", "n_agent", ",", "\n", "\"model_lr\"", ":", "model_lr", ",", "\n", "\"model_hidden_dim\"", ":", "model_hidden_dim", ",", "\n", "\"ensemble_size\"", ":", "ensemble_size", ",", "\n", "\"MB_batch_size\"", ":", "config", ".", "MB_batch_size", ",", "\n", "}", "\n", ")", "\n", "agent_init_params", "[", "i", "]", ".", "update", "(", "\n", "{", "\n", "\"replay_buffer\"", ":", "ReplayBuffer", "(", "\n", "env_model_buffer_size", ",", "\n", "n_agent", ",", "\n", "dim_obs_list", ",", "\n", "dim_out_pol_list", ",", "\n", ")", "\n", "}", "\n", ")", "\n", "", "if", "\"Opp\"", "in", "alg_types", "[", "i", "]", "or", "\"AOR\"", "in", "alg_types", "[", "i", "]", ":", "\n", "                ", "tmp_dim_in_pol_list", "=", "dim_in_pol_list", ".", "copy", "(", ")", "\n", "tmp_dim_in_pol_list", ".", "pop", "(", "i", ")", "\n", "tmp_dim_out_pol_list", "=", "dim_out_pol_list", ".", "copy", "(", ")", "\n", "tmp_dim_out_pol_list", ".", "pop", "(", "i", ")", ",", "\n", "agent_init_params", "[", "i", "]", ".", "update", "(", "\n", "{", "\n", "\"dim_in_opp_pols\"", ":", "tmp_dim_in_pol_list", ",", "\n", "\"dim_out_opp_pols\"", ":", "tmp_dim_out_pol_list", ",", "\n", "}", "\n", ")", "\n", "\n", "", "", "init_dict", "=", "{", "\n", "\"gamma\"", ":", "gamma", ",", "\n", "\"tau\"", ":", "tau", ",", "\n", "\"lr\"", ":", "lr", ",", "\n", "\"rew_scale\"", ":", "rew_scale", ",", "\n", "\"hidden_dim\"", ":", "hidden_dim", ",", "\n", "\"alg_types\"", ":", "alg_types", ",", "\n", "\"agent_init_params\"", ":", "agent_init_params", ",", "\n", "\"discrete_action\"", ":", "discrete_action", ",", "\n", "}", "\n", "\n", "instance", "=", "cls", "(", "**", "init_dict", ")", "\n", "instance", ".", "init_dict", "=", "init_dict", "\n", "if", "env_id", "==", "\"simple_schedule\"", ":", "\n", "            ", "instance", ".", "aggressive_opp_eval", "=", "True", "\n", "", "else", ":", "\n", "            ", "instance", ".", "aggressive_opp_eval", "=", "False", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_opp_model": [[430, 510], ["enumerate", "zip", "isinstance", "enumerate", "opp_pol_loss_list.append", "match_rate_list.append", "opp_pol", "enumerate", "match_rates.append", "opp_pol_losses.append", "torch.square", "logger.add_scalars", "logger.add_scalars", "torch.tensor().sum", "torch.tensor().mean", "torch.argmax", "torch.sqrt", "torch.tensor", "torch.tensor().max", "dict", "NLLLoss", "numpy.square().sum", "torch.tensor().sum", "torch.tensor().mean", "zip", "torch.tensor", "torch.tensor", "torch.log_softmax", "len", "torch.tensor", "sum", "numpy.square", "torch.tensor", "torch.tensor", "sum", "sum", "range", "sum", "raw_opp_act.argmax"], "methods", ["None"], ["", "def", "evaluate_opp_model", "(", "\n", "self", ",", "sample", ":", "Tuple", ",", "logger", ":", "SummaryWriter", "=", "False", ",", "ep_i", ":", "int", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"\n        evaluate opponent model with latest sample\n        \"\"\"", "\n", "obs", ",", "acts", ",", "_", ",", "_", ",", "_", "=", "sample", "\n", "opp_pol_loss_list", "=", "[", "]", "\n", "match_rate_list", "=", "[", "]", "\n", "for", "agent_i", ",", "(", "a", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "agents", ",", "obs", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "a", ",", "AgentOppMd", ")", ":", "\n", "                ", "opp_pol_losses", "=", "[", "]", "\n", "match_rates", "=", "[", "]", "\n", "for", "opp_i", ",", "opp_pol", "in", "enumerate", "(", "a", ".", "opp_policies", ")", ":", "\n", "                    ", "a_i", "=", "opp_i", "if", "opp_i", "<", "a", ".", "index", "else", "opp_i", "+", "1", "\n", "raw_opp_acts", "=", "opp_pol", "(", "obs", "[", "a_i", "]", ")", "\n", "action_shape_list", "=", "a", ".", "all_action_shape_list", "[", "a_i", "]", "\n", "loss", "=", "0", "\n", "match_rate", "=", "0", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "action_shape_list", ")", ":", "\n", "                        ", "real_opp_ind", "=", "torch", ".", "argmax", "(", "\n", "acts", "[", "a_i", "]", "[", "\n", ":", ",", "\n", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "\n", "+", "dim", ",", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "raw_opp_act", "=", "raw_opp_acts", "[", "\n", ":", ",", "\n", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "\n", "+", "dim", ",", "\n", "]", "\n", "loss", "+=", "(", "\n", "NLLLoss", "(", "\n", "torch", ".", "log_softmax", "(", "raw_opp_act", ",", "dim", "=", "1", ")", ",", "\n", "real_opp_ind", ",", "\n", ")", "\n", "/", "a", ".", "dim_out_pol", "\n", ")", "\n", "match_rate", "+=", "(", "\n", "raw_opp_act", ".", "argmax", "(", "dim", "=", "1", ")", "==", "real_opp_ind", "\n", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "*", "(", "\n", "dim", "**", "2", "\n", ")", "# weighted actions", "\n", "", "match_rates", ".", "append", "(", "match_rate", "/", "np", ".", "square", "(", "action_shape_list", ")", ".", "sum", "(", ")", ")", "\n", "opp_pol_losses", ".", "append", "(", "torch", ".", "sqrt", "(", "loss", "/", "len", "(", "action_shape_list", ")", ")", ")", "\n", "", "a", ".", "match_rates", "=", "match_rates", "\n", "a", ".", "norm_eval_opp_pol_err", "=", "(", "torch", ".", "tensor", "(", "match_rates", ")", "+", "1e-10", ")", "/", "(", "\n", "torch", ".", "tensor", "(", "match_rates", ")", ".", "max", "(", ")", "+", "1e-10", "\n", ")", "\n", "if", "self", ".", "aggressive_opp_eval", ":", "\n", "                    ", "a", ".", "norm_eval_opp_pol_err", "=", "torch", ".", "square", "(", "a", ".", "norm_eval_opp_pol_err", ")", "\n", "\n", "", "if", "logger", ":", "\n", "                    ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/evl_losses\"", "%", "(", "agent_i", ")", ",", "\n", "{", "\n", "\"epsilon_phi^-i\"", ":", "torch", ".", "tensor", "(", "opp_pol_losses", ")", ".", "sum", "(", ")", ",", "\n", "\"match_rate_phi^-i\"", ":", "torch", ".", "tensor", "(", "match_rates", ")", ".", "mean", "(", ")", ",", "\n", "}", ",", "\n", "ep_i", ",", "\n", ")", "\n", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/evl_losses\"", "%", "(", "agent_i", ")", ",", "\n", "dict", "(", "\n", "zip", "(", "\n", "[", "\n", "\"match_rate_phi^-i,%i\"", "%", "a_i", "\n", "for", "a_i", "in", "range", "(", "self", ".", "n_agent", "-", "1", ")", "\n", "]", ",", "\n", "match_rates", ",", "\n", ")", "\n", ")", ",", "\n", "ep_i", ",", "\n", ")", "\n", "\n", "", "opp_pol_loss_list", ".", "append", "(", "torch", ".", "tensor", "(", "opp_pol_losses", ")", ".", "sum", "(", ")", ")", "\n", "match_rate_list", ".", "append", "(", "torch", ".", "tensor", "(", "match_rates", ")", ".", "mean", "(", ")", ")", "\n", "", "", "return", "opp_pol_loss_list", ",", "match_rate_list", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.maframework.MA_Controller.evaluate_dynamics_model": [[511, 535], ["torch.cat", "torch.cat", "range", "a.predict", "torch.abs().mean", "eval_loss_list.append", "logger.add_scalars", "torch.abs", "r.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.predict"], ["", "def", "evaluate_dynamics_model", "(", "\n", "self", ",", "sample", ":", "Tuple", ",", "logger", ":", "SummaryWriter", "=", "None", ",", "logger_iter", ":", "int", "=", "0", "\n", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "\"\"\"\n        dynamics model with true environment under opponent models and current policy\n        return:\n            List of eval loss for each agent\n        \"\"\"", "\n", "obses", ",", "acts", ",", "rews", ",", "next_obses", ",", "_", "=", "sample", "\n", "model_in", "=", "torch", ".", "cat", "(", "[", "*", "obses", ",", "*", "acts", "]", ",", "dim", "=", "1", ")", "\n", "model_target", "=", "torch", ".", "cat", "(", "[", "*", "next_obses", ",", "*", "[", "r", ".", "unsqueeze", "(", "1", ")", "for", "r", "in", "rews", "]", "]", ",", "dim", "=", "1", ")", "\n", "eval_loss_list", "=", "[", "]", "\n", "for", "agent_i", "in", "range", "(", "self", ".", "n_agent", ")", ":", "\n", "            ", "a", "=", "self", ".", "agents", "[", "agent_i", "]", "\n", "model_pred", "=", "a", ".", "predict", "(", "model_in", ")", "\n", "loss", "=", "torch", ".", "abs", "(", "model_pred", "-", "model_target", ")", ".", "mean", "(", ")", "\n", "if", "logger", ":", "\n", "                ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/evl_losses\"", "%", "(", "agent_i", ")", ",", "\n", "{", "\"abs_m_error\"", ":", "loss", "}", ",", "\n", "logger_iter", ",", "\n", ")", "\n", "", "eval_loss_list", ".", "append", "(", "loss", ")", "\n", "", "return", "eval_loss_list", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.__init__": [[14, 41], ["len"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "alg_types", ":", "List", "[", "str", "]", ",", "\n", "rew_scale", ":", "float", "=", "1.0", ",", "\n", "gamma", ":", "float", "=", "0.95", ",", "\n", "tau", ":", "float", "=", "0.01", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "hidden_dim", ":", "int", "=", "64", ",", "\n", "discrete_action", ":", "int", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            gamma (float): Discount factor\n            tau (float): Target update rate\n            lr (float): Learning rate for policy and critic\n            hidden_dim (int): Number of hidden dimensions for networks\n            discrete_action (bool): Whether or not to use discrete action space\n        \"\"\"", "\n", "self", ".", "n_agent", "=", "len", "(", "alg_types", ")", "\n", "self", ".", "alg_types", "=", "alg_types", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "discrete_action", "=", "discrete_action", "\n", "self", ".", "rew_sacle", "=", "rew_scale", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "niter", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.scale_noise": [[42, 50], ["a.scale_noise"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.scale_noise"], ["", "def", "scale_noise", "(", "self", ",", "scale", ")", ":", "\n", "        ", "\"\"\"\n        Scale noise for each agent\n        Inputs:\n            scale (float): scale of noise\n        \"\"\"", "\n", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "a", ".", "scale_noise", "(", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.reset_noise": [[51, 54], ["a.reset_noise"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.reset_noise"], ["", "", "def", "reset_noise", "(", "self", ")", ":", "\n", "        ", "for", "a", "in", "self", ".", "agents", ":", "\n", "            ", "a", ".", "reset_noise", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.step": [[55, 66], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "observations", ":", "List", ",", "explore", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment with all agents\n        Inputs:\n            observations: List of observations for each agent\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            actions: List of actions for each agent\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.update": [[67, 82], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "update", "(", "self", ",", "sample", ":", "Tuple", ",", "agent_i", ":", "int", ",", "logger", ":", "SummaryWriter", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Update parameters of agent model based on sample from replay buffer\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            agent_i (int): index of agent to update\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.update_all_targets": [[83, 90], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "update_all_targets", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Update all target networks (called after normal updates have been\n        performed for each agent)\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.prep_training": [[91, 94], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.prep_rollouts": [[95, 98], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save": [[99, 109], ["baseframework.BaseFramework.prep_training", "torch.save", "a.get_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.save", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params"], ["", "def", "save", "(", "self", ",", "model_path", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Save trained parameters of all agents into one file\n        \"\"\"", "\n", "self", ".", "prep_training", "(", "device", "=", "\"cpu\"", ")", "# move parameters to CPU before saving", "\n", "save_dict", "=", "{", "\n", "\"init_dict\"", ":", "self", ".", "init_dict", ",", "\n", "\"agent_params\"", ":", "[", "a", ".", "get_params", "(", ")", "for", "a", "in", "self", ".", "agents", "]", ",", "\n", "}", "\n", "torch", ".", "save", "(", "save_dict", ",", "model_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.init_from_save": [[110, 121], ["torch.load", "cls", "zip", "a.load_params", "torch.device"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "@", "classmethod", "\n", "def", "init_from_save", "(", "cls", ",", "model_path", ":", "str", ")", "->", "\"BaseFramework\"", ":", "\n", "        ", "\"\"\"\n        Instantiate instance of this class from file created by 'save' method\n        \"\"\"", "\n", "save_dict", "=", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "\n", "instance", "=", "cls", "(", "**", "save_dict", "[", "\"init_dict\"", "]", ")", "\n", "instance", ".", "init_dict", "=", "save_dict", "[", "\"init_dict\"", "]", "\n", "for", "a", ",", "params", "in", "zip", "(", "instance", ".", "agents", ",", "save_dict", "[", "\"agent_params\"", "]", ")", ":", "\n", "            ", "a", ".", "load_params", "(", "params", ")", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.algorithms.baseframework.BaseFramework.init_from_env": [[122, 137], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "init_from_env", "(", "\n", "cls", ",", "\n", "env", ",", "\n", "alg", ":", "str", "=", "\"AORPO\"", ",", "\n", "rew_scale", ":", "float", "=", "1.0", ",", "\n", "gamma", ":", "float", "=", "0.95", ",", "\n", "tau", ":", "float", "=", "0.01", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "hidden_dim", ":", "int", "=", "64", ",", "\n", ")", "->", "\"BaseFramework\"", ":", "\n", "        ", "\"\"\"\n        Instantiate instance of this class from multi-agent environment\n        \"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.__init__": [[21, 54], ["numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "discrete_action", ":", "bool", "=", "True", ",", "\n", "action_shape_list", ":", "List", "=", "[", "]", ",", "\n", "dim_in_pol", ":", "int", "=", "None", ",", "\n", "hidden_dim", ":", "int", "=", "None", ",", "\n", "lr", ":", "float", "=", "None", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", ")", ":", "\n", "        ", "self", ".", "exploration", "=", "0.3", "# epsilon for eps-greedy", "\n", "self", ".", "discrete_action", "=", "discrete_action", "\n", "self", ".", "all_action_shape_list", "=", "action_shape_list", "\n", "self", ".", "action_dim", "=", "action_shape_list", "[", "agent_index", "]", "\n", "self", ".", "alg_type", "=", "alg_type", "\n", "self", ".", "n_agent", "=", "n_agent", "\n", "self", ".", "index", "=", "agent_index", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "rew_scale", "=", "rew_scale", "\n", "self", ".", "dim_out_pol", "=", "dim_out_pol", "\n", "self", ".", "dim_in_pol", "=", "dim_in_pol", "\n", "self", ".", "grad_bound", "=", "grad_bound", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "\n", "self", ".", "dev", "=", "\"cpu\"", "\n", "\n", "self", ".", "opp_sample_num", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_agent", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.scale_noise": [[55, 57], ["None"], "methods", ["None"], ["", "def", "scale_noise", "(", "self", ",", "scale", ":", "float", ")", ":", "\n", "        ", "self", ".", "exploration", "=", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.random_step": [[58, 69], ["len", "[].to", "torch.cat().to", "acts.append", "torch.cat", "torch.eye", "torch.eye", "torch.randint", "torch.randint"], "methods", ["None"], ["", "def", "random_step", "(", "self", ",", "obs", ":", "List", ")", ":", "\n", "        ", "n", "=", "obs", ".", "shape", "[", "0", "]", "\n", "if", "len", "(", "self", ".", "action_dim", ")", "==", "1", ":", "\n", "            ", "return", "torch", ".", "eye", "(", "self", ".", "dim_out_pol", ")", "[", "\n", "torch", ".", "randint", "(", "0", ",", "self", ".", "dim_out_pol", ",", "size", "=", "(", "n", ",", ")", ")", "\n", "]", ".", "to", "(", "self", ".", "dev", ")", "\n", "", "else", ":", "\n", "            ", "acts", "=", "[", "]", "\n", "for", "act_dim", "in", "self", ".", "action_dim", ":", "\n", "                ", "acts", ".", "append", "(", "torch", ".", "eye", "(", "act_dim", ")", "[", "torch", ".", "randint", "(", "0", ",", "act_dim", ",", "size", "=", "(", "n", ",", ")", ")", "]", ")", "\n", "", "return", "torch", ".", "cat", "(", "acts", ",", "dim", "=", "1", ")", ".", "to", "(", "self", ".", "dev", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.step": [[70, 82], ["baseagent.BaseAgent.random_step"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.random_step"], ["", "", "@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "obs", ",", "explore", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment for a minibatch of observations\n        Inputs:\n            obs (PyTorch Variable): Observations for this agent\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            action (PyTorch Variable): Actions for this agent\n        \"\"\"", "\n", "# raise NotImplementedError", "\n", "return", "self", ".", "random_step", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.update": [[83, 100], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "update", "(", "self", ",", "sample", ",", "agent_i", ",", "parallel", "=", "False", ",", "logger", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        update the parameters\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            agent_i (int): index of agent to update\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            losses for the logger\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.update_targets": [[101, 104], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "update_targets", "(", "self", ",", "tau", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.prep_training": [[105, 108], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "self", ".", "dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.prep_rollouts": [[109, 112], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "self", ".", "dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.get_params": [[113, 117], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_params", "(", "self", ")", ":", "\n", "# raise NotImplementedError", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.BaseAgent.load_params": [[118, 122], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "# raise NotImplementedError", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentOppMd.update_opponent": [[129, 132], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "update_opponent", "(", "self", ",", "sample", ",", "parallel", "=", "False", ",", "logger", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentOppMd.get_opp_action": [[133, 136], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_opp_action", "(", "self", ",", "opp", ",", "obs", ",", "requires_grad", "=", "False", ",", "return_log_prob", "=", "False", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentCond.step": [[143, 154], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "obses", ":", "List", ",", "explore", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment for a minibatch of observations\n        Inputs:\n            obses (PyTorch Variable): Observations for all agents\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            action (PyTorch Variable): Actions for this agent\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.__init__": [[163, 238], ["baseagent.BaseAgent.__init__", "utils.networks.PtModel", "torch.optim.Adam", "baseagent.AgentMB.dynamics_model.parameters", "sum", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ":", "int", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "dim_in_critic", ":", "int", ",", "\n", "dim_obs_list", ":", "List", "[", "int", "]", ",", "\n", "dim_actions", ":", "List", "[", "int", "]", ",", "\n", "dim_rewards", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "discrete_action", ":", "bool", ",", "\n", "replay_buffer", ":", "ReplayBuffer", ",", "\n", "action_shape_list", ":", "list", "=", "[", "]", ",", "\n", "model_lr", ":", "float", "=", "0.001", ",", "\n", "model_hidden_dim", ":", "int", "=", "32", ",", "\n", "ensemble_size", ":", "int", "=", "7", ",", "\n", "MB_batch_size", ":", "int", "=", "4096", ",", "\n", "grad_bound", ":", "int", "=", "1", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        dim_obs_list (list): the agent's dimensions\n        dim_actions (list): the agent's actions\n        dim_rewards (int): sum of the agent's rewards\n\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dim_out_pol", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "discrete_action", ",", "\n", "action_shape_list", ",", "\n", "dim_in_pol", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "grad_bound", ",", "\n", ")", "\n", "\n", "self", ".", "dynamics_model", "=", "PtModel", "(", "\n", "ensemble_size", ",", "\n", "sum", "(", "dim_obs_list", ")", "+", "sum", "(", "dim_actions", ")", ",", "\n", "(", "sum", "(", "dim_obs_list", ")", "+", "dim_rewards", ")", "*", "2", ",", "\n", "model_hidden_dim", ",", "\n", ")", "\n", "self", ".", "model_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "dynamics_model", ".", "parameters", "(", ")", ",", "lr", "=", "model_lr", "\n", ")", "\n", "\n", "self", ".", "ensemble_size", "=", "ensemble_size", "\n", "self", ".", "dim_out_pol", "=", "dim_out_pol", "\n", "self", ".", "dim_obs_list", "=", "dim_obs_list", "\n", "self", ".", "dim_actions", "=", "dim_actions", "\n", "self", ".", "dim_rewards", "=", "dim_rewards", "\n", "\n", "self", ".", "MB_batch_size", "=", "MB_batch_size", "\n", "\n", "self", ".", "replay_buffer", "=", "replay_buffer", "\n", "\n", "self", ".", "model_device", "=", "\"cpu\"", "\n", "\n", "self", ".", "init_attr", "=", "{", "\n", "\"action_shape_list\"", ":", "self", ".", "all_action_shape_list", ",", "\n", "\"alg_type\"", ":", "self", ".", "alg_type", ",", "\n", "\"n_agent\"", ":", "self", ".", "n_agent", ",", "\n", "\"index\"", ":", "self", ".", "index", ",", "\n", "\"dim_out_pol\"", ":", "self", ".", "dim_out_pol", ",", "\n", "\"ensemble_size\"", ":", "self", ".", "ensemble_size", ",", "\n", "}", "# the subclass's init_attr dict should be updated from it", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.predict": [[240, 264], ["baseagent.AgentMB.dynamics_model", "means.detach.detach.detach", "stds.detach.detach.detach", "numpy.random.randint", "torch.distributions.Normal", "torch.distributions.Normal.rsample", "torch.distributions.Normal.sample"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample"], ["", "def", "predict", "(", "\n", "self", ",", "\n", "model_in", ":", "torch", ".", "Tensor", ",", "\n", "reparameterize", ":", "bool", "=", "False", ",", "\n", "return_normal", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Output:\n            next_state (list): predicted observations to all agents\n        \"\"\"", "\n", "means", ",", "stds", "=", "self", ".", "dynamics_model", "(", "model_in", ")", "\n", "means", "=", "means", ".", "detach", "(", ")", "\n", "stds", "=", "stds", ".", "detach", "(", ")", "\n", "ind", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "ensemble_size", ")", "\n", "mean", ",", "std", "=", "means", "[", "ind", "]", ",", "stds", "[", "ind", "]", "\n", "normal", "=", "Normal", "(", "mean", ",", "std", ")", "\n", "\n", "if", "return_normal", ":", "\n", "            ", "return", "normal", "\n", "\n", "", "if", "reparameterize", ":", "\n", "            ", "return", "normal", ".", "rsample", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "normal", ".", "sample", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.rollout_model": [[265, 367], ["range", "torch.cat", "range", "torch.floor", "torch.zeros", "logger.add_scalars", "range", "acts.insert", "torch.cat", "torch.cat", "baseagent.AgentMB.predict", "baseagent.AgentMB.replay_buffer.push_agent_first", "ts.cpu().detach().numpy", "len", "baseagent.AgentMB.rollout_model.convert"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.predict", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.push_agent_first"], ["", "", "def", "rollout_model", "(", "\n", "self", ",", "\n", "K", ":", "int", ",", "\n", "init_states", ":", "List", ",", "\n", "agents", ":", "List", "[", "BaseAgent", "]", ",", "\n", "logger", ":", "SummaryWriter", "=", "None", ",", "\n", "n_iter", ":", "int", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        return samples from the dynamics model\n        Input:\n            init_states (list): a list of each agent's observation list, each agent has M states from which the rollouts start, torch.tensor\n            rollout_lengths (list): rollout_length for each opponent model\n            agents (list): all the true agents\n        Output:\n            sample (list): actions should be \"per agent\", while others are \"per state\"\n        process should be M-steps -> M-steps\n        \"\"\"", "\n", "\n", "def", "convert", "(", "list_tensor", ":", "List", "[", "torch", ".", "Tensor", "]", ")", ":", "\n", "            ", "return", "[", "ts", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "for", "ts", "in", "list_tensor", "]", "\n", "\n", "", "if", "self", ".", "norm_eval_opp_pol_err", "is", "not", "[", "]", ":", "\n", "            ", "rollout_lengths", "=", "torch", ".", "floor", "(", "K", "*", "self", ".", "norm_eval_opp_pol_err", ")", "\n", "", "else", ":", "\n", "            ", "rollout_lengths", "=", "torch", ".", "zeros", "(", "len", "(", "self", ".", "opp_policies", ")", ")", "\n", "\n", "", "if", "logger", "is", "not", "None", ":", "\n", "            ", "logger", ".", "add_scalars", "(", "\n", "\"agent%i/hyper\"", "%", "(", "self", ".", "index", ")", ",", "\n", "{", "\"min_rollout_len\"", ":", "torch", ".", "min", "(", "rollout_lengths", ".", "detach", "(", ")", ")", "}", ",", "\n", "n_iter", ",", "\n", ")", "\n", "\n", "# opponent sample complexity", "\n", "", "for", "i", "in", "range", "(", "self", ".", "n_agent", ")", ":", "\n", "            ", "if", "not", "i", "==", "self", ".", "index", ":", "\n", "                ", "if", "i", "<", "self", ".", "index", ":", "\n", "                    ", "opp_i", "=", "i", "\n", "", "elif", "i", ">", "self", ".", "index", ":", "\n", "                    ", "opp_i", "=", "i", "-", "1", "\n", "", "self", ".", "opp_sample_num", "[", "opp_i", "]", "+=", "(", "K", "-", "rollout_lengths", "[", "opp_i", "]", ")", "*", "len", "(", "\n", "init_states", "\n", ")", "\n", "\n", "", "", "obs", "=", "init_states", "# agent, M, feature", "\n", "state", "=", "torch", ".", "cat", "(", "obs", ",", "dim", "=", "-", "1", ")", "\n", "\n", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "            ", "acts", "=", "[", "]", "# [num_agents, M]", "\n", "for", "opp_i", "in", "range", "(", "self", ".", "n_agent", "-", "1", ")", ":", "\n", "                ", "agent_i", "=", "opp_i", "if", "opp_i", "<", "self", ".", "index", "else", "opp_i", "+", "1", "\n", "if", "k", "+", "1", "<", "rollout_lengths", "[", "opp_i", "]", ":", "\n", "                    ", "acts", ".", "append", "(", "\n", "self", ".", "get_opp_action", "(", "\n", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "obs", "[", "agent_i", "]", ",", "agent_i", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "# Communication", "\n", "                    ", "acts", ".", "append", "(", "\n", "agents", "[", "agent_i", "]", ".", "step", "(", "\n", "obs", "\n", "if", "isinstance", "(", "agents", "[", "agent_i", "]", ",", "AgentCond", ")", "\n", "else", "obs", "[", "agent_i", "]", ",", "\n", "explore", "=", "True", ",", "\n", ")", "\n", ")", "\n", "", "", "acts", ".", "insert", "(", "\n", "self", ".", "index", ",", "\n", "self", ".", "step", "(", "obs", ",", "explore", "=", "True", ")", "\n", "if", "isinstance", "(", "self", ",", "AgentCond", ")", "\n", "else", "self", ".", "step", "(", "obs", "[", "self", ".", "index", "]", ",", "explore", "=", "True", ")", ",", "\n", ")", "\n", "action", "=", "torch", ".", "cat", "(", "acts", ",", "dim", "=", "-", "1", ")", "\n", "model_in", "=", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "-", "1", ")", "\n", "pred", "=", "self", ".", "predict", "(", "model_in", ")", "\n", "next_obs", "=", "pred", "[", "...", ",", ":", "sum", "(", "self", ".", "dim_obs_list", ")", "]", "\n", "\n", "state", "=", "next_obs", "\n", "next_obs", "=", "[", "\n", "next_obs", "[", "\n", "...", ",", "\n", "sum", "(", "self", ".", "dim_obs_list", "[", ":", "i", "]", ")", ":", "sum", "(", "self", ".", "dim_obs_list", "[", ":", "i", "+", "1", "]", ")", ",", "\n", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "n_agent", ")", "\n", "]", "\n", "\n", "rews", "=", "pred", "[", "\n", "...", ",", "\n", "sum", "(", "self", ".", "dim_obs_list", ")", ":", "sum", "(", "self", ".", "dim_obs_list", ")", "+", "self", ".", "dim_rewards", ",", "\n", "]", "\n", "rews", "=", "[", "rews", "[", "...", ",", "i", "]", "for", "i", "in", "range", "(", "self", ".", "n_agent", ")", "]", "\n", "self", ".", "replay_buffer", ".", "push_agent_first", "(", "\n", "convert", "(", "obs", ")", ",", "\n", "convert", "(", "acts", ")", ",", "\n", "convert", "(", "rews", ")", ",", "\n", "convert", "(", "next_obs", ")", ",", "\n", "np", ".", "zeros", "(", "(", "self", ".", "n_agent", ",", "action", ".", "shape", "[", "0", "]", ")", ")", ",", "\n", "action", ".", "shape", "[", "0", "]", ",", "\n", ")", "# need to discuss the *done*", "\n", "obs", "=", "next_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.update_model": [[368, 435], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "baseagent.AgentMB.dynamics_model.fit_input_stats", "torch.randint", "math.ceil", "range", "range", "r.unsqueeze", "baseagent.AgentMB.dynamics_model.compute_decays", "baseagent.AgentMB.dynamics_model", "torch.exp", "train_losses.mean().mean().sum.mean().mean().sum.mean().mean().sum", "baseagent.AgentMB.model_optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "baseagent.AgentMB.model_optimizer.step", "baseagent.AgentMB.dynamics_model.parameters", "baseagent.AgentMB.dynamics_model.max_logvar.sum", "baseagent.AgentMB.dynamics_model.min_logvar.sum", "train_losses.mean().mean().sum.mean().mean().sum.mean().mean", "min", "train_losses.mean().mean().sum.mean().mean().sum.mean"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.fit_input_stats", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.networks.PtModel.compute_decays", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step"], ["", "", "def", "update_model", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ",", "epochs", ":", "int", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n            (observations, actions)[:dim_obs_list] <- (next_observations)\n            (observations, actions)[dim_obs_list:] <- (rews)\n        Outputs:\n            m_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "_", "=", "sample", "\n", "\n", "state", "=", "torch", ".", "cat", "(", "obs", ",", "dim", "=", "1", ")", "\n", "actions", "=", "torch", ".", "cat", "(", "acs", ",", "dim", "=", "1", ")", "\n", "target_rews", "=", "torch", ".", "cat", "(", "[", "r", ".", "unsqueeze", "(", "1", ")", "for", "r", "in", "rews", "]", ",", "dim", "=", "1", ")", "\n", "target_nobs", "=", "torch", ".", "cat", "(", "next_obs", ",", "dim", "=", "1", ")", "\n", "model_in", "=", "torch", ".", "cat", "(", "[", "state", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "model_target", "=", "torch", ".", "cat", "(", "[", "target_nobs", ",", "target_rews", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# update normalize variables", "\n", "self", ".", "dynamics_model", ".", "fit_input_stats", "(", "model_in", ")", "\n", "\n", "# bootstrapping sampling", "\n", "inds", "=", "torch", ".", "randint", "(", "\n", "0", ",", "high", "=", "model_in", ".", "shape", "[", "0", "]", ",", "size", "=", "(", "self", ".", "ensemble_size", ",", "model_in", ".", "shape", "[", "0", "]", ")", "\n", ")", "\n", "\n", "n_batch", "=", "math", ".", "ceil", "(", "model_in", ".", "shape", "[", "0", "]", "/", "self", ".", "MB_batch_size", ")", "\n", "\n", "total_loss", "=", "0", "\n", "\n", "for", "_", "in", "range", "(", "epochs", ")", ":", "\n", "            ", "for", "batch_i", "in", "range", "(", "n_batch", ")", ":", "\n", "                ", "batch_inds", "=", "inds", "[", "\n", ":", ",", "\n", "batch_i", "\n", "*", "self", ".", "MB_batch_size", ":", "min", "(", "\n", "(", "batch_i", "+", "1", ")", "*", "self", ".", "MB_batch_size", ",", "model_in", ".", "shape", "[", "0", "]", "\n", ")", ",", "\n", "]", "\n", "\n", "loss", "=", "0.01", "*", "(", "\n", "self", ".", "dynamics_model", ".", "max_logvar", ".", "sum", "(", ")", "\n", "-", "self", ".", "dynamics_model", ".", "min_logvar", ".", "sum", "(", ")", "\n", ")", "\n", "loss", "+=", "self", ".", "dynamics_model", ".", "compute_decays", "(", ")", "\n", "batch_in", "=", "model_in", "[", "batch_inds", "]", "\n", "batch_target", "=", "model_target", "[", "batch_inds", "]", "\n", "\n", "means", ",", "log_stds", "=", "self", ".", "dynamics_model", "(", "batch_in", ",", "ret_logvar", "=", "True", ")", "\n", "\n", "inv_stds", "=", "torch", ".", "exp", "(", "-", "log_stds", ")", "# reciprocal", "\n", "\n", "train_losses", "=", "(", "(", "means", "-", "batch_target", ")", "**", "2", ")", "*", "inv_stds", "+", "log_stds", "\n", "train_losses", "=", "(", "\n", "train_losses", ".", "mean", "(", "-", "1", ")", ".", "mean", "(", "-", "1", ")", ".", "sum", "(", ")", "\n", ")", "# mean operation on the ensemble_size dim makes no sense", "\n", "\n", "loss", "+=", "train_losses", "\n", "\n", "self", ".", "model_optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "dynamics_model", ".", "parameters", "(", ")", ",", "MODEL_GRAD_BOUND", "\n", ")", "\n", "self", ".", "model_optimizer", ".", "step", "(", ")", "\n", "\n", "total_loss", "+=", "loss", "\n", "\n", "", "", "return", "{", "\"m_loss\"", ":", "total_loss", "/", "epochs", "/", "n_batch", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.prep_training": [[436, 451], ["baseagent.AgentMB.dynamics_model.train", "baseagent.AgentMB.prep_training.fn"], "methods", ["None"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "self", ".", "dynamics_model", ".", "train", "(", ")", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "model_device", "==", "device", ":", "\n", "            ", "self", ".", "dynamics_model", "=", "fn", "(", "self", ".", "dynamics_model", ")", "\n", "self", ".", "model_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.prep_rollouts": [[452, 467], ["baseagent.AgentMB.dynamics_model.eval", "baseagent.AgentMB.prep_training.fn"], "methods", ["None"], ["", "", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "self", ".", "dynamics_model", ".", "eval", "(", ")", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "model_device", "==", "device", ":", "\n", "            ", "self", ".", "dynamics_model", "=", "fn", "(", "self", ".", "dynamics_model", ")", "\n", "self", ".", "model_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.get_params": [[468, 477], ["ret.update", "baseagent.AgentMB.dynamics_model.state_dict", "baseagent.AgentMB.model_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update"], ["", "", "def", "get_params", "(", "self", ",", "return_dynamics_model", ":", "bool", "=", "False", ")", ":", "\n", "        ", "ret", "=", "{", "\n", "\"init_attr\"", ":", "self", ".", "init_attr", ",", "\n", "}", "\n", "if", "return_dynamics_model", ":", "\n", "            ", "ret", ".", "update", "(", "\n", "{", "\"dynamics_model\"", ":", "self", ".", "dynamics_model", ".", "state_dict", "(", ")", ",", "\"model_optimizer\"", ":", "self", ".", "model_optimizer", ".", "state_dict", "(", ")", "}", "\n", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.baseagent.AgentMB.load_params": [[478, 485], ["params[].items", "baseagent.AgentMB.dynamics_model.load_state_dict", "baseagent.AgentMB.model_optimizer.load_state_dict", "hasattr", "setattr"], "methods", ["None"], ["", "def", "load_params", "(", "self", ",", "params", ",", "load_dynamics_model", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "load_dynamics_model", ":", "\n", "            ", "self", ".", "dynamics_model", ".", "load_state_dict", "(", "params", "[", "\"dynamics_model\"", "]", ")", "\n", "self", ".", "model_optimizer", ".", "load_state_dict", "(", "params", "[", "\"model_optimizer\"", "]", ")", "\n", "", "for", "key", ",", "val", "in", "params", "[", "\"init_attr\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "val", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.__init__": [[32, 96], ["baseagent.BaseAgent.__init__", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.misc.hard_update", "utils.misc.hard_update", "torch.optim.Adam", "torch.optim.Adam", "ddpgagent.DDPGAgent.policy.parameters", "ddpgagent.DDPGAgent.critic.parameters"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.hard_update", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.hard_update"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ":", "int", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "dim_in_critic", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "discrete_action", ":", "bool", ",", "\n", "action_shape_list", ":", "list", "=", "[", "]", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            dim_in_pol (int): number of dimensions for policy input\n            dim_out_pol (int): number of dimensions for policy output\n            dim_in_critic (int): number of dimensions for critic input\n        \"\"\"", "\n", "\n", "BaseAgent", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_out_pol", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "discrete_action", ",", "\n", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", ")", "\n", "self", ".", "dim_in_pol", "=", "dim_in_pol", "\n", "\n", "self", ".", "policy", "=", "MLPNetwork", "(", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", ")", "\n", "self", ".", "critic", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "self", ".", "target_policy", "=", "MLPNetwork", "(", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", ")", "\n", "self", ".", "target_critic", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "hard_update", "(", "self", ".", "target_policy", ",", "self", ".", "policy", ")", "\n", "hard_update", "(", "self", ".", "target_critic", ",", "self", ".", "critic", ")", "\n", "self", ".", "policy_optimizer", "=", "Adam", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "critic_optimizer", "=", "Adam", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "self", ".", "pol_dev", "=", "\"cpu\"", "# device for policies", "\n", "self", ".", "critic_dev", "=", "\"cpu\"", "# device for critics", "\n", "self", ".", "trgt_pol_dev", "=", "\"cpu\"", "# device for target policies", "\n", "self", ".", "trgt_critic_dev", "=", "\"cpu\"", "# device for target critics", "\n", "\n", "self", ".", "init_attr", "=", "{", "\n", "\"index\"", ":", "self", ".", "index", ",", "\n", "\"action_shape_list\"", ":", "self", ".", "all_action_shape_list", ",", "\n", "\"alg_type\"", ":", "self", ".", "alg_type", ",", "\n", "\"n_agent\"", ":", "self", ".", "n_agent", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.step": [[98, 122], ["ddpgagent.DDPGAgent.policy", "utils.misc.get_multi_discrete_action", "utils.misc.gumbel_softmax"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["", "def", "step", "(", "self", ",", "obs", ":", "torch", ".", "Tensor", ",", "explore", ":", "bool", "=", "False", ",", "return_raw", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment for a minibatch of observations\n        Inputs:\n            obs (PyTorch Variable): Observations for this agent\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            action (PyTorch Variable): Actions for this agent\n        \"\"\"", "\n", "action", "=", "self", ".", "policy", "(", "obs", ")", "\n", "if", "return_raw", ":", "\n", "            ", "return", "action", "\n", "", "if", "explore", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "\n", "", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "processfun", "\n", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.get_target_action": [[123, 129], ["ddpgagent.DDPGAgent.target_policy", "utils.misc.get_multi_discrete_action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action"], ["", "def", "get_target_action", "(", "self", ",", "obs", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "action", "=", "self", ".", "target_policy", "(", "obs", ")", "\n", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "onehot_from_logits", "\n", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.update": [[130, 218], ["ddpgagent.DDPGAgent.critic", "MSELoss", "ddpgagent.DDPGAgent.critic_optimizer.zero_grad", "MSELoss.backward", "torch.nn.utils.clip_grad_norm_", "ddpgagent.DDPGAgent.critic_optimizer.step", "ddpgagent.DDPGAgent.policy", "utils.misc.get_multi_discrete_action", "ddpgagent.DDPGAgent.policy_optimizer.zero_grad", "pol_loss.backward", "torch.nn.utils.clip_grad_norm_", "ddpgagent.DDPGAgent.policy_optimizer.step", "enumerate", "torch.cat", "torch.cat", "rews[].view", "torch.cat", "torch.cat", "target_value.detach", "ddpgagent.DDPGAgent.critic.parameters", "enumerate", "torch.cat", "torch.cat", "ddpgagent.DDPGAgent.critic().mean", "ddpgagent.DDPGAgent.policy.parameters", "ddpgagent.DDPGAgent.target_critic", "utils.misc.gumbel_softmax", "all_trgt_acs.append", "all_trgt_acs.append", "ddpgagent.DDPGAgent.get_target_action", "all_pol_acs.append", "all_pol_acs.append", "ddpgagent.DDPGAgent.critic", "ddpgagent.DDPGAgent.get_target_action().detach", "DDPGAgentOppMd.get_opp_action().detach", "DDPGAgentOppMd.get_opp_action().detach", "ddpgagent.DDPGAgent.get_target_action", "ddpgagent.DDPGAgentOppMd.get_opp_action", "ddpgagent.DDPGAgentOppMd.get_opp_action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.get_target_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.get_target_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["", "def", "update", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        Update parameters of agent model based on sample from replay buffer\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            agents (list of agents): instances include *self*\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            pol_loss\n            cf_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "dones", "=", "sample", "\n", "\n", "# update critic", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "all_trgt_acs", "=", "[", "]", "\n", "for", "a_i", ",", "nobs", "in", "enumerate", "(", "next_obs", ")", ":", "\n", "                ", "if", "a_i", "==", "self", ".", "index", ":", "\n", "                    ", "all_trgt_acs", ".", "append", "(", "self", ".", "get_target_action", "(", "nobs", ")", ".", "detach", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "opp_i", "=", "a_i", "if", "a_i", "<", "self", ".", "index", "else", "a_i", "-", "1", "\n", "all_trgt_acs", ".", "append", "(", "\n", "DDPGAgentOppMd", ".", "get_opp_action", "(", "\n", "self", ",", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "nobs", ",", "a_i", "\n", ")", ".", "detach", "(", ")", "\n", ")", "\n", "\n", "", "", "trgt_vf_in", "=", "torch", ".", "cat", "(", "(", "*", "next_obs", ",", "*", "all_trgt_acs", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "# DDPG", "\n", "            ", "trgt_vf_in", "=", "torch", ".", "cat", "(", "\n", "(", "next_obs", "[", "self", ".", "index", "]", ",", "self", ".", "get_target_action", "(", "next_obs", "[", "self", ".", "index", "]", ")", ")", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "\n", "", "target_value", "=", "rews", "[", "self", ".", "index", "]", ".", "view", "(", "-", "1", ",", "1", ")", "+", "self", ".", "gamma", "*", "self", ".", "target_critic", "(", "\n", "trgt_vf_in", "\n", ")", "\n", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "vf_in", "=", "torch", ".", "cat", "(", "(", "*", "obs", ",", "*", "acs", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "# DDPG", "\n", "            ", "vf_in", "=", "torch", ".", "cat", "(", "(", "obs", "[", "self", ".", "index", "]", ",", "acs", "[", "self", ".", "index", "]", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "actual_value", "=", "self", ".", "critic", "(", "vf_in", ")", "\n", "vf_loss", "=", "MSELoss", "(", "actual_value", ",", "target_value", ".", "detach", "(", ")", ")", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "vf_loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "self", ".", "grad_bound", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "# update policy", "\n", "curr_pol_out", "=", "self", ".", "policy", "(", "obs", "[", "self", ".", "index", "]", ")", "\n", "curr_pol_vf_in", "=", "get_multi_discrete_action", "(", "\n", "curr_pol_out", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", ")", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "all_pol_acs", "=", "[", "]", "\n", "for", "a_i", ",", "ob", "in", "enumerate", "(", "obs", ")", ":", "\n", "                ", "if", "a_i", "==", "self", ".", "index", ":", "\n", "                    ", "all_pol_acs", ".", "append", "(", "curr_pol_vf_in", ")", "\n", "", "else", ":", "\n", "                    ", "opp_i", "=", "a_i", "if", "a_i", "<", "self", ".", "index", "else", "a_i", "-", "1", "\n", "all_pol_acs", ".", "append", "(", "\n", "DDPGAgentOppMd", ".", "get_opp_action", "(", "\n", "self", ",", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "ob", ",", "a_i", "\n", ")", ".", "detach", "(", ")", "\n", ")", "\n", "", "", "vf_in", "=", "torch", ".", "cat", "(", "(", "*", "obs", ",", "*", "all_pol_acs", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "# DDPG", "\n", "            ", "vf_in", "=", "torch", ".", "cat", "(", "(", "obs", "[", "self", ".", "index", "]", ",", "curr_pol_vf_in", ")", ",", "dim", "=", "1", ")", "\n", "", "pol_loss", "=", "-", "self", ".", "critic", "(", "vf_in", ")", ".", "mean", "(", ")", "\n", "pol_loss", "+=", "(", "curr_pol_out", "**", "2", ")", ".", "mean", "(", ")", "*", "1e-3", "# regularizer", "\n", "self", ".", "policy_optimizer", ".", "zero_grad", "(", ")", "\n", "pol_loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "self", ".", "grad_bound", ")", "\n", "self", ".", "policy_optimizer", ".", "step", "(", ")", "\n", "return", "{", "\n", "\"cf_loss\"", ":", "vf_loss", ",", "\n", "\"pol_loss\"", ":", "pol_loss", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.update_targets": [[220, 223], ["utils.misc.soft_update", "utils.misc.soft_update"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.soft_update", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.soft_update"], ["", "def", "update_targets", "(", "self", ",", "tau", ":", "float", ")", ":", "\n", "        ", "soft_update", "(", "self", ".", "target_critic", ",", "self", ".", "critic", ",", "tau", ")", "\n", "soft_update", "(", "self", ".", "target_policy", ",", "self", ".", "policy", ",", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.prep_training": [[224, 251], ["ddpgagent.DDPGAgent.policy.train", "ddpgagent.DDPGAgent.critic.train", "ddpgagent.DDPGAgent.target_policy.train", "ddpgagent.DDPGAgent.target_critic.train", "ddpgagent.DDPGAgent.prep_training.fn"], "methods", ["None"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "self", ".", "policy", ".", "train", "(", ")", "\n", "self", ".", "critic", ".", "train", "(", ")", "\n", "self", ".", "target_policy", ".", "train", "(", ")", "\n", "self", ".", "target_critic", ".", "train", "(", ")", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "pol_dev", "==", "device", ":", "\n", "            ", "self", ".", "policy", "=", "fn", "(", "self", ".", "policy", ")", "\n", "self", ".", "pol_dev", "=", "device", "\n", "", "if", "not", "self", ".", "critic_dev", "==", "device", ":", "\n", "            ", "self", ".", "critic", "=", "fn", "(", "self", ".", "critic", ")", "\n", "self", ".", "critic_dev", "=", "device", "\n", "", "if", "not", "self", ".", "trgt_pol_dev", "==", "device", ":", "\n", "            ", "self", ".", "target_policy", "=", "fn", "(", "self", ".", "target_policy", ")", "\n", "self", ".", "trgt_pol_dev", "=", "device", "\n", "", "if", "not", "self", ".", "trgt_critic_dev", "==", "device", ":", "\n", "            ", "self", ".", "target_critic", "=", "fn", "(", "self", ".", "target_critic", ")", "\n", "self", ".", "trgt_critic_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.prep_rollouts": [[252, 268], ["ddpgagent.DDPGAgent.policy.eval", "ddpgagent.DDPGAgent.prep_training.fn"], "methods", ["None"], ["", "", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "self", ".", "policy", ".", "eval", "(", ")", "\n", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "pol_dev", "==", "device", ":", "\n", "            ", "self", ".", "policy", "=", "fn", "(", "self", ".", "policy", ")", "\n", "self", ".", "pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.get_params": [[269, 278], ["ddpgagent.DDPGAgent.policy.state_dict", "ddpgagent.DDPGAgent.critic.state_dict", "ddpgagent.DDPGAgent.target_policy.state_dict", "ddpgagent.DDPGAgent.target_critic.state_dict", "ddpgagent.DDPGAgent.policy_optimizer.state_dict", "ddpgagent.DDPGAgent.critic_optimizer.state_dict"], "methods", ["None"], ["", "", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "\"policy\"", ":", "self", ".", "policy", ".", "state_dict", "(", ")", ",", "\n", "\"critic\"", ":", "self", ".", "critic", ".", "state_dict", "(", ")", ",", "\n", "\"target_policy\"", ":", "self", ".", "target_policy", ".", "state_dict", "(", ")", ",", "\n", "\"target_critic\"", ":", "self", ".", "target_critic", ".", "state_dict", "(", ")", ",", "\n", "\"policy_optimizer\"", ":", "self", ".", "policy_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"critic_optimizer\"", ":", "self", ".", "critic_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"init_attr\"", ":", "self", ".", "init_attr", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgent.load_params": [[280, 290], ["ddpgagent.DDPGAgent.policy.load_state_dict", "ddpgagent.DDPGAgent.critic.load_state_dict", "ddpgagent.DDPGAgent.target_policy.load_state_dict", "ddpgagent.DDPGAgent.target_critic.load_state_dict", "ddpgagent.DDPGAgent.policy_optimizer.load_state_dict", "ddpgagent.DDPGAgent.critic_optimizer.load_state_dict", "params[].items", "hasattr", "setattr"], "methods", ["None"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "policy", ".", "load_state_dict", "(", "params", "[", "\"policy\"", "]", ")", "\n", "self", ".", "critic", ".", "load_state_dict", "(", "params", "[", "\"critic\"", "]", ")", "\n", "self", ".", "target_policy", ".", "load_state_dict", "(", "params", "[", "\"target_policy\"", "]", ")", "\n", "self", ".", "target_critic", ".", "load_state_dict", "(", "params", "[", "\"target_critic\"", "]", ")", "\n", "self", ".", "policy_optimizer", ".", "load_state_dict", "(", "params", "[", "\"policy_optimizer\"", "]", ")", "\n", "self", ".", "critic_optimizer", ".", "load_state_dict", "(", "params", "[", "\"critic_optimizer\"", "]", ")", "\n", "for", "key", ",", "val", "in", "params", "[", "\"init_attr\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.__init__": [[299, 359], ["ddpgagent.DDPGAgent.__init__", "zip", "ddpgagent.DDPGAgentOppMd.opp_policies.append", "torch.optim.Adam", "utils.networks.MLPNetwork", "opp_policy.parameters"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ":", "int", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "dim_in_critic", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "dim_in_opp_pols", ":", "List", "[", "int", "]", ",", "\n", "dim_out_opp_pols", ":", "List", "[", "int", "]", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "discrete_action", ":", "bool", ",", "\n", "action_shape_list", ":", "list", "=", "[", "]", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", "opp_lr", ":", "float", "=", "0.001", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            dim_in_pol (int): number of dimensions for policy input\n            dim_out_pol (int): number of dimensions for policy output\n            dim_in_critic (int): number of dimensions for critic input\n            agent_index (int): the index of the agent\n            dim_in_opp_pols (list of int): numbers of dimensions for opponents' policy input\n            dim_out_opp_pols (list of int): numbers of dimensions for opponents' policy output\n        \"\"\"", "\n", "super", "(", "DDPGAgentOppMd", ",", "self", ")", ".", "__init__", "(", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", "lr", "=", "lr", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", ")", "\n", "\n", "self", ".", "opp_lr", "=", "opp_lr", "\n", "self", ".", "opp_policies", "=", "[", "]", "\n", "for", "num_in_opp_pol", ",", "num_out_opp_pol", "in", "zip", "(", "dim_in_opp_pols", ",", "dim_out_opp_pols", ")", ":", "\n", "            ", "self", ".", "opp_policies", ".", "append", "(", "\n", "MLPNetwork", "(", "\n", "num_in_opp_pol", ",", "\n", "num_out_opp_pol", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "opp_policy_optimizers", "=", "[", "\n", "Adam", "(", "opp_policy", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "opp_lr", ")", "\n", "for", "opp_policy", "in", "self", ".", "opp_policies", "\n", "]", "\n", "\n", "self", ".", "opp_pol_dev", "=", "\"cpu\"", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.get_opp_action": [[360, 398], ["opp", "utils.misc.get_multi_discrete_action", "utils.misc.get_multi_discrete_action", "utils.misc.gumbel_softmax"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["", "def", "get_opp_action", "(", "\n", "self", ",", "\n", "opp", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "obs", ":", "torch", ".", "Tensor", ",", "\n", "agent_ind", ":", "int", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "return_log_prob", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            requires_grad (bool):\n                for discrete action, true means gumbel_softmax with *hard=True*, otherwise means onehot_from_logits.\n        \"\"\"", "\n", "if", "requires_grad", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "\n", "", "raw_action", "=", "opp", "(", "obs", ")", "\n", "if", "return_log_prob", ":", "\n", "\n", "            ", "action", ",", "log_prob", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "\n", "self", ".", "all_action_shape_list", "[", "agent_ind", "]", ",", "\n", "processfun", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "self", ".", "all_action_shape_list", "[", "agent_ind", "]", ",", "processfun", "\n", ")", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "return", "action", ",", "log_prob", "\n", "", "else", ":", "\n", "            ", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.update_opponent": [[399, 449], ["enumerate", "zip", "opp_pl", "utils.misc.get_multi_discrete_action", "enumerate", "opp_pl_optimizer.zero_grad", "opp_pol_loss.backward", "torch.nn.utils.clip_grad_norm_", "opp_pl_optimizer.step", "opp_pol_losses.append", "torch.mean", "torch.log_softmax", "torch.argmax", "NLLLoss().mean", "opp_log_pi.mean", "opp_pl.parameters", "torch.tensor", "utils.misc.gumbel_softmax", "NLLLoss", "sum", "sum", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["", "", "def", "update_opponent", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        update the parameters of opponent models\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            opp_pol_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "dones", "=", "sample", "\n", "# update opponent models", "\n", "opp_pol_losses", "=", "[", "]", "\n", "\n", "for", "opp_i", ",", "(", "opp_pl", ",", "opp_pl_optimizer", ")", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "opp_policies", ",", "self", ".", "opp_policy_optimizers", ")", "\n", ")", ":", "\n", "            ", "agent_i", "=", "opp_i", "if", "opp_i", "<", "self", ".", "index", "else", "opp_i", "+", "1", "\n", "action", "=", "opp_pl", "(", "obs", "[", "agent_i", "]", ")", "\n", "opp_pl_out", ",", "opp_log_pi", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "\n", "self", ".", "all_action_shape_list", "[", "agent_i", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "opp_pol_loss", "=", "0", "\n", "action_shape_list", "=", "self", ".", "all_action_shape_list", "[", "agent_i", "]", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "action_shape_list", ")", ":", "\n", "                ", "action_", "=", "action", "[", "\n", ":", ",", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "+", "dim", "\n", "]", "\n", "opp_pl_out", "=", "torch", ".", "log_softmax", "(", "action_", ",", "dim", "=", "1", ")", "\n", "actual_opp_pl", "=", "acs", "[", "agent_i", "]", "[", "\n", ":", ",", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "i", "]", ")", "+", "dim", "\n", "]", "\n", "acutal_opp_pl_ind", "=", "torch", ".", "argmax", "(", "actual_opp_pl", ",", "dim", "=", "1", ")", "# shape: [R, N]", "\n", "opp_pol_loss", "+=", "NLLLoss", "(", "opp_pl_out", ",", "acutal_opp_pl_ind", ")", ".", "mean", "(", ")", "\n", "", "opp_pol_loss", "+=", "opp_log_pi", ".", "mean", "(", ")", "*", "0.1", "\n", "opp_pl_optimizer", ".", "zero_grad", "(", ")", "\n", "opp_pol_loss", ".", "backward", "(", ")", "\n", "grad", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "opp_pl", ".", "parameters", "(", ")", ",", "OPPMD_GRAD_BOUND", ")", "\n", "opp_pl_optimizer", ".", "step", "(", ")", "\n", "opp_pol_losses", ".", "append", "(", "opp_pol_loss", ")", "\n", "", "return", "{", "\"opp_pol_loss\"", ":", "torch", ".", "mean", "(", "torch", ".", "tensor", "(", "opp_pol_losses", ")", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.prep_training": [[450, 468], ["ddpgagent.DDPGAgent.prep_training", "opp_pol.train", "range", "x.cuda", "x.cpu", "len", "ddpgagent.DDPGAgentOppMd.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "prep_training", "(", "device", ")", "\n", "for", "opp_pol", "in", "self", ".", "opp_policies", ":", "\n", "            ", "opp_pol", ".", "train", "(", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "opp_pol_dev", "==", "device", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "                ", "self", ".", "opp_policies", "[", "i", "]", "=", "fn", "(", "self", ".", "opp_policies", "[", "i", "]", ")", "\n", "", "self", ".", "opp_pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.prep_rollouts": [[469, 489], ["ddpgagent.DDPGAgent.prep_rollouts", "opp_pol.eval", "range", "x.cuda", "x.cpu", "len", "ddpgagent.DDPGAgentOppMd.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "prep_rollouts", "(", "device", ")", "\n", "for", "(", "\n", "opp_pol", "\n", ")", "in", "self", ".", "opp_policies", ":", "# opponent policies are also need in the rollouts", "\n", "            ", "opp_pol", ".", "eval", "(", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "opp_pol_dev", "==", "device", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "                ", "self", ".", "opp_policies", "[", "i", "]", "=", "fn", "(", "self", ".", "opp_policies", "[", "i", "]", ")", "\n", "", "self", ".", "opp_pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.get_params": [[490, 502], ["ddpgagent.DDPGAgent.get_params", "ddpgagent.DDPGAgent.update", "opp_pl.state_dict", "opp_pl_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update"], ["", "", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "get_params", "(", ")", "\n", "ret", ".", "update", "(", "\n", "{", "\n", "\"opp_policies\"", ":", "[", "opp_pl", ".", "state_dict", "(", ")", "for", "opp_pl", "in", "self", ".", "opp_policies", "]", ",", "\n", "\"opp_policy_optimizers\"", ":", "[", "\n", "opp_pl_optimizer", ".", "state_dict", "(", ")", "\n", "for", "opp_pl_optimizer", "in", "self", ".", "opp_policy_optimizers", "\n", "]", ",", "\n", "}", "\n", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMd.load_params": [[503, 509], ["ddpgagent.DDPGAgent.load_params", "range", "len", "ddpgagent.DDPGAgentOppMd.opp_policies[].load_state_dict", "ddpgagent.DDPGAgentOppMd.opp_policy_optimizers[].load_state_dict"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "load_params", "(", "params", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "            ", "self", ".", "opp_policies", "[", "i", "]", ".", "load_state_dict", "(", "params", "[", "\"opp_policies\"", "]", "[", "i", "]", ")", "\n", "self", ".", "opp_policy_optimizers", "[", "i", "]", ".", "load_state_dict", "(", "\n", "params", "[", "\"opp_policy_optimizers\"", "]", "[", "i", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.step": [[519, 551], ["obs.copy", "obs.copy.pop", "torch.cat", "ddpgagent.DDPGAgentOppMdCond.policy", "utils.misc.get_multi_discrete_action", "ddpgagent.DDPGAgentOppMdCond.get_opp_action", "enumerate", "utils.misc.gumbel_softmax", "zip"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["def", "step", "(", "self", ",", "obs", ":", "torch", ".", "Tensor", ",", "explore", ":", "bool", "=", "False", ",", "return_raw", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment for a minibatch of observations\n        Inputs:\n            obses (PyTorch Variable): Observations for all agents\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            action (PyTorch Variable): Actions for this agent\n        \"\"\"", "\n", "agent_obs", "=", "obs", "[", "self", ".", "index", "]", "\n", "obs_", "=", "obs", ".", "copy", "(", ")", "\n", "obs_", ".", "pop", "(", "self", ".", "index", ")", "\n", "opp_acts", "=", "[", "\n", "self", ".", "get_opp_action", "(", "opp_pl", ",", "ob", ",", "i", "if", "i", "<", "self", ".", "index", "else", "i", "+", "1", ")", "\n", "for", "i", ",", "(", "opp_pl", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "opp_policies", ",", "obs_", ")", ")", "\n", "]", "\n", "\n", "pol_in", "=", "torch", ".", "cat", "(", "[", "agent_obs", ",", "*", "opp_acts", "]", ",", "dim", "=", "1", ")", "\n", "action", "=", "self", ".", "policy", "(", "pol_in", ")", "\n", "if", "return_raw", ":", "\n", "            ", "return", "action", "\n", "", "if", "explore", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "processfun", "\n", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.get_target_action": [[552, 570], ["obs.copy", "obs.copy.pop", "torch.cat", "ddpgagent.DDPGAgentOppMdCond.target_policy", "utils.misc.get_multi_discrete_action", "ddpgagent.DDPGAgentOppMdCond.get_opp_action", "enumerate", "zip"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["", "def", "get_target_action", "(", "self", ",", "obs", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        obs (list): observations of all agents\n        \"\"\"", "\n", "agent_obs", "=", "obs", "[", "self", ".", "index", "]", "\n", "obs_", "=", "obs", ".", "copy", "(", ")", "\n", "obs_", ".", "pop", "(", "self", ".", "index", ")", "\n", "opp_acts", "=", "[", "\n", "self", ".", "get_opp_action", "(", "opp_pl", ",", "ob", ",", "i", "if", "i", "<", "self", ".", "index", "else", "i", "+", "1", ")", "\n", "for", "i", ",", "(", "opp_pl", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "opp_policies", ",", "obs_", ")", ")", "\n", "]", "\n", "pol_in", "=", "torch", ".", "cat", "(", "[", "agent_obs", ",", "*", "opp_acts", "]", ",", "dim", "=", "1", ")", "\n", "action", "=", "self", ".", "target_policy", "(", "pol_in", ")", "\n", "\n", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "onehot_from_logits", "\n", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.update": [[571, 647], ["ddpgagent.DDPGAgentOppMdCond.opp_policies.copy", "ddpgagent.DDPGAgentOppMdCond.insert", "enumerate", "torch.cat", "torch.cat", "ddpgagent.DDPGAgentOppMdCond.critic", "MSELoss", "ddpgagent.DDPGAgentOppMdCond.critic_optimizer.zero_grad", "MSELoss.backward", "torch.nn.utils.clip_grad_norm_", "ddpgagent.DDPGAgentOppMdCond.critic_optimizer.step", "obs.copy", "obs.copy.pop", "torch.cat", "ddpgagent.DDPGAgentOppMdCond.policy", "utils.misc.get_multi_discrete_action", "enumerate", "torch.cat", "ddpgagent.DDPGAgentOppMdCond.policy_optimizer.zero_grad", "pol_loss.backward", "torch.nn.utils.clip_grad_norm_", "ddpgagent.DDPGAgentOppMdCond.policy_optimizer.step", "zip", "rews[].view", "target_value.detach", "ddpgagent.DDPGAgentOppMdCond.critic.parameters", "ddpgagent.DDPGAgentOppMdCond.get_opp_action", "zip", "ddpgagent.DDPGAgentOppMdCond.critic().mean", "ddpgagent.DDPGAgentOppMdCond.policy.parameters", "all_trgt_acs.append", "all_trgt_acs.append", "ddpgagent.DDPGAgentOppMdCond.target_critic", "enumerate", "utils.misc.gumbel_softmax", "all_pol_acs.append", "all_pol_acs.append", "ddpgagent.DDPGAgentOppMdCond.get_target_action", "ddpgagent.DDPGAgentOppMdCond.get_opp_action", "zip", "ddpgagent.DDPGAgentOppMdCond.get_opp_action", "ddpgagent.DDPGAgentOppMdCond.critic"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.get_target_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["", "def", "update", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        Update parameters of agent model based on sample from replay buffer\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            pol_loss\n            cf_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "dones", "=", "sample", "\n", "\n", "# update critic", "\n", "policies", "=", "self", ".", "opp_policies", ".", "copy", "(", ")", "\n", "policies", ".", "insert", "(", "self", ".", "index", ",", "self", ".", "policy", ")", "\n", "all_trgt_acs", "=", "[", "]", "\n", "for", "i", ",", "(", "a", ",", "nobs", ")", "in", "enumerate", "(", "zip", "(", "policies", ",", "next_obs", ")", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "index", ":", "\n", "                ", "all_trgt_acs", ".", "append", "(", "self", ".", "get_target_action", "(", "next_obs", ")", ")", "\n", "", "else", ":", "\n", "                ", "all_trgt_acs", ".", "append", "(", "self", ".", "get_opp_action", "(", "a", ",", "nobs", ",", "i", ")", ")", "\n", "\n", "", "", "trgt_vf_in", "=", "torch", ".", "cat", "(", "(", "*", "next_obs", ",", "*", "all_trgt_acs", ")", ",", "dim", "=", "1", ")", "\n", "\n", "target_value", "=", "rews", "[", "self", ".", "index", "]", ".", "view", "(", "-", "1", ",", "1", ")", "+", "self", ".", "gamma", "*", "self", ".", "target_critic", "(", "\n", "trgt_vf_in", "\n", ")", "\n", "\n", "vf_in", "=", "torch", ".", "cat", "(", "(", "*", "obs", ",", "*", "acs", ")", ",", "dim", "=", "1", ")", "\n", "\n", "actual_value", "=", "self", ".", "critic", "(", "vf_in", ")", "\n", "vf_loss", "=", "MSELoss", "(", "actual_value", ",", "target_value", ".", "detach", "(", ")", ")", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "vf_loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "self", ".", "grad_bound", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "# update policy", "\n", "agent_obs", "=", "obs", "[", "self", ".", "index", "]", "\n", "obs_", "=", "obs", ".", "copy", "(", ")", "\n", "obs_", ".", "pop", "(", "self", ".", "index", ")", "\n", "opp_acts", "=", "[", "\n", "self", ".", "get_opp_action", "(", "opp_pl", ",", "ob", ",", "i", "if", "i", "<", "self", ".", "index", "else", "i", "+", "1", ")", "\n", "for", "i", ",", "(", "opp_pl", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "opp_policies", ",", "obs_", ")", ")", "\n", "]", "\n", "pol_in", "=", "torch", ".", "cat", "(", "[", "agent_obs", ",", "*", "opp_acts", "]", ",", "dim", "=", "1", ")", "\n", "curr_pol_out", "=", "self", ".", "policy", "(", "pol_in", ")", "\n", "curr_pol_vf_in", "=", "get_multi_discrete_action", "(", "\n", "curr_pol_out", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", ")", "\n", "all_pol_acs", "=", "[", "]", "\n", "for", "i", ",", "(", "a", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "policies", ",", "obs", ")", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "index", ":", "\n", "                ", "all_pol_acs", ".", "append", "(", "curr_pol_vf_in", ")", "\n", "", "else", ":", "\n", "                ", "all_pol_acs", ".", "append", "(", "self", ".", "get_opp_action", "(", "a", ",", "ob", ",", "i", ")", ")", "\n", "", "", "vf_in", "=", "torch", ".", "cat", "(", "(", "*", "obs", ",", "*", "all_pol_acs", ")", ",", "dim", "=", "1", ")", "\n", "\n", "pol_loss", "=", "-", "self", ".", "critic", "(", "vf_in", ")", ".", "mean", "(", ")", "\n", "pol_loss", "+=", "(", "curr_pol_out", "**", "2", ")", ".", "mean", "(", ")", "*", "1e-3", "# regularizer", "\n", "self", ".", "policy_optimizer", ".", "zero_grad", "(", ")", "\n", "pol_loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "self", ".", "grad_bound", ")", "\n", "self", ".", "policy_optimizer", ".", "step", "(", ")", "\n", "return", "{", "\n", "\"cf_loss\"", ":", "vf_loss", ",", "\n", "\"pol_loss\"", ":", "pol_loss", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCond.prep_rollouts": [[649, 669], ["ddpgagent.DDPGAgentOppMd.prep_rollouts", "opp_pol.eval", "range", "x.cuda", "x.cpu", "len", "ddpgagent.DDPGAgentOppMdCond.prep_rollouts.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", ")", ".", "prep_rollouts", "(", "device", ")", "\n", "for", "(", "\n", "opp_pol", "\n", ")", "in", "self", ".", "opp_policies", ":", "# opponent policies are also need in the rollouts", "\n", "            ", "opp_pol", ".", "eval", "(", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "opp_pol_dev", "==", "device", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "                ", "self", ".", "opp_policies", "[", "i", "]", "=", "fn", "(", "self", ".", "opp_policies", "[", "i", "]", ")", "\n", "", "self", ".", "opp_pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCondMB.__init__": [[672, 747], ["baseagent.AgentMB.__init__", "ddpgagent.DDPGAgentOppMd.__init__", "init_attr.update"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "dim_obs_list", ",", "\n", "dim_actions", ",", "\n", "dim_rewards", ",", "\n", "agent_index", ",", "\n", "dim_in_opp_pols", ",", "\n", "dim_out_opp_pols", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "discrete_action", ",", "\n", "replay_buffer", ",", "\n", "model_lr", "=", "0.001", ",", "\n", "model_hidden_dim", "=", "32", ",", "\n", "ensemble_size", "=", "7", ",", "\n", "action_shape_list", "=", "[", "]", ",", "\n", "opp_lr", "=", "0.001", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", "MB_batch_size", ":", "int", "=", "4096", ",", "\n", ")", ":", "\n", "        ", "AgentMB", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "dim_obs_list", ",", "\n", "dim_actions", ",", "\n", "dim_rewards", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "discrete_action", ",", "\n", "replay_buffer", ",", "\n", "model_lr", "=", "model_lr", ",", "\n", "model_hidden_dim", "=", "model_hidden_dim", ",", "\n", "ensemble_size", "=", "ensemble_size", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", "MB_batch_size", "=", "MB_batch_size", ",", "\n", ")", "\n", "\n", "init_attr", "=", "self", ".", "init_attr", "\n", "\n", "DDPGAgentOppMdCond", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "agent_index", ",", "\n", "dim_in_opp_pols", ",", "\n", "dim_out_opp_pols", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", "lr", "=", "lr", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "opp_lr", "=", "opp_lr", ",", "\n", ")", "\n", "\n", "init_attr", ".", "update", "(", "self", ".", "init_attr", ")", "\n", "self", ".", "init_attr", "=", "init_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCondMB.get_params": [[748, 752], ["ddpgagent.DDPGAgentOppMd.get_params", "ddpgagent.DDPGAgentOppMd.get_params", "baseagent.AgentMB.get_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params"], ["", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "ret", "=", "DDPGAgentOppMdCond", ".", "get_params", "(", "self", ")", "\n", "ret", ".", "update", "(", "AgentMB", ".", "get_params", "(", "self", ")", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCondMB.load_params": [[753, 756], ["ddpgagent.DDPGAgentOppMd.load_params", "baseagent.AgentMB.load_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "DDPGAgentOppMdCond", ".", "load_params", "(", "self", ",", "params", ")", "\n", "AgentMB", ".", "load_params", "(", "self", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCondMB.prep_rollouts": [[757, 760], ["ddpgagent.DDPGAgentOppMdCond.prep_rollouts", "baseagent.AgentMB.prep_rollouts"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "DDPGAgentOppMdCond", ".", "prep_rollouts", "(", "self", ",", "device", ")", "\n", "AgentMB", ".", "prep_rollouts", "(", "self", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.ddpgagent.DDPGAgentOppMdCondMB.prep_training": [[761, 764], ["ddpgagent.DDPGAgentOppMd.prep_training", "baseagent.AgentMB.prep_training"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "DDPGAgentOppMdCond", ".", "prep_training", "(", "self", ",", "device", ")", "\n", "AgentMB", ".", "prep_training", "(", "self", ",", "device", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.__init__": [[29, 107], ["baseagent.BaseAgent.__init__", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.networks.MLPNetwork", "utils.misc.hard_update", "utils.misc.hard_update", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.optim.Adam", "torch.optim.Adam", "sacagent.SACAgent.policy.parameters", "sacagent.SACAgent.critic1.parameters", "sacagent.SACAgent.critic2.parameters", "numpy.prod().item", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.hard_update", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.hard_update"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ":", "int", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "dim_in_critic", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "discrete_action", ":", "bool", ",", "\n", "auto_target_entropy", ":", "bool", "=", "True", ",", "\n", "target_entropy", ":", "int", "=", "None", ",", "\n", "reparameterize", ":", "bool", "=", "True", ",", "\n", "action_shape_list", ":", "list", "=", "[", "]", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            dim_in_pol (int): number of dimensions for policy input\n            dim_out_pol (int): number of dimensions for policy output\n            dim_in_critic (int): number of dimensions for critic input\n            auto_target_entropy(bool): whether to use auto target entropy\n            target_entropy(float): the target entropy used\n        \"\"\"", "\n", "BaseAgent", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_out_pol", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "discrete_action", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", ")", "\n", "self", ".", "dim_in_pol", "=", "dim_in_pol", "\n", "self", ".", "policy", "=", "MLPNetwork", "(", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", ")", "\n", "self", ".", "critic1", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "self", ".", "critic2", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "self", ".", "target_critic1", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "self", ".", "target_critic2", "=", "MLPNetwork", "(", "dim_in_critic", ",", "1", ",", "hidden_dim", "=", "hidden_dim", ")", "\n", "hard_update", "(", "self", ".", "target_critic1", ",", "self", ".", "critic1", ")", "\n", "hard_update", "(", "self", ".", "target_critic2", ",", "self", ".", "critic2", ")", "\n", "\n", "self", ".", "policy_optimizer", "=", "Adam", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "critic1_optimizer", "=", "Adam", "(", "self", ".", "critic1", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "critic2_optimizer", "=", "Adam", "(", "self", ".", "critic2", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "auto_target_entropy", "=", "auto_target_entropy", "\n", "self", ".", "reparameterize", "=", "reparameterize", "\n", "self", ".", "target_entropy", "=", "(", "\n", "-", "np", ".", "prod", "(", "dim_out_pol", ")", ".", "item", "(", ")", "if", "target_entropy", "is", "None", "else", "target_entropy", "\n", ")", "\n", "\n", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "alpha_optimizer", "=", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "lr", ")", "\n", "\n", "self", ".", "policy_dev", "=", "\"cpu\"", "# device for policies", "\n", "self", ".", "critic_dev", "=", "\"cpu\"", "# device for critics", "\n", "self", ".", "trgt_critic_dev", "=", "\"cpu\"", "# device for target critics", "\n", "self", ".", "trgt_value_dev", "=", "\"cpu\"", "# device for target value", "\n", "\n", "self", ".", "init_attr", "=", "{", "\n", "\"action_shape_list\"", ":", "self", ".", "all_action_shape_list", ",", "\n", "\"alg_type\"", ":", "self", ".", "alg_type", ",", "\n", "\"n_agent\"", ":", "self", ".", "n_agent", ",", "\n", "\"auto_target_entropy\"", ":", "self", ".", "auto_target_entropy", ",", "\n", "\"reparameterize\"", ":", "self", ".", "reparameterize", ",", "\n", "\"target_entropy\"", ":", "self", ".", "target_entropy", ",", "\n", "\"log_alpha\"", ":", "self", ".", "log_alpha", ",", "\n", "\"index\"", ":", "self", ".", "index", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.step": [[109, 146], ["sacagent.SACAgent.policy", "utils.misc.gumbel_softmax", "utils.misc.get_multi_discrete_action", "utils.misc.get_multi_discrete_action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action"], ["", "def", "step", "(", "\n", "self", ",", "\n", "obs", ":", "torch", ".", "Tensor", ",", "\n", "explore", ":", "bool", "=", "False", ",", "\n", "return_log_prob", ":", "bool", "=", "False", ",", "\n", "return_raw", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "explore", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "\n", "", "if", "self", ".", "discrete_action", ":", "\n", "            ", "action", "=", "self", ".", "policy", "(", "obs", ")", "\n", "if", "return_raw", ":", "\n", "                ", "return", "action", "\n", "", "if", "return_log_prob", ":", "\n", "                ", "action", ",", "log_prob", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "processfun", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "processfun", ",", "\n", "return_log_prob", "=", "False", ",", "\n", ")", "\n", "", "", "if", "return_log_prob", ":", "\n", "            ", "return", "action", ",", "log_prob", "\n", "", "else", ":", "\n", "            ", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.update": [[147, 286], ["sacagent.SACAgent.policy", "utils.misc.get_multi_discrete_action", "sacagent.SACAgent.critic1", "sacagent.SACAgent.critic2", "torch.min", "torch.min", "torch.min", "torch.min", "sacagent.SACAgent.policy_optimizer.zero_grad", "policy_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sacagent.SACAgent.policy_optimizer.step", "sacagent.SACAgent.critic1", "sacagent.SACAgent.critic2", "sacagent.SACAgent.step", "sacagent.SACAgent.critic1_optimizer.zero_grad", "sacagent.SACAgent.critic2_optimizer.zero_grad", "MSELoss", "MSELoss", "MSELoss.backward", "MSELoss.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "sacagent.SACAgent.critic1_optimizer.step", "sacagent.SACAgent.critic2_optimizer.step", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgent.alpha_optimizer.zero_grad", "alpha_loss.backward", "sacagent.SACAgent.alpha_optimizer.step", "sacagent.SACAgent.log_alpha.exp().detach", "sacagent.SACAgent.policy.parameters", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.min", "torch.min", "torch.min", "torch.min", "target.detach", "target.detach", "sacagent.SACAgent.critic1.parameters", "sacagent.SACAgent.critic2.parameters", "torch.min.mean", "torch.min.mean", "utils.misc.gumbel_softmax", "sacagent.SACAgent.target_critic1", "sacagent.SACAgent.target_critic2", "next_obs_log_pi.detach", "rews[].view", "all_pol_acs.append", "all_pol_acs.append", "sacagent.SACAgent.log_alpha.exp", "next_obs_actions.append", "next_obs_actions.append", "next_obs_action.detach", "SACAgentOppMd.get_opp_action().detach", "next_obs_action.detach", "sacagent.SACAgent.get_opp_action().detach", "sacagent.SACAgentOppMd.get_opp_action", "sacagent.SACAgent.get_opp_action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["", "", "def", "update", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        update the parameters\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            pol_loss\n            vf_loss\n            cf1_loss\n            cf2_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "dones", "=", "sample", "\n", "\n", "# update policy", "\n", "raw_action", "=", "self", ".", "policy", "(", "obs", "[", "self", ".", "index", "]", ")", "\n", "action", ",", "log_pi", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "all_pol_acs", "=", "[", "]", "\n", "for", "a_i", ",", "n_ob", "in", "enumerate", "(", "obs", ")", ":", "\n", "                ", "if", "a_i", "==", "self", ".", "index", ":", "\n", "                    ", "all_pol_acs", ".", "append", "(", "action", ")", "\n", "", "else", ":", "\n", "                    ", "opp_i", "=", "a_i", "if", "a_i", "<", "self", ".", "index", "else", "a_i", "-", "1", "\n", "all_pol_acs", ".", "append", "(", "\n", "SACAgentOppMd", ".", "get_opp_action", "(", "\n", "self", ",", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "n_ob", ",", "a_i", "\n", ")", ".", "detach", "(", ")", "\n", ")", "\n", "# Mutli-agent Q (s, \\vec{a})", "\n", "", "", "critic_in", "=", "torch", ".", "cat", "(", "[", "*", "obs", ",", "*", "all_pol_acs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "# SAC", "\n", "            ", "critic_in", "=", "torch", ".", "cat", "(", "[", "obs", "[", "self", ".", "index", "]", ",", "action", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "critic1_output", "=", "self", ".", "critic1", "(", "critic_in", ")", "\n", "critic2_output", "=", "self", ".", "critic2", "(", "critic_in", ")", "\n", "min_critic_output", "=", "torch", ".", "min", "(", "critic1_output", ",", "critic2_output", ")", "\n", "\n", "# update alpha", "\n", "if", "self", ".", "auto_target_entropy", ":", "\n", "            ", "alpha_loss", "=", "-", "(", "\n", "self", ".", "log_alpha", "*", "(", "(", "log_pi", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", ")", "\n", ")", ".", "mean", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "step", "(", ")", "\n", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "1", "\n", "\n", "# update policy", "\n", "", "if", "self", ".", "reparameterize", ":", "\n", "            ", "policy_kl", "=", "(", "alpha", "*", "log_pi", "-", "min_critic_output", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "# it seems useless", "\n", "            ", "raise", "NotImplementedError", "\n", "", "policy_loss", "=", "policy_kl", "+", "(", "raw_action", "**", "2", ")", ".", "mean", "(", ")", "*", "1e-3", "# regularizer", "\n", "\n", "self", ".", "policy_optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "grad", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "policy", ".", "parameters", "(", ")", ",", "POLICY_GRAD_BOUND", "\n", ")", "\n", "self", ".", "policy_optimizer", ".", "step", "(", ")", "\n", "\n", "# update critic", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "critic_in", "=", "torch", ".", "cat", "(", "[", "*", "obs", ",", "*", "acs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "critic_in", "=", "torch", ".", "cat", "(", "[", "obs", "[", "self", ".", "index", "]", ",", "acs", "[", "self", ".", "index", "]", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "critic1_output", "=", "self", ".", "critic1", "(", "critic_in", ")", "\n", "critic2_output", "=", "self", ".", "critic2", "(", "critic_in", ")", "\n", "\n", "next_obs_action", ",", "next_obs_log_pi", "=", "self", ".", "step", "(", "\n", "next_obs", "[", "self", ".", "index", "]", ",", "explore", "=", "False", ",", "return_log_prob", "=", "True", "\n", ")", "\n", "if", "\"MA\"", "in", "self", ".", "alg_type", "or", "\"AOR\"", "in", "self", ".", "alg_type", ":", "\n", "            ", "next_obs_actions", "=", "[", "]", "\n", "for", "a_i", ",", "n_ob", "in", "enumerate", "(", "next_obs", ")", ":", "\n", "                ", "if", "a_i", "==", "self", ".", "index", ":", "\n", "                    ", "next_obs_actions", ".", "append", "(", "next_obs_action", ".", "detach", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "opp_i", "=", "a_i", "if", "a_i", "<", "self", ".", "index", "else", "a_i", "-", "1", "\n", "next_obs_actions", ".", "append", "(", "\n", "self", ".", "get_opp_action", "(", "\n", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "n_ob", ",", "a_i", "\n", ")", ".", "detach", "(", ")", "\n", ")", "\n", "", "", "target_critic_in", "=", "torch", ".", "cat", "(", "[", "*", "next_obs", ",", "*", "next_obs_actions", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "target_critic_in", "=", "torch", ".", "cat", "(", "\n", "[", "next_obs", "[", "self", ".", "index", "]", ",", "next_obs_action", ".", "detach", "(", ")", "]", ",", "dim", "=", "1", "\n", ")", "\n", "", "min_target_critic_output", "=", "(", "\n", "torch", ".", "min", "(", "\n", "self", ".", "target_critic1", "(", "target_critic_in", ")", ",", "\n", "self", ".", "target_critic2", "(", "target_critic_in", ")", ",", "\n", ")", "\n", "-", "alpha", "*", "next_obs_log_pi", ".", "detach", "(", ")", "\n", ")", "\n", "\n", "target", "=", "(", "\n", "self", ".", "rew_scale", "*", "rews", "[", "self", ".", "index", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "+", "self", ".", "gamma", "*", "min_target_critic_output", "\n", ")", "\n", "\n", "self", ".", "critic1_optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "critic2_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "critic1_loss", "=", "MSELoss", "(", "critic1_output", ",", "target", ".", "detach", "(", ")", ")", "\n", "critic2_loss", "=", "MSELoss", "(", "critic2_output", ",", "target", ".", "detach", "(", ")", ")", "\n", "\n", "critic1_loss", ".", "backward", "(", ")", "\n", "critic2_loss", ".", "backward", "(", ")", "\n", "grad1", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic1", ".", "parameters", "(", ")", ",", "CRITIC_GRAD_BOUND", ")", "\n", "grad2", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic2", ".", "parameters", "(", ")", ",", "CRITIC_GRAD_BOUND", ")", "\n", "self", ".", "critic1_optimizer", ".", "step", "(", ")", "\n", "self", ".", "critic2_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "{", "\n", "\"alpha\"", ":", "alpha", ",", "\n", "\"log_pol_pi\"", ":", "(", "alpha", "*", "log_pi", ")", ".", "mean", "(", ")", ",", "\n", "\"pol_critic\"", ":", "min_critic_output", ".", "mean", "(", ")", ",", "\n", "\"pol_loss\"", ":", "policy_loss", ",", "\n", "\"cf1_loss\"", ":", "critic1_loss", ",", "\n", "\"cf2_loss\"", ":", "critic2_loss", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.update_targets": [[288, 291], ["utils.misc.soft_update", "utils.misc.soft_update"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.soft_update", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.soft_update"], ["", "def", "update_targets", "(", "self", ",", "tau", ":", "float", ")", ":", "\n", "        ", "soft_update", "(", "self", ".", "target_critic1", ",", "self", ".", "critic1", ",", "tau", ")", "\n", "soft_update", "(", "self", ".", "target_critic2", ",", "self", ".", "critic2", ",", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.prep_training": [[292, 325], ["sacagent.SACAgent.policy.train", "sacagent.SACAgent.critic1.train", "sacagent.SACAgent.critic2.train", "sacagent.SACAgent.target_critic1.train", "sacagent.SACAgent.target_critic2.train", "utils.misc.apply_with_grad", "sacagent.SACAgent.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.apply_with_grad"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "self", ".", "policy", ".", "train", "(", ")", "\n", "\n", "self", ".", "critic1", ".", "train", "(", ")", "\n", "self", ".", "critic2", ".", "train", "(", ")", "\n", "\n", "self", ".", "target_critic1", ".", "train", "(", ")", "\n", "self", ".", "target_critic2", ".", "train", "(", ")", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "policy_dev", "==", "device", ":", "\n", "            ", "self", ".", "policy", "=", "fn", "(", "self", ".", "policy", ")", "\n", "self", ".", "policy_dev", "=", "device", "\n", "\n", "", "if", "not", "self", ".", "critic_dev", "==", "device", ":", "\n", "            ", "self", ".", "critic1", "=", "fn", "(", "self", ".", "critic1", ")", "\n", "self", ".", "critic2", "=", "fn", "(", "self", ".", "critic2", ")", "\n", "self", ".", "critic_dev", "=", "device", "\n", "\n", "", "if", "not", "self", ".", "trgt_critic_dev", "==", "device", ":", "\n", "            ", "self", ".", "target_critic1", "=", "fn", "(", "self", ".", "target_critic1", ")", "\n", "self", ".", "target_critic2", "=", "fn", "(", "self", ".", "target_critic2", ")", "\n", "self", ".", "trgt_critic_dev", "=", "device", "\n", "\n", "", "self", ".", "log_alpha", "=", "apply_with_grad", "(", "self", ".", "log_alpha", ",", "fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.prep_rollouts": [[326, 344], ["sacagent.SACAgent.policy.eval", "utils.misc.apply_with_grad", "sacagent.SACAgent.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.apply_with_grad"], ["", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "self", ".", "policy", ".", "eval", "(", ")", "\n", "\n", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "policy_dev", "==", "device", ":", "\n", "            ", "self", ".", "policy", "=", "fn", "(", "self", ".", "policy", ")", "\n", "self", ".", "policy_dev", "=", "device", "\n", "\n", "", "self", ".", "log_alpha", "=", "apply_with_grad", "(", "self", ".", "log_alpha", ",", "fn", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.get_params": [[345, 357], ["sacagent.SACAgent.policy.state_dict", "sacagent.SACAgent.critic1.state_dict", "sacagent.SACAgent.critic2.state_dict", "sacagent.SACAgent.target_critic1.state_dict", "sacagent.SACAgent.target_critic2.state_dict", "sacagent.SACAgent.policy_optimizer.state_dict", "sacagent.SACAgent.critic1_optimizer.state_dict", "sacagent.SACAgent.critic2_optimizer.state_dict", "sacagent.SACAgent.alpha_optimizer.state_dict"], "methods", ["None"], ["", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "\"policy\"", ":", "self", ".", "policy", ".", "state_dict", "(", ")", ",", "\n", "\"critic1\"", ":", "self", ".", "critic1", ".", "state_dict", "(", ")", ",", "\n", "\"critic2\"", ":", "self", ".", "critic2", ".", "state_dict", "(", ")", ",", "\n", "\"target_critic1\"", ":", "self", ".", "target_critic1", ".", "state_dict", "(", ")", ",", "\n", "\"target_critic2\"", ":", "self", ".", "target_critic2", ".", "state_dict", "(", ")", ",", "\n", "\"policy_optimizer\"", ":", "self", ".", "policy_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"critic1_optimizer\"", ":", "self", ".", "critic1_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"critic2_optimizer\"", ":", "self", ".", "critic2_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"alpha_optimizer\"", ":", "self", ".", "alpha_optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"init_attr\"", ":", "self", ".", "init_attr", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgent.load_params": [[359, 372], ["sacagent.SACAgent.policy.load_state_dict", "sacagent.SACAgent.critic1.load_state_dict", "sacagent.SACAgent.critic2.load_state_dict", "sacagent.SACAgent.target_critic1.load_state_dict", "sacagent.SACAgent.target_critic2.load_state_dict", "sacagent.SACAgent.policy_optimizer.load_state_dict", "sacagent.SACAgent.critic1_optimizer.load_state_dict", "sacagent.SACAgent.critic2_optimizer.load_state_dict", "sacagent.SACAgent.alpha_optimizer.load_state_dict", "params[].items", "hasattr", "setattr"], "methods", ["None"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "policy", ".", "load_state_dict", "(", "params", "[", "\"policy\"", "]", ")", "\n", "self", ".", "critic1", ".", "load_state_dict", "(", "params", "[", "\"critic1\"", "]", ")", "\n", "self", ".", "critic2", ".", "load_state_dict", "(", "params", "[", "\"critic2\"", "]", ")", "\n", "self", ".", "target_critic1", ".", "load_state_dict", "(", "params", "[", "\"target_critic1\"", "]", ")", "\n", "self", ".", "target_critic2", ".", "load_state_dict", "(", "params", "[", "\"target_critic2\"", "]", ")", "\n", "self", ".", "policy_optimizer", ".", "load_state_dict", "(", "params", "[", "\"policy_optimizer\"", "]", ")", "\n", "self", ".", "critic1_optimizer", ".", "load_state_dict", "(", "params", "[", "\"critic1_optimizer\"", "]", ")", "\n", "self", ".", "critic2_optimizer", ".", "load_state_dict", "(", "params", "[", "\"critic2_optimizer\"", "]", ")", "\n", "self", ".", "alpha_optimizer", ".", "load_state_dict", "(", "params", "[", "\"alpha_optimizer\"", "]", ")", "\n", "for", "key", ",", "val", "in", "params", "[", "\"init_attr\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.__init__": [[375, 442], ["sacagent.SACAgent.__init__", "zip", "sacagent.SACAgentOppMd.opp_policies.append", "torch.optim.Adam", "torch.optim.Adam", "utils.networks.MLPNetwork", "opp_policy.parameters"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ":", "int", ",", "\n", "dim_out_pol", ":", "int", ",", "\n", "dim_in_critic", ":", "int", ",", "\n", "agent_index", ":", "int", ",", "\n", "dim_in_opp_pols", ":", "List", "[", "int", "]", ",", "\n", "dim_out_opp_pols", ":", "List", "[", "int", "]", ",", "\n", "n_agent", ":", "int", ",", "\n", "alg_type", ":", "str", ",", "\n", "rew_scale", ":", "float", ",", "\n", "gamma", ":", "float", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "lr", ":", "float", ",", "\n", "discrete_action", ":", "bool", ",", "\n", "auto_target_entropy", ":", "bool", "=", "True", ",", "\n", "target_entropy", ":", "int", "=", "None", ",", "\n", "reparameterize", ":", "bool", "=", "True", ",", "\n", "action_shape_list", ":", "list", "=", "[", "]", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", "opp_lr", ":", "float", "=", "0.001", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            dim_in_pol (int): number of dimensions for policy input\n            dim_out_pol (int): number of dimensions for policy output\n            dim_in_critic (int): number of dimensions for critic input\n            auto_target_entropy(bool): whether to use auto target entropy\n            target_entropy(float): the target entropy used\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "discrete_action", ",", "\n", "auto_target_entropy", ",", "\n", "target_entropy", ",", "\n", "reparameterize", ",", "\n", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", ")", "\n", "self", ".", "opp_lr", "=", "opp_lr", "\n", "\n", "self", ".", "opp_policies", "=", "[", "]", "\n", "for", "num_in_opp_pol", ",", "num_out_opp_pol", "in", "zip", "(", "dim_in_opp_pols", ",", "dim_out_opp_pols", ")", ":", "\n", "            ", "self", ".", "opp_policies", ".", "append", "(", "\n", "MLPNetwork", "(", "\n", "num_in_opp_pol", ",", "\n", "num_out_opp_pol", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "opp_policy_optimizers", "=", "[", "\n", "Adam", "(", "opp_policy", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "opp_lr", ")", "\n", "for", "opp_policy", "in", "self", ".", "opp_policies", "\n", "]", "\n", "\n", "self", ".", "norm_evl_opp_pol_err", "=", "[", "]", "\n", "self", ".", "opp_pol_dev", "=", "\"cpu\"", "# device for opponent policies", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action": [[443, 482], ["opp", "utils.misc.get_multi_discrete_action", "utils.misc.get_multi_discrete_action", "utils.misc.gumbel_softmax"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["", "def", "get_opp_action", "(", "\n", "self", ",", "\n", "opp", ":", "nn", ".", "Module", ",", "\n", "obs", ":", "Tensor", ",", "\n", "agent_ind", ":", "int", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "return_log_prob", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Inputs:\n            requires_grad (bool):\n            for discrete action, true means gumbel_softmax with *hard=True*, otherwise means onehot_from_logits.\n            for continous action, true means *reparameterize=True*, otherwise means *reparameterize=False*\n        \"\"\"", "\n", "if", "requires_grad", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "\n", "", "raw_action", "=", "opp", "(", "obs", ")", "\n", "if", "return_log_prob", ":", "\n", "\n", "            ", "action", ",", "log_prob", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "\n", "self", ".", "all_action_shape_list", "[", "agent_ind", "]", ",", "\n", "processfun", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "self", ".", "all_action_shape_list", "[", "agent_ind", "]", ",", "processfun", "\n", ")", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "return", "action", ",", "log_prob", "\n", "", "else", ":", "\n", "            ", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.update_opponent": [[483, 544], ["enumerate", "zip", "opp_pl", "utils.misc.get_multi_discrete_action", "enumerate", "opp_pl_optimizer.zero_grad", "opp_pol_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "opp_pl_optimizer.step", "opp_pol_losses.append", "match_rates.append", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "NLLLoss().mean", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "opp_log_pi.mean", "opp_pl.parameters", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.misc.gumbel_softmax", "NLLLoss", "sum", "sum", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax"], ["", "", "def", "update_opponent", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        update the parameters of opponent models\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            opp_pol_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "_", ",", "_", ",", "_", "=", "sample", "\n", "# update opponent models", "\n", "opp_pol_losses", "=", "[", "]", "\n", "match_rates", "=", "[", "]", "\n", "for", "opp_i", ",", "(", "opp_pl", ",", "opp_pl_optimizer", ")", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "opp_policies", ",", "self", ".", "opp_policy_optimizers", ")", "\n", ")", ":", "\n", "            ", "agent_i", "=", "opp_i", "if", "opp_i", "<", "self", ".", "index", "else", "opp_i", "+", "1", "\n", "action", "=", "opp_pl", "(", "obs", "[", "agent_i", "]", ")", "\n", "opp_pl_out", ",", "opp_log_pi", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "\n", "self", ".", "all_action_shape_list", "[", "agent_i", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "\n", "opp_pol_loss", "=", "0", "\n", "action_shape_list", "=", "self", ".", "all_action_shape_list", "[", "agent_i", "]", "\n", "for", "opp_i", ",", "dim", "in", "enumerate", "(", "action_shape_list", ")", ":", "\n", "                ", "action_", "=", "action", "[", "\n", ":", ",", "\n", "sum", "(", "action_shape_list", "[", ":", "opp_i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "opp_i", "]", ")", "\n", "+", "dim", ",", "\n", "]", "\n", "opp_pl_out", "=", "torch", ".", "log_softmax", "(", "action_", ",", "dim", "=", "1", ")", "\n", "actual_opp_pl", "=", "acs", "[", "agent_i", "]", "[", "\n", ":", ",", "\n", "sum", "(", "action_shape_list", "[", ":", "opp_i", "]", ")", ":", "sum", "(", "action_shape_list", "[", ":", "opp_i", "]", ")", "\n", "+", "dim", ",", "\n", "]", "\n", "acutal_opp_pl_ind", "=", "torch", ".", "argmax", "(", "actual_opp_pl", ",", "dim", "=", "1", ")", "\n", "opp_pol_loss", "+=", "NLLLoss", "(", "opp_pl_out", ",", "acutal_opp_pl_ind", ")", ".", "mean", "(", ")", "\n", "opp_pl_ind", "=", "torch", ".", "argmax", "(", "action_", ",", "dim", "=", "-", "1", ")", "\n", "match_rate", "=", "(", "opp_pl_ind", "==", "acutal_opp_pl_ind", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "", "opp_pol_loss", "+=", "opp_log_pi", ".", "mean", "(", ")", "*", "0.1", "\n", "opp_pl_optimizer", ".", "zero_grad", "(", ")", "\n", "opp_pol_loss", ".", "backward", "(", ")", "\n", "grad", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "opp_pl", ".", "parameters", "(", ")", ",", "OPPMD_GRAD_BOUND", ")", "\n", "\n", "opp_pl_optimizer", ".", "step", "(", ")", "\n", "opp_pol_losses", ".", "append", "(", "opp_pol_loss", ")", "\n", "match_rates", ".", "append", "(", "match_rate", ")", "\n", "", "return", "{", "\n", "\"opp_pol_loss\"", ":", "torch", ".", "mean", "(", "torch", ".", "tensor", "(", "opp_pol_losses", ")", ")", ",", "\n", "\"opp_match_rate\"", ":", "torch", ".", "mean", "(", "torch", ".", "tensor", "(", "match_rates", ")", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.prep_training": [[546, 564], ["sacagent.SACAgent.prep_training", "opp_pol.train", "range", "x.cuda", "x.cpu", "len", "sacagent.SACAgentOppMd.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training"], ["", "def", "prep_training", "(", "self", ",", "device", ":", "str", "=", "\"cuda\"", ")", ":", "\n", "        ", "SACAgent", ".", "prep_training", "(", "self", ",", "device", ")", "\n", "for", "opp_pol", "in", "self", ".", "opp_policies", ":", "\n", "            ", "opp_pol", ".", "train", "(", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "opp_pol_dev", "==", "device", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "                ", "self", ".", "opp_policies", "[", "i", "]", "=", "fn", "(", "self", ".", "opp_policies", "[", "i", "]", ")", "\n", "", "self", ".", "opp_pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.prep_rollouts": [[565, 583], ["sacagent.SACAgent.prep_rollouts", "opp_pol.eval", "range", "x.cuda", "x.cpu", "len", "sacagent.SACAgentOppMd.prep_training.fn"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "SACAgent", ".", "prep_rollouts", "(", "self", ",", "device", ")", "\n", "for", "opp_pol", "in", "self", ".", "opp_policies", ":", "\n", "            ", "opp_pol", ".", "eval", "(", ")", "\n", "", "if", "device", "==", "\"cuda\"", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cuda", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "            ", "def", "fn", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "cpu", "(", ")", "\n", "\n", "", "", "if", "not", "self", ".", "opp_pol_dev", "==", "device", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "                ", "self", ".", "opp_policies", "[", "i", "]", "=", "fn", "(", "self", ".", "opp_policies", "[", "i", "]", ")", "\n", "", "self", ".", "opp_pol_dev", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_params": [[584, 596], ["sacagent.SACAgent.get_params", "sacagent.SACAgent.get_params", "opp_pl.state_dict", "opp_pl_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params"], ["", "", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "ret", "=", "SACAgent", ".", "get_params", "(", "self", ")", "\n", "ret", ".", "update", "(", "\n", "{", "\n", "\"opp_policies\"", ":", "[", "opp_pl", ".", "state_dict", "(", ")", "for", "opp_pl", "in", "self", ".", "opp_policies", "]", ",", "\n", "\"opp_policy_optimizers\"", ":", "[", "\n", "opp_pl_optimizer", ".", "state_dict", "(", ")", "\n", "for", "opp_pl_optimizer", "in", "self", ".", "opp_policy_optimizers", "\n", "]", ",", "\n", "}", "\n", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.load_params": [[597, 603], ["sacagent.SACAgent.load_params", "range", "len", "sacagent.SACAgentOppMd.opp_policies[].load_state_dict", "sacagent.SACAgentOppMd.opp_policy_optimizers[].load_state_dict"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "SACAgent", ".", "load_params", "(", "self", ",", "params", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "opp_policies", ")", ")", ":", "\n", "            ", "self", ".", "opp_policies", "[", "i", "]", ".", "load_state_dict", "(", "params", "[", "\"opp_policies\"", "]", "[", "i", "]", ")", "\n", "self", ".", "opp_policy_optimizers", "[", "i", "]", ".", "load_state_dict", "(", "\n", "params", "[", "\"opp_policy_optimizers\"", "]", "[", "i", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.step": [[607, 656], ["obs.copy", "obs.copy.pop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgentOppMdCond.policy", "sacagent.SACAgentOppMdCond.get_opp_action().detach", "utils.misc.get_multi_discrete_action", "utils.misc.get_multi_discrete_action", "enumerate", "utils.misc.gumbel_softmax", "sacagent.SACAgentOppMdCond.get_opp_action", "zip"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["    ", "def", "step", "(", "\n", "self", ",", "\n", "obs", ":", "Tensor", ",", "\n", "explore", ":", "bool", "=", "False", ",", "\n", "return_log_prob", ":", "bool", "=", "False", ",", "\n", "return_raw", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Take a step forward in environment for a minibatch of observations\n        Inputs:\n            obs (list): Observations for all agents\n            explore (boolean): Whether or not to add exploration noise\n        Outputs:\n            action (PyTorch Variable): Actions for this agent\n        \"\"\"", "\n", "agent_obs", "=", "obs", "[", "self", ".", "index", "]", "\n", "obs_", "=", "obs", ".", "copy", "(", ")", "\n", "obs_", ".", "pop", "(", "self", ".", "index", ")", "\n", "opp_acts", "=", "[", "\n", "self", ".", "get_opp_action", "(", "opp_pl", ",", "ob", ",", "i", "if", "i", "<", "self", ".", "index", "else", "i", "+", "1", ")", ".", "detach", "(", ")", "\n", "for", "i", ",", "(", "opp_pl", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "opp_policies", ",", "obs_", ")", ")", "\n", "]", "\n", "pol_in", "=", "torch", ".", "cat", "(", "[", "agent_obs", ",", "*", "opp_acts", "]", ",", "dim", "=", "1", ")", "\n", "\n", "if", "explore", ":", "\n", "\n", "            ", "def", "processfun", "(", "x", ",", "return_log_prob", ")", ":", "\n", "                ", "return", "gumbel_softmax", "(", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "processfun", "=", "onehot_from_logits", "\n", "", "action", "=", "self", ".", "policy", "(", "pol_in", ")", "\n", "if", "return_raw", ":", "\n", "            ", "return", "action", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "action", ",", "log_prob", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "processfun", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "get_multi_discrete_action", "(", "\n", "action", ",", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "processfun", "\n", ")", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "return", "action", ",", "log_prob", "\n", "", "else", ":", "\n", "            ", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update": [[657, 777], ["obs.copy", "obs.copy.pop", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgentOppMdCond.policy", "utils.misc.get_multi_discrete_action", "opp_acts.insert", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgentOppMdCond.critic1", "sacagent.SACAgentOppMdCond.critic2", "torch.min", "torch.min", "torch.min", "torch.min", "sacagent.SACAgentOppMdCond.policy_optimizer.zero_grad", "policy_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sacagent.SACAgentOppMdCond.policy_optimizer.step", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgentOppMdCond.critic1", "sacagent.SACAgentOppMdCond.critic2", "sacagent.SACAgentOppMdCond.step", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sacagent.SACAgentOppMdCond.critic1_optimizer.zero_grad", "sacagent.SACAgentOppMdCond.critic2_optimizer.zero_grad", "MSELoss", "MSELoss", "MSELoss.backward", "MSELoss.backward", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "sacagent.SACAgentOppMdCond.critic1_optimizer.step", "sacagent.SACAgentOppMdCond.critic2_optimizer.step", "sacagent.SACAgentOppMdCond.get_opp_action().detach", "sacagent.SACAgentOppMdCond.alpha_optimizer.zero_grad", "alpha_loss.backward", "sacagent.SACAgentOppMdCond.alpha_optimizer.step", "sacagent.SACAgentOppMdCond.log_alpha.exp().detach", "sacagent.SACAgentOppMdCond.policy.parameters", "torch.min", "torch.min", "torch.min", "torch.min", "target.detach", "target.detach", "sacagent.SACAgentOppMdCond.critic1.parameters", "sacagent.SACAgentOppMdCond.critic2.parameters", "torch.min.mean", "torch.min.mean", "enumerate", "utils.misc.gumbel_softmax", "next_obs_actions.append", "next_obs_actions.append", "sacagent.SACAgentOppMdCond.target_critic1", "sacagent.SACAgentOppMdCond.target_critic2", "rews[].view", "sacagent.SACAgentOppMdCond.get_opp_action", "zip", "sacagent.SACAgentOppMdCond.log_alpha.exp", "next_obs_action.detach", "sacagent.SACAgentOppMdCond.get_opp_action().detach", "sacagent.SACAgentOppMdCond.get_opp_action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.get_multi_discrete_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.utils.misc.gumbel_softmax", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMd.get_opp_action"], ["", "", "def", "update", "(", "self", ",", "sample", ":", "Tuple", "[", "List", "]", ")", ":", "\n", "        ", "\"\"\"\n        update the parameters\n        Inputs:\n            sample: tuple of (observations, actions, rewards, next\n                    observations, and episode end masks) sampled randomly from\n                    the replay buffer. Each is a list with entries\n                    corresponding to each agent\n            parallel (bool): If true, will average gradients across threads\n            logger (SummaryWriter from Tensorboard-Pytorch):\n                If passed in, important quantities will be logged\n        Outputs:\n            pol_loss\n            cf1_loss\n            cf2_loss\n        \"\"\"", "\n", "obs", ",", "acs", ",", "rews", ",", "next_obs", ",", "dones", "=", "sample", "\n", "# update policy", "\n", "agent_obs", "=", "obs", "[", "self", ".", "index", "]", "\n", "obs_", "=", "obs", ".", "copy", "(", ")", "\n", "obs_", ".", "pop", "(", "self", ".", "index", ")", "\n", "opp_acts", "=", "[", "\n", "self", ".", "get_opp_action", "(", "opp_pl", ",", "ob", ",", "i", "if", "i", "<", "self", ".", "index", "else", "i", "+", "1", ")", ".", "detach", "(", ")", "\n", "for", "i", ",", "(", "opp_pl", ",", "ob", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "opp_policies", ",", "obs_", ")", ")", "\n", "]", "\n", "pol_in", "=", "torch", ".", "cat", "(", "[", "agent_obs", ",", "*", "opp_acts", "]", ",", "dim", "=", "1", ")", "\n", "raw_action", "=", "self", ".", "policy", "(", "pol_in", ")", "\n", "action", ",", "log_pi", "=", "get_multi_discrete_action", "(", "\n", "raw_action", ",", "\n", "self", ".", "all_action_shape_list", "[", "self", ".", "index", "]", ",", "\n", "lambda", "x", ",", "return_log_prob", ":", "gumbel_softmax", "(", "\n", "x", ",", "hard", "=", "True", ",", "return_log_prob", "=", "return_log_prob", "\n", ")", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", "\n", "\n", "opp_acts", ".", "insert", "(", "self", ".", "index", ",", "action", ")", "\n", "all_pol_acs", "=", "opp_acts", "\n", "critic_in", "=", "torch", ".", "cat", "(", "[", "*", "obs", ",", "*", "all_pol_acs", "]", ",", "dim", "=", "1", ")", "\n", "critic1_output", "=", "self", ".", "critic1", "(", "critic_in", ")", "\n", "critic2_output", "=", "self", ".", "critic2", "(", "critic_in", ")", "\n", "min_critic_output", "=", "torch", ".", "min", "(", "critic1_output", ",", "critic2_output", ")", "\n", "\n", "# update alpha", "\n", "if", "self", ".", "auto_target_entropy", ":", "\n", "            ", "alpha_loss", "=", "-", "(", "\n", "self", ".", "log_alpha", "*", "(", "log_pi", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", "\n", ")", ".", "mean", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "step", "(", ")", "\n", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "1", "\n", "\n", "# update policy", "\n", "", "if", "self", ".", "reparameterize", ":", "\n", "            ", "policy_kl", "=", "(", "alpha", "*", "log_pi", "-", "min_critic_output", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "# it seems useless", "\n", "            ", "raise", "NotImplementedError", "\n", "", "policy_loss", "=", "policy_kl", "+", "(", "raw_action", "**", "2", ")", ".", "mean", "(", ")", "*", "1e-3", "# regularizer", "\n", "\n", "self", ".", "policy_optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "grad", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "policy", ".", "parameters", "(", ")", ",", "POLICY_GRAD_BOUND", "\n", ")", "\n", "self", ".", "policy_optimizer", ".", "step", "(", ")", "\n", "\n", "# update critic", "\n", "critic_in", "=", "torch", ".", "cat", "(", "[", "*", "obs", ",", "*", "acs", "]", ",", "dim", "=", "1", ")", "\n", "critic1_output", "=", "self", ".", "critic1", "(", "critic_in", ")", "\n", "critic2_output", "=", "self", ".", "critic2", "(", "critic_in", ")", "\n", "\n", "next_obs_action", ",", "next_obs_log_pi", "=", "self", ".", "step", "(", "\n", "next_obs", ",", "explore", "=", "False", ",", "return_log_prob", "=", "True", "\n", ")", "\n", "next_obs_actions", "=", "[", "]", "\n", "for", "a_i", ",", "n_ob", "in", "enumerate", "(", "next_obs", ")", ":", "\n", "            ", "if", "a_i", "==", "self", ".", "index", ":", "\n", "                ", "next_obs_actions", ".", "append", "(", "next_obs_action", ".", "detach", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "opp_i", "=", "a_i", "if", "a_i", "<", "self", ".", "index", "else", "a_i", "-", "1", "\n", "next_obs_actions", ".", "append", "(", "\n", "self", ".", "get_opp_action", "(", "self", ".", "opp_policies", "[", "opp_i", "]", ",", "n_ob", ",", "a_i", ")", ".", "detach", "(", ")", "\n", ")", "\n", "", "", "target_critic_in", "=", "torch", ".", "cat", "(", "[", "*", "next_obs", ",", "*", "next_obs_actions", "]", ",", "dim", "=", "1", ")", "\n", "min_target_critic_output", "=", "(", "\n", "torch", ".", "min", "(", "\n", "self", ".", "target_critic1", "(", "target_critic_in", ")", ",", "\n", "self", ".", "target_critic2", "(", "target_critic_in", ")", ",", "\n", ")", "\n", "-", "alpha", "*", "next_obs_log_pi", "\n", ")", "\n", "\n", "target", "=", "(", "\n", "self", ".", "rew_scale", "*", "rews", "[", "self", ".", "index", "]", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "+", "self", ".", "gamma", "*", "min_target_critic_output", "\n", ")", "\n", "\n", "self", ".", "critic1_optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "critic2_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "critic1_loss", "=", "MSELoss", "(", "critic1_output", ",", "target", ".", "detach", "(", ")", ")", "\n", "critic2_loss", "=", "MSELoss", "(", "critic2_output", ",", "target", ".", "detach", "(", ")", ")", "\n", "\n", "critic1_loss", ".", "backward", "(", ")", "\n", "critic2_loss", ".", "backward", "(", ")", "\n", "grad1", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic1", ".", "parameters", "(", ")", ",", "CRITIC_GRAD_BOUND", ")", "\n", "grad2", "=", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic2", ".", "parameters", "(", ")", ",", "CRITIC_GRAD_BOUND", ")", "\n", "self", ".", "critic1_optimizer", ".", "step", "(", ")", "\n", "self", ".", "critic2_optimizer", ".", "step", "(", ")", "\n", "\n", "return", "{", "\n", "\"alpha\"", ":", "alpha", ",", "\n", "\"log_pol_pi\"", ":", "(", "alpha", "*", "log_pi", ")", ".", "mean", "(", ")", ",", "\n", "\"pol_critic\"", ":", "min_critic_output", ".", "mean", "(", ")", ",", "\n", "\"pol_loss\"", ":", "policy_loss", ",", "\n", "\"cf1_loss\"", ":", "critic1_loss", ",", "\n", "\"cf2_loss\"", ":", "critic2_loss", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.__init__": [[781, 862], ["baseagent.AgentMB.__init__", "sacagent.SACAgentOppMd.__init__", "init_attr.update"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCond.update"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "dim_obs_list", ",", "\n", "dim_actions", ",", "\n", "dim_rewards", ",", "\n", "agent_index", ",", "\n", "dim_in_opp_pols", ",", "\n", "dim_out_opp_pols", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "discrete_action", ",", "\n", "auto_target_entropy", "=", "True", ",", "\n", "target_entropy", "=", "None", ",", "\n", "reparameterize", "=", "True", ",", "\n", "replay_buffer", "=", "None", ",", "\n", "model_lr", "=", "0.001", ",", "\n", "model_hidden_dim", "=", "32", ",", "\n", "ensemble_size", "=", "7", ",", "\n", "action_shape_list", "=", "[", "]", ",", "\n", "MB_batch_size", ":", "int", "=", "4096", ",", "\n", "grad_bound", ":", "float", "=", "1.0", ",", "\n", "opp_lr", "=", "0.001", ",", "\n", ")", ":", "\n", "        ", "AgentMB", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "dim_obs_list", ",", "\n", "dim_actions", ",", "\n", "dim_rewards", ",", "\n", "agent_index", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", ",", "\n", "lr", ",", "\n", "discrete_action", ",", "\n", "replay_buffer", ",", "\n", "model_lr", "=", "model_lr", ",", "\n", "model_hidden_dim", "=", "model_hidden_dim", ",", "\n", "ensemble_size", "=", "ensemble_size", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "grad_bound", "=", "grad_bound", ",", "\n", "MB_batch_size", "=", "MB_batch_size", ",", "\n", ")", "\n", "\n", "init_attr", "=", "self", ".", "init_attr", "\n", "\n", "SACAgentOppMdCond", ".", "__init__", "(", "\n", "self", ",", "\n", "dim_in_pol", ",", "\n", "dim_out_pol", ",", "\n", "dim_in_critic", ",", "\n", "agent_index", ",", "\n", "dim_in_opp_pols", ",", "\n", "dim_out_opp_pols", ",", "\n", "n_agent", ",", "\n", "alg_type", ",", "\n", "rew_scale", ",", "\n", "gamma", ",", "\n", "hidden_dim", "=", "hidden_dim", ",", "\n", "lr", "=", "lr", ",", "\n", "discrete_action", "=", "discrete_action", ",", "\n", "auto_target_entropy", "=", "auto_target_entropy", ",", "\n", "target_entropy", "=", "target_entropy", ",", "\n", "reparameterize", "=", "reparameterize", ",", "\n", "action_shape_list", "=", "action_shape_list", ",", "\n", "opp_lr", "=", "opp_lr", ",", "\n", ")", "\n", "\n", "init_attr", ".", "update", "(", "self", ".", "init_attr", ")", "\n", "self", ".", "init_attr", "=", "init_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params": [[863, 867], ["sacagent.SACAgentOppMd.get_params", "sacagent.SACAgentOppMd.get_params", "baseagent.AgentMB.get_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.get_params"], ["", "def", "get_params", "(", "self", ")", ":", "\n", "        ", "ret", "=", "SACAgentOppMdCond", ".", "get_params", "(", "self", ")", "\n", "ret", ".", "update", "(", "AgentMB", ".", "get_params", "(", "self", ")", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params": [[868, 871], ["sacagent.SACAgentOppMd.load_params", "baseagent.AgentMB.load_params"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.load_params"], ["", "def", "load_params", "(", "self", ",", "params", ")", ":", "\n", "        ", "SACAgentOppMdCond", ".", "load_params", "(", "self", ",", "params", ")", "\n", "AgentMB", ".", "load_params", "(", "self", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts": [[872, 875], ["sacagent.SACAgentOppMd.prep_rollouts", "baseagent.AgentMB.prep_rollouts"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_rollouts"], ["", "def", "prep_rollouts", "(", "self", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "SACAgentOppMdCond", ".", "prep_rollouts", "(", "self", ",", "device", ")", "\n", "AgentMB", ".", "prep_rollouts", "(", "self", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training": [[876, 879], ["sacagent.SACAgentOppMd.prep_training", "baseagent.AgentMB.prep_training"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training", "home.repos.pwc.inspect_result.apexrl_AORPO.agents.sacagent.SACAgentOppMdCondMB.prep_training"], ["", "def", "prep_training", "(", "self", ",", "device", "=", "\"cuda\"", ")", ":", "\n", "        ", "SACAgentOppMdCond", ".", "prep_training", "(", "self", ",", "device", ")", "\n", "AgentMB", ".", "prep_training", "(", "self", ",", "device", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent-particle-envs.make_env.make_env": [[16, 54], ["scenarios.load().Scenario", "scenarios.load().Scenario.make_world", "MultiAgentEnv", "MultiAgentEnv", "scenarios.load"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.make_world", "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load"], ["\n", "\n", "\n", "def", "make_env", "(", "scenario_name", ",", "benchmark", ":", "bool", "=", "False", ",", "discrete_action", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Creates a MultiAgentEnv object as env. This can be used similar to a gym\n    environment by calling env.reset() and env.step().\n    Use env.render() to view the environment on the screen.\n\n    Input:\n        scenario_name   :   name of the scenario from ./scenarios/ to be Returns\n                            (without the .py extension)\n        benchmark       :   whether you want to produce benchmarking data\n                            (usually only done during evaluation)\n\n    Some useful env properties (see environment.py):\n        .observation_space  :   Returns the observation space for each agent\n        .action_space       :   Returns the action space for each agent\n        .n                  :   Returns the number of Agents\n    \"\"\"", "\n", "from", "multiagent", ".", "environment", "import", "MultiAgentEnv", "\n", "import", "multiagent", ".", "scenarios", "as", "scenarios", "\n", "\n", "if", "\"v0\"", "in", "scenario_name", ":", "\n", "        ", "env", "=", "gym", ".", "make", "(", "scenario_name", ")", "\n", "return", "env", "\n", "# load scenario from script", "\n", "", "scenario", "=", "scenarios", ".", "load", "(", "scenario_name", "+", "\".py\"", ")", ".", "Scenario", "(", ")", "\n", "# create world", "\n", "world", "=", "scenario", ".", "make_world", "(", ")", "\n", "# create multiagent environment", "\n", "\n", "if", "benchmark", ":", "\n", "        ", "env", "=", "MultiAgentEnv", "(", "\n", "world", ",", "\n", "scenario", ".", "reset_world", ",", "\n", "scenario", ".", "reward", ",", "\n", "scenario", ".", "observation", ",", "\n", "scenario", ".", "benchmark_data", ",", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.EntityState.__init__": [[5, 10], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# physical position", "\n", "        ", "self", ".", "p_pos", "=", "None", "\n", "# physical velocity", "\n", "self", ".", "p_vel", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.AgentState.__init__": [[14, 18], ["core.EntityState.__init__"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "AgentState", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# communication utterance", "\n", "self", ".", "c", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.Action.__init__": [[22, 27], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# physical action", "\n", "        ", "self", ".", "u", "=", "None", "\n", "# communication action", "\n", "self", ".", "c", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.Entity.__init__": [[31, 51], ["core.EntityState"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# name", "\n", "        ", "self", ".", "name", "=", "\"\"", "\n", "# properties:", "\n", "self", ".", "size", "=", "0.050", "\n", "# entity can move / be pushed", "\n", "self", ".", "movable", "=", "False", "\n", "# entity collides with others", "\n", "self", ".", "collide", "=", "True", "\n", "# material density (affects mass)", "\n", "self", ".", "density", "=", "25.0", "\n", "# color", "\n", "self", ".", "color", "=", "None", "\n", "# max speed and accel", "\n", "self", ".", "max_speed", "=", "None", "\n", "self", ".", "accel", "=", "None", "\n", "# state", "\n", "self", ".", "state", "=", "EntityState", "(", ")", "\n", "# mass", "\n", "self", ".", "initial_mass", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.Entity.mass": [[52, 55], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "mass", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "initial_mass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.Landmark.__init__": [[59, 61], ["core.Entity.__init__"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Landmark", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.Agent.__init__": [[65, 86], ["core.Entity.__init__", "core.AgentState", "core.Action"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "Agent", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# agents are movable by default", "\n", "self", ".", "movable", "=", "True", "\n", "# cannot send communication signals", "\n", "self", ".", "silent", "=", "False", "\n", "self", ".", "channel", "=", "1", "\n", "# cannot observe the world", "\n", "self", ".", "blind", "=", "False", "\n", "# physical motor noise amount", "\n", "self", ".", "u_noise", "=", "None", "\n", "# communication noise amount", "\n", "self", ".", "c_noise", "=", "None", "\n", "# control range", "\n", "self", ".", "u_range", "=", "1.0", "\n", "# state", "\n", "self", ".", "state", "=", "AgentState", "(", ")", "\n", "# action", "\n", "self", ".", "action", "=", "Action", "(", ")", "\n", "# script behavior to execute", "\n", "self", ".", "action_callback", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.__init__": [[90, 107], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "# list of agents and entities (can change at execution-time!)", "\n", "        ", "self", ".", "agents", "=", "[", "]", "\n", "self", ".", "landmarks", "=", "[", "]", "\n", "# communication channel dimensionality", "\n", "self", ".", "dim_c", "=", "0", "\n", "# position dimensionality", "\n", "self", ".", "dim_p", "=", "2", "\n", "# color dimensionality", "\n", "self", ".", "dim_color", "=", "3", "\n", "# simulation timestep", "\n", "self", ".", "dt", "=", "0.1", "\n", "# physical damping", "\n", "self", ".", "damping", "=", "0.25", "\n", "# contact response parameters", "\n", "self", ".", "contact_force", "=", "1e2", "\n", "self", ".", "contact_margin", "=", "1e-3", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.entities": [[109, 112], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "entities", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "agents", "+", "self", ".", "landmarks", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.policy_agents": [[114, 117], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "policy_agents", "(", "self", ")", ":", "\n", "        ", "return", "[", "agent", "for", "agent", "in", "self", ".", "agents", "if", "agent", ".", "action_callback", "is", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.scripted_agents": [[119, 122], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "scripted_agents", "(", "self", ")", ":", "\n", "        ", "return", "[", "agent", "for", "agent", "in", "self", ".", "agents", "if", "agent", ".", "action_callback", "is", "not", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.step": [[124, 139], ["core.World.apply_action_force", "core.World.apply_environment_force", "core.World.integrate_state", "agent.action_callback", "len", "core.World.update_agent_state"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.apply_action_force", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.apply_environment_force", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.integrate_state", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.update_agent_state"], ["", "def", "step", "(", "self", ")", ":", "\n", "# set actions for scripted agents", "\n", "        ", "for", "agent", "in", "self", ".", "scripted_agents", ":", "\n", "            ", "agent", ".", "action", "=", "agent", ".", "action_callback", "(", "agent", ",", "self", ")", "\n", "# gather forces applied to entities", "\n", "", "p_force", "=", "[", "None", "]", "*", "len", "(", "self", ".", "entities", ")", "\n", "# apply agent physical controls", "\n", "p_force", "=", "self", ".", "apply_action_force", "(", "p_force", ")", "\n", "# apply environment forces", "\n", "p_force", "=", "self", ".", "apply_environment_force", "(", "p_force", ")", "\n", "# integrate physical state", "\n", "self", ".", "integrate_state", "(", "p_force", ")", "\n", "# update agent state", "\n", "for", "agent", "in", "self", ".", "agents", ":", "\n", "            ", "self", ".", "update_agent_state", "(", "agent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.apply_action_force": [[141, 152], ["enumerate", "numpy.random.randn"], "methods", ["None"], ["", "", "def", "apply_action_force", "(", "self", ",", "p_force", ")", ":", "\n", "# set applied forces", "\n", "        ", "for", "i", ",", "agent", "in", "enumerate", "(", "self", ".", "agents", ")", ":", "\n", "            ", "if", "agent", ".", "movable", ":", "\n", "                ", "noise", "=", "(", "\n", "np", ".", "random", ".", "randn", "(", "*", "agent", ".", "action", ".", "u", ".", "shape", ")", "*", "agent", ".", "u_noise", "\n", "if", "agent", ".", "u_noise", "\n", "else", "0.0", "\n", ")", "\n", "p_force", "[", "i", "]", "=", "agent", ".", "action", ".", "u", "+", "noise", "\n", "", "", "return", "p_force", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.apply_environment_force": [[154, 170], ["enumerate", "enumerate", "core.World.get_collision_force"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.get_collision_force"], ["", "def", "apply_environment_force", "(", "self", ",", "p_force", ")", ":", "\n", "# simple (but inefficient) collision response", "\n", "        ", "for", "a", ",", "entity_a", "in", "enumerate", "(", "self", ".", "entities", ")", ":", "\n", "            ", "for", "b", ",", "entity_b", "in", "enumerate", "(", "self", ".", "entities", ")", ":", "\n", "                ", "if", "b", "<=", "a", ":", "\n", "                    ", "continue", "\n", "", "[", "f_a", ",", "f_b", "]", "=", "self", ".", "get_collision_force", "(", "entity_a", ",", "entity_b", ")", "\n", "if", "f_a", "is", "not", "None", ":", "\n", "                    ", "if", "p_force", "[", "a", "]", "is", "None", ":", "\n", "                        ", "p_force", "[", "a", "]", "=", "0.0", "\n", "", "p_force", "[", "a", "]", "=", "f_a", "+", "p_force", "[", "a", "]", "\n", "", "if", "f_b", "is", "not", "None", ":", "\n", "                    ", "if", "p_force", "[", "b", "]", "is", "None", ":", "\n", "                        ", "p_force", "[", "b", "]", "=", "0.0", "\n", "", "p_force", "[", "b", "]", "=", "f_b", "+", "p_force", "[", "b", "]", "\n", "", "", "", "return", "p_force", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.integrate_state": [[172, 193], ["enumerate", "numpy.sqrt", "numpy.square", "numpy.square", "numpy.sqrt", "numpy.square", "numpy.square"], "methods", ["None"], ["", "def", "integrate_state", "(", "self", ",", "p_force", ")", ":", "\n", "        ", "for", "i", ",", "entity", "in", "enumerate", "(", "self", ".", "entities", ")", ":", "\n", "            ", "if", "not", "entity", ".", "movable", ":", "\n", "                ", "continue", "\n", "", "entity", ".", "state", ".", "p_vel", "=", "entity", ".", "state", ".", "p_vel", "*", "(", "1", "-", "self", ".", "damping", ")", "\n", "if", "p_force", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "entity", ".", "state", ".", "p_vel", "+=", "(", "p_force", "[", "i", "]", "/", "entity", ".", "mass", ")", "*", "self", ".", "dt", "\n", "", "if", "entity", ".", "max_speed", "is", "not", "None", ":", "\n", "                ", "speed", "=", "np", ".", "sqrt", "(", "\n", "np", ".", "square", "(", "entity", ".", "state", ".", "p_vel", "[", "0", "]", ")", "+", "np", ".", "square", "(", "entity", ".", "state", ".", "p_vel", "[", "1", "]", ")", "\n", ")", "\n", "if", "speed", ">", "entity", ".", "max_speed", ":", "\n", "                    ", "entity", ".", "state", ".", "p_vel", "=", "(", "\n", "entity", ".", "state", ".", "p_vel", "\n", "/", "np", ".", "sqrt", "(", "\n", "np", ".", "square", "(", "entity", ".", "state", ".", "p_vel", "[", "0", "]", ")", "\n", "+", "np", ".", "square", "(", "entity", ".", "state", ".", "p_vel", "[", "1", "]", ")", "\n", ")", "\n", "*", "entity", ".", "max_speed", "\n", ")", "\n", "", "", "entity", ".", "state", ".", "p_pos", "+=", "entity", ".", "state", ".", "p_vel", "*", "self", ".", "dt", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.update_agent_state": [[194, 205], ["numpy.zeros", "numpy.random.randn"], "methods", ["None"], ["", "", "def", "update_agent_state", "(", "self", ",", "agent", ")", ":", "\n", "# set communication state (directly for now)", "\n", "        ", "if", "agent", ".", "silent", ":", "\n", "            ", "agent", ".", "state", ".", "c", "=", "np", ".", "zeros", "(", "self", ".", "dim_c", ")", "\n", "", "else", ":", "\n", "            ", "noise", "=", "(", "\n", "np", ".", "random", ".", "randn", "(", "*", "agent", ".", "action", ".", "c", ".", "shape", ")", "*", "agent", ".", "c_noise", "\n", "if", "agent", ".", "c_noise", "\n", "else", "0.0", "\n", ")", "\n", "agent", ".", "state", ".", "c", "=", "agent", ".", "action", ".", "c", "+", "noise", "\n", "# print(agent.state.c)", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.core.World.get_collision_force": [[208, 225], ["numpy.sqrt", "numpy.sum", "numpy.logaddexp", "numpy.square"], "methods", ["None"], ["", "", "def", "get_collision_force", "(", "self", ",", "entity_a", ",", "entity_b", ")", ":", "\n", "        ", "if", "(", "not", "entity_a", ".", "collide", ")", "or", "(", "not", "entity_b", ".", "collide", ")", ":", "\n", "            ", "return", "[", "None", ",", "None", "]", "# not a collider", "\n", "", "if", "entity_a", "is", "entity_b", ":", "\n", "            ", "return", "[", "None", ",", "None", "]", "# don't collide against itself", "\n", "# compute actual distance between entities", "\n", "", "delta_pos", "=", "entity_a", ".", "state", ".", "p_pos", "-", "entity_b", ".", "state", ".", "p_pos", "\n", "dist", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "square", "(", "delta_pos", ")", ")", ")", "\n", "# minimum allowable distance", "\n", "dist_min", "=", "entity_a", ".", "size", "+", "entity_b", ".", "size", "\n", "# softmax penetration", "\n", "k", "=", "self", ".", "contact_margin", "\n", "penetration", "=", "np", ".", "logaddexp", "(", "0", ",", "-", "(", "dist", "-", "dist_min", ")", "/", "k", ")", "*", "k", "\n", "force", "=", "self", ".", "contact_force", "*", "delta_pos", "/", "dist", "*", "penetration", "\n", "force_a", "=", "+", "force", "if", "entity_a", ".", "movable", "else", "None", "\n", "force_b", "=", "-", "force", "if", "entity_b", ".", "movable", "else", "None", "\n", "return", "[", "force_a", ",", "force_b", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.__init__": [[51, 72], ["rendering.get_display", "pyglet.window.Window", "rendering.Transform", "glEnable", "glEnable", "glHint", "glLineWidth", "glBlendFunc"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.get_display"], ["    ", "def", "__init__", "(", "self", ",", "width", ",", "height", ",", "display", "=", "None", ")", ":", "\n", "        ", "display", "=", "get_display", "(", "display", ")", "\n", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "height", "=", "height", "\n", "\n", "self", ".", "window", "=", "pyglet", ".", "window", ".", "Window", "(", "width", "=", "width", ",", "height", "=", "height", ",", "display", "=", "display", ")", "\n", "self", ".", "window", ".", "on_close", "=", "self", ".", "window_closed_by_user", "\n", "self", ".", "geoms", "=", "[", "]", "\n", "self", ".", "onetime_geoms", "=", "[", "]", "\n", "self", ".", "text_lines", "=", "[", "]", "\n", "self", ".", "transform", "=", "Transform", "(", ")", "\n", "self", ".", "max_size", "=", "1", "\n", "\n", "glEnable", "(", "GL_BLEND", ")", "\n", "# glEnable(GL_MULTISAMPLE)", "\n", "glEnable", "(", "GL_LINE_SMOOTH", ")", "\n", "# glHint(GL_LINE_SMOOTH_HINT, GL_DONT_CARE)", "\n", "glHint", "(", "GL_LINE_SMOOTH_HINT", ",", "GL_NICEST", ")", "\n", "glLineWidth", "(", "2.0", ")", "\n", "glBlendFunc", "(", "GL_SRC_ALPHA", ",", "GL_ONE_MINUS_SRC_ALPHA", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.close": [[73, 75], ["rendering.Viewer.window.close"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "window", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.window_closed_by_user": [[76, 78], ["rendering.Viewer.close"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["", "def", "window_closed_by_user", "(", "self", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.set_max_size": [[79, 91], ["max", "rendering.Transform"], "methods", ["None"], ["", "def", "set_max_size", "(", "self", ",", "current_size", ")", ":", "\n", "        ", "max_size", "=", "self", ".", "max_size", "=", "max", "(", "current_size", ",", "self", ".", "max_size", ")", "\n", "left", "=", "-", "max_size", "\n", "right", "=", "max_size", "\n", "bottom", "=", "-", "max_size", "\n", "top", "=", "max_size", "\n", "assert", "right", ">", "left", "and", "top", ">", "bottom", "\n", "scalex", "=", "self", ".", "width", "/", "(", "right", "-", "left", ")", "\n", "scaley", "=", "self", ".", "height", "/", "(", "top", "-", "bottom", ")", "\n", "self", ".", "transform", "=", "Transform", "(", "\n", "translation", "=", "(", "-", "left", "*", "scalex", ",", "-", "bottom", "*", "scaley", ")", ",", "\n", "scale", "=", "(", "scalex", ",", "scaley", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_geom": [[92, 94], ["rendering.Viewer.geoms.append"], "methods", ["None"], ["", "def", "add_geom", "(", "self", ",", "geom", ")", ":", "\n", "        ", "self", ".", "geoms", ".", "append", "(", "geom", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_onetime": [[95, 97], ["rendering.Viewer.onetime_geoms.append"], "methods", ["None"], ["", "def", "add_onetime", "(", "self", ",", "geom", ")", ":", "\n", "        ", "self", ".", "onetime_geoms", ".", "append", "(", "geom", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.render": [[98, 132], ["glClearColor", "rendering.Viewer.window.clear", "rendering.Viewer.window.switch_to", "rendering.Viewer.window.dispatch_events", "rendering.Viewer.transform.enable", "rendering.Viewer.transform.disable", "pyglet.gl.glMatrixMode", "pyglet.gl.glLoadIdentity", "gluOrtho2D", "rendering.Viewer.window.flip", "geom.render", "geom.render", "geom.render", "pyglet.image.get_buffer_manager().get_color_buffer", "pyglet.image.get_buffer_manager().get_color_buffer.get_image_data", "numpy.fromstring", "arr.reshape.reshape.reshape", "pyglet.image.get_buffer_manager().get_color_buffer.get_image_data.get_data", "pyglet.image.get_buffer_manager"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.clear", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineWidth.enable", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineStyle.disable", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render"], ["", "def", "render", "(", "self", ",", "return_rgb_array", "=", "False", ")", ":", "\n", "        ", "glClearColor", "(", "1", ",", "1", ",", "1", ",", "1", ")", "\n", "self", ".", "window", ".", "clear", "(", ")", "\n", "self", ".", "window", ".", "switch_to", "(", ")", "\n", "self", ".", "window", ".", "dispatch_events", "(", ")", "\n", "self", ".", "transform", ".", "enable", "(", ")", "\n", "for", "geom", "in", "self", ".", "geoms", ":", "\n", "            ", "geom", ".", "render", "(", ")", "\n", "", "for", "geom", "in", "self", ".", "onetime_geoms", ":", "\n", "            ", "geom", ".", "render", "(", ")", "\n", "", "self", ".", "transform", ".", "disable", "(", ")", "\n", "\n", "pyglet", ".", "gl", ".", "glMatrixMode", "(", "pyglet", ".", "gl", ".", "GL_PROJECTION", ")", "\n", "pyglet", ".", "gl", ".", "glLoadIdentity", "(", ")", "\n", "gluOrtho2D", "(", "0", ",", "self", ".", "window", ".", "width", ",", "0", ",", "self", ".", "window", ".", "height", ")", "\n", "for", "geom", "in", "self", ".", "text_lines", ":", "\n", "            ", "geom", ".", "render", "(", ")", "\n", "\n", "", "arr", "=", "None", "\n", "if", "return_rgb_array", ":", "\n", "            ", "buffer", "=", "pyglet", ".", "image", ".", "get_buffer_manager", "(", ")", ".", "get_color_buffer", "(", ")", "\n", "image_data", "=", "buffer", ".", "get_image_data", "(", ")", "\n", "arr", "=", "np", ".", "fromstring", "(", "image_data", ".", "get_data", "(", ")", ",", "dtype", "=", "np", ".", "uint8", ",", "sep", "=", "''", ")", "\n", "# In https://github.com/openai/gym-http-api/issues/2, we", "\n", "# discovered that someone using Xmonad on Arch was having", "\n", "# a window of size 598 x 398, though a 600 x 400 window", "\n", "# was requested. (Guess Xmonad was preserving a pixel for", "\n", "# the boundary.) So we use the buffer height/width rather", "\n", "# than the requested one.", "\n", "arr", "=", "arr", ".", "reshape", "(", "buffer", ".", "height", ",", "buffer", ".", "width", ",", "4", ")", "\n", "arr", "=", "arr", "[", ":", ":", "-", "1", ",", ":", ",", "0", ":", "3", "]", "\n", "", "self", ".", "window", ".", "flip", "(", ")", "\n", "self", ".", "onetime_geoms", "=", "[", "]", "\n", "return", "arr", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.draw_circle": [[134, 139], ["rendering.make_circle", "rendering._add_attrs", "rendering.Viewer.add_onetime"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_circle", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering._add_attrs", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_onetime"], ["", "def", "draw_circle", "(", "self", ",", "radius", "=", "10", ",", "res", "=", "30", ",", "filled", "=", "True", ",", "**", "attrs", ")", ":", "\n", "        ", "geom", "=", "make_circle", "(", "radius", "=", "radius", ",", "res", "=", "res", ",", "filled", "=", "filled", ")", "\n", "_add_attrs", "(", "geom", ",", "attrs", ")", "\n", "self", ".", "add_onetime", "(", "geom", ")", "\n", "return", "geom", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.draw_polygon": [[140, 145], ["rendering.make_polygon", "rendering._add_attrs", "rendering.Viewer.add_onetime"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_polygon", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering._add_attrs", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_onetime"], ["", "def", "draw_polygon", "(", "self", ",", "v", ",", "filled", "=", "True", ",", "**", "attrs", ")", ":", "\n", "        ", "geom", "=", "make_polygon", "(", "v", "=", "v", ",", "filled", "=", "filled", ")", "\n", "_add_attrs", "(", "geom", ",", "attrs", ")", "\n", "self", ".", "add_onetime", "(", "geom", ")", "\n", "return", "geom", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.draw_polyline": [[146, 151], ["rendering.make_polyline", "rendering._add_attrs", "rendering.Viewer.add_onetime"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_polyline", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering._add_attrs", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_onetime"], ["", "def", "draw_polyline", "(", "self", ",", "v", ",", "**", "attrs", ")", ":", "\n", "        ", "geom", "=", "make_polyline", "(", "v", "=", "v", ")", "\n", "_add_attrs", "(", "geom", ",", "attrs", ")", "\n", "self", ".", "add_onetime", "(", "geom", ")", "\n", "return", "geom", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.draw_line": [[152, 157], ["rendering.Line", "rendering._add_attrs", "rendering.Viewer.add_onetime"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering._add_attrs", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_onetime"], ["", "def", "draw_line", "(", "self", ",", "start", ",", "end", ",", "**", "attrs", ")", ":", "\n", "        ", "geom", "=", "Line", "(", "start", ",", "end", ")", "\n", "_add_attrs", "(", "geom", ",", "attrs", ")", "\n", "self", ".", "add_onetime", "(", "geom", ")", "\n", "return", "geom", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.get_array": [[158, 165], ["rendering.Viewer.window.flip", "pyglet.image.get_buffer_manager().get_color_buffer().get_image_data", "rendering.Viewer.window.flip", "numpy.fromstring", "arr.reshape.reshape.reshape", "pyglet.image.get_buffer_manager().get_color_buffer().get_image_data.get_data", "pyglet.image.get_buffer_manager().get_color_buffer", "pyglet.image.get_buffer_manager"], "methods", ["None"], ["", "def", "get_array", "(", "self", ")", ":", "\n", "        ", "self", ".", "window", ".", "flip", "(", ")", "\n", "image_data", "=", "pyglet", ".", "image", ".", "get_buffer_manager", "(", ")", ".", "get_color_buffer", "(", ")", ".", "get_image_data", "(", ")", "\n", "self", ".", "window", ".", "flip", "(", ")", "\n", "arr", "=", "np", ".", "fromstring", "(", "image_data", ".", "get_data", "(", ")", ",", "dtype", "=", "np", ".", "uint8", ",", "sep", "=", "''", ")", "\n", "arr", "=", "arr", ".", "reshape", "(", "self", ".", "height", ",", "self", ".", "width", ",", "4", ")", "\n", "return", "arr", "[", ":", ":", "-", "1", ",", ":", ",", "0", ":", "3", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.__init__": [[175, 178], ["rendering.Color"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_color", "=", "Color", "(", "(", "0", ",", "0", ",", "0", ",", "1.0", ")", ")", "\n", "self", ".", "attrs", "=", "[", "self", ".", "_color", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.render": [[179, 185], ["reversed", "rendering.Geom.render1", "attr.enable", "attr.disable"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Image.render1", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineWidth.enable", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineStyle.disable"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "for", "attr", "in", "reversed", "(", "self", ".", "attrs", ")", ":", "\n", "            ", "attr", ".", "enable", "(", ")", "\n", "", "self", ".", "render1", "(", ")", "\n", "for", "attr", "in", "self", ".", "attrs", ":", "\n", "            ", "attr", ".", "disable", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.render1": [[186, 188], ["None"], "methods", ["None"], ["", "", "def", "render1", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.add_attr": [[189, 191], ["rendering.Geom.attrs.append"], "methods", ["None"], ["", "def", "add_attr", "(", "self", ",", "attr", ")", ":", "\n", "        ", "self", ".", "attrs", ".", "append", "(", "attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.set_color": [[192, 194], ["None"], "methods", ["None"], ["", "def", "set_color", "(", "self", ",", "r", ",", "g", ",", "b", ",", "alpha", "=", "1", ")", ":", "\n", "        ", "self", ".", "_color", ".", "vec4", "=", "(", "r", ",", "g", ",", "b", ",", "alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Attr.enable": [[197, 199], ["None"], "methods", ["None"], ["    ", "def", "enable", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Attr.disable": [[200, 202], ["None"], "methods", ["None"], ["", "def", "disable", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.__init__": [[205, 209], ["rendering.Transform.set_translation", "rendering.Transform.set_rotation", "rendering.Transform.set_scale"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_translation", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_rotation", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_scale"], ["    ", "def", "__init__", "(", "self", ",", "translation", "=", "(", "0.0", ",", "0.0", ")", ",", "rotation", "=", "0.0", ",", "scale", "=", "(", "1", ",", "1", ")", ")", ":", "\n", "        ", "self", ".", "set_translation", "(", "*", "translation", ")", "\n", "self", ".", "set_rotation", "(", "rotation", ")", "\n", "self", ".", "set_scale", "(", "*", "scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.enable": [[210, 215], ["glPushMatrix", "glTranslatef", "glRotatef", "glScalef"], "methods", ["None"], ["", "def", "enable", "(", "self", ")", ":", "\n", "        ", "glPushMatrix", "(", ")", "\n", "glTranslatef", "(", "self", ".", "translation", "[", "0", "]", ",", "self", ".", "translation", "[", "1", "]", ",", "0", ")", "# translate to GL loc ppint", "\n", "glRotatef", "(", "RAD2DEG", "*", "self", ".", "rotation", ",", "0", ",", "0", ",", "1.0", ")", "\n", "glScalef", "(", "self", ".", "scale", "[", "0", "]", ",", "self", ".", "scale", "[", "1", "]", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.disable": [[216, 218], ["glPopMatrix"], "methods", ["None"], ["", "def", "disable", "(", "self", ")", ":", "\n", "        ", "glPopMatrix", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_translation": [[219, 221], ["float", "float"], "methods", ["None"], ["", "def", "set_translation", "(", "self", ",", "newx", ",", "newy", ")", ":", "\n", "        ", "self", ".", "translation", "=", "(", "float", "(", "newx", ")", ",", "float", "(", "newy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_rotation": [[222, 224], ["float"], "methods", ["None"], ["", "def", "set_rotation", "(", "self", ",", "new", ")", ":", "\n", "        ", "self", ".", "rotation", "=", "float", "(", "new", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_scale": [[225, 227], ["float", "float"], "methods", ["None"], ["", "def", "set_scale", "(", "self", ",", "newx", ",", "newy", ")", ":", "\n", "        ", "self", ".", "scale", "=", "(", "float", "(", "newx", ")", ",", "float", "(", "newy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Color.__init__": [[230, 232], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vec4", ")", ":", "\n", "        ", "self", ".", "vec4", "=", "vec4", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Color.enable": [[233, 235], ["glColor4f"], "methods", ["None"], ["", "def", "enable", "(", "self", ")", ":", "\n", "        ", "glColor4f", "(", "*", "self", ".", "vec4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineStyle.__init__": [[238, 240], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "style", ")", ":", "\n", "        ", "self", ".", "style", "=", "style", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineStyle.enable": [[241, 244], ["glEnable", "glLineStipple"], "methods", ["None"], ["", "def", "enable", "(", "self", ")", ":", "\n", "        ", "glEnable", "(", "GL_LINE_STIPPLE", ")", "\n", "glLineStipple", "(", "1", ",", "self", ".", "style", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineStyle.disable": [[245, 247], ["glDisable"], "methods", ["None"], ["", "def", "disable", "(", "self", ")", ":", "\n", "        ", "glDisable", "(", "GL_LINE_STIPPLE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineWidth.__init__": [[250, 252], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "stroke", ")", ":", "\n", "        ", "self", ".", "stroke", "=", "stroke", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.LineWidth.enable": [[253, 255], ["glLineWidth"], "methods", ["None"], ["", "def", "enable", "(", "self", ")", ":", "\n", "        ", "glLineWidth", "(", "self", ".", "stroke", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Point.__init__": [[258, 260], ["rendering.Geom.__init__"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Point.render1": [[261, 265], ["glVertex3f", "glEnd"], "methods", ["None"], ["", "def", "render1", "(", "self", ")", ":", "\n", "        ", "(", "GL_POINTS", ")", "# draw point", "\n", "glVertex3f", "(", "0.0", ",", "0.0", ",", "0.0", ")", "\n", "glEnd", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.TextLine.__init__": [[268, 274], ["pyglet.font.add_file", "rendering.TextLine.set_text", "os.path.join", "os.path.dirname"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.TextLine.set_text"], ["    ", "def", "__init__", "(", "self", ",", "window", ",", "idx", ")", ":", "\n", "        ", "self", ".", "idx", "=", "idx", "\n", "self", ".", "window", "=", "window", "\n", "pyglet", ".", "font", ".", "add_file", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"secrcode.ttf\"", ")", ")", "\n", "self", ".", "label", "=", "None", "\n", "self", ".", "set_text", "(", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.TextLine.render": [[275, 278], ["rendering.TextLine.label.draw"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "label", "is", "not", "None", ":", "\n", "            ", "self", ".", "label", ".", "draw", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.TextLine.set_text": [[279, 295], ["pyglet.font.have_font", "pyglet.text.Label", "rendering.TextLine.label.draw", "pyglet.font.have_font"], "methods", ["None"], ["", "", "def", "set_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "pyglet", ".", "font", ".", "have_font", "(", "'Courier'", ")", ":", "\n", "            ", "font", "=", "\"Courier\"", "\n", "", "elif", "pyglet", ".", "font", ".", "have_font", "(", "'Secret Code'", ")", ":", "\n", "            ", "font", "=", "\"Secret Code\"", "\n", "", "else", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "label", "=", "pyglet", ".", "text", ".", "Label", "(", "text", ",", "\n", "font_name", "=", "font", ",", "\n", "color", "=", "(", "0", ",", "0", ",", "0", ",", "255", ")", ",", "\n", "font_size", "=", "20", ",", "\n", "x", "=", "0", ",", "y", "=", "self", ".", "idx", "*", "40", "+", "20", ",", "\n", "anchor_x", "=", "\"left\"", ",", "anchor_y", "=", "\"bottom\"", ")", "\n", "\n", "self", ".", "label", ".", "draw", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.FilledPolygon.__init__": [[298, 301], ["rendering.Geom.__init__"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "v", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "self", ".", "v", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.FilledPolygon.render1": [[302, 319], ["glEnd", "glColor4f", "glBegin", "glEnd", "len", "glBegin", "glVertex3f", "glVertex3f", "len", "glBegin", "glBegin"], "methods", ["None"], ["", "def", "render1", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "v", ")", "==", "4", ":", "\n", "            ", "glBegin", "(", "GL_QUADS", ")", "\n", "", "elif", "len", "(", "self", ".", "v", ")", ">", "4", ":", "\n", "            ", "glBegin", "(", "GL_POLYGON", ")", "\n", "", "else", ":", "\n", "            ", "glBegin", "(", "GL_TRIANGLES", ")", "\n", "", "for", "p", "in", "self", ".", "v", ":", "\n", "            ", "glVertex3f", "(", "p", "[", "0", "]", ",", "p", "[", "1", "]", ",", "0", ")", "# draw each vertex", "\n", "", "glEnd", "(", ")", "\n", "\n", "color", "=", "(", "self", ".", "_color", ".", "vec4", "[", "0", "]", "*", "0.5", ",", "self", ".", "_color", ".", "vec4", "[", "1", "]", "*", "0.5", ",", "self", ".", "_color", ".", "vec4", "[", "2", "]", "*", "0.5", ",", "self", ".", "_color", ".", "vec4", "[", "3", "]", "*", "0.5", ")", "\n", "glColor4f", "(", "*", "color", ")", "\n", "glBegin", "(", "GL_LINE_LOOP", ")", "\n", "for", "p", "in", "self", ".", "v", ":", "\n", "            ", "glVertex3f", "(", "p", "[", "0", "]", ",", "p", "[", "1", "]", ",", "0", ")", "# draw each vertex", "\n", "", "glEnd", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Compound.__init__": [[354, 359], ["rendering.Geom.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gs", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "self", ".", "gs", "=", "gs", "\n", "for", "g", "in", "self", ".", "gs", ":", "\n", "            ", "g", ".", "attrs", "=", "[", "a", "for", "a", "in", "g", ".", "attrs", "if", "not", "isinstance", "(", "a", ",", "Color", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Compound.render1": [[360, 363], ["g.render"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render"], ["", "", "def", "render1", "(", "self", ")", ":", "\n", "        ", "for", "g", "in", "self", ".", "gs", ":", "\n", "            ", "g", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.PolyLine.__init__": [[366, 372], ["rendering.Geom.__init__", "rendering.LineWidth", "rendering.PolyLine.add_attr"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.add_attr"], ["    ", "def", "__init__", "(", "self", ",", "v", ",", "close", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "self", ".", "v", "=", "v", "\n", "self", ".", "close", "=", "close", "\n", "self", ".", "linewidth", "=", "LineWidth", "(", "1", ")", "\n", "self", ".", "add_attr", "(", "self", ".", "linewidth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.PolyLine.render1": [[373, 378], ["glBegin", "glEnd", "glVertex3f"], "methods", ["None"], ["", "def", "render1", "(", "self", ")", ":", "\n", "        ", "glBegin", "(", "GL_LINE_LOOP", "if", "self", ".", "close", "else", "GL_LINE_STRIP", ")", "\n", "for", "p", "in", "self", ".", "v", ":", "\n", "            ", "glVertex3f", "(", "p", "[", "0", "]", ",", "p", "[", "1", "]", ",", "0", ")", "# draw each vertex", "\n", "", "glEnd", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.PolyLine.set_linewidth": [[379, 381], ["None"], "methods", ["None"], ["", "def", "set_linewidth", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "linewidth", ".", "stroke", "=", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Line.__init__": [[384, 390], ["rendering.Geom.__init__", "rendering.LineWidth", "rendering.Line.add_attr"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.add_attr"], ["    ", "def", "__init__", "(", "self", ",", "start", "=", "(", "0.0", ",", "0.0", ")", ",", "end", "=", "(", "0.0", ",", "0.0", ")", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "self", ".", "start", "=", "start", "\n", "self", ".", "end", "=", "end", "\n", "self", ".", "linewidth", "=", "LineWidth", "(", "1", ")", "\n", "self", ".", "add_attr", "(", "self", ".", "linewidth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Line.render1": [[391, 396], ["glBegin", "glVertex2f", "glVertex2f", "glEnd"], "methods", ["None"], ["", "def", "render1", "(", "self", ")", ":", "\n", "        ", "glBegin", "(", "GL_LINES", ")", "\n", "glVertex2f", "(", "*", "self", ".", "start", ")", "\n", "glVertex2f", "(", "*", "self", ".", "end", ")", "\n", "glEnd", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Image.__init__": [[399, 406], ["rendering.Geom.__init__", "pyglet.image.load"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__", "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load"], ["    ", "def", "__init__", "(", "self", ",", "fname", ",", "width", ",", "height", ")", ":", "\n", "        ", "Geom", ".", "__init__", "(", "self", ")", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "height", "=", "height", "\n", "img", "=", "pyglet", ".", "image", ".", "load", "(", "fname", ")", "\n", "self", ".", "img", "=", "img", "\n", "self", ".", "flip", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Image.render1": [[407, 409], ["rendering.Image.img.blit"], "methods", ["None"], ["", "def", "render1", "(", "self", ")", ":", "\n", "        ", "self", ".", "img", ".", "blit", "(", "-", "self", ".", "width", "/", "2", ",", "-", "self", ".", "height", "/", "2", ",", "width", "=", "self", ".", "width", ",", "height", "=", "self", ".", "height", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.__init__": [[412, 416], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "display", "=", "None", ")", ":", "\n", "        ", "self", ".", "window", "=", "None", "\n", "self", ".", "isopen", "=", "False", "\n", "self", ".", "display", "=", "display", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.imshow": [[417, 431], ["pyglet.image.ImageData", "rendering.SimpleImageViewer.window.clear", "rendering.SimpleImageViewer.window.switch_to", "rendering.SimpleImageViewer.window.dispatch_events", "pyglet.image.ImageData.blit", "rendering.SimpleImageViewer.window.flip", "pyglet.window.Window", "arr.tobytes"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.utils.buffer.ReplayBuffer.clear"], ["", "def", "imshow", "(", "self", ",", "arr", ")", ":", "\n", "        ", "if", "self", ".", "window", "is", "None", ":", "\n", "            ", "height", ",", "width", ",", "channels", "=", "arr", ".", "shape", "\n", "self", ".", "window", "=", "pyglet", ".", "window", ".", "Window", "(", "width", "=", "width", ",", "height", "=", "height", ",", "display", "=", "self", ".", "display", ")", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "height", "=", "height", "\n", "self", ".", "isopen", "=", "True", "\n", "", "assert", "arr", ".", "shape", "==", "(", "self", ".", "height", ",", "self", ".", "width", ",", "3", ")", ",", "\"You passed in an image with the wrong number shape\"", "\n", "image", "=", "pyglet", ".", "image", ".", "ImageData", "(", "self", ".", "width", ",", "self", ".", "height", ",", "'RGB'", ",", "arr", ".", "tobytes", "(", ")", ",", "pitch", "=", "self", ".", "width", "*", "-", "3", ")", "\n", "self", ".", "window", ".", "clear", "(", ")", "\n", "self", ".", "window", ".", "switch_to", "(", ")", "\n", "self", ".", "window", ".", "dispatch_events", "(", ")", "\n", "image", ".", "blit", "(", "0", ",", "0", ")", "\n", "self", ".", "window", ".", "flip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close": [[432, 436], ["rendering.SimpleImageViewer.window.close"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "isopen", ":", "\n", "            ", "self", ".", "window", ".", "close", "(", ")", "\n", "self", ".", "isopen", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.__del__": [[437, 439], ["rendering.SimpleImageViewer.close"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.SimpleImageViewer.close"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.get_display": [[36, 48], ["isinstance", "pyglet.canvas.Display", "gym.error.Error"], "function", ["None"], ["def", "get_display", "(", "spec", ")", ":", "\n", "    ", "\"\"\"Convert a display specification (such as :0) into an actual Display\n    object.\n\n    Pyglet only supports multiple Displays on Linux.\n    \"\"\"", "\n", "if", "spec", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "elif", "isinstance", "(", "spec", ",", "str", ")", ":", "\n", "        ", "return", "pyglet", ".", "canvas", ".", "Display", "(", "spec", ")", "\n", "", "else", ":", "\n", "        ", "raise", "error", ".", "Error", "(", "f'Invalid display specification: {spec}. (Must be a string like :0 or None.)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering._add_attrs": [[167, 172], ["geom.set_color", "geom.set_linewidth"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.set_color", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.PolyLine.set_linewidth"], ["", "", "def", "_add_attrs", "(", "geom", ",", "attrs", ")", ":", "\n", "    ", "if", "\"color\"", "in", "attrs", ":", "\n", "        ", "geom", ".", "set_color", "(", "*", "attrs", "[", "\"color\"", "]", ")", "\n", "", "if", "\"linewidth\"", "in", "attrs", ":", "\n", "        ", "geom", ".", "set_linewidth", "(", "attrs", "[", "\"linewidth\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_circle": [[321, 330], ["range", "points.append", "rendering.FilledPolygon", "rendering.PolyLine", "math.cos", "math.sin"], "function", ["None"], ["", "", "def", "make_circle", "(", "radius", "=", "10", ",", "res", "=", "30", ",", "filled", "=", "True", ")", ":", "\n", "    ", "points", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "res", ")", ":", "\n", "        ", "ang", "=", "2", "*", "math", ".", "pi", "*", "i", "/", "res", "\n", "points", ".", "append", "(", "(", "math", ".", "cos", "(", "ang", ")", "*", "radius", ",", "math", ".", "sin", "(", "ang", ")", "*", "radius", ")", ")", "\n", "", "if", "filled", ":", "\n", "        ", "return", "FilledPolygon", "(", "points", ")", "\n", "", "else", ":", "\n", "        ", "return", "PolyLine", "(", "points", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_polygon": [[332, 337], ["rendering.FilledPolygon", "rendering.PolyLine"], "function", ["None"], ["", "", "def", "make_polygon", "(", "v", ",", "filled", "=", "True", ")", ":", "\n", "    ", "if", "filled", ":", "\n", "        ", "return", "FilledPolygon", "(", "v", ")", "\n", "", "else", ":", "\n", "        ", "return", "PolyLine", "(", "v", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_polyline": [[339, 341], ["rendering.PolyLine"], "function", ["None"], ["", "", "def", "make_polyline", "(", "v", ")", ":", "\n", "    ", "return", "PolyLine", "(", "v", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_capsule": [[343, 351], ["rendering.make_polygon", "rendering.make_circle", "rendering.make_circle", "make_circle.add_attr", "rendering.Compound", "rendering.Transform"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_polygon", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_circle", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_circle", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.add_attr"], ["", "def", "make_capsule", "(", "length", ",", "width", ")", ":", "\n", "    ", "l", ",", "r", ",", "t", ",", "b", "=", "0", ",", "length", ",", "width", "/", "2", ",", "-", "width", "/", "2", "\n", "box", "=", "make_polygon", "(", "[", "(", "l", ",", "b", ")", ",", "(", "l", ",", "t", ")", ",", "(", "r", ",", "t", ")", ",", "(", "r", ",", "b", ")", "]", ")", "\n", "circ0", "=", "make_circle", "(", "width", "/", "2", ")", "\n", "circ1", "=", "make_circle", "(", "width", "/", "2", ")", "\n", "circ1", ".", "add_attr", "(", "Transform", "(", "translation", "=", "(", "length", ",", "0", ")", ")", ")", "\n", "geom", "=", "Compound", "(", "[", "box", ",", "circ0", ",", "circ1", "]", ")", "\n", "return", "geom", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.__init__": [[26, 30], ["numpy.array", "numpy.array"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "array_of_param_array", ")", ":", "\n", "        ", "self", ".", "low", "=", "np", ".", "array", "(", "[", "x", "[", "0", "]", "for", "x", "in", "array_of_param_array", "]", ")", "\n", "self", ".", "high", "=", "np", ".", "array", "(", "[", "x", "[", "1", "]", "for", "x", "in", "array_of_param_array", "]", ")", "\n", "self", ".", "num_discrete_space", "=", "self", ".", "low", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.sample": [[31, 39], ["numpy.random.RandomState().rand", "int", "numpy.random.RandomState", "numpy.floor", "numpy.multiply"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a array with one sample from each discrete action space\"\"\"", "\n", "# For each row: round(random .* (max - min) + min, 0)", "\n", "random_array", "=", "np", ".", "random", ".", "RandomState", "(", ")", ".", "rand", "(", "self", ".", "num_discrete_space", ")", "\n", "return", "[", "\n", "int", "(", "x", ")", "\n", "for", "x", "in", "np", ".", "floor", "(", "\n", "np", ".", "multiply", "(", "(", "self", ".", "high", "-", "self", ".", "low", "+", "1.0", ")", ",", "random_array", ")", "+", "self", ".", "low", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.contains": [[42, 47], ["len", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "contains", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "(", "\n", "len", "(", "x", ")", "==", "self", ".", "num_discrete_space", "\n", "and", "(", "np", ".", "array", "(", "x", ")", ">=", "self", ".", "low", ")", ".", "all", "(", ")", "\n", "and", "(", "np", ".", "array", "(", "x", ")", "<=", "self", ".", "high", ")", ".", "all", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.shape": [[49, 52], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "shape", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_discrete_space", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.__repr__": [[53, 55], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"MultiDiscrete(%d)\"", "%", "self", ".", "num_discrete_space", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.multi_discrete.MultiDiscrete.__eq__": [[56, 59], ["numpy.array_equal", "numpy.array_equal"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "np", ".", "array_equal", "(", "self", ".", "low", ",", "other", ".", "low", ")", "and", "np", ".", "array_equal", "(", "\n", "self", ".", "high", ",", "other", ".", "high", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.Policy.__init__": [[6, 8], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.Policy.action": [[9, 11], ["NotImplementedError"], "methods", ["None"], ["", "def", "action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.InteractivePolicy.__init__": [[16, 25], ["policy.Policy.__init__", "range", "range"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "agent_index", ")", ":", "\n", "        ", "super", "(", "InteractivePolicy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "env", "=", "env", "\n", "# hard-coded keyboard events", "\n", "self", ".", "move", "=", "[", "False", "for", "i", "in", "range", "(", "4", ")", "]", "\n", "self", ".", "comm", "=", "[", "False", "for", "i", "in", "range", "(", "env", ".", "world", ".", "dim_c", ")", "]", "\n", "# register keyboard events with this environment's window", "\n", "env", ".", "viewers", "[", "agent_index", "]", ".", "window", ".", "on_key_press", "=", "self", ".", "key_press", "\n", "env", ".", "viewers", "[", "agent_index", "]", ".", "window", ".", "on_key_release", "=", "self", ".", "key_release", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.InteractivePolicy.action": [[26, 51], ["numpy.concatenate", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["", "def", "action", "(", "self", ",", "obs", ")", ":", "\n", "# ignore observation and just act based on keyboard events", "\n", "        ", "if", "self", ".", "env", ".", "discrete_action_input", ":", "\n", "            ", "u", "=", "0", "\n", "if", "self", ".", "move", "[", "0", "]", ":", "\n", "                ", "u", "=", "1", "\n", "", "if", "self", ".", "move", "[", "1", "]", ":", "\n", "                ", "u", "=", "2", "\n", "", "if", "self", ".", "move", "[", "2", "]", ":", "\n", "                ", "u", "=", "4", "\n", "", "if", "self", ".", "move", "[", "3", "]", ":", "\n", "                ", "u", "=", "3", "\n", "", "", "else", ":", "\n", "            ", "u", "=", "np", ".", "zeros", "(", "5", ")", "# 5-d because of no-move action", "\n", "if", "self", ".", "move", "[", "0", "]", ":", "\n", "                ", "u", "[", "1", "]", "+=", "1.0", "\n", "", "if", "self", ".", "move", "[", "1", "]", ":", "\n", "                ", "u", "[", "2", "]", "+=", "1.0", "\n", "", "if", "self", ".", "move", "[", "3", "]", ":", "\n", "                ", "u", "[", "3", "]", "+=", "1.0", "\n", "", "if", "self", ".", "move", "[", "2", "]", ":", "\n", "                ", "u", "[", "4", "]", "+=", "1.0", "\n", "", "if", "True", "not", "in", "self", ".", "move", ":", "\n", "                ", "u", "[", "0", "]", "+=", "1.0", "\n", "", "", "return", "np", ".", "concatenate", "(", "[", "u", ",", "np", ".", "zeros", "(", "self", ".", "env", ".", "world", ".", "dim_c", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.InteractivePolicy.key_press": [[53, 62], ["None"], "methods", ["None"], ["", "def", "key_press", "(", "self", ",", "k", ",", "mod", ")", ":", "\n", "        ", "if", "k", "==", "key", ".", "LEFT", ":", "\n", "            ", "self", ".", "move", "[", "0", "]", "=", "True", "\n", "", "if", "k", "==", "key", ".", "RIGHT", ":", "\n", "            ", "self", ".", "move", "[", "1", "]", "=", "True", "\n", "", "if", "k", "==", "key", ".", "UP", ":", "\n", "            ", "self", ".", "move", "[", "2", "]", "=", "True", "\n", "", "if", "k", "==", "key", ".", "DOWN", ":", "\n", "            ", "self", ".", "move", "[", "3", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.policy.InteractivePolicy.key_release": [[63, 72], ["None"], "methods", ["None"], ["", "", "def", "key_release", "(", "self", ",", "k", ",", "mod", ")", ":", "\n", "        ", "if", "k", "==", "key", ".", "LEFT", ":", "\n", "            ", "self", ".", "move", "[", "0", "]", "=", "False", "\n", "", "if", "k", "==", "key", ".", "RIGHT", ":", "\n", "            ", "self", ".", "move", "[", "1", "]", "=", "False", "\n", "", "if", "k", "==", "key", ".", "UP", ":", "\n", "            ", "self", ".", "move", "[", "2", "]", "=", "False", "\n", "", "if", "k", "==", "key", ".", "DOWN", ":", "\n", "            ", "self", ".", "move", "[", "3", "]", "=", "False", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.scenario.BaseScenario.make_world": [[6, 8], ["NotImplementedError"], "methods", ["None"], ["    ", "def", "make_world", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.scenario.BaseScenario.reset_world": [[10, 12], ["NotImplementedError"], "methods", ["None"], ["", "def", "reset_world", "(", "self", ",", "world", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.__init__.make_env": [[35, 78], ["scenarios.load().Scenario", "scenarios.load().Scenario.make_world", "MultiAgentEnv", "MultiAgentEnv", "scenarios.load"], "function", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.make_world", "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load"], []], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.__init__": [[13, 110], ["len", "environment.MultiAgentEnv._reset_render", "hasattr", "hasattr", "len", "environment.MultiAgentEnv.observation_space.append", "numpy.zeros", "gym.spaces.Discrete", "gym.spaces.Box", "total_action_space.append", "gym.spaces.Discrete", "gym.spaces.Box", "range", "len", "all", "environment.MultiAgentEnv.action_space.append", "environment.MultiAgentEnv.action_space.append", "observation_callback", "gym.spaces.Box", "total_action_space.append", "multiagent.multi_discrete.MultiDiscrete", "gym.spaces.Tuple", "isinstance"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._reset_render"], ["def", "__init__", "(", "\n", "self", ",", "\n", "world", ",", "\n", "reset_callback", "=", "None", ",", "\n", "reward_callback", "=", "None", ",", "\n", "observation_callback", "=", "None", ",", "\n", "info_callback", "=", "None", ",", "\n", "done_callback", "=", "None", ",", "\n", "shared_viewer", "=", "True", ",", "\n", "discrete_action", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "self", ".", "world", "=", "world", "\n", "self", ".", "agents", "=", "self", ".", "world", ".", "policy_agents", "\n", "# set required vectorized gym env property", "\n", "self", ".", "n", "=", "len", "(", "world", ".", "policy_agents", ")", "\n", "# scenario callbacks", "\n", "self", ".", "reset_callback", "=", "reset_callback", "\n", "self", ".", "reward_callback", "=", "reward_callback", "\n", "self", ".", "observation_callback", "=", "observation_callback", "\n", "self", ".", "info_callback", "=", "info_callback", "\n", "self", ".", "done_callback", "=", "done_callback", "\n", "# environment parameters", "\n", "self", ".", "discrete_action_space", "=", "discrete_action", "\n", "# if true, action is a number 0...N, otherwise action is a one-hot N-dimensional vector", "\n", "self", ".", "discrete_action_input", "=", "False", "\n", "# if true, even the action is continuous, action will be performed discretely", "\n", "self", ".", "force_discrete_action", "=", "(", "\n", "world", ".", "discrete_action", "if", "hasattr", "(", "world", ",", "\"discrete_action\"", ")", "else", "False", "\n", ")", "\n", "# if true, every agent has the same reward", "\n", "self", ".", "shared_reward", "=", "(", "\n", "world", ".", "collaborative", "if", "hasattr", "(", "world", ",", "\"collaborative\"", ")", "else", "False", "\n", ")", "\n", "self", ".", "time", "=", "0", "\n", "\n", "# configure spaces", "\n", "self", ".", "action_space", "=", "[", "]", "\n", "self", ".", "observation_space", "=", "[", "]", "\n", "for", "agent", "in", "self", ".", "agents", ":", "\n", "            ", "total_action_space", "=", "[", "]", "\n", "# physical action space", "\n", "if", "self", ".", "discrete_action_space", ":", "\n", "                ", "u_action_space", "=", "spaces", ".", "Discrete", "(", "world", ".", "dim_p", "*", "2", "+", "1", ")", "\n", "", "else", ":", "\n", "                ", "u_action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "agent", ".", "u_range", ",", "\n", "high", "=", "+", "agent", ".", "u_range", ",", "\n", "shape", "=", "(", "world", ".", "dim_p", ",", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ",", "\n", ")", "\n", "", "if", "agent", ".", "movable", ":", "\n", "                ", "total_action_space", ".", "append", "(", "u_action_space", ")", "\n", "# communication action space", "\n", "", "if", "self", ".", "discrete_action_space", ":", "\n", "                ", "c_action_space", "=", "spaces", ".", "Discrete", "(", "world", ".", "dim_c", ")", "\n", "", "else", ":", "\n", "                ", "c_action_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "0.0", ",", "high", "=", "1.0", ",", "shape", "=", "(", "world", ".", "dim_c", ",", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "", "if", "not", "agent", ".", "silent", ":", "\n", "                ", "for", "_", "in", "range", "(", "agent", ".", "channel", ")", ":", "\n", "                    ", "total_action_space", ".", "append", "(", "c_action_space", ")", "\n", "# total action space", "\n", "", "", "if", "len", "(", "total_action_space", ")", ">", "1", ":", "\n", "# all action spaces are discrete, so simplify to MultiDiscrete action space", "\n", "                ", "if", "all", "(", "\n", "[", "\n", "isinstance", "(", "act_space", ",", "spaces", ".", "Discrete", ")", "\n", "for", "act_space", "in", "total_action_space", "\n", "]", "\n", ")", ":", "\n", "                    ", "act_space", "=", "MultiDiscrete", "(", "\n", "[", "[", "0", ",", "act_space", ".", "n", "-", "1", "]", "for", "act_space", "in", "total_action_space", "]", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "act_space", "=", "spaces", ".", "Tuple", "(", "total_action_space", ")", "\n", "", "self", ".", "action_space", ".", "append", "(", "act_space", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "action_space", ".", "append", "(", "total_action_space", "[", "0", "]", ")", "\n", "# observation space", "\n", "", "obs_dim", "=", "len", "(", "observation_callback", "(", "agent", ",", "self", ".", "world", ")", ")", "\n", "self", ".", "observation_space", ".", "append", "(", "\n", "spaces", ".", "Box", "(", "\n", "low", "=", "-", "np", ".", "inf", ",", "high", "=", "+", "np", ".", "inf", ",", "shape", "=", "(", "obs_dim", ",", ")", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", ")", "\n", "agent", ".", "action", ".", "c", "=", "np", ".", "zeros", "(", "self", ".", "world", ".", "dim_c", "*", "agent", ".", "channel", ")", "\n", "\n", "# rendering", "\n", "", "self", ".", "viewer", "=", "None", "\n", "self", ".", "shared_viewer", "=", "shared_viewer", "\n", "if", "self", ".", "shared_viewer", ":", "\n", "            ", "self", ".", "viewers", "=", "[", "None", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "viewers", "=", "[", "None", "]", "*", "self", ".", "n", "\n", "", "self", ".", "_reset_render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.step": [[111, 136], ["enumerate", "environment.MultiAgentEnv.world.step", "numpy.sum", "environment.MultiAgentEnv._set_action", "obs_n.append", "reward_n.append", "done_n.append", "info_n[].append", "environment.MultiAgentEnv._get_obs", "environment.MultiAgentEnv._get_reward", "environment.MultiAgentEnv._get_done", "environment.MultiAgentEnv._get_info"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._set_action", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_obs", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_reward", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_done", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_info"], ["", "def", "step", "(", "self", ",", "action_n", ")", ":", "\n", "        ", "obs_n", "=", "[", "]", "\n", "reward_n", "=", "[", "]", "\n", "done_n", "=", "[", "]", "\n", "info_n", "=", "{", "\"n\"", ":", "[", "]", "}", "\n", "self", ".", "agents", "=", "self", ".", "world", ".", "policy_agents", "\n", "# set action for each agent", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "self", ".", "agents", ")", ":", "\n", "            ", "self", ".", "_set_action", "(", "action_n", "[", "i", "]", ",", "agent", ",", "self", ".", "action_space", "[", "i", "]", ")", "\n", "# advance world state", "\n", "", "self", ".", "world", ".", "step", "(", ")", "\n", "# record observation for each agent", "\n", "for", "agent", "in", "self", ".", "agents", ":", "\n", "            ", "obs_n", ".", "append", "(", "self", ".", "_get_obs", "(", "agent", ")", ")", "\n", "reward_n", ".", "append", "(", "self", ".", "_get_reward", "(", "agent", ")", ")", "\n", "done_n", ".", "append", "(", "self", ".", "_get_done", "(", "agent", ")", ")", "\n", "\n", "info_n", "[", "\"n\"", "]", ".", "append", "(", "self", ".", "_get_info", "(", "agent", ")", ")", "\n", "\n", "# all agents get total reward in cooperative case", "\n", "", "reward", "=", "np", ".", "sum", "(", "reward_n", ")", "\n", "if", "self", ".", "shared_reward", ":", "\n", "            ", "reward_n", "=", "[", "reward", "]", "*", "self", ".", "n", "\n", "\n", "", "return", "obs_n", ",", "reward_n", ",", "done_n", ",", "info_n", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.reset": [[137, 148], ["environment.MultiAgentEnv.reset_callback", "environment.MultiAgentEnv._reset_render", "obs_n.append", "environment.MultiAgentEnv._get_obs"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._reset_render", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# reset world", "\n", "        ", "self", ".", "reset_callback", "(", "self", ".", "world", ")", "\n", "# reset renderer", "\n", "self", ".", "_reset_render", "(", ")", "\n", "# record observations for each agent", "\n", "obs_n", "=", "[", "]", "\n", "self", ".", "agents", "=", "self", ".", "world", ".", "policy_agents", "\n", "for", "agent", "in", "self", ".", "agents", ":", "\n", "            ", "obs_n", ".", "append", "(", "self", ".", "_get_obs", "(", "agent", ")", ")", "\n", "", "return", "obs_n", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_info": [[150, 154], ["environment.MultiAgentEnv.info_callback"], "methods", ["None"], ["", "def", "_get_info", "(", "self", ",", "agent", ")", ":", "\n", "        ", "if", "self", ".", "info_callback", "is", "None", ":", "\n", "            ", "return", "{", "}", "\n", "", "return", "self", ".", "info_callback", "(", "agent", ",", "self", ".", "world", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_obs": [[156, 160], ["environment.MultiAgentEnv.observation_callback", "numpy.zeros"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ",", "agent", ")", ":", "\n", "        ", "if", "self", ".", "observation_callback", "is", "None", ":", "\n", "            ", "return", "np", ".", "zeros", "(", "0", ")", "\n", "", "return", "self", ".", "observation_callback", "(", "agent", ",", "self", ".", "world", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_done": [[163, 167], ["environment.MultiAgentEnv.done_callback"], "methods", ["None"], ["", "def", "_get_done", "(", "self", ",", "agent", ")", ":", "\n", "        ", "if", "self", ".", "done_callback", "is", "None", ":", "\n", "            ", "return", "False", "\n", "", "return", "self", ".", "done_callback", "(", "agent", ",", "self", ".", "world", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._get_reward": [[169, 173], ["environment.MultiAgentEnv.reward_callback"], "methods", ["None"], ["", "def", "_get_reward", "(", "self", ",", "agent", ")", ":", "\n", "        ", "if", "self", ".", "reward_callback", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "return", "self", ".", "reward_callback", "(", "agent", ",", "self", ".", "world", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._set_action": [[175, 232], ["numpy.zeros", "numpy.zeros", "isinstance", "len", "act.append", "numpy.zeros", "numpy.zeros", "len", "numpy.concatenate", "numpy.argmax"], "methods", ["None"], ["", "def", "_set_action", "(", "self", ",", "action", ",", "agent", ",", "action_space", ",", "time", "=", "None", ")", ":", "\n", "        ", "agent", ".", "action", ".", "u", "=", "np", ".", "zeros", "(", "self", ".", "world", ".", "dim_p", ")", "\n", "agent", ".", "action", ".", "c", "=", "np", ".", "zeros", "(", "self", ".", "world", ".", "dim_c", "*", "agent", ".", "channel", ")", "\n", "# process action", "\n", "if", "isinstance", "(", "action_space", ",", "MultiDiscrete", ")", ":", "\n", "            ", "act", "=", "[", "]", "\n", "size", "=", "action_space", ".", "high", "-", "action_space", ".", "low", "+", "1", "\n", "index", "=", "0", "\n", "for", "s", "in", "size", ":", "\n", "                ", "act", ".", "append", "(", "action", "[", "index", ":", "(", "index", "+", "s", ")", "]", ")", "\n", "index", "+=", "s", "\n", "", "action", "=", "act", "\n", "", "else", ":", "\n", "            ", "action", "=", "[", "action", "]", "\n", "\n", "", "if", "agent", ".", "movable", ":", "\n", "# physical action", "\n", "            ", "if", "self", ".", "discrete_action_input", ":", "\n", "                ", "agent", ".", "action", ".", "u", "=", "np", ".", "zeros", "(", "self", ".", "world", ".", "dim_p", ")", "\n", "# process discrete action", "\n", "if", "action", "[", "0", "]", "==", "1", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "[", "0", "]", "=", "-", "1.0", "\n", "", "if", "action", "[", "0", "]", "==", "2", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "[", "0", "]", "=", "+", "1.0", "\n", "", "if", "action", "[", "0", "]", "==", "3", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "[", "1", "]", "=", "-", "1.0", "\n", "", "if", "action", "[", "0", "]", "==", "4", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "[", "1", "]", "=", "+", "1.0", "\n", "", "", "else", ":", "\n", "                ", "if", "self", ".", "force_discrete_action", ":", "\n", "                    ", "d", "=", "np", ".", "argmax", "(", "action", "[", "0", "]", ")", "\n", "action", "[", "0", "]", "[", ":", "]", "=", "0.0", "\n", "action", "[", "0", "]", "[", "d", "]", "=", "1.0", "\n", "", "if", "self", ".", "discrete_action_space", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "[", "0", "]", "+=", "action", "[", "0", "]", "[", "1", "]", "-", "action", "[", "0", "]", "[", "2", "]", "\n", "agent", ".", "action", ".", "u", "[", "1", "]", "+=", "action", "[", "0", "]", "[", "3", "]", "-", "action", "[", "0", "]", "[", "4", "]", "\n", "", "else", ":", "\n", "                    ", "agent", ".", "action", ".", "u", "=", "action", "[", "0", "]", "\n", "", "", "sensitivity", "=", "5.0", "\n", "if", "agent", ".", "accel", "is", "not", "None", ":", "\n", "                ", "sensitivity", "=", "agent", ".", "accel", "\n", "", "agent", ".", "action", ".", "u", "*=", "sensitivity", "\n", "action", "=", "action", "[", "1", ":", "]", "\n", "", "if", "not", "agent", ".", "silent", ":", "\n", "# communication action", "\n", "            ", "if", "self", ".", "discrete_action_input", ":", "\n", "                ", "agent", ".", "action", ".", "c", "=", "np", ".", "zeros", "(", "self", ".", "world", ".", "dim_c", "*", "agent", ".", "channel", ")", "\n", "agent", ".", "action", ".", "c", "[", "action", "[", "0", "]", "]", "=", "1.0", "\n", "", "else", ":", "\n", "                ", "agent", ".", "action", ".", "c", "=", "action", "[", "0", "]", "\n", "", "action", "=", "action", "[", "1", ":", "]", "\n", "if", "len", "(", "action", ")", "==", "1", ":", "# comm 2", "\n", "                ", "agent", ".", "action", ".", "c", "=", "np", ".", "concatenate", "(", "[", "agent", ".", "action", ".", "c", ",", "action", "[", "0", "]", "]", ",", "axis", "=", "0", ")", "\n", "# print(agent.action.c)", "\n", "", "action", "=", "action", "[", "1", ":", "]", "\n", "# make sure we used all elements of action", "\n", "", "assert", "len", "(", "action", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._reset_render": [[234, 237], ["None"], "methods", ["None"], ["", "def", "_reset_render", "(", "self", ")", ":", "\n", "        ", "self", ".", "render_geoms", "=", "None", "\n", "self", ".", "render_geoms_xform", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.render": [[239, 312], ["enumerate", "environment.MultiAgentEnv.viewer.set_max_size", "enumerate", "time.sleep", "environment.MultiAgentEnv.viewer.render", "rendering.Viewer", "numpy.all", "environment.MultiAgentEnv.viewer.text_lines[].set_text", "numpy.max", "environment.MultiAgentEnv.render_geoms_xform[].set_translation", "rendering.make_circle", "rendering.Transform", "rendering.make_circle.add_attr", "environment.MultiAgentEnv.render_geoms.append", "environment.MultiAgentEnv.render_geoms_xform.append", "environment.MultiAgentEnv.viewer.add_geom", "range", "numpy.abs", "rendering.make_circle.set_color", "rendering.make_circle.set_color", "range", "word.append", "numpy.array", "rendering.TextLine", "environment.MultiAgentEnv.viewer.text_lines.append", "numpy.argmax"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.set_max_size", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.TextLine.set_text", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Transform.set_translation", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.make_circle", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.add_attr", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Viewer.add_geom", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.set_color", "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.rendering.Geom.set_color"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ")", ":", "\n", "        ", "from", ".", "import", "rendering", "\n", "if", "self", ".", "viewer", "is", "None", ":", "\n", "            ", "self", ".", "viewer", "=", "rendering", ".", "Viewer", "(", "700", ",", "700", ")", "\n", "\n", "# create rendering geometry", "\n", "", "if", "self", ".", "render_geoms", "is", "None", ":", "\n", "# import rendering only if we need it (and don't import for headless machines)", "\n", "# from gym.envs.classic_control import rendering", "\n", "# from multiagent._mpe_utils import rendering", "\n", "            ", "self", ".", "render_geoms", "=", "[", "]", "\n", "self", ".", "render_geoms_xform", "=", "[", "]", "\n", "for", "entity", "in", "self", ".", "world", ".", "entities", ":", "\n", "                ", "geom", "=", "rendering", ".", "make_circle", "(", "entity", ".", "size", ")", "\n", "xform", "=", "rendering", ".", "Transform", "(", ")", "\n", "if", "\"agent\"", "in", "entity", ".", "name", ":", "\n", "                    ", "geom", ".", "set_color", "(", "*", "entity", ".", "color", "[", ":", "3", "]", ",", "alpha", "=", "0.5", ")", "\n", "", "else", ":", "\n", "                    ", "geom", ".", "set_color", "(", "*", "entity", ".", "color", "[", ":", "3", "]", ")", "\n", "", "geom", ".", "add_attr", "(", "xform", ")", "\n", "self", ".", "render_geoms", ".", "append", "(", "geom", ")", "\n", "self", ".", "render_geoms_xform", ".", "append", "(", "xform", ")", "\n", "\n", "# add geoms to viewer", "\n", "", "self", ".", "viewer", ".", "geoms", "=", "[", "]", "\n", "for", "geom", "in", "self", ".", "render_geoms", ":", "\n", "                ", "self", ".", "viewer", ".", "add_geom", "(", "geom", ")", "\n", "\n", "", "self", ".", "viewer", ".", "text_lines", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "agent", "in", "self", ".", "world", ".", "agents", ":", "\n", "                ", "if", "not", "agent", ".", "silent", ":", "\n", "                    ", "for", "_", "in", "range", "(", "agent", ".", "channel", ")", ":", "\n", "                        ", "tline", "=", "rendering", ".", "TextLine", "(", "self", ".", "viewer", ".", "window", ",", "idx", ")", "\n", "self", ".", "viewer", ".", "text_lines", ".", "append", "(", "tline", ")", "\n", "idx", "+=", "1", "\n", "\n", "", "", "", "", "alphabet", "=", "\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"", "\n", "for", "idx", ",", "other", "in", "enumerate", "(", "self", ".", "world", ".", "agents", ")", ":", "\n", "            ", "if", "other", ".", "silent", ":", "\n", "                ", "continue", "\n", "", "if", "np", ".", "all", "(", "other", ".", "state", ".", "c", "==", "0", ")", ":", "\n", "                ", "word", "=", "\"_\"", "\n", "", "else", ":", "\n", "                ", "word", "=", "[", "]", "\n", "# print(other.state.c)", "\n", "for", "c_i", "in", "range", "(", "other", ".", "channel", ")", ":", "\n", "                    ", "word", ".", "append", "(", "\n", "alphabet", "[", "\n", "np", ".", "argmax", "(", "\n", "other", ".", "state", ".", "c", "[", "\n", "self", ".", "world", ".", "dim_c", "\n", "*", "c_i", ":", "self", ".", "world", ".", "dim_c", "\n", "*", "(", "c_i", "+", "1", ")", "\n", "]", "\n", ")", "\n", "]", "\n", ")", "\n", "\n", "", "", "message", "=", "other", ".", "name", "+", "\" sends \"", "+", "\",\"", ".", "join", "(", "word", ")", "+", "\"   \"", "\n", "\n", "self", ".", "viewer", ".", "text_lines", "[", "idx", "]", ".", "set_text", "(", "message", ")", "\n", "\n", "# update bounds to center around agent", "\n", "", "all_poses", "=", "[", "entity", ".", "state", ".", "p_pos", "for", "entity", "in", "self", ".", "world", ".", "entities", "]", "\n", "cam_range", "=", "np", ".", "max", "(", "np", ".", "abs", "(", "np", ".", "array", "(", "all_poses", ")", ")", ")", "+", "1", "\n", "self", ".", "viewer", ".", "set_max_size", "(", "cam_range", ")", "\n", "# update geometry positions", "\n", "for", "e", ",", "entity", "in", "enumerate", "(", "self", ".", "world", ".", "entities", ")", ":", "\n", "            ", "self", ".", "render_geoms_xform", "[", "e", "]", ".", "set_translation", "(", "*", "entity", ".", "state", ".", "p_pos", ")", "\n", "# render to display or array", "\n", "", "time", ".", "sleep", "(", "0.05", ")", "\n", "return", "self", ".", "viewer", ".", "render", "(", "return_rgb_array", "=", "mode", "==", "\"rgb_array\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv._make_receptor_locations": [[314, 332], ["numpy.linspace", "dx.append", "numpy.linspace", "numpy.linspace", "numpy.array", "numpy.linspace", "dx.append", "dx.append", "numpy.array", "numpy.array", "numpy.cos", "numpy.sin"], "methods", ["None"], ["", "def", "_make_receptor_locations", "(", "self", ",", "agent", ")", ":", "\n", "        ", "receptor_type", "=", "\"polar\"", "\n", "range_min", "=", "0.05", "*", "2.0", "\n", "range_max", "=", "1.00", "\n", "dx", "=", "[", "]", "\n", "# circular receptive field", "\n", "if", "receptor_type", "==", "\"polar\"", ":", "\n", "            ", "for", "angle", "in", "np", ".", "linspace", "(", "-", "np", ".", "pi", ",", "+", "np", ".", "pi", ",", "8", ",", "endpoint", "=", "False", ")", ":", "\n", "                ", "for", "distance", "in", "np", ".", "linspace", "(", "range_min", ",", "range_max", ",", "3", ")", ":", "\n", "                    ", "dx", ".", "append", "(", "distance", "*", "np", ".", "array", "(", "[", "np", ".", "cos", "(", "angle", ")", ",", "np", ".", "sin", "(", "angle", ")", "]", ")", ")", "\n", "# add origin", "\n", "", "", "dx", ".", "append", "(", "np", ".", "array", "(", "[", "0.0", ",", "0.0", "]", ")", ")", "\n", "# grid receptive field", "\n", "", "if", "receptor_type", "==", "\"grid\"", ":", "\n", "            ", "for", "x", "in", "np", ".", "linspace", "(", "-", "range_max", ",", "+", "range_max", ",", "5", ")", ":", "\n", "                ", "for", "y", "in", "np", ".", "linspace", "(", "-", "range_max", ",", "+", "range_max", ",", "5", ")", ":", "\n", "                    ", "dx", ".", "append", "(", "np", ".", "array", "(", "[", "x", ",", "y", "]", ")", ")", "\n", "", "", "", "return", "dx", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed": [[333, 335], ["numpy.random.seed"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.MultiAgentEnv.seed"], ["", "def", "seed", "(", "self", ",", "seed", ":", "int", "=", "0", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.__init__": [[342, 344], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "env_batch", ")", ":", "\n", "        ", "self", ".", "env_batch", "=", "env_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.n": [[345, 348], ["numpy.sum"], "methods", ["None"], ["", "@", "property", "\n", "def", "n", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "sum", "(", "[", "env", ".", "n", "for", "env", "in", "self", ".", "env_batch", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.action_space": [[349, 352], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env_batch", "[", "0", "]", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.observation_space": [[353, 356], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env_batch", "[", "0", "]", ".", "observation_space", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step": [[357, 371], ["env.step"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.step"], ["", "def", "step", "(", "self", ",", "action_n", ",", "time", ")", ":", "\n", "        ", "obs_n", "=", "[", "]", "\n", "reward_n", "=", "[", "]", "\n", "done_n", "=", "[", "]", "\n", "info_n", "=", "{", "\"n\"", ":", "[", "]", "}", "\n", "i", "=", "0", "\n", "for", "env", "in", "self", ".", "env_batch", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action_n", "[", "i", ":", "(", "i", "+", "env", ".", "n", ")", "]", ",", "time", ")", "\n", "i", "+=", "env", ".", "n", "\n", "obs_n", "+=", "obs", "\n", "# reward = [r / len(self.env_batch) for r in reward]", "\n", "reward_n", "+=", "reward", "\n", "done_n", "+=", "done", "\n", "", "return", "obs_n", ",", "reward_n", ",", "done_n", ",", "info_n", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset": [[372, 377], ["env.reset"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs_n", "=", "[", "]", "\n", "for", "env", "in", "self", ".", "env_batch", ":", "\n", "            ", "obs_n", "+=", "env", ".", "reset", "(", ")", "\n", "", "return", "obs_n", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render": [[379, 384], ["env.render"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.multiagent.environment.BatchMultiAgentEnv.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ",", "close", "=", "True", ")", ":", "\n", "        ", "results_n", "=", "[", "]", "\n", "for", "env", "in", "self", ".", "env_batch", ":", "\n", "            ", "results_n", "+=", "env", ".", "render", "(", "mode", ",", "close", ")", "\n", "", "return", "results_n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_speaker_listener.Scenario.make_world": [[7, 33], ["multiagent.core.World", "enumerate", "enumerate", "simple_speaker_listener.Scenario.reset_world", "multiagent.core.Agent", "multiagent.core.Landmark", "range", "range"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reset_world"], ["    ", "def", "make_world", "(", "self", ")", ":", "\n", "        ", "world", "=", "World", "(", ")", "\n", "# set any world properties first", "\n", "world", ".", "dim_c", "=", "3", "\n", "num_landmarks", "=", "3", "\n", "world", ".", "collaborative", "=", "True", "\n", "# add agents", "\n", "world", ".", "agents", "=", "[", "Agent", "(", ")", "for", "i", "in", "range", "(", "2", ")", "]", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "name", "=", "\"agent %d\"", "%", "i", "\n", "agent", ".", "collide", "=", "False", "\n", "agent", ".", "size", "=", "0.075", "\n", "# speaker", "\n", "", "world", ".", "agents", "[", "0", "]", ".", "movable", "=", "False", "\n", "# listener", "\n", "world", ".", "agents", "[", "1", "]", ".", "silent", "=", "True", "\n", "# add landmarks", "\n", "world", ".", "landmarks", "=", "[", "Landmark", "(", ")", "for", "i", "in", "range", "(", "num_landmarks", ")", "]", "\n", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "name", "=", "\"landmark %d\"", "%", "i", "\n", "landmark", ".", "collide", "=", "False", "\n", "landmark", ".", "movable", "=", "False", "\n", "landmark", ".", "size", "=", "0.04", "\n", "# make initial conditions", "\n", "", "self", ".", "reset_world", "(", "world", ")", "\n", "return", "world", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_speaker_listener.Scenario.reset_world": [[34, 61], ["numpy.random.choice", "enumerate", "numpy.array", "numpy.array", "numpy.array", "enumerate", "numpy.array", "numpy.array", "numpy.random.uniform", "numpy.zeros", "numpy.zeros", "numpy.random.uniform", "numpy.zeros"], "methods", ["None"], ["", "def", "reset_world", "(", "self", ",", "world", ")", ":", "\n", "# assign goals to agents", "\n", "        ", "for", "agent", "in", "world", ".", "agents", ":", "\n", "            ", "agent", ".", "goal_a", "=", "None", "\n", "agent", ".", "goal_b", "=", "None", "\n", "# want listener to go to the goal landmark", "\n", "", "world", ".", "agents", "[", "0", "]", ".", "goal_a", "=", "world", ".", "agents", "[", "1", "]", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_b", "=", "np", ".", "random", ".", "choice", "(", "world", ".", "landmarks", ")", "\n", "# random properties for agents", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "color", "=", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.25", "]", ")", "\n", "# random properties for landmarks", "\n", "", "world", ".", "landmarks", "[", "0", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.65", ",", "0.15", ",", "0.15", "]", ")", "\n", "world", ".", "landmarks", "[", "1", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.15", ",", "0.65", ",", "0.15", "]", ")", "\n", "world", ".", "landmarks", "[", "2", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.15", ",", "0.15", ",", "0.65", "]", ")", "\n", "# special colors for goals", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_a", ".", "color", "=", "world", ".", "agents", "[", "0", "]", ".", "goal_b", ".", "color", "+", "np", ".", "array", "(", "\n", "[", "0.45", ",", "0.45", ",", "0.45", "]", "\n", ")", "\n", "# set random initial states", "\n", "for", "agent", "in", "world", ".", "agents", ":", "\n", "            ", "agent", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "c", "=", "np", ".", "zeros", "(", "world", ".", "dim_c", ")", "\n", "", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "landmark", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_speaker_listener.Scenario.benchmark_data": [[62, 65], ["simple_speaker_listener.Scenario.reward"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reward"], ["", "", "def", "benchmark_data", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# returns data for benchmarking purposes", "\n", "        ", "return", "self", ".", "reward", "(", "agent", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_speaker_listener.Scenario.reward": [[66, 71], ["numpy.sum", "numpy.square"], "methods", ["None"], ["", "def", "reward", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# squared distance from listener to landmark", "\n", "        ", "a", "=", "world", ".", "agents", "[", "0", "]", "\n", "dist2", "=", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "goal_a", ".", "state", ".", "p_pos", "-", "a", ".", "goal_b", ".", "state", ".", "p_pos", ")", ")", "\n", "return", "-", "dist2", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_speaker_listener.Scenario.observation": [[72, 96], ["numpy.zeros", "entity_pos.append", "comm.append", "numpy.concatenate", "numpy.concatenate"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# goal color", "\n", "        ", "goal_color", "=", "np", ".", "zeros", "(", "world", ".", "dim_color", ")", "\n", "if", "agent", ".", "goal_b", "is", "not", "None", ":", "\n", "            ", "goal_color", "=", "agent", ".", "goal_b", ".", "color", "\n", "\n", "# get positions of all entities in this agent's reference frame", "\n", "", "entity_pos", "=", "[", "]", "\n", "for", "entity", "in", "world", ".", "landmarks", ":", "\n", "            ", "entity_pos", ".", "append", "(", "entity", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", ")", "\n", "\n", "# communication of all other agents", "\n", "", "comm", "=", "[", "]", "\n", "for", "other", "in", "world", ".", "agents", ":", "\n", "            ", "if", "other", "is", "agent", "or", "(", "other", ".", "state", ".", "c", "is", "None", ")", ":", "\n", "                ", "continue", "\n", "", "comm", ".", "append", "(", "other", ".", "state", ".", "c", ")", "\n", "\n", "# speaker", "\n", "", "if", "not", "agent", ".", "movable", ":", "\n", "            ", "return", "np", ".", "concatenate", "(", "[", "goal_color", "]", ")", "\n", "# listener", "\n", "", "if", "agent", ".", "silent", ":", "\n", "            ", "return", "np", ".", "concatenate", "(", "[", "agent", ".", "state", ".", "p_vel", "]", "+", "entity_pos", "+", "comm", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.make_world": [[7, 34], ["multiagent.core.World", "enumerate", "enumerate", "simple_schedule.Scenario.reset_world", "multiagent.core.Agent", "multiagent.core.Landmark", "range", "range"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reset_world"], ["    ", "def", "make_world", "(", "self", ")", ":", "\n", "        ", "world", "=", "World", "(", ")", "\n", "# set any world properties first", "\n", "world", ".", "dim_c", "=", "3", "\n", "num_agents", "=", "3", "\n", "num_landmarks", "=", "3", "\n", "world", ".", "collaborative", "=", "True", "\n", "# add agents", "\n", "world", ".", "agents", "=", "[", "Agent", "(", ")", "for", "i", "in", "range", "(", "num_agents", ")", "]", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "name", "=", "\"agent %d\"", "%", "i", "\n", "agent", ".", "collide", "=", "False", "\n", "agent", ".", "silent", "=", "True", "\n", "agent", ".", "size", "=", "0.075", "\n", "", "world", ".", "agents", "[", "0", "]", ".", "silent", "=", "False", "\n", "world", ".", "agents", "[", "0", "]", ".", "channel", "=", "2", "\n", "\n", "# add landmarks", "\n", "world", ".", "landmarks", "=", "[", "Landmark", "(", ")", "for", "i", "in", "range", "(", "num_landmarks", ")", "]", "\n", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "name", "=", "\"landmark %d\"", "%", "i", "\n", "landmark", ".", "collide", "=", "False", "\n", "landmark", ".", "movable", "=", "False", "\n", "landmark", ".", "size", "=", "0.04", "\n", "# make initial conditions", "\n", "", "self", ".", "reset_world", "(", "world", ")", "\n", "return", "world", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.reset_world": [[35, 77], ["world.landmarks.copy", "numpy.random.shuffle", "numpy.array", "numpy.array", "numpy.array", "enumerate", "numpy.array", "enumerate", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.random.uniform", "numpy.zeros", "numpy.zeros", "numpy.random.uniform", "numpy.zeros"], "methods", ["None"], ["", "def", "reset_world", "(", "self", ",", "world", ")", ":", "\n", "        ", "for", "agent", "in", "world", ".", "agents", ":", "\n", "            ", "agent", ".", "goal_a", "=", "None", "\n", "agent", ".", "goal_b", "=", "None", "\n", "agent", ".", "goal_c", "=", "None", "\n", "agent", ".", "goal_d", "=", "None", "\n", "agent", ".", "goal_e", "=", "None", "\n", "", "goal_landmarks", "=", "world", ".", "landmarks", ".", "copy", "(", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "goal_landmarks", ")", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_a", "=", "world", ".", "agents", "[", "1", "]", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_b", "=", "goal_landmarks", "[", "0", "]", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_c", "=", "world", ".", "agents", "[", "2", "]", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_d", "=", "goal_landmarks", "[", "1", "]", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_e", "=", "goal_landmarks", "[", "2", "]", "\n", "\n", "# random properties for landmarks", "\n", "world", ".", "landmarks", "[", "0", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.65", ",", "0.15", ",", "0.15", "]", ")", "\n", "world", ".", "landmarks", "[", "1", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.15", ",", "0.65", ",", "0.15", "]", ")", "\n", "world", ".", "landmarks", "[", "2", "]", ".", "color", "=", "np", ".", "array", "(", "[", "0.15", ",", "0.15", ",", "0.65", "]", ")", "\n", "# random properties for agents", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "color", "=", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.25", "]", ")", "\n", "\n", "", "world", ".", "agents", "[", "0", "]", ".", "goal_a", ".", "color", "=", "world", ".", "agents", "[", "0", "]", ".", "goal_b", ".", "color", "+", "np", ".", "array", "(", "\n", "[", "0.45", ",", "0.45", ",", "0.45", "]", "\n", ")", "\n", "world", ".", "agents", "[", "0", "]", ".", "goal_c", ".", "color", "=", "world", ".", "agents", "[", "0", "]", ".", "goal_d", ".", "color", "+", "np", ".", "array", "(", "\n", "[", "0.45", ",", "0.45", ",", "0.45", "]", "\n", ")", "\n", "world", ".", "agents", "[", "0", "]", ".", "color", "=", "world", ".", "agents", "[", "0", "]", ".", "goal_e", ".", "color", "+", "np", ".", "array", "(", "\n", "[", "0.45", ",", "0.45", ",", "0.45", "]", "\n", ")", "\n", "world", ".", "agents", "[", "0", "]", ".", "color", "-=", "np", ".", "array", "(", "[", "0.3", ",", "0.3", ",", "0.3", "]", ")", "\n", "\n", "# set random initial states", "\n", "for", "agent", "in", "world", ".", "agents", ":", "\n", "            ", "agent", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "c", "=", "np", ".", "zeros", "(", "world", ".", "dim_c", "*", "agent", ".", "channel", ")", "\n", "", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "landmark", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.benchmark_data": [[78, 90], ["simple_schedule.Scenario.reward", "simple_schedule.Scenario.is_collision"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reward", "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.is_collision"], ["", "", "def", "benchmark_data", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "        ", "rew", "=", "0", "\n", "collisions", "=", "0", "\n", "occupied_landmarks", "=", "0", "\n", "min_dists", "=", "0", "\n", "rew", "+=", "self", ".", "reward", "(", "agent", ",", "world", ")", "\n", "if", "agent", ".", "collide", ":", "\n", "            ", "for", "a", "in", "world", ".", "agents", ":", "\n", "                ", "if", "self", ".", "is_collision", "(", "a", ",", "agent", ")", ":", "\n", "                    ", "rew", "-=", "1", "\n", "collisions", "+=", "1", "\n", "", "", "", "return", "(", "rew", ",", "collisions", ",", "min_dists", ",", "occupied_landmarks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.is_collision": [[91, 96], ["numpy.sqrt", "numpy.sum", "numpy.square"], "methods", ["None"], ["", "def", "is_collision", "(", "self", ",", "agent1", ",", "agent2", ")", ":", "\n", "        ", "delta_pos", "=", "agent1", ".", "state", ".", "p_pos", "-", "agent2", ".", "state", ".", "p_pos", "\n", "dist", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "square", "(", "delta_pos", ")", ")", ")", "\n", "dist_min", "=", "agent1", ".", "size", "+", "agent2", ".", "size", "\n", "return", "True", "if", "dist", "<", "dist_min", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.reward": [[97, 109], ["numpy.sum", "numpy.sum", "numpy.sum", "numpy.square", "numpy.square", "numpy.square", "simple_schedule.Scenario.is_collision"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.is_collision"], ["", "def", "reward", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# Agents are rewarded based on minimum agent distance to each landmark, penalized for collisions", "\n", "        ", "rew", "=", "0", "\n", "a", "=", "world", ".", "agents", "[", "0", "]", "\n", "rew", "-=", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "goal_a", ".", "state", ".", "p_pos", "-", "a", ".", "goal_b", ".", "state", ".", "p_pos", ")", ")", "\n", "rew", "-=", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "goal_c", ".", "state", ".", "p_pos", "-", "a", ".", "goal_d", ".", "state", ".", "p_pos", ")", ")", "\n", "rew", "-=", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "state", ".", "p_pos", "-", "a", ".", "goal_e", ".", "state", ".", "p_pos", ")", ")", "\n", "if", "agent", ".", "collide", ":", "\n", "            ", "for", "a", "in", "world", ".", "agents", ":", "\n", "                ", "if", "self", ".", "is_collision", "(", "a", ",", "agent", ")", ":", "\n", "                    ", "rew", "-=", "1", "\n", "", "", "", "return", "rew", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_schedule.Scenario.observation": [[110, 141], ["entity_pos.append", "entity_color.append", "other_pos.append", "numpy.concatenate", "numpy.concatenate"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# get positions of all entities in this agent's reference frame", "\n", "        ", "entity_pos", "=", "[", "]", "\n", "for", "entity", "in", "world", ".", "landmarks", ":", "# world.entities:", "\n", "            ", "entity_pos", ".", "append", "(", "entity", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", ")", "\n", "# entity colors", "\n", "", "entity_color", "=", "[", "]", "\n", "for", "entity", "in", "world", ".", "landmarks", ":", "# world.entities:", "\n", "            ", "entity_color", ".", "append", "(", "entity", ".", "color", ")", "\n", "# communication of all other agents", "\n", "", "comm", "=", "[", "world", ".", "agents", "[", "0", "]", ".", "state", ".", "c", "]", "\n", "other_pos", "=", "[", "]", "\n", "for", "other", "in", "world", ".", "agents", ":", "\n", "            ", "if", "other", "is", "agent", ":", "\n", "                ", "continue", "\n", "", "other_pos", ".", "append", "(", "other", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", ")", "\n", "", "if", "agent", ".", "silent", "is", "False", ":", "\n", "            ", "return", "np", ".", "concatenate", "(", "\n", "[", "agent", ".", "state", ".", "p_vel", "]", "\n", "+", "[", "agent", ".", "goal_e", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", "]", "\n", "+", "[", "agent", ".", "goal_b", ".", "color", ",", "agent", ".", "goal_d", ".", "color", "]", "\n", "+", "other_pos", "\n", "+", "comm", "\n", ")", "\n", "", "else", ":", "\n", "            ", "if", "agent", ".", "name", "==", "\"agent 1\"", ":", "\n", "                ", "comm", "=", "[", "world", ".", "agents", "[", "0", "]", ".", "state", ".", "c", "[", ":", "world", ".", "dim_c", "]", "]", "\n", "", "else", ":", "\n", "                ", "comm", "=", "[", "world", ".", "agents", "[", "0", "]", ".", "state", ".", "c", "[", "world", ".", "dim_c", ":", "]", "]", "\n", "# print(comm)", "\n", "", "return", "np", ".", "concatenate", "(", "[", "agent", ".", "state", ".", "p_vel", "]", "+", "entity_pos", "+", "other_pos", "+", "comm", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.make_world": [[7, 30], ["multiagent.core.World", "enumerate", "enumerate", "simple_spread.Scenario.reset_world", "multiagent.core.Agent", "multiagent.core.Landmark", "range", "range"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reset_world"], ["    ", "def", "make_world", "(", "self", ")", ":", "\n", "        ", "world", "=", "World", "(", ")", "\n", "# set any world properties first", "\n", "world", ".", "dim_c", "=", "2", "\n", "num_agents", "=", "3", "\n", "num_landmarks", "=", "3", "\n", "world", ".", "collaborative", "=", "True", "\n", "# add agents", "\n", "world", ".", "agents", "=", "[", "Agent", "(", ")", "for", "i", "in", "range", "(", "num_agents", ")", "]", "\n", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "name", "=", "\"agent %d\"", "%", "i", "\n", "agent", ".", "collide", "=", "True", "\n", "agent", ".", "silent", "=", "True", "\n", "agent", ".", "size", "=", "0.15", "\n", "# add landmarks", "\n", "", "world", ".", "landmarks", "=", "[", "Landmark", "(", ")", "for", "i", "in", "range", "(", "num_landmarks", ")", "]", "\n", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "name", "=", "\"landmark %d\"", "%", "i", "\n", "landmark", ".", "collide", "=", "False", "\n", "landmark", ".", "movable", "=", "False", "\n", "# make initial conditions", "\n", "", "self", ".", "reset_world", "(", "world", ")", "\n", "return", "world", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reset_world": [[31, 46], ["enumerate", "enumerate", "enumerate", "numpy.array", "numpy.array", "numpy.random.uniform", "numpy.zeros", "numpy.zeros", "numpy.random.uniform", "numpy.zeros"], "methods", ["None"], ["", "def", "reset_world", "(", "self", ",", "world", ")", ":", "\n", "# random properties for agents", "\n", "        ", "for", "i", ",", "agent", "in", "enumerate", "(", "world", ".", "agents", ")", ":", "\n", "            ", "agent", ".", "color", "=", "np", ".", "array", "(", "[", "0.35", ",", "0.35", ",", "0.85", "]", ")", "\n", "# random properties for landmarks", "\n", "", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "color", "=", "np", ".", "array", "(", "[", "0.25", ",", "0.25", ",", "0.25", "]", ")", "\n", "# set random initial states", "\n", "", "for", "agent", "in", "world", ".", "agents", ":", "\n", "            ", "agent", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "agent", ".", "state", ".", "c", "=", "np", ".", "zeros", "(", "world", ".", "dim_c", ")", "\n", "", "for", "i", ",", "landmark", "in", "enumerate", "(", "world", ".", "landmarks", ")", ":", "\n", "            ", "landmark", ".", "state", ".", "p_pos", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "+", "1", ",", "world", ".", "dim_p", ")", "\n", "landmark", ".", "state", ".", "p_vel", "=", "np", ".", "zeros", "(", "world", ".", "dim_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.benchmark_data": [[47, 67], ["min", "min", "numpy.sqrt", "min", "simple_spread.Scenario.is_collision", "numpy.sum", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.is_collision"], ["", "", "def", "benchmark_data", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "        ", "rew", "=", "0", "\n", "collisions", "=", "0", "\n", "occupied_landmarks", "=", "0", "\n", "min_dists", "=", "0", "\n", "for", "l", "in", "world", ".", "landmarks", ":", "\n", "            ", "dists", "=", "[", "\n", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "state", ".", "p_pos", "-", "l", ".", "state", ".", "p_pos", ")", ")", ")", "\n", "for", "a", "in", "world", ".", "agents", "\n", "]", "\n", "min_dists", "+=", "min", "(", "dists", ")", "\n", "rew", "-=", "min", "(", "dists", ")", "\n", "if", "min", "(", "dists", ")", "<", "0.1", ":", "\n", "                ", "occupied_landmarks", "+=", "1", "\n", "", "", "if", "agent", ".", "collide", ":", "\n", "            ", "for", "a", "in", "world", ".", "agents", ":", "\n", "                ", "if", "self", ".", "is_collision", "(", "a", ",", "agent", ")", ":", "\n", "                    ", "rew", "-=", "1", "\n", "collisions", "+=", "1", "\n", "", "", "", "return", "(", "rew", ",", "collisions", ",", "min_dists", ",", "occupied_landmarks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.is_collision": [[68, 73], ["numpy.sqrt", "numpy.sum", "numpy.square"], "methods", ["None"], ["", "def", "is_collision", "(", "self", ",", "agent1", ",", "agent2", ")", ":", "\n", "        ", "delta_pos", "=", "agent1", ".", "state", ".", "p_pos", "-", "agent2", ".", "state", ".", "p_pos", "\n", "dist", "=", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "square", "(", "delta_pos", ")", ")", ")", "\n", "dist_min", "=", "agent1", ".", "size", "+", "agent2", ".", "size", "\n", "return", "True", "if", "dist", "<", "dist_min", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.reward": [[74, 88], ["min", "numpy.sqrt", "simple_spread.Scenario.is_collision", "numpy.sum", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.is_collision"], ["", "def", "reward", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# Agents are rewarded based on minimum agent distance to each landmark, penalized for collisions", "\n", "        ", "rew", "=", "0", "\n", "for", "l", "in", "world", ".", "landmarks", ":", "\n", "            ", "dists", "=", "[", "\n", "np", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "square", "(", "a", ".", "state", ".", "p_pos", "-", "l", ".", "state", ".", "p_pos", ")", ")", ")", "\n", "for", "a", "in", "world", ".", "agents", "\n", "]", "\n", "rew", "-=", "min", "(", "dists", ")", "\n", "", "if", "agent", ".", "collide", ":", "\n", "            ", "for", "a", "in", "world", ".", "agents", ":", "\n", "                ", "if", "self", ".", "is_collision", "(", "a", ",", "agent", ")", ":", "\n", "                    ", "rew", "-=", "1", "\n", "", "", "", "return", "rew", "\n", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.simple_spread.Scenario.observation": [[89, 108], ["numpy.concatenate", "entity_pos.append", "entity_color.append", "comm.append", "other_pos.append"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "agent", ",", "world", ")", ":", "\n", "# get positions of all entities in this agent's reference frame", "\n", "        ", "entity_pos", "=", "[", "]", "\n", "for", "entity", "in", "world", ".", "landmarks", ":", "# world.entities:", "\n", "            ", "entity_pos", ".", "append", "(", "entity", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", ")", "\n", "# entity colors", "\n", "", "entity_color", "=", "[", "]", "\n", "for", "entity", "in", "world", ".", "landmarks", ":", "# world.entities:", "\n", "            ", "entity_color", ".", "append", "(", "entity", ".", "color", ")", "\n", "# communication of all other agents", "\n", "", "comm", "=", "[", "]", "\n", "other_pos", "=", "[", "]", "\n", "for", "other", "in", "world", ".", "agents", ":", "\n", "            ", "if", "other", "is", "agent", ":", "\n", "                ", "continue", "\n", "", "comm", ".", "append", "(", "other", ".", "state", ".", "c", ")", "\n", "other_pos", ".", "append", "(", "other", ".", "state", ".", "p_pos", "-", "agent", ".", "state", ".", "p_pos", ")", "\n", "", "return", "np", ".", "concatenate", "(", "\n", "[", "agent", ".", "state", ".", "p_vel", "]", "+", "[", "agent", ".", "state", ".", "p_pos", "]", "+", "entity_pos", "+", "other_pos", "+", "comm", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.apexrl_AORPO.scenarios.__init__.load": [[5, 8], ["os.join", "imp.load_source", "os.dirname"], "function", ["None"], []]}