{"home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.make_predictions.main": [[20, 31], ["parser.parse_args", "datahandler.get_papers_after", "make_predictions.predict_texts", "zip", "print", "datahandler.update_paper"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.make_predictions.predict_texts", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "papers", "=", "datahandler", ".", "get_papers_after", "(", "args", ".", "startdate", ")", "\n", "\n", "texts", "=", "[", "paper", "[", "args", ".", "textpart", "]", "for", "paper", "in", "papers", "]", "\n", "predictions", "=", "predict_texts", "(", "texts", ",", "textpart", "=", "args", ".", "textpart", ",", "regression", "=", "True", ")", "\n", "\n", "for", "pred", ",", "paper", "in", "zip", "(", "predictions", ",", "papers", ")", ":", "\n", "        ", "print", "(", "pred", ")", "\n", "paper", "[", "\"prediction_%s\"", "%", "args", ".", "textpart", "]", "=", "pred", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "\"arxivid\"", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.make_predictions.predict_texts": [[33, 79], ["enumerate", "encode_sentences.infersent_encode_texts", "train_models.vectorize", "numpy.array", "datahandler.get_all_papers_iterator", "vocab_texts.append", "str", "pathlib.Path().glob", "pathlib.Path().glob", "print", "predictions.transpose.append", "numpy.mean().tolist", "predictions.transpose.transpose", "paper[].lower", "nltk.tokenize.word_tokenize", "str", "keras.models.load_model", "joblib.load.predict().flatten", "sklearn.externals.joblib.load", "joblib.load.predict().flatten", "numpy.unique", "numpy.argmax", "pred.append", "text.lower", "pathlib.Path", "pathlib.Path", "str", "str", "numpy.mean", "joblib.load.predict", "joblib.load.predict"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_texts", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.vectorize", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator"], ["", "", "def", "predict_texts", "(", "texts", ",", "textpart", "=", "'abstract'", ",", "regression", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n        TODO test for classification\n        makes a prediction using pre trained models. Takes a majority-vote/mean from all models\n    Args:\n        texts(list str): text\n        textpart(str): abstract or title\n        regression(bool): True to use regression models false to use classification models\n\n    Returns:\n        float: the prediction\n    \"\"\"", "\n", "vocab_texts", "=", "[", "]", "\n", "for", "i", ",", "paper", "in", "enumerate", "(", "datahandler", ".", "get_all_papers_iterator", "(", ")", ")", ":", "\n", "        ", "vocab_texts", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "", "embeddings", "=", "infersent_encode_texts", "(", "vocab_texts", ",", "texts", ")", "\n", "\n", "texts", "=", "[", "str", "(", "word_tokenize", "(", "text", ".", "lower", "(", ")", ")", ")", "for", "text", "in", "texts", "]", "\n", "x_tfidf", "=", "vectorize", "(", "texts", ")", "\n", "\n", "if", "regression", ":", "\n", "        ", "pathlist", "=", "Path", "(", "'models/regression/'", ")", ".", "glob", "(", "'*.%s.model'", "%", "textpart", ")", "\n", "", "else", ":", "\n", "        ", "pathlist", "=", "Path", "(", "'models/classification/'", ")", ".", "glob", "(", "'*.%s.model'", "%", "textpart", ")", "\n", "", "predictions", "=", "[", "]", "\n", "for", "path", "in", "pathlist", ":", "\n", "        ", "if", "'mlp'", "in", "str", "(", "path", ")", ":", "\n", "            ", "model", "=", "load_model", "(", "str", "(", "path", ")", ")", "\n", "p", "=", "model", ".", "predict", "(", "embeddings", ")", ".", "flatten", "(", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "joblib", ".", "load", "(", "str", "(", "path", ")", ")", "\n", "p", "=", "model", ".", "predict", "(", "x_tfidf", ")", ".", "flatten", "(", ")", "\n", "", "print", "(", "p", ")", "\n", "predictions", ".", "append", "(", "p", ")", "\n", "", "predictions", "=", "np", ".", "array", "(", "predictions", ")", "\n", "\n", "if", "regression", ":", "\n", "        ", "return", "np", ".", "mean", "(", "predictions", ",", "axis", "=", "1", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "        ", "pred", "=", "[", "]", "\n", "predictions", "=", "predictions", ".", "transpose", "(", ")", "\n", "for", "p", "in", "predictions", ":", "\n", "            ", "(", "values", ",", "counts", ")", "=", "np", ".", "unique", "(", "p", ",", "return_counts", "=", "True", ")", "\n", "ind", "=", "np", ".", "argmax", "(", "counts", ")", "\n", "pred", ".", "append", "(", "values", "[", "ind", "]", ")", "\n", "", "return", "pred", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.main": [[19, 33], ["parser.parse_args", "get_train.write_train_data", "get_train.write_train_data", "get_train.write_train_data"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_train_data", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_train_data", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_train_data"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Usage: python3 get_train.py -h\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "textpart", "==", "'all'", ":", "\n", "\n", "        ", "write_train_data", "(", "args", ".", "startdate", ",", "args", ".", "enddate", ",", "textpart", "=", "'abstract'", ",", "binning_steps", "=", "args", ".", "binning_steps", ",", "\n", "teststartdate", "=", "args", ".", "teststartdate", ",", "testenddate", "=", "args", ".", "testenddate", ")", "\n", "write_train_data", "(", "args", ".", "startdate", ",", "args", ".", "enddate", ",", "textpart", "=", "'title'", ",", "binning_steps", "=", "args", ".", "binning_steps", ",", "\n", "teststartdate", "=", "args", ".", "teststartdate", ",", "testenddate", "=", "args", ".", "testenddate", ")", "\n", "", "else", ":", "\n", "        ", "write_train_data", "(", "args", ".", "startdate", ",", "args", ".", "enddate", ",", "textpart", "=", "args", ".", "textpart", ",", "binning_steps", "=", "args", ".", "binning_steps", ",", "\n", "teststartdate", "=", "args", ".", "teststartdate", ",", "testenddate", "=", "args", ".", "testenddate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.bin": [[35, 52], ["None"], "function", ["None"], ["", "", "def", "bin", "(", "label", ",", "steps", "=", "[", "1", ",", "3", "]", ")", ":", "\n", "    ", "\"\"\"Put a label into bins between steps\n    \n    Args:\n        label (int): input label\n        steps (list, optional): where to seperate the bins\n    \n    Returns:\n        int: binned label\n    \"\"\"", "\n", "i", "=", "0", "\n", "for", "step", "in", "steps", ":", "\n", "        ", "if", "label", "<", "step", ":", "\n", "            ", "return", "i", "\n", "", "else", ":", "\n", "            ", "i", "+=", "1", "\n", "", "", "return", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_train_data": [[54, 74], ["get_train.write_data", "get_train.write_embeddingdata", "get_train.write_embeddingdata", "get_train.write_data", "get_train.write_embeddingdata", "get_train.write_embeddingdata"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_data", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_embeddingdata", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_embeddingdata", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_data", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_embeddingdata", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_embeddingdata"], ["", "def", "write_train_data", "(", "startdate", ",", "enddate", ",", "textpart", "=", "'abstract'", ",", "out_dir", "=", "'data/train/'", ",", "binning_steps", "=", "[", "1", ",", "3", "]", ",", "\n", "teststartdate", "=", "None", ",", "testenddate", "=", "None", ")", ":", "\n", "    ", "\"\"\"writes train data for one textpart for a timewindow to a file\n    \n    Args:\n        startdate (datetime.datetime): startdate\n        enddate (datetime.datetime): enddate\n        textpart (str): abstract or title\n        out_dir (str): path to a directory to write the files in\n        binning_steps (list, optional): steps for the binning\n    \"\"\"", "\n", "\n", "write_data", "(", "startdate", ",", "enddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'train'", ")", "\n", "write_embeddingdata", "(", "startdate", ",", "enddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'train'", ",", "'infersent'", ")", "\n", "write_embeddingdata", "(", "startdate", ",", "enddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'train'", ",", "'unisent'", ")", "\n", "\n", "if", "teststartdate", "and", "testenddate", ":", "\n", "        ", "write_data", "(", "teststartdate", ",", "testenddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'test'", ")", "\n", "write_embeddingdata", "(", "teststartdate", ",", "testenddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'test'", ",", "'infersent'", ")", "\n", "write_embeddingdata", "(", "teststartdate", ",", "testenddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "'test'", ",", "'unisent'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_data": [[76, 105], ["datahandler.get_papers_in_timewindow", "nltk.tokenize.word_tokenize", "x.append", "len", "y_real.append", "open", "open", "open", "nltk.tokenize.word_tokenize.lower", "f.write", "fr.write", "fr.write", "fb.write", "fb.write", "str", "str", "int", "get_train.bin"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_in_timewindow", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.bin"], ["", "", "def", "write_data", "(", "startdate", ",", "enddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "trainortest", ")", ":", "\n", "    ", "papers", "=", "datahandler", ".", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", "\n", "\n", "x", "=", "[", "]", "\n", "y_real", "=", "[", "]", "\n", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "citations", "=", "[", "c", "for", "c", "in", "paper", "[", "'citations'", "]", "if", "int", "(", "c", "[", "'year'", "]", ")", "<=", "(", "paper", "[", "'created'", "]", ".", "year", "+", "1", ")", "]", "\n", "text", "=", "paper", "[", "textpart", "]", "\n", "text", "=", "word_tokenize", "(", "text", ".", "lower", "(", ")", ")", "\n", "x", ".", "append", "(", "text", ")", "\n", "label", "=", "len", "(", "citations", ")", "\n", "y_real", ".", "append", "(", "label", ")", "\n", "\n", "", "p", "=", "out_dir", "+", "textpart", "\n", "xfile", "=", "p", "+", "'.'", "+", "trainortest", "\n", "labelfilereal", "=", "out_dir", "+", "'label.%s.real'", "%", "trainortest", "\n", "labelfilebinned", "=", "out_dir", "+", "'label.%s.binned'", "%", "trainortest", "\n", "\n", "with", "open", "(", "xfile", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "text", "in", "x", ":", "\n", "            ", "f", ".", "write", "(", "' '", ".", "join", "(", "text", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "labelfilereal", ",", "'w'", ")", "as", "fr", ",", "open", "(", "labelfilebinned", ",", "'w'", ")", "as", "fb", ":", "\n", "        ", "for", "label", "in", "y_real", ":", "\n", "            ", "fr", ".", "write", "(", "str", "(", "label", ")", ")", "\n", "fr", ".", "write", "(", "'\\n'", ")", "\n", "fb", ".", "write", "(", "str", "(", "bin", "(", "label", ",", "steps", "=", "binning_steps", ")", ")", ")", "\n", "fb", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.write_embeddingdata": [[107, 136], ["datahandler.get_papers_in_timewindow", "datahandler.get_arxivid_embedding", "len", "open", "open", "open", "x.append", "y.append", "f.write", "f.write", "fr.write", "fr.write", "fb.write", "fb.write", "str", "str", "int", "get_train.bin", "str", "embedding.tolist"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_in_timewindow", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_arxivid_embedding", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_train.bin"], ["", "", "", "def", "write_embeddingdata", "(", "startdate", ",", "enddate", ",", "textpart", ",", "out_dir", ",", "binning_steps", ",", "trainortest", ",", "embedding", ")", ":", "\n", "    ", "papers", "=", "datahandler", ".", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", "\n", "arxivid_embedding", "=", "datahandler", ".", "get_arxivid_embedding", "(", "embedding", "=", "embedding", ",", "textpart", "=", "textpart", ")", "\n", "\n", "x", "=", "[", "]", "\n", "y", "=", "[", "]", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "citations", "=", "[", "c", "for", "c", "in", "paper", "[", "'citations'", "]", "if", "int", "(", "c", "[", "'year'", "]", ")", "<=", "(", "paper", "[", "'created'", "]", ".", "year", "+", "1", ")", "]", "\n", "label", "=", "len", "(", "citations", ")", "\n", "if", "paper", "[", "'arxivid'", "]", "in", "arxivid_embedding", ":", "\n", "            ", "x", ".", "append", "(", "arxivid_embedding", "[", "paper", "[", "'arxivid'", "]", "]", ")", "\n", "y", ".", "append", "(", "label", ")", "\n", "\n", "", "", "p", "=", "out_dir", "+", "textpart", "\n", "xfile", "=", "p", "+", "'%s.'", "%", "embedding", "+", "trainortest", "\n", "labelfilereal", "=", "out_dir", "+", "'%slabel.%s.real'", "%", "(", "embedding", ",", "trainortest", ")", "\n", "labelfilebinned", "=", "out_dir", "+", "'%slabel.%s.binned'", "%", "(", "embedding", ",", "trainortest", ")", "\n", "\n", "with", "open", "(", "xfile", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "embedding", "in", "x", ":", "\n", "            ", "f", ".", "write", "(", "' '", ".", "join", "(", "str", "(", "x", ")", "for", "x", "in", "embedding", ".", "tolist", "(", ")", ")", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "labelfilereal", ",", "'w'", ")", "as", "fr", ",", "open", "(", "labelfilebinned", ",", "'w'", ")", "as", "fb", ":", "\n", "        ", "for", "label", "in", "y", ":", "\n", "            ", "fr", ".", "write", "(", "str", "(", "label", ")", ")", "\n", "fr", ".", "write", "(", "'\\n'", ")", "\n", "fb", ".", "write", "(", "str", "(", "bin", "(", "label", ",", "steps", "=", "binning_steps", ")", ")", ")", "\n", "fb", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.main": [[34, 50], ["parser.parse_args", "encode_sentences.infersent_encode_papers", "encode_sentences.infersent_encode_papers", "encode_sentences.infersent_encode_papers", "encode_sentences.unisent_encode", "encode_sentences.unisent_encode", "encode_sentences.unisent_encode"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.unisent_encode", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.unisent_encode", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.unisent_encode"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Usage: python3 encode_sentences.py -h\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "embedding", "==", "\"infersent\"", "or", "args", ".", "embedding", "==", "\"both\"", ":", "\n", "        ", "if", "args", ".", "textpart", "==", "\"both\"", ":", "\n", "            ", "infersent_encode_papers", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "\"title\"", ",", "usegpu", "=", "args", ".", "usegpu", ")", "\n", "infersent_encode_papers", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "\"abstract\"", ",", "usegpu", "=", "args", ".", "usegpu", ")", "\n", "", "else", ":", "\n", "            ", "infersent_encode_papers", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "args", ".", "textpart", ",", "usegpu", "=", "args", ".", "usegpu", ")", "\n", "", "", "if", "args", ".", "embedding", "==", "\"unisent\"", "or", "args", ".", "embedding", "==", "\"both\"", ":", "\n", "        ", "if", "args", ".", "textpart", "==", "\"both\"", ":", "\n", "            ", "unisent_encode", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "\"title\"", ")", "\n", "unisent_encode", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "\"abstract\"", ")", "\n", "", "else", ":", "\n", "            ", "unisent_encode", "(", "startdate", "=", "args", ".", "startdate", ",", "textpart", "=", "args", ".", "textpart", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_papers": [[52, 88], ["enumerate", "datahandler.get_all_papers_iterator", "encode_sentences.infersent_encode_texts", "encode_sentences.infersent_encode_texts", "os.path.exists", "open", "index_arxivid.items", "vocab_texts.append", "sentences.append", "f.write", "f.write", "paper[].lower", "sentences.append", "paper[].lower", "paper[].lower", "list", "map"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_texts", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_texts"], ["", "", "", "def", "infersent_encode_papers", "(", "startdate", "=", "None", ",", "textpart", "=", "'abstract'", ",", "usegpu", "=", "False", ",", "outdir", "=", "'data/'", ")", ":", "\n", "    ", "\"\"\"Encodes all papers as infersent embeddings for one text part. Creates a csv file with arxivids and sentence vectors\n    \n    Args:\n        startdate (datetime.datetime, optional): Startdate. If None all papers are encoded\n        textpart (str, optional): abstract or title\n        usegpu (bool, optional): set to True if run with cuda\n        outdir (str, optional): dir to write the embeddings to\n    \"\"\"", "\n", "outfile", "=", "outdir", "+", "'infersent_%s.csv'", "%", "textpart", "\n", "\n", "\n", "sentences", "=", "[", "]", "\n", "vocab_texts", "=", "[", "]", "\n", "index_arxivid", "=", "{", "}", "\n", "for", "i", ",", "paper", "in", "enumerate", "(", "datahandler", ".", "get_all_papers_iterator", "(", ")", ")", ":", "\n", "        ", "if", "startdate", ":", "\n", "            ", "vocab_texts", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "if", "paper", "[", "'created'", "]", ">=", "startdate", ":", "\n", "                ", "index_arxivid", "[", "i", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "sentences", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "index_arxivid", "[", "i", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "sentences", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "\n", "\n", "", "", "if", "startdate", ":", "\n", "        ", "embeddings", "=", "infersent_encode_texts", "(", "vocab_texts", ",", "sentences", ",", "usegpu", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "embeddings", "=", "infersent_encode_texts", "(", "sentences", ",", "sentences", ",", "usegpu", "=", "False", ")", "\n", "\n", "", "wa", "=", "'a'", "if", "os", ".", "path", ".", "exists", "(", "outfile", ")", "else", "'w'", "\n", "with", "open", "(", "outfile", ",", "wa", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "arxivid", "in", "index_arxivid", ".", "items", "(", ")", ":", "\n", "            ", "f", ".", "write", "(", "\" \"", ".", "join", "(", "[", "arxivid", "]", "+", "list", "(", "map", "(", "str", ",", "embeddings", "[", "i", ",", ":", "]", ")", ")", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.infersent_encode_texts": [[89, 101], ["torch.load.set_glove_path", "torch.load.build_vocab", "torch.load.encode", "torch.load", "torch.load"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.mydate.DateTimeSerializer.encode"], ["", "", "", "def", "infersent_encode_texts", "(", "vocab_texts", ",", "texts", ",", "usegpu", "=", "False", ")", ":", "\n", "    ", "if", "usegpu", ":", "\n", "        ", "infersent", "=", "torch", ".", "load", "(", "infersentpath", ")", "\n", "", "else", ":", "\n", "        ", "infersent", "=", "torch", ".", "load", "(", "\n", "infersentpath", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "\n", "", "infersent", ".", "set_glove_path", "(", "glovePath", ")", "\n", "\n", "infersent", ".", "build_vocab", "(", "vocab_texts", ",", "tokenize", "=", "True", ")", "\n", "\n", "return", "infersent", ".", "encode", "(", "texts", ",", "tokenize", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.encode_sentences.unisent_encode": [[103, 138], ["tensorflow_hub.Module", "enumerate", "tensorflow.logging.set_verbosity", "datahandler.get_all_papers_iterator", "tensorflow.Session", "session.run", "session.run", "os.path.exists", "open", "enumerate", "texts.append", "hub.Module.", "numpy.array().tolist", "f.write", "f.write", "texts.append", "paper[].lower", "tensorflow.global_variables_initializer", "tensorflow.tables_initializer", "paper[].lower", "numpy.array", "list", "map"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator"], ["", "def", "unisent_encode", "(", "startdate", "=", "None", ",", "textpart", "=", "'abstract'", ",", "outdir", "=", "'data/'", ")", ":", "\n", "    ", "\"\"\"Encodes all papers as unisent embeddings for one text part. Creates a csv file with arxivids and sentence vectors\n\n    Args:\n        startdate (datetime.datetime, optional): Startdate. If None all papers are encoded\n        textpart (str, optional): abstract or title\n        outdir (str, optional): dir to write the embeddings to\n    \"\"\"", "\n", "outfile", "=", "outdir", "+", "'unisent_%s.csv'", "%", "textpart", "\n", "\n", "embed", "=", "hub", ".", "Module", "(", "\"https://tfhub.dev/google/\"", "\n", "\"universal-sentence-encoder/1\"", ")", "\n", "texts", "=", "[", "]", "\n", "index_arxivid", "=", "{", "}", "\n", "\n", "for", "i", ",", "paper", "in", "enumerate", "(", "datahandler", ".", "get_all_papers_iterator", "(", ")", ")", ":", "\n", "        ", "if", "startdate", ":", "\n", "            ", "if", "paper", "[", "'created'", "]", ">=", "startdate", ":", "\n", "                ", "index_arxivid", "[", "i", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "texts", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "index_arxivid", "[", "i", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "texts", ".", "append", "(", "paper", "[", "textpart", "]", ".", "lower", "(", ")", ")", "\n", "\n", "", "", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "ERROR", ")", "\n", "with", "tf", ".", "Session", "(", ")", "as", "session", ":", "\n", "        ", "session", ".", "run", "(", "[", "tf", ".", "global_variables_initializer", "(", ")", ",", "tf", ".", "tables_initializer", "(", ")", "]", ")", "\n", "text_embeddings", "=", "session", ".", "run", "(", "embed", "(", "texts", ")", ")", "\n", "\n", "", "wa", "=", "'a'", "if", "os", ".", "path", ".", "exists", "(", "outfile", ")", "else", "'w'", "\n", "with", "open", "(", "outfile", ",", "wa", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "embedding", "in", "enumerate", "(", "np", ".", "array", "(", "text_embeddings", ")", ".", "tolist", "(", ")", ")", ":", "\n", "            ", "arxivid", "=", "index_arxivid", "[", "i", "]", "\n", "f", ".", "write", "(", "\" \"", ".", "join", "(", "[", "arxivid", "]", "+", "list", "(", "map", "(", "str", ",", "embedding", ")", ")", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.main": [[20, 27], ["parser.parse_args", "print", "top_papers.top_prediction", "top_papers.top_zscore"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.top_prediction", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.top_zscore"], ["def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "if", "args", ".", "top_predictions", ":", "\n", "        ", "top_prediction", "(", "args", ".", "startdate", ",", "k", "=", "args", ".", "k", ",", "print_output", "=", "True", ",", "textpart", "=", "\"abstract\"", ")", "\n", "", "else", ":", "\n", "        ", "top_zscore", "(", "args", ".", "startdate", ",", "k", "=", "args", ".", "k", ",", "mincitations", "=", "args", ".", "min_citations", ",", "print_output", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.top_zscore": [[29, 58], ["datahandler.get_papers_after", "sorted", "arxivid_z.items", "datahandler.get_paper", "sys.stdout.write", "print", "len", "paper[].replace", "len"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "", "def", "top_zscore", "(", "startdate", ",", "k", "=", "10", ",", "print_output", "=", "False", ",", "mincitations", "=", "4", ")", ":", "\n", "    ", "\"\"\"return the top k papers by z-score ignoring papers published before Startdate or less than mincitations\n    \n    Args:\n        startdate (datetime.datetime): startdate\n        k (int, optional): how many papers are returned\n        print_output (bool, optional): If true output is print in a formatted way\n        mincitations (int, optional): Ignore papers with less than mincitations\n    \n    Returns:\n        list:list of tupels with arxiv id and z-scores \n    \"\"\"", "\n", "arxivid_z", "=", "{", "}", "\n", "for", "paper", "in", "datahandler", ".", "get_papers_after", "(", "startdate", ")", ":", "\n", "        ", "if", "'z-score'", "in", "paper", "and", "len", "(", "paper", "[", "'citations'", "]", ")", ">=", "mincitations", ":", "\n", "            ", "arxivid_z", "[", "paper", "[", "'arxivid'", "]", "]", "=", "paper", "[", "'z-score'", "]", "\n", "\n", "", "", "top", "=", "sorted", "(", "arxivid_z", ".", "items", "(", ")", ",", "key", "=", "(", "lambda", "x", ":", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "if", "print_output", ":", "\n", "        ", "for", "arxivid", ",", "z", "in", "top", "[", ":", "k", "]", ":", "\n", "            ", "paper", "=", "datahandler", ".", "get_paper", "(", "arxivid", ")", "\n", "#print(paper.keys(),paper[\"created\"],paper[\"year\"])", "\n", "created", "=", "paper", "[", "\"created\"", "]", "\n", "sys", ".", "stdout", ".", "write", "(", "\"%s\\t%s\\tnum_citations:%i\\tzscore: %f\\t\"", "%", "(", "\n", "paper", "[", "'arxivid'", "]", ",", "paper", "[", "'title'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", ",", "len", "(", "paper", "[", "'citations'", "]", ")", ",", "z", ")", ")", "\n", "print", "(", "\"created:\"", ",", "created", ")", "\n", "# print(\"\\n\")", "\n", "\n", "", "", "return", "top", "[", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.top_prediction": [[60, 86], ["datahandler.get_papers_after", "sorted", "arxivid_pred.items", "datahandler.get_paper", "print", "paper[].replace", "len"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "def", "top_prediction", "(", "startdate", ",", "k", "=", "10", ",", "print_output", "=", "False", ",", "textpart", "=", "\"abstract\"", ")", ":", "\n", "    ", "\"\"\"return the top k papers by prediction ignoring papers published before Startdate or less than mincitations\n\n    Args:\n        startdate (datetime.datetime): startdate\n        k (int, optional): how many papers are returned\n        print_output (bool, optional): If true output is print in a formatted way\n\n    Returns:\n        list:list of tupels with arxiv id and predictions\n    \"\"\"", "\n", "field", "=", "\"prediction_%s\"", "%", "textpart", "\n", "arxivid_pred", "=", "{", "}", "\n", "for", "paper", "in", "datahandler", ".", "get_papers_after", "(", "startdate", ")", ":", "\n", "        ", "if", "field", "in", "paper", ":", "\n", "            ", "arxivid_pred", "[", "paper", "[", "'arxivid'", "]", "]", "=", "paper", "[", "field", "]", "\n", "\n", "", "", "top", "=", "sorted", "(", "arxivid_pred", ".", "items", "(", ")", ",", "key", "=", "(", "lambda", "x", ":", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "if", "print_output", ":", "\n", "        ", "for", "arxivid", ",", "z", "in", "top", "[", ":", "k", "]", ":", "\n", "            ", "paper", "=", "datahandler", ".", "get_paper", "(", "arxivid", ")", "\n", "print", "(", "\"%s\\t%s\\tnum_citations:%i\\tprediction: %f\"", "%", "(", "\n", "paper", "[", "'arxivid'", "]", ",", "paper", "[", "'title'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", ",", "len", "(", "paper", "[", "'citations'", "]", ")", ",", "z", ")", ")", "\n", "# print(\"\\n\")", "\n", "\n", "", "", "return", "top", "[", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.top_similarity": [[88, 119], ["datahandler.get_arxivid_embedding", "datahandler.get_arxivid_embedding.items", "sorted", "print", "arxivid_similarity.items", "print", "datahandler.get_paper", "print", "print", "sklearn.metrics.pairwise.cosine_similarity().tolist", "datahandler.get_paper", "sklearn.metrics.pairwise.cosine_similarity", "paper[].replace", "numpy.array"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_arxivid_embedding", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "def", "top_similarity", "(", "arxivid", ",", "k", "=", "10", ",", "print_output", "=", "False", ")", ":", "\n", "    ", "\"\"\"Returns the most similar papers by cosine similarity on abstract embeddigs.\n    \n    Args:\n        arxivid (str): arxivid \n        k (int, optional): how many similar papers\n        print_output (bool, optional): If true output is printed in a formatted way\n    \n    Returns:\n        list: list of tupels with arxiv id and cosine similarity\n    \"\"\"", "\n", "arxivid_embedding", "=", "datahandler", ".", "get_arxivid_embedding", "(", "textpart", "=", "'abstract'", ")", "\n", "\n", "if", "not", "arxivid", "in", "arxivid_embedding", ":", "\n", "        ", "print", "(", "'no embedding for %s'", "%", "arxivid", ")", "\n", "return", "False", "\n", "\n", "", "emb", "=", "arxivid_embedding", "[", "arxivid", "]", "\n", "arxivid_similarity", "=", "{", "}", "\n", "for", "id", ",", "embedding", "in", "arxivid_embedding", ".", "items", "(", ")", ":", "\n", "        ", "arxivid_similarity", "[", "id", "]", "=", "cosine_similarity", "(", "np", ".", "array", "(", "[", "emb", ",", "embedding", "]", ")", ")", ".", "tolist", "(", ")", "[", "0", "]", "[", "1", "]", "\n", "", "top", "=", "sorted", "(", "arxivid_similarity", ".", "items", "(", ")", ",", "key", "=", "(", "lambda", "x", ":", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "if", "print_output", ":", "\n", "        ", "print", "(", "\"PAPERS SIMILAR TO:\\n%s\\n\"", "%", "datahandler", ".", "get_paper", "(", "arxivid", ")", "[", "'title'", "]", ")", "\n", "for", "arxivid", ",", "sim", "in", "top", "[", ":", "k", "]", ":", "\n", "            ", "paper", "=", "datahandler", ".", "get_paper", "(", "arxivid", ")", "\n", "print", "(", "\"%s\\t%s\\tcosinesimilarity: %f\\n\"", "%", "(", "paper", "[", "'arxivid'", "]", ",", "paper", "[", "'title'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", ",", "sim", ")", ")", "\n", "print", "(", "\"\\n\"", ")", "\n", "\n", "", "", "return", "top", "[", ":", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.top_papers.cluster_papers": [[121, 171], ["datahandler.get_arxivid_embedding", "datahandler.get_arxivid_embedding.items", "numpy.array", "sklearn.cluster.KMeans", "sklearn.cluster.KMeans.fit_predict().tolist", "enumerate", "np.array.append", "collections.defaultdict", "arxivid_cluster.items", "collections.defaultdict.items", "print", "datahandler.get_arxivid_embedding.items", "sklearn.cluster.KMeans.fit_predict", "cluster_papers[].append", "print", "print", "datahandler.get_paper", "z_scores.append", "print", "numpy.mean", "numpy.mean", "paper[].replace", "len"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_arxivid_embedding", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "def", "cluster_papers", "(", "arxivids", "=", "None", ",", "n_clusters", "=", "20", ",", "print_output", "=", "True", ")", ":", "\n", "    ", "\"\"\"Cluster papers by their abstract infersent embeddings\n    \n    Args:\n        arxivids (list, optional): list of arxivids to cluster or if None all papers with abstract infersent embeddings are clusterd\n        n_clusters (int, optional): number of clusters\n        print_output (bool, optional):  If true output is printed in a formatted way\n    \n    Returns:\n        dict: dict with arxivids as keys and their clusters as int as value\n    \"\"\"", "\n", "arxivid_embedding", "=", "datahandler", ".", "get_arxivid_embedding", "(", "textpart", "=", "'abstract'", ")", "\n", "arxivid_embedding", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "arxivid_embedding", ".", "items", "(", ")", "if", "(", "not", "arxivids", "or", "(", "k", "in", "arxivids", ")", ")", "}", "\n", "\n", "i", "=", "0", "\n", "index_arxivid", "=", "{", "}", "\n", "x", "=", "[", "]", "\n", "for", "id", ",", "emb", "in", "arxivid_embedding", ".", "items", "(", ")", ":", "\n", "        ", "x", ".", "append", "(", "emb", ")", "\n", "index_arxivid", "[", "i", "]", "=", "id", "\n", "i", "+=", "1", "\n", "", "x", "=", "np", ".", "array", "(", "x", ")", "\n", "\n", "# clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='cosine', linkage='complete')", "\n", "clustering", "=", "KMeans", "(", "n_clusters", "=", "n_clusters", ")", "\n", "y", "=", "clustering", ".", "fit_predict", "(", "x", ")", ".", "tolist", "(", ")", "\n", "\n", "arxivid_cluster", "=", "{", "}", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "y", ")", ":", "\n", "        ", "arxivid_cluster", "[", "index_arxivid", "[", "i", "]", "]", "=", "label", "\n", "\n", "", "if", "print_output", ":", "\n", "        ", "cluster_papers", "=", "defaultdict", "(", "list", ")", "\n", "for", "id", ",", "label", "in", "arxivid_cluster", ".", "items", "(", ")", ":", "\n", "            ", "cluster_papers", "[", "label", "]", ".", "append", "(", "datahandler", ".", "get_paper", "(", "id", ")", ")", "\n", "\n", "", "allzscores", "=", "[", "]", "\n", "for", "cluster", ",", "papers", "in", "cluster_papers", ".", "items", "(", ")", ":", "\n", "            ", "z_scores", "=", "[", "]", "\n", "print", "(", "'CLUSTER:%i\\n'", "%", "cluster", ")", "\n", "for", "paper", "in", "papers", ":", "\n", "                ", "z", "=", "paper", "[", "'z-score'", "]", "\n", "z_scores", ".", "append", "(", "z", ")", "\n", "print", "(", "\"%s\\t%s\\tnum_citations:%i\\tz-score: %f\"", "%", "(", "\n", "paper", "[", "'arxivid'", "]", ",", "paper", "[", "'title'", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", ",", "len", "(", "paper", "[", "'citations'", "]", ")", ",", "z", ")", ")", "\n", "", "print", "(", "\"MEAN Z-SCORE:%f\\n\\n\"", "%", "np", ".", "mean", "(", "z_scores", ")", ")", "\n", "allzscores", "+=", "z_scores", "\n", "", "print", "(", "\"MEAN Z-SCORE OVER ALL SAMPLES:%f\\n\\n\"", "%", "np", ".", "mean", "(", "allzscores", ")", ")", "\n", "\n", "", "return", "arxivid_cluster", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_semanticscholar_data.semantic_scholar_api": [[7, 31], ["print", "urllib.request.urlopen", "json.loads", "print", "url.read().decode", "url.read"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.mydate.DateTimeSerializer.decode"], ["def", "semantic_scholar_api", "(", "id", ")", ":", "\n", "    ", "\"\"\" Uses the sematic scholar api to get metadata and citation infrmation from semantic scholar\n    \n    Args:\n        id (TYPE): arxiv id\n    \n    Returns:\n        dict: metadata as a dict. None if paper not found\n    \"\"\"", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "query", "=", "\"http://api.semanticscholar.org/v1/paper/arXiv:\"", "+", "id", "\n", "print", "(", "query", ")", "\n", "with", "urllib", ".", "request", ".", "urlopen", "(", "query", ")", "as", "url", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "url", ".", "read", "(", ")", ".", "decode", "(", ")", ")", "\n", "data", "[", "'arxiv_id'", "]", "=", "id", "\n", "if", "'error'", "in", "data", ":", "\n", "                    ", "return", "None", "\n", "", "break", "\n", "", "", "except", "urllib", ".", "error", ".", "URLError", "as", "e", ":", "\n", "            ", "print", "(", "\"paper not found\"", ")", "\n", "return", "None", "\n", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_arxivid_docid_dict": [[32, 43], ["None"], "function", ["None"], ["def", "get_arxivid_docid_dict", "(", ")", ":", "\n", "    ", "\"\"\"Iterates over the whole db and crates a dict with the arxivids of a paper and\n    the document id in the dataset. \n    \n    Returns:\n        dict: dict with arxivids and the id of the document in the db\n    \"\"\"", "\n", "arxivid_docid", "=", "{", "}", "\n", "for", "paper", "in", "db", ":", "\n", "        ", "arxivid_docid", "[", "paper", "[", "'arxivid'", "]", "]", "=", "paper", ".", "doc_id", "\n", "", "return", "arxivid_docid", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.add_paper": [[52, 64], ["db.insert", "datahandler.update_paper"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["def", "add_paper", "(", "paper", ")", ":", "\n", "    ", "\"\"\"Adds a single paper to the database.\n    If a paper with the same arxiv id is already in the db, the paper will be updated and not inserted.\n    \n    Args:\n        paper (dict): a dict containing the keys arxivid,created,citations, authors, title, abstract \n    \"\"\"", "\n", "if", "paper", "[", "'arxivid'", "]", "not", "in", "arxivid_docid", ":", "\n", "        ", "did", "=", "db", ".", "insert", "(", "paper", ")", "\n", "arxivid_docid", "[", "paper", "[", "'arxivid'", "]", "]", "=", "did", "\n", "", "else", ":", "\n", "        ", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.update_paper": [[65, 73], ["db.update"], "function", ["None"], ["", "", "def", "update_paper", "(", "arxivid", ",", "fields", ")", ":", "\n", "    ", "\"\"\"Updates a paper in the db by its arxivid\n    \n    Args:\n        arxivid (str): the id the a paper has on arxiv\n        fields (dict): a dict containing a fields of the paper to update\n    \"\"\"", "\n", "db", ".", "update", "(", "fields", ",", "doc_ids", "=", "[", "arxivid_docid", "[", "arxivid", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.remove_paper": [[74, 81], ["db.remove"], "function", ["None"], ["", "def", "remove_paper", "(", "arxivid", ")", ":", "\n", "    ", "\"\"\"\n        remove one paper from the db\n    Args:\n        arxivid (str): the id the a paper has on arxiv\n    \"\"\"", "\n", "db", ".", "remove", "(", "doc_ids", "=", "[", "arxivid_docid", "[", "arxivid", "]", "]", ")", "\n", "", "def", "get_paper", "(", "arxivid", ")", ":", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_paper": [[81, 91], ["db.get"], "function", ["None"], ["", "def", "get_paper", "(", "arxivid", ")", ":", "\n", "    ", "\"\"\"Returns a paper in the db\n    \n    Args:\n        arxivid (str): the id the apaper has on arxiv\n    \n    Returns:\n        dict: The paper as a dict\n    \"\"\"", "\n", "return", "db", ".", "get", "(", "doc_id", "=", "arxivid_docid", "[", "arxivid", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.find_paper": [[92, 96], ["None"], "function", ["None"], ["", "def", "find_paper", "(", ")", ":", "\n", "    ", "\"\"\"TODO:implement \n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_papers_after": [[97, 108], ["tinydb.Query", "db.search"], "function", ["None"], ["", "def", "get_papers_after", "(", "startdate", ")", ":", "\n", "    ", "\"\"\"Gets all papers created after a date\n    \n    Args:\n        startdate (datetime.datetime): startdate\n    \n    Returns:\n        list: all papers creted after and on startdate\n    \"\"\"", "\n", "query", "=", "Query", "(", ")", "\n", "return", "db", ".", "search", "(", "query", ".", "created", ">=", "startdate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_papers_in_timewindow": [[109, 122], ["tinydb.Query", "db.search", "tinydb.Query.created.test"], "function", ["None"], ["", "def", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", ":", "\n", "    ", "\"\"\"Gets all papers created between two dates\n    \n    Args:\n        startdate (datetime.datetime): startdate\n        enddate (datetime.datetime): enddate\n    \n    Returns:\n        list: all papers creted after and on startdate and before and on enddate\n    \"\"\"", "\n", "\n", "query", "=", "Query", "(", ")", "\n", "return", "db", ".", "search", "(", "query", ".", "created", ".", "test", "(", "lambda", "d", ":", "startdate", "<=", "d", "<=", "enddate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_all_papers_iterator": [[123, 130], ["iter"], "function", ["None"], ["", "def", "get_all_papers_iterator", "(", ")", ":", "\n", "    ", "\"\"\"Returns an iterator object for all papers in the dataset\n    \n    Returns:\n        iterator: Iterator over all papers in the db\n    \"\"\"", "\n", "return", "iter", "(", "db", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler.get_arxivid_embedding": [[131, 150], ["open", "line.strip().split", "numpy.array", "line.strip", "float"], "function", ["None"], ["", "def", "get_arxivid_embedding", "(", "embedding", "=", "'infersent'", ",", "textpart", "=", "'abstract'", ")", ":", "\n", "    ", "\"\"\"returns a dict with arxiv ids and their infersent embedding. \n    Only includes papers if their embeddings had been calculated for that textpart\n    \n    Args:\n        embedding (str): infersent or unisent\n        textpart (str, optional): abstract or title\n    \n    Returns:\n        dict: dict with arxiv ids and their infersent embedding\n    \"\"\"", "\n", "arxivid_embedding", "=", "{", "}", "\n", "with", "open", "(", "'data/%s_%s.csv'", "%", "(", "embedding", ",", "textpart", ")", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "l", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "id", "=", "l", "[", "0", "]", "\n", "embedding", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "l", "[", "1", ":", "]", "]", ")", "\n", "arxivid_embedding", "[", "id", "]", "=", "embedding", "\n", "", "", "return", "arxivid_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.main": [[25, 37], ["parser.parse_args", "update_dataset.filter_wrong_papers", "update_dataset.harvest_new_papers", "update_dataset.filter_double_citations", "update_dataset.update_db_citations", "update_dataset.filter_double_citations"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_wrong_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.harvest_new_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_double_citations", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.update_db_citations", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_double_citations"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Usage: python3 update_dataset.py -h\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "newpapersstartdate", ":", "\n", "        ", "harvest_new_papers", "(", "args", ".", "newpapersstartdate", ",", "category", "=", "args", ".", "category", ")", "\n", "filter_double_citations", "(", "args", ".", "newpapersstartdate", ")", "\n", "", "if", "args", ".", "updatemetastartdate", ":", "\n", "        ", "update_db_citations", "(", "args", ".", "updatemetastartdate", ")", "\n", "filter_double_citations", "(", "args", ".", "updatemetastartdate", ")", "\n", "\n", "", "filter_wrong_papers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.harvest_new_papers": [[39, 71], ["datetime.date.today", "get_arxiv_data.harvest", "datahandler.get_papers_after", "startdate.strftime", "datetime.date.today.strftime", "get_semanticscholar_data.semantic_scholar_api", "update_dataset.calculate_z_score", "datahandler.update_paper", "datahandler.add_paper", "int", "paper[].append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_arxiv_data.harvest", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_semanticscholar_data.semantic_scholar_api", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.calculate_z_score", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.add_paper"], ["", "def", "harvest_new_papers", "(", "startdate", ",", "category", "=", "\"cs.LG\"", ")", ":", "\n", "    ", "\"\"\"harvestes new papers from arxiv, gets their citation counts and adds them to the db\n\n    Args:\n        startdate (datetime.datetime): startdate\n        category (str, optional): category on arxiv(default \"cs.LG\")\n    \"\"\"", "\n", "today", "=", "datetime", ".", "date", ".", "today", "(", ")", "\n", "papers", "=", "harvest", "(", "arxiv", "=", "\"cs\"", ",", "startdate", "=", "startdate", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", ",", "enddate", "=", "today", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", ")", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "if", "category", "in", "paper", "[", "'categories'", "]", "and", "paper", "[", "'created'", "]", ">=", "startdate", ":", "\n", "            ", "data", "=", "semantic_scholar_api", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "data", "and", "(", "paper", "[", "'created'", "]", ".", "year", "<=", "data", "[", "'year'", "]", ")", ":", "\n", "                ", "paper", "[", "'authors'", "]", "=", "data", "[", "'authors'", "]", "\n", "paper", "[", "'year'", "]", "=", "data", "[", "'year'", "]", "\n", "paper", "[", "'citations'", "]", "=", "[", "]", "\n", "for", "c", "in", "data", "[", "'citations'", "]", ":", "\n", "                    ", "try", ":", "\n", "                        ", "c", "[", "'year'", "]", "=", "int", "(", "c", "[", "'year'", "]", ")", "\n", "", "except", "TypeError", ":", "\n", "                        ", "continue", "\n", "", "if", "c", "[", "'year'", "]", ">=", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                        ", "paper", "[", "'citations'", "]", ".", "append", "(", "c", ")", "\n", "", "else", ":", "# If the paper has citations prior to the created date it has been published already somewhere else and will not be safed", "\n", "                        ", "break", "\n", "", "", "else", ":", "\n", "                    ", "datahandler", ".", "add_paper", "(", "paper", ")", "\n", "\n", "", "", "", "", "for", "paper", "in", "datahandler", ".", "get_papers_after", "(", "startdate", ")", ":", "\n", "        ", "if", "'citations'", "in", "paper", ":", "\n", "            ", "paper", "[", "'z-score'", "]", "=", "calculate_z_score", "(", "paper", ")", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.update_db_citations": [[73, 97], ["datahandler.get_papers_after", "get_semanticscholar_data.semantic_scholar_api", "datahandler.update_paper", "update_dataset.calculate_z_score", "datahandler.update_paper", "int", "paper[].append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_semanticscholar_data.semantic_scholar_api", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.calculate_z_score", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["", "", "", "def", "update_db_citations", "(", "startdate", ")", ":", "\n", "    ", "\"\"\"Updates citation counts and z-score for each paper in the db created before and on startdate\n\n    Args:\n        startdate (datetime.datetime): startdate\n    \"\"\"", "\n", "papers", "=", "datahandler", ".", "get_papers_after", "(", "startdate", ")", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "data", "=", "semantic_scholar_api", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "data", ":", "\n", "            ", "paper", "[", "'citations'", "]", "=", "[", "]", "\n", "for", "c", "in", "data", "[", "'citations'", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "c", "[", "'year'", "]", "=", "int", "(", "c", "[", "'year'", "]", ")", "\n", "", "except", "TypeError", ":", "\n", "                    ", "continue", "\n", "", "if", "c", "[", "'year'", "]", ">=", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                    ", "paper", "[", "'citations'", "]", ".", "append", "(", "c", ")", "\n", "", "", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n", "", "", "for", "paper", "in", "papers", ":", "\n", "        ", "if", "'citations'", "in", "paper", ":", "\n", "            ", "paper", "[", "'z-score'", "]", "=", "calculate_z_score", "(", "paper", ")", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.calculate_z_score": [[99, 131], ["datahandler.get_papers_in_timewindow", "len", "numpy.std", "numpy.mean", "datetime.timedelta", "datetime.timedelta", "len"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_in_timewindow"], ["", "", "", "def", "calculate_z_score", "(", "paper", ",", "timewindow_days", "=", "10", ")", ":", "\n", "    ", "\"\"\"calculates the z-score for a paper using papers gatherd from the db created in the same timewindow around (+-timewindow_days) \n    the creation of the paper\n\n    z-score defined by newman:\n    \"we take the count of citations received by a paper, subtract the\n    mean for papers published around the same time, and\n    divide by the standard deviation\"\n    Args:\n        paper (dict): paper in form of a dict containing the keys arxivid,created,citations, authors\n        timewindow_days (int, optional): timewindow of days to use as a context to calculate the z score(default 10)\n    \n    Returns:\n        float: z-score of the paper\n    \"\"\"", "\n", "startdate", "=", "paper", "[", "'created'", "]", "-", "datetime", ".", "timedelta", "(", "days", "=", "timewindow_days", ")", "\n", "enddate", "=", "paper", "[", "'created'", "]", "+", "datetime", ".", "timedelta", "(", "days", "=", "timewindow_days", ")", "\n", "papers_in_timewindow", "=", "datahandler", ".", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", "\n", "\n", "ncitations", "=", "len", "(", "paper", "[", "'citations'", "]", ")", "\n", "citations", "=", "[", "len", "(", "p", "[", "'citations'", "]", ")", "for", "p", "in", "papers_in_timewindow", "]", "\n", "\n", "std", "=", "np", ".", "std", "(", "citations", ")", "\n", "mean", "=", "np", ".", "mean", "(", "citations", ")", "\n", "z", "=", "0", "\n", "try", ":", "\n", "        ", "z", "=", "(", "ncitations", "-", "mean", ")", "/", "std", "\n", "", "except", "ZeroDivisionError", ":", "\n", "        ", "z", "=", "(", "ncitations", "-", "mean", ")", "\n", "", "if", "z", "!=", "z", ":", "\n", "        ", "z", "=", "0", "\n", "", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.filter_double_citations": [[133, 160], ["print", "datahandler.get_papers_after", "datahandler.get_all_papers_iterator", "set", "len", "len", "datahandler.update_paper", "set.add", "filtered_citations.append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["", "def", "filter_double_citations", "(", "startdate", "=", "None", ")", ":", "\n", "    ", "\"\"\" removes citations from a paper if they have the same title as another citation for the same papers\n\n    Args:\n        startdate (datetime.datetime): startdate. None to filter all papers\n    \"\"\"", "\n", "if", "startdate", ":", "\n", "        ", "papers", "=", "datahandler", ".", "get_papers_after", "(", "startdate", ")", "\n", "", "else", ":", "\n", "        ", "papers", "=", "datahandler", ".", "get_all_papers_iterator", "(", ")", "\n", "\n", "", "n", ",", "p", "=", "0", ",", "0", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "titles", "=", "set", "(", ")", "\n", "filtered_citations", "=", "[", "]", "\n", "for", "citation", "in", "paper", "[", "'citations'", "]", ":", "\n", "            ", "if", "not", "citation", "[", "'title'", "]", "in", "titles", ":", "\n", "                ", "titles", ".", "add", "(", "citation", "[", "'title'", "]", ")", "\n", "filtered_citations", ".", "append", "(", "citation", ")", "\n", "", "else", ":", "\n", "                ", "n", "+=", "1", "\n", "\n", "", "", "if", "len", "(", "filtered_citations", ")", "!=", "len", "(", "paper", "[", "'citations'", "]", ")", ":", "\n", "            ", "paper", "[", "'citations'", "]", "=", "filtered_citations", "\n", "p", "+=", "1", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "", "", "print", "(", "p", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.filter_wrong_papers": [[162, 207], ["set", "datahandler.get_all_papers_iterator", "datahandler.remove_paper", "update_dataset.is_duplicate", "paper[].lower", "set.add", "set.add", "paper[].lower", "paper[].lower"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.remove_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.is_duplicate"], ["", "def", "filter_wrong_papers", "(", ")", ":", "\n", "    ", "\"\"\"\n        removes papers from the database\n        if there is an older paper with the same title and authors\n        or citations with a year previous to the publication year\n\n    \"\"\"", "\n", "title_arxivid", "=", "{", "}", "\n", "npaper", "=", "0", "\n", "papers_to_remove", "=", "set", "(", ")", "\n", "for", "paper", "in", "datahandler", ".", "get_all_papers_iterator", "(", ")", ":", "\n", "        ", "npaper", "+=", "1", "\n", "if", "not", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "in", "title_arxivid", ":", "\n", "            ", "title_arxivid", "[", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "", "else", ":", "\n", "            ", "duplicate", ",", "toremove", "=", "is_duplicate", "(", "title_arxivid", "[", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "]", ",", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "duplicate", ":", "\n", "                ", "papers_to_remove", ".", "add", "(", "toremove", "[", "'arxivid'", "]", ")", "\n", "# print(toremove['title'],toremove['created'] )", "\n", "# datahandler.remove_paper(toremove['arxivid'])", "\n", "\n", "", "", "for", "citation", "in", "paper", "[", "'citations'", "]", ":", "\n", "            ", "if", "citation", "[", "'year'", "]", "<", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                ", "papers_to_remove", ".", "add", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "# print(\"year\", paper['title'], paper['created'], citation['title'], citation_date, len(paper['citations']))", "\n", "# if 'z-score' in paper:", "\n", "# print(paper['z-score'])", "\n", "\n", "", "\"\"\"\n            #should remove papers with citation previous to publication date.\n            #Does not work, becaus it is unclear weather the date of the citation or the paper is wrong\n            elif citation['title'].lower() in title_arxivid:\n                found += 1\n                citation_date = datahandler.get_paper(title_arxivid[citation['title'].lower()])['created']\n                if citation_date < paper['created']:\n                    print(paper['title'], paper['created'],citation['title'],citation_date, len(paper['citations']))\n                    filter = True\n                    if 'z-score' in paper:\n                        print(paper['z-score'])\n\n            else:\n                notfound += 1\n            \"\"\"", "\n", "", "", "for", "arxivid", "in", "papers_to_remove", ":", "\n", "        ", "datahandler", ".", "remove_paper", "(", "arxivid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset.is_duplicate": [[209, 234], ["datahandler.get_paper", "datahandler.get_paper", "p1[].lower", "p2[].lower", "set", "set"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "", "def", "is_duplicate", "(", "arxivid1", ",", "arxivid2", ")", ":", "\n", "    ", "\"\"\" checks weather two papers have the same author and title\n\n    Args:\n        arxivid1(str): arxivid of the first paper\n        arxivid2(str): arxivid of the second paper\n\n    Returns:\n        either True and the newer paper if they are duplicates\n        or False,None if they are no duplicates\n\n\n    \"\"\"", "\n", "p1", "=", "datahandler", ".", "get_paper", "(", "arxivid1", ")", "\n", "p2", "=", "datahandler", ".", "get_paper", "(", "arxivid2", ")", "\n", "if", "p1", "[", "'title'", "]", ".", "lower", "(", ")", "==", "p2", "[", "'title'", "]", ".", "lower", "(", ")", ":", "\n", "        ", "p1names", "=", "set", "(", "[", "a", "[", "'name'", "]", "for", "a", "in", "p1", "[", "'authors'", "]", "]", ")", "\n", "p2names", "=", "set", "(", "[", "a", "[", "'name'", "]", "for", "a", "in", "p2", "[", "'authors'", "]", "]", ")", "\n", "if", "p1names", "==", "p2names", ":", "\n", "            ", "if", "p1", "[", "'created'", "]", "<", "p2", "[", "'created'", "]", ":", "\n", "                ", "return", "True", ",", "p2", "\n", "", "else", ":", "\n", "                ", "return", "True", ",", "p1", "\n", "\n", "", "", "", "return", "False", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_arxiv_data2.harvest": [[25, 116], ["print", "urllib.request.urlopen.read", "xml.fromstring", "ET.fromstring.find().findall", "ET.fromstring.find().find", "urllib.request.urlopen", "record.find().find", "record.find", "record.find.find", "datetime.datetime.strptime", "meta.find.find().findall", "meta.find.find", "re.sub", "papers.append", "ET.fromstring.find", "meta.find.find", "meta.find.find", "authors.append", "meta.find.find", "meta.find.find().text.strip", "categories.split", "ET.fromstring.find", "int", "print", "time.sleep", "record.find", "meta.find.find", "author.find", "info.find.text.split", "meta.find.find", "e.hdrs.get", "author.find", "meta.find.find"], "function", ["None"], ["def", "harvest", "(", "arxiv", "=", "\"cs\"", ",", "startdate", "=", "\"2000-01-01\"", ",", "enddate", "=", "\"2017-12-31\"", ")", ":", "#physics:hep-ex", "\n", "    ", "\"\"\"\n    Harvestes metadata for a specific category on arxiv\n    \n    Args:\n        arxiv (str, optional): category on arxiv (cs, physics:hep-ex)\n    \n    Returns:\n        pandas dataframe: a dataframe with metadata harvested from arxiv\n    \"\"\"", "\n", "\n", "papers", "=", "[", "]", "\n", "base_url", "=", "\"http://export.arxiv.org/oai2?verb=ListRecords&\"", "\n", "url", "=", "(", "base_url", "+", "\n", "\"from=%s&until=%s&\"", "%", "(", "startdate", ",", "enddate", ")", "+", "\n", "\"metadataPrefix=arXiv&set=%s\"", "%", "arxiv", ")", "\n", "\n", "maxG", "=", "10", "\n", "i", "=", "0", "\n", "\n", "while", "True", ":", "\n", "        ", "print", "(", "\"fetching\"", ",", "url", ")", "\n", "try", ":", "\n", "            ", "response", "=", "urllib", ".", "request", ".", "urlopen", "(", "url", ")", "\n", "\n", "", "except", "urllib", ".", "error", ".", "HTTPError", "as", "e", ":", "\n", "            ", "if", "e", ".", "code", "==", "503", ":", "\n", "                ", "to", "=", "int", "(", "e", ".", "hdrs", ".", "get", "(", "\"retry-after\"", ",", "30", ")", ")", "\n", "print", "(", "\"Got 503. Retrying after {0:d} seconds.\"", ".", "format", "(", "to", ")", ")", "\n", "\n", "time", ".", "sleep", "(", "to", ")", "\n", "continue", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "xml", "=", "response", ".", "read", "(", ")", "\n", "\n", "root", "=", "ET", ".", "fromstring", "(", "xml", ")", "\n", "\n", "for", "record", "in", "root", ".", "find", "(", "OAI", "+", "'ListRecords'", ")", ".", "findall", "(", "OAI", "+", "\"record\"", ")", ":", "\n", "            ", "arxiv_id", "=", "record", ".", "find", "(", "OAI", "+", "'header'", ")", ".", "find", "(", "OAI", "+", "'identifier'", ")", "\n", "meta", "=", "record", ".", "find", "(", "OAI", "+", "'metadata'", ")", "\n", "info", "=", "meta", ".", "find", "(", "ARXIV", "+", "\"arXiv\"", ")", "\n", "created", "=", "info", ".", "find", "(", "ARXIV", "+", "\"created\"", ")", ".", "text", "\n", "created", "=", "datetime", ".", "datetime", ".", "strptime", "(", "created", ",", "\"%Y-%m-%d\"", ")", "\n", "categories", "=", "info", ".", "find", "(", "ARXIV", "+", "\"categories\"", ")", ".", "text", "\n", "#print(ET.tostring(info))", "\n", "authors", "=", "[", "]", "\n", "for", "author", "in", "info", ".", "find", "(", "ARXIV", "+", "\"authors\"", ")", ".", "findall", "(", "ARXIV", "+", "\"author\"", ")", ":", "\n", "                ", "a", "=", "{", "}", "\n", "\n", "a", "[", "'keyname'", "]", "=", "author", ".", "find", "(", "ARXIV", "+", "\"keyname\"", ")", ".", "text", "\n", "try", ":", "\n", "                    ", "a", "[", "'forenames'", "]", "=", "author", ".", "find", "(", "ARXIV", "+", "'forenames'", ")", ".", "text", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "                    ", "a", "[", "'forenames'", "]", "=", "''", "\n", "", "authors", ".", "append", "(", "a", ")", "\n", "# if there is more than one DOI use the first one", "\n", "# often the second one (if it exists at all) refers", "\n", "# to an eratum or similar", "\n", "", "doi", "=", "info", ".", "find", "(", "ARXIV", "+", "\"doi\"", ")", "\n", "if", "doi", "is", "not", "None", ":", "\n", "                ", "doi", "=", "doi", ".", "text", ".", "split", "(", ")", "[", "0", "]", "\n", "", "arxivid", "=", "info", ".", "find", "(", "ARXIV", "+", "\"id\"", ")", ".", "text", "\n", "arxivid", "=", "re", ".", "sub", "(", "'/'", ",", "''", ",", "arxivid", ")", "\n", "contents", "=", "{", "'title'", ":", "info", ".", "find", "(", "ARXIV", "+", "\"title\"", ")", ".", "text", ",", "\n", "'arxivid'", ":", "arxivid", ",", "\n", "'abstract'", ":", "info", ".", "find", "(", "ARXIV", "+", "\"abstract\"", ")", ".", "text", ".", "strip", "(", ")", ",", "\n", "'created'", ":", "created", ",", "\n", "'categories'", ":", "categories", ".", "split", "(", ")", ",", "\n", "'doi'", ":", "doi", ",", "\n", "'authors'", ":", "authors", "\n", "}", "\n", "\n", "papers", ".", "append", "(", "contents", ")", "\n", "\n", "# The list of articles returned by the API comes in chunks of", "\n", "# 1000 articles. The presence of a resumptionToken tells us that", "\n", "# there is more to be fetched.", "\n", "", "token", "=", "root", ".", "find", "(", "OAI", "+", "'ListRecords'", ")", ".", "find", "(", "OAI", "+", "\"resumptionToken\"", ")", "\n", "if", "token", "is", "None", "or", "token", ".", "text", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "else", ":", "\n", "            ", "url", "=", "base_url", "+", "\"resumptionToken=%s\"", "%", "(", "token", ".", "text", ")", "\n", "", "if", "i", ">", "maxG", ":", "\n", "          ", "break", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "papers", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.extractAbstract.retrieveAbstracts": [[7, 13], ["None"], "function", ["None"], ["def", "retrieveAbstracts", "(", "h", ",", "dta", ")", ":", "\n", "  ", "for", "x", "in", "dta", ":", "\n", "    ", "arxivid", "=", "dta", "[", "x", "]", "[", "\"arxivid\"", "]", "\n", "abstract", "=", "dta", "[", "x", "]", "[", "\"abstract\"", "]", "\n", "h", "[", "arxivid", "]", "=", "abstract", "\n", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.main": [[25, 37], ["parser.parse_args", "update_dataset_filterByDate.filter_wrong_papers", "update_dataset_filterByDate.harvest_new_papers", "update_dataset_filterByDate.filter_double_citations", "update_dataset_filterByDate.update_db_citations", "update_dataset_filterByDate.filter_double_citations"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_wrong_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.harvest_new_papers", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_double_citations", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.update_db_citations", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_double_citations"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"Usage: python3 update_dataset.py -h\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "newpapersstartdate", ":", "\n", "        ", "harvest_new_papers", "(", "args", ".", "newpapersstartdate", ",", "category", "=", "args", ".", "category", ")", "\n", "filter_double_citations", "(", "args", ".", "newpapersstartdate", ")", "\n", "", "if", "args", ".", "updatemetastartdate", ":", "\n", "        ", "update_db_citations", "(", "args", ".", "updatemetastartdate", ")", "\n", "filter_double_citations", "(", "args", ".", "updatemetastartdate", ")", "\n", "\n", "", "filter_wrong_papers", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.harvest_new_papers": [[39, 72], ["datetime.date.today", "get_arxiv_data2.harvest", "datahandler2.get_papers_after", "startdate.strftime", "datetime.date.today.strftime", "get_semanticscholar_data.semantic_scholar_api", "update_dataset_filterByDate.calculate_z_score", "datahandler2.update_paper", "print", "datahandler2.add_paper", "c.keys", "int", "paper[].append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_arxiv_data.harvest", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_semanticscholar_data.semantic_scholar_api", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.calculate_z_score", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.add_paper"], ["", "def", "harvest_new_papers", "(", "startdate", ",", "category", "=", "\"cs.LG\"", ")", ":", "\n", "    ", "\"\"\"harvestes new papers from arxiv, gets their citation counts and adds them to the db\n\n    Args:\n        startdate (datetime.datetime): startdate\n        category (str, optional): category on arxiv(default \"cs.LG\")\n    \"\"\"", "\n", "today", "=", "datetime", ".", "date", ".", "today", "(", ")", "\n", "papers", "=", "harvest", "(", "arxiv", "=", "\"cs\"", ",", "startdate", "=", "startdate", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", ",", "enddate", "=", "today", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", ")", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "if", "category", "in", "paper", "[", "'categories'", "]", "and", "paper", "[", "'created'", "]", ">=", "startdate", ":", "\n", "            ", "data", "=", "semantic_scholar_api", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "data", "and", "(", "paper", "[", "'created'", "]", ".", "year", "<=", "data", "[", "'year'", "]", ")", ":", "\n", "                ", "paper", "[", "'authors'", "]", "=", "data", "[", "'authors'", "]", "\n", "paper", "[", "'year'", "]", "=", "data", "[", "'year'", "]", "\n", "paper", "[", "'citations'", "]", "=", "[", "]", "\n", "for", "c", "in", "data", "[", "'citations'", "]", ":", "\n", "                    ", "print", "(", "c", ".", "keys", "(", ")", ",", "c", "[", "\"paperId\"", "]", ",", "paper", "[", "'arxivid'", "]", ")", "\n", "try", ":", "\n", "                        ", "c", "[", "'year'", "]", "=", "int", "(", "c", "[", "'year'", "]", ")", "\n", "", "except", "TypeError", ":", "\n", "                        ", "continue", "\n", "", "if", "c", "[", "'year'", "]", ">=", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                        ", "paper", "[", "'citations'", "]", ".", "append", "(", "c", ")", "\n", "", "else", ":", "# If the paper has citations prior to the created date it has been published already somewhere else and will not be safed", "\n", "                        ", "break", "\n", "", "", "else", ":", "\n", "                    ", "datahandler", ".", "add_paper", "(", "paper", ")", "\n", "\n", "", "", "", "", "for", "paper", "in", "datahandler", ".", "get_papers_after", "(", "startdate", ")", ":", "\n", "        ", "if", "'citations'", "in", "paper", ":", "\n", "            ", "paper", "[", "'z-score'", "]", "=", "calculate_z_score", "(", "paper", ")", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.update_db_citations": [[74, 98], ["datahandler2.get_papers_after", "get_semanticscholar_data.semantic_scholar_api", "datahandler2.update_paper", "update_dataset_filterByDate.calculate_z_score", "datahandler2.update_paper", "int", "paper[].append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_semanticscholar_data.semantic_scholar_api", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.calculate_z_score", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["", "", "", "def", "update_db_citations", "(", "startdate", ")", ":", "\n", "    ", "\"\"\"Updates citation counts and z-score for each paper in the db created before and on startdate\n\n    Args:\n        startdate (datetime.datetime): startdate\n    \"\"\"", "\n", "papers", "=", "datahandler", ".", "get_papers_after", "(", "startdate", ")", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "data", "=", "semantic_scholar_api", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "data", ":", "\n", "            ", "paper", "[", "'citations'", "]", "=", "[", "]", "\n", "for", "c", "in", "data", "[", "'citations'", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "c", "[", "'year'", "]", "=", "int", "(", "c", "[", "'year'", "]", ")", "\n", "", "except", "TypeError", ":", "\n", "                    ", "continue", "\n", "", "if", "c", "[", "'year'", "]", ">=", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                    ", "paper", "[", "'citations'", "]", ".", "append", "(", "c", ")", "\n", "", "", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n", "", "", "for", "paper", "in", "papers", ":", "\n", "        ", "if", "'citations'", "in", "paper", ":", "\n", "            ", "paper", "[", "'z-score'", "]", "=", "calculate_z_score", "(", "paper", ")", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.calculate_z_score": [[100, 132], ["datahandler2.get_papers_in_timewindow", "len", "numpy.std", "numpy.mean", "datetime.timedelta", "datetime.timedelta", "len"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_in_timewindow"], ["", "", "", "def", "calculate_z_score", "(", "paper", ",", "timewindow_days", "=", "10", ")", ":", "\n", "    ", "\"\"\"calculates the z-score for a paper using papers gatherd from the db created in the same timewindow around (+-timewindow_days) \n    the creation of the paper\n\n    z-score defined by newman:\n    \"we take the count of citations received by a paper, subtract the\n    mean for papers published around the same time, and\n    divide by the standard deviation\"\n    Args:\n        paper (dict): paper in form of a dict containing the keys arxivid,created,citations, authors\n        timewindow_days (int, optional): timewindow of days to use as a context to calculate the z score(default 10)\n    \n    Returns:\n        float: z-score of the paper\n    \"\"\"", "\n", "startdate", "=", "paper", "[", "'created'", "]", "-", "datetime", ".", "timedelta", "(", "days", "=", "timewindow_days", ")", "\n", "enddate", "=", "paper", "[", "'created'", "]", "+", "datetime", ".", "timedelta", "(", "days", "=", "timewindow_days", ")", "\n", "papers_in_timewindow", "=", "datahandler", ".", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", "\n", "\n", "ncitations", "=", "len", "(", "paper", "[", "'citations'", "]", ")", "\n", "citations", "=", "[", "len", "(", "p", "[", "'citations'", "]", ")", "for", "p", "in", "papers_in_timewindow", "]", "\n", "\n", "std", "=", "np", ".", "std", "(", "citations", ")", "\n", "mean", "=", "np", ".", "mean", "(", "citations", ")", "\n", "z", "=", "0", "\n", "try", ":", "\n", "        ", "z", "=", "(", "ncitations", "-", "mean", ")", "/", "std", "\n", "", "except", "ZeroDivisionError", ":", "\n", "        ", "z", "=", "(", "ncitations", "-", "mean", ")", "\n", "", "if", "z", "!=", "z", ":", "\n", "        ", "z", "=", "0", "\n", "", "return", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_double_citations": [[134, 161], ["print", "datahandler2.get_papers_after", "datahandler2.get_all_papers_iterator", "set", "len", "len", "datahandler2.update_paper", "set.add", "filtered_citations.append"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["", "def", "filter_double_citations", "(", "startdate", "=", "None", ")", ":", "\n", "    ", "\"\"\" removes citations from a paper if they have the same title as another citation for the same papers\n\n    Args:\n        startdate (datetime.datetime): startdate. None to filter all papers\n    \"\"\"", "\n", "if", "startdate", ":", "\n", "        ", "papers", "=", "datahandler", ".", "get_papers_after", "(", "startdate", ")", "\n", "", "else", ":", "\n", "        ", "papers", "=", "datahandler", ".", "get_all_papers_iterator", "(", ")", "\n", "\n", "", "n", ",", "p", "=", "0", ",", "0", "\n", "for", "paper", "in", "papers", ":", "\n", "        ", "titles", "=", "set", "(", ")", "\n", "filtered_citations", "=", "[", "]", "\n", "for", "citation", "in", "paper", "[", "'citations'", "]", ":", "\n", "            ", "if", "not", "citation", "[", "'title'", "]", "in", "titles", ":", "\n", "                ", "titles", ".", "add", "(", "citation", "[", "'title'", "]", ")", "\n", "filtered_citations", ".", "append", "(", "citation", ")", "\n", "", "else", ":", "\n", "                ", "n", "+=", "1", "\n", "\n", "", "", "if", "len", "(", "filtered_citations", ")", "!=", "len", "(", "paper", "[", "'citations'", "]", ")", ":", "\n", "            ", "paper", "[", "'citations'", "]", "=", "filtered_citations", "\n", "p", "+=", "1", "\n", "datahandler", ".", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "", "", "print", "(", "p", ",", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.filter_wrong_papers": [[163, 208], ["set", "datahandler2.get_all_papers_iterator", "datahandler2.remove_paper", "update_dataset_filterByDate.is_duplicate", "paper[].lower", "set.add", "set.add", "paper[].lower", "paper[].lower"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.remove_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.is_duplicate"], ["", "def", "filter_wrong_papers", "(", ")", ":", "\n", "    ", "\"\"\"\n        removes papers from the database\n        if there is an older paper with the same title and authors\n        or citations with a year previous to the publication year\n\n    \"\"\"", "\n", "title_arxivid", "=", "{", "}", "\n", "npaper", "=", "0", "\n", "papers_to_remove", "=", "set", "(", ")", "\n", "for", "paper", "in", "datahandler", ".", "get_all_papers_iterator", "(", ")", ":", "\n", "        ", "npaper", "+=", "1", "\n", "if", "not", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "in", "title_arxivid", ":", "\n", "            ", "title_arxivid", "[", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "]", "=", "paper", "[", "'arxivid'", "]", "\n", "", "else", ":", "\n", "            ", "duplicate", ",", "toremove", "=", "is_duplicate", "(", "title_arxivid", "[", "paper", "[", "'title'", "]", ".", "lower", "(", ")", "]", ",", "paper", "[", "'arxivid'", "]", ")", "\n", "if", "duplicate", ":", "\n", "                ", "papers_to_remove", ".", "add", "(", "toremove", "[", "'arxivid'", "]", ")", "\n", "# print(toremove['title'],toremove['created'] )", "\n", "# datahandler.remove_paper(toremove['arxivid'])", "\n", "\n", "", "", "for", "citation", "in", "paper", "[", "'citations'", "]", ":", "\n", "            ", "if", "citation", "[", "'year'", "]", "<", "paper", "[", "'created'", "]", ".", "year", ":", "\n", "                ", "papers_to_remove", ".", "add", "(", "paper", "[", "'arxivid'", "]", ")", "\n", "# print(\"year\", paper['title'], paper['created'], citation['title'], citation_date, len(paper['citations']))", "\n", "# if 'z-score' in paper:", "\n", "# print(paper['z-score'])", "\n", "\n", "", "\"\"\"\n            #should remove papers with citation previous to publication date.\n            #Does not work, becaus it is unclear weather the date of the citation or the paper is wrong\n            elif citation['title'].lower() in title_arxivid:\n                found += 1\n                citation_date = datahandler.get_paper(title_arxivid[citation['title'].lower()])['created']\n                if citation_date < paper['created']:\n                    print(paper['title'], paper['created'],citation['title'],citation_date, len(paper['citations']))\n                    filter = True\n                    if 'z-score' in paper:\n                        print(paper['z-score'])\n\n            else:\n                notfound += 1\n            \"\"\"", "\n", "", "", "for", "arxivid", "in", "papers_to_remove", ":", "\n", "        ", "datahandler", ".", "remove_paper", "(", "arxivid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.update_dataset_filterByDate.is_duplicate": [[210, 235], ["datahandler2.get_paper", "datahandler2.get_paper", "p1[].lower", "p2[].lower", "set", "set"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper"], ["", "", "def", "is_duplicate", "(", "arxivid1", ",", "arxivid2", ")", ":", "\n", "    ", "\"\"\" checks weather two papers have the same author and title\n\n    Args:\n        arxivid1(str): arxivid of the first paper\n        arxivid2(str): arxivid of the second paper\n\n    Returns:\n        either True and the newer paper if they are duplicates\n        or False,None if they are no duplicates\n\n\n    \"\"\"", "\n", "p1", "=", "datahandler", ".", "get_paper", "(", "arxivid1", ")", "\n", "p2", "=", "datahandler", ".", "get_paper", "(", "arxivid2", ")", "\n", "if", "p1", "[", "'title'", "]", ".", "lower", "(", ")", "==", "p2", "[", "'title'", "]", ".", "lower", "(", ")", ":", "\n", "        ", "p1names", "=", "set", "(", "[", "a", "[", "'name'", "]", "for", "a", "in", "p1", "[", "'authors'", "]", "]", ")", "\n", "p2names", "=", "set", "(", "[", "a", "[", "'name'", "]", "for", "a", "in", "p2", "[", "'authors'", "]", "]", ")", "\n", "if", "p1names", "==", "p2names", ":", "\n", "            ", "if", "p1", "[", "'created'", "]", "<", "p2", "[", "'created'", "]", ":", "\n", "                ", "return", "True", ",", "p2", "\n", "", "else", ":", "\n", "                ", "return", "True", ",", "p1", "\n", "\n", "", "", "", "return", "False", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.mydate.DateTimeSerializer.encode": [[14, 16], ["obj.strftime"], "methods", ["None"], ["def", "encode", "(", "self", ",", "obj", ")", ":", "\n", "        ", "return", "obj", ".", "strftime", "(", "'%Y-%m-%d'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.mydate.DateTimeSerializer.decode": [[17, 19], ["datetime.datetime.datetime.strptime"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "s", ")", ":", "\n", "        ", "return", "datetime", ".", "strptime", "(", "s", ",", "'%Y-%m-%d'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.mydate.valid_date": [[4, 10], ["datetime.datetime.strptime", "argparse.ArgumentTypeError"], "function", ["None"], ["def", "valid_date", "(", "s", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "datetime", ".", "strptime", "(", "s", ",", "\"%Y-%m-%d\"", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "msg", "=", "\"Not a valid date: '{0}'.\"", ".", "format", "(", "s", ")", "\n", "raise", "argparse", ".", "ArgumentTypeError", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.main": [[31, 133], ["parser.parse_args", "train_models.save_vocab", "numpy.array", "sklearn.model_selection.train_test_split", "open", "open", "numpy.ma.log().filled", "numpy.array", "range", "range", "train_models.vectorize", "numpy.array", "labels.append", "open", "open", "numpy.ma.log().filled", "numpy.random.uniform", "train_models.regression_model", "sklearn.externals.joblib.dump", "numpy.random.uniform", "numpy.random.choice", "numpy.random.choice", "numpy.random.choice", "train_models.mlp", "train.append", "train.append", "int", "numpy.ma.log", "train_models.vectorize", "numpy.array", "labelstest.append", "mlp.predict", "print", "mlp.predict", "print", "numpy.arange", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "print", "print", "line.strip", "line.strip().split", "line.strip", "test.append", "test.append", "int", "numpy.ma.log", "train_models.evaluate", "train_models.evaluate", "train_models.evaluate", "train_models.evaluate", "line.strip", "line.strip().split", "line.strip", "mlp.predict().tolist", "mlp.predict().tolist", "line.strip", "line.strip", "mlp.predict", "mlp.predict", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.save_vocab", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.vectorize", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.regression_model", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.mlp", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.vectorize", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.evaluate", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.evaluate", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.evaluate", "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.evaluate"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    python3 train_models.py -h\n    Examples:\n        python3 train_models.py mlp embeddings data/train/abstractunisent.train data/train/unisentlabel.train.real -tf data/train/abstractunisent.test -tl data/train/unisentlabel.test.real  -log -dev 0.1\n        python3 train_models.py mlp embeddings data/train/abstractinfersent.train data/train/infersentlabel.train.real -tf data/train/abstractinfersent.test -tl data/train/infersentlabel.test.real  -log -dev 0.1\n        python3 train_models.py reg tfidf data/train/abstract.train data/train/label.train.real -tf data/train/abstract.test -tl data/train/label.test.real -log -dev 0.1\n        python3 train_models.py mlp embeddings data/train/titleunisent.train data/train/unisentlabel.train.real -tf data/train/titleunisent.test -tl data/train/unisentlabel.test.real  -log -dev 0.1\n        python3 train_models.py mlp embeddings data/train/titleinfersent.train data/train/infersentlabel.train.real -tf data/train/titleinfersent.test -tl data/train/infersentlabel.test.real  -log -dev 0.1\n        python3 train_models.py reg tfidf data/train/title.train data/train/label.train.real -tf data/train/title.test -tl data/train/label.test.real -log -dev 0.1\n    \"\"\"", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "trainfile", "=", "args", ".", "trainfile", "\n", "labelfile", "=", "args", ".", "trainlabelsfile", "\n", "testfile", "=", "args", ".", "testfile", "\n", "testlabelfile", "=", "args", ".", "testlabelsfile", "\n", "\n", "save_vocab", "(", ")", "\n", "nummodels", "=", "args", ".", "nummodels", "\n", "regression", "=", "True", "\n", "devsplit", "=", "args", ".", "devsplit", "\n", "alphalow", ",", "alphahigh", "=", "0.1", ",", "1.0", "\n", "\n", "train", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "trainfile", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "if", "args", ".", "representation", "==", "'tfidf'", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "train", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "X", "=", "vectorize", "(", "train", ")", "\n", "", "else", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "train", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "X", "=", "np", ".", "array", "(", "train", ",", "dtype", "=", "'float64'", ")", "\n", "", "", "with", "open", "(", "labelfile", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "labels", ".", "append", "(", "int", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "\n", "", "", "y", "=", "np", ".", "array", "(", "labels", ",", "dtype", "=", "'int32'", ")", "\n", "if", "args", ".", "log_labels", ":", "\n", "        ", "y", "=", "np", ".", "ma", ".", "log", "(", "y", ")", ".", "filled", "(", "0", ")", "\n", "", "if", "testfile", ":", "\n", "        ", "test", "=", "[", "]", "\n", "labelstest", "=", "[", "]", "\n", "\n", "with", "open", "(", "testfile", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "if", "args", ".", "representation", "==", "'tfidf'", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "test", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "X_test", "=", "vectorize", "(", "test", ")", "\n", "", "else", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "test", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "X_test", "=", "np", ".", "array", "(", "test", ",", "dtype", "=", "'float64'", ")", "\n", "\n", "", "", "with", "open", "(", "testlabelfile", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "labelstest", ".", "append", "(", "int", "(", "line", ".", "strip", "(", ")", ")", ")", "\n", "", "", "y_test", "=", "np", ".", "array", "(", "labelstest", ",", "dtype", "=", "'int32'", ")", "\n", "if", "args", ".", "log_labels", ":", "\n", "            ", "y_test", "=", "np", ".", "ma", ".", "log", "(", "y_test", ")", ".", "filled", "(", "0", ")", "\n", "\n", "\n", "\n", "", "", "X_train", ",", "X_dev", ",", "y_train", ",", "y_dev", "=", "train_test_split", "(", "X", ",", "y", ",", "test_size", "=", "devsplit", ")", "\n", "\n", "if", "args", ".", "model", "==", "'reg'", ":", "\n", "        ", "for", "i", "in", "range", "(", "nummodels", ")", ":", "\n", "            ", "filename", "=", "'models/regression/reg%i.%s.model'", "%", "(", "i", ",", "args", ".", "textpart", ")", "\n", "a", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "alphalow", ",", "high", "=", "alphahigh", ",", "size", "=", "None", ")", "\n", "model", "=", "regression_model", "(", "X_train", ",", "y_train", ",", "alpha", "=", "a", ")", "\n", "joblib", ".", "dump", "(", "model", ",", "filename", ")", "\n", "if", "devsplit", ">", "0", ":", "\n", "                ", "pred", "=", "model", ".", "predict", "(", "X_dev", ")", "\n", "print", "(", "a", ",", "evaluate", "(", "y_dev", ",", "pred", ")", ")", "\n", "\n", "", "if", "testfile", ":", "\n", "                ", "pred", "=", "model", ".", "predict", "(", "X_test", ")", "\n", "print", "(", "'test:'", ",", "evaluate", "(", "y_test", ",", "pred", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "model", "==", "'mlp'", ":", "\n", "        ", "for", "i", "in", "range", "(", "nummodels", ")", ":", "\n", "            ", "filename", "=", "'models/regression/mlp%i.%s.model'", "%", "(", "i", ",", "args", ".", "textpart", ")", "\n", "a", "=", "np", ".", "random", ".", "uniform", "(", "low", "=", "alphalow", ",", "high", "=", "alphahigh", ",", "size", "=", "None", ")", "\n", "hidden_layer_size", "=", "np", ".", "random", ".", "choice", "(", "[", "100", "]", ")", "\n", "dropout", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "0.1", ",", "0.31", ",", "0.05", ")", ")", "\n", "activation", "=", "np", ".", "random", ".", "choice", "(", "[", "'relu'", "]", ")", "\n", "\n", "model", "=", "mlp", "(", "np", ".", "array", "(", "X_train", ")", ",", "np", ".", "array", "(", "y_train", ")", ",", "np", ".", "array", "(", "X_dev", ")", ",", "np", ".", "array", "(", "y_dev", ")", ",", "filename", ",", "\n", "regression", "=", "regression", ",", "\n", "hidden_layers", "=", "1", ",", "hidden_layer_size", "=", "hidden_layer_size", ",", "dropout", "=", "dropout", ",", "activation", "=", "activation", ")", "\n", "\n", "if", "devsplit", ">", "0", ":", "\n", "                ", "pred", "=", "[", "x", "[", "0", "]", "for", "x", "in", "model", ".", "predict", "(", "np", ".", "array", "(", "X_dev", ")", ")", ".", "tolist", "(", ")", "]", "\n", "print", "(", "a", ",", "evaluate", "(", "y_dev", ",", "pred", ")", ")", "\n", "\n", "", "if", "testfile", ":", "\n", "                ", "pred", "=", "[", "x", "[", "0", "]", "for", "x", "in", "model", ".", "predict", "(", "np", ".", "array", "(", "X_test", ")", ")", ".", "tolist", "(", ")", "]", "\n", "print", "(", "'test:'", ",", "evaluate", "(", "y_test", ",", "pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.save_vocab": [[135, 151], ["set", "open", "open", "f.write", "f.write", "line.strip().split", "set.add", "line.strip"], "function", ["None"], ["", "", "", "", "def", "save_vocab", "(", ")", ":", "\n", "    ", "\"\"\"\n        Saves the vocab of the trainings files to reuse for tfidf\n    \"\"\"", "\n", "files", "=", "[", "'data/train/abstract.train'", ",", "'data/train/title.train'", "]", "\n", "words", "=", "set", "(", ")", "\n", "for", "file", "in", "files", ":", "\n", "        ", "with", "open", "(", "file", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "for", "word", "in", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ":", "\n", "                    ", "words", ".", "add", "(", "word", ")", "\n", "\n", "", "", "", "", "with", "open", "(", "'data/vocab.dat'", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "words", ":", "\n", "            ", "f", ".", "write", "(", "word", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.load_vocab": [[153, 164], ["open", "words.append", "line.strip"], "function", ["None"], ["", "", "", "def", "load_vocab", "(", ")", ":", "\n", "    ", "\"\"\"\n    loads the vocab\n    Returns: list of words\n\n    \"\"\"", "\n", "words", "=", "[", "]", "\n", "with", "open", "(", "'data/vocab.dat'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "words", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.vectorize": [[166, 180], ["train_models.load_vocab", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.load_vocab"], ["", "def", "vectorize", "(", "documents", ")", ":", "\n", "    ", "\"\"\"\n    encodes documents with tfidf\n    Args:\n        documents: list of documents as strings\n\n    Returns: Sparse matrix\n\n    \"\"\"", "\n", "vocab", "=", "load_vocab", "(", ")", "\n", "vec", "=", "TfidfVectorizer", "(", "\n", "lowercase", "=", "False", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ",", "vocabulary", "=", "vocab", ")", "\n", "\n", "return", "vec", ".", "fit_transform", "(", "documents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.regression_model": [[183, 198], ["sklearn.linear_model.Ridge", "linear_model.Ridge.fit"], "function", ["None"], ["", "def", "regression_model", "(", "X", ",", "y", ",", "alpha", "=", ".5", ")", ":", "\n", "    ", "\"\"\"\n        trains a simple ridge regession model\n    Args:\n        X:\n        y:\n        alpha:\n\n    Returns: model\n\n    \"\"\"", "\n", "reg", "=", "linear_model", ".", "Ridge", "(", "alpha", "=", "alpha", ",", "fit_intercept", "=", "True", ")", "\n", "# reg = linear_model.Lasso(alpha = alpha,fit_intercept = True)", "\n", "reg", ".", "fit", "(", "X", ",", "y", ")", "\n", "return", "reg", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.mlp": [[200, 236], ["keras.layers.Input", "range", "keras.callbacks.EarlyStopping", "keras.models.Model.fit", "keras.models.load_model", "keras.layers.Dense", "keras.layers.Dropout", "keras.callbacks.ModelCheckpoint", "keras.models.Model", "keras.models.Model.compile", "keras.callbacks.ModelCheckpoint", "set", "len", "keras.models.Model", "keras.models.Model.compile", "keras.layers.Dense", "keras.layers.Dropout", "keras.layers.Dense", "y.tolist", "y_dev.tolist", "set.add", "keras.layers.Dense"], "function", ["None"], ["", "def", "mlp", "(", "X", ",", "y", ",", "X_dev", ",", "y_dev", ",", "filename", ",", "regression", "=", "True", ",", "hidden_layers", "=", "1", ",", "hidden_layer_size", "=", "100", ",", "dropout", "=", "0.25", ",", "\n", "activation", "=", "'relu'", ",", "batch_size", "=", "10", ",", "maxepochs", "=", "100", ")", ":", "\n", "    ", "input_shape", "=", "X", ".", "shape", "\n", "inputs", "=", "Input", "(", "shape", "=", "(", "input_shape", "[", "1", "]", ",", ")", ")", "\n", "x", "=", "Dense", "(", "hidden_layer_size", ",", "activation", "=", "activation", ")", "(", "inputs", ")", "\n", "x", "=", "Dropout", "(", "dropout", ")", "(", "x", ")", "\n", "for", "i", "in", "range", "(", "hidden_layers", "-", "1", ")", ":", "\n", "        ", "x", "=", "Dense", "(", "hidden_layer_size", ",", "activation", "=", "activation", ")", "(", "x", ")", "\n", "x", "=", "Dropout", "(", "dropout", ")", "(", "x", ")", "\n", "\n", "", "if", "regression", ":", "\n", "        ", "checkpointer", "=", "ModelCheckpoint", "(", "\n", "filename", ",", "monitor", "=", "'val_loss'", ",", "verbose", "=", "0", ",", "save_best_only", "=", "True", ",", "save_weights_only", "=", "False", ")", "\n", "\n", "out", "=", "Dense", "(", "1", ",", "activation", "=", "'relu'", ")", "(", "x", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "inputs", ",", "outputs", "=", "out", ")", "\n", "model", ".", "compile", "(", "optimizer", "=", "'adam'", ",", "loss", "=", "'mean_squared_error'", ")", "\n", "", "else", ":", "\n", "        ", "checkpointer", "=", "ModelCheckpoint", "(", "\n", "filename", ",", "monitor", "=", "'val_acc'", ",", "verbose", "=", "0", ",", "save_best_only", "=", "True", ",", "save_weights_only", "=", "False", ")", "\n", "\n", "classes", "=", "set", "(", ")", "\n", "for", "label", "in", "(", "y", ".", "tolist", "(", ")", "+", "y_dev", ".", "tolist", "(", ")", ")", ":", "\n", "            ", "classes", ".", "add", "(", "label", ")", "\n", "", "num_classes", "=", "len", "(", "classes", ")", "\n", "out", "=", "Dense", "(", "num_classes", ",", "activation", "=", "'softmax'", ")", "(", "x", ")", "\n", "model", "=", "Model", "(", "inputs", "=", "inputs", ",", "outputs", "=", "out", ")", "\n", "model", ".", "compile", "(", "optimizer", "=", "'adam'", ",", "\n", "loss", "=", "'categorical_crossentropy'", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "\n", "", "es", "=", "EarlyStopping", "(", "monitor", "=", "'val_loss'", ",", "min_delta", "=", "0", ",", "patience", "=", "5", ",", "verbose", "=", "0", ",", "mode", "=", "'auto'", ")", "\n", "\n", "model", ".", "fit", "(", "X", ",", "y", ",", "validation_data", "=", "(", "X_dev", ",", "y_dev", ")", ",", "callbacks", "=", "[", "checkpointer", ",", "es", "]", ",", "batch_size", "=", "batch_size", ",", "\n", "epochs", "=", "maxepochs", ",", "verbose", "=", "0", ")", "\n", "model", "=", "load_model", "(", "filename", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.train_models.evaluate": [[240, 247], ["scipy.stats.pearsonr", "zip", "print"], "function", ["None"], ["", "def", "evaluate", "(", "true", ",", "pred", ",", "verbose", "=", "False", ")", ":", "\n", "# acc = accuracy_score(true,pred)", "\n", "    ", "if", "verbose", ":", "\n", "        ", "for", "t", ",", "p", "in", "zip", "(", "true", ",", "pred", ")", ":", "\n", "            ", "print", "(", "t", ",", "p", ")", "\n", "", "", "corr", ",", "p", "=", "pearsonr", "(", "true", ",", "pred", ")", "\n", "return", "corr", ",", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.get_arxiv_data.harvest": [[25, 110], ["print", "urllib.request.urlopen.read", "xml.fromstring", "ET.fromstring.find().findall", "ET.fromstring.find().find", "urllib.request.urlopen", "record.find().find", "record.find", "record.find.find", "datetime.datetime.strptime", "meta.find.find().findall", "meta.find.find", "re.sub", "papers.append", "ET.fromstring.find", "meta.find.find", "meta.find.find", "authors.append", "meta.find.find", "meta.find.find().text.strip", "categories.split", "ET.fromstring.find", "int", "print", "time.sleep", "record.find", "meta.find.find", "author.find", "info.find.text.split", "meta.find.find", "e.hdrs.get", "author.find", "meta.find.find"], "function", ["None"], ["def", "harvest", "(", "arxiv", "=", "\"cs\"", ",", "startdate", "=", "\"2000-01-01\"", ",", "enddate", "=", "\"2017-12-31\"", ")", ":", "#physics:hep-ex", "\n", "    ", "\"\"\"\n    Harvestes metadata for a specific category on arxiv\n    \n    Args:\n        arxiv (str, optional): category on arxiv (cs, physics:hep-ex)\n    \n    Returns:\n        pandas dataframe: a dataframe with metadata harvested from arxiv\n    \"\"\"", "\n", "\n", "papers", "=", "[", "]", "\n", "base_url", "=", "\"http://export.arxiv.org/oai2?verb=ListRecords&\"", "\n", "url", "=", "(", "base_url", "+", "\n", "\"from=%s&until=%s&\"", "%", "(", "startdate", ",", "enddate", ")", "+", "\n", "\"metadataPrefix=arXiv&set=%s\"", "%", "arxiv", ")", "\n", "\n", "while", "True", ":", "\n", "        ", "print", "(", "\"fetching\"", ",", "url", ")", "\n", "try", ":", "\n", "            ", "response", "=", "urllib", ".", "request", ".", "urlopen", "(", "url", ")", "\n", "\n", "", "except", "urllib", ".", "error", ".", "HTTPError", "as", "e", ":", "\n", "            ", "if", "e", ".", "code", "==", "503", ":", "\n", "                ", "to", "=", "int", "(", "e", ".", "hdrs", ".", "get", "(", "\"retry-after\"", ",", "30", ")", ")", "\n", "print", "(", "\"Got 503. Retrying after {0:d} seconds.\"", ".", "format", "(", "to", ")", ")", "\n", "\n", "time", ".", "sleep", "(", "to", ")", "\n", "continue", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "xml", "=", "response", ".", "read", "(", ")", "\n", "\n", "root", "=", "ET", ".", "fromstring", "(", "xml", ")", "\n", "\n", "for", "record", "in", "root", ".", "find", "(", "OAI", "+", "'ListRecords'", ")", ".", "findall", "(", "OAI", "+", "\"record\"", ")", ":", "\n", "            ", "arxiv_id", "=", "record", ".", "find", "(", "OAI", "+", "'header'", ")", ".", "find", "(", "OAI", "+", "'identifier'", ")", "\n", "meta", "=", "record", ".", "find", "(", "OAI", "+", "'metadata'", ")", "\n", "info", "=", "meta", ".", "find", "(", "ARXIV", "+", "\"arXiv\"", ")", "\n", "created", "=", "info", ".", "find", "(", "ARXIV", "+", "\"created\"", ")", ".", "text", "\n", "created", "=", "datetime", ".", "datetime", ".", "strptime", "(", "created", ",", "\"%Y-%m-%d\"", ")", "\n", "categories", "=", "info", ".", "find", "(", "ARXIV", "+", "\"categories\"", ")", ".", "text", "\n", "#print(ET.tostring(info))", "\n", "authors", "=", "[", "]", "\n", "for", "author", "in", "info", ".", "find", "(", "ARXIV", "+", "\"authors\"", ")", ".", "findall", "(", "ARXIV", "+", "\"author\"", ")", ":", "\n", "                ", "a", "=", "{", "}", "\n", "\n", "a", "[", "'keyname'", "]", "=", "author", ".", "find", "(", "ARXIV", "+", "\"keyname\"", ")", ".", "text", "\n", "try", ":", "\n", "                    ", "a", "[", "'forenames'", "]", "=", "author", ".", "find", "(", "ARXIV", "+", "'forenames'", ")", ".", "text", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "                    ", "a", "[", "'forenames'", "]", "=", "''", "\n", "", "authors", ".", "append", "(", "a", ")", "\n", "# if there is more than one DOI use the first one", "\n", "# often the second one (if it exists at all) refers", "\n", "# to an eratum or similar", "\n", "", "doi", "=", "info", ".", "find", "(", "ARXIV", "+", "\"doi\"", ")", "\n", "if", "doi", "is", "not", "None", ":", "\n", "                ", "doi", "=", "doi", ".", "text", ".", "split", "(", ")", "[", "0", "]", "\n", "", "arxivid", "=", "info", ".", "find", "(", "ARXIV", "+", "\"id\"", ")", ".", "text", "\n", "arxivid", "=", "re", ".", "sub", "(", "'/'", ",", "''", ",", "arxivid", ")", "\n", "contents", "=", "{", "'title'", ":", "info", ".", "find", "(", "ARXIV", "+", "\"title\"", ")", ".", "text", ",", "\n", "'arxivid'", ":", "arxivid", ",", "\n", "'abstract'", ":", "info", ".", "find", "(", "ARXIV", "+", "\"abstract\"", ")", ".", "text", ".", "strip", "(", ")", ",", "\n", "'created'", ":", "created", ",", "\n", "'categories'", ":", "categories", ".", "split", "(", ")", ",", "\n", "'doi'", ":", "doi", ",", "\n", "'authors'", ":", "authors", "\n", "}", "\n", "\n", "papers", ".", "append", "(", "contents", ")", "\n", "\n", "# The list of articles returned by the API comes in chunks of", "\n", "# 1000 articles. The presence of a resumptionToken tells us that", "\n", "# there is more to be fetched.", "\n", "", "token", "=", "root", ".", "find", "(", "OAI", "+", "'ListRecords'", ")", ".", "find", "(", "OAI", "+", "\"resumptionToken\"", ")", "\n", "if", "token", "is", "None", "or", "token", ".", "text", "is", "None", ":", "\n", "            ", "break", "\n", "\n", "", "else", ":", "\n", "            ", "url", "=", "base_url", "+", "\"resumptionToken=%s\"", "%", "(", "token", ".", "text", ")", "\n", "\n", "", "", "return", "papers", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_arxivid_docid_dict": [[27, 38], ["None"], "function", ["None"], ["def", "get_arxivid_docid_dict", "(", ")", ":", "\n", "    ", "\"\"\"Iterates over the whole db and crates a dict with the arxivids of a paper and\n    the document id in the dataset. \n    \n    Returns:\n        dict: dict with arxivids and the id of the document in the db\n    \"\"\"", "\n", "arxivid_docid", "=", "{", "}", "\n", "for", "paper", "in", "db", ":", "\n", "        ", "arxivid_docid", "[", "paper", "[", "'arxivid'", "]", "]", "=", "paper", ".", "doc_id", "\n", "", "return", "arxivid_docid", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.add_paper": [[47, 59], ["db.insert", "datahandler2.update_paper"], "function", ["home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper"], ["def", "add_paper", "(", "paper", ")", ":", "\n", "    ", "\"\"\"Adds a single paper to the database.\n    If a paper with the same arxiv id is already in the db, the paper will be updated and not inserted.\n    \n    Args:\n        paper (dict): a dict containing the keys arxivid,created,citations, authors, title, abstract \n    \"\"\"", "\n", "if", "paper", "[", "'arxivid'", "]", "not", "in", "arxivid_docid", ":", "\n", "        ", "did", "=", "db", ".", "insert", "(", "paper", ")", "\n", "arxivid_docid", "[", "paper", "[", "'arxivid'", "]", "]", "=", "did", "\n", "", "else", ":", "\n", "        ", "update_paper", "(", "paper", "[", "'arxivid'", "]", ",", "paper", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.update_paper": [[60, 68], ["db.update"], "function", ["None"], ["", "", "def", "update_paper", "(", "arxivid", ",", "fields", ")", ":", "\n", "    ", "\"\"\"Updates a paper in the db by its arxivid\n    \n    Args:\n        arxivid (str): the id the a paper has on arxiv\n        fields (dict): a dict containing a fields of the paper to update\n    \"\"\"", "\n", "db", ".", "update", "(", "fields", ",", "doc_ids", "=", "[", "arxivid_docid", "[", "arxivid", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.remove_paper": [[69, 76], ["db.remove"], "function", ["None"], ["", "def", "remove_paper", "(", "arxivid", ")", ":", "\n", "    ", "\"\"\"\n        remove one paper from the db\n    Args:\n        arxivid (str): the id the a paper has on arxiv\n    \"\"\"", "\n", "db", ".", "remove", "(", "doc_ids", "=", "[", "arxivid_docid", "[", "arxivid", "]", "]", ")", "\n", "", "def", "get_paper", "(", "arxivid", ")", ":", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_paper": [[76, 86], ["db.get"], "function", ["None"], ["", "def", "get_paper", "(", "arxivid", ")", ":", "\n", "    ", "\"\"\"Returns a paper in the db\n    \n    Args:\n        arxivid (str): the id the apaper has on arxiv\n    \n    Returns:\n        dict: The paper as a dict\n    \"\"\"", "\n", "return", "db", ".", "get", "(", "doc_id", "=", "arxivid_docid", "[", "arxivid", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.find_paper": [[87, 91], ["None"], "function", ["None"], ["", "def", "find_paper", "(", ")", ":", "\n", "    ", "\"\"\"TODO:implement \n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_after": [[92, 103], ["tinydb.Query", "db.search"], "function", ["None"], ["", "def", "get_papers_after", "(", "startdate", ")", ":", "\n", "    ", "\"\"\"Gets all papers created after a date\n    \n    Args:\n        startdate (datetime.datetime): startdate\n    \n    Returns:\n        list: all papers creted after and on startdate\n    \"\"\"", "\n", "query", "=", "Query", "(", ")", "\n", "return", "db", ".", "search", "(", "query", ".", "created", ">=", "startdate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_papers_in_timewindow": [[104, 117], ["tinydb.Query", "db.search", "tinydb.Query.created.test"], "function", ["None"], ["", "def", "get_papers_in_timewindow", "(", "startdate", ",", "enddate", ")", ":", "\n", "    ", "\"\"\"Gets all papers created between two dates\n    \n    Args:\n        startdate (datetime.datetime): startdate\n        enddate (datetime.datetime): enddate\n    \n    Returns:\n        list: all papers creted after and on startdate and before and on enddate\n    \"\"\"", "\n", "\n", "query", "=", "Query", "(", ")", "\n", "return", "db", ".", "search", "(", "query", ".", "created", ".", "test", "(", "lambda", "d", ":", "startdate", "<=", "d", "<=", "enddate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_all_papers_iterator": [[118, 125], ["iter"], "function", ["None"], ["", "def", "get_all_papers_iterator", "(", ")", ":", "\n", "    ", "\"\"\"Returns an iterator object for all papers in the dataset\n    \n    Returns:\n        iterator: Iterator over all papers in the db\n    \"\"\"", "\n", "return", "iter", "(", "db", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.UKPLab_refresh2018-predicting-trends-from-arxiv.None.datahandler2.get_arxivid_embedding": [[126, 145], ["open", "line.strip().split", "numpy.array", "line.strip", "float"], "function", ["None"], ["", "def", "get_arxivid_embedding", "(", "embedding", "=", "'infersent'", ",", "textpart", "=", "'abstract'", ")", ":", "\n", "    ", "\"\"\"returns a dict with arxiv ids and their infersent embedding. \n    Only includes papers if their embeddings had been calculated for that textpart\n    \n    Args:\n        embedding (str): infersent or unisent\n        textpart (str, optional): abstract or title\n    \n    Returns:\n        dict: dict with arxiv ids and their infersent embedding\n    \"\"\"", "\n", "arxivid_embedding", "=", "{", "}", "\n", "with", "open", "(", "'data/%s_%s.csv'", "%", "(", "embedding", ",", "textpart", ")", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "l", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "id", "=", "l", "[", "0", "]", "\n", "embedding", "=", "np", ".", "array", "(", "[", "float", "(", "v", ")", "for", "v", "in", "l", "[", "1", ":", "]", "]", ")", "\n", "arxivid_embedding", "[", "id", "]", "=", "embedding", "\n", "", "", "return", "arxivid_embedding", "\n", "\n"]]}