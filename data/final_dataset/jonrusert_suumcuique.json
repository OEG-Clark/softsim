{"home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.MIDAS.__init__": [[28, 51], ["torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "MIDAS.MIDAS.TEXT.build_vocab", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "nltk.word_tokenize", "torchtext.data.TabularDataset.splits", "x.lower", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "train_data", "=", "'offenseval-training-v1.tsv'", ",", "trained_cnn_model", "=", "'MIDAS_CNN.pt'", ",", "trained_blstm_model", "=", "'MIDAS_BLSTM.pt'", ",", "trained_blstmGru_model", "=", "'MIDAS_BLSTM-GRU.pt'", ")", ":", "\n", "\n", "        ", "self", ".", "tokenize", "=", "lambda", "x", ":", "nltk", ".", "word_tokenize", "(", "x", ".", "lower", "(", ")", ")", "\n", "\n", "self", ".", "TEXT", "=", "Field", "(", "sequential", "=", "True", ",", "tokenize", "=", "self", ".", "tokenize", ",", "lower", "=", "True", ",", "include_lengths", "=", "True", ")", "\n", "self", ".", "LABEL", "=", "Field", "(", "sequential", "=", "False", ",", "use_vocab", "=", "False", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "self", ".", "ID", "=", "Field", "(", "sequential", "=", "False", ",", "use_vocab", "=", "False", ")", "\n", "\n", "off_datafields", "=", "[", "(", "'id'", ",", "None", ")", ",", "(", "'text'", ",", "self", ".", "TEXT", ")", ",", "(", "'label'", ",", "self", ".", "LABEL", ")", ",", "(", "'is_target'", ",", "None", ")", ",", "(", "'target'", ",", "None", ")", "]", "\n", "\n", "trn", "=", "TabularDataset", ".", "splits", "(", "path", "=", "'.'", ",", "train", "=", "train_data", ",", "format", "=", "'tsv'", ",", "fields", "=", "off_datafields", ")", "[", "0", "]", "\n", "\n", "self", ".", "TEXT", ".", "build_vocab", "(", "trn", ",", "vectors", "=", "'glove.6B.200d'", ")", "\n", "\n", "self", ".", "BATCH_SIZE", "=", "64", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "\n", "# load pre-trained model", "\n", "self", ".", "cnn_model", "=", "torch", ".", "load", "(", "trained_cnn_model", ")", "\n", "self", ".", "blstm_model", "=", "torch", ".", "load", "(", "trained_blstm_model", ")", "\n", "self", ".", "blstmGru_model", "=", "torch", ".", "load", "(", "trained_blstmGru_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.MIDAS.predict": [[54, 149], ["MIDAS.cnn_predict_sentence", "MIDAS.blstm_predict_sentence", "MIDAS.blstm_predict_sentence", "int", "predictions.append", "probs.append", "round"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.cnn_predict_sentence", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.blstm_predict_sentence", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.blstm_predict_sentence"], ["", "def", "predict", "(", "self", ",", "test_query", ")", ":", "\n", "\n", "\n", "        ", "'''\n        tmpFile = open('tmpTstFile.tsv', 'w')\n        tmpFile.write('1' + '\\t' + test_query.replace('\\0', ''))\n        tmpFile.close()\n        \n        tst_datafields = [('id', self.ID), ('text', self.TEXT)]\n\n        tst = TabularDataset(path='tmpTstFile.tsv', format='tsv', fields = tst_datafields)\n\n        test_iterator = Iterator(tst, batch_size=self.BATCH_SIZE, sort_key = lambda x: len(x.text), sort_within_batch=True)\n        '''", "\n", "\n", "#load three models, get predictions, let vote on final prediction", "\n", "cnn_votes", "=", "{", "}", "\n", "\n", "#NOTE: These try-except blocks are here to deal with cases of text consisting only of stop words which cause a runtime error", "\n", "'''\n        try:\n            cnn_predictions, cnn_probs = test_cnn(self.cnn_model, test_iterator)\n        \n            for id, pred in cnn_probs:\n                cnn_votes[str(id)] = pred\n\n        except:\n            cnn_votes['1'] = 0\n        '''", "\n", "_", ",", "cnn_prob", "=", "cnn_predict_sentence", "(", "self", ".", "cnn_model", ",", "self", ".", "TEXT", ",", "self", ".", "device", ",", "test_query", ")", "\n", "cnn_votes", "[", "'1'", "]", "=", "cnn_prob", "\n", "#print(cnn_prob)", "\n", "\n", "#BLSTM", "\n", "\n", "blstm_votes", "=", "{", "}", "\n", "\n", "'''\n        try:\n            blstm_predictions, blstm_probs = test_blstm(self.blstm_model, test_iterator)\n            \n            for id, pred in blstm_probs:\n                blstm_votes[str(id)] = pred\n        except:\n            blstm_votes['1'] = 0\n        '''", "\n", "_", ",", "blstm_prob", "=", "blstm_predict_sentence", "(", "self", ".", "blstm_model", ",", "self", ".", "TEXT", ",", "self", ".", "device", ",", "test_query", ")", "\n", "blstm_votes", "[", "'1'", "]", "=", "blstm_prob", "\n", "\n", "#print(blstm_prob)", "\n", "\n", "#BLSTM_BGRU", "\n", "\n", "blstm_gru_votes", "=", "{", "}", "\n", "\n", "'''\n        try:\n            blstm_gru_predictions, blstm_gru_probs = test_blstm(self.blstmGru_model, test_iterator)\n        \n            for id, pred in blstm_gru_probs:\n                blstm_gru_votes[str(id)] = pred\n        except:\n            blstm_gru_votes['1'] = 0\n        '''", "\n", "_", ",", "blstm_gru_prob", "=", "blstm_predict_sentence", "(", "self", ".", "blstm_model", ",", "self", ".", "TEXT", ",", "self", ".", "device", ",", "test_query", ")", "\n", "blstm_gru_votes", "[", "'1'", "]", "=", "blstm_gru_prob", "\n", "#print(blstm_gru_prob)", "\n", "\n", "\n", "predictions", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "\n", "# have each system vote for prediction (0 or 1) if at least 2 vote for 1 score will be 2 or higher, if at least 2 vote for 0 score will be less than 2", "\n", "for", "id", "in", "cnn_votes", ":", "\n", "#votes = cnn_votes[id] + blstm_votes[id] + blstm_gru_votes[id]", "\n", "#if(votes >= 2):", "\n", "#    pred = 1", "\n", "#else:", "\n", "#    pred = 0", "\n", "            ", "pred", "=", "int", "(", "round", "(", "(", "cnn_votes", "[", "id", "]", "+", "blstm_votes", "[", "id", "]", "+", "blstm_gru_votes", "[", "id", "]", ")", "/", "3", ")", ")", "\n", "prob", "=", "(", "cnn_votes", "[", "id", "]", "+", "blstm_votes", "[", "id", "]", "+", "blstm_gru_votes", "[", "id", "]", ")", "/", "3", "\n", "\n", "\n", "# work around for now as MIDAS returns 0 or 1 and needs to return OFF or NOT", "\n", "if", "(", "pred", "==", "0", ")", ":", "\n", "                ", "pred", "=", "'NOT'", "\n", "", "else", ":", "\n", "                ", "pred", "=", "'OFF'", "\n", "\n", "", "predictions", ".", "append", "(", "pred", ")", "\n", "probs", ".", "append", "(", "prob", ")", "\n", "\n", "\n", "\n", "", "return", "predictions", "[", "0", "]", ",", "probs", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.MIDAS.predictMultiple": [[153, 164], ["MIDAS.MIDAS.predict", "predictions.append", "probs.append"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.predict"], ["", "def", "predictMultiple", "(", "self", ",", "test_queries", ")", ":", "\n", "\n", "        ", "predictions", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "\n", "for", "test", "in", "test_queries", ":", "\n", "            ", "pred", ",", "prob", "=", "self", ".", "predict", "(", "test", ")", "\n", "predictions", ".", "append", "(", "pred", ")", "\n", "probs", ".", "append", "(", "prob", ")", "\n", "\n", "", "return", "predictions", ",", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.CNN.__init__": [[175, 193], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "len"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embedding_dim", ",", "n_filters", ",", "filter_sizes", ",", "output_dim", ",", "\n", "dropout", ",", "pad_idx", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_dim", ",", "padding_idx", "=", "pad_idx", ")", "\n", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "[", "\n", "nn", ".", "Conv2d", "(", "in_channels", "=", "1", ",", "\n", "out_channels", "=", "n_filters", ",", "\n", "kernel_size", "=", "(", "fs", ",", "embedding_dim", ")", ")", "\n", "for", "fs", "in", "filter_sizes", "\n", "]", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "len", "(", "filter_sizes", ")", "*", "n_filters", ",", "256", ")", "\n", "self", ".", "out", "=", "nn", ".", "Linear", "(", "256", ",", "output_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.CNN.forward": [[195, 228], ["text.permute.permute.permute", "MIDAS.CNN.embedding", "embedded.unsqueeze.unsqueeze.unsqueeze", "MIDAS.CNN.dropout", "MIDAS.CNN.dropout", "MIDAS.CNN.out", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "MIDAS.CNN.fc", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ")", ":", "\n", "\n", "#text = [sent len, batch size]", "\n", "\n", "        ", "text", "=", "text", ".", "permute", "(", "1", ",", "0", ")", "\n", "\n", "#text = [batch size, sent len]", "\n", "\n", "embedded", "=", "self", ".", "embedding", "(", "text", ")", "\n", "\n", "#embedded = [batch size, sent len, emb dim]", "\n", "\n", "embedded", "=", "embedded", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "#embedded = [batch size, 1, sent len, emb dim]", "\n", "\n", "conved", "=", "[", "F", ".", "relu", "(", "conv", "(", "embedded", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "convs", "]", "\n", "\n", "#conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]", "\n", "\n", "pooled", "=", "[", "F", ".", "max_pool1d", "(", "conv", ",", "conv", ".", "shape", "[", "2", "]", ")", ".", "squeeze", "(", "2", ")", "for", "conv", "in", "conved", "]", "\n", "\n", "#pooled_n = [batch size, n_filters]", "\n", "\n", "cat", "=", "self", ".", "dropout", "(", "torch", ".", "cat", "(", "pooled", ",", "dim", "=", "1", ")", ")", "\n", "\n", "#cat = [batch size, n_filters * len(filter_sizes)]", "\n", "\n", "fc1", "=", "self", ".", "dropout", "(", "self", ".", "fc", "(", "cat", ")", ")", "\n", "\n", "# output", "\n", "\n", "return", "self", ".", "out", "(", "fc1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.BLSTM.__init__": [[236, 244], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embedding_dim", ",", "hidden_dim", ",", "output_dim", ",", "dropout", ",", "pad_idx", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_dim", ",", "padding_idx", "=", "pad_idx", ")", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "embedding_dim", ",", "\n", "hidden_dim", ",", "\n", "bidirectional", "=", "True", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "output_dim", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.BLSTM.forward": [[246, 253], ["MIDAS.BLSTM.dropout", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "MIDAS.BLSTM.rnn", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "MIDAS.BLSTM.dropout", "MIDAS.BLSTM.fc", "MIDAS.BLSTM.embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "MIDAS.BLSTM.squeeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ",", "text_lengths", ")", ":", "\n", "        ", "embedded", "=", "self", ".", "dropout", "(", "self", ".", "embedding", "(", "text", ")", ")", "\n", "packed_embedded", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "embedded", ",", "text_lengths", ")", "\n", "packed_output", ",", "(", "hidden", ",", "cell", ")", "=", "self", ".", "rnn", "(", "packed_embedded", ")", "\n", "output", ",", "output_lengths", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_output", ")", "\n", "hidden", "=", "self", ".", "dropout", "(", "torch", ".", "cat", "(", "(", "hidden", "[", "-", "2", ",", ":", ",", ":", "]", ",", "hidden", "[", "-", "1", ",", ":", ",", ":", "]", ")", ",", "dim", "=", "1", ")", ")", "\n", "return", "self", ".", "fc", "(", "hidden", ".", "squeeze", "(", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.BLSTM_GRU.__init__": [[258, 268], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.GRU", "torch.GRU", "torch.GRU", "torch.GRU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.__init__"], ["    ", "def", "__init__", "(", "self", ",", "vocab_size", ",", "embedding_dim", ",", "hidden_dim", ",", "output_dim", ",", "dropout", ",", "pad_idx", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embedding_dim", ",", "padding_idx", "=", "pad_idx", ")", "\n", "self", ".", "rnn", "=", "nn", ".", "LSTM", "(", "embedding_dim", ",", "\n", "hidden_dim", ",", "\n", "bidirectional", "=", "True", ")", "\n", "\n", "self", ".", "gru", "=", "nn", ".", "GRU", "(", "hidden_dim", "*", "2", ",", "hidden_dim", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "hidden_dim", "*", "2", ",", "output_dim", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.BLSTM_GRU.forward": [[270, 285], ["MIDAS.BLSTM_GRU.dropout", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "MIDAS.BLSTM_GRU.rnn", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "MIDAS.BLSTM_GRU.gru", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "MIDAS.BLSTM_GRU.fc", "MIDAS.BLSTM_GRU.embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "text", ",", "text_lengths", ")", ":", "\n", "\n", "        ", "embedded", "=", "self", ".", "dropout", "(", "self", ".", "embedding", "(", "text", ")", ")", "\n", "packed_embedded", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "embedded", ",", "text_lengths", ")", "\n", "packed_output", ",", "(", "hidden", ",", "cell", ")", "=", "self", ".", "rnn", "(", "packed_embedded", ")", "\n", "output", ",", "output_lengths", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_output", ")", "\n", "#hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))", "\n", "\n", "# pass to BGRU", "\n", "packed_lstm_outputs", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "output", ",", "output_lengths", ")", "\n", "packed_output", ",", "(", "hidden", ",", "cell", ")", "=", "self", ".", "gru", "(", "packed_lstm_outputs", ")", "\n", "output", ",", "output_lengths", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_output", ")", "\n", "#hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))", "\n", "\n", "return", "self", ".", "fc", "(", "output", "[", "-", "1", ",", ":", ",", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.binary_accuracy": [[288, 298], ["torch.round", "torch.round", "torch.round", "torch.round", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "correct.sum", "len"], "function", ["None"], ["", "", "def", "binary_accuracy", "(", "preds", ",", "y", ")", ":", "\n", "    ", "\"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"", "\n", "\n", "#round predictions to the closest integer", "\n", "rounded_preds", "=", "torch", ".", "round", "(", "torch", ".", "sigmoid", "(", "preds", ")", ")", "\n", "correct", "=", "(", "rounded_preds", "==", "y", ")", ".", "float", "(", ")", "#convert into float for division ", "\n", "acc", "=", "correct", ".", "sum", "(", ")", "/", "len", "(", "correct", ")", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.train_cnn": [[300, 329], ["model.train", "optimizer.zero_grad", "model().squeeze", "criterion", "MIDAS.binary_accuracy", "criterion.backward", "optimizer.step", "criterion.item", "binary_accuracy.item", "len", "len", "model"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.binary_accuracy"], ["", "def", "train_cnn", "(", "model", ",", "iterator", ",", "optimizer", ",", "criterion", ")", ":", "\n", "\n", "    ", "epoch_loss", "=", "0", "\n", "epoch_acc", "=", "0", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "batch", "in", "iterator", ":", "\n", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "text", ",", "_", "=", "batch", ".", "text", "\n", "\n", "predictions", "=", "model", "(", "text", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "#print(predictions)", "\n", "#print(batch.label)", "\n", "loss", "=", "criterion", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "acc", "=", "binary_accuracy", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "epoch_acc", "+=", "acc", ".", "item", "(", ")", "\n", "\n", "", "return", "epoch_loss", "/", "len", "(", "iterator", ")", ",", "epoch_acc", "/", "len", "(", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.train_blstm": [[331, 361], ["model.train", "optimizer.zero_grad", "model().squeeze", "criterion", "MIDAS.binary_accuracy", "criterion.backward", "optimizer.step", "criterion.item", "binary_accuracy.item", "len", "len", "model"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.binary_accuracy"], ["", "def", "train_blstm", "(", "model", ",", "iterator", ",", "optimizer", ",", "criterion", ")", ":", "\n", "\n", "    ", "epoch_loss", "=", "0", "\n", "epoch_acc", "=", "0", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "for", "batch", "in", "iterator", ":", "\n", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "text", ",", "text_lengths", "=", "batch", ".", "text", "\n", "\n", "predictions", "=", "model", "(", "text", ",", "text_lengths", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "\n", "#print(predictions)", "\n", "#print(batch.label)", "\n", "loss", "=", "criterion", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "acc", "=", "binary_accuracy", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "epoch_acc", "+=", "acc", ".", "item", "(", ")", "\n", "\n", "", "return", "epoch_loss", "/", "len", "(", "iterator", ")", ",", "epoch_acc", "/", "len", "(", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.evaluate": [[365, 386], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model().squeeze", "criterion", "MIDAS.binary_accuracy", "criterion.item", "binary_accuracy.item", "len", "len", "model"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.binary_accuracy"], ["", "def", "evaluate", "(", "model", ",", "iterator", ",", "criterion", ")", ":", "\n", "\n", "    ", "epoch_loss", "=", "0", "\n", "epoch_acc", "=", "0", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "for", "batch", "in", "iterator", ":", "\n", "\n", "            ", "predictions", "=", "model", "(", "batch", ".", "text", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "loss", "=", "criterion", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "acc", "=", "binary_accuracy", "(", "predictions", ",", "batch", ".", "label", ")", "\n", "\n", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "epoch_acc", "+=", "acc", ".", "item", "(", ")", "\n", "\n", "", "", "return", "epoch_loss", "/", "len", "(", "iterator", ")", ",", "epoch_acc", "/", "len", "(", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.cnn_predict_sentence": [[389, 400], ["model.eval", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "tensor.unsqueeze.unsqueeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "nltk.word_tokenize", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "model", "round", "torch.sigmoid.item", "sentence.lower", "torch.sigmoid.item"], "function", ["None"], ["", "def", "cnn_predict_sentence", "(", "model", ",", "TEXT", ",", "device", ",", "sentence", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "tokenized", "=", "[", "x", "for", "x", "in", "nltk", ".", "word_tokenize", "(", "sentence", ".", "lower", "(", ")", ")", "]", "\n", "indexed", "=", "[", "TEXT", ".", "vocab", ".", "stoi", "[", "t", "]", "for", "t", "in", "tokenized", "]", "\n", "tensor", "=", "torch", ".", "LongTensor", "(", "indexed", ")", ".", "to", "(", "device", ")", "\n", "tensor", "=", "tensor", ".", "unsqueeze", "(", "1", ")", "\n", "try", ":", "\n", "        ", "prediction", "=", "torch", ".", "sigmoid", "(", "model", "(", "tensor", ")", ")", "\n", "return", "round", "(", "prediction", ".", "item", "(", ")", ")", ",", "prediction", ".", "item", "(", ")", "\n", "", "except", ":", "\n", "        ", "return", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.blstm_predict_sentence": [[401, 414], ["model.eval", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "tensor.unsqueeze.unsqueeze", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "nltk.word_tokenize", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "model", "round", "torch.sigmoid.item", "sentence.lower", "torch.sigmoid.item"], "function", ["None"], ["", "", "def", "blstm_predict_sentence", "(", "model", ",", "TEXT", ",", "device", ",", "sentence", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "tokenized", "=", "[", "x", "for", "x", "in", "nltk", ".", "word_tokenize", "(", "sentence", ".", "lower", "(", ")", ")", "]", "\n", "indexed", "=", "[", "TEXT", ".", "vocab", ".", "stoi", "[", "t", "]", "for", "t", "in", "tokenized", "]", "\n", "length", "=", "[", "len", "(", "indexed", ")", "]", "\n", "tensor", "=", "torch", ".", "LongTensor", "(", "indexed", ")", ".", "to", "(", "device", ")", "\n", "tensor", "=", "tensor", ".", "unsqueeze", "(", "1", ")", "\n", "length_tensor", "=", "torch", ".", "LongTensor", "(", "length", ")", "\n", "try", ":", "\n", "        ", "prediction", "=", "torch", ".", "sigmoid", "(", "model", "(", "tensor", ",", "length_tensor", ")", ")", "\n", "return", "round", "(", "prediction", ".", "item", "(", ")", ")", ",", "prediction", ".", "item", "(", ")", "\n", "", "except", ":", "\n", "        ", "return", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.test_cnn": [[415, 443], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model().squeeze", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.round", "torch.round", "torch.round", "torch.round", "range", "len", "final_preds.append", "pred_probs.append", "model", "batch.id[].item", "int", "batch.id[].item", "sigmoid_preds[].item", "rounded_preds[].item"], "function", ["None"], ["", "", "def", "test_cnn", "(", "model", ",", "iterator", ")", ":", "\n", "\n", "    ", "epoch_loss", "=", "0", "\n", "epoch_acc", "=", "0", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "final_preds", "=", "[", "]", "\n", "pred_probs", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "for", "batch", "in", "iterator", ":", "\n", "            ", "text", ",", "_", "=", "batch", ".", "text", "\n", "\n", "predictions", "=", "model", "(", "text", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "#print(predictions)", "\n", "sigmoid_preds", "=", "torch", ".", "sigmoid", "(", "predictions", ")", "\n", "rounded_preds", "=", "torch", ".", "round", "(", "sigmoid_preds", ")", "\n", "\n", "\n", "# save each prediction with corresponding id to list to write to file later", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ".", "id", ")", ")", ":", "\n", "                ", "final_preds", ".", "append", "(", "(", "batch", ".", "id", "[", "i", "]", ".", "item", "(", ")", ",", "int", "(", "rounded_preds", "[", "i", "]", ".", "item", "(", ")", ")", ")", ")", "\n", "pred_probs", ".", "append", "(", "(", "batch", ".", "id", "[", "i", "]", ".", "item", "(", ")", ",", "sigmoid_preds", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "", "", "", "return", "final_preds", ",", "pred_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.test_blstm": [[444, 468], ["model.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.round", "torch.round", "torch.round", "torch.round", "range", "len", "final_preds.append", "pred_probs.append", "batch.id[].item", "int", "batch.id[].item", "sigmoid_preds[].item", "rounded_preds[].item"], "function", ["None"], ["", "def", "test_blstm", "(", "model", ",", "iterator", ")", ":", "\n", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "final_preds", "=", "[", "]", "\n", "pred_probs", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "for", "batch", "in", "iterator", ":", "\n", "            ", "text", ",", "text_lengths", "=", "batch", ".", "text", "\n", "\n", "predictions", "=", "model", "(", "text", ",", "text_lengths", ")", "#.squeeze(1)", "\n", "\n", "#print(predictions)", "\n", "sigmoid_preds", "=", "torch", ".", "sigmoid", "(", "predictions", ")", "\n", "rounded_preds", "=", "torch", ".", "round", "(", "sigmoid_preds", ")", "\n", "\n", "# save each prediction with corresponding id to list to write to file later", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ".", "id", ")", ")", ":", "\n", "                ", "final_preds", ".", "append", "(", "(", "batch", ".", "id", "[", "i", "]", ".", "item", "(", ")", ",", "int", "(", "rounded_preds", "[", "i", "]", ".", "item", "(", ")", ")", ")", ")", "\n", "pred_probs", ".", "append", "(", "(", "batch", ".", "id", "[", "i", "]", ".", "item", "(", ")", ",", "sigmoid_preds", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "", "", "", "return", "final_preds", ",", "pred_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.epoch_time": [[471, 476], ["int", "int"], "function", ["None"], ["", "def", "epoch_time", "(", "start_time", ",", "end_time", ")", ":", "\n", "    ", "elapsed_time", "=", "end_time", "-", "start_time", "\n", "elapsed_mins", "=", "int", "(", "elapsed_time", "/", "60", ")", "\n", "elapsed_secs", "=", "int", "(", "elapsed_time", "-", "(", "elapsed_mins", "*", "60", ")", ")", "\n", "return", "elapsed_mins", ",", "elapsed_secs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.main": [[479, 736], ["torchtext.data.Field", "torchtext.data.Field", "torchtext.data.Field", "torchtext.data.TabularDataset", "torchtext.data.Field.build_vocab", "torch.device", "torch.device", "torch.device", "torch.device", "torchtext.data.BucketIterator", "torchtext.data.Iterator", "nltk.word_tokenize", "torchtext.data.TabularDataset.splits", "len", "MIDAS.CNN", "torch.load.embedding.weight.data.copy_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Adam", "torch.BCEWithLogitsLoss", "torch.load.to", "criterion.to.to", "float", "print", "range", "torch.save", "torch.save", "torch.save", "torch.save", "len", "MIDAS.BLSTM", "torch.load.embedding.weight.data.copy_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Adam", "torch.BCEWithLogitsLoss", "torch.load.to", "criterion.to.to", "float", "print", "range", "torch.save", "torch.save", "torch.save", "torch.save", "len", "MIDAS.BLSTM_GRU", "torch.load.embedding.weight.data.copy_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Adam", "torch.BCEWithLogitsLoss", "torch.load.to", "criterion.to.to", "float", "print", "range", "torch.save", "torch.save", "torch.save", "torch.save", "x.lower", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.load.parameters", "time.time", "MIDAS.train_cnn", "time.time", "MIDAS.epoch_time", "print", "print", "torch.load.parameters", "time.time", "MIDAS.train_blstm", "time.time", "MIDAS.epoch_time", "print", "print", "torch.load.parameters", "time.time", "MIDAS.train_blstm", "time.time", "MIDAS.epoch_time", "print", "print", "torch.load", "torch.load", "torch.load", "torch.load", "MIDAS.test_cnn", "torch.load", "torch.load", "torch.load", "torch.load", "MIDAS.test_blstm", "torch.load", "torch.load", "torch.load", "torch.load", "MIDAS.test_blstm", "open.close", "len", "len", "open", "open", "int", "open.write", "round", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.train_cnn", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.epoch_time", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.train_blstm", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.epoch_time", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.train_blstm", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.epoch_time", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.test_cnn", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.test_blstm", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.MIDAS.test_blstm"], ["", "def", "main", "(", "trainFile", ",", "testFile", ",", "train_test", ",", "out_file", "=", "None", ")", ":", "\n", "\n", "    ", "tokenize", "=", "lambda", "x", ":", "nltk", ".", "word_tokenize", "(", "x", ".", "lower", "(", ")", ")", "\n", "\n", "TEXT", "=", "Field", "(", "sequential", "=", "True", ",", "tokenize", "=", "tokenize", ",", "lower", "=", "True", ",", "include_lengths", "=", "True", ")", "\n", "LABEL", "=", "Field", "(", "sequential", "=", "False", ",", "use_vocab", "=", "False", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "ID", "=", "Field", "(", "sequential", "=", "False", ",", "use_vocab", "=", "False", ")", "\n", "\n", "#off_datafields = [('id', None), ('text', TEXT), ('label', LABEL), ('is_target', None), ('target', None)]", "\n", "off_datafields", "=", "[", "(", "'id'", ",", "None", ")", ",", "(", "'text'", ",", "TEXT", ")", ",", "(", "'label'", ",", "LABEL", ")", "]", "\n", "\n", "\n", "trn", "=", "TabularDataset", ".", "splits", "(", "path", "=", "'.'", ",", "train", "=", "trainFile", ",", "format", "=", "'tsv'", ",", "fields", "=", "off_datafields", ")", "[", "0", "]", "\n", "\n", "tst_datafields", "=", "[", "(", "'id'", ",", "ID", ")", ",", "(", "'label'", ",", "None", ")", ",", "(", "'text'", ",", "TEXT", ")", "]", "\n", "\n", "tst", "=", "TabularDataset", "(", "path", "=", "testFile", ",", "format", "=", "'tsv'", ",", "fields", "=", "tst_datafields", ")", "\n", "\n", "TEXT", ".", "build_vocab", "(", "trn", ",", "vectors", "=", "'glove.6B.200d'", ")", "\n", "\n", "BATCH_SIZE", "=", "64", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "train_iterator", "=", "BucketIterator", "(", "trn", ",", "batch_size", "=", "BATCH_SIZE", ",", "device", "=", "device", ",", "sort_key", "=", "lambda", "x", ":", "len", "(", "x", ".", "text", ")", ",", "sort_within_batch", "=", "True", ")", "\n", "\n", "test_iterator", "=", "Iterator", "(", "tst", ",", "batch_size", "=", "BATCH_SIZE", ",", "sort_key", "=", "lambda", "x", ":", "len", "(", "x", ".", "text", ")", ",", "sort_within_batch", "=", "True", ")", "\n", "\n", "\n", "if", "(", "train_test", "==", "'train'", "or", "train_test", "==", "'both'", ")", ":", "\n", "\n", "# train CNN first", "\n", "# MIDAS CNN use 256 filters, with 2, 3, 4 as filter_sizes, dropout = 0.3", "\n", "\n", "        ", "INPUT_DIM", "=", "len", "(", "TEXT", ".", "vocab", ")", "\n", "EMBEDDING_DIM", "=", "200", "\n", "N_FILTERS", "=", "256", "\n", "FILTER_SIZES", "=", "[", "2", ",", "3", ",", "4", "]", "\n", "OUTPUT_DIM", "=", "1", "\n", "DROPOUT", "=", "0.3", "\n", "PAD_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "pad_token", "]", "\n", "\n", "model", "=", "CNN", "(", "INPUT_DIM", ",", "EMBEDDING_DIM", ",", "N_FILTERS", ",", "FILTER_SIZES", ",", "OUTPUT_DIM", ",", "DROPOUT", ",", "PAD_IDX", ")", "\n", "\n", "pretrained_embeddings", "=", "TEXT", ".", "vocab", ".", "vectors", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "pretrained_embeddings", ")", "\n", "\n", "\n", "UNK_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "unk_token", "]", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "UNK_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "PAD_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "criterion", "=", "criterion", ".", "to", "(", "device", ")", "\n", "\n", "N_EPOCHS", "=", "3", "\n", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "print", "(", "'training CNN'", ")", "\n", "for", "epoch", "in", "range", "(", "N_EPOCHS", ")", ":", "\n", "\n", "            ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "train_loss", ",", "train_acc", "=", "train_cnn", "(", "model", ",", "train_iterator", ",", "optimizer", ",", "criterion", ")", "\n", "#valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)", "\n", "\n", "end_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "epoch_mins", ",", "epoch_secs", "=", "epoch_time", "(", "start_time", ",", "end_time", ")", "\n", "\n", "#if valid_loss < best_valid_loss:", "\n", "#    best_valid_loss = valid_loss", "\n", "#    torch.save(model.state_dict(), 'tut4-model.pt')", "\n", "\n", "print", "(", "f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'", ")", "\n", "print", "(", "f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%'", ")", "\n", "#print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')", "\n", "\n", "", "torch", ".", "save", "(", "model", ",", "'MIDAS_CNN.pt'", ")", "\n", "\n", "\n", "# train BLSTM next", "\n", "# ", "\n", "\n", "INPUT_DIM", "=", "len", "(", "TEXT", ".", "vocab", ")", "\n", "EMBEDDING_DIM", "=", "200", "\n", "HIDDEN", "=", "64", "\n", "OUTPUT_DIM", "=", "1", "\n", "DROPOUT", "=", "0.2", "\n", "PAD_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "pad_token", "]", "\n", "\n", "model", "=", "BLSTM", "(", "INPUT_DIM", ",", "EMBEDDING_DIM", ",", "HIDDEN", ",", "OUTPUT_DIM", ",", "DROPOUT", ",", "PAD_IDX", ")", "\n", "\n", "pretrained_embeddings", "=", "TEXT", ".", "vocab", ".", "vectors", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "pretrained_embeddings", ")", "\n", "\n", "\n", "UNK_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "unk_token", "]", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "UNK_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "PAD_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "criterion", "=", "criterion", ".", "to", "(", "device", ")", "\n", "\n", "N_EPOCHS", "=", "3", "\n", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "print", "(", "'training BLSTM'", ")", "\n", "for", "epoch", "in", "range", "(", "N_EPOCHS", ")", ":", "\n", "\n", "            ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "train_loss", ",", "train_acc", "=", "train_blstm", "(", "model", ",", "train_iterator", ",", "optimizer", ",", "criterion", ")", "\n", "#valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)", "\n", "\n", "end_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "epoch_mins", ",", "epoch_secs", "=", "epoch_time", "(", "start_time", ",", "end_time", ")", "\n", "\n", "#if valid_loss < best_valid_loss:", "\n", "#    best_valid_loss = valid_loss", "\n", "#    torch.save(model.state_dict(), 'tut4-model.pt')", "\n", "\n", "print", "(", "f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'", ")", "\n", "print", "(", "f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%'", ")", "\n", "#print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')", "\n", "\n", "", "torch", ".", "save", "(", "model", ",", "'MIDAS_BLSTM.pt'", ")", "\n", "\n", "\n", "# train BLSTM-GRU last", "\n", "# ", "\n", "\n", "INPUT_DIM", "=", "len", "(", "TEXT", ".", "vocab", ")", "\n", "EMBEDDING_DIM", "=", "200", "\n", "HIDDEN", "=", "64", "\n", "OUTPUT_DIM", "=", "1", "\n", "DROPOUT", "=", "0.3", "\n", "PAD_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "pad_token", "]", "\n", "\n", "model", "=", "BLSTM_GRU", "(", "INPUT_DIM", ",", "EMBEDDING_DIM", ",", "HIDDEN", ",", "OUTPUT_DIM", ",", "DROPOUT", ",", "PAD_IDX", ")", "\n", "\n", "pretrained_embeddings", "=", "TEXT", ".", "vocab", ".", "vectors", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "pretrained_embeddings", ")", "\n", "\n", "\n", "UNK_IDX", "=", "TEXT", ".", "vocab", ".", "stoi", "[", "TEXT", ".", "unk_token", "]", "\n", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "UNK_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "model", ".", "embedding", ".", "weight", ".", "data", "[", "PAD_IDX", "]", "=", "torch", ".", "zeros", "(", "EMBEDDING_DIM", ")", "\n", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "criterion", "=", "criterion", ".", "to", "(", "device", ")", "\n", "\n", "N_EPOCHS", "=", "3", "\n", "\n", "best_valid_loss", "=", "float", "(", "'inf'", ")", "\n", "\n", "print", "(", "'training BLSTM-GRU'", ")", "\n", "for", "epoch", "in", "range", "(", "N_EPOCHS", ")", ":", "\n", "\n", "            ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "train_loss", ",", "train_acc", "=", "train_blstm", "(", "model", ",", "train_iterator", ",", "optimizer", ",", "criterion", ")", "\n", "#valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)", "\n", "\n", "end_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "epoch_mins", ",", "epoch_secs", "=", "epoch_time", "(", "start_time", ",", "end_time", ")", "\n", "\n", "#if valid_loss < best_valid_loss:", "\n", "#    best_valid_loss = valid_loss", "\n", "#    torch.save(model.state_dict(), 'tut4-model.pt')", "\n", "\n", "print", "(", "f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'", ")", "\n", "print", "(", "f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%'", ")", "\n", "#print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')", "\n", "\n", "", "torch", ".", "save", "(", "model", ",", "'MIDAS_BLSTM-GRU.pt'", ")", "\n", "\n", "\n", "\n", "", "elif", "(", "train_test", "==", "'test'", "or", "train_test", "==", "'both'", ")", ":", "\n", "\n", "#load three models, get predictions, let vote on final prediction", "\n", "        ", "model", "=", "torch", ".", "load", "(", "'MIDAS_CNN.pt'", ")", "\n", "\n", "cnn_predictions", ",", "cnn_probs", "=", "test_cnn", "(", "model", ",", "test_iterator", ")", "\n", "\n", "cnn_votes", "=", "{", "}", "\n", "\n", "for", "id", ",", "pred", "in", "cnn_probs", ":", "\n", "            ", "cnn_votes", "[", "str", "(", "id", ")", "]", "=", "pred", "\n", "\n", "\n", "#BLSTM", "\n", "", "model", "=", "torch", ".", "load", "(", "'MIDAS_BLSTM.pt'", ")", "\n", "\n", "blstm_predictions", ",", "blstm_probs", "=", "test_blstm", "(", "model", ",", "test_iterator", ")", "\n", "\n", "blstm_votes", "=", "{", "}", "\n", "\n", "for", "id", ",", "pred", "in", "blstm_probs", ":", "\n", "            ", "blstm_votes", "[", "str", "(", "id", ")", "]", "=", "pred", "\n", "\n", "\n", "#BLSTM_BGRU", "\n", "", "model", "=", "torch", ".", "load", "(", "'MIDAS_BLSTM-GRU.pt'", ")", "\n", "\n", "blstm_gru_predictions", ",", "blstm_gru_probs", "=", "test_blstm", "(", "model", ",", "test_iterator", ")", "\n", "\n", "blstm_gru_votes", "=", "{", "}", "\n", "\n", "for", "id", ",", "pred", "in", "blstm_gru_probs", ":", "\n", "            ", "blstm_gru_votes", "[", "str", "(", "id", ")", "]", "=", "pred", "\n", "\n", "\n", "\n", "", "if", "(", "out_file", ")", ":", "\n", "            ", "output", "=", "open", "(", "out_file", ",", "'w'", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "open", "(", "'MIDAS_predictionsOut'", ",", "'w'", ")", "\n", "\n", "# have each system vote for prediction (0 or 1) if at least 2 vote for 1 score will be 2 or higher, if at least 2 vote for 0 score will be less than 2", "\n", "", "for", "id", "in", "cnn_votes", ":", "\n", "#votes = cnn_votes[id] + blstm_votes[id] + blstm_gru_votes[id]", "\n", "#if(votes >= 2):", "\n", "#    pred = 1", "\n", "#else:", "\n", "#    pred = 0", "\n", "            ", "pred", "=", "int", "(", "round", "(", "(", "cnn_votes", "[", "id", "]", "+", "blstm_votes", "[", "id", "]", "+", "blstm_gru_votes", "[", "id", "]", ")", "/", "3", ")", ")", "\n", "\n", "output", ".", "write", "(", "str", "(", "id", ")", "+", "','", "+", "str", "(", "pred", ")", "+", "'\\n'", ")", "\n", "\n", "", "output", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.BertForSequenceClassification.__init__": [[24, 31], ["torch.Module.__init__", "BertModel.from_pretrained", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.init.xavier_normal_", "torch.init.xavier_normal_"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "classifier", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.BertForSequenceClassification.forward": [[32, 38], ["NULI.BertForSequenceClassification.bert", "NULI.BertForSequenceClassification.dropout", "NULI.BertForSequenceClassification.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "#, output_all_encoded_layers=False)", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.__init__": [[138, 160], ["wordsegment.load", "open", "open.readlines", "json.loads", "BertTokenizer.from_pretrained", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "train_data", "=", "'offenseval-training-v1.tsv'", ",", "trained_model", "=", "'NULI.pt'", ",", "params_file", "=", "'NULI_params.json'", ")", ":", "\n", "#x_train, y_train, x_test, labelNum, testTweets, labelsAsNums, numsAsLabels, max_seq_length = load_dataset(train_data)", "\n", "\n", "        ", "wordsegment", ".", "load", "(", ")", "\n", "\n", "\n", "# load in params", "\n", "params_in", "=", "open", "(", "params_file", ")", "\n", "params_lines", "=", "params_in", ".", "readlines", "(", ")", "\n", "params", "=", "json", ".", "loads", "(", "params_lines", "[", "0", "]", ")", "\n", "\n", "self", ".", "labelNum", "=", "params", "[", "'labelNum'", "]", "\n", "self", ".", "labelsAsNums", "=", "params", "[", "'labelsAsNums'", "]", "\n", "self", ".", "numsAsLabels", "=", "params", "[", "'numsAsLabels'", "]", "\n", "self", ".", "max_seq_length", "=", "params", "[", "'max_seq_length'", "]", "\n", "\n", "# Load pre-trained tokenizer (vocabulary)", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "# load pre-trained model", "\n", "self", ".", "model", "=", "torch", ".", "load", "(", "trained_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.preprocessQuery": [[163, 190], ["test_query.lower().strip", "text.split.split.replace", "text.split.split.split", "wordsegment.segment", "test_query.lower", "emoji.demojize", "out_text.append"], "methods", ["None"], ["", "def", "preprocessQuery", "(", "self", ",", "test_query", ")", ":", "\n", "        ", "text", "=", "test_query", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "\n", "# emoji used in NULI: https://github.com/carpedm20/emoji", "\n", "# wordsegment used in NULI: https://github.com/grantjenks/python-wordsegment", "\n", "text", "=", "' '", ".", "join", "(", "wordsegment", ".", "segment", "(", "emoji", ".", "demojize", "(", "text", ")", ")", ")", "\n", "\n", "# NULI replaced URL with http", "\n", "text", "=", "text", ".", "replace", "(", "'URL'", ",", "'http'", ")", "\n", "\n", "# NULI limited @USER to three instances", "\n", "text", "=", "text", ".", "split", "(", ")", "\n", "user_count", "=", "0", "\n", "out_text", "=", "[", "]", "\n", "\n", "for", "word", "in", "text", ":", "\n", "            ", "if", "(", "word", "==", "'@USER'", ")", ":", "\n", "                ", "user_count", "+=", "1", "\n", "", "else", ":", "\n", "                ", "user_count", "=", "0", "\n", "\n", "", "if", "(", "user_count", "<=", "3", ")", ":", "\n", "                ", "out_text", ".", "append", "(", "word", ")", "\n", "\n", "", "", "text", "=", "' '", ".", "join", "(", "out_text", ")", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.tokenizeQuery": [[194, 209], ["NULI.NULI.tokenizer.tokenize", "NULI.NULI.tokenizer.convert_tokens_to_ids", "len", "len", "len"], "methods", ["None"], ["", "def", "tokenizeQuery", "(", "self", ",", "text", ")", ":", "\n", "        ", "tokenized_review", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n", "if", "len", "(", "tokenized_review", ")", ">", "self", ".", "max_seq_length", ":", "\n", "            ", "tokenized_review", "=", "tokenized_review", "[", ":", "self", ".", "max_seq_length", "]", "\n", "\n", "", "ids_review", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_review", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "self", ".", "max_seq_length", "-", "len", "(", "ids_review", ")", ")", "\n", "\n", "ids_review", "+=", "padding", "\n", "\n", "assert", "len", "(", "ids_review", ")", "==", "self", ".", "max_seq_length", "\n", "\n", "return", "ids_review", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.predict": [[211, 242], ["NULI.NULI.preprocessQuery", "NULI.NULI.tokenizeQuery", "test.append", "NULI.NULI.model.eval", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "NULI.NULI.model", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Softmax.", "out_probs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist", "predicted.append", "probs.append", "out_probs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu().numpy", "cur_output.index", "str", "max", "out_probs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu", "out_probs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.preprocessQuery", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.tokenizeQuery"], ["", "def", "predict", "(", "self", ",", "test_query", ")", ":", "\n", "\n", "        ", "text", "=", "self", ".", "preprocessQuery", "(", "test_query", ")", "\n", "ids_review", "=", "self", ".", "tokenizeQuery", "(", "text", ")", "\n", "\n", "test", "=", "[", "]", "\n", "test", ".", "append", "(", "ids_review", ")", "\n", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "# make prediction", "\n", "predicted", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "\n", "tests", "=", "torch", ".", "tensor", "(", "test", ")", "\n", "outputs", "=", "self", ".", "model", "(", "tests", ")", "\n", "soft", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "out_probs", "=", "soft", "(", "outputs", ")", "\n", "out_probs", "=", "out_probs", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "#print(probs)", "\n", "\n", "for", "cur_output", "in", "out_probs", ":", "\n", "            ", "predicted", ".", "append", "(", "cur_output", ".", "index", "(", "max", "(", "cur_output", ")", ")", ")", "\n", "# append both class probs", "\n", "probs", ".", "append", "(", "cur_output", "[", "self", ".", "labelsAsNums", "[", "'OFF'", "]", "]", ")", "\n", "\n", "# should only be length 1 since only one query was sent in", "\n", "", "predicted_label", "=", "self", ".", "numsAsLabels", "[", "str", "(", "predicted", "[", "0", "]", ")", "]", "\n", "label_prob", "=", "probs", "[", "0", "]", "\n", "\n", "return", "predicted_label", ",", "label_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.predictMultiple": [[245, 269], ["NULI.NULI.model.eval", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "NULI.NULI.model", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist", "NULI.NULI.preprocessQuery", "NULI.NULI.tokenizeQuery", "test.append", "predicted.append", "probs.append", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu().numpy", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach().cpu", "str", "cur_output.index", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist.detach", "max"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.preprocessQuery", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.NULI.tokenizeQuery"], ["", "def", "predictMultiple", "(", "self", ",", "test_queries", ")", ":", "\n", "\n", "        ", "test", "=", "[", "]", "\n", "for", "test_query", "in", "test_queries", ":", "\n", "            ", "text", "=", "self", ".", "preprocessQuery", "(", "test_query", ")", "\n", "ids_review", "=", "self", ".", "tokenizeQuery", "(", "text", ")", "\n", "test", ".", "append", "(", "ids_review", ")", "\n", "\n", "\n", "", "self", ".", "model", ".", "eval", "(", ")", "\n", "# make prediction", "\n", "predicted", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "\n", "tests", "=", "torch", ".", "tensor", "(", "test", ")", "\n", "outputs", "=", "self", ".", "model", "(", "tests", ")", "\n", "outputs", "=", "outputs", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "for", "cur_output", "in", "outputs", ":", "\n", "            ", "predicted", ".", "append", "(", "self", ".", "numsAsLabels", "[", "str", "(", "cur_output", ".", "index", "(", "max", "(", "cur_output", ")", ")", ")", "]", ")", "\n", "# append positive class probs", "\n", "probs", ".", "append", "(", "cur_output", "[", "self", ".", "labelsAsNums", "[", "'OFF'", "]", "]", ")", "\n", "\n", "", "return", "predicted", ",", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.text_dataset.__init__": [[274, 279], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "x_y_list", ",", "max_seq_length", ",", "transform", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "x_y_list", "=", "x_y_list", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "transform", "=", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.text_dataset.__getitem__": [[281, 306], ["BertTokenizer.from_pretrained", "BertTokenizer.from_pretrained.tokenize", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "len", "numpy.array"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "        ", "max_seq_length", "=", "self", ".", "max_seq_length", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "tokenized_review", "=", "tokenizer", ".", "tokenize", "(", "self", ".", "x_y_list", "[", "0", "]", "[", "index", "]", ")", "\n", "\n", "if", "len", "(", "tokenized_review", ")", ">", "max_seq_length", ":", "\n", "            ", "tokenized_review", "=", "tokenized_review", "[", ":", "max_seq_length", "]", "\n", "\n", "", "ids_review", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_review", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "ids_review", ")", ")", "\n", "\n", "ids_review", "+=", "padding", "\n", "\n", "assert", "len", "(", "ids_review", ")", "==", "max_seq_length", "\n", "\n", "#print(ids_review)", "\n", "ids_review", "=", "torch", ".", "tensor", "(", "ids_review", ")", "\n", "\n", "sentiment", "=", "self", ".", "x_y_list", "[", "1", "]", "[", "index", "]", "# color        ", "\n", "list_of_labels", "=", "[", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "sentiment", ")", ")", "]", "\n", "\n", "\n", "return", "ids_review", ",", "list_of_labels", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.text_dataset.__len__": [[307, 309], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "x_y_list", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.load_dataset": [[42, 133], ["wordsegment.load", "open", "csv.reader", "open", "csv.reader", "tweet[].lower().strip", "text.split.replace", "text.split.split", "x_train.append", "y_train.append", "tweet[].lower().strip", "text.split.replace", "text.split.split", "testTweets.append", "x_test.append", "wordsegment.segment", "wordsegment.segment", "tweet[].lower", "emoji.demojize", "out_text.append", "tweet[].lower", "emoji.demojize", "out_text.append"], "function", ["None"], ["", "", "def", "load_dataset", "(", "trFile", "=", "None", ",", "teFile", "=", "None", ")", ":", "\n", "    ", "labelsAsNums", "=", "{", "}", "\n", "numsAsLabels", "=", "{", "}", "\n", "labelNum", "=", "0", "\n", "numTweets", "=", "0", "\n", "testTweets", "=", "[", "]", "\n", "\n", "x_train", "=", "[", "]", "\n", "y_train", "=", "[", "]", "\n", "x_test", "=", "[", "]", "\n", "\n", "# NULI used a max_sequence_length of 64", "\n", "max_sequence_length", "=", "64", "\n", "wordsegment", ".", "load", "(", ")", "\n", "\n", "\n", "#load in train tweets and corresponding labels", "\n", "if", "(", "trFile", ")", ":", "\n", "        ", "with", "open", "(", "trFile", ",", "'r'", ")", "as", "csvfile", ":", "\n", "            ", "tweetreader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "tweet", "in", "tweetreader", ":", "\n", "                ", "text", "=", "tweet", "[", "1", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "# emoji used in NULI: https://github.com/carpedm20/emoji", "\n", "# wordsegment used in NULI: https://github.com/grantjenks/python-wordsegment", "\n", "text", "=", "' '", ".", "join", "(", "wordsegment", ".", "segment", "(", "emoji", ".", "demojize", "(", "text", ")", ")", ")", "\n", "\n", "#if(len(text.split()) > max_sequence_length):", "\n", "#    max_sequence_length = len(text.split())", "\n", "\n", "# NULI replaced URL with http", "\n", "text", "=", "text", ".", "replace", "(", "'URL'", ",", "'http'", ")", "\n", "\n", "#x_train.append(text)", "\n", "\n", "# NULI limited @USER to three instances", "\n", "text", "=", "text", ".", "split", "(", ")", "\n", "user_count", "=", "0", "\n", "out_text", "=", "[", "]", "\n", "\n", "for", "word", "in", "text", ":", "\n", "                    ", "if", "(", "word", "==", "'@USER'", ")", ":", "\n", "                        ", "user_count", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "user_count", "=", "0", "\n", "\n", "", "if", "(", "user_count", "<=", "3", ")", ":", "\n", "                        ", "out_text", ".", "append", "(", "word", ")", "\n", "", "", "text", "=", "' '", ".", "join", "(", "out_text", ")", "\n", "\n", "x_train", ".", "append", "(", "text", ")", "\n", "\n", "if", "tweet", "[", "2", "]", "not", "in", "labelsAsNums", ":", "\n", "                    ", "labelsAsNums", "[", "tweet", "[", "2", "]", "]", "=", "labelNum", "\n", "numsAsLabels", "[", "labelNum", "]", "=", "tweet", "[", "2", "]", "\n", "labelNum", "+=", "1", "\n", "", "y_train", ".", "append", "(", "labelsAsNums", "[", "tweet", "[", "2", "]", "]", ")", "\n", "\n", "\n", "#load in test tweets and corresponding labels", "\n", "", "", "", "if", "(", "teFile", ")", ":", "\n", "        ", "with", "open", "(", "teFile", ",", "'r'", ")", "as", "csvfile", ":", "\n", "            ", "tweetreader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "tweet", "in", "tweetreader", ":", "\n", "                ", "text", "=", "tweet", "[", "1", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "# emoji used in NULI: https://github.com/carpedm20/emoji", "\n", "# wordsegment used in NULI: https://github.com/grantjenks/python-wordsegment", "\n", "text", "=", "' '", ".", "join", "(", "wordsegment", ".", "segment", "(", "emoji", ".", "demojize", "(", "text", ")", ")", ")", "\n", "\n", "# NULI replaced URL with http", "\n", "text", "=", "text", ".", "replace", "(", "'URL'", ",", "'http'", ")", "\n", "\n", "# NULI limited @USER to three instances", "\n", "text", "=", "text", ".", "split", "(", ")", "\n", "user_count", "=", "0", "\n", "out_text", "=", "[", "]", "\n", "\n", "for", "word", "in", "text", ":", "\n", "                    ", "if", "(", "word", "==", "'@USER'", ")", ":", "\n", "                        ", "user_count", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "user_count", "=", "0", "\n", "\n", "", "if", "(", "user_count", "<=", "3", ")", ":", "\n", "                        ", "out_text", ".", "append", "(", "word", ")", "\n", "", "", "text", "=", "' '", ".", "join", "(", "out_text", ")", "\n", "\n", "testTweets", ".", "append", "(", "tweet", ")", "\n", "x_test", ".", "append", "(", "text", ")", "\n", "\n", "\n", "", "", "", "return", "x_train", ",", "y_train", ",", "x_test", ",", "labelNum", ",", "testTweets", ",", "labelsAsNums", ",", "numsAsLabels", ",", "max_sequence_length", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.train_model": [[311, 355], ["time.time", "print", "copy.deepcopy", "range", "print", "model.state_dict", "open", "print", "print", "open.write", "open.write", "open.write", "open.close", "scheduler.step", "model.train", "inputs.to.to", "labels.to", "optimizer.zero_grad", "model", "criterion", "criterion.backward", "optimizer.step", "str", "datetime.datetime.now"], "function", ["None"], ["", "", "def", "train_model", "(", "model", ",", "criterion", ",", "optimizer", ",", "scheduler", ",", "dataloaders_dict", ",", "device", ",", "num_epochs", "=", "2", ")", ":", "\n", "    ", "since", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'starting'", ")", "\n", "best_model_wts", "=", "copy", ".", "deepcopy", "(", "model", ".", "state_dict", "(", ")", ")", "\n", "best_loss", "=", "100", "\n", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "        ", "nlog", "=", "open", "(", "log", ",", "'a'", ")", "\n", "print", "(", "'Epoch {}/{}'", ".", "format", "(", "epoch", ",", "num_epochs", "-", "1", ")", ")", "\n", "print", "(", "'-'", "*", "10", ")", "\n", "nlog", ".", "write", "(", "'Epoch {}/{}\\n'", ".", "format", "(", "epoch", ",", "num_epochs", "-", "1", ")", ")", "\n", "nlog", ".", "write", "(", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "+", "'\\n'", ")", "\n", "nlog", ".", "write", "(", "'-'", "*", "10", "+", "'\\n'", ")", "\n", "nlog", ".", "close", "(", ")", "\n", "\n", "scheduler", ".", "step", "(", ")", "\n", "model", ".", "train", "(", ")", "# Set model to training mode", "\n", "\n", "running_loss", "=", "0.0", "\n", "\n", "sentiment_corrects", "=", "0", "\n", "\n", "phase", "=", "'train'", "\n", "# Iterate over data.", "\n", "for", "inputs", ",", "labels", "in", "dataloaders_dict", "[", "phase", "]", ":", "\n", "#inputs = inputs", "\n", "#print(len(inputs),type(inputs),inputs)", "\n", "#inputs = torch.from_numpy(np.array(inputs)).to(device) ", "\n", "            ", "inputs", "=", "inputs", ".", "to", "(", "device", ")", "\n", "\n", "sentiment", "=", "labels", ".", "to", "(", "device", ")", "\n", "\n", "# zero the parameter gradients", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# forward", "\n", "#print(inputs)", "\n", "outputs", "=", "model", "(", "inputs", ")", "\n", "loss", "=", "criterion", "(", "outputs", ",", "labels", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "print", "(", "\"Finished training\"", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.main": [[358, 495], ["open", "print", "open.write", "open.write", "open.close", "open", "print", "open.write", "open.write", "open.close", "BertTokenizer.from_pretrained", "torch.device", "torch.device", "torch.device", "torch.device", "NULI.load_dataset", "open", "open.readlines", "json.loads", "NULI.load_dataset", "NULI.text_dataset", "BertConfig", "NULI.BertForSequenceClassification", "torch.Adam", "torch.CrossEntropyLoss", "torch.optim.lr_scheduler.StepLR", "NULI.train_model", "torch.save", "torch.save", "torch.save", "torch.save", "json.dumps", "open", "open.write", "open.close", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load.eval", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.load.", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy().tolist", "range", "open.close", "str", "str", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "BertTokenizer.from_pretrained.tokenize", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "torch.tensor.append", "open", "open", "predicted.append", "len", "open.write", "datetime.datetime.now", "datetime.datetime.now", "len", "len", "outputs.detach().cpu().numpy().tolist.detach().cpu().numpy", "cur_output.index", "BertForSequenceClassification.bert.parameters", "BertForSequenceClassification.classifier.parameters", "len", "max", "outputs.detach().cpu().numpy().tolist.detach().cpu", "str", "outputs.detach().cpu().numpy().tolist.detach", "str"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.load_dataset", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.load_dataset", "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.NULI.train_model"], ["", "def", "main", "(", "trainFile", ",", "testFile", ",", "train_test", ",", "out_file", "=", "None", ")", ":", "\n", "#trainFile = 'offenseval-training-v1.tsv'", "\n", "#testFile = 'testset-taska_part1.tsv'", "\n", "#trainFile = 'small_offenseval-training-v1.tsv'", "\n", "#testFile = 'small_testset-taska.tsv'", "\n", "    ", "nlog", "=", "open", "(", "log", ",", "'a'", ")", "\n", "print", "(", "'loading data...'", ")", "\n", "nlog", ".", "write", "(", "'loading data...\\n'", ")", "\n", "nlog", ".", "write", "(", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "+", "'\\n'", ")", "\n", "nlog", ".", "close", "(", ")", "\n", "\n", "if", "(", "train_test", "==", "'train'", "or", "train_test", "==", "'both'", ")", ":", "\n", "        ", "x_train", ",", "y_train", ",", "x_test", ",", "labelNum", ",", "testTweets", ",", "labelsAsNums", ",", "numsAsLabels", ",", "max_seq_length", "=", "load_dataset", "(", "trainFile", ",", "testFile", ")", "\n", "", "else", ":", "\n", "# load in params", "\n", "        ", "params_in", "=", "open", "(", "'NULI_params.json'", ")", "\n", "params_lines", "=", "params_in", ".", "readlines", "(", ")", "\n", "params", "=", "json", ".", "loads", "(", "params_lines", "[", "0", "]", ")", "\n", "\n", "labelNum", "=", "params", "[", "'labelNum'", "]", "\n", "labelsAsNums", "=", "params", "[", "'labelsAsNums'", "]", "\n", "numsAsLabels", "=", "params", "[", "'numsAsLabels'", "]", "\n", "max_seq_length", "=", "params", "[", "'max_seq_length'", "]", "\n", "_", ",", "_", ",", "x_test", ",", "_", ",", "testTweets", ",", "_", ",", "_", ",", "_", "=", "load_dataset", "(", "None", ",", "testFile", ")", "\n", "\n", "#else:", "\n", "#    # load in params", "\n", "#    params_in = open('NULI_params.json')", "\n", "#    params_lines = params_in.readlines()", "\n", "#    params = json.loads(params_lines[0])", "\n", "\n", "#   labelNum = params['labelNum']", "\n", "#    labelsAsNums = params['labelsAsNums']", "\n", "#    numsAsLabels = params['numsAsLabels']", "\n", "#    max_seq_length = params['max_seq_length']", "\n", "\n", "\n", "", "nlog", "=", "open", "(", "log", ",", "'a'", ")", "\n", "print", "(", "'finished loading data...'", ")", "\n", "nlog", ".", "write", "(", "'finished loading data...\\n'", ")", "\n", "nlog", ".", "write", "(", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "+", "'\\n'", ")", "\n", "nlog", ".", "close", "(", ")", "\n", "batch_size", "=", "32", "\n", "# Load pre-trained model tokenizer (vocabulary)", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda:0\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "#print(device)", "\n", "\n", "if", "(", "train_test", "==", "'train'", "or", "train_test", "==", "'both'", ")", ":", "\n", "        ", "train_lists", "=", "[", "x_train", ",", "y_train", "]", "\n", "\n", "training_dataset", "=", "text_dataset", "(", "x_y_list", "=", "train_lists", ",", "max_seq_length", "=", "max_seq_length", ")", "\n", "\n", "dataloaders_dict", "=", "{", "'train'", ":", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "training_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "0", ")", "}", "\n", "dataset_sizes", "=", "{", "'train'", ":", "len", "(", "train_lists", "[", "0", "]", ")", "}", "\n", "\n", "num_labels", "=", "labelNum", "+", "1", "\n", "config", "=", "BertConfig", "(", ")", "#vocab_size_or_config_json_file=32000, hidden_size=768,", "\n", "#num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)", "\n", "model", "=", "BertForSequenceClassification", "(", "config", ",", "num_labels", ")", "\n", "lrlast", "=", ".001", "\n", "lrmain", "=", ".00001", "\n", "optim1", "=", "optim", ".", "Adam", "(", "\n", "[", "\n", "{", "\"params\"", ":", "model", ".", "bert", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lrmain", "}", ",", "\n", "{", "\"params\"", ":", "model", ".", "classifier", ".", "parameters", "(", ")", ",", "\"lr\"", ":", "lrlast", "}", ",", "\n", "\n", "]", ")", "\n", "\n", "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)", "\n", "# Observe that all parameters are being optimized", "\n", "optimizer_ft", "=", "optim1", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "# Decay LR by a factor of 0.1 every 7 epochs", "\n", "exp_lr_scheduler", "=", "lr_scheduler", ".", "StepLR", "(", "optimizer_ft", ",", "step_size", "=", "3", ",", "gamma", "=", "0.1", ")", "\n", "\n", "model_ft1", "=", "train_model", "(", "model", ",", "criterion", ",", "optimizer_ft", ",", "exp_lr_scheduler", ",", "dataloaders_dict", ",", "device", ",", "num_epochs", "=", "3", ")", "\n", "\n", "torch", ".", "save", "(", "model_ft1", ",", "'NULI.pt'", ")", "\n", "\n", "# create a json to store dictionaries and information for loading use.", "\n", "out_info", "=", "{", "'labelNum'", ":", "labelNum", ",", "'labelsAsNums'", ":", "labelsAsNums", ",", "'numsAsLabels'", ":", "numsAsLabels", ",", "'max_seq_length'", ":", "max_seq_length", "}", "\n", "\n", "outtmp", "=", "json", ".", "dumps", "(", "out_info", ")", "\n", "\n", "outfile", "=", "open", "(", "'NULI_params.json'", ",", "'w'", ")", "\n", "outfile", ".", "write", "(", "outtmp", ")", "\n", "outfile", ".", "close", "(", ")", "\n", "\n", "\n", "", "if", "(", "train_test", "==", "'test'", "or", "train_test", "==", "'both'", ")", ":", "\n", "\n", "        ", "model_ft1", "=", "torch", ".", "load", "(", "'NULI.pt'", ")", "\n", "model_ft1", ".", "eval", "(", ")", "\n", "\n", "# load test set for predictions", "\n", "tests", "=", "[", "]", "\n", "for", "cur_x", "in", "x_test", ":", "\n", "\n", "            ", "tokenized_review", "=", "tokenizer", ".", "tokenize", "(", "cur_x", ")", "\n", "\n", "if", "len", "(", "tokenized_review", ")", ">", "max_seq_length", ":", "\n", "                ", "tokenized_review", "=", "tokenized_review", "[", ":", "max_seq_length", "]", "\n", "\n", "", "ids_review", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenized_review", ")", "\n", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "ids_review", ")", ")", "\n", "\n", "ids_review", "+=", "padding", "\n", "\n", "assert", "len", "(", "ids_review", ")", "==", "max_seq_length", "\n", "\n", "#print(ids_review)", "\n", "#ids_review = torch.tensor(ids_review)", "\n", "\n", "tests", ".", "append", "(", "ids_review", ")", "\n", "\n", "", "if", "(", "out_file", "==", "None", ")", ":", "\n", "            ", "prediction_file", "=", "open", "(", "'NULI_predictionsOut'", ",", "'w'", ")", "\n", "", "else", ":", "\n", "            ", "prediction_file", "=", "open", "(", "out_file", ",", "'w'", ")", "\n", "\n", "", "predicted", "=", "[", "]", "\n", "\n", "tests", "=", "torch", ".", "tensor", "(", "tests", ")", "\n", "outputs", "=", "model_ft1", "(", "tests", ")", "\n", "outputs", "=", "outputs", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "#print(outputs)", "\n", "for", "cur_output", "in", "outputs", ":", "\n", "            ", "predicted", ".", "append", "(", "cur_output", ".", "index", "(", "max", "(", "cur_output", ")", ")", ")", "\n", "\n", "# write predictions to file", "\n", "", "for", "j", "in", "range", "(", "len", "(", "predicted", ")", ")", ":", "\n", "            ", "prediction_file", ".", "write", "(", "str", "(", "testTweets", "[", "j", "]", "[", "0", "]", ")", "+", "','", "+", "numsAsLabels", "[", "str", "(", "predicted", "[", "j", "]", ")", "]", "+", "'\\n'", ")", "\n", "\n", "", "prediction_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.__init__": [[11, 16], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "threshold", "=", "0.5", ")", ":", "\n", "        ", "self", ".", "threshold", "=", "threshold", "\n", "api_key", "=", "''", "\n", "self", ".", "url", "=", "(", "'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze'", "+", "\n", "'?key='", "+", "api_key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.predict": [[18, 51], ["test_query.lower().strip", "Perspective.createRequestDict", "time.sleep", "test_query.lower", "requests.post", "json.loads", "time.sleep", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.createRequestDict"], ["", "def", "predict", "(", "self", ",", "test_query", ")", ":", "\n", "        ", "text", "=", "test_query", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "request", "=", "createRequestDict", "(", "text", ")", "\n", "\n", "# sleep 2 second to comply with perspective api", "\n", "time", ".", "sleep", "(", "2", ")", "\n", "\n", "success", "=", "False", "\n", "\n", "response", "=", "\"\"", "\n", "response_dict", "=", "{", "}", "\n", "\n", "while", "(", "not", "success", ")", ":", "\n", "# send request to google perspective api", "\n", "            ", "try", ":", "\n", "                ", "response", "=", "requests", ".", "post", "(", "url", "=", "self", ".", "url", ",", "data", "=", "json", ".", "dumps", "(", "request", ")", ")", "\n", "response_dict", "=", "json", ".", "loads", "(", "response", ".", "content", ")", "\n", "success", "=", "True", "\n", "", "except", ":", "\n", "# sleep 5 minutes if no response from server", "\n", "                ", "time", ".", "sleep", "(", "300", ")", "\n", "\n", "", "", "if", "(", "'attributeScores'", "in", "response_dict", ")", ":", "\n", "            ", "toxicity_level", "=", "response_dict", "[", "'attributeScores'", "]", "[", "'TOXICITY'", "]", "[", "'summaryScore'", "]", "[", "'value'", "]", "\n", "", "else", ":", "\n", "            ", "toxicity_level", "=", "0.0", "\n", "\n", "", "if", "(", "toxicity_level", ">", "self", ".", "threshold", ")", ":", "\n", "            ", "prediction", "=", "'OFF'", "\n", "", "else", ":", "\n", "            ", "prediction", "=", "'NOT'", "\n", "\n", "", "return", "prediction", ",", "toxicity_level", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.predictMultiple": [[54, 63], ["Perspective.Perspective.predict", "predictions.append", "probs.append"], "methods", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.Perspective.predict"], ["", "def", "predictMultiple", "(", "self", ",", "test_queries", ")", ":", "\n", "        ", "predictions", "=", "[", "]", "\n", "probs", "=", "[", "]", "\n", "for", "test", "in", "test_queries", ":", "\n", "            ", "pred", ",", "prob", "=", "self", ".", "predict", "(", "test", ")", "\n", "predictions", ".", "append", "(", "pred", ")", "\n", "probs", ".", "append", "(", "prob", ")", "\n", "\n", "", "return", "predictions", ",", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.createRequestDict": [[66, 74], ["None"], "function", ["None"], ["", "", "def", "createRequestDict", "(", "text", ")", ":", "\n", "    ", "data_dict", "=", "{", "\n", "'comment'", ":", "{", "'text'", ":", "text", "}", ",", "\n", "'languages'", ":", "[", "'en'", "]", ",", "\n", "'requestedAttributes'", ":", "{", "'TOXICITY'", ":", "{", "}", "}", "\n", "}", "\n", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.PerspectivePredict": [[76, 114], ["float", "open", "open", "open", "csv.reader", "open.close", "tweet[].lower().strip", "Perspective.createRequestDict", "time.sleep", "requests.post", "json.loads", "open.write", "tweet[].lower", "json.dumps"], "function", ["home.repos.pwc.inspect_result.jonrusert_suumcuique.code.Perspective.createRequestDict"], ["", "def", "PerspectivePredict", "(", "testFile", ",", "threshold", "=", "0.5", ",", "outfile", "=", "None", ")", ":", "\n", "    ", "api_key", "=", "''", "\n", "url", "=", "(", "'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze'", "+", "\n", "'?key='", "+", "api_key", ")", "\n", "\n", "threshold", "=", "float", "(", "threshold", ")", "\n", "\n", "if", "(", "outfile", ")", ":", "\n", "        ", "output", "=", "open", "(", "outfile", ",", "'w'", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "open", "(", "'perspectivePredictionsOut'", ",", "'w'", ")", "\n", "\n", "# walk through test file and predict offensive or not", "\n", "", "with", "open", "(", "testFile", ",", "'r'", ")", "as", "csvfile", ":", "\n", "        ", "tweetreader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "'\\t'", ")", "\n", "for", "tweet", "in", "tweetreader", ":", "\n", "            ", "text", "=", "tweet", "[", "1", "]", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "request", "=", "createRequestDict", "(", "text", ")", "\n", "\n", "# sleep 2 seconds to comply with perspective api ", "\n", "time", ".", "sleep", "(", "2", ")", "\n", "# send request to google perspective api", "\n", "response", "=", "requests", ".", "post", "(", "url", "=", "url", ",", "data", "=", "json", ".", "dumps", "(", "request", ")", ")", "\n", "response_dict", "=", "json", ".", "loads", "(", "response", ".", "content", ")", "\n", "\n", "if", "(", "'attributeScores'", "in", "response_dict", ")", ":", "\n", "                ", "toxicity_level", "=", "response_dict", "[", "'attributeScores'", "]", "[", "'TOXICITY'", "]", "[", "'summaryScore'", "]", "[", "'value'", "]", "\n", "", "else", ":", "\n", "                ", "toxicity_level", "=", "0.0", "\n", "\n", "", "if", "(", "toxicity_level", ">", "threshold", ")", ":", "\n", "                ", "prediction", "=", "'OFF'", "\n", "", "else", ":", "\n", "                ", "prediction", "=", "'NOT'", "\n", "\n", "", "output", ".", "write", "(", "tweet", "[", "0", "]", "+", "','", "+", "prediction", "+", "'\\n'", ")", "\n", "\n", "", "output", ".", "close", "(", ")", "\n", "\n"]]}